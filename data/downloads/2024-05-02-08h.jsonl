{"created":"2024-05-01 17:59:20","title":"Self-Play Preference Optimization for Language Model Alignment","abstract":"Traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment. In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbed \\textit{Self-Play Preference Optimization} (SPPO), approximates the Nash equilibrium through iterative policy updates and enjoys theoretical convergence guarantee. Our method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO). In our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and the Open LLM Leaderboard. Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models.","sentences":["Traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences.","Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment.","In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy.","Our approach, dubbed \\textit{Self-Play Preference Optimization} (SPPO), approximates the Nash equilibrium through iterative policy updates and enjoys theoretical convergence guarantee.","Our method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO).","In our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0.","It also outperforms the (iterative) DPO and IPO on MT-Bench and the Open LLM Leaderboard.","Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.)","from GPT-4 or other stronger language models."],"url":"http://arxiv.org/abs/2405.00675v1","category":"cs.LG"}
{"created":"2024-05-01 17:57:21","title":"TexSliders: Diffusion-Based Texture Editing in CLIP Space","abstract":"Generative models have enabled intuitive image creation and manipulation using natural language. In particular, diffusion models have recently shown remarkable results for natural image editing. In this work, we propose to apply diffusion techniques to edit textures, a specific class of images that are an essential part of 3D content creation pipelines. We analyze existing editing methods and show that they are not directly applicable to textures, since their common underlying approach, manipulating attention maps, is unsuitable for the texture domain. To address this, we propose a novel approach that instead manipulates CLIP image embeddings to condition the diffusion generation. We define editing directions using simple text prompts (e.g., \"aged wood\" to \"new wood\") and map these to CLIP image embedding space using a texture prior, with a sampling-based approach that gives us identity-preserving directions in CLIP space. To further improve identity preservation, we project these directions to a CLIP subspace that minimizes identity variations resulting from entangled texture attributes. Our editing pipeline facilitates the creation of arbitrary sliders using natural language prompts only, with no ground-truth annotated data necessary.","sentences":["Generative models have enabled intuitive image creation and manipulation using natural language.","In particular, diffusion models have recently shown remarkable results for natural image editing.","In this work, we propose to apply diffusion techniques to edit textures, a specific class of images that are an essential part of 3D content creation pipelines.","We analyze existing editing methods and show that they are not directly applicable to textures, since their common underlying approach, manipulating attention maps, is unsuitable for the texture domain.","To address this, we propose a novel approach that instead manipulates CLIP image embeddings to condition the diffusion generation.","We define editing directions using simple text prompts (e.g., \"aged wood\" to \"new wood\") and map these to CLIP image embedding space using a texture prior, with a sampling-based approach that gives us identity-preserving directions in CLIP space.","To further improve identity preservation, we project these directions to a CLIP subspace that minimizes identity variations resulting from entangled texture attributes.","Our editing pipeline facilitates the creation of arbitrary sliders using natural language prompts only, with no ground-truth annotated data necessary."],"url":"http://arxiv.org/abs/2405.00672v1","category":"cs.GR"}
{"created":"2024-05-01 17:57:18","title":"The scalar product formula for parahoric Deligne--Lusztig induction","abstract":"Parahoric Deligne--Lusztig induction gives rise to positive-depth representations of parahoric subgroups of $p$-adic groups. The most fundamental basic question about parahoric Deligne--Lusztig induction is whether it satisfies the scalar product formula. We resolve this conjecture for all split-generic pairs $(T,\\theta)$ -- in particular, for all characters $\\theta$ if $T$ is elliptic.","sentences":["Parahoric Deligne--Lusztig induction gives rise to positive-depth representations of parahoric subgroups of $p$-adic groups.","The most fundamental basic question about parahoric Deligne--Lusztig induction is whether it satisfies the scalar product formula.","We resolve this conjecture for all split-generic pairs $(T,\\theta)$ -- in particular, for all characters $\\theta$ if $T$ is elliptic."],"url":"http://arxiv.org/abs/2405.00671v1","category":"math.RT"}
{"created":"2024-05-01 17:57:12","title":"Adapting Pretrained Networks for Image Quality Assessment on High Dynamic Range Displays","abstract":"Conventional image quality metrics (IQMs), such as PSNR and SSIM, are designed for perceptually uniform gamma-encoded pixel values and cannot be directly applied to perceptually non-uniform linear high-dynamic-range (HDR) colors. Similarly, most of the available datasets consist of standard-dynamic-range (SDR) images collected in standard and possibly uncontrolled viewing conditions. Popular pre-trained neural networks are likewise intended for SDR inputs, restricting their direct application to HDR content. On the other hand, training HDR models from scratch is challenging due to limited available HDR data. In this work, we explore more effective approaches for training deep learning-based models for image quality assessment (IQA) on HDR data. We leverage networks pre-trained on SDR data (source domain) and re-target these models to HDR (target domain) with additional fine-tuning and domain adaptation. We validate our methods on the available HDR IQA datasets, demonstrating that models trained with our combined recipe outperform previous baselines, converge much quicker, and reliably generalize to HDR inputs.","sentences":["Conventional image quality metrics (IQMs), such as PSNR and SSIM, are designed for perceptually uniform gamma-encoded pixel values and cannot be directly applied to perceptually non-uniform linear high-dynamic-range (HDR) colors.","Similarly, most of the available datasets consist of standard-dynamic-range (SDR) images collected in standard and possibly uncontrolled viewing conditions.","Popular pre-trained neural networks are likewise intended for SDR inputs, restricting their direct application to HDR content.","On the other hand, training HDR models from scratch is challenging due to limited available HDR data.","In this work, we explore more effective approaches for training deep learning-based models for image quality assessment (IQA) on HDR data.","We leverage networks pre-trained on SDR data (source domain) and re-target these models to HDR (target domain) with additional fine-tuning and domain adaptation.","We validate our methods on the available HDR IQA datasets, demonstrating that models trained with our combined recipe outperform previous baselines, converge much quicker, and reliably generalize to HDR inputs."],"url":"http://arxiv.org/abs/2405.00670v1","category":"cs.CV"}
{"created":"2024-05-01 17:54:48","title":"Environmental cosmic acceleration from a phase transition in the dark sector","abstract":"A new degravitation mechanism within the framework of scalar tensor gravity is proposed. The mechanism eliminates all constant contributions from the potential to the Friedmann equation, leaving only the kinematic and the dynamic terms of the potential to drive cosmic acceleration. We explore a scenario involving a density-triggered phase transition in the late-time universe, and argue that the resulting effective energy density and equation of state parameter can explain late-time cosmology when extrapolated to a region of the parameter space.","sentences":["A new degravitation mechanism within the framework of scalar tensor gravity is proposed.","The mechanism eliminates all constant contributions from the potential to the Friedmann equation, leaving only the kinematic and the dynamic terms of the potential to drive cosmic acceleration.","We explore a scenario involving a density-triggered phase transition in the late-time universe, and argue that the resulting effective energy density and equation of state parameter can explain late-time cosmology when extrapolated to a region of the parameter space."],"url":"http://arxiv.org/abs/2405.00668v1","category":"astro-ph.CO"}
{"created":"2024-05-01 17:54:05","title":"RGB$\\leftrightarrow$X: Image decomposition and synthesis using material- and lighting-aware diffusion models","abstract":"The three areas of realistic forward rendering, per-pixel inverse rendering, and generative image synthesis may seem like separate and unrelated sub-fields of graphics and vision. However, recent work has demonstrated improved estimation of per-pixel intrinsic channels (albedo, roughness, metallicity) based on a diffusion architecture; we call this the RGB$\\rightarrow$X problem. We further show that the reverse problem of synthesizing realistic images given intrinsic channels, X$\\rightarrow$RGB, can also be addressed in a diffusion framework.   Focusing on the image domain of interior scenes, we introduce an improved diffusion model for RGB$\\rightarrow$X, which also estimates lighting, as well as the first diffusion X$\\rightarrow$RGB model capable of synthesizing realistic images from (full or partial) intrinsic channels. Our X$\\rightarrow$RGB model explores a middle ground between traditional rendering and generative models: we can specify only certain appearance properties that should be followed, and give freedom to the model to hallucinate a plausible version of the rest.   This flexibility makes it possible to use a mix of heterogeneous training datasets, which differ in the available channels. We use multiple existing datasets and extend them with our own synthetic and real data, resulting in a model capable of extracting scene properties better than previous work and of generating highly realistic images of interior scenes.","sentences":["The three areas of realistic forward rendering, per-pixel inverse rendering, and generative image synthesis may seem like separate and unrelated sub-fields of graphics and vision.","However, recent work has demonstrated improved estimation of per-pixel intrinsic channels (albedo, roughness, metallicity) based on a diffusion architecture; we call this the RGB$\\rightarrow$X problem.","We further show that the reverse problem of synthesizing realistic images given intrinsic channels, X$\\rightarrow$RGB, can also be addressed in a diffusion framework.   ","Focusing on the image domain of interior scenes, we introduce an improved diffusion model for RGB$\\rightarrow$X, which also estimates lighting, as well as the first diffusion X$\\rightarrow$RGB model capable of synthesizing realistic images from (full or partial) intrinsic channels.","Our X$\\rightarrow$RGB model explores a middle ground between traditional rendering and generative models: we can specify only certain appearance properties that should be followed, and give freedom to the model to hallucinate a plausible version of the rest.   ","This flexibility makes it possible to use a mix of heterogeneous training datasets, which differ in the available channels.","We use multiple existing datasets and extend them with our own synthetic and real data, resulting in a model capable of extracting scene properties better than previous work and of generating highly realistic images of interior scenes."],"url":"http://arxiv.org/abs/2405.00666v1","category":"cs.CV"}
{"created":"2024-05-01 17:50:37","title":"Is Bigger Edit Batch Size Always Better? -- An Empirical Study on Model Editing with Llama-3","abstract":"This study presents a targeted model editing analysis focused on the latest large language model, Llama-3. We explore the efficacy of popular model editing techniques - ROME, MEMIT, and EMMET, which are designed for precise layer interventions. We identify the most effective layers for targeted edits through an evaluation that encompasses up to 4096 edits across three distinct strategies: sequential editing, batch editing, and a hybrid approach we call as sequential-batch editing. Our findings indicate that increasing edit batch-sizes may degrade model performance more significantly than using smaller edit batches sequentially for equal number of edits. With this, we argue that sequential model editing is an important component for scaling model editing methods and future research should focus on methods that combine both batched and sequential editing. This observation suggests a potential limitation in current model editing methods which push towards bigger edit batch sizes, and we hope it paves way for future investigations into optimizing batch sizes and model editing performance.","sentences":["This study presents a targeted model editing analysis focused on the latest large language model, Llama-3.","We explore the efficacy of popular model editing techniques - ROME, MEMIT, and EMMET, which are designed for precise layer interventions.","We identify the most effective layers for targeted edits through an evaluation that encompasses up to 4096 edits across three distinct strategies: sequential editing, batch editing, and a hybrid approach we call as sequential-batch editing.","Our findings indicate that increasing edit batch-sizes may degrade model performance more significantly than using smaller edit batches sequentially for equal number of edits.","With this, we argue that sequential model editing is an important component for scaling model editing methods and future research should focus on methods that combine both batched and sequential editing.","This observation suggests a potential limitation in current model editing methods which push towards bigger edit batch sizes, and we hope it paves way for future investigations into optimizing batch sizes and model editing performance."],"url":"http://arxiv.org/abs/2405.00664v1","category":"cs.CL"}
{"created":"2024-05-01 17:50:36","title":"Quantum cryptographic protocols with dual messaging system via 2D alternate quantum walks and genuine single particle entangled states","abstract":"Single-particle entangled states (SPES) can offer a more secure way of encoding and processing quantum information than their multi-particle counterparts. The SPES generated via a 2D alternate quantum-walk setup from initially separable states can be either 3-way or 2-way entangled. This letter shows that the generated genuine three-way and nonlocal two-way SPES can be used as cryptographic keys to securely encode two distinct messages simultaneously. We detail the message encryption-decryption steps and show the resilience of the 3-way and 2-way SPES-based cryptographic protocols against eavesdropper attacks like intercept-and-resend and man-in-the-middle. We also detail how these protocols can be experimentally realized using single photons, with the three degrees of freedom being OAM, path, and polarization. These have unparalleled security for quantum communication tasks. The ability to simultaneously encode two distinct messages using the generated SPES showcases the versatility and efficiency of the proposed cryptographic protocol. This capability could significantly improve the throughput of quantum communication systems.","sentences":["Single-particle entangled states (SPES) can offer a more secure way of encoding and processing quantum information than their multi-particle counterparts.","The SPES generated via a 2D alternate quantum-walk setup from initially separable states can be either 3-way or 2-way entangled.","This letter shows that the generated genuine three-way and nonlocal two-way SPES can be used as cryptographic keys to securely encode two distinct messages simultaneously.","We detail the message encryption-decryption steps and show the resilience of the 3-way and 2-way SPES-based cryptographic protocols against eavesdropper attacks like intercept-and-resend and man-in-the-middle.","We also detail how these protocols can be experimentally realized using single photons, with the three degrees of freedom being OAM, path, and polarization.","These have unparalleled security for quantum communication tasks.","The ability to simultaneously encode two distinct messages using the generated SPES showcases the versatility and efficiency of the proposed cryptographic protocol.","This capability could significantly improve the throughput of quantum communication systems."],"url":"http://arxiv.org/abs/2405.00663v1","category":"quant-ph"}
{"created":"2024-05-01 17:50:16","title":"No Representation, No Trust: Connecting Representation, Collapse, and Trust Issues in PPO","abstract":"Reinforcement learning (RL) is inherently rife with non-stationarity since the states and rewards the agent observes during training depend on its changing policy. Therefore, networks in deep RL must be capable of adapting to new observations and fitting new targets. However, previous works have observed that networks in off-policy deep value-based methods exhibit a decrease in representation rank, often correlated with an inability to continue learning or a collapse in performance. Although this phenomenon has generally been attributed to neural network learning under non-stationarity, it has been overlooked in on-policy policy optimization methods which are often thought capable of training indefinitely. In this work, we empirically study representation dynamics in Proximal Policy Optimization (PPO) on the Atari and MuJoCo environments, revealing that PPO agents are also affected by feature rank deterioration and loss of plasticity. We show that this is aggravated with stronger non-stationarity, ultimately driving the actor's performance to collapse, regardless of the performance of the critic. We draw connections between representation collapse, performance collapse, and trust region issues in PPO, and present Proximal Feature Optimization (PFO), a novel auxiliary loss, that along with other interventions shows that regularizing the representation dynamics improves the performance of PPO agents.","sentences":["Reinforcement learning (RL) is inherently rife with non-stationarity since the states and rewards the agent observes during training depend on its changing policy.","Therefore, networks in deep RL must be capable of adapting to new observations and fitting new targets.","However, previous works have observed that networks in off-policy deep value-based methods exhibit a decrease in representation rank, often correlated with an inability to continue learning or a collapse in performance.","Although this phenomenon has generally been attributed to neural network learning under non-stationarity, it has been overlooked in on-policy policy optimization methods which are often thought capable of training indefinitely.","In this work, we empirically study representation dynamics in Proximal Policy Optimization (PPO) on the Atari and MuJoCo environments, revealing that PPO agents are also affected by feature rank deterioration and loss of plasticity.","We show that this is aggravated with stronger non-stationarity, ultimately driving the actor's performance to collapse, regardless of the performance of the critic.","We draw connections between representation collapse, performance collapse, and trust region issues in PPO, and present Proximal Feature Optimization (PFO), a novel auxiliary loss, that along with other interventions shows that regularizing the representation dynamics improves the performance of PPO agents."],"url":"http://arxiv.org/abs/2405.00662v1","category":"cs.LG"}
{"created":"2024-05-01 17:50:06","title":"Towards quantum gravity with neural networks: Solving quantum Hamilton constraints of 3d Euclidean gravity in the weak coupling limit","abstract":"We consider 3-dimensional Euclidean gravity in the weak coupling limit of Smolin and show that it is BF-theory with $\\text{U(1)}^3$ as a Lie group. The theory is quantised using loop quantum gravity methods. The kinematical degrees of freedom are truncated, on account of computational feasibility, by fixing a graph and deforming the algebra of the holonomies to impose a cutoff on the charge vectors. This leads to a quantum theory related to $\\text{U}_q \\text{(1)}^3$ BF-theory. The effect of imposing the cutoff on the charges is examined. We also implement the quantum volume operator of 3d loop quantum gravity. Most importantly we compare two constraints for the quantum model obtained: a master constraint enforcing curvature and Gauss constraint, as well as a combination of a quantum Hamilton constraint constructed using Thiemann's strategy and the Gauss master constraint. The two constraints are solved using the neural network quantum state ansatz, demonstrating its ability to explore models which are out of reach for exact numerical methods. The solutions spaces are quantitatively compared and although the forms of the constraints are radically different, the solutions turn out to have a surprisingly large overlap. We also investigate the behavior of the quantum volume in solutions to the constraints.","sentences":["We consider 3-dimensional Euclidean gravity in the weak coupling limit of Smolin and show that it is BF-theory with $\\text{U(1)}^3$ as a Lie group.","The theory is quantised using loop quantum gravity methods.","The kinematical degrees of freedom are truncated, on account of computational feasibility, by fixing a graph and deforming the algebra of the holonomies to impose a cutoff on the charge vectors.","This leads to a quantum theory related to $\\text{U}_q \\text{(1)}^3$ BF-theory.","The effect of imposing the cutoff on the charges is examined.","We also implement the quantum volume operator of 3d loop quantum gravity.","Most importantly we compare two constraints for the quantum model obtained: a master constraint enforcing curvature and Gauss constraint, as well as a combination of a quantum Hamilton constraint constructed using Thiemann's strategy and the Gauss master constraint.","The two constraints are solved using the neural network quantum state ansatz, demonstrating its ability to explore models which are out of reach for exact numerical methods.","The solutions spaces are quantitatively compared and although the forms of the constraints are radically different, the solutions turn out to have a surprisingly large overlap.","We also investigate the behavior of the quantum volume in solutions to the constraints."],"url":"http://arxiv.org/abs/2405.00661v1","category":"gr-qc"}
{"created":"2024-05-01 17:44:05","title":"NLU-STR at SemEval-2024 Task 1: Generative-based Augmentation and Encoder-based Scoring for Semantic Textual Relatedness","abstract":"Semantic textual relatedness is a broader concept of semantic similarity. It measures the extent to which two chunks of text convey similar meaning or topics, or share related concepts or contexts. This notion of relatedness can be applied in various applications, such as document clustering and summarizing. SemRel-2024, a shared task in SemEval-2024, aims at reducing the gap in the semantic relatedness task by providing datasets for fourteen languages and dialects including Arabic. This paper reports on our participation in Track A (Algerian and Moroccan dialects) and Track B (Modern Standard Arabic). A BERT-based model is augmented and fine-tuned for regression scoring in supervised track (A), while BERT-based cosine similarity is employed for unsupervised track (B). Our system ranked 1st in SemRel-2024 for MSA with a Spearman correlation score of 0.49. We ranked 5th for Moroccan and 12th for Algerian with scores of 0.83 and 0.53, respectively.","sentences":["Semantic textual relatedness is a broader concept of semantic similarity.","It measures the extent to which two chunks of text convey similar meaning or topics, or share related concepts or contexts.","This notion of relatedness can be applied in various applications, such as document clustering and summarizing.","SemRel-2024, a shared task in SemEval-2024, aims at reducing the gap in the semantic relatedness task by providing datasets for fourteen languages and dialects including Arabic.","This paper reports on our participation in Track A (Algerian and Moroccan dialects) and Track B (Modern Standard Arabic).","A BERT-based model is augmented and fine-tuned for regression scoring in supervised track (A), while BERT-based cosine similarity is employed for unsupervised track (B).","Our system ranked 1st in SemRel-2024 for MSA with a Spearman correlation score of 0.49.","We ranked 5th for Moroccan and 12th for Algerian with scores of 0.83 and 0.53, respectively."],"url":"http://arxiv.org/abs/2405.00659v1","category":"cs.CL"}
{"created":"2024-05-01 17:37:50","title":"RST-LoRA: A Discourse-Aware Low-Rank Adaptation for Long Document Abstractive Summarization","abstract":"For long document summarization, discourse structure is important to discern the key content of the text and the differences in importance level between sentences. Unfortunately, the integration of rhetorical structure theory (RST) into parameter-efficient fine-tuning strategies for long document summarization remains unexplored. Therefore, this paper introduces RST-LoRA and proposes four RST-aware variants to explicitly incorporate RST into the LoRA model. Our empirical evaluation demonstrates that incorporating the type and uncertainty of rhetorical relations can complementarily enhance the performance of LoRA in summarization tasks. Furthermore, the best-performing variant we introduced outperforms the vanilla LoRA and full-parameter fine-tuning models, as confirmed by multiple automatic and human evaluations, and even surpasses previous state-of-the-art methods.","sentences":["For long document summarization, discourse structure is important to discern the key content of the text and the differences in importance level between sentences.","Unfortunately, the integration of rhetorical structure theory (RST) into parameter-efficient fine-tuning strategies for long document summarization remains unexplored.","Therefore, this paper introduces RST-LoRA and proposes four RST-aware variants to explicitly incorporate RST into the LoRA model.","Our empirical evaluation demonstrates that incorporating the type and uncertainty of rhetorical relations can complementarily enhance the performance of LoRA in summarization tasks.","Furthermore, the best-performing variant we introduced outperforms the vanilla LoRA and full-parameter fine-tuning models, as confirmed by multiple automatic and human evaluations, and even surpasses previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.00657v1","category":"cs.CL"}
{"created":"2024-05-01 17:34:32","title":"Linearly simplified QAOA parameters and transferability","abstract":"Quantum Approximate Optimization Algorithm (QAOA) provides a way to solve combinatorial optimization problems using quantum computers. QAOA circuits consist of time evolution operators by the cost Hamiltonian and of state mixing operators, and embedded variational parameter for each operator is tuned so that the expectation value of the cost function is minimized. The optimization of the variational parameters is taken place on classical devices while the cost function is measured in the sense of quantum. To facilitate the classical optimization, there are several previous works on making decision strategies for optimal/initial parameters and on extracting similarities among instances. In our current work, we consider simplified QAOA parameters that take linear forms along with the depth in the circuit. Such a simplification, which would be suggested from an analogy to quantum annealing, leads to a drastic reduction of the parameter space from 2p to 4 dimensions with the any number of QAOA layers p. In addition, cost landscapes in the reduced parameter space have some stability on differing instances. This fact suggests that an optimal parameter set for a given instance can be transferred to other instances. In this paper we present some numerical results that are obtained for instances of the random Ising model and of the max-cut problem. The transferability of linearized parameters is demonstrated for randomly generated source and destination instances, and its dependence on features of the instances are investigated.","sentences":["Quantum Approximate Optimization Algorithm (QAOA) provides a way to solve combinatorial optimization problems using quantum computers.","QAOA circuits consist of time evolution operators by the cost Hamiltonian and of state mixing operators, and embedded variational parameter for each operator is tuned so that the expectation value of the cost function is minimized.","The optimization of the variational parameters is taken place on classical devices while the cost function is measured in the sense of quantum.","To facilitate the classical optimization, there are several previous works on making decision strategies for optimal/initial parameters and on extracting similarities among instances.","In our current work, we consider simplified QAOA parameters that take linear forms along with the depth in the circuit.","Such a simplification, which would be suggested from an analogy to quantum annealing, leads to a drastic reduction of the parameter space from 2p to 4 dimensions with the any number of QAOA layers p.","In addition, cost landscapes in the reduced parameter space have some stability on differing instances.","This fact suggests that an optimal parameter set for a given instance can be transferred to other instances.","In this paper we present some numerical results that are obtained for instances of the random Ising model and of the max-cut problem.","The transferability of linearized parameters is demonstrated for randomly generated source and destination instances, and its dependence on features of the instances are investigated."],"url":"http://arxiv.org/abs/2405.00655v1","category":"quant-ph"}
{"created":"2024-05-01 17:27:11","title":"Grains of Saliency: Optimizing Saliency-based Training of Biometric Attack Detection Models","abstract":"Incorporating human-perceptual intelligence into model training has shown to increase the generalization capability of models in several difficult biometric tasks, such as presentation attack detection (PAD) and detection of synthetic samples. After the initial collection phase, human visual saliency (e.g., eye-tracking data, or handwritten annotations) can be integrated into model training through attention mechanisms, augmented training samples, or through human perception-related components of loss functions. Despite their successes, a vital, but seemingly neglected, aspect of any saliency-based training is the level of salience granularity (e.g., bounding boxes, single saliency maps, or saliency aggregated from multiple subjects) necessary to find a balance between reaping the full benefits of human saliency and the cost of its collection. In this paper, we explore several different levels of salience granularity and demonstrate that increased generalization capabilities of PAD and synthetic face detection can be achieved by using simple yet effective saliency post-processing techniques across several different CNNs.","sentences":["Incorporating human-perceptual intelligence into model training has shown to increase the generalization capability of models in several difficult biometric tasks, such as presentation attack detection (PAD) and detection of synthetic samples.","After the initial collection phase, human visual saliency (e.g., eye-tracking data, or handwritten annotations) can be integrated into model training through attention mechanisms, augmented training samples, or through human perception-related components of loss functions.","Despite their successes, a vital, but seemingly neglected, aspect of any saliency-based training is the level of salience granularity (e.g., bounding boxes, single saliency maps, or saliency aggregated from multiple subjects) necessary to find a balance between reaping the full benefits of human saliency and the cost of its collection.","In this paper, we explore several different levels of salience granularity and demonstrate that increased generalization capabilities of PAD and synthetic face detection can be achieved by using simple yet effective saliency post-processing techniques across several different CNNs."],"url":"http://arxiv.org/abs/2405.00650v1","category":"cs.CV"}
{"created":"2024-05-01 17:24:42","title":"HalluVault: A Novel Logic Programming-aided Metamorphic Testing Framework for Detecting Fact-Conflicting Hallucinations in Large Language Models","abstract":"Large language models (LLMs) have transformed the landscape of language processing, yet struggle with significant challenges in terms of security, privacy, and the generation of seemingly coherent but factually inaccurate outputs, commonly referred to as hallucinations. Among these challenges, one particularly pressing issue is Fact-Conflicting Hallucination (FCH), where LLMs generate content that directly contradicts established facts. Tackling FCH poses a formidable task due to two primary obstacles: Firstly, automating the construction and updating of benchmark datasets is challenging, as current methods rely on static benchmarks that don't cover the diverse range of FCH scenarios. Secondly, validating LLM outputs' reasoning process is inherently complex, especially with intricate logical relations involved.   In addressing these obstacles, we propose an innovative approach leveraging logic programming to enhance metamorphic testing for detecting Fact-Conflicting Hallucinations (FCH). Our method gathers data from sources like Wikipedia, expands it with logical reasoning to create diverse test cases, assesses LLMs through structured prompts, and validates their coherence using semantic-aware assessment mechanisms. Our method generates test cases and detects hallucinations across six different LLMs spanning nine domains, revealing hallucination rates ranging from 24.7% to 59.8%. Key observations indicate that LLMs encounter challenges, particularly with temporal concepts, handling out-of-distribution knowledge, and exhibiting deficiencies in logical reasoning capabilities. The outcomes underscore the efficacy of logic-based test cases generated by our tool in both triggering and identifying hallucinations. These findings underscore the imperative for ongoing collaborative endeavors within the community to detect and address LLM hallucinations.","sentences":["Large language models (LLMs) have transformed the landscape of language processing, yet struggle with significant challenges in terms of security, privacy, and the generation of seemingly coherent but factually inaccurate outputs, commonly referred to as hallucinations.","Among these challenges, one particularly pressing issue is Fact-Conflicting Hallucination (FCH), where LLMs generate content that directly contradicts established facts.","Tackling FCH poses a formidable task due to two primary obstacles: Firstly, automating the construction and updating of benchmark datasets is challenging, as current methods rely on static benchmarks that don't cover the diverse range of FCH scenarios.","Secondly, validating LLM outputs' reasoning process is inherently complex, especially with intricate logical relations involved.   ","In addressing these obstacles, we propose an innovative approach leveraging logic programming to enhance metamorphic testing for detecting Fact-Conflicting Hallucinations (FCH).","Our method gathers data from sources like Wikipedia, expands it with logical reasoning to create diverse test cases, assesses LLMs through structured prompts, and validates their coherence using semantic-aware assessment mechanisms.","Our method generates test cases and detects hallucinations across six different LLMs spanning nine domains, revealing hallucination rates ranging from 24.7% to 59.8%.","Key observations indicate that LLMs encounter challenges, particularly with temporal concepts, handling out-of-distribution knowledge, and exhibiting deficiencies in logical reasoning capabilities.","The outcomes underscore the efficacy of logic-based test cases generated by our tool in both triggering and identifying hallucinations.","These findings underscore the imperative for ongoing collaborative endeavors within the community to detect and address LLM hallucinations."],"url":"http://arxiv.org/abs/2405.00648v1","category":"cs.SE"}
{"created":"2024-05-01 17:24:20","title":"Screening of BindingDB database ligands against EGFR, HER2, Estrogen, Progesterone and NF-kB receptors based on machine learning and molecular docking","abstract":"Breast cancer, the second most prevalent cancer among women worldwide, necessitates the exploration of novel therapeutic approaches. To target the four subgroups of breast cancer \"hormone receptor-positive and HER2-negative, hormone receptor-positive and HER2-positive, hormone receptor-negative and HER2-positive, and hormone receptor-negative and HER2-negative\" it is crucial to inhibit specific targets such as EGFR, HER2, ER, NF-kB, and PR.   In this study, we evaluated various methods for binary and multiclass classification. Among them, the GA-SVM-SVM:GA-SVM-SVM model was selected with an accuracy of 0.74, an F1-score of 0.73, and an AUC of 0.94 for virtual screening of ligands from the BindingDB database. This model successfully identified 4454, 803, 438, and 378 ligands with over 90% precision in both active/inactive and target prediction for the classes of EGFR+HER2, ER, NF-kB, and PR, respectively, from the BindingDB database. Based on to the selected ligands, we created a dendrogram that categorizes different ligands based on their targets. This dendrogram aims to facilitate the exploration of chemical space for various therapeutic targets.   Ligands that surpassed a 90% threshold in the product of activity probability and correct target selection probability were chosen for further investigation using molecular docking. The binding energy range for these ligands against their respective targets was calculated to be between -15 and -5 kcal/mol. Finally, based on general and common rules in medicinal chemistry, we selected 2, 3, 3, and 8 new ligands with high priority for further studies in the EGFR+HER2, ER, NF-kB, and PR classes, respectively.","sentences":["Breast cancer, the second most prevalent cancer among women worldwide, necessitates the exploration of novel therapeutic approaches.","To target the four subgroups of breast cancer \"hormone receptor-positive and HER2-negative, hormone receptor-positive and HER2-positive, hormone receptor-negative and HER2-positive, and hormone receptor-negative and HER2-negative\" it is crucial to inhibit specific targets such as EGFR, HER2, ER, NF-kB, and PR.   ","In this study, we evaluated various methods for binary and multiclass classification.","Among them, the GA-SVM-SVM:GA-SVM-SVM model was selected with an accuracy of 0.74, an F1-score of 0.73, and an AUC of 0.94 for virtual screening of ligands from the BindingDB database.","This model successfully identified 4454, 803, 438, and 378 ligands with over 90% precision in both active/inactive and target prediction for the classes of EGFR+HER2, ER, NF-kB, and PR, respectively, from the BindingDB database.","Based on to the selected ligands, we created a dendrogram that categorizes different ligands based on their targets.","This dendrogram aims to facilitate the exploration of chemical space for various therapeutic targets.   ","Ligands that surpassed a 90% threshold in the product of activity probability and correct target selection probability were chosen for further investigation using molecular docking.","The binding energy range for these ligands against their respective targets was calculated to be between -15 and -5 kcal/mol.","Finally, based on general and common rules in medicinal chemistry, we selected 2, 3, 3, and 8 new ligands with high priority for further studies in the EGFR+HER2, ER, NF-kB, and PR classes, respectively."],"url":"http://arxiv.org/abs/2405.00647v1","category":"physics.med-ph"}
{"created":"2024-05-01 17:21:36","title":"Learning to Compose: Improving Object Centric Learning by Injecting Compositionality","abstract":"Learning compositional representation is a key aspect of object-centric learning as it enables flexible systematic generalization and supports complex visual reasoning. However, most of the existing approaches rely on auto-encoding objective, while the compositionality is implicitly imposed by the architectural or algorithmic bias in the encoder. This misalignment between auto-encoding objective and learning compositionality often results in failure of capturing meaningful object representations. In this study, we propose a novel objective that explicitly encourages compositionality of the representations. Built upon the existing object-centric learning framework (e.g., slot attention), our method incorporates additional constraints that an arbitrary mixture of object representations from two images should be valid by maximizing the likelihood of the composite data. We demonstrate that incorporating our objective to the existing framework consistently improves the objective-centric learning and enhances the robustness to the architectural choices.","sentences":["Learning compositional representation is a key aspect of object-centric learning as it enables flexible systematic generalization and supports complex visual reasoning.","However, most of the existing approaches rely on auto-encoding objective, while the compositionality is implicitly imposed by the architectural or algorithmic bias in the encoder.","This misalignment between auto-encoding objective and learning compositionality often results in failure of capturing meaningful object representations.","In this study, we propose a novel objective that explicitly encourages compositionality of the representations.","Built upon the existing object-centric learning framework (e.g., slot attention), our method incorporates additional constraints that an arbitrary mixture of object representations from two images should be valid by maximizing the likelihood of the composite data.","We demonstrate that incorporating our objective to the existing framework consistently improves the objective-centric learning and enhances the robustness to the architectural choices."],"url":"http://arxiv.org/abs/2405.00646v1","category":"cs.CV"}
{"created":"2024-05-01 17:17:22","title":"ConstrainedZero: Chance-Constrained POMDP Planning using Learned Probabilistic Failure Surrogates and Adaptive Safety Constraints","abstract":"To plan safely in uncertain environments, agents must balance utility with safety constraints. Safe planning problems can be modeled as a chance-constrained partially observable Markov decision process (CC-POMDP) and solutions often use expensive rollouts or heuristics to estimate the optimal value and action-selection policy. This work introduces the ConstrainedZero policy iteration algorithm that solves CC-POMDPs in belief space by learning neural network approximations of the optimal value and policy with an additional network head that estimates the failure probability given a belief. This failure probability guides safe action selection during online Monte Carlo tree search (MCTS). To avoid overemphasizing search based on the failure estimates, we introduce $\\Delta$-MCTS, which uses adaptive conformal inference to update the failure threshold during planning. The approach is tested on a safety-critical POMDP benchmark, an aircraft collision avoidance system, and the sustainability problem of safe CO$_2$ storage. Results show that by separating safety constraints from the objective we can achieve a target level of safety without optimizing the balance between rewards and costs.","sentences":["To plan safely in uncertain environments, agents must balance utility with safety constraints.","Safe planning problems can be modeled as a chance-constrained partially observable Markov decision process (CC-POMDP) and solutions often use expensive rollouts or heuristics to estimate the optimal value and action-selection policy.","This work introduces the ConstrainedZero policy iteration algorithm that solves CC-POMDPs in belief space by learning neural network approximations of the optimal value and policy with an additional network head that estimates the failure probability given a belief.","This failure probability guides safe action selection during online Monte Carlo tree search (MCTS).","To avoid overemphasizing search based on the failure estimates, we introduce $\\Delta$-MCTS, which uses adaptive conformal inference to update the failure threshold during planning.","The approach is tested on a safety-critical POMDP benchmark, an aircraft collision avoidance system, and the sustainability problem of safe CO$_2$ storage.","Results show that by separating safety constraints from the objective we can achieve a target level of safety without optimizing the balance between rewards and costs."],"url":"http://arxiv.org/abs/2405.00644v1","category":"cs.AI"}
{"created":"2024-05-01 17:07:22","title":"Engine-fed Kilonovae (Mergernovae) -- II. Radiation","abstract":"The radioactive power generated by materials within the ejecta of a binary-neutron-star (BNS) merger powers an optical transient known as a kilonova. When the central remnant of a BNS merger is a long-lived magnetar, it continuously produces a highly magnetized wind, altering both the dynamics and temperature of the ejecta, leading to the expected emergence of an engine-fed kilonova. In the first paper of this series, we conducted a detailed study of the dynamics of wind-ejecta interaction and the efficiency of energy injection through shocks. In this work, we combine this dynamical evolution with both shock-heating and additional X-ray irradiation to model photon diffusion within a constant-opacity ejecta. By calculating the radiation, we obtain the light curve and spectral energy distribution (SED). Our findings reveal that, with energy injection, a blue bump typically appears in the early stages ($\\lesssim 1$ day). Furthermore, if the magnetar has not spun down by that time, a brightening in the later stages occurs. Despite this, in a large parameter space, the expected luminosity of the engine-fed kilonova is not significantly higher than the typical r-process kilonova due to limited heating efficiency. The SED of engine-fed kilonovae peaks in the relatively blue band in the early stages and evolves towards the red, but at a slower rate compared to the typical r-process kilonova.","sentences":["The radioactive power generated by materials within the ejecta of a binary-neutron-star (BNS) merger powers an optical transient known as a kilonova.","When the central remnant of a BNS merger is a long-lived magnetar, it continuously produces a highly magnetized wind, altering both the dynamics and temperature of the ejecta, leading to the expected emergence of an engine-fed kilonova.","In the first paper of this series, we conducted a detailed study of the dynamics of wind-ejecta interaction and the efficiency of energy injection through shocks.","In this work, we combine this dynamical evolution with both shock-heating and additional X-ray irradiation to model photon diffusion within a constant-opacity ejecta.","By calculating the radiation, we obtain the light curve and spectral energy distribution (SED).","Our findings reveal that, with energy injection, a blue bump typically appears in the early stages ($\\lesssim 1$ day).","Furthermore, if the magnetar has not spun down by that time, a brightening in the later stages occurs.","Despite this, in a large parameter space, the expected luminosity of the engine-fed kilonova is not significantly higher than the typical r-process kilonova due to limited heating efficiency.","The SED of engine-fed kilonovae peaks in the relatively blue band in the early stages and evolves towards the red, but at a slower rate compared to the typical r-process kilonova."],"url":"http://arxiv.org/abs/2405.00638v1","category":"astro-ph.HE"}
{"created":"2024-05-01 17:01:02","title":"Observational constraints on the stellar recycled gas in active galactic nuclei feeding","abstract":"Near-infrared long-slit spectroscopy has been used to study the stellar population (SP) of the low luminosity active galactic nuclei (AGN) and matched analogues (LLAMA) sample. To perform the SP fits we have employed the X-shooter simple stellar population models together with the \\st\\ code. Our main conclusions are: The star formation history of the AGNs is very complex, presenting many episodes of star formation during their lifetimes. In general, AGN hosts have higher fractions of intermediate-age SP (light-weighted mean ages, $<t>_L\\lesssim$ 4.5 Gyr) when compared with their analogues ($<t>_L\\lesssim$ 8.0 Gyr). AGN are more affected by reddening and require significant fractions of featureless continuum and hot dust components. The ratio between the AGN radiated energy and the gravitational potential energy of the molecular gas ($E_{Rad}$/$E_{PG}$) for the AGN is compared with the \\maL\\ and a possible anti-correlation is observed. This suggests that the AGN is affecting the star formation in these galaxies, in the sense that more energetic AGN (log$(E_{Rad}$/$E_{PG}) \\gtrsim 3$) tend to host nuclear younger SP ($ <t>_L \\lesssim$4Gyr). We found that the recent ($t<$2~Gyr) returned (recycled) stellar mass is higher in AGN than in the controls. We also provide evidence that the mass loss of stars would be enough to feed the AGN, thus providing observational constraints for models that predict that AGN feeding is partially due to the recycled gas from dying stars.","sentences":["Near-infrared long-slit spectroscopy has been used to study the stellar population (SP) of the low luminosity active galactic nuclei (AGN) and matched analogues (LLAMA) sample.","To perform the SP fits we have employed the X-shooter simple stellar population models together with the \\st\\ code.","Our main conclusions are: The star formation history of the AGNs is very complex, presenting many episodes of star formation during their lifetimes.","In general, AGN hosts have higher fractions of intermediate-age SP (light-weighted mean ages, $<t>_L\\lesssim$ 4.5 Gyr) when compared with their analogues ($<t>_L\\lesssim$ 8.0 Gyr).","AGN are more affected by reddening and require significant fractions of featureless continuum and hot dust components.","The ratio between the AGN radiated energy and the gravitational potential energy of the molecular gas ($E_{Rad}$/$E_{PG}$) for the AGN is compared with the \\maL\\ and a possible anti-correlation is observed.","This suggests that the AGN is affecting the star formation in these galaxies, in the sense that more energetic AGN (log$(E_{Rad}$/$E_{PG})","\\gtrsim 3$) tend to host nuclear younger SP ($ <t>_L \\lesssim$4Gyr).","We found that the recent ($t<$2~Gyr) returned (recycled) stellar mass is higher in AGN than in the controls.","We also provide evidence that the mass loss of stars would be enough to feed the AGN, thus providing observational constraints for models that predict that AGN feeding is partially due to the recycled gas from dying stars."],"url":"http://arxiv.org/abs/2405.00634v1","category":"astro-ph.GA"}
{"created":"2024-05-01 16:58:32","title":"Predictions for Composite Higgs models from gauge/gravity dualities","abstract":"Gauge/gravity dualities provide a very useful approach into solving strongly coupled systems. We apply this to Composite Higgs models and determine the mass hierarchies of the corresponding bound states. As a cross check we apply this to QCD and compare the results to existing lattice calculations for which we find good agreement. We then focus on a particular example whose phenomenology has recently been studied in the literature in a generic way and outline first phenomenological implications of our findings for the spectrum.","sentences":["Gauge/gravity dualities provide a very useful approach into solving strongly coupled systems.","We apply this to Composite Higgs models and determine the mass hierarchies of the corresponding bound states.","As a cross check we apply this to QCD and compare the results to existing lattice calculations for which we find good agreement.","We then focus on a particular example whose phenomenology has recently been studied in the literature in a generic way and outline first phenomenological implications of our findings for the spectrum."],"url":"http://arxiv.org/abs/2405.00633v1","category":"hep-ph"}
{"created":"2024-05-01 16:58:28","title":"When Quantization Affects Confidence of Large Language Models?","abstract":"Recent studies introduced effective compression techniques for Large Language Models (LLMs) via post-training quantization or low-bit weight representation. Although quantized weights offer storage efficiency and allow for faster inference, existing works have indicated that quantization might compromise performance and exacerbate biases in LLMs. This study investigates the confidence and calibration of quantized models, considering factors such as language model type and scale as contributors to quantization loss. Firstly, we reveal that quantization with GPTQ to 4-bit results in a decrease in confidence regarding true labels, with varying impacts observed among different language models. Secondly, we observe fluctuations in the impact on confidence across different scales. Finally, we propose an explanation for quantization loss based on confidence levels, indicating that quantization disproportionately affects samples where the full model exhibited low confidence levels in the first place.","sentences":["Recent studies introduced effective compression techniques for Large Language Models (LLMs) via post-training quantization or low-bit weight representation.","Although quantized weights offer storage efficiency and allow for faster inference, existing works have indicated that quantization might compromise performance and exacerbate biases in LLMs.","This study investigates the confidence and calibration of quantized models, considering factors such as language model type and scale as contributors to quantization loss.","Firstly, we reveal that quantization with GPTQ to 4-bit results in a decrease in confidence regarding true labels, with varying impacts observed among different language models.","Secondly, we observe fluctuations in the impact on confidence across different scales.","Finally, we propose an explanation for quantization loss based on confidence levels, indicating that quantization disproportionately affects samples where the full model exhibited low confidence levels in the first place."],"url":"http://arxiv.org/abs/2405.00632v1","category":"cs.CL"}
{"created":"2024-05-01 16:58:22","title":"Deep Metric Learning-Based Out-of-Distribution Detection with Synthetic Outlier Exposure","abstract":"In this paper, we present a novel approach that combines deep metric learning and synthetic data generation using diffusion models for out-of-distribution (OOD) detection. One popular approach for OOD detection is outlier exposure, where models are trained using a mixture of in-distribution (ID) samples and ``seen\" OOD samples. For the OOD samples, the model is trained to minimize the KL divergence between the output probability and the uniform distribution while correctly classifying the in-distribution (ID) data. In this paper, we propose a label-mixup approach to generate synthetic OOD data using Denoising Diffusion Probabilistic Models (DDPMs). Additionally, we explore recent advancements in metric learning to train our models.   In the experiments, we found that metric learning-based loss functions perform better than the softmax. Furthermore, the baseline models (including softmax, and metric learning) show a significant improvement when trained with the generated OOD data. Our approach outperforms strong baselines in conventional OOD detection metrics.","sentences":["In this paper, we present a novel approach that combines deep metric learning and synthetic data generation using diffusion models for out-of-distribution (OOD) detection.","One popular approach for OOD detection is outlier exposure, where models are trained using a mixture of in-distribution (ID) samples and ``seen\" OOD samples.","For the OOD samples, the model is trained to minimize the KL divergence between the output probability and the uniform distribution while correctly classifying the in-distribution (ID) data.","In this paper, we propose a label-mixup approach to generate synthetic OOD data using Denoising Diffusion Probabilistic Models (DDPMs).","Additionally, we explore recent advancements in metric learning to train our models.   ","In the experiments, we found that metric learning-based loss functions perform better than the softmax.","Furthermore, the baseline models (including softmax, and metric learning) show a significant improvement when trained with the generated OOD data.","Our approach outperforms strong baselines in conventional OOD detection metrics."],"url":"http://arxiv.org/abs/2405.00631v1","category":"cs.CV"}
{"created":"2024-05-01 16:55:08","title":"Depth Priors in Removal Neural Radiance Fields","abstract":"Neural Radiance Fields (NeRF) have shown impressive results in 3D reconstruction and generating novel views. A key challenge within NeRF is the editing of reconstructed scenes, such as object removal, which requires maintaining consistency across multiple views and ensuring high-quality synthesised perspectives. Previous studies have incorporated depth priors, typically from LiDAR or sparse depth measurements provided by COLMAP, to improve the performance of object removal in NeRF. However, these methods are either costly or time-consuming. In this paper, we propose a novel approach that integrates monocular depth estimates with NeRF-based object removal models to significantly reduce time consumption and enhance the robustness and quality of scene generation and object removal. We conducted a thorough evaluation of COLMAP's dense depth reconstruction on the KITTI dataset to verify its accuracy in depth map generation. Our findings suggest that COLMAP can serve as an effective alternative to a ground truth depth map where such information is missing or costly to obtain. Additionally, we integrated various monocular depth estimation methods into the removal NeRF model, i.e., SpinNeRF, to assess their capacity to improve object removal performance. Our experimental results highlight the potential of monocular depth estimation to substantially improve NeRF applications.","sentences":["Neural Radiance Fields (NeRF) have shown impressive results in 3D reconstruction and generating novel views.","A key challenge within NeRF is the editing of reconstructed scenes, such as object removal, which requires maintaining consistency across multiple views and ensuring high-quality synthesised perspectives.","Previous studies have incorporated depth priors, typically from LiDAR or sparse depth measurements provided by COLMAP, to improve the performance of object removal in NeRF.","However, these methods are either costly or time-consuming.","In this paper, we propose a novel approach that integrates monocular depth estimates with NeRF-based object removal models to significantly reduce time consumption and enhance the robustness and quality of scene generation and object removal.","We conducted a thorough evaluation of COLMAP's dense depth reconstruction on the KITTI dataset to verify its accuracy in depth map generation.","Our findings suggest that COLMAP can serve as an effective alternative to a ground truth depth map where such information is missing or costly to obtain.","Additionally, we integrated various monocular depth estimation methods into the removal NeRF model, i.e., SpinNeRF, to assess their capacity to improve object removal performance.","Our experimental results highlight the potential of monocular depth estimation to substantially improve NeRF applications."],"url":"http://arxiv.org/abs/2405.00630v1","category":"cs.CV"}
{"created":"2024-05-01 16:54:12","title":"HUGO -- Highlighting Unseen Grid Options: Combining Deep Reinforcement Learning with a Heuristic Target Topology Approach","abstract":"With the growth of Renewable Energy (RE) generation, the operation of power grids has become increasingly complex. One solution is automated grid operation, where Deep Reinforcement Learning (DRL) has repeatedly shown significant potential in Learning to Run a Power Network (L2RPN) challenges. However, only individual actions at the substation level have been subjected to topology optimization by most existing DRL algorithms. In contrast, we propose a more holistic approach in this paper by proposing specific Target Topologies (TTs) as actions. These topologies are selected based on their robustness. As part of this paper, we present a search algorithm to find the TTs and upgrade our previously developed DRL agent CurriculumAgent (CAgent) to a novel topology agent. We compare the upgrade to the previous CAgent agent and can increase their scores significantly by 10%. Further, we achieve a 25% better median survival with our TTs included. Later analysis shows that almost all TTs are close to the base topology, explaining their robustness.","sentences":["With the growth of Renewable Energy (RE) generation, the operation of power grids has become increasingly complex.","One solution is automated grid operation, where Deep Reinforcement Learning (DRL) has repeatedly shown significant potential in Learning to Run a Power Network (L2RPN) challenges.","However, only individual actions at the substation level have been subjected to topology optimization by most existing DRL algorithms.","In contrast, we propose a more holistic approach in this paper by proposing specific Target Topologies (TTs) as actions.","These topologies are selected based on their robustness.","As part of this paper, we present a search algorithm to find the TTs and upgrade our previously developed DRL agent CurriculumAgent (CAgent) to a novel topology agent.","We compare the upgrade to the previous CAgent agent and can increase their scores significantly by 10%.","Further, we achieve a 25% better median survival with our TTs included.","Later analysis shows that almost all TTs are close to the base topology, explaining their robustness."],"url":"http://arxiv.org/abs/2405.00629v1","category":"cs.LG"}
{"created":"2024-05-01 16:47:23","title":"Hysteresis and Self-Oscillations in an Artificial Memristive Quantum Neuron","abstract":"We theoretically study an artificial neuron circuit containing a quantum memristor in the presence of relaxation and dephasing. The charge transport in the quantum element is realized via tunneling of a charge through a quantum particle which shuttles between two terminals -- a functionality reminiscent of classical diffusive memristors. We demonstrate that this physical principle enables hysteretic behavior of the current-voltage characteristics of the quantum device. In addition, being used in artificial neural circuit, the quantum switcher is able to generate self-sustained current oscillations. Our analysis reveals that these self-oscillations are triggered only in quantum regime with a moderate rate of relaxation, and cannot exist either in a purely coherent regime or at a very high decoherence. We investigate the hysteresis and instability leading to the onset of current self-oscillations and analyze their properties depending on the circuit parameters. Our results provide a generic approach to the use of quantum regimes for controlling hysteresis and generating self-oscillations.","sentences":["We theoretically study an artificial neuron circuit containing a quantum memristor in the presence of relaxation and dephasing.","The charge transport in the quantum element is realized via tunneling of a charge through a quantum particle which shuttles between two terminals -- a functionality reminiscent of classical diffusive memristors.","We demonstrate that this physical principle enables hysteretic behavior of the current-voltage characteristics of the quantum device.","In addition, being used in artificial neural circuit, the quantum switcher is able to generate self-sustained current oscillations.","Our analysis reveals that these self-oscillations are triggered only in quantum regime with a moderate rate of relaxation, and cannot exist either in a purely coherent regime or at a very high decoherence.","We investigate the hysteresis and instability leading to the onset of current self-oscillations and analyze their properties depending on the circuit parameters.","Our results provide a generic approach to the use of quantum regimes for controlling hysteresis and generating self-oscillations."],"url":"http://arxiv.org/abs/2405.00624v1","category":"quant-ph"}
{"created":"2024-05-01 16:43:55","title":"\"I'm Not Sure, But...\": Examining the Impact of Large Language Models' Uncertainty Expression on User Reliance and Trust","abstract":"Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs' expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants' reliance, trust, and overall task performance. We find that first-person expressions (e.g., \"I'm not sure, but...\") decrease participants' confidence in the system and tendency to agree with the system's answers, while increasing participants' accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., \"It's not clear, but...\"), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.","sentences":["Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct.","To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users.","However, there has been little empirical work examining how users perceive and act upon LLMs' expressions of uncertainty.","We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine.","Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants' reliance, trust, and overall task performance.","We find that first-person expressions (e.g., \"I'm not sure, but...\") decrease participants' confidence in the system and tendency to agree with the system's answers, while increasing participants' accuracy.","An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers.","While we observe similar effects for uncertainty expressed from a general perspective (e.g., \"It's not clear, but...\"), these effects are weaker and not statistically significant.","Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters.","This highlights the importance of user testing before deploying LLMs at scale."],"url":"http://arxiv.org/abs/2405.00623v1","category":"cs.HC"}
{"created":"2024-05-01 16:43:21","title":"Causal Evaluation of Language Models","abstract":"Causal reasoning is viewed as crucial for achieving human-level machine intelligence. Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for causal reasoning. In this work, we introduce Causal evaluation of Language Models (CaLM), which, to the best of our knowledge, is the first comprehensive benchmark for evaluating the causal reasoning capabilities of language models. First, we propose the CaLM framework, which establishes a foundational taxonomy consisting of four modules: causal target (i.e., what to evaluate), adaptation (i.e., how to obtain the results), metric (i.e., how to measure the results), and error (i.e., how to analyze the bad results). This taxonomy defines a broad evaluation design space while systematically selecting criteria and priorities. Second, we compose the CaLM dataset, comprising 126,334 data samples, to provide curated sets of causal targets, adaptations, metrics, and errors, offering extensive coverage for diverse research pursuits. Third, we conduct an extensive evaluation of 28 leading language models on a core set of 92 causal targets, 9 adaptations, 7 metrics, and 12 error types. Fourth, we perform detailed analyses of the evaluation results across various dimensions (e.g., adaptation, scale). Fifth, we present 50 high-level empirical findings across 9 dimensions (e.g., model), providing valuable guidance for future language model development. Finally, we develop a multifaceted platform, including a website, leaderboards, datasets, and toolkits, to support scalable and adaptable assessments. We envision CaLM as an ever-evolving benchmark for the community, systematically updated with new causal targets, adaptations, models, metrics, and error types to reflect ongoing research advancements. Project website is at https://opencausalab.github.io/CaLM.","sentences":["Causal reasoning is viewed as crucial for achieving human-level machine intelligence.","Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for causal reasoning.","In this work, we introduce Causal evaluation of Language Models (CaLM), which, to the best of our knowledge, is the first comprehensive benchmark for evaluating the causal reasoning capabilities of language models.","First, we propose the CaLM framework, which establishes a foundational taxonomy consisting of four modules: causal target (i.e., what to evaluate), adaptation (i.e., how to obtain the results), metric (i.e., how to measure the results), and error (i.e., how to analyze the bad results).","This taxonomy defines a broad evaluation design space while systematically selecting criteria and priorities.","Second, we compose the CaLM dataset, comprising 126,334 data samples, to provide curated sets of causal targets, adaptations, metrics, and errors, offering extensive coverage for diverse research pursuits.","Third, we conduct an extensive evaluation of 28 leading language models on a core set of 92 causal targets, 9 adaptations, 7 metrics, and 12 error types.","Fourth, we perform detailed analyses of the evaluation results across various dimensions (e.g., adaptation, scale).","Fifth, we present 50 high-level empirical findings across 9 dimensions (e.g., model), providing valuable guidance for future language model development.","Finally, we develop a multifaceted platform, including a website, leaderboards, datasets, and toolkits, to support scalable and adaptable assessments.","We envision CaLM as an ever-evolving benchmark for the community, systematically updated with new causal targets, adaptations, models, metrics, and error types to reflect ongoing research advancements.","Project website is at https://opencausalab.github.io/CaLM."],"url":"http://arxiv.org/abs/2405.00622v1","category":"cs.CL"}
{"created":"2024-05-01 16:36:24","title":"Universality of the second correlation function of the deformed Ginibre ensemble","abstract":"We study the deformed complex Ginibre ensemble $H=A_0+H_0$, where $H_0$ is the complex matrix with iid Gaussian entries, and $A_0$ is some general $n\\times n$ matrix (it can be random and in this case it is independent of $H_0$). Assuming rather general assumptions on $A_0$, we prove that the asymptotic local behavior of the second correlation function of the eigenvalues of such matrices in the bulk coincides with that for the pure complex Ginibre ensemble.","sentences":["We study the deformed complex Ginibre ensemble $H=A_0+H_0$, where $H_0$ is the complex matrix with iid Gaussian entries, and $A_0$ is some general $n\\times n$ matrix (it can be random and in this case it is independent of $H_0$).","Assuming rather general assumptions on $A_0$, we prove that the asymptotic local behavior of the second correlation function of the eigenvalues of such matrices in the bulk coincides with that for the pure complex Ginibre ensemble."],"url":"http://arxiv.org/abs/2405.00617v1","category":"math-ph"}
{"created":"2024-05-01 16:32:07","title":"Addressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling","abstract":"Large language models (LLMs) with their strong zero-shot topic extraction capabilities offer an alternative to probabilistic topic modelling and closed-set topic classification approaches. As zero-shot topic extractors, LLMs are expected to understand human instructions to generate relevant and non-hallucinated topics based on the given documents. However, LLM-based topic modelling approaches often face difficulties in generating topics with adherence to granularity as specified in human instructions, often resulting in many near-duplicate topics. Furthermore, methods for addressing hallucinated topics generated by LLMs have not yet been investigated. In this paper, we focus on addressing the issues of topic granularity and hallucinations for better LLM-based topic modelling. To this end, we introduce a novel approach that leverages Direct Preference Optimisation (DPO) to fine-tune open-source LLMs, such as Mistral-7B. Our approach does not rely on traditional human annotation to rank preferred answers but employs a reconstruction pipeline to modify raw topics generated by LLMs, thus enabling a fast and efficient training and inference framework. Comparative experiments show that our fine-tuning approach not only significantly improves the LLM's capability to produce more coherent, relevant, and precise topics, but also reduces the number of hallucinated topics.","sentences":["Large language models (LLMs) with their strong zero-shot topic extraction capabilities offer an alternative to probabilistic topic modelling and closed-set topic classification approaches.","As zero-shot topic extractors, LLMs are expected to understand human instructions to generate relevant and non-hallucinated topics based on the given documents.","However, LLM-based topic modelling approaches often face difficulties in generating topics with adherence to granularity as specified in human instructions, often resulting in many near-duplicate topics.","Furthermore, methods for addressing hallucinated topics generated by LLMs have not yet been investigated.","In this paper, we focus on addressing the issues of topic granularity and hallucinations for better LLM-based topic modelling.","To this end, we introduce a novel approach that leverages Direct Preference Optimisation (DPO) to fine-tune open-source LLMs, such as Mistral-7B. Our approach does not rely on traditional human annotation to rank preferred answers but employs a reconstruction pipeline to modify raw topics generated by LLMs, thus enabling a fast and efficient training and inference framework.","Comparative experiments show that our fine-tuning approach not only significantly improves the LLM's capability to produce more coherent, relevant, and precise topics, but also reduces the number of hallucinated topics."],"url":"http://arxiv.org/abs/2405.00611v1","category":"cs.CL"}
{"created":"2024-05-01 16:31:29","title":"Growth in products of matrices: fastest, average, and generic","abstract":"The problems that we consider in this paper are as follows. Let A and B be 2x2 matrices (over reals). Let w(A, B) be a word of length n. After evaluating w(A, B) as a product of matrices, we get a 2x2 matrix, call it W. What is the largest (by the absolute value) possible entry of W, over all w(A, B) of length n, as a function of n? What is the expected absolute value of the largest (by the absolute value) entry in a random product of n matrices, where each matrix is A or B with probability 0.5? What is the Lyapunov exponent for a random matrix product like that? We give partial answer to the first of these questions and an essentially complete answer to the second question. For the third question (the most difficult of the three), we offer a very simple method to produce an upper bound on the Lyapunov exponent in the case where all entries of the matrices A and B are nonnegative.","sentences":["The problems that we consider in this paper are as follows.","Let A and B be 2x2 matrices (over reals).","Let w(A, B) be a word of length n. After evaluating w(A, B) as a product of matrices, we get a 2x2 matrix, call it W.","What is the largest (by the absolute value) possible entry of W, over all w(A, B) of length n, as a function of n?","What is the expected absolute value of the largest (by the absolute value) entry in a random product of n matrices, where each matrix is A or B with probability 0.5?","What is the Lyapunov exponent for a random matrix product like that?","We give partial answer to the first of these questions and an essentially complete answer to the second question.","For the third question (the most difficult of the three), we offer a very simple method to produce an upper bound on the Lyapunov exponent in the case where all entries of the matrices A and B are nonnegative."],"url":"http://arxiv.org/abs/2405.00610v1","category":"math.GR"}
{"created":"2024-05-01 16:28:59","title":"$f(T)$ gravity after DESI Baryon Acoustic Oscillation and DES Supernovae 2024 data","abstract":"In this letter we investigate new constraints on $f(T)$ gravity using the recent Baryon Acoustic Oscillation (BAO) data released by the Dark Energy Spectroscopic Instrument (DESI) and the Type Ia supernovae (SNIa) catalog from the full 5-years of the Dark Energy Survey Supernova Program (DES-SN5YR). The $f(T)$ cosmological models considered are characterised by power law late-time accelerated expansion. Our results show that the combination DESI BAO +$r_d$ CMB Planck suggests a Bayesian preference for late-time $f(T)$ cosmological models over $\\Lambda$CDM, obtaining a value of $H_0=72.4\\pm 2.9$[km/s/Mpc] in agreement with SH0ES collaboration.","sentences":["In this letter we investigate new constraints on $f(T)$ gravity using the recent Baryon Acoustic Oscillation (BAO) data released by the Dark Energy Spectroscopic Instrument (DESI) and the Type Ia supernovae (SNIa) catalog from the full 5-years of the Dark Energy Survey Supernova Program (DES-SN5YR).","The $f(T)$ cosmological models considered are characterised by power law late-time accelerated expansion.","Our results show that the combination DESI BAO +$r_d$ CMB Planck suggests a Bayesian preference for late-time $f(T)$ cosmological models over $\\Lambda$CDM, obtaining a value of $H_0=72.4\\pm 2.9$[km/s/Mpc] in agreement with SH0ES collaboration."],"url":"http://arxiv.org/abs/2405.00608v1","category":"astro-ph.CO"}
{"created":"2024-05-01 16:20:44","title":"Preperiodic points of polynomial dynamical systems over finite fields","abstract":"For a prime $p$, positive integers $r,n$, and a polynomial $f$ with coefficients in $\\mathbb{F}_{p^r}$, let $W_{p,r,n}(f)=f^n\\left(\\mathbb{F}_{p^r}\\right)\\setminus f^{n+1}\\left(\\mathbb{F}_{p^r}\\right)$. As $n$ varies, the $W_{p,r,n}(f)$ partition the set of strictly preperiodic points of the dynamical system induced by the action of $f$ on $\\mathbb{F}_{p^r}$. In this paper we compute statistics of strictly preperiodic points of dynamical systems induced by unicritical polynomials over finite fields by obtaining effective upper bounds for the proportion of $\\mathbb{F}_{p^r}$ lying in a given $W_{p,r,n}(f)$. Moreover, when we generalize our definition of $W_{p,r,n}(f)$, we obtain both upper and lower bounds for the resulting averages.","sentences":["For a prime $p$, positive integers $r,n$, and a polynomial $f$ with coefficients in $\\mathbb{F}_{p^r}$, let $W_{p,r,n}(f)=f^n\\left(\\mathbb{F}_{p^r}\\right)\\setminus f^{n+1}\\left(\\mathbb{F}_{p^r}\\right)$. As $n$ varies, the $W_{p,r,n}(f)$ partition the set of strictly preperiodic points of the dynamical system induced by the action of $f$ on $\\mathbb{F}_{p^r}$. In this paper we compute statistics of strictly preperiodic points of dynamical systems induced by unicritical polynomials over finite fields by obtaining effective upper bounds for the proportion of $\\mathbb{F}_{p^r}$ lying in a given $W_{p,r,n}(f)$.","Moreover, when we generalize our definition of $W_{p,r,n}(f)$, we obtain both upper and lower bounds for the resulting averages."],"url":"http://arxiv.org/abs/2405.00605v1","category":"math.DS"}
{"created":"2024-05-01 16:14:22","title":"Learning Expressive Disentangled Speech Representations with Soft Speech Units and Adversarial Style Augmentation","abstract":"Voice conversion is the task to transform voice characteristics of source speech while preserving content information. Nowadays, self-supervised representation learning models are increasingly utilized in content extraction. However, in these representations, a lot of hidden speaker information leads to timbre leakage while the prosodic information of hidden units lacks use. To address these issues, we propose a novel framework for expressive voice conversion called \"SAVC\" based on soft speech units from HuBert-soft. Taking soft speech units as input, we design an attribute encoder to extract content and prosody features respectively. Specifically, we first introduce statistic perturbation imposed by adversarial style augmentation to eliminate speaker information. Then the prosody is implicitly modeled on soft speech units with knowledge distillation. Experiment results show that the intelligibility and naturalness of converted speech outperform previous work.","sentences":["Voice conversion is the task to transform voice characteristics of source speech while preserving content information.","Nowadays, self-supervised representation learning models are increasingly utilized in content extraction.","However, in these representations, a lot of hidden speaker information leads to timbre leakage while the prosodic information of hidden units lacks use.","To address these issues, we propose a novel framework for expressive voice conversion called \"SAVC\" based on soft speech units from HuBert-soft.","Taking soft speech units as input, we design an attribute encoder to extract content and prosody features respectively.","Specifically, we first introduce statistic perturbation imposed by adversarial style augmentation to eliminate speaker information.","Then the prosody is implicitly modeled on soft speech units with knowledge distillation.","Experiment results show that the intelligibility and naturalness of converted speech outperform previous work."],"url":"http://arxiv.org/abs/2405.00603v1","category":"cs.SD"}
{"created":"2024-05-01 16:13:54","title":"Investigating Automatic Scoring and Feedback using Large Language Models","abstract":"Automatic grading and feedback have been long studied using traditional machine learning and deep learning techniques using language models. With the recent accessibility to high performing large language models (LLMs) like LLaMA-2, there is an opportunity to investigate the use of these LLMs for automatic grading and feedback generation. Despite the increase in performance, LLMs require significant computational resources for fine-tuning and additional specific adjustments to enhance their performance for such tasks. To address these issues, Parameter Efficient Fine-tuning (PEFT) methods, such as LoRA and QLoRA, have been adopted to decrease memory and computational requirements in model fine-tuning. This paper explores the efficacy of PEFT-based quantized models, employing classification or regression head, to fine-tune LLMs for automatically assigning continuous numerical grades to short answers and essays, as well as generating corresponding feedback. We conducted experiments on both proprietary and open-source datasets for our tasks. The results show that prediction of grade scores via finetuned LLMs are highly accurate, achieving less than 3% error in grade percentage on average. For providing graded feedback fine-tuned 4-bit quantized LLaMA-2 13B models outperform competitive base models and achieve high similarity with subject matter expert feedback in terms of high BLEU and ROUGE scores and qualitatively in terms of feedback. The findings from this study provide important insights into the impacts of the emerging capabilities of using quantization approaches to fine-tune LLMs for various downstream tasks, such as automatic short answer scoring and feedback generation at comparatively lower costs and latency.","sentences":["Automatic grading and feedback have been long studied using traditional machine learning and deep learning techniques using language models.","With the recent accessibility to high performing large language models (LLMs) like LLaMA-2, there is an opportunity to investigate the use of these LLMs for automatic grading and feedback generation.","Despite the increase in performance, LLMs require significant computational resources for fine-tuning and additional specific adjustments to enhance their performance for such tasks.","To address these issues, Parameter Efficient Fine-tuning (PEFT) methods, such as LoRA and QLoRA, have been adopted to decrease memory and computational requirements in model fine-tuning.","This paper explores the efficacy of PEFT-based quantized models, employing classification or regression head, to fine-tune LLMs for automatically assigning continuous numerical grades to short answers and essays, as well as generating corresponding feedback.","We conducted experiments on both proprietary and open-source datasets for our tasks.","The results show that prediction of grade scores via finetuned LLMs are highly accurate, achieving less than 3% error in grade percentage on average.","For providing graded feedback fine-tuned 4-bit quantized LLaMA-2 13B models outperform competitive base models and achieve high similarity with subject matter expert feedback in terms of high BLEU and ROUGE scores and qualitatively in terms of feedback.","The findings from this study provide important insights into the impacts of the emerging capabilities of using quantization approaches to fine-tune LLMs for various downstream tasks, such as automatic short answer scoring and feedback generation at comparatively lower costs and latency."],"url":"http://arxiv.org/abs/2405.00602v1","category":"cs.CL"}
{"created":"2024-05-01 16:06:48","title":"Non-abelian symmetry-resolved entanglement entropy","abstract":"We introduce a mathematical framework for symmetry-resolved entanglement entropy with a non-abelian symmetry group. To obtain a reduced density matrix that is block-diagonal in the non-abelian charges, we define subsystems operationally in terms of subalgebras of invariant observables. We derive exact formulas for the average and the variance of the typical entanglement entropy for the ensemble of random pure states with fixed non-abelian charges. We focus on compact, semisimple Lie groups. We show that, compared to the abelian case, new phenomena arise from the interplay of locality and non-abelian symmetry, such as the asymmetry of the entanglement entropy under subsystem exchange, which we show in detail by computing the Page curve of a many-body system with $SU(2)$ symmetry.","sentences":["We introduce a mathematical framework for symmetry-resolved entanglement entropy with a non-abelian symmetry group.","To obtain a reduced density matrix that is block-diagonal in the non-abelian charges, we define subsystems operationally in terms of subalgebras of invariant observables.","We derive exact formulas for the average and the variance of the typical entanglement entropy for the ensemble of random pure states with fixed non-abelian charges.","We focus on compact, semisimple Lie groups.","We show that, compared to the abelian case, new phenomena arise from the interplay of locality and non-abelian symmetry, such as the asymmetry of the entanglement entropy under subsystem exchange, which we show in detail by computing the Page curve of a many-body system with $SU(2)$ symmetry."],"url":"http://arxiv.org/abs/2405.00597v1","category":"quant-ph"}
{"created":"2024-05-01 16:04:42","title":"Unbundle-Rewrite-Rebundle: Runtime Detection and Rewriting of Privacy-Harming Code in JavaScript Bundles","abstract":"This work presents Unbundle-Rewrite-Rebundle (URR), a system for detecting privacy-harming portions of bundled JavaScript code, and rewriting that code at runtime to remove the privacy harming behavior without breaking the surrounding code or overall application. URR is a novel solution to the problem of JavaScript bundles, where websites pre-compile multiple code units into a single file, making it impossible for content filters and ad-blockers to differentiate between desired and unwanted resources. Where traditional content filtering tools rely on URLs, URR analyzes the code at the AST level, and replaces harmful AST sub-trees with privacy-and-functionality maintaining alternatives.   We present an open-sourced implementation of URR as a Firefox extension, and evaluate it against JavaScript bundles generated by the most popular bundling system (Webpack) deployed on the Tranco 10k. We measure the performance, measured by precision (1.00), recall (0.95), and speed (0.43s per-script) when detecting and rewriting three representative privacy harming libraries often included in JavaScript bundles, and find URR to be an effective approach to a large-and-growing blind spot unaddressed by current privacy tools.","sentences":["This work presents Unbundle-Rewrite-Rebundle (URR), a system for detecting privacy-harming portions of bundled JavaScript code, and rewriting that code at runtime to remove the privacy harming behavior without breaking the surrounding code or overall application.","URR is a novel solution to the problem of JavaScript bundles, where websites pre-compile multiple code units into a single file, making it impossible for content filters and ad-blockers to differentiate between desired and unwanted resources.","Where traditional content filtering tools rely on URLs, URR analyzes the code at the AST level, and replaces harmful AST sub-trees with privacy-and-functionality maintaining alternatives.   ","We present an open-sourced implementation of URR as a Firefox extension, and evaluate it against JavaScript bundles generated by the most popular bundling system (Webpack) deployed on the Tranco 10k.","We measure the performance, measured by precision (1.00), recall (0.95), and speed (0.43s per-script) when detecting and rewriting three representative privacy harming libraries often included in JavaScript bundles, and find URR to be an effective approach to a large-and-growing blind spot unaddressed by current privacy tools."],"url":"http://arxiv.org/abs/2405.00596v1","category":"cs.CR"}
{"created":"2024-05-01 16:03:20","title":"Infinitely Many Half-Volume Constant Mean Curvature Hypersurfaces via Min-Max Theory","abstract":"Let $(M^{n+1},g)$ be a closed Riemannian manifold of dimension $3\\le n+1\\le 5$. We show that, if the metric $g$ is generic, then $M$ contains infinitely many geometrically distinct constant mean curvature hypersurfaces, each enclosing half the volume of $M$. As an essential part of the proof, we develop an Almgren-Pitts type min-max theory for certain non-local functionals of the general form $$\\Omega \\mapsto \\operatorname{Area}(\\partial \\Omega) - \\int_\\Omega h + f(\\operatorname{Vol}(\\Omega)).$$","sentences":["Let $(M^{n+1},g)$ be a closed Riemannian manifold of dimension $3\\le n+1\\le 5$.","We show that, if the metric $g$ is generic, then $M$ contains infinitely many geometrically distinct constant mean curvature hypersurfaces, each enclosing half the volume of $M$. As an essential part of the proof, we develop an Almgren-Pitts type min-max theory for certain non-local functionals of the general form $$\\Omega \\mapsto \\operatorname{Area}(\\partial \\Omega) - \\int_\\Omega h + f(\\operatorname{Vol}(\\Omega)).$$"],"url":"http://arxiv.org/abs/2405.00595v1","category":"math.DG"}
{"created":"2024-05-01 16:01:37","title":"Anomalous diffusion and factor ordering in (1+1)-dimensional Lorentzian quantum gravity","abstract":"Using properties of diffusion according to a quantum heat kernel constructed as an expectation over classical heat kernels on $S^1$, we probe the non-manifold-like nature of quantized space in a model of (1+1)-dimensional quantum gravity. By computing the mean squared displacement of a diffusing particle, we find that diffusion is anomalous, behaving similarly to that on a porous substrate, network, or fractal over short distances. The walk dimension of the path for a particle diffusing in quantized space is calculated to have an infimum of 4, rising to arbitrarily large values depending on a parameter labeling the choice of factor ordering in the quantum Hamiltonian for our model and figuring in the asymptotic behavior of the wavefunction used to construct the quantum heat kernel. Additionally, we derive an expansion for return probability of a diffusing particle, whose modifications from the classical power-series form depend on the factor-ordering parameter.","sentences":["Using properties of diffusion according to a quantum heat kernel constructed as an expectation over classical heat kernels on $S^1$, we probe the non-manifold-like nature of quantized space in a model of (1+1)-dimensional quantum gravity.","By computing the mean squared displacement of a diffusing particle, we find that diffusion is anomalous, behaving similarly to that on a porous substrate, network, or fractal over short distances.","The walk dimension of the path for a particle diffusing in quantized space is calculated to have an infimum of 4, rising to arbitrarily large values depending on a parameter labeling the choice of factor ordering in the quantum Hamiltonian for our model and figuring in the asymptotic behavior of the wavefunction used to construct the quantum heat kernel.","Additionally, we derive an expansion for return probability of a diffusing particle, whose modifications from the classical power-series form depend on the factor-ordering parameter."],"url":"http://arxiv.org/abs/2405.00594v1","category":"gr-qc"}
{"created":"2024-05-01 15:59:31","title":"Silting reduction and picture categories of 0-Auslander extriangulated categories","abstract":"Let $\\mathcal{C}$ be an extriangulated category and let $\\mathcal{R}\\subseteq \\mathcal{C}$ be a rigid subcategory. Generalizing Iyama--Yang silting reduction, we devise a technical condition $\\textbf{(gCP)}$ on $\\mathcal{R}$ which is sufficient for the Verdier quotient $\\mathcal{C}/\\mathrm{thick}(\\mathcal{R})$ to be equivalent to an ideal quotient. In particular, the Verdier quotient $\\mathcal{C}/\\mathrm{thick}(\\mathcal{R})$ will admit an extriangulation in such a way that the localization functor $L_{\\mathcal{R}}\\colon \\mathcal{C} \\rightarrow \\mathcal{C}/\\mathrm{thick}(\\mathcal{R})$ is extriangulated. When $\\mathcal{C}$ is 0-Auslander, the condition $\\textbf{(gCP)}$ holds for all rigid subcategories $\\mathcal{R}$ admitting Bongartz completions. Furthermore, we prove that the Verdier quotient $\\mathcal{C}/\\mathrm{thick}(\\mathcal{R})$ then remains 0-Auslander. As an application, we define the picture category of a connective $0$-Auslander exact dg category $\\mathscr{A}$ with Bongartz completions, which generalizes the notion of $\\tau$-cluster morphism category. We show that the picture category of $\\mathscr{A}$ is a cubical category, in the sense of Igusa. The picture group of $\\mathscr{A}$ is defined as the fundamental group of its picture category. When $H_0\\mathscr{A}$ is $\\mathbf{g}$-finite, the picture group of $\\mathscr{A}$ is finitely presented.","sentences":["Let $\\mathcal{C}$ be an extriangulated category and let $\\mathcal{R}\\subseteq \\mathcal{C}$ be a rigid subcategory.","Generalizing Iyama--Yang silting reduction, we devise a technical condition $\\textbf{(gCP)}$ on $\\mathcal{R}$ which is sufficient for the Verdier quotient $\\mathcal{C}/\\mathrm{thick}(\\mathcal{R})$ to be equivalent to an ideal quotient.","In particular, the Verdier quotient $\\mathcal{C}/\\mathrm{thick}(\\mathcal{R})$ will admit an extriangulation in such a way that the localization functor $L_{\\mathcal{R}}\\colon \\mathcal{C} \\rightarrow \\mathcal{C}/\\mathrm{thick}(\\mathcal{R})$ is extriangulated.","When $\\mathcal{C}$ is 0-Auslander, the condition $\\textbf{(gCP)}$ holds for all rigid subcategories $\\mathcal{R}$ admitting Bongartz completions.","Furthermore, we prove that the Verdier quotient $\\mathcal{C}/\\mathrm{thick}(\\mathcal{R})$ then remains 0-Auslander.","As an application, we define the picture category of a connective $0$-Auslander exact dg category $\\mathscr{A}$ with Bongartz completions, which generalizes the notion of $\\tau$-cluster morphism category.","We show that the picture category of $\\mathscr{A}$ is a cubical category, in the sense of Igusa.","The picture group of $\\mathscr{A}$ is defined as the fundamental group of its picture category.","When $H_0\\mathscr{A}$ is $\\mathbf{g}$-finite, the picture group of $\\mathscr{A}$ is finitely presented."],"url":"http://arxiv.org/abs/2405.00593v1","category":"math.RT"}
{"created":"2024-05-01 15:59:00","title":"Scaling and renormalization in high-dimensional regression","abstract":"This paper presents a succinct derivation of the training and generalization performance of a variety of high-dimensional ridge regression models using the basic tools of random matrix theory and free probability. We provide an introduction and review of recent results on these topics, aimed at readers with backgrounds in physics and deep learning. Analytic formulas for the training and generalization errors are obtained in a few lines of algebra directly from the properties of the $S$-transform of free probability. This allows for a straightforward identification of the sources of power-law scaling in model performance. We compute the generalization error of a broad class of random feature models. We find that in all models, the $S$-transform corresponds to the train-test generalization gap, and yields an analogue of the generalized-cross-validation estimator. Using these techniques, we derive fine-grained bias-variance decompositions for a very general class of random feature models with structured covariates. These novel results allow us to discover a scaling regime for random feature models where the variance due to the features limits performance in the overparameterized setting. We also demonstrate how anisotropic weight structure in random feature models can limit performance and lead to nontrivial exponents for finite-width corrections in the overparameterized setting. Our results extend and provide a unifying perspective on earlier models of neural scaling laws.","sentences":["This paper presents a succinct derivation of the training and generalization performance of a variety of high-dimensional ridge regression models using the basic tools of random matrix theory and free probability.","We provide an introduction and review of recent results on these topics, aimed at readers with backgrounds in physics and deep learning.","Analytic formulas for the training and generalization errors are obtained in a few lines of algebra directly from the properties of the $S$-transform of free probability.","This allows for a straightforward identification of the sources of power-law scaling in model performance.","We compute the generalization error of a broad class of random feature models.","We find that in all models, the $S$-transform corresponds to the train-test generalization gap, and yields an analogue of the generalized-cross-validation estimator.","Using these techniques, we derive fine-grained bias-variance decompositions for a very general class of random feature models with structured covariates.","These novel results allow us to discover a scaling regime for random feature models where the variance due to the features limits performance in the overparameterized setting.","We also demonstrate how anisotropic weight structure in random feature models can limit performance and lead to nontrivial exponents for finite-width corrections in the overparameterized setting.","Our results extend and provide a unifying perspective on earlier models of neural scaling laws."],"url":"http://arxiv.org/abs/2405.00592v1","category":"stat.ML"}
{"created":"2024-05-01 15:55:25","title":"Nonlinear Poisson effect in affine semiflexible polymer networks","abstract":"Stretching an elastic material along one axis typically induces contraction along the transverse axes, a phenomenon known as the Poisson effect. From these strains, one can compute the specific volume, which generally either increases or, in the incompressible limit, remains constant as the material is stretched. However, in networks of semiflexible or stiff polymers, which are typically highly compressible yet stiffen significantly when stretched, one instead sees a significant reduction in specific volume under finite strains. This volume reduction is accompanied by increasing alignment of filaments along the strain axis and a nonlinear elastic response, with stiffening of the apparent Young's modulus. For semiflexible networks, in which entropic bending elasticity governs the linear elastic regime, the nonlinear Poisson effect is caused by the nonlinear force-extension relationship of the constituent filaments, which produces a highly asymmetric response of the constituent polymers to stretching and compression. The details of this relationship depend on the geometric and elastic properties of the underlying filaments, which can vary greatly in experimental systems. Here, we provide a comprehensive characterization of the nonlinear Poisson effect in an affine network model and explore the influence of filament properties on essential features of the macroscopic response, including strain-driven alignment and volume reduction.","sentences":["Stretching an elastic material along one axis typically induces contraction along the transverse axes, a phenomenon known as the Poisson effect.","From these strains, one can compute the specific volume, which generally either increases or, in the incompressible limit, remains constant as the material is stretched.","However, in networks of semiflexible or stiff polymers, which are typically highly compressible yet stiffen significantly when stretched, one instead sees a significant reduction in specific volume under finite strains.","This volume reduction is accompanied by increasing alignment of filaments along the strain axis and a nonlinear elastic response, with stiffening of the apparent Young's modulus.","For semiflexible networks, in which entropic bending elasticity governs the linear elastic regime, the nonlinear Poisson effect is caused by the nonlinear force-extension relationship of the constituent filaments, which produces a highly asymmetric response of the constituent polymers to stretching and compression.","The details of this relationship depend on the geometric and elastic properties of the underlying filaments, which can vary greatly in experimental systems.","Here, we provide a comprehensive characterization of the nonlinear Poisson effect in an affine network model and explore the influence of filament properties on essential features of the macroscopic response, including strain-driven alignment and volume reduction."],"url":"http://arxiv.org/abs/2405.00590v1","category":"cond-mat.soft"}
{"created":"2024-05-01 15:51:15","title":"Are Models Biased on Text without Gender-related Language?","abstract":"Gender bias research has been pivotal in revealing undesirable behaviors in large language models, exposing serious gender stereotypes associated with occupations, and emotions. A key observation in prior work is that models reinforce stereotypes as a consequence of the gendered correlations that are present in the training data. In this paper, we focus on bias where the effect from training data is unclear, and instead address the question: Do language models still exhibit gender bias in non-stereotypical settings? To do so, we introduce UnStereoEval (USE), a novel framework tailored for investigating gender bias in stereotype-free scenarios. USE defines a sentence-level score based on pretraining data statistics to determine if the sentence contain minimal word-gender associations. To systematically benchmark the fairness of popular language models in stereotype-free scenarios, we utilize USE to automatically generate benchmarks without any gender-related language. By leveraging USE's sentence-level score, we also repurpose prior gender bias benchmarks (Winobias and Winogender) for non-stereotypical evaluation. Surprisingly, we find low fairness across all 28 tested models. Concretely, models demonstrate fair behavior in only 9%-41% of stereotype-free sentences, suggesting that bias does not solely stem from the presence of gender-related words. These results raise important questions about where underlying model biases come from and highlight the need for more systematic and comprehensive bias evaluation. We release the full dataset and code at https://ucinlp.github.io/unstereo-eval.","sentences":["Gender bias research has been pivotal in revealing undesirable behaviors in large language models, exposing serious gender stereotypes associated with occupations, and emotions.","A key observation in prior work is that models reinforce stereotypes as a consequence of the gendered correlations that are present in the training data.","In this paper, we focus on bias where the effect from training data is unclear, and instead address the question: Do language models still exhibit gender bias in non-stereotypical settings?","To do so, we introduce UnStereoEval (USE), a novel framework tailored for investigating gender bias in stereotype-free scenarios.","USE defines a sentence-level score based on pretraining data statistics to determine if the sentence contain minimal word-gender associations.","To systematically benchmark the fairness of popular language models in stereotype-free scenarios, we utilize USE to automatically generate benchmarks without any gender-related language.","By leveraging USE's sentence-level score, we also repurpose prior gender bias benchmarks (Winobias and Winogender) for non-stereotypical evaluation.","Surprisingly, we find low fairness across all 28 tested models.","Concretely, models demonstrate fair behavior in only 9%-41% of stereotype-free sentences, suggesting that bias does not solely stem from the presence of gender-related words.","These results raise important questions about where underlying model biases come from and highlight the need for more systematic and comprehensive bias evaluation.","We release the full dataset and code at https://ucinlp.github.io/unstereo-eval."],"url":"http://arxiv.org/abs/2405.00588v1","category":"cs.CL"}
{"created":"2024-05-01 15:50:16","title":"GraCo: Granularity-Controllable Interactive Segmentation","abstract":"Interactive Segmentation (IS) segments specific objects or parts in the image according to user input. Current IS pipelines fall into two categories: single-granularity output and multi-granularity output. The latter aims to alleviate the spatial ambiguity present in the former. However, the multi-granularity output pipeline suffers from limited interaction flexibility and produces redundant results. In this work, we introduce Granularity-Controllable Interactive Segmentation (GraCo), a novel approach that allows precise control of prediction granularity by introducing additional parameters to input. This enhances the customization of the interactive system and eliminates redundancy while resolving ambiguity. Nevertheless, the exorbitant cost of annotating multi-granularity masks and the lack of available datasets with granularity annotations make it difficult for models to acquire the necessary guidance to control output granularity. To address this problem, we design an any-granularity mask generator that exploits the semantic property of the pre-trained IS model to automatically generate abundant mask-granularity pairs without requiring additional manual annotation. Based on these pairs, we propose a granularity-controllable learning strategy that efficiently imparts the granularity controllability to the IS model. Extensive experiments on intricate scenarios at object and part levels demonstrate that our GraCo has significant advantages over previous methods. This highlights the potential of GraCo to be a flexible annotation tool, capable of adapting to diverse segmentation scenarios. The project page: https://zhao-yian.github.io/GraCo.","sentences":["Interactive Segmentation (IS) segments specific objects or parts in the image according to user input.","Current IS pipelines fall into two categories: single-granularity output and multi-granularity output.","The latter aims to alleviate the spatial ambiguity present in the former.","However, the multi-granularity output pipeline suffers from limited interaction flexibility and produces redundant results.","In this work, we introduce Granularity-Controllable Interactive Segmentation (GraCo), a novel approach that allows precise control of prediction granularity by introducing additional parameters to input.","This enhances the customization of the interactive system and eliminates redundancy while resolving ambiguity.","Nevertheless, the exorbitant cost of annotating multi-granularity masks and the lack of available datasets with granularity annotations make it difficult for models to acquire the necessary guidance to control output granularity.","To address this problem, we design an any-granularity mask generator that exploits the semantic property of the pre-trained IS model to automatically generate abundant mask-granularity pairs without requiring additional manual annotation.","Based on these pairs, we propose a granularity-controllable learning strategy that efficiently imparts the granularity controllability to the IS model.","Extensive experiments on intricate scenarios at object and part levels demonstrate that our GraCo has significant advantages over previous methods.","This highlights the potential of GraCo to be a flexible annotation tool, capable of adapting to diverse segmentation scenarios.","The project page: https://zhao-yian.github.io/GraCo."],"url":"http://arxiv.org/abs/2405.00587v1","category":"cs.CV"}
{"created":"2024-05-01 15:35:53","title":"Implementing Bayesian inference on a stochastic CO2-based grey-box model for assessing indoor air quality in Canadian primary schools","abstract":"The COVID-19 pandemic brought global attention to indoor air quality (IAQ), which is intrinsically linked to clean air change rates. Estimating the air change rate in indoor environments, however, remains challenging. It is primarily due to the uncertainties associated with the air change rate estimation, such as pollutant generation rates, dynamics including weather and occupancies, and the limitations of deterministic approaches to accommodate these factors. In this study, Bayesian inference was implemented on a stochastic CO2-based grey-box model to infer modeled parameters and quantify uncertainties. The accuracy and robustness of the ventilation rate and CO2 emission rate estimated by the model were confirmed with CO2 tracer gas experiments conducted in an airtight chamber. Both prior and posterior predictive checks (PPC) were performed to demonstrate the advantage of this approach. In addition, uncertainties in real-life contexts were quantified with an incremental variance {\\sigma} for the Wiener process. This approach was later applied to evaluate the ventilation conditions within two primary school classrooms in Montreal. The Equivalent Clean Airflow Rate (ECAi) was calculated following ASHRAE 241, and an insufficient clean air supply within both classrooms was identified. A supplement of 800 cfm clear air delivery rate (CADR) from air-cleaning devices is recommended for a sufficient ECAi. Finally, steady-state CO2 thresholds (Climit, Ctarget, and Cideal) were carried out to indicate when ECAi requirements could be achieved under various mitigation strategies, such as portable air cleaners and in-room ultraviolet light, with CADR values ranging from 200 to 1000 cfm.","sentences":["The COVID-19 pandemic brought global attention to indoor air quality (IAQ), which is intrinsically linked to clean air change rates.","Estimating the air change rate in indoor environments, however, remains challenging.","It is primarily due to the uncertainties associated with the air change rate estimation, such as pollutant generation rates, dynamics including weather and occupancies, and the limitations of deterministic approaches to accommodate these factors.","In this study, Bayesian inference was implemented on a stochastic CO2-based grey-box model to infer modeled parameters and quantify uncertainties.","The accuracy and robustness of the ventilation rate and CO2 emission rate estimated by the model were confirmed with CO2 tracer gas experiments conducted in an airtight chamber.","Both prior and posterior predictive checks (PPC) were performed to demonstrate the advantage of this approach.","In addition, uncertainties in real-life contexts were quantified with an incremental variance {\\sigma} for the Wiener process.","This approach was later applied to evaluate the ventilation conditions within two primary school classrooms in Montreal.","The Equivalent Clean Airflow Rate (ECAi) was calculated following ASHRAE 241, and an insufficient clean air supply within both classrooms was identified.","A supplement of 800 cfm clear air delivery rate (CADR) from air-cleaning devices is recommended for a sufficient ECAi.","Finally, steady-state CO2 thresholds (Climit, Ctarget, and Cideal) were carried out to indicate when ECAi requirements could be achieved under various mitigation strategies, such as portable air cleaners and in-room ultraviolet light, with CADR values ranging from 200 to 1000 cfm."],"url":"http://arxiv.org/abs/2405.00582v1","category":"stat.AP"}
{"created":"2024-05-01 15:32:58","title":"Semiorthogonal decompositions for generalized Severi-Brauer schemes","abstract":"The purpose of this paper is to use conservative descent to study semi-orthogonal decompositions for some homogeneous varieties over general bases. We produce a semi-orthogonal decomposition for the bounded derived category of coherent sheaves on a generalized Severi-Brauer scheme. This extends known results for Sever-Brauer varieties and Grassmanianns. We use our results to construct semi-orthogonal decompositions for flag varieties over arbitrary bases. This generalises a result of Kapranov.","sentences":["The purpose of this paper is to use conservative descent to study semi-orthogonal decompositions for some homogeneous varieties over general bases.","We produce a semi-orthogonal decomposition for the bounded derived category of coherent sheaves on a generalized Severi-Brauer scheme.","This extends known results for Sever-Brauer varieties and Grassmanianns.","We use our results to construct semi-orthogonal decompositions for flag varieties over arbitrary bases.","This generalises a result of Kapranov."],"url":"http://arxiv.org/abs/2405.00580v1","category":"math.AG"}
{"created":"2024-05-01 15:30:41","title":"The Real, the Better: Aligning Large Language Models with Online Human Behaviors","abstract":"Large language model alignment is widely used and studied to avoid LLM producing unhelpful and harmful responses. However, the lengthy training process and predefined preference bias hinder adaptation to online diverse human preferences. To this end, this paper proposes an alignment framework, called Reinforcement Learning with Human Behavior (RLHB), to align LLMs by directly leveraging real online human behaviors. By taking the generative adversarial framework, the generator is trained to respond following expected human behavior; while the discriminator tries to verify whether the triplets of query, response, and human behavior come from real online environments. Behavior modeling in natural-language form and the multi-model joint training mechanism enable an active and sustainable online alignment. Experimental results confirm the effectiveness of our proposed methods by both human and automatic evaluations.","sentences":["Large language model alignment is widely used and studied to avoid LLM producing unhelpful and harmful responses.","However, the lengthy training process and predefined preference bias hinder adaptation to online diverse human preferences.","To this end, this paper proposes an alignment framework, called Reinforcement Learning with Human Behavior (RLHB), to align LLMs by directly leveraging real online human behaviors.","By taking the generative adversarial framework, the generator is trained to respond following expected human behavior; while the discriminator tries to verify whether the triplets of query, response, and human behavior come from real online environments.","Behavior modeling in natural-language form and the multi-model joint training mechanism enable an active and sustainable online alignment.","Experimental results confirm the effectiveness of our proposed methods by both human and automatic evaluations."],"url":"http://arxiv.org/abs/2405.00578v1","category":"cs.CL"}
{"created":"2024-05-01 15:28:17","title":"Complex analytic solutions for the TQG model","abstract":"We present a condition under which the thermal quasi-geostrophic (TQG) model possesses a solution that is holomorphic in time with values in the Gevrey space of complex analytic functions. This can be seen as the complex extension of the work by Levermore and Oliver (1997) for the generalized Euler equation but applied to the TQG model.","sentences":["We present a condition under which the thermal quasi-geostrophic (TQG) model possesses a solution that is holomorphic in time with values in the Gevrey space of complex analytic functions.","This can be seen as the complex extension of the work by Levermore and Oliver (1997) for the generalized Euler equation but applied to the TQG model."],"url":"http://arxiv.org/abs/2405.00575v1","category":"math.AP"}
{"created":"2024-05-01 15:22:36","title":"JWST/NIRCam Detection of the Fomalhaut C Debris Disk in Scattered Light","abstract":"Observations of debris disks offer important insights into the formation and evolution of planetary systems. Though M dwarfs make up approximately 80% of nearby stars, very few M-dwarf debris disks have been studied in detail -- making it unclear how or if the information gleaned from studying debris disks around more massive stars extends to the more abundant M dwarf systems. We report the first scattered-light detection of the debris disk around the M4 star Fomalhaut C using JWST's Near Infrared Camera (NIRCam; 3.6$~\\mu$m and 4.4$~\\mu$m). This result adds to the prior sample of only four M-dwarf debris disks with detections in scattered light, and marks the latest spectral type and oldest star among them. The size and orientation of the disk in these data are generally consistent with the prior ALMA sub-mm detection. Though no companions are identified, these data provide strong constraints on their presence -- with sensitivity sufficient to recover sub-Saturn mass objects in the vicinity of the disk. This result illustrates the unique capability of JWST for uncovering elusive M-dwarf debris disks in scattered light, and lays the groundwork for deeper studies of such objects in the 2--5$~\\mu$m regime.","sentences":["Observations of debris disks offer important insights into the formation and evolution of planetary systems.","Though M dwarfs make up approximately 80% of nearby stars, very few M-dwarf debris disks have been studied in detail -- making it unclear how or if the information gleaned from studying debris disks around more massive stars extends to the more abundant M dwarf systems.","We report the first scattered-light detection of the debris disk around the M4 star Fomalhaut C using JWST's Near Infrared Camera (NIRCam; 3.6$~\\mu$m and 4.4$~\\mu$m).","This result adds to the prior sample of only four M-dwarf debris disks with detections in scattered light, and marks the latest spectral type and oldest star among them.","The size and orientation of the disk in these data are generally consistent with the prior ALMA sub-mm detection.","Though no companions are identified, these data provide strong constraints on their presence -- with sensitivity sufficient to recover sub-Saturn mass objects in the vicinity of the disk.","This result illustrates the unique capability of JWST for uncovering elusive M-dwarf debris disks in scattered light, and lays the groundwork for deeper studies of such objects in the 2--5$~\\mu$m regime."],"url":"http://arxiv.org/abs/2405.00573v1","category":"astro-ph.EP"}
{"created":"2024-05-01 15:19:54","title":"Spherical Linear Interpolation and Text-Anchoring for Zero-shot Composed Image Retrieval","abstract":"Composed Image Retrieval (CIR) is a complex task that retrieves images using a query, which is configured with an image and a caption that describes desired modifications to that image. Supervised CIR approaches have shown strong performance, but their reliance on expensive manually-annotated datasets restricts their scalability and broader applicability. To address these issues, previous studies have proposed pseudo-word token-based Zero-Shot CIR (ZS-CIR) methods, which utilize a projection module to map images to word tokens. However, we conjecture that this approach has a downside: the projection module distorts the original image representation and confines the resulting composed embeddings to the text-side. In order to resolve this, we introduce a novel ZS-CIR method that uses Spherical Linear Interpolation (Slerp) to directly merge image and text representations by identifying an intermediate embedding of both. Furthermore, we introduce Text-Anchored-Tuning (TAT), a method that fine-tunes the image encoder while keeping the text encoder fixed. TAT closes the modality gap between images and text, making the Slerp process much more effective. Notably, the TAT method is not only efficient in terms of the scale of the training dataset and training time, but it also serves as an excellent initial checkpoint for training supervised CIR models, thereby highlighting its wider potential. The integration of the Slerp-based ZS-CIR with a TAT-tuned model enables our approach to deliver state-of-the-art retrieval performance across CIR benchmarks.","sentences":["Composed Image Retrieval (CIR) is a complex task that retrieves images using a query, which is configured with an image and a caption that describes desired modifications to that image.","Supervised CIR approaches have shown strong performance, but their reliance on expensive manually-annotated datasets restricts their scalability and broader applicability.","To address these issues, previous studies have proposed pseudo-word token-based Zero-Shot CIR (ZS-CIR) methods, which utilize a projection module to map images to word tokens.","However, we conjecture that this approach has a downside: the projection module distorts the original image representation and confines the resulting composed embeddings to the text-side.","In order to resolve this, we introduce a novel ZS-CIR method that uses Spherical Linear Interpolation (Slerp) to directly merge image and text representations by identifying an intermediate embedding of both.","Furthermore, we introduce Text-Anchored-Tuning (TAT), a method that fine-tunes the image encoder while keeping the text encoder fixed.","TAT closes the modality gap between images and text, making the Slerp process much more effective.","Notably, the TAT method is not only efficient in terms of the scale of the training dataset and training time, but it also serves as an excellent initial checkpoint for training supervised CIR models, thereby highlighting its wider potential.","The integration of the Slerp-based ZS-CIR with a TAT-tuned model enables our approach to deliver state-of-the-art retrieval performance across CIR benchmarks."],"url":"http://arxiv.org/abs/2405.00571v1","category":"cs.CV"}
{"created":"2024-05-01 15:19:19","title":"WEST GCN-LSTM: Weighted Stacked Spatio-Temporal Graph Neural Networks for Regional Traffic Forecasting","abstract":"Regional traffic forecasting is a critical challenge in urban mobility, with applications to various fields such as the Internet of Everything. In recent years, spatio-temporal graph neural networks have achieved state-of-the-art results in the context of numerous traffic forecasting challenges. This work aims at expanding upon the conventional spatio-temporal graph neural network architectures in a manner that may facilitate the inclusion of information regarding the examined regions, as well as the populations that traverse them, in order to establish a more efficient prediction model. The end-product of this scientific endeavour is a novel spatio-temporal graph neural network architecture that is referred to as WEST (WEighted STacked) GCN-LSTM. Furthermore, the inclusion of the aforementioned information is conducted via the use of two novel dedicated algorithms that are referred to as the Shared Borders Policy and the Adjustable Hops Policy. Through information fusion and distillation, the proposed solution manages to significantly outperform its competitors in the frame of an experimental evaluation that consists of 19 forecasting models, across several datasets. Finally, an additional ablation study determined that each of the components of the proposed solution contributes towards enhancing its overall performance.","sentences":["Regional traffic forecasting is a critical challenge in urban mobility, with applications to various fields such as the Internet of Everything.","In recent years, spatio-temporal graph neural networks have achieved state-of-the-art results in the context of numerous traffic forecasting challenges.","This work aims at expanding upon the conventional spatio-temporal graph neural network architectures in a manner that may facilitate the inclusion of information regarding the examined regions, as well as the populations that traverse them, in order to establish a more efficient prediction model.","The end-product of this scientific endeavour is a novel spatio-temporal graph neural network architecture that is referred to as WEST (WEighted STacked) GCN-LSTM.","Furthermore, the inclusion of the aforementioned information is conducted via the use of two novel dedicated algorithms that are referred to as the Shared Borders Policy and the Adjustable Hops Policy.","Through information fusion and distillation, the proposed solution manages to significantly outperform its competitors in the frame of an experimental evaluation that consists of 19 forecasting models, across several datasets.","Finally, an additional ablation study determined that each of the components of the proposed solution contributes towards enhancing its overall performance."],"url":"http://arxiv.org/abs/2405.00570v1","category":"cs.LG"}
{"created":"2024-05-01 15:18:12","title":"Powering In-Database Dynamic Model Slicing for Structured Data Analytics","abstract":"Relational database management systems (RDBMS) are widely used for the storage and retrieval of structured data. To derive insights beyond statistical aggregation, we typically have to extract specific subdatasets from the database using conventional database operations, and then apply deep neural networks (DNN) training and inference on these respective subdatasets in a separate machine learning system. The process can be prohibitively expensive, especially when there are a combinatorial number of subdatasets extracted for different analytical purposes. This calls for efficient in-database support of advanced analytical methods In this paper, we introduce LEADS, a novel SQL-aware dynamic model slicing technique to customize models for subdatasets specified by SQL queries. LEADS improves the predictive modeling of structured data via the mixture of experts (MoE) technique and maintains inference efficiency by a SQL-aware gating network. At the core of LEADS is the construction of a general model with multiple expert sub-models via MoE trained over the entire database. This SQL-aware MoE technique scales up the modeling capacity, enhances effectiveness, and preserves efficiency by activating only necessary experts via the gating network during inference. Additionally, we introduce two regularization terms during the training process of LEADS to strike a balance between effectiveness and efficiency. We also design and build an in-database inference system, called INDICES, to support end-to-end advanced structured data analytics by non-intrusively incorporating LEADS onto PostgreSQL. Our extensive experiments on real-world datasets demonstrate that LEADS consistently outperforms baseline models, and INDICES delivers effective in-database analytics with a considerable reduction in inference latency compared to traditional solutions.","sentences":["Relational database management systems (RDBMS) are widely used for the storage and retrieval of structured data.","To derive insights beyond statistical aggregation, we typically have to extract specific subdatasets from the database using conventional database operations, and then apply deep neural networks (DNN) training and inference on these respective subdatasets in a separate machine learning system.","The process can be prohibitively expensive, especially when there are a combinatorial number of subdatasets extracted for different analytical purposes.","This calls for efficient in-database support of advanced analytical methods In this paper, we introduce LEADS, a novel SQL-aware dynamic model slicing technique to customize models for subdatasets specified by SQL queries.","LEADS improves the predictive modeling of structured data via the mixture of experts (MoE) technique and maintains inference efficiency by a SQL-aware gating network.","At the core of LEADS is the construction of a general model with multiple expert sub-models via MoE trained over the entire database.","This SQL-aware MoE technique scales up the modeling capacity, enhances effectiveness, and preserves efficiency by activating only necessary experts via the gating network during inference.","Additionally, we introduce two regularization terms during the training process of LEADS to strike a balance between effectiveness and efficiency.","We also design and build an in-database inference system, called INDICES, to support end-to-end advanced structured data analytics by non-intrusively incorporating LEADS onto PostgreSQL.","Our extensive experiments on real-world datasets demonstrate that LEADS consistently outperforms baseline models, and INDICES delivers effective in-database analytics with a considerable reduction in inference latency compared to traditional solutions."],"url":"http://arxiv.org/abs/2405.00568v1","category":"cs.DB"}
{"created":"2024-05-01 15:17:27","title":"NumLLM: Numeric-Sensitive Large Language Model for Chinese Finance","abstract":"Recently, many works have proposed various financial large language models (FinLLMs) by pre-training from scratch or fine-tuning open-sourced LLMs on financial corpora. However, existing FinLLMs exhibit unsatisfactory performance in understanding financial text when numeric variables are involved in questions. In this paper, we propose a novel LLM, called numeric-sensitive large language model (NumLLM), for Chinese finance. We first construct a financial corpus from financial textbooks which is essential for improving numeric capability of LLMs during fine-tuning. After that, we train two individual low-rank adaptation (LoRA) modules by fine-tuning on our constructed financial corpus. One module is for adapting general-purpose LLMs to financial domain, and the other module is for enhancing the ability of NumLLM to understand financial text with numeric variables. Lastly, we merge the two LoRA modules into the foundation model to obtain NumLLM for inference. Experiments on financial question-answering benchmark show that NumLLM can boost the performance of the foundation model and can achieve the best overall performance compared to all baselines, on both numeric and non-numeric questions.","sentences":["Recently, many works have proposed various financial large language models (FinLLMs) by pre-training from scratch or fine-tuning open-sourced LLMs on financial corpora.","However, existing FinLLMs exhibit unsatisfactory performance in understanding financial text when numeric variables are involved in questions.","In this paper, we propose a novel LLM, called numeric-sensitive large language model (NumLLM), for Chinese finance.","We first construct a financial corpus from financial textbooks which is essential for improving numeric capability of LLMs during fine-tuning.","After that, we train two individual low-rank adaptation (LoRA) modules by fine-tuning on our constructed financial corpus.","One module is for adapting general-purpose LLMs to financial domain, and the other module is for enhancing the ability of NumLLM to understand financial text with numeric variables.","Lastly, we merge the two LoRA modules into the foundation model to obtain NumLLM for inference.","Experiments on financial question-answering benchmark show that NumLLM can boost the performance of the foundation model and can achieve the best overall performance compared to all baselines, on both numeric and non-numeric questions."],"url":"http://arxiv.org/abs/2405.00566v1","category":"cs.CE"}
{"created":"2024-05-01 15:14:52","title":"Resolution analysis of magnetically arrested disk simulations","abstract":"Polarisation measurements by the Event Horizon Telescope from M87$^{\\ast}$ and Sgr A$^\\ast$ suggest that there is a dynamically strong, ordered magnetic field, typical of what is expected of a magnetically arrested accretion disk (MAD). In such disks the strong poloidal magnetic field can suppress the accretion flow and cause episodic flux eruptions. Recent work shows that General Relativistic Magnetohydrodynamic (GRMHD) MAD simulations feature dynamics of turbulence and mixing instabilities that are becoming resolved at higher resolutions. We perform a convergence study of MADs exceeding the status quo by an order of magnitude in resolution. We use existing 3D simulations performed with the H-AMR code, up to resolution of 5376 x 2304 x 2304 in a logarithmic spherical-polar grid. We find consistent time-averaged disk properties across all resolutions. However, higher resolutions reveal signs of inward angular momentum transport attributed to turbulent convection, particularly evident when mixing instabilities occur at the surfaces of flux tubes during flux eruptions. Additionally, we see wave-like features in the jet sheath, which become more prominent at higher resolutions, that may induce mixing between jet and disk. At higher resolutions, we observe the sheath to be thinner, resulting in increased temperature, reduced magnetisation, and greater variability. Those differences could affect the dissipation of energy, that would eventually result in distinct observable radiative emission from high-resolution simulations. With higher resolutions, we can delve into crucial questions about horizon-scale physics and its impact on the dynamics and emission properties of larger-scale jets.","sentences":["Polarisation measurements by the Event Horizon Telescope from M87$^{\\ast}$ and Sgr A$^\\ast$ suggest that there is a dynamically strong, ordered magnetic field, typical of what is expected of a magnetically arrested accretion disk (MAD).","In such disks the strong poloidal magnetic field can suppress the accretion flow and cause episodic flux eruptions.","Recent work shows that General Relativistic Magnetohydrodynamic (GRMHD) MAD simulations feature dynamics of turbulence and mixing instabilities that are becoming resolved at higher resolutions.","We perform a convergence study of MADs exceeding the status quo by an order of magnitude in resolution.","We use existing 3D simulations performed with the H-AMR code, up to resolution of 5376 x 2304 x 2304 in a logarithmic spherical-polar grid.","We find consistent time-averaged disk properties across all resolutions.","However, higher resolutions reveal signs of inward angular momentum transport attributed to turbulent convection, particularly evident when mixing instabilities occur at the surfaces of flux tubes during flux eruptions.","Additionally, we see wave-like features in the jet sheath, which become more prominent at higher resolutions, that may induce mixing between jet and disk.","At higher resolutions, we observe the sheath to be thinner, resulting in increased temperature, reduced magnetisation, and greater variability.","Those differences could affect the dissipation of energy, that would eventually result in distinct observable radiative emission from high-resolution simulations.","With higher resolutions, we can delve into crucial questions about horizon-scale physics and its impact on the dynamics and emission properties of larger-scale jets."],"url":"http://arxiv.org/abs/2405.00564v1","category":"astro-ph.HE"}
{"created":"2024-05-01 15:07:32","title":"Informationally overcomplete measurements from generalized equiangular tight frames","abstract":"Informationally overcomplete measurements find important applications in quantum tomography and quantum state estimation. The most popular are maximal sets of mutually unbiased bases, for which trace relations between measurement operators are well known. In this paper, we introduce a more general class of informationally overcomplete POVMs that are generated by equiangular tight frames of arbitrary rank. This class provides a generalization of equiangular measurements to non-projective POVMs, which include rescaled mutually unbiased measurements and bases. We provide a method of their construction, analyze their symmetry properties, and provide examples for highly symmetric cases. In particular, we find a wide class of generalized equiangular measurements that are conical 2-designs, which allows us to derive the index of coincidence. Our results show benefits of considering a single informationally overcomplete measurement over informationally complete collections of POVMs.","sentences":["Informationally overcomplete measurements find important applications in quantum tomography and quantum state estimation.","The most popular are maximal sets of mutually unbiased bases, for which trace relations between measurement operators are well known.","In this paper, we introduce a more general class of informationally overcomplete POVMs that are generated by equiangular tight frames of arbitrary rank.","This class provides a generalization of equiangular measurements to non-projective POVMs, which include rescaled mutually unbiased measurements and bases.","We provide a method of their construction, analyze their symmetry properties, and provide examples for highly symmetric cases.","In particular, we find a wide class of generalized equiangular measurements that are conical 2-designs, which allows us to derive the index of coincidence.","Our results show benefits of considering a single informationally overcomplete measurement over informationally complete collections of POVMs."],"url":"http://arxiv.org/abs/2405.00560v1","category":"quant-ph"}
{"created":"2024-05-01 15:06:05","title":"Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment","abstract":"As the capabilities of large language models (LLMs) have expanded dramatically, aligning these models with human values presents a significant challenge, posing potential risks during deployment. Traditional alignment strategies rely heavily on human intervention, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), or on the self-alignment capacities of LLMs, which usually require a strong LLM's emergent ability to improve its original bad answer. To address these challenges, we propose a novel self-alignment method that utilizes a Chain of Thought (CoT) approach, termed AlignCoT. This method encompasses stages of Question Analysis, Answer Guidance, and Safe Answer production. It is designed to enable LLMs to generate high-quality, safe responses throughout various stages of their development. Furthermore, we introduce the Mixture of insighTful Experts (MoTE) architecture, which applies the mixture of experts to enhance each component of the AlignCoT process, markedly increasing alignment efficiency. The MoTE approach not only outperforms existing methods in aligning LLMs with human values but also highlights the benefits of using self-generated data, revealing the dual benefits of improved alignment and training efficiency.","sentences":["As the capabilities of large language models (LLMs) have expanded dramatically, aligning these models with human values presents a significant challenge, posing potential risks during deployment.","Traditional alignment strategies rely heavily on human intervention, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), or on the self-alignment capacities of LLMs, which usually require a strong LLM's emergent ability to improve its original bad answer.","To address these challenges, we propose a novel self-alignment method that utilizes a Chain of Thought (CoT) approach, termed AlignCoT.","This method encompasses stages of Question Analysis, Answer Guidance, and Safe Answer production.","It is designed to enable LLMs to generate high-quality, safe responses throughout various stages of their development.","Furthermore, we introduce the Mixture of insighTful Experts (MoTE) architecture, which applies the mixture of experts to enhance each component of the AlignCoT process, markedly increasing alignment efficiency.","The MoTE approach not only outperforms existing methods in aligning LLMs with human values but also highlights the benefits of using self-generated data, revealing the dual benefits of improved alignment and training efficiency."],"url":"http://arxiv.org/abs/2405.00557v1","category":"cs.CL"}
{"created":"2024-05-01 14:59:24","title":"Swarm Learning: A Survey of Concepts, Applications, and Trends","abstract":"Deep learning models have raised privacy and security concerns due to their reliance on large datasets on central servers. As the number of Internet of Things (IoT) devices increases, artificial intelligence (AI) will be crucial for resource management, data processing, and knowledge acquisition. To address those issues, federated learning (FL) has introduced a novel approach to building a versatile, large-scale machine learning framework that operates in a decentralized and hardware-agnostic manner. However, FL faces network bandwidth limitations and data breaches. To reduce the central dependency in FL and increase scalability, swarm learning (SL) has been proposed in collaboration with Hewlett Packard Enterprise (HPE). SL represents a decentralized machine learning framework that leverages blockchain technology for secure, scalable, and private data management. A blockchain-based network enables the exchange and aggregation of model parameters among participants, thus mitigating the risk of a single point of failure and eliminating communication bottlenecks. To the best of our knowledge, this survey is the first to introduce the principles of Swarm Learning, its architectural design, and its fields of application. In addition, it highlights numerous research avenues that require further exploration by academic and industry communities to unlock the full potential and applications of SL.","sentences":["Deep learning models have raised privacy and security concerns due to their reliance on large datasets on central servers.","As the number of Internet of Things (IoT) devices increases, artificial intelligence (AI) will be crucial for resource management, data processing, and knowledge acquisition.","To address those issues, federated learning (FL) has introduced a novel approach to building a versatile, large-scale machine learning framework that operates in a decentralized and hardware-agnostic manner.","However, FL faces network bandwidth limitations and data breaches.","To reduce the central dependency in FL and increase scalability, swarm learning (SL) has been proposed in collaboration with Hewlett Packard Enterprise (HPE).","SL represents a decentralized machine learning framework that leverages blockchain technology for secure, scalable, and private data management.","A blockchain-based network enables the exchange and aggregation of model parameters among participants, thus mitigating the risk of a single point of failure and eliminating communication bottlenecks.","To the best of our knowledge, this survey is the first to introduce the principles of Swarm Learning, its architectural design, and its fields of application.","In addition, it highlights numerous research avenues that require further exploration by academic and industry communities to unlock the full potential and applications of SL."],"url":"http://arxiv.org/abs/2405.00556v1","category":"cs.LG"}
{"created":"2024-05-01 14:57:59","title":"Derivative-based regularization for regression","abstract":"In this work, we introduce a novel approach to regularization in multivariable regression problems. Our regularizer, called DLoss, penalises differences between the model's derivatives and derivatives of the data generating function as estimated from the training data. We call these estimated derivatives data derivatives. The goal of our method is to align the model to the data, not only in terms of target values but also in terms of the derivatives involved. To estimate data derivatives, we select (from the training data) 2-tuples of input-value pairs, using either nearest neighbour or random, selection. On synthetic and real datasets, we evaluate the effectiveness of adding DLoss, with different weights, to the standard mean squared error loss. The experimental results show that with DLoss (using nearest neighbour selection) we obtain, on average, the best rank with respect to MSE on validation data sets, compared to no regularization, L2 regularization, and Dropout.","sentences":["In this work, we introduce a novel approach to regularization in multivariable regression problems.","Our regularizer, called DLoss, penalises differences between the model's derivatives and derivatives of the data generating function as estimated from the training data.","We call these estimated derivatives data derivatives.","The goal of our method is to align the model to the data, not only in terms of target values but also in terms of the derivatives involved.","To estimate data derivatives, we select (from the training data) 2-tuples of input-value pairs, using either nearest neighbour or random, selection.","On synthetic and real datasets, we evaluate the effectiveness of adding DLoss, with different weights, to the standard mean squared error loss.","The experimental results show that with DLoss (using nearest neighbour selection) we obtain, on average, the best rank with respect to MSE on validation data sets, compared to no regularization, L2 regularization, and Dropout."],"url":"http://arxiv.org/abs/2405.00555v1","category":"cs.LG"}
{"created":"2024-05-01 14:54:05","title":"Applying the starquake model to study the formation of elastic mountains on spinning neutron stars","abstract":"When a neutron star is spun-up or spun-down, the changing strains in its solid elastic crust can give rise to sudden fractures known as starquakes. Early interest in starquakes focused on their possible connection to pulsar glitches. While modern glitch models rely on pinned superfluid vorticity rather than crustal fracture, starquakes may nevertheless play a role in the glitch mechanism. Recently, there has been interest in the issue of starquakes resulting in non-axisymmetric shape changes, potentially linking the quake phenomenon to the building of neutron star mountains, which would then produce continuous gravitational waves. Motivated by this issue, we present a simple model that extends the energy minimisation-based calculations, originally developed to model axisymmetric glitches, to also include non-axisymmetric shape changes. We show that the creation of a mountain in a quake necessarily requires a change in the axisymmetric shape too. We apply our model to the specific problem of the spin-up of an initially non-rotating star, and estimate the maximum mountain that can be built in such a process, subject only to the constraints of energy and angular momentum conservation.","sentences":["When a neutron star is spun-up or spun-down, the changing strains in its solid elastic crust can give rise to sudden fractures known as starquakes.","Early interest in starquakes focused on their possible connection to pulsar glitches.","While modern glitch models rely on pinned superfluid vorticity rather than crustal fracture, starquakes may nevertheless play a role in the glitch mechanism.","Recently, there has been interest in the issue of starquakes resulting in non-axisymmetric shape changes, potentially linking the quake phenomenon to the building of neutron star mountains, which would then produce continuous gravitational waves.","Motivated by this issue, we present a simple model that extends the energy minimisation-based calculations, originally developed to model axisymmetric glitches, to also include non-axisymmetric shape changes.","We show that the creation of a mountain in a quake necessarily requires a change in the axisymmetric shape too.","We apply our model to the specific problem of the spin-up of an initially non-rotating star, and estimate the maximum mountain that can be built in such a process, subject only to the constraints of energy and angular momentum conservation."],"url":"http://arxiv.org/abs/2405.00553v1","category":"astro-ph.HE"}
{"created":"2024-05-01 14:35:12","title":"Ultralight Primordial Black Holes","abstract":"The fate of ultralight black holes depends on whether or not evaporation stops at or around the Planck scale. If evaporation stops, the general expectation is that a population of Planck-scale will be left over, possibly including a significant fraction of electrically charged relics. If evaporation does not stop, a runaway \"explosion\" would occur, with significant and potentially detectable high-energy emission. Here, I review both possibilities, with an emphasis on current status and future detection prospects.","sentences":["The fate of ultralight black holes depends on whether or not evaporation stops at or around the Planck scale.","If evaporation stops, the general expectation is that a population of Planck-scale will be left over, possibly including a significant fraction of electrically charged relics.","If evaporation does not stop, a runaway \"explosion\" would occur, with significant and potentially detectable high-energy emission.","Here, I review both possibilities, with an emphasis on current status and future detection prospects."],"url":"http://arxiv.org/abs/2405.00546v1","category":"astro-ph.HE"}
{"created":"2024-05-01 14:31:43","title":"A Double Maximization Approach for Optimizing the LM Rate of Mismatched Decoding","abstract":"An approach is established for maximizing the Lower bound on the Mismatch capacity (hereafter abbreviated as LM rate), a key performance bound in mismatched decoding, by optimizing the channel input probability distribution. Under a fixed channel input probability distribution, the computation of the corresponding LM rate is a convex optimization problem. When optimizing the channel input probability distribution, however, the corresponding optimization problem adopts a max-min formulation, which is generally non-convex and is intractable with standard approaches. To solve this problem, a novel dual form of the LM rate is proposed, thereby transforming the max-min formulation into an equivalent double maximization formulation. This new formulation leads to a maximization problem setup wherein each individual optimization direction is convex. Consequently, an alternating maximization algorithm is established to solve the resultant maximization problem setup. Each step of the algorithm only involves a closed-form iteration, which is efficiently implemented with standard optimization procedures. Numerical experiments show the proposed approach for optimizing the LM rate leads to noticeable rate gains.","sentences":["An approach is established for maximizing the Lower bound on the Mismatch capacity (hereafter abbreviated as LM rate), a key performance bound in mismatched decoding, by optimizing the channel input probability distribution.","Under a fixed channel input probability distribution, the computation of the corresponding LM rate is a convex optimization problem.","When optimizing the channel input probability distribution, however, the corresponding optimization problem adopts a max-min formulation, which is generally non-convex and is intractable with standard approaches.","To solve this problem, a novel dual form of the LM rate is proposed, thereby transforming the max-min formulation into an equivalent double maximization formulation.","This new formulation leads to a maximization problem setup wherein each individual optimization direction is convex.","Consequently, an alternating maximization algorithm is established to solve the resultant maximization problem setup.","Each step of the algorithm only involves a closed-form iteration, which is efficiently implemented with standard optimization procedures.","Numerical experiments show the proposed approach for optimizing the LM rate leads to noticeable rate gains."],"url":"http://arxiv.org/abs/2405.00545v1","category":"cs.IT"}
{"created":"2024-05-01 14:29:03","title":"New Benchmark Dataset and Fine-Grained Cross-Modal Fusion Framework for Vietnamese Multimodal Aspect-Category Sentiment Analysis","abstract":"The emergence of multimodal data on social media platforms presents new opportunities to better understand user sentiments toward a given aspect. However, existing multimodal datasets for Aspect-Category Sentiment Analysis (ACSA) often focus on textual annotations, neglecting fine-grained information in images. Consequently, these datasets fail to fully exploit the richness inherent in multimodal. To address this, we introduce a new Vietnamese multimodal dataset, named ViMACSA, which consists of 4,876 text-image pairs with 14,618 fine-grained annotations for both text and image in the hotel domain. Additionally, we propose a Fine-Grained Cross-Modal Fusion Framework (FCMF) that effectively learns both intra- and inter-modality interactions and then fuses these information to produce a unified multimodal representation. Experimental results show that our framework outperforms SOTA models on the ViMACSA dataset, achieving the highest F1 score of 79.73%. We also explore characteristics and challenges in Vietnamese multimodal sentiment analysis, including misspellings, abbreviations, and the complexities of the Vietnamese language. This work contributes both a benchmark dataset and a new framework that leverages fine-grained multimodal information to improve multimodal aspect-category sentiment analysis. Our dataset is available for research purposes: https://github.com/hoangquy18/Multimodal-Aspect-Category-Sentiment-Analysis.","sentences":["The emergence of multimodal data on social media platforms presents new opportunities to better understand user sentiments toward a given aspect.","However, existing multimodal datasets for Aspect-Category Sentiment Analysis (ACSA) often focus on textual annotations, neglecting fine-grained information in images.","Consequently, these datasets fail to fully exploit the richness inherent in multimodal.","To address this, we introduce a new Vietnamese multimodal dataset, named ViMACSA, which consists of 4,876 text-image pairs with 14,618 fine-grained annotations for both text and image in the hotel domain.","Additionally, we propose a Fine-Grained Cross-Modal Fusion Framework (FCMF) that effectively learns both intra- and inter-modality interactions and then fuses these information to produce a unified multimodal representation.","Experimental results show that our framework outperforms SOTA models on the ViMACSA dataset, achieving the highest F1 score of 79.73%.","We also explore characteristics and challenges in Vietnamese multimodal sentiment analysis, including misspellings, abbreviations, and the complexities of the Vietnamese language.","This work contributes both a benchmark dataset and a new framework that leverages fine-grained multimodal information to improve multimodal aspect-category sentiment analysis.","Our dataset is available for research purposes: https://github.com/hoangquy18/Multimodal-Aspect-Category-Sentiment-Analysis."],"url":"http://arxiv.org/abs/2405.00543v1","category":"cs.CL"}
{"created":"2024-05-01 14:27:43","title":"UWAFA-GAN: Ultra-Wide-Angle Fluorescein Angiography Transformation via Multi-scale Generation and Registration Enhancement","abstract":"Fundus photography, in combination with the ultra-wide-angle fundus (UWF) techniques, becomes an indispensable diagnostic tool in clinical settings by offering a more comprehensive view of the retina. Nonetheless, UWF fluorescein angiography (UWF-FA) necessitates the administration of a fluorescent dye via injection into the patient's hand or elbow unlike UWF scanning laser ophthalmoscopy (UWF-SLO). To mitigate potential adverse effects associated with injections, researchers have proposed the development of cross-modality medical image generation algorithms capable of converting UWF-SLO images into their UWF-FA counterparts. Current image generation techniques applied to fundus photography encounter difficulties in producing high-resolution retinal images, particularly in capturing minute vascular lesions. To address these issues, we introduce a novel conditional generative adversarial network (UWAFA-GAN) to synthesize UWF-FA from UWF-SLO. This approach employs multi-scale generators and an attention transmit module to efficiently extract both global structures and local lesions. Additionally, to counteract the image blurriness issue that arises from training with misaligned data, a registration module is integrated within this framework. Our method performs non-trivially on inception scores and details generation. Clinical user studies further indicate that the UWF-FA images generated by UWAFA-GAN are clinically comparable to authentic images in terms of diagnostic reliability. Empirical evaluations on our proprietary UWF image datasets elucidate that UWAFA-GAN outperforms extant methodologies. The code is accessible at https://github.com/Tinysqua/UWAFA-GAN.","sentences":["Fundus photography, in combination with the ultra-wide-angle fundus (UWF) techniques, becomes an indispensable diagnostic tool in clinical settings by offering a more comprehensive view of the retina.","Nonetheless, UWF fluorescein angiography (UWF-FA) necessitates the administration of a fluorescent dye via injection into the patient's hand or elbow unlike UWF scanning laser ophthalmoscopy (UWF-SLO).","To mitigate potential adverse effects associated with injections, researchers have proposed the development of cross-modality medical image generation algorithms capable of converting UWF-SLO images into their UWF-FA counterparts.","Current image generation techniques applied to fundus photography encounter difficulties in producing high-resolution retinal images, particularly in capturing minute vascular lesions.","To address these issues, we introduce a novel conditional generative adversarial network (UWAFA-GAN) to synthesize UWF-FA from UWF-SLO.","This approach employs multi-scale generators and an attention transmit module to efficiently extract both global structures and local lesions.","Additionally, to counteract the image blurriness issue that arises from training with misaligned data, a registration module is integrated within this framework.","Our method performs non-trivially on inception scores and details generation.","Clinical user studies further indicate that the UWF-FA images generated by UWAFA-GAN are clinically comparable to authentic images in terms of diagnostic reliability.","Empirical evaluations on our proprietary UWF image datasets elucidate that UWAFA-GAN outperforms extant methodologies.","The code is accessible at https://github.com/Tinysqua/UWAFA-GAN."],"url":"http://arxiv.org/abs/2405.00542v1","category":"eess.IV"}
{"created":"2024-05-01 14:27:15","title":"Heat, Health, and Habitats: Analyzing the Intersecting Risks of Climate and Demographic Shifts in Austrian Districts","abstract":"The impact of hot weather on health outcomes of a population is mediated by a variety of factors, including its age profile and local green infrastructure. The combination of warming due to climate change and demographic aging suggests that heat-related health outcomes will deteriorate in the coming decades. Here, we measure the relationship between weekly all-cause mortality and heat days in Austrian districts using a panel dataset covering $2015-2022$. An additional day reaching $30$ degrees is associated with a $2.4\\%$ increase in mortality per $1000$ inhabitants during summer. This association is roughly doubled in districts with a two standard deviation above average share of the population over $65$. Using forecasts of hot days (RCP) and demographics in $2050$, we observe that districts will have elderly populations and hot days $2-5$ standard deviations above the current mean in just $25$ years. This predicts a drastic increase in heat-related mortality. At the same time, district green scores, measured using $10\\times 10$ meter resolution satellite images of residential areas, significantly moderate the relationship between heat and mortality. Thus, although local policies likely cannot reverse warming or demographic trends, they can take measures to mediate the health consequences of these growing risks, which are highly heterogeneous across regions, even in Austria.","sentences":["The impact of hot weather on health outcomes of a population is mediated by a variety of factors, including its age profile and local green infrastructure.","The combination of warming due to climate change and demographic aging suggests that heat-related health outcomes will deteriorate in the coming decades.","Here, we measure the relationship between weekly all-cause mortality and heat days in Austrian districts using a panel dataset covering $2015-2022$.","An additional day reaching $30$ degrees is associated with a $2.4\\%$ increase in mortality per $1000$ inhabitants during summer.","This association is roughly doubled in districts with a two standard deviation above average share of the population over $65$. Using forecasts of hot days (RCP) and demographics in $2050$, we observe that districts will have elderly populations and hot days $2-5$ standard deviations above the current mean in just $25$ years.","This predicts a drastic increase in heat-related mortality.","At the same time, district green scores, measured using $10\\times 10$ meter resolution satellite images of residential areas, significantly moderate the relationship between heat and mortality.","Thus, although local policies likely cannot reverse warming or demographic trends, they can take measures to mediate the health consequences of these growing risks, which are highly heterogeneous across regions, even in Austria."],"url":"http://arxiv.org/abs/2405.00540v1","category":"cs.CY"}
{"created":"2024-05-01 14:26:37","title":"Data-driven approximation of Koopman operators and generators: Convergence rates and error bounds","abstract":"Global information about dynamical systems can be extracted by analysing associated infinite-dimensional transfer operators, such as Perron-Frobenius and Koopman operators as well as their infinitesimal generators. In practice, these operators typically need to be approximated from data. Popular approximation methods are extended dynamic mode decomposition (EDMD) and generator extended mode decomposition (gEDMD). We propose a unified framework that leverages Monte Carlo sampling to approximate the operator of interest on a finite-dimensional space spanned by a set of basis functions. Our framework contains EDMD and gEDMD as special cases, but can also be used to approximate more general operators. Our key contributions are proofs of the convergence of the approximating operator and its spectrum under non-restrictive conditions. Moreover, we derive explicit convergence rates and account for the presence of noise in the observations. Whilst all these results are broadly applicable, they also refine previous analyses of EDMD and gEDMD. We verify the analytical results with the aid of several numerical experiments.","sentences":["Global information about dynamical systems can be extracted by analysing associated infinite-dimensional transfer operators, such as Perron-Frobenius and Koopman operators as well as their infinitesimal generators.","In practice, these operators typically need to be approximated from data.","Popular approximation methods are extended dynamic mode decomposition (EDMD) and generator extended mode decomposition (gEDMD).","We propose a unified framework that leverages Monte Carlo sampling to approximate the operator of interest on a finite-dimensional space spanned by a set of basis functions.","Our framework contains EDMD and gEDMD as special cases, but can also be used to approximate more general operators.","Our key contributions are proofs of the convergence of the approximating operator and its spectrum under non-restrictive conditions.","Moreover, we derive explicit convergence rates and account for the presence of noise in the observations.","Whilst all these results are broadly applicable, they also refine previous analyses of EDMD and gEDMD.","We verify the analytical results with the aid of several numerical experiments."],"url":"http://arxiv.org/abs/2405.00539v1","category":"math.NA"}
{"created":"2024-05-01 14:18:50","title":"A Legal Framework for Natural Language Processing Model Training in Portugal","abstract":"Recent advances in deep learning have promoted the advent of many computational systems capable of performing intelligent actions that, until then, were restricted to the human intellect. In the particular case of human languages, these advances allowed the introduction of applications like ChatGPT that are capable of generating coherent text without being explicitly programmed to do so. Instead, these models use large volumes of textual data to learn meaningful representations of human languages. Associated with these advances, concerns about copyright and data privacy infringements caused by these applications have emerged. Despite these concerns, the pace at which new natural language processing applications continued to be developed largely outperformed the introduction of new regulations. Today, communication barriers between legal experts and computer scientists motivate many unintentional legal infringements during the development of such applications. In this paper, a multidisciplinary team intends to bridge this communication gap and promote more compliant Portuguese NLP research by presenting a series of everyday NLP use cases, while highlighting the Portuguese legislation that may arise during its development.","sentences":["Recent advances in deep learning have promoted the advent of many computational systems capable of performing intelligent actions that, until then, were restricted to the human intellect.","In the particular case of human languages, these advances allowed the introduction of applications like ChatGPT that are capable of generating coherent text without being explicitly programmed to do so.","Instead, these models use large volumes of textual data to learn meaningful representations of human languages.","Associated with these advances, concerns about copyright and data privacy infringements caused by these applications have emerged.","Despite these concerns, the pace at which new natural language processing applications continued to be developed largely outperformed the introduction of new regulations.","Today, communication barriers between legal experts and computer scientists motivate many unintentional legal infringements during the development of such applications.","In this paper, a multidisciplinary team intends to bridge this communication gap and promote more compliant Portuguese NLP research by presenting a series of everyday NLP use cases, while highlighting the Portuguese legislation that may arise during its development."],"url":"http://arxiv.org/abs/2405.00536v1","category":"cs.CL"}
{"created":"2024-05-01 14:08:03","title":"Non-Abelian line graph: A generalized approach to flat bands","abstract":"Line graph (LG) lattices are well known to host flat bands (FBs) with isotropic hoppings in $s$-orbital models. Yet, higher-angular-momentum orbitals with spin-orbit coupling (SOC), which are more common in real materials, lack a general approach for their inclusion in LG to achieve FBs. Here, we introduce a non-Abelian LG theory to construct FBs in realistic systems, which incorporates internal degrees of freedom and goes beyond real-valued isotropic hoppings. The lattice edges and sites in the LG are modified to be associated with arbitrary Hermitian matrices, refereed to as multiple LG. A crucial step is to map the multiple LG Hamiltonian to a tight-binding (TB) model that respects the lattice symmetry through appropriate local non-Abelian transformations in the internal space. We find the general conditions to determine the local transformation. Based on this mechanism, we discuss the realization of $d$-orbital FBs in the Kagome lattice, which may serve as a minimal model for understanding the high-orbital FBs with SOC in Kagome materials. Our approach bridges the known FBs in pure lattice models and the realization in multi-orbital systems.","sentences":["Line graph (LG) lattices are well known to host flat bands (FBs) with isotropic hoppings in $s$-orbital models.","Yet, higher-angular-momentum orbitals with spin-orbit coupling (SOC), which are more common in real materials, lack a general approach for their inclusion in LG to achieve FBs.","Here, we introduce a non-Abelian LG theory to construct FBs in realistic systems, which incorporates internal degrees of freedom and goes beyond real-valued isotropic hoppings.","The lattice edges and sites in the LG are modified to be associated with arbitrary Hermitian matrices, refereed to as multiple LG.","A crucial step is to map the multiple LG Hamiltonian to a tight-binding (TB) model that respects the lattice symmetry through appropriate local non-Abelian transformations in the internal space.","We find the general conditions to determine the local transformation.","Based on this mechanism, we discuss the realization of $d$-orbital FBs in the Kagome lattice, which may serve as a minimal model for understanding the high-orbital FBs with SOC in Kagome materials.","Our approach bridges the known FBs in pure lattice models and the realization in multi-orbital systems."],"url":"http://arxiv.org/abs/2405.00534v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-01 14:05:52","title":"ULLER: A Unified Language for Learning and Reasoning","abstract":"The field of neuro-symbolic artificial intelligence (NeSy), which combines learning and reasoning, has recently experienced significant growth. There now are a wide variety of NeSy frameworks, each with its own specific language for expressing background knowledge and how to relate it to neural networks. This heterogeneity hinders accessibility for newcomers and makes comparing different NeSy frameworks challenging. We propose a unified language for NeSy, which we call ULLER, a Unified Language for LEarning and Reasoning. ULLER encompasses a wide variety of settings, while ensuring that knowledge described in it can be used in existing NeSy systems. ULLER has a neuro-symbolic first-order syntax for which we provide example semantics including classical, fuzzy, and probabilistic logics. We believe ULLER is a first step towards making NeSy research more accessible and comparable, paving the way for libraries that streamline training and evaluation across a multitude of semantics, knowledge bases, and NeSy systems.","sentences":["The field of neuro-symbolic artificial intelligence (NeSy), which combines learning and reasoning, has recently experienced significant growth.","There now are a wide variety of NeSy frameworks, each with its own specific language for expressing background knowledge and how to relate it to neural networks.","This heterogeneity hinders accessibility for newcomers and makes comparing different NeSy frameworks challenging.","We propose a unified language for NeSy, which we call ULLER, a Unified Language for LEarning and Reasoning.","ULLER encompasses a wide variety of settings, while ensuring that knowledge described in it can be used in existing NeSy systems.","ULLER has a neuro-symbolic first-order syntax for which we provide example semantics including classical, fuzzy, and probabilistic logics.","We believe ULLER is a first step towards making NeSy research more accessible and comparable, paving the way for libraries that streamline training and evaluation across a multitude of semantics, knowledge bases, and NeSy systems."],"url":"http://arxiv.org/abs/2405.00532v1","category":"cs.AI"}
{"created":"2024-05-01 14:01:22","title":"ChatBI: Towards Natural Language to Complex Business Intelligence SQL","abstract":"The Natural Language to SQL (NL2SQL) technology provides non-expert users who are unfamiliar with databases the opportunity to use SQL for data analysis.Converting Natural Language to Business Intelligence (NL2BI) is a popular practical scenario for NL2SQL in actual production systems. Compared to NL2SQL, NL2BI introduces more challenges.   In this paper, we propose ChatBI, a comprehensive and efficient technology for solving the NL2BI task. First, we analyze the interaction mode, an important module where NL2SQL and NL2BI differ in use, and design a smaller and cheaper model to match this interaction mode. In BI scenarios, tables contain a huge number of columns, making it impossible for existing NL2SQL methods that rely on Large Language Models (LLMs) for schema linking to proceed due to token limitations. The higher proportion of ambiguous columns in BI scenarios also makes schema linking difficult. ChatBI combines existing view technology in the database community to first decompose the schema linking problem into a Single View Selection problem and then uses a smaller and cheaper machine learning model to select the single view with a significantly reduced number of columns. The columns of this single view are then passed as the required columns for schema linking into the LLM. Finally, ChatBI proposes a phased process flow different from existing process flows, which allows ChatBI to generate SQL containing complex semantics and comparison relations more accurately.   We have deployed ChatBI on Baidu's data platform and integrated it into multiple product lines for large-scale production task evaluation. The obtained results highlight its superiority in practicality, versatility, and efficiency. At the same time, compared with the current mainstream NL2SQL technology under our real BI scenario data tables and queries, it also achieved the best results.","sentences":["The Natural Language to SQL (NL2SQL) technology provides non-expert users who are unfamiliar with databases the opportunity to use SQL for data analysis.","Converting Natural Language to Business Intelligence (NL2BI) is a popular practical scenario for NL2SQL in actual production systems.","Compared to NL2SQL, NL2BI introduces more challenges.   ","In this paper, we propose ChatBI, a comprehensive and efficient technology for solving the NL2BI task.","First, we analyze the interaction mode, an important module where NL2SQL and NL2BI differ in use, and design a smaller and cheaper model to match this interaction mode.","In BI scenarios, tables contain a huge number of columns, making it impossible for existing NL2SQL methods that rely on Large Language Models (LLMs) for schema linking to proceed due to token limitations.","The higher proportion of ambiguous columns in BI scenarios also makes schema linking difficult.","ChatBI combines existing view technology in the database community to first decompose the schema linking problem into a Single View Selection problem and then uses a smaller and cheaper machine learning model to select the single view with a significantly reduced number of columns.","The columns of this single view are then passed as the required columns for schema linking into the LLM.","Finally, ChatBI proposes a phased process flow different from existing process flows, which allows ChatBI to generate SQL containing complex semantics and comparison relations more accurately.   ","We have deployed ChatBI on Baidu's data platform and integrated it into multiple product lines for large-scale production task evaluation.","The obtained results highlight its superiority in practicality, versatility, and efficiency.","At the same time, compared with the current mainstream NL2SQL technology under our real BI scenario data tables and queries, it also achieved the best results."],"url":"http://arxiv.org/abs/2405.00527v1","category":"cs.DB"}
{"created":"2024-05-01 13:58:28","title":"FMLFS: A federated multi-label feature selection based on information theory in IoT environment","abstract":"In certain emerging applications such as health monitoring wearable and traffic monitoring systems, Internet-of-Things (IoT) devices generate or collect a huge amount of multi-label datasets. Within these datasets, each instance is linked to a set of labels. The presence of noisy, redundant, or irrelevant features in these datasets, along with the curse of dimensionality, poses challenges for multi-label classifiers. Feature selection (FS) proves to be an effective strategy in enhancing classifier performance and addressing these challenges. Yet, there is currently no existing distributed multi-label FS method documented in the literature that is suitable for distributed multi-label datasets within IoT environments. This paper introduces FMLFS, the first federated multi-label feature selection method. Here, mutual information between features and labels serves as the relevancy metric, while the correlation distance between features, derived from mutual information and joint entropy, is utilized as the redundancy measure. Following aggregation of these metrics on the edge server and employing Pareto-based bi-objective and crowding distance strategies, the sorted features are subsequently sent back to the IoT devices. The proposed method is evaluated through two scenarios: 1) transmitting reduced-size datasets to the edge server for centralized classifier usage, and 2) employing federated learning with reduced-size datasets. Evaluation across three metrics - performance, time complexity, and communication cost - demonstrates that FMLFS outperforms five other comparable methods in the literature and provides a good trade-off on three real-world datasets.","sentences":["In certain emerging applications such as health monitoring wearable and traffic monitoring systems, Internet-of-Things (IoT) devices generate or collect a huge amount of multi-label datasets.","Within these datasets, each instance is linked to a set of labels.","The presence of noisy, redundant, or irrelevant features in these datasets, along with the curse of dimensionality, poses challenges for multi-label classifiers.","Feature selection (FS) proves to be an effective strategy in enhancing classifier performance and addressing these challenges.","Yet, there is currently no existing distributed multi-label FS method documented in the literature that is suitable for distributed multi-label datasets within IoT environments.","This paper introduces FMLFS, the first federated multi-label feature selection method.","Here, mutual information between features and labels serves as the relevancy metric, while the correlation distance between features, derived from mutual information and joint entropy, is utilized as the redundancy measure.","Following aggregation of these metrics on the edge server and employing Pareto-based bi-objective and crowding distance strategies, the sorted features are subsequently sent back to the IoT devices.","The proposed method is evaluated through two scenarios: 1) transmitting reduced-size datasets to the edge server for centralized classifier usage, and 2) employing federated learning with reduced-size datasets.","Evaluation across three metrics - performance, time complexity, and communication cost - demonstrates that FMLFS outperforms five other comparable methods in the literature and provides a good trade-off on three real-world datasets."],"url":"http://arxiv.org/abs/2405.00524v1","category":"cs.LG"}
{"created":"2024-05-01 13:58:09","title":"CookingSense: A Culinary Knowledgebase with Multidisciplinary Assertions","abstract":"This paper introduces CookingSense, a descriptive collection of knowledge assertions in the culinary domain extracted from various sources, including web data, scientific papers, and recipes, from which knowledge covering a broad range of aspects is acquired. CookingSense is constructed through a series of dictionary-based filtering and language model-based semantic filtering techniques, which results in a rich knowledgebase of multidisciplinary food-related assertions. Additionally, we present FoodBench, a novel benchmark to evaluate culinary decision support systems. From evaluations with FoodBench, we empirically prove that CookingSense improves the performance of retrieval augmented language models. We also validate the quality and variety of assertions in CookingSense through qualitative analysis.","sentences":["This paper introduces CookingSense, a descriptive collection of knowledge assertions in the culinary domain extracted from various sources, including web data, scientific papers, and recipes, from which knowledge covering a broad range of aspects is acquired.","CookingSense is constructed through a series of dictionary-based filtering and language model-based semantic filtering techniques, which results in a rich knowledgebase of multidisciplinary food-related assertions.","Additionally, we present FoodBench, a novel benchmark to evaluate culinary decision support systems.","From evaluations with FoodBench, we empirically prove that CookingSense improves the performance of retrieval augmented language models.","We also validate the quality and variety of assertions in CookingSense through qualitative analysis."],"url":"http://arxiv.org/abs/2405.00523v1","category":"cs.AI"}
{"created":"2024-05-01 13:58:01","title":"DAM: A Universal Dual Attention Mechanism for Multimodal Timeseries Cryptocurrency Trend Forecasting","abstract":"In the distributed systems landscape, Blockchain has catalyzed the rise of cryptocurrencies, merging enhanced security and decentralization with significant investment opportunities. Despite their potential, current research on cryptocurrency trend forecasting often falls short by simplistically merging sentiment data without fully considering the nuanced interplay between financial market dynamics and external sentiment influences. This paper presents a novel Dual Attention Mechanism (DAM) for forecasting cryptocurrency trends using multimodal time-series data. Our approach, which integrates critical cryptocurrency metrics with sentiment data from news and social media analyzed through CryptoBERT, addresses the inherent volatility and prediction challenges in cryptocurrency markets. By combining elements of distributed systems, natural language processing, and financial forecasting, our method outperforms conventional models like LSTM and Transformer by up to 20\\% in prediction accuracy. This advancement deepens the understanding of distributed systems and has practical implications in financial markets, benefiting stakeholders in cryptocurrency and blockchain technologies. Moreover, our enhanced forecasting approach can significantly support decentralized science (DeSci) by facilitating strategic planning and the efficient adoption of blockchain technologies, improving operational efficiency and financial risk management in the rapidly evolving digital asset domain, thus ensuring optimal resource allocation.","sentences":["In the distributed systems landscape, Blockchain has catalyzed the rise of cryptocurrencies, merging enhanced security and decentralization with significant investment opportunities.","Despite their potential, current research on cryptocurrency trend forecasting often falls short by simplistically merging sentiment data without fully considering the nuanced interplay between financial market dynamics and external sentiment influences.","This paper presents a novel Dual Attention Mechanism (DAM) for forecasting cryptocurrency trends using multimodal time-series data.","Our approach, which integrates critical cryptocurrency metrics with sentiment data from news and social media analyzed through CryptoBERT, addresses the inherent volatility and prediction challenges in cryptocurrency markets.","By combining elements of distributed systems, natural language processing, and financial forecasting, our method outperforms conventional models like LSTM and Transformer by up to 20\\% in prediction accuracy.","This advancement deepens the understanding of distributed systems and has practical implications in financial markets, benefiting stakeholders in cryptocurrency and blockchain technologies.","Moreover, our enhanced forecasting approach can significantly support decentralized science (DeSci) by facilitating strategic planning and the efficient adoption of blockchain technologies, improving operational efficiency and financial risk management in the rapidly evolving digital asset domain, thus ensuring optimal resource allocation."],"url":"http://arxiv.org/abs/2405.00522v1","category":"econ.GN"}
{"created":"2024-05-01 13:57:04","title":"Thermodynamic formalism of countably generated self-affine sets","abstract":"In this article, we further develop the thermodynamic formalism of affine iterated function systems with countably many transformations by showing the existence and extending earlier characterisations of the equilibrium states of finite affine iterated function systems to the countably infinite case. As an application, under mild conditions, we prove that the affinity dimension of a countable affine iterated function system is equal to the supremum of the affinity dimensions of its finite subsystems. We deduce corollaries concerning the Hausdorff dimension of countably generated self-affine sets in dimensions $1$, $2$, and $3$ satisfying mild deterministic assumptions and in arbitrary dimension with generic translations.","sentences":["In this article, we further develop the thermodynamic formalism of affine iterated function systems with countably many transformations by showing the existence and extending earlier characterisations of the equilibrium states of finite affine iterated function systems to the countably infinite case.","As an application, under mild conditions, we prove that the affinity dimension of a countable affine iterated function system is equal to the supremum of the affinity dimensions of its finite subsystems.","We deduce corollaries concerning the Hausdorff dimension of countably generated self-affine sets in dimensions $1$, $2$, and $3$ satisfying mild deterministic assumptions and in arbitrary dimension with generic translations."],"url":"http://arxiv.org/abs/2405.00520v1","category":"math.DS"}
{"created":"2024-05-01 13:56:31","title":"Design Implications for a Social and Collaborative Understanding of online Information Assessment Practices, Challenges and Heuristics","abstract":"The broader adoption of social media platforms (e.g., TikTok), combined with recent developments in Generative AI (GAI) technologies has had a transformative effect on many peoples' ability to confidently assess the veracity and meaning of information online. In this paper, building on recent related work that surfaced the social ways that young people evaluate information online, we explore the decision-making practices, challenges and heuristics involved in young adults' assessments of information online. To do so, we designed and conducted a novel digital diary study, followed by data-informed interviews with young adults. Our findings uncover the information practices of young adults including the social and emotional motivations for ignoring, avoiding, and engaging with online information and the ways this is entangled with collaborative arrangements with algorithms as agents. In our discussion we bring these findings in close dialogue with work on information sensibility and contribute rich insights into young peoples' information sensibility practices embedded within social worlds. Finally, we surface how such practices are attuned to prioritise wellbeing over convenience or other commonly associated sufficing heuristics.","sentences":["The broader adoption of social media platforms (e.g., TikTok), combined with recent developments in Generative AI (GAI) technologies has had a transformative effect on many peoples' ability to confidently assess the veracity and meaning of information online.","In this paper, building on recent related work that surfaced the social ways that young people evaluate information online, we explore the decision-making practices, challenges and heuristics involved in young adults' assessments of information online.","To do so, we designed and conducted a novel digital diary study, followed by data-informed interviews with young adults.","Our findings uncover the information practices of young adults including the social and emotional motivations for ignoring, avoiding, and engaging with online information and the ways this is entangled with collaborative arrangements with algorithms as agents.","In our discussion we bring these findings in close dialogue with work on information sensibility and contribute rich insights into young peoples' information sensibility practices embedded within social worlds.","Finally, we surface how such practices are attuned to prioritise wellbeing over convenience or other commonly associated sufficing heuristics."],"url":"http://arxiv.org/abs/2405.00519v1","category":"cs.HC"}
{"created":"2024-05-01 13:51:45","title":"Navigating WebAI: Training Agents to Complete Web Tasks with Large Language Models and Reinforcement Learning","abstract":"Recent advancements in language models have demonstrated remarkable improvements in various natural language processing (NLP) tasks such as web navigation. Supervised learning (SL) approaches have achieved impressive performance while utilizing significantly less training data compared to previous methods. However, these SL-based models fall short when compared to reinforcement learning (RL) approaches, which have shown superior results. In this paper, we propose a novel approach that combines SL and RL techniques over the MiniWoB benchmark to leverage the strengths of both methods. We also address a critical limitation in previous models' understanding of HTML content, revealing a tendency to memorize target elements rather than comprehend the underlying structure. To rectify this, we propose methods to enhance true understanding and present a new baseline of results. Our experiments demonstrate that our approach outperforms previous SL methods on certain tasks using less data and narrows the performance gap with RL models, achieving 43.58\\% average accuracy in SL and 36.69\\% when combined with a multimodal RL approach. This study sets a new direction for future web navigation and offers insights into the limitations and potential of language modeling for computer tasks.","sentences":["Recent advancements in language models have demonstrated remarkable improvements in various natural language processing (NLP) tasks such as web navigation.","Supervised learning (SL) approaches have achieved impressive performance while utilizing significantly less training data compared to previous methods.","However, these SL-based models fall short when compared to reinforcement learning (RL) approaches, which have shown superior results.","In this paper, we propose a novel approach that combines SL and RL techniques over the MiniWoB benchmark to leverage the strengths of both methods.","We also address a critical limitation in previous models' understanding of HTML content, revealing a tendency to memorize target elements rather than comprehend the underlying structure.","To rectify this, we propose methods to enhance true understanding and present a new baseline of results.","Our experiments demonstrate that our approach outperforms previous SL methods on certain tasks using less data and narrows the performance gap with RL models, achieving 43.58\\% average accuracy in SL and 36.69\\% when combined with a multimodal RL approach.","This study sets a new direction for future web navigation and offers insights into the limitations and potential of language modeling for computer tasks."],"url":"http://arxiv.org/abs/2405.00516v1","category":"cs.LG"}
{"created":"2024-05-01 13:51:39","title":"GAD-Generative Learning for HD Map-Free Autonomous Driving","abstract":"Deep-learning-based techniques have been widely adopted for autonomous driving software stacks for mass production in recent years, focusing primarily on perception modules, with some work extending this method to prediction modules. However, the downstream planning and control modules are still designed with hefty handcrafted rules, dominated by optimization-based methods such as quadratic programming or model predictive control. This results in a performance bottleneck for autonomous driving systems in that corner cases simply cannot be solved by enumerating hand-crafted rules. We present a deep-learning-based approach that brings prediction, decision, and planning modules together with the attempt to overcome the rule-based methods' deficiency in real-world applications of autonomous driving, especially for urban scenes. The DNN model we proposed is solely trained with 10 hours of human driver data, and it supports all mass-production ADAS features available on the market to date. This method is deployed onto a Jiyue test car with no modification to its factory-ready sensor set and compute platform. the feasibility, usability, and commercial potential are demonstrated in this article.","sentences":["Deep-learning-based techniques have been widely adopted for autonomous driving software stacks for mass production in recent years, focusing primarily on perception modules, with some work extending this method to prediction modules.","However, the downstream planning and control modules are still designed with hefty handcrafted rules, dominated by optimization-based methods such as quadratic programming or model predictive control.","This results in a performance bottleneck for autonomous driving systems in that corner cases simply cannot be solved by enumerating hand-crafted rules.","We present a deep-learning-based approach that brings prediction, decision, and planning modules together with the attempt to overcome the rule-based methods' deficiency in real-world applications of autonomous driving, especially for urban scenes.","The DNN model we proposed is solely trained with 10 hours of human driver data, and it supports all mass-production ADAS features available on the market to date.","This method is deployed onto a Jiyue test car with no modification to its factory-ready sensor set and compute platform.","the feasibility, usability, and commercial potential are demonstrated in this article."],"url":"http://arxiv.org/abs/2405.00515v1","category":"cs.RO"}
{"created":"2024-05-01 13:49:09","title":"Get Your Embedding Space in Order: Domain-Adaptive Regression for Forest Monitoring","abstract":"Image-level regression is an important task in Earth observation, where visual domain and label shifts are a core challenge hampering generalization. However, cross-domain regression with remote sensing data remains understudied due to the absence of suited datasets. We introduce a new dataset with aerial and satellite imagery in five countries with three forest-related regression tasks. To match real-world applicative interests, we compare methods through a restrictive setup where no prior on the target domain is available during training, and models are adapted with limited information during testing. Building on the assumption that ordered relationships generalize better, we propose manifold diffusion for regression as a strong baseline for transduction in low-data regimes. Our comparison highlights the comparative advantages of inductive and transductive methods in cross-domain regression.","sentences":["Image-level regression is an important task in Earth observation, where visual domain and label shifts are a core challenge hampering generalization.","However, cross-domain regression with remote sensing data remains understudied due to the absence of suited datasets.","We introduce a new dataset with aerial and satellite imagery in five countries with three forest-related regression tasks.","To match real-world applicative interests, we compare methods through a restrictive setup where no prior on the target domain is available during training, and models are adapted with limited information during testing.","Building on the assumption that ordered relationships generalize better, we propose manifold diffusion for regression as a strong baseline for transduction in low-data regimes.","Our comparison highlights the comparative advantages of inductive and transductive methods in cross-domain regression."],"url":"http://arxiv.org/abs/2405.00514v1","category":"cs.CV"}
{"created":"2024-05-01 13:39:28","title":"Hyperuniformity in phase ordering: the roles of activity, noise, and non-constant mobility","abstract":"Hyperuniformity emerges generically in the coarsening regime of phase-separating fluids. Numerical studies of active and passive systems have shown that the structure factor $S(q)$ behaves as $q^\\varsigma$ for $q\\to 0$, with hyperuniformity exponent $\\varsigma = 4$. For passive systems, this result was explained in 1991 by a qualitative scaling analysis of Tomita, exploiting isotropy at scales much larger than the coarsening length $\\ell$. Here we reconsider and extend Tomita's argument to address cases of active phase separation and of non-constant mobility, again finding $\\varsigma=4$. We further show that dynamical noise of variance $D$ creates a transient $\\varsigma = 2$ regime for $\\hat q\\ll \\hat{q}_\\ast \\sim \\sqrt{D} t^{[1-(d+2)\\nu]/2}$, crossing over to $\\varsigma = 4$ at larger $\\hat{q}$. Here, $\\nu$ is the coarsening exponent, with $\\ell\\sim t^\\nu$, and $\\hat{q} \\propto q \\ell$ is the rescaled wavenumber. In diffusive coarsening, $\\nu=1/3$, so the rescaled crossover wavevector $\\hat{q}_\\ast$ vanishes at large times when $d\\geq 2$. The slowness of this decay suggests a natural explanation for experiments that observe a long-lived $\\varsigma = 2$ scaling in phase-separating active fluids (where noise is typically large). Conversely, in $d=1$, we demonstrate that with noise the $\\varsigma = 2$ regime survives as $t\\to\\infty$, with $\\hat{q}_\\ast\\sim D^{5/6}$. (The structure factor is not then determined by the zero-temperature fixed point.) We confirm our analytical predictions by numerical simulations of active and passive continuum theories in the deterministic case and of Model B for the stochastic case. We also compare them with related findings for a system near an absorbing-state transition rather than undergoing phase separation. A central role is played throughout by the presence or absence of a conservation law for the centre of mass position of the order parameter field.","sentences":["Hyperuniformity emerges generically in the coarsening regime of phase-separating fluids.","Numerical studies of active and passive systems have shown that the structure factor $S(q)$ behaves as $q^\\varsigma$ for $q\\to 0$, with hyperuniformity exponent $\\varsigma = 4$. For passive systems, this result was explained in 1991 by a qualitative scaling analysis of Tomita, exploiting isotropy at scales much larger than the coarsening length $\\ell$. Here we reconsider and extend Tomita's argument to address cases of active phase separation and of non-constant mobility, again finding $\\varsigma=4$. We further show that dynamical noise of variance $D$ creates a transient $\\varsigma = 2$ regime for $\\hat q\\ll \\hat{q}_\\ast \\sim \\sqrt{D} t^{[1-(d+2)\\nu]/2}$, crossing over to $\\varsigma = 4$ at larger $\\hat{q}$.","Here, $\\nu$ is the coarsening exponent, with $\\ell\\sim t^\\nu$, and $\\hat{q} \\propto q \\ell$ is the rescaled wavenumber.","In diffusive coarsening, $\\nu=1/3$, so the rescaled crossover wavevector $\\hat{q}_\\ast$ vanishes at large times when $d\\geq 2$.","The slowness of this decay suggests a natural explanation for experiments that observe a long-lived $\\varsigma = 2$ scaling in phase-separating active fluids (where noise is typically large).","Conversely, in $d=1$, we demonstrate that with noise the $\\varsigma = 2$ regime survives as $t\\to\\infty$, with $\\hat{q}_\\ast\\sim D^{5/6}$. (The structure factor is not then determined by the zero-temperature fixed point.)","We confirm our analytical predictions by numerical simulations of active and passive continuum theories in the deterministic case and of Model B for the stochastic case.","We also compare them with related findings for a system near an absorbing-state transition rather than undergoing phase separation.","A central role is played throughout by the presence or absence of a conservation law for the centre of mass position of the order parameter field."],"url":"http://arxiv.org/abs/2405.00508v1","category":"cond-mat.soft"}
{"created":"2024-05-01 13:20:37","title":"Using non-DESI data to confirm and strengthen the DESI 2024 spatially-flat $w_0w_a$CDM cosmological parameterization result","abstract":"We use a combination of Planck cosmic microwave background (CMB) anisotropy data and non-CMB data that include Pantheon+ type Ia supernovae, Hubble parameter [$H(z)$], growth factor ($f\\sigma_8$) measurements, and a collection of baryon acoustic oscillation (BAO) data, but not recent DESI 2024 BAO measurements, to confirm the DESI 2024 (DESI+CMB+PantheonPlus) data compilation support for dynamical dark energy with an evolving equation of state parameter $w(z) = w_0 + w_a z/(1+z)$. From our joint compilation of CMB and non-CMB data, in a spatially-flat cosmological model, we obtain $w_0 = -0.850 \\pm 0.059$ and $w_a = -0.59^{+0.26}_{-0.22}$ and find that this dynamical dark energy is favored over a cosmological constant by $\\sim 2\\sigma$. Our data constraints on the flat $w_0w_a$CDM model are slightly more restrictive than the DESI 2024 constraints, with the DESI 2024 and our values of $w_0$ and $w_a$ differing by $-0.27\\sigma$ and $0.44\\sigma$, respectively. Our data compilation slightly more strongly favors the flat $w_0w_a$CDM model over the flat $\\Lambda$CDM model than does the DESI 2024 data compilation.","sentences":["We use a combination of Planck cosmic microwave background (CMB) anisotropy data and non-CMB data that include Pantheon+ type Ia supernovae, Hubble parameter [$H(z)$], growth factor ($f\\sigma_8$) measurements, and a collection of baryon acoustic oscillation (BAO) data, but not recent DESI 2024 BAO measurements, to confirm the DESI 2024 (DESI+CMB+PantheonPlus) data compilation support for dynamical dark energy with an evolving equation of state parameter $w(z) = w_0 + w_a","z/(1+z)$. From our joint compilation of CMB and non-CMB data, in a spatially-flat cosmological model, we obtain $w_0 = -0.850 \\pm 0.059$ and $w_a = -0.59^{+0.26}_{-0.22}$ and find that this dynamical dark energy is favored over a cosmological constant by $\\sim 2\\sigma$. Our data constraints on the flat $w_0w_a$CDM model are slightly more restrictive than the DESI 2024 constraints, with the DESI 2024 and our values of $w_0$ and $w_a$ differing by $-0.27\\sigma$ and $0.44\\sigma$, respectively.","Our data compilation slightly more strongly favors the flat $w_0w_a$CDM model over the flat $\\Lambda$CDM model than does the DESI 2024 data compilation."],"url":"http://arxiv.org/abs/2405.00502v1","category":"astro-ph.CO"}
{"created":"2024-05-01 13:10:42","title":"The Natural Display Topos of Coalgebras","abstract":"A classical result of topos theory holds that the category of coalgebras for a Cartesian comonad on a topos is again a topos (Kock and Wraith, 1971).   It is natural to refine this result to a topos-theoretic setting that includes universes. To this end, we introduce the notions of natural display topos and natural Cartesian display comonad, and show that the natural model of coalgebras for a natural Cartesian display comonad on a natural display topos is again a natural display topos. As an application, this result extends the approach to universes of Hofmann and Streicher (1997) from presheaf toposes to sheaf toposes with enough points.   Whereas natural display toposes provide a categorical semantics for a form of extensional Martin-L\\\"of type theory, we also prove our main result in the more general setting of natural typoses, which encompasses models of intensional Martin-L\\\"of type theory.   A natural Cartesian display comonad on a natural typos may also be used as a model for dependent type theory with an S4 box operator, or comonadic modality, as introduced by Nanevski et al. (2008). Modal contexts, which have been regarded as tricky to handle semantically, are interpreted as contexts of the natural typos of coalgebras. We sketch an interpretation within this approach.   As part of the framework in which the above takes place, we introduce a refinement of the notion of natural model (see Awodey, 2018), which is (strictly 2-)equivalent to the notion of full, split comprehension category (see Jacobs, 1993), rather than the notion of category with attributes (Cartmell 1978).","sentences":["A classical result of topos theory holds that the category of coalgebras for a Cartesian comonad on a topos is again a topos (Kock and Wraith, 1971).   ","It is natural to refine this result to a topos-theoretic setting that includes universes.","To this end, we introduce the notions of natural display topos and natural Cartesian display comonad, and show that the natural model of coalgebras for a natural Cartesian display comonad on a natural display topos is again a natural display topos.","As an application, this result extends the approach to universes of Hofmann and Streicher (1997) from presheaf toposes to sheaf toposes with enough points.   ","Whereas natural display toposes provide a categorical semantics for a form of extensional Martin-L\\\"of type theory, we also prove our main result in the more general setting of natural typoses, which encompasses models of intensional Martin-L\\\"of type theory.   ","A natural Cartesian display comonad on a natural typos may also be used as a model for dependent type theory with an S4 box operator, or comonadic modality, as introduced by Nanevski et al. (2008).","Modal contexts, which have been regarded as tricky to handle semantically, are interpreted as contexts of the natural typos of coalgebras.","We sketch an interpretation within this approach.   ","As part of the framework in which the above takes place, we introduce a refinement of the notion of natural model (see Awodey, 2018), which is (strictly 2-)equivalent to the notion of full, split comprehension category (see Jacobs, 1993), rather than the notion of category with attributes (Cartmell 1978)."],"url":"http://arxiv.org/abs/2405.00498v1","category":"math.CT"}
{"created":"2024-05-01 13:07:09","title":"Variational inequalities for the Ornstein--Uhlenbeck semigroup: the higher--dimensional case","abstract":"We study the $\\varrho$-th order variation seminorm of a general Ornstein--Uhlenbeck semigroup $\\left(\\mathcal H_t\\right)_{t>0}$ in $\\mathbb R^n$, taken with respect to $t$. We prove that this seminorm defines an operator of weak type $(1,1)$ with respect to the invariant measure when $\\varrho> 2$. For large $t$, one has an enhanced version of the standard weak-type $(1,1)$ bound. For small $t$, the proof hinges on vector-valued Calder\\'on--Zygmund techniques in the local region, and on the fact that the $t$ derivative of the integral kernel of $\\mathcal H_t$ in the global region has a bounded number of zeros in $(0,1]$. A counterexample is given for $\\varrho= 2$; in fact, we prove that the second order variation seminorm of $\\left(\\mathcal H_t\\right)_{t>0}$, and therefore also the $\\varrho$-th order variation seminorm for any $\\varrho\\in [1,2)$, is not of strong nor weak type $(p,p)$ for any $p \\in [1,\\infty)$ with respect to the invariant measure.","sentences":["We study the $\\varrho$-th order variation seminorm of a general Ornstein--Uhlenbeck semigroup $\\left(\\mathcal H_t\\right)_{t>0}$ in $\\mathbb R^n$, taken with respect to $t$. We prove that this seminorm defines an operator of weak type $(1,1)$ with respect to the invariant measure when $\\varrho> 2$. For large $t$, one has an enhanced version of the standard weak-type $(1,1)$ bound.","For small $t$, the proof hinges on vector-valued Calder\\'on--Zygmund techniques in the local region, and on the fact that the $t$ derivative of the integral kernel of $\\mathcal H_t$ in the global region has a bounded number of zeros in $(0,1]$. A counterexample is given for $\\varrho= 2$; in fact, we prove that the second order variation seminorm of $\\left(\\mathcal H_t\\right)_{t>0}$, and therefore also the $\\varrho$-th order variation seminorm for any $\\varrho\\in [1,2)$, is not of strong nor weak type $(p,p)$ for any $p \\in [1,\\infty)$ with respect to the invariant measure."],"url":"http://arxiv.org/abs/2405.00497v1","category":"math.FA"}
{"created":"2024-05-01 13:03:56","title":"The Loewner framework for parametric systems: Taming the curse of dimensionality","abstract":"The Loewner framework is an interpolatory framework for the approximation of linear and nonlinear systems. The purpose here is to extend this framework to linear parametric systems with an arbitrary number n of parameters. One main innovation established here is the construction of data-based realizations for any number of parameters. Equally importantly, we show how to alleviate the computational burden, by avoiding the explicit construction of large-scale n-dimensional Loewner matrices of size $N \\times N$. This reduces the complexity from $O(N^3)$ to about $O(N^{1.4})$, thus taming the curse of dimensionality and making the solution scalable to very large data sets. To achieve this, a new generalized multivariate rational function realization is defined. Then, we introduce the n-dimensional multivariate Loewner matrices and show that they can be computed by solving a coupled set of Sylvester equations. The null space of these Loewner matrices then allows the construction of the multivariate barycentric transfer function. The principal result of this work is to show how the null space of the n-dimensional Loewner matrix can be computed using a sequence of 1-dimensional Loewner matrices, leading to a drastic computational burden reduction. Finally, we suggest two algorithms (one direct and one iterative) to construct, directly from data, multivariate (or parametric) realizations ensuring (approximate) interpolation. Numerical examples highlight the effectiveness and scalability of the method.","sentences":["The Loewner framework is an interpolatory framework for the approximation of linear and nonlinear systems.","The purpose here is to extend this framework to linear parametric systems with an arbitrary number n of parameters.","One main innovation established here is the construction of data-based realizations for any number of parameters.","Equally importantly, we show how to alleviate the computational burden, by avoiding the explicit construction of large-scale n-dimensional Loewner matrices of size $N \\times N$. This reduces the complexity from $O(N^3)$ to about $O(N^{1.4})$, thus taming the curse of dimensionality and making the solution scalable to very large data sets.","To achieve this, a new generalized multivariate rational function realization is defined.","Then, we introduce the n-dimensional multivariate Loewner matrices and show that they can be computed by solving a coupled set of Sylvester equations.","The null space of these Loewner matrices then allows the construction of the multivariate barycentric transfer function.","The principal result of this work is to show how the null space of the n-dimensional Loewner matrix can be computed using a sequence of 1-dimensional Loewner matrices, leading to a drastic computational burden reduction.","Finally, we suggest two algorithms (one direct and one iterative) to construct, directly from data, multivariate (or parametric) realizations ensuring (approximate) interpolation.","Numerical examples highlight the effectiveness and scalability of the method."],"url":"http://arxiv.org/abs/2405.00495v1","category":"math.NA"}
{"created":"2024-05-01 13:00:51","title":"GOLD: Geometry Problem Solver with Natural Language Description","abstract":"Addressing the challenge of automated geometry math problem-solving in artificial intelligence (AI) involves understanding multi-modal information and mathematics. Current methods struggle with accurately interpreting geometry diagrams, which hinders effective problem-solving. To tackle this issue, we present the Geometry problem sOlver with natural Language Description (GOLD) model. GOLD enhances the extraction of geometric relations by separately processing symbols and geometric primitives within the diagram. Subsequently, it converts the extracted relations into natural language descriptions, efficiently utilizing large language models to solve geometry math problems. Experiments show that the GOLD model outperforms the Geoformer model, the previous best method on the UniGeo dataset, by achieving accuracy improvements of 12.7% and 42.1% in calculation and proving subsets. Additionally, it surpasses the former best model on the PGPS9K and Geometry3K datasets, PGPSNet, by obtaining accuracy enhancements of 1.8% and 3.2%, respectively.","sentences":["Addressing the challenge of automated geometry math problem-solving in artificial intelligence (AI) involves understanding multi-modal information and mathematics.","Current methods struggle with accurately interpreting geometry diagrams, which hinders effective problem-solving.","To tackle this issue, we present the Geometry problem sOlver with natural Language Description (GOLD) model.","GOLD enhances the extraction of geometric relations by separately processing symbols and geometric primitives within the diagram.","Subsequently, it converts the extracted relations into natural language descriptions, efficiently utilizing large language models to solve geometry math problems.","Experiments show that the GOLD model outperforms the Geoformer model, the previous best method on the UniGeo dataset, by achieving accuracy improvements of 12.7% and 42.1% in calculation and proving subsets.","Additionally, it surpasses the former best model on the PGPS9K and Geometry3K datasets, PGPSNet, by obtaining accuracy enhancements of 1.8% and 3.2%, respectively."],"url":"http://arxiv.org/abs/2405.00494v1","category":"cs.AI"}
{"created":"2024-05-01 12:59:37","title":"Is Temperature the Creativity Parameter of Large Language Models?","abstract":"Large language models (LLMs) are applied to all sorts of creative tasks, and their outputs vary from beautiful, to peculiar, to pastiche, into plain plagiarism. The temperature parameter of an LLM regulates the amount of randomness, leading to more diverse outputs; therefore, it is often claimed to be the creativity parameter. Here, we investigate this claim using a narrative generation task with a predetermined fixed context, model and prompt. Specifically, we present an empirical analysis of the LLM output for different temperature values using four necessary conditions for creativity in narrative generation: novelty, typicality, cohesion, and coherence. We find that temperature is weakly correlated with novelty, and unsurprisingly, moderately correlated with incoherence, but there is no relationship with either cohesion or typicality. However, the influence of temperature on creativity is far more nuanced and weak than suggested by the \"creativity parameter\" claim; overall results suggest that the LLM generates slightly more novel outputs as temperatures get higher. Finally, we discuss ideas to allow more controlled LLM creativity, rather than relying on chance via changing the temperature parameter.","sentences":["Large language models (LLMs) are applied to all sorts of creative tasks, and their outputs vary from beautiful, to peculiar, to pastiche, into plain plagiarism.","The temperature parameter of an LLM regulates the amount of randomness, leading to more diverse outputs; therefore, it is often claimed to be the creativity parameter.","Here, we investigate this claim using a narrative generation task with a predetermined fixed context, model and prompt.","Specifically, we present an empirical analysis of the LLM output for different temperature values using four necessary conditions for creativity in narrative generation: novelty, typicality, cohesion, and coherence.","We find that temperature is weakly correlated with novelty, and unsurprisingly, moderately correlated with incoherence, but there is no relationship with either cohesion or typicality.","However, the influence of temperature on creativity is far more nuanced and weak than suggested by the \"creativity parameter\" claim; overall results suggest that the LLM generates slightly more novel outputs as temperatures get higher.","Finally, we discuss ideas to allow more controlled LLM creativity, rather than relying on chance via changing the temperature parameter."],"url":"http://arxiv.org/abs/2405.00492v1","category":"cs.CL"}
{"created":"2024-05-01 12:57:14","title":"On the Relevance of Byzantine Robust Optimization Against Data Poisoning","abstract":"The success of machine learning (ML) has been intimately linked with the availability of large amounts of data, typically collected from heterogeneous sources and processed on vast networks of computing devices (also called {\\em workers}). Beyond accuracy, the use of ML in critical domains such as healthcare and autonomous driving calls for robustness against {\\em data poisoning}and some {\\em faulty workers}. The problem of {\\em Byzantine ML} formalizes these robustness issues by considering a distributed ML environment in which workers (storing a portion of the global dataset) can deviate arbitrarily from the prescribed algorithm. Although the problem has attracted a lot of attention from a theoretical point of view, its practical importance for addressing realistic faults (where the behavior of any worker is locally constrained) remains unclear. It has been argued that the seemingly weaker threat model where only workers' local datasets get poisoned is more reasonable. We prove that, while tolerating a wider range of faulty behaviors, Byzantine ML yields solutions that are, in a precise sense, optimal even under the weaker data poisoning threat model. Then, we study a generic data poisoning model wherein some workers have {\\em fully-poisonous local data}, i.e., their datasets are entirely corruptible, and the remainders have {\\em partially-poisonous local data}, i.e., only a fraction of their local datasets is corruptible. We prove that Byzantine-robust schemes yield optimal solutions against both these forms of data poisoning, and that the former is more harmful when workers have {\\em heterogeneous} local data.","sentences":["The success of machine learning (ML) has been intimately linked with the availability of large amounts of data, typically collected from heterogeneous sources and processed on vast networks of computing devices (also called {\\em workers}).","Beyond accuracy, the use of ML in critical domains such as healthcare and autonomous driving calls for robustness against {\\em data poisoning}and some {\\em faulty workers}.","The problem of {\\em Byzantine ML} formalizes these robustness issues by considering a distributed ML environment in which workers (storing a portion of the global dataset) can deviate arbitrarily from the prescribed algorithm.","Although the problem has attracted a lot of attention from a theoretical point of view, its practical importance for addressing realistic faults (where the behavior of any worker is locally constrained) remains unclear.","It has been argued that the seemingly weaker threat model where only workers' local datasets get poisoned is more reasonable.","We prove that, while tolerating a wider range of faulty behaviors, Byzantine ML yields solutions that are, in a precise sense, optimal even under the weaker data poisoning threat model.","Then, we study a generic data poisoning model wherein some workers have {\\em fully-poisonous local data}, i.e., their datasets are entirely corruptible, and the remainders have {\\em partially-poisonous local data}, i.e., only a fraction of their local datasets is corruptible.","We prove that Byzantine-robust schemes yield optimal solutions against both these forms of data poisoning, and that the former is more harmful when workers have {\\em heterogeneous} local data."],"url":"http://arxiv.org/abs/2405.00491v1","category":"cs.LG"}
{"created":"2024-05-01 12:57:08","title":"A first-order deconfinement phase transition in the early universe and gravitational waves","abstract":"We clarify the conditions of the cosmic quantum chromodynamics (QCD) first-order phase transition in the early universe by carefully distinguishing the chiral and deconfinement phase transitions. While the chiral one with light quarks at zero chemical potential is unlikely to be first order based on the recent lattice QCD calculations, the latter one can be naturally extended with one extra rolling scalar to be first order. The argument is also valid for the dark QCD theory with arbitrary $N_c$ with a wide range of phase transition temperatures, which can be from hundreds of MeV up to beyond TeV. Notably, here we derive the general formula for the deconfinement phase transition potential of SU($N_c$) gauge theory characterized by the Polyakov loop. With the effective potential in hand, the gravitational wave spectrum is then determined via the sound shell model, which then enables us to give for the first time the quantitative analysis of the gravitational wave signals coming from the QCD deconfinement phase transition and awaits the check from future space interferometers.","sentences":["We clarify the conditions of the cosmic quantum chromodynamics (QCD) first-order phase transition in the early universe by carefully distinguishing the chiral and deconfinement phase transitions.","While the chiral one with light quarks at zero chemical potential is unlikely to be first order based on the recent lattice QCD calculations, the latter one can be naturally extended with one extra rolling scalar to be first order.","The argument is also valid for the dark QCD theory with arbitrary $N_c$ with a wide range of phase transition temperatures, which can be from hundreds of MeV up to beyond TeV. Notably, here we derive the general formula for the deconfinement phase transition potential of SU($N_c$) gauge theory characterized by the Polyakov loop.","With the effective potential in hand, the gravitational wave spectrum is then determined via the sound shell model, which then enables us to give for the first time the quantitative analysis of the gravitational wave signals coming from the QCD deconfinement phase transition and awaits the check from future space interferometers."],"url":"http://arxiv.org/abs/2405.00490v1","category":"hep-ph"}
{"created":"2024-05-01 12:56:14","title":"Explainable Automatic Grading with Neural Additive Models","abstract":"The use of automatic short answer grading (ASAG) models may help alleviate the time burden of grading while encouraging educators to frequently incorporate open-ended items in their curriculum. However, current state-of-the-art ASAG models are large neural networks (NN) often described as \"black box\", providing no explanation for which characteristics of an input are important for the produced output. This inexplicable nature can be frustrating to teachers and students when trying to interpret, or learn from an automatically-generated grade. To create a powerful yet intelligible ASAG model, we experiment with a type of model called a Neural Additive Model that combines the performance of a NN with the explainability of an additive model. We use a Knowledge Integration (KI) framework from the learning sciences to guide feature engineering to create inputs that reflect whether a student includes certain ideas in their response. We hypothesize that indicating the inclusion (or exclusion) of predefined ideas as features will be sufficient for the NAM to have good predictive power and interpretability, as this may guide a human scorer using a KI rubric. We compare the performance of the NAM with another explainable model, logistic regression, using the same features, and to a non-explainable neural model, DeBERTa, that does not require feature engineering.","sentences":["The use of automatic short answer grading (ASAG) models may help alleviate the time burden of grading while encouraging educators to frequently incorporate open-ended items in their curriculum.","However, current state-of-the-art ASAG models are large neural networks (NN) often described as \"black box\", providing no explanation for which characteristics of an input are important for the produced output.","This inexplicable nature can be frustrating to teachers and students when trying to interpret, or learn from an automatically-generated grade.","To create a powerful yet intelligible ASAG model, we experiment with a type of model called a Neural Additive Model that combines the performance of a NN with the explainability of an additive model.","We use a Knowledge Integration (KI) framework from the learning sciences to guide feature engineering to create inputs that reflect whether a student includes certain ideas in their response.","We hypothesize that indicating the inclusion (or exclusion) of predefined ideas as features will be sufficient for the NAM to have good predictive power and interpretability, as this may guide a human scorer using a KI rubric.","We compare the performance of the NAM with another explainable model, logistic regression, using the same features, and to a non-explainable neural model, DeBERTa, that does not require feature engineering."],"url":"http://arxiv.org/abs/2405.00489v1","category":"cs.LG"}
{"created":"2024-05-01 12:54:47","title":"Colax adjunctions and lax-idempotent pseudomonads","abstract":"We prove a generalization of a theorem of Bunge and Gray about forming colax adjunctions out of relative Kan extensions and apply it to the study of the Kleisli 2-category for a lax-idempotent pseudomonad. For instance, we establish the weak completeness of the Kleisli 2-category and describe colax change-of-base adjunctions between Kleisli 2-categories. Our approach covers such examples as the bicategory of small profunctors and the 2-category of lax triangles in a 2-category. The duals of our results provide lax analogues of classical results in two-dimensional monad theory: for instance, establishing the weak cocompleteness of the 2-category of strict algebras and lax morphisms and the existence of colax change-of-base adjunctions.","sentences":["We prove a generalization of a theorem of Bunge and Gray about forming colax adjunctions out of relative Kan extensions and apply it to the study of the Kleisli 2-category for a lax-idempotent pseudomonad.","For instance, we establish the weak completeness of the Kleisli 2-category and describe colax change-of-base adjunctions between Kleisli 2-categories.","Our approach covers such examples as the bicategory of small profunctors and the 2-category of lax triangles in a 2-category.","The duals of our results provide lax analogues of classical results in two-dimensional monad theory: for instance, establishing the weak cocompleteness of the 2-category of strict algebras and lax morphisms and the existence of colax change-of-base adjunctions."],"url":"http://arxiv.org/abs/2405.00488v1","category":"math.CT"}
{"created":"2024-05-01 12:49:57","title":"The Pyramid of Captions","abstract":"We introduce a formal information-theoretic framework for image captioning by regarding it as a representation learning task. Our framework defines three key objectives: task sufficiency, minimal redundancy, and human interpretability. Building upon this foundation, we propose a novel Pyramid of Captions (PoCa) method, which constructs caption pyramids by generating localized captions for zoomed-in image patches and integrating them with global caption information using large language models. This approach leverages intuition that the detailed examination of local patches can reduce error risks and address inaccuracies in global captions, either by correcting the hallucination or adding missing details. Based on our theoretical framework, we formalize this intuition and provide formal proof demonstrating the effectiveness of PoCa under certain assumptions. Empirical tests with various image captioning models and large language models show that PoCa consistently yields more informative and semantically aligned captions, maintaining brevity and interpretability.","sentences":["We introduce a formal information-theoretic framework for image captioning by regarding it as a representation learning task.","Our framework defines three key objectives: task sufficiency, minimal redundancy, and human interpretability.","Building upon this foundation, we propose a novel Pyramid of Captions (PoCa) method, which constructs caption pyramids by generating localized captions for zoomed-in image patches and integrating them with global caption information using large language models.","This approach leverages intuition that the detailed examination of local patches can reduce error risks and address inaccuracies in global captions, either by correcting the hallucination or adding missing details.","Based on our theoretical framework, we formalize this intuition and provide formal proof demonstrating the effectiveness of PoCa under certain assumptions.","Empirical tests with various image captioning models and large language models show that PoCa consistently yields more informative and semantically aligned captions, maintaining brevity and interpretability."],"url":"http://arxiv.org/abs/2405.00485v1","category":"cs.CV"}
{"created":"2024-05-01 12:49:38","title":"Spin Hamiltonians in the Modulated Momenta of Light","abstract":"Photonic solvers that are able to find the ground states of different spin Hamiltonians can be used to study many interactive physical systems and combinatorial optimization problems. Here, we establish a real-and-momentum space correspondence of spin Hamiltonians by spatial light transport. The real-space spin interaction is determined by modulating the momentum-space flow of light. This principle is formulated as a generalized Plancherel theorem, allowing us to implement a simple optical simulator that can find the ground states for any displacement-dependent spin interactions. Particularly, we use this principle to reveal the exotic magnetic phase diagram from a J1-J2-J3 model, and we also observe the vortex-mediated Berezinskii-Kosterlitz-Thouless dynamics from the XY model. These experiments exhibit high calculation precision by subtly controlling spin interactions from the momentum space of light, offering a promising scheme to explore novel physical effects.","sentences":["Photonic solvers that are able to find the ground states of different spin Hamiltonians can be used to study many interactive physical systems and combinatorial optimization problems.","Here, we establish a real-and-momentum space correspondence of spin Hamiltonians by spatial light transport.","The real-space spin interaction is determined by modulating the momentum-space flow of light.","This principle is formulated as a generalized Plancherel theorem, allowing us to implement a simple optical simulator that can find the ground states for any displacement-dependent spin interactions.","Particularly, we use this principle to reveal the exotic magnetic phase diagram from a J1-J2-J3 model, and we also observe the vortex-mediated Berezinskii-Kosterlitz-Thouless dynamics from the XY model.","These experiments exhibit high calculation precision by subtly controlling spin interactions from the momentum space of light, offering a promising scheme to explore novel physical effects."],"url":"http://arxiv.org/abs/2405.00484v1","category":"physics.optics"}
{"created":"2024-05-01 12:48:13","title":"In Anticipation of Perfect Deepfake: Identity-anchored Artifact-agnostic Detection under Rebalanced Deepfake Detection Protocol","abstract":"As deep generative models advance, we anticipate deepfakes achieving \"perfection\"-generating no discernible artifacts or noise. However, current deepfake detectors, intentionally or inadvertently, rely on such artifacts for detection, as they are exclusive to deepfakes and absent in genuine examples. To bridge this gap, we introduce the Rebalanced Deepfake Detection Protocol (RDDP) to stress-test detectors under balanced scenarios where genuine and forged examples bear similar artifacts. We offer two RDDP variants: RDDP-WHITEHAT uses white-hat deepfake algorithms to create 'self-deepfakes,' genuine portrait videos with the resemblance of the underlying identity, yet carry similar artifacts to deepfake videos; RDDP-SURROGATE employs surrogate functions (e.g., Gaussian noise) to process both genuine and forged examples, introducing equivalent noise, thereby sidestepping the need of deepfake algorithms.   Towards detecting perfect deepfake videos that aligns with genuine ones, we present ID-Miner, a detector that identifies the puppeteer behind the disguise by focusing on motion over artifacts or appearances. As an identity-based detector, it authenticates videos by comparing them with reference footage. Equipped with the artifact-agnostic loss at frame-level and the identity-anchored loss at video-level, ID-Miner effectively singles out identity signals amidst distracting variations. Extensive experiments comparing ID-Miner with 12 baseline detectors under both conventional and RDDP evaluations with two deepfake datasets, along with additional qualitative studies, affirm the superiority of our method and the necessity for detectors designed to counter perfect deepfakes.","sentences":["As deep generative models advance, we anticipate deepfakes achieving \"perfection\"-generating no discernible artifacts or noise.","However, current deepfake detectors, intentionally or inadvertently, rely on such artifacts for detection, as they are exclusive to deepfakes and absent in genuine examples.","To bridge this gap, we introduce the Rebalanced Deepfake Detection Protocol (RDDP) to stress-test detectors under balanced scenarios where genuine and forged examples bear similar artifacts.","We offer two RDDP variants: RDDP-WHITEHAT uses white-hat deepfake algorithms to create 'self-deepfakes,' genuine portrait videos with the resemblance of the underlying identity, yet carry similar artifacts to deepfake videos; RDDP-SURROGATE employs surrogate functions (e.g., Gaussian noise) to process both genuine and forged examples, introducing equivalent noise, thereby sidestepping the need of deepfake algorithms.   ","Towards detecting perfect deepfake videos that aligns with genuine ones, we present ID-Miner, a detector that identifies the puppeteer behind the disguise by focusing on motion over artifacts or appearances.","As an identity-based detector, it authenticates videos by comparing them with reference footage.","Equipped with the artifact-agnostic loss at frame-level and the identity-anchored loss at video-level, ID-Miner effectively singles out identity signals amidst distracting variations.","Extensive experiments comparing ID-Miner with 12 baseline detectors under both conventional and RDDP evaluations with two deepfake datasets, along with additional qualitative studies, affirm the superiority of our method and the necessity for detectors designed to counter perfect deepfakes."],"url":"http://arxiv.org/abs/2405.00483v1","category":"cs.CV"}
{"created":"2024-05-01 12:39:41","title":"Better Bounded Bisimulation Contractions (Preprint)","abstract":"Bisimulations are standard in modal logic and, more generally, in the theory of state-transition systems. The quotient structure of a Kripke model with respect to the bisimulation relation is called a bisimulation contraction. The bisimulation contraction is a minimal model bisimilar to the original model, and hence, for (image-)finite models, a minimal model modally equivalent to the original. Similar definitions exist for bounded bisimulations ($k$-bisimulations) and bounded bisimulation contractions. Two finite models are $k$-bisimilar if and only if they are modally equivalent up to modal depth $k$. However, the quotient structure with respect to the $k$-bisimulation relation does not guarantee a minimal model preserving modal equivalence to depth $k$. In this paper, we remedy this asymmetry to standard bisimulations and provide a novel definition of bounded contractions called rooted $k$-contractions. We prove that rooted $k$-contractions preserve $k$-bisimilarity and are minimal with this property. Finally, we show that rooted $k$-contractions can be exponentially more succinct than standard $k$-contractions.","sentences":["Bisimulations are standard in modal logic and, more generally, in the theory of state-transition systems.","The quotient structure of a Kripke model with respect to the bisimulation relation is called a bisimulation contraction.","The bisimulation contraction is a minimal model bisimilar to the original model, and hence, for (image-)finite models, a minimal model modally equivalent to the original.","Similar definitions exist for bounded bisimulations ($k$-bisimulations) and bounded bisimulation contractions.","Two finite models are $k$-bisimilar if and only if they are modally equivalent up to modal depth $k$.","However, the quotient structure with respect to the $k$-bisimulation relation does not guarantee a minimal model preserving modal equivalence to depth $k$. In this paper, we remedy this asymmetry to standard bisimulations and provide a novel definition of bounded contractions called rooted $k$-contractions.","We prove that rooted $k$-contractions preserve $k$-bisimilarity and are minimal with this property.","Finally, we show that rooted $k$-contractions can be exponentially more succinct than standard $k$-contractions."],"url":"http://arxiv.org/abs/2405.00480v1","category":"cs.LO"}
{"created":"2024-05-01 12:13:54","title":"Valuations, bijections, and bases","abstract":"The aim of this paper is to build a theory of commutative and noncommutative injective valuations of various algebras. The targets of our valuations are (well-)ordered commutative and noncommutative (partial or entire) semigroups including any sub-semigroups of the free monoid $F_n$ on $n$ generators and various quotients. In the case when the (partial) valuation semigroup is finitely generated, we construct a generalization of the standard monomial bases for the so-valued algebra, which seems to be new in noncommutative case. Quite remarkably, for any pair of well-ordered valuations one has canonical bijections between the valuation semigroups, which serve as analogs of the celebrated Jordan-H\\\"older correspondences and these bijections are \"almost\" homomorphisms of the involved (partial and entire) semigroups.","sentences":["The aim of this paper is to build a theory of commutative and noncommutative injective valuations of various algebras.","The targets of our valuations are (well-)ordered commutative and noncommutative (partial or entire) semigroups including any sub-semigroups of the free monoid $F_n$ on $n$ generators and various quotients.","In the case when the (partial) valuation semigroup is finitely generated, we construct a generalization of the standard monomial bases for the so-valued algebra, which seems to be new in noncommutative case.","Quite remarkably, for any pair of well-ordered valuations one has canonical bijections between the valuation semigroups, which serve as analogs of the celebrated Jordan-H\\\"older correspondences and these bijections are \"almost\" homomorphisms of the involved (partial and entire) semigroups."],"url":"http://arxiv.org/abs/2405.00470v1","category":"math.RA"}
{"created":"2024-05-01 12:12:59","title":"Exploiting Positional Bias for Query-Agnostic Generative Content in Search","abstract":"In recent years, neural ranking models (NRMs) have been shown to substantially outperform their lexical counterparts in text retrieval. In traditional search pipelines, a combination of features leads to well-defined behaviour. However, as neural approaches become increasingly prevalent as the final scoring component of engines or as standalone systems, their robustness to malicious text and, more generally, semantic perturbation needs to be better understood. We posit that the transformer attention mechanism can induce exploitable defects through positional bias in search models, leading to an attack that could generalise beyond a single query or topic. We demonstrate such defects by showing that non-relevant text--such as promotional content--can be easily injected into a document without adversely affecting its position in search results. Unlike previous gradient-based attacks, we demonstrate these biases in a query-agnostic fashion. In doing so, without the knowledge of topicality, we can still reduce the negative effects of non-relevant content injection by controlling injection position. Our experiments are conducted with simulated on-topic promotional text automatically generated by prompting LLMs with topical context from target documents. We find that contextualisation of a non-relevant text further reduces negative effects whilst likely circumventing existing content filtering mechanisms. In contrast, lexical models are found to be more resilient to such content injection attacks. We then investigate a simple yet effective compensation for the weaknesses of the NRMs in search, validating our hypotheses regarding transformer bias.","sentences":["In recent years, neural ranking models (NRMs) have been shown to substantially outperform their lexical counterparts in text retrieval.","In traditional search pipelines, a combination of features leads to well-defined behaviour.","However, as neural approaches become increasingly prevalent as the final scoring component of engines or as standalone systems, their robustness to malicious text and, more generally, semantic perturbation needs to be better understood.","We posit that the transformer attention mechanism can induce exploitable defects through positional bias in search models, leading to an attack that could generalise beyond a single query or topic.","We demonstrate such defects by showing that non-relevant text--such as promotional content--can be easily injected into a document without adversely affecting its position in search results.","Unlike previous gradient-based attacks, we demonstrate these biases in a query-agnostic fashion.","In doing so, without the knowledge of topicality, we can still reduce the negative effects of non-relevant content injection by controlling injection position.","Our experiments are conducted with simulated on-topic promotional text automatically generated by prompting LLMs with topical context from target documents.","We find that contextualisation of a non-relevant text further reduces negative effects whilst likely circumventing existing content filtering mechanisms.","In contrast, lexical models are found to be more resilient to such content injection attacks.","We then investigate a simple yet effective compensation for the weaknesses of the NRMs in search, validating our hypotheses regarding transformer bias."],"url":"http://arxiv.org/abs/2405.00469v1","category":"cs.IR"}
{"created":"2024-05-01 12:08:38","title":"Feature-Aware Noise Contrastive Learning For Unsupervised Red Panda Re-Identification","abstract":"To facilitate the re-identification (Re-ID) of individual animals, existing methods primarily focus on maximizing feature similarity within the same individual and enhancing distinctiveness between different individuals. However, most of them still rely on supervised learning and require substantial labeled data, which is challenging to obtain. To avoid this issue, we propose a Feature-Aware Noise Contrastive Learning (FANCL) method to explore an unsupervised learning solution, which is then validated on the task of red panda re-ID. FANCL employs a Feature-Aware Noise Addition module to produce noised images that conceal critical features and designs two contrastive learning modules to calculate the losses. Firstly, a feature consistency module is designed to bridge the gap between the original and noised features. Secondly, the neural networks are trained through a cluster contrastive learning module. Through these more challenging learning tasks, FANCL can adaptively extract deeper representations of red pandas. The experimental results on a set of red panda images collected in both indoor and outdoor environments prove that FANCL outperforms several related state-of-the-art unsupervised methods, achieving high performance comparable to supervised learning methods.","sentences":["To facilitate the re-identification (Re-ID) of individual animals, existing methods primarily focus on maximizing feature similarity within the same individual and enhancing distinctiveness between different individuals.","However, most of them still rely on supervised learning and require substantial labeled data, which is challenging to obtain.","To avoid this issue, we propose a Feature-Aware Noise Contrastive Learning (FANCL) method to explore an unsupervised learning solution, which is then validated on the task of red panda re-ID.","FANCL employs a Feature-Aware Noise Addition module to produce noised images that conceal critical features and designs two contrastive learning modules to calculate the losses.","Firstly, a feature consistency module is designed to bridge the gap between the original and noised features.","Secondly, the neural networks are trained through a cluster contrastive learning module.","Through these more challenging learning tasks, FANCL can adaptively extract deeper representations of red pandas.","The experimental results on a set of red panda images collected in both indoor and outdoor environments prove that FANCL outperforms several related state-of-the-art unsupervised methods, achieving high performance comparable to supervised learning methods."],"url":"http://arxiv.org/abs/2405.00468v1","category":"cs.CV"}
{"created":"2024-05-01 12:03:39","title":"Lazy Layers to Make Fine-Tuned Diffusion Models More Traceable","abstract":"Foundational generative models should be traceable to protect their owners and facilitate safety regulation. To achieve this, traditional approaches embed identifiers based on supervisory trigger-response signals, which are commonly known as backdoor watermarks. They are prone to failure when the model is fine-tuned with nontrigger data. Our experiments show that this vulnerability is due to energetic changes in only a few 'busy' layers during fine-tuning. This yields a novel arbitrary-in-arbitrary-out (AIAO) strategy that makes watermarks resilient to fine-tuning-based removal. The trigger-response pairs of AIAO samples across various neural network depths can be used to construct watermarked subpaths, employing Monte Carlo sampling to achieve stable verification results. In addition, unlike the existing methods of designing a backdoor for the input/output space of diffusion models, in our method, we propose to embed the backdoor into the feature space of sampled subpaths, where a mask-controlled trigger function is proposed to preserve the generation performance and ensure the invisibility of the embedded backdoor. Our empirical studies on the MS-COCO, AFHQ, LSUN, CUB-200, and DreamBooth datasets confirm the robustness of AIAO; while the verification rates of other trigger-based methods fall from ~90% to ~70% after fine-tuning, those of our method remain consistently above 90%.","sentences":["Foundational generative models should be traceable to protect their owners and facilitate safety regulation.","To achieve this, traditional approaches embed identifiers based on supervisory trigger-response signals, which are commonly known as backdoor watermarks.","They are prone to failure when the model is fine-tuned with nontrigger data.","Our experiments show that this vulnerability is due to energetic changes in only a few 'busy' layers during fine-tuning.","This yields a novel arbitrary-in-arbitrary-out (AIAO) strategy that makes watermarks resilient to fine-tuning-based removal.","The trigger-response pairs of AIAO samples across various neural network depths can be used to construct watermarked subpaths, employing Monte Carlo sampling to achieve stable verification results.","In addition, unlike the existing methods of designing a backdoor for the input/output space of diffusion models, in our method, we propose to embed the backdoor into the feature space of sampled subpaths, where a mask-controlled trigger function is proposed to preserve the generation performance and ensure the invisibility of the embedded backdoor.","Our empirical studies on the MS-COCO, AFHQ, LSUN, CUB-200, and DreamBooth datasets confirm the robustness of AIAO; while the verification rates of other trigger-based methods fall from ~90% to ~70% after fine-tuning, those of our method remain consistently above 90%."],"url":"http://arxiv.org/abs/2405.00466v1","category":"cs.CV"}
{"created":"2024-05-01 12:01:39","title":"BiomedRAG: A Retrieval Augmented Large Language Model for Biomedicine","abstract":"Large Language Models (LLMs) have swiftly emerged as vital resources for different applications in the biomedical and healthcare domains; however, these models encounter issues such as generating inaccurate information or hallucinations. Retrieval-augmented generation provided a solution for these models to update knowledge and enhance their performance. In contrast to previous retrieval-augmented LMs, which utilize specialized cross-attention mechanisms to help LLM encode retrieved text, BiomedRAG adopts a simpler approach by directly inputting the retrieved chunk-based documents into the LLM. This straightforward design is easily applicable to existing retrieval and language models, effectively bypassing noise information in retrieved documents, particularly in noise-intensive tasks. Moreover, we demonstrate the potential for utilizing the LLM to supervise the retrieval model in the biomedical domain, enabling it to retrieve the document that assists the LM in improving its predictions. Our experiments reveal that with the tuned scorer,\\textsc{ BiomedRAG} attains superior performance across 5 biomedical NLP tasks, encompassing information extraction (triple extraction, relation extraction), text classification, link prediction, and question-answering, leveraging over 9 datasets. For instance, in the triple extraction task, \\textsc{BiomedRAG} outperforms other triple extraction systems with micro-F1 scores of 81.42 and 88.83 on GIT and ChemProt corpora, respectively.","sentences":["Large Language Models (LLMs) have swiftly emerged as vital resources for different applications in the biomedical and healthcare domains; however, these models encounter issues such as generating inaccurate information or hallucinations.","Retrieval-augmented generation provided a solution for these models to update knowledge and enhance their performance.","In contrast to previous retrieval-augmented LMs, which utilize specialized cross-attention mechanisms to help LLM encode retrieved text, BiomedRAG adopts a simpler approach by directly inputting the retrieved chunk-based documents into the LLM.","This straightforward design is easily applicable to existing retrieval and language models, effectively bypassing noise information in retrieved documents, particularly in noise-intensive tasks.","Moreover, we demonstrate the potential for utilizing the LLM to supervise the retrieval model in the biomedical domain, enabling it to retrieve the document that assists the LM in improving its predictions.","Our experiments reveal that with the tuned scorer,\\textsc{ BiomedRAG} attains superior performance across 5 biomedical NLP tasks, encompassing information extraction (triple extraction, relation extraction), text classification, link prediction, and question-answering, leveraging over 9 datasets.","For instance, in the triple extraction task, \\textsc{BiomedRAG} outperforms other triple extraction systems with micro-F1 scores of 81.42 and 88.83 on GIT and ChemProt corpora, respectively."],"url":"http://arxiv.org/abs/2405.00465v1","category":"cs.CL"}
{"created":"2024-05-01 11:41:59","title":"Saturation Level of Ion Weibel Instability and Isotropization Length Scale in Electron-Ion Weibel-Mediated Shocks","abstract":"Ion Weibel instability is considered to be the dominant physics for the dissipation in high-Mach number astrophysical shocks such as supernova remnant shocks and gamma-ray burst shocks. We study the instability dependence on various parameters using theory and particle-in-cell simulations. We demonstrate that electron physics determines the saturation level of the Weibel-generated magnetic field, even though the instability is driven by the ions. We discuss the application to astrophysical and laboratory laser experiment environments to clarify the roles of the ion Weibel instability. We develop a model for the isotropization length scale in Weibel-mediated shocks and compare its value to other characteristic length scales of each system. We find that electron heating to near equipartition is crucial for the formation of ultra-relativistic Weibel-mediated shocks. On the other hand, our results imply that non-relativistic shocks in typical interstellar medium are not purely mediated by the Weibel instability.","sentences":["Ion Weibel instability is considered to be the dominant physics for the dissipation in high-Mach number astrophysical shocks such as supernova remnant shocks and gamma-ray burst shocks.","We study the instability dependence on various parameters using theory and particle-in-cell simulations.","We demonstrate that electron physics determines the saturation level of the Weibel-generated magnetic field, even though the instability is driven by the ions.","We discuss the application to astrophysical and laboratory laser experiment environments to clarify the roles of the ion Weibel instability.","We develop a model for the isotropization length scale in Weibel-mediated shocks and compare its value to other characteristic length scales of each system.","We find that electron heating to near equipartition is crucial for the formation of ultra-relativistic Weibel-mediated shocks.","On the other hand, our results imply that non-relativistic shocks in typical interstellar medium are not purely mediated by the Weibel instability."],"url":"http://arxiv.org/abs/2405.00462v1","category":"astro-ph.HE"}
{"created":"2024-05-01 11:39:38","title":"Enhancing Surgical Robots with Embodied Intelligence for Autonomous Ultrasound Scanning","abstract":"Ultrasound robots are increasingly used in medical diagnostics and early disease screening. However, current ultrasound robots lack the intelligence to understand human intentions and instructions, hindering autonomous ultrasound scanning. To solve this problem, we propose a novel Ultrasound Embodied Intelligence system that equips ultrasound robots with the large language model (LLM) and domain knowledge, thereby improving the efficiency of ultrasound robots. Specifically, we first design an ultrasound operation knowledge database to add expertise in ultrasound scanning to the LLM, enabling the LLM to perform precise motion planning. Furthermore, we devise a dynamic ultrasound scanning strategy based on a \\textit{think-observe-execute} prompt engineering, allowing LLMs to dynamically adjust motion planning strategies during the scanning procedures. Extensive experiments demonstrate that our system significantly improves ultrasound scan efficiency and quality from verbal commands. This advancement in autonomous medical scanning technology contributes to non-invasive diagnostics and streamlined medical workflows.","sentences":["Ultrasound robots are increasingly used in medical diagnostics and early disease screening.","However, current ultrasound robots lack the intelligence to understand human intentions and instructions, hindering autonomous ultrasound scanning.","To solve this problem, we propose a novel Ultrasound Embodied Intelligence system that equips ultrasound robots with the large language model (LLM) and domain knowledge, thereby improving the efficiency of ultrasound robots.","Specifically, we first design an ultrasound operation knowledge database to add expertise in ultrasound scanning to the LLM, enabling the LLM to perform precise motion planning.","Furthermore, we devise a dynamic ultrasound scanning strategy based on a \\textit{think-observe-execute} prompt engineering, allowing LLMs to dynamically adjust motion planning strategies during the scanning procedures.","Extensive experiments demonstrate that our system significantly improves ultrasound scan efficiency and quality from verbal commands.","This advancement in autonomous medical scanning technology contributes to non-invasive diagnostics and streamlined medical workflows."],"url":"http://arxiv.org/abs/2405.00461v1","category":"cs.RO"}
{"created":"2024-05-01 11:39:02","title":"Metallic local-moment magnetocalorics as a route to cryogenic refrigeration","abstract":"Commercial adiabatic demagnetisation refrigerators still employ the same hydrated salts that were first introduced over 85 years ago. The inherent limitations of these insulating magnetocalorics - poor thermal conductivity at sub-Kelvin temperatures, low entropy density, corrosiveness - can be overcome by a new generation of rare-earth based metallic magnetocalorics. Here, we present the metallic magnetocaloric YbNi1.6 Sn as an attractive alternative to conventional refrigerants. YbNi1.6Sn retains high entropy into the 100 mK regime and avoids the noble metal constituents of alternative refrigerants. Demagnetisation tests demonstrate that YbNi1.6Sn enables economical and durable alternatives to traditional cooling devices for temperatures reaching below 120 mK. We find that the magnetocaloric properties of this material are facilitated by unusually small Kondo and RKKY interactions, which position YbNi1.6Sn in the extreme local moment limit on the generalised Kondo lattice phase diagram.","sentences":["Commercial adiabatic demagnetisation refrigerators still employ the same hydrated salts that were first introduced over 85 years ago.","The inherent limitations of these insulating magnetocalorics - poor thermal conductivity at sub-Kelvin temperatures, low entropy density, corrosiveness - can be overcome by a new generation of rare-earth based metallic magnetocalorics.","Here, we present the metallic magnetocaloric YbNi1.6 Sn as an attractive alternative to conventional refrigerants.","YbNi1.6Sn retains high entropy into the 100 mK regime and avoids the noble metal constituents of alternative refrigerants.","Demagnetisation tests demonstrate that YbNi1.6Sn enables economical and durable alternatives to traditional cooling devices for temperatures reaching below 120 mK. We find that the magnetocaloric properties of this material are facilitated by unusually small Kondo and RKKY interactions, which position YbNi1.6Sn in the extreme local moment limit on the generalised Kondo lattice phase diagram."],"url":"http://arxiv.org/abs/2405.00460v1","category":"cond-mat.str-el"}
{"created":"2024-05-01 11:33:14","title":"Bulk-Boundary Correspondence in Ergodic and Nonergodic One-Dimensional Stochastic Processes","abstract":"Bulk-boundary correspondence is a fundamental principle in topological physics. In recent years, there have been considerable efforts in extending the idea of geometry and topology to classical stochastic systems far from equilibrium. However, it has been unknown whether or not the bulk-boundary correspondence can be extended to the steady states of stochastic processes accompanied by additional constraints such as the conservation of probability. The present study reveals the general form of bulk-boundary correspondence in classical stochastic processes. Specifically, we prove a correspondence between the winding number and the number of localized steady states in both ergodic and nonergodic systems. Furthermore, we extend the argument of the bulk-boundary correspondence to a many-body stochastic model called the asymmetric simple exclusion process (ASEP). These results would provide a guiding principle for exploring topological origin of localization in various stochastic processes including biological systems.","sentences":["Bulk-boundary correspondence is a fundamental principle in topological physics.","In recent years, there have been considerable efforts in extending the idea of geometry and topology to classical stochastic systems far from equilibrium.","However, it has been unknown whether or not the bulk-boundary correspondence can be extended to the steady states of stochastic processes accompanied by additional constraints such as the conservation of probability.","The present study reveals the general form of bulk-boundary correspondence in classical stochastic processes.","Specifically, we prove a correspondence between the winding number and the number of localized steady states in both ergodic and nonergodic systems.","Furthermore, we extend the argument of the bulk-boundary correspondence to a many-body stochastic model called the asymmetric simple exclusion process (ASEP).","These results would provide a guiding principle for exploring topological origin of localization in various stochastic processes including biological systems."],"url":"http://arxiv.org/abs/2405.00458v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-01 11:26:31","title":"Counterfactual Explanations for Deep Learning-Based Traffic Forecasting","abstract":"Deep learning models are widely used in traffic forecasting and have achieved state-of-the-art prediction accuracy. However, the black-box nature of those models makes the results difficult to interpret by users. This study aims to leverage an Explainable AI approach, counterfactual explanations, to enhance the explainability and usability of deep learning-based traffic forecasting models. Specifically, the goal is to elucidate relationships between various input contextual features and their corresponding predictions. We present a comprehensive framework that generates counterfactual explanations for traffic forecasting and provides usable insights through the proposed scenario-driven counterfactual explanations. The study first implements a deep learning model to predict traffic speed based on historical traffic data and contextual variables. Counterfactual explanations are then used to illuminate how alterations in these input variables affect predicted outcomes, thereby enhancing the transparency of the deep learning model. We investigated the impact of contextual features on traffic speed prediction under varying spatial and temporal conditions. The scenario-driven counterfactual explanations integrate two types of user-defined constraints, directional and weighting constraints, to tailor the search for counterfactual explanations to specific use cases. These tailored explanations benefit machine learning practitioners who aim to understand the model's learning mechanisms and domain experts who seek insights for real-world applications. The results showcase the effectiveness of counterfactual explanations in revealing traffic patterns learned by deep learning models, showing its potential for interpreting black-box deep learning models used for spatiotemporal predictions in general.","sentences":["Deep learning models are widely used in traffic forecasting and have achieved state-of-the-art prediction accuracy.","However, the black-box nature of those models makes the results difficult to interpret by users.","This study aims to leverage an Explainable AI approach, counterfactual explanations, to enhance the explainability and usability of deep learning-based traffic forecasting models.","Specifically, the goal is to elucidate relationships between various input contextual features and their corresponding predictions.","We present a comprehensive framework that generates counterfactual explanations for traffic forecasting and provides usable insights through the proposed scenario-driven counterfactual explanations.","The study first implements a deep learning model to predict traffic speed based on historical traffic data and contextual variables.","Counterfactual explanations are then used to illuminate how alterations in these input variables affect predicted outcomes, thereby enhancing the transparency of the deep learning model.","We investigated the impact of contextual features on traffic speed prediction under varying spatial and temporal conditions.","The scenario-driven counterfactual explanations integrate two types of user-defined constraints, directional and weighting constraints, to tailor the search for counterfactual explanations to specific use cases.","These tailored explanations benefit machine learning practitioners who aim to understand the model's learning mechanisms and domain experts who seek insights for real-world applications.","The results showcase the effectiveness of counterfactual explanations in revealing traffic patterns learned by deep learning models, showing its potential for interpreting black-box deep learning models used for spatiotemporal predictions in general."],"url":"http://arxiv.org/abs/2405.00456v1","category":"cs.LG"}
{"created":"2024-05-01 11:12:22","title":"Fuzzy Intelligent System for Student Software Project Evaluation","abstract":"Developing software projects allows students to put knowledge into practice and gain teamwork skills. However, assessing student performance in project-oriented courses poses significant challenges, particularly as the size of classes increases. The current paper introduces a fuzzy intelligent system designed to evaluate academic software projects using object-oriented programming and design course as an example. To establish evaluation criteria, we first conducted a survey of student project teams (n=31) and faculty (n=3) to identify key parameters and their applicable ranges. The selected criteria - clean code, use of inheritance, and functionality - were selected as essential for assessing the quality of academic software projects. These criteria were then represented as fuzzy variables with corresponding fuzzy sets. Collaborating with three experts, including one professor and two course instructors, we defined a set of fuzzy rules for a fuzzy inference system. This system processes the input criteria to produce a quantifiable measure of project success. The system demonstrated promising results in automating the evaluation of projects. Our approach standardizes project evaluations and helps to reduce the subjective bias in manual grading.","sentences":["Developing software projects allows students to put knowledge into practice and gain teamwork skills.","However, assessing student performance in project-oriented courses poses significant challenges, particularly as the size of classes increases.","The current paper introduces a fuzzy intelligent system designed to evaluate academic software projects using object-oriented programming and design course as an example.","To establish evaluation criteria, we first conducted a survey of student project teams (n=31) and faculty (n=3) to identify key parameters and their applicable ranges.","The selected criteria - clean code, use of inheritance, and functionality - were selected as essential for assessing the quality of academic software projects.","These criteria were then represented as fuzzy variables with corresponding fuzzy sets.","Collaborating with three experts, including one professor and two course instructors, we defined a set of fuzzy rules for a fuzzy inference system.","This system processes the input criteria to produce a quantifiable measure of project success.","The system demonstrated promising results in automating the evaluation of projects.","Our approach standardizes project evaluations and helps to reduce the subjective bias in manual grading."],"url":"http://arxiv.org/abs/2405.00453v1","category":"cs.SE"}
{"created":"2024-05-01 11:10:24","title":"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning","abstract":"We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the critical importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and SciQ, with substantial percentage increases in accuracy to $80.7\\%$ (+$4.8\\%$), $32.2\\%$ (+$3.3\\%$), and $88.5\\%$ (+$7.7\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains.","sentences":["We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero.","Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals.","To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data.","The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data.","Theoretical analysis reveals the critical importance of using on-policy sampled data for successful self-improving.","Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models.","For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and SciQ, with substantial percentage increases in accuracy to $80.7\\%$ (+$4.8\\%$), $32.2\\%$ (+$3.3\\%$), and $88.5\\%$ (+$7.7\\%$), respectively.","Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains."],"url":"http://arxiv.org/abs/2405.00451v1","category":"cs.AI"}
{"created":"2024-05-01 11:08:26","title":"Quantum Global Minimum Finder based on Variational Quantum Search","abstract":"The search for global minima is a critical challenge across multiple fields including engineering, finance, and artificial intelligence, particularly with non-convex functions that feature multiple local optima, complicating optimization efforts. We introduce the Quantum Global Minimum Finder (QGMF), an innovative quantum computing approach that efficiently identifies global minima. QGMF combines binary search techniques to shift the objective function to a suitable position and then employs Variational Quantum Search to precisely locate the global minimum within this targeted subspace. Designed with a low-depth circuit architecture, QGMF is optimized for Noisy Intermediate-Scale Quantum (NISQ) devices, utilizing the logarithmic benefits of binary search to enhance scalability and efficiency. This work demonstrates the impact of QGMF in advancing the capabilities of quantum computing to overcome complex non-convex optimization challenges effectively.","sentences":["The search for global minima is a critical challenge across multiple fields including engineering, finance, and artificial intelligence, particularly with non-convex functions that feature multiple local optima, complicating optimization efforts.","We introduce the Quantum Global Minimum Finder (QGMF), an innovative quantum computing approach that efficiently identifies global minima.","QGMF combines binary search techniques to shift the objective function to a suitable position and then employs Variational Quantum Search to precisely locate the global minimum within this targeted subspace.","Designed with a low-depth circuit architecture, QGMF is optimized for Noisy Intermediate-Scale Quantum (NISQ) devices, utilizing the logarithmic benefits of binary search to enhance scalability and efficiency.","This work demonstrates the impact of QGMF in advancing the capabilities of quantum computing to overcome complex non-convex optimization challenges effectively."],"url":"http://arxiv.org/abs/2405.00450v1","category":"quant-ph"}
{"created":"2024-05-01 11:06:31","title":"RAG-based Explainable Prediction of Road Users Behaviors for Automated Driving using Knowledge Graphs and Large Language Models","abstract":"Prediction of road users' behaviors in the context of autonomous driving has gained considerable attention by the scientific community in the last years. Most works focus on predicting behaviors based on kinematic information alone, a simplification of the reality since road users are humans, and as such they are highly influenced by their surrounding context. In addition, a large plethora of research works rely on powerful Deep Learning techniques, which exhibit high performance metrics in prediction tasks but may lack the ability to fully understand and exploit the contextual semantic information contained in the road scene, not to mention their inability to provide explainable predictions that can be understood by humans. In this work, we propose an explainable road users' behavior prediction system that integrates the reasoning abilities of Knowledge Graphs (KG) and the expressiveness capabilities of Large Language Models (LLM) by using Retrieval Augmented Generation (RAG) techniques. For that purpose, Knowledge Graph Embeddings (KGE) and Bayesian inference are combined to allow the deployment of a fully inductive reasoning system that enables the issuing of predictions that rely on legacy information contained in the graph as well as on current evidence gathered in real time by onboard sensors. Two use cases have been implemented following the proposed approach: 1) Prediction of pedestrians' crossing actions; 2) Prediction of lane change maneuvers. In both cases, the performance attained surpasses the current state of the art in terms of anticipation and F1-score, showing a promising avenue for future research in this field.","sentences":["Prediction of road users' behaviors in the context of autonomous driving has gained considerable attention by the scientific community in the last years.","Most works focus on predicting behaviors based on kinematic information alone, a simplification of the reality since road users are humans, and as such they are highly influenced by their surrounding context.","In addition, a large plethora of research works rely on powerful Deep Learning techniques, which exhibit high performance metrics in prediction tasks but may lack the ability to fully understand and exploit the contextual semantic information contained in the road scene, not to mention their inability to provide explainable predictions that can be understood by humans.","In this work, we propose an explainable road users' behavior prediction system that integrates the reasoning abilities of Knowledge Graphs (KG) and the expressiveness capabilities of Large Language Models (LLM) by using Retrieval Augmented Generation (RAG) techniques.","For that purpose, Knowledge Graph Embeddings (KGE) and Bayesian inference are combined to allow the deployment of a fully inductive reasoning system that enables the issuing of predictions that rely on legacy information contained in the graph as well as on current evidence gathered in real time by onboard sensors.","Two use cases have been implemented following the proposed approach: 1) Prediction of pedestrians' crossing actions; 2) Prediction of lane change maneuvers.","In both cases, the performance attained surpasses the current state of the art in terms of anticipation and F1-score, showing a promising avenue for future research in this field."],"url":"http://arxiv.org/abs/2405.00449v1","category":"cs.LG"}
{"created":"2024-05-01 11:04:22","title":"MMTryon: Multi-Modal Multi-Reference Control for High-Quality Fashion Generation","abstract":"This paper introduces MMTryon, a multi-modal multi-reference VIrtual Try-ON (VITON) framework, which can generate high-quality compositional try-on results by taking as inputs a text instruction and multiple garment images. Our MMTryon mainly addresses two problems overlooked in prior literature: 1) Support of multiple try-on items and dressing styleExisting methods are commonly designed for single-item try-on tasks (e.g., upper/lower garments, dresses) and fall short on customizing dressing styles (e.g., zipped/unzipped, tuck-in/tuck-out, etc.) 2) Segmentation Dependency. They further heavily rely on category-specific segmentation models to identify the replacement regions, with segmentation errors directly leading to significant artifacts in the try-on results. For the first issue, our MMTryon introduces a novel multi-modality and multi-reference attention mechanism to combine the garment information from reference images and dressing-style information from text instructions. Besides, to remove the segmentation dependency, MMTryon uses a parsing-free garment encoder and leverages a novel scalable data generation pipeline to convert existing VITON datasets to a form that allows MMTryon to be trained without requiring any explicit segmentation. Extensive experiments on high-resolution benchmarks and in-the-wild test sets demonstrate MMTryon's superiority over existing SOTA methods both qualitatively and quantitatively. Besides, MMTryon's impressive performance on multi-items and style-controllable virtual try-on scenarios and its ability to try on any outfit in a large variety of scenarios from any source image, opens up a new avenue for future investigation in the fashion community.","sentences":["This paper introduces MMTryon, a multi-modal multi-reference VIrtual Try-ON (VITON) framework, which can generate high-quality compositional try-on results by taking as inputs a text instruction and multiple garment images.","Our MMTryon mainly addresses two problems overlooked in prior literature: 1) Support of multiple try-on items and dressing styleExisting methods are commonly designed for single-item try-on tasks (e.g., upper/lower garments, dresses) and fall short on customizing dressing styles (e.g., zipped/unzipped, tuck-in/tuck-out, etc.)","2) Segmentation Dependency.","They further heavily rely on category-specific segmentation models to identify the replacement regions, with segmentation errors directly leading to significant artifacts in the try-on results.","For the first issue, our MMTryon introduces a novel multi-modality and multi-reference attention mechanism to combine the garment information from reference images and dressing-style information from text instructions.","Besides, to remove the segmentation dependency, MMTryon uses a parsing-free garment encoder and leverages a novel scalable data generation pipeline to convert existing VITON datasets to a form that allows MMTryon to be trained without requiring any explicit segmentation.","Extensive experiments on high-resolution benchmarks and in-the-wild test sets demonstrate MMTryon's superiority over existing SOTA methods both qualitatively and quantitatively.","Besides, MMTryon's impressive performance on multi-items and style-controllable virtual try-on scenarios and its ability to try on any outfit in a large variety of scenarios from any source image, opens up a new avenue for future investigation in the fashion community."],"url":"http://arxiv.org/abs/2405.00448v1","category":"cs.CV"}
{"created":"2024-05-01 11:01:52","title":"Transverse distortion and single-spin asymmetries for low-lying octet baryons","abstract":"Being the fountainhead of transverse distortion and asymmetries in accordance to $``SSA=GPD \\ast FSI\\,\"$, we study the spin flip generalized parton distribution. We demonstrate this transverse deformation by using the quark-scalar diquark model and unveil a comparison among the low-lying strange baryons for distinct feasible combinations of quark-diquark pairs. Final-state interaction originating the correlation of a target spin and a virtual photon to the generated pion plane in semi-inclusive deep inelastic scattering has also been analyzed.","sentences":["Being the fountainhead of transverse distortion and asymmetries in accordance to $``SSA=GPD \\ast FSI\\,\"$, we study the spin flip generalized parton distribution.","We demonstrate this transverse deformation by using the quark-scalar diquark model and unveil a comparison among the low-lying strange baryons for distinct feasible combinations of quark-diquark pairs.","Final-state interaction originating the correlation of a target spin and a virtual photon to the generated pion plane in semi-inclusive deep inelastic scattering has also been analyzed."],"url":"http://arxiv.org/abs/2405.00445v1","category":"hep-ph"}
{"created":"2024-05-01 10:53:54","title":"Geometric Insights into Focal Loss: Reducing Curvature for Enhanced Model Calibration","abstract":"The key factor in implementing machine learning algorithms in decision-making situations is not only the accuracy of the model but also its confidence level. The confidence level of a model in a classification problem is often given by the output vector of a softmax function for convenience. However, these values are known to deviate significantly from the actual expected model confidence. This problem is called model calibration and has been studied extensively. One of the simplest techniques to tackle this task is focal loss, a generalization of cross-entropy by introducing one positive parameter. Although many related studies exist because of the simplicity of the idea and its formalization, the theoretical analysis of its behavior is still insufficient. In this study, our objective is to understand the behavior of focal loss by reinterpreting this function geometrically. Our analysis suggests that focal loss reduces the curvature of the loss surface in training the model. This indicates that curvature may be one of the essential factors in achieving model calibration. We design numerical experiments to support this conjecture to reveal the behavior of focal loss and the relationship between calibration performance and curvature.","sentences":["The key factor in implementing machine learning algorithms in decision-making situations is not only the accuracy of the model but also its confidence level.","The confidence level of a model in a classification problem is often given by the output vector of a softmax function for convenience.","However, these values are known to deviate significantly from the actual expected model confidence.","This problem is called model calibration and has been studied extensively.","One of the simplest techniques to tackle this task is focal loss, a generalization of cross-entropy by introducing one positive parameter.","Although many related studies exist because of the simplicity of the idea and its formalization, the theoretical analysis of its behavior is still insufficient.","In this study, our objective is to understand the behavior of focal loss by reinterpreting this function geometrically.","Our analysis suggests that focal loss reduces the curvature of the loss surface in training the model.","This indicates that curvature may be one of the essential factors in achieving model calibration.","We design numerical experiments to support this conjecture to reveal the behavior of focal loss and the relationship between calibration performance and curvature."],"url":"http://arxiv.org/abs/2405.00442v1","category":"stat.ML"}
{"created":"2024-05-01 10:43:55","title":"MetaRM: Shifted Distributions Alignment via Meta-Learning","abstract":"The success of Reinforcement Learning from Human Feedback (RLHF) in language model alignment is critically dependent on the capability of the reward model (RM). However, as the training process progresses, the output distribution of the policy model shifts, leading to the RM's reduced ability to distinguish between responses. This issue is further compounded when the RM, trained on a specific data distribution, struggles to generalize to examples outside of that distribution. These two issues can be united as a challenge posed by the shifted distribution of the environment. To surmount this challenge, we introduce MetaRM, a method leveraging meta-learning to align the RM with the shifted environment distribution. MetaRM is designed to train the RM by minimizing data loss, particularly for data that can improve the differentiation ability to examples of the shifted target distribution. Extensive experiments demonstrate that MetaRM significantly improves the RM's distinguishing ability in iterative RLHF optimization, and also provides the capacity to identify subtle differences in out-of-distribution samples.","sentences":["The success of Reinforcement Learning from Human Feedback (RLHF) in language model alignment is critically dependent on the capability of the reward model (RM).","However, as the training process progresses, the output distribution of the policy model shifts, leading to the RM's reduced ability to distinguish between responses.","This issue is further compounded when the RM, trained on a specific data distribution, struggles to generalize to examples outside of that distribution.","These two issues can be united as a challenge posed by the shifted distribution of the environment.","To surmount this challenge, we introduce MetaRM, a method leveraging meta-learning to align the RM with the shifted environment distribution.","MetaRM is designed to train the RM by minimizing data loss, particularly for data that can improve the differentiation ability to examples of the shifted target distribution.","Extensive experiments demonstrate that MetaRM significantly improves the RM's distinguishing ability in iterative RLHF optimization, and also provides the capacity to identify subtle differences in out-of-distribution samples."],"url":"http://arxiv.org/abs/2405.00438v1","category":"cs.LG"}
{"created":"2024-05-01 10:38:06","title":"Porting HPC Applications to AMD Instinct$^\\text{TM}$ MI300A Using Unified Memory and OpenMP","abstract":"AMD Instinct$^\\text{TM}$ MI300A is the world's first data center accelerated processing unit (APU) with memory shared between the AMD \"Zen 4\" EPYC$^\\text{TM}$ cores and third generation CDNA$^\\text{TM}$ compute units. A single memory space offers several advantages: i) it eliminates the need for data replication and costly data transfers, ii) it substantially simplifies application development and allows an incremental acceleration of applications, iii) is easy to maintain, and iv) its potential can be well realized via the abstractions in the OpenMP 5.2 standard, where the host and the device data environments can be unified in a more performant way. In this article, we provide a blueprint of the APU programming model leveraging unified memory and highlight key distinctions compared to the conventional approach with discrete GPUs. OpenFOAM, an open-source C++ library for computational fluid dynamics, is presented as a case study to emphasize the flexibility and ease of offloading a full-scale production-ready application on MI300 APUs using directive-based OpenMP programming.","sentences":["AMD Instinct$^\\text{TM}$ MI300A is the world's first data center accelerated processing unit (APU) with memory shared between the AMD \"Zen 4\" EPYC$^\\text{TM}$ cores and third generation CDNA$^\\text{TM}$ compute units.","A single memory space offers several advantages: i) it eliminates the need for data replication and costly data transfers, ii) it substantially simplifies application development and allows an incremental acceleration of applications, iii) is easy to maintain, and iv) its potential can be well realized via the abstractions in the OpenMP 5.2 standard, where the host and the device data environments can be unified in a more performant way.","In this article, we provide a blueprint of the APU programming model leveraging unified memory and highlight key distinctions compared to the conventional approach with discrete GPUs.","OpenFOAM, an open-source C++ library for computational fluid dynamics, is presented as a case study to emphasize the flexibility and ease of offloading a full-scale production-ready application on MI300 APUs using directive-based OpenMP programming."],"url":"http://arxiv.org/abs/2405.00436v1","category":"cs.DC"}
{"created":"2024-05-01 10:33:36","title":"Weight Sparsity Complements Activity Sparsity in Neuromorphic Language Models","abstract":"Activity and parameter sparsity are two standard methods of making neural networks computationally more efficient. Event-based architectures such as spiking neural networks (SNNs) naturally exhibit activity sparsity, and many methods exist to sparsify their connectivity by pruning weights. While the effect of weight pruning on feed-forward SNNs has been previously studied for computer vision tasks, the effects of pruning for complex sequence tasks like language modeling are less well studied since SNNs have traditionally struggled to achieve meaningful performance on these tasks. Using a recently published SNN-like architecture that works well on small-scale language modeling, we study the effects of weight pruning when combined with activity sparsity. Specifically, we study the trade-off between the multiplicative efficiency gains the combination affords and its effect on task performance for language modeling. To dissect the effects of the two sparsities, we conduct a comparative analysis between densely activated models and sparsely activated event-based models across varying degrees of connectivity sparsity. We demonstrate that sparse activity and sparse connectivity complement each other without a proportional drop in task performance for an event-based neural network trained on the Penn Treebank and WikiText-2 language modeling datasets. Our results suggest sparsely connected event-based neural networks are promising candidates for effective and efficient sequence modeling.","sentences":["Activity and parameter sparsity are two standard methods of making neural networks computationally more efficient.","Event-based architectures such as spiking neural networks (SNNs) naturally exhibit activity sparsity, and many methods exist to sparsify their connectivity by pruning weights.","While the effect of weight pruning on feed-forward SNNs has been previously studied for computer vision tasks, the effects of pruning for complex sequence tasks like language modeling are less well studied since SNNs have traditionally struggled to achieve meaningful performance on these tasks.","Using a recently published SNN-like architecture that works well on small-scale language modeling, we study the effects of weight pruning when combined with activity sparsity.","Specifically, we study the trade-off between the multiplicative efficiency gains the combination affords and its effect on task performance for language modeling.","To dissect the effects of the two sparsities, we conduct a comparative analysis between densely activated models and sparsely activated event-based models across varying degrees of connectivity sparsity.","We demonstrate that sparse activity and sparse connectivity complement each other without a proportional drop in task performance for an event-based neural network trained on the Penn Treebank and WikiText-2 language modeling datasets.","Our results suggest sparsely connected event-based neural networks are promising candidates for effective and efficient sequence modeling."],"url":"http://arxiv.org/abs/2405.00433v1","category":"cs.LG"}
{"created":"2024-05-01 10:29:16","title":"Dynamic Mueller matrix polarimetry using generalized measurements","abstract":"Mueller matrices provide a complete description of a medium's response to excitation by polarized light, and their characterization is important across a broad range of applications from ellipsometry in material science to polarimetry in biochemistry, medicine and astronomy. Here we introduce single-shot Mueller matrix polarimetry based on generalized measurements performed with a Poincar\\'e beam. We determine the Mueller matrix of a homogeneous medium with unknown optical activity by detecting its optical response to a Poincar\\'e beam, which across its profile contains all polarization states, and analyze the resulting polarization pattern in terms of four generalized measurements, which are implemented as a path-displaced Sagnac interferometer. We illustrate the working of our Mueller matrix polarimetry on the example of tilted and rotated wave plates and find excellent agreement with predictions as well as alternative Stokes measurements. After initial calibration, the alignment of the device stays stable for up to 8 hours, promising suitability for the dynamic characterization of Mueller matrices that change in time.","sentences":["Mueller matrices provide a complete description of a medium's response to excitation by polarized light, and their characterization is important across a broad range of applications from ellipsometry in material science to polarimetry in biochemistry, medicine and astronomy.","Here we introduce single-shot Mueller matrix polarimetry based on generalized measurements performed with a Poincar\\'e beam.","We determine the Mueller matrix of a homogeneous medium with unknown optical activity by detecting its optical response to a Poincar\\'e beam, which across its profile contains all polarization states, and analyze the resulting polarization pattern in terms of four generalized measurements, which are implemented as a path-displaced Sagnac interferometer.","We illustrate the working of our Mueller matrix polarimetry on the example of tilted and rotated wave plates and find excellent agreement with predictions as well as alternative Stokes measurements.","After initial calibration, the alignment of the device stays stable for up to 8 hours, promising suitability for the dynamic characterization of Mueller matrices that change in time."],"url":"http://arxiv.org/abs/2405.00432v1","category":"physics.optics"}
{"created":"2024-05-01 10:27:22","title":"Detail-Enhancing Framework for Reference-Based Image Super-Resolution","abstract":"Recent years have witnessed the prosperity of reference-based image super-resolution (Ref-SR). By importing the high-resolution (HR) reference images into the single image super-resolution (SISR) approach, the ill-posed nature of this long-standing field has been alleviated with the assistance of texture transferred from reference images. Although the significant improvement in quantitative and qualitative results has verified the superiority of Ref-SR methods, the presence of misalignment before texture transfer indicates room for further performance improvement. Existing methods tend to neglect the significance of details in the context of comparison, therefore not fully leveraging the information contained within low-resolution (LR) images. In this paper, we propose a Detail-Enhancing Framework (DEF) for reference-based super-resolution, which introduces the diffusion model to generate and enhance the underlying detail in LR images. If corresponding parts are present in the reference image, our method can facilitate rigorous alignment. In cases where the reference image lacks corresponding parts, it ensures a fundamental improvement while avoiding the influence of the reference image. Extensive experiments demonstrate that our proposed method achieves superior visual results while maintaining comparable numerical outcomes.","sentences":["Recent years have witnessed the prosperity of reference-based image super-resolution (Ref-SR).","By importing the high-resolution (HR) reference images into the single image super-resolution (SISR) approach, the ill-posed nature of this long-standing field has been alleviated with the assistance of texture transferred from reference images.","Although the significant improvement in quantitative and qualitative results has verified the superiority of Ref-SR methods, the presence of misalignment before texture transfer indicates room for further performance improvement.","Existing methods tend to neglect the significance of details in the context of comparison, therefore not fully leveraging the information contained within low-resolution (LR) images.","In this paper, we propose a Detail-Enhancing Framework (DEF) for reference-based super-resolution, which introduces the diffusion model to generate and enhance the underlying detail in LR images.","If corresponding parts are present in the reference image, our method can facilitate rigorous alignment.","In cases where the reference image lacks corresponding parts, it ensures a fundamental improvement while avoiding the influence of the reference image.","Extensive experiments demonstrate that our proposed method achieves superior visual results while maintaining comparable numerical outcomes."],"url":"http://arxiv.org/abs/2405.00431v1","category":"cs.CV"}
{"created":"2024-05-01 10:22:57","title":"Clique-free t-matchings in degree-bounded graphs","abstract":"We consider problems of finding a maximum size/weight $t$-matching without forbidden subgraphs in an undirected graph $G$ with the maximum degree bounded by $t+1$, where $t$ is an integer greater than $2$. Depending on the variant forbidden subgraphs denote certain subsets of $t$-regular complete partite subgraphs of $G$. A graph is complete partite if there exists a partition of its vertex set such that every pair of vertices from different sets is connected by an edge and vertices from the same set form an independent set. A clique $K_t$ and a bipartite clique $K_{t,t}$ are examples of complete partite graphs. These problems are natural generalizations of the triangle-free and square-free $2$-matching problems in subcubic graphs. In the weighted setting we assume that the weights of edges of $G$ are vertex-induced on every forbidden subgraph. We present simple and fast combinatorial algorithms for these problems. The presented algorithms are the first ones for the weighted versions, and for the unweighted ones, are faster than those known previously. Our approach relies on the use of gadgets with so-called half-edges. A half-edge of edge $e$ is, informally speaking, a half of $e$ containing exactly one of its endpoints.","sentences":["We consider problems of finding a maximum size/weight $t$-matching without forbidden subgraphs in an undirected graph $G$ with the maximum degree bounded by $t+1$, where $t$ is an integer greater than $2$. Depending on the variant forbidden subgraphs denote certain subsets of $t$-regular complete partite subgraphs of $G$. A graph is complete partite if there exists a partition of its vertex set such that every pair of vertices from different sets is connected by an edge and vertices from the same set form an independent set.","A clique $K_t$ and a bipartite clique $K_{t,t}$ are examples of complete partite graphs.","These problems are natural generalizations of the triangle-free and square-free $2$-matching problems in subcubic graphs.","In the weighted setting we assume that the weights of edges of $G$ are vertex-induced on every forbidden subgraph.","We present simple and fast combinatorial algorithms for these problems.","The presented algorithms are the first ones for the weighted versions, and for the unweighted ones, are faster than those known previously.","Our approach relies on the use of gadgets with so-called half-edges.","A half-edge of edge $e$ is, informally speaking, a half of $e$ containing exactly one of its endpoints."],"url":"http://arxiv.org/abs/2405.00429v1","category":"cs.DS"}
{"created":"2024-05-01 10:17:24","title":"On the Potential of RIS in the Context of PLA in Wireless Communication Systems","abstract":"Re-configurable Intelligent Surfaces (RIS) technology has proven itself a promising candidate for the next generation of wireless networks through its enhanced performance in terms of throughput, spectral, and energy efficiency. However, the broadcast nature of RIS-assisted wireless communication makes it vulnerable to malicious attacks at the physical layer. On the other hand, physical layer authentication is an emerging area in the security domain to thwart different attacks such as cloning, spoofing, and impersonation by using the random features of the physical layer. In this paper, we investigate RIS-assisted wireless communication systems to unlock the potential of using RIS for physical layer authentication (PLA). Specifically, we exploit two distinct features of the physical layer: pathloss and channel impulse response (CIR) for PLA in RIS-assisted wireless communication. We construct hypothesis tests for the estimated features and derive the closed-form errors' expressions. Further, we chose the critical error, i.e., missed detection as our objective function for minimization by optimizing the phase shift of the RIS pannel. We compare the performance of our proposed mechanisms with baseline mechanisms which are PLA schemes using the same features but with no RIS assistance. Furthermore, we thoroughly evaluate our proposed schemes using performance metrics such as the probability of false alarm (PFA), the probability of missed detection (PMD), and the receiver operating characteristic (ROC) curves. The results demonstrate the significant positive impact of RIS on PLA, as it effectively reduces PMD values to zero when determining the optimal phase shift.","sentences":["Re-configurable Intelligent Surfaces (RIS) technology has proven itself a promising candidate for the next generation of wireless networks through its enhanced performance in terms of throughput, spectral, and energy efficiency.","However, the broadcast nature of RIS-assisted wireless communication makes it vulnerable to malicious attacks at the physical layer.","On the other hand, physical layer authentication is an emerging area in the security domain to thwart different attacks such as cloning, spoofing, and impersonation by using the random features of the physical layer.","In this paper, we investigate RIS-assisted wireless communication systems to unlock the potential of using RIS for physical layer authentication (PLA).","Specifically, we exploit two distinct features of the physical layer: pathloss and channel impulse response (CIR) for PLA in RIS-assisted wireless communication.","We construct hypothesis tests for the estimated features and derive the closed-form errors' expressions.","Further, we chose the critical error, i.e., missed detection as our objective function for minimization by optimizing the phase shift of the RIS pannel.","We compare the performance of our proposed mechanisms with baseline mechanisms which are PLA schemes using the same features but with no RIS assistance.","Furthermore, we thoroughly evaluate our proposed schemes using performance metrics such as the probability of false alarm (PFA), the probability of missed detection (PMD), and the receiver operating characteristic (ROC) curves.","The results demonstrate the significant positive impact of RIS on PLA, as it effectively reduces PMD values to zero when determining the optimal phase shift."],"url":"http://arxiv.org/abs/2405.00426v1","category":"cs.CR"}
{"created":"2024-05-01 10:01:41","title":"$\u03b1$-leakage by R\u00e9nyi Divergence and Sibson Mutual Information","abstract":"For $\\tilde{f}(t) = \\exp(\\frac{\\alpha-1}{\\alpha}t)$, this paper proposes a $\\tilde{f}$-mean information gain measure. R\\'{e}nyi divergence is shown to be the maximum $\\tilde{f}$-mean information gain incurred at each elementary event $y$ of channel output $Y$ and Sibson mutual information is the $\\tilde{f}$-mean of this $Y$-elementary information gain. Both are proposed as $\\alpha$-leakage measures, indicating the most information an adversary can obtain on sensitive data. It is shown that the existing $\\alpha$-leakage by Arimoto mutual information can be expressed as $\\tilde{f}$-mean measures by a scaled probability. Further, Sibson mutual information is interpreted as the maximum $\\tilde{f}$-mean information gain over all estimation decisions applied to channel output. This reveals that the exiting generalized Blahut-Arimoto method for computing R\\'{e}nyi capacity (or Gallager's error exponent) in fact maximizes a $\\tilde{f}$-mean information gain iteratively over estimation decision and channel input. This paper also derives a decomposition of $\\tilde{f}$-mean information gain, analogous to the Sibson identity for R\\'{e}nyi divergence.","sentences":["For $\\tilde{f}(t) = \\exp(\\frac{\\alpha-1}{\\alpha}t)$, this paper proposes a $\\tilde{f}$-mean information gain measure.","R\\'{e}nyi divergence is shown to be the maximum $\\tilde{f}$-mean information gain incurred at each elementary event $y$ of channel output $Y$ and Sibson mutual information is the $\\tilde{f}$-mean of this $Y$-elementary information gain.","Both are proposed as $\\alpha$-leakage measures, indicating the most information an adversary can obtain on sensitive data.","It is shown that the existing $\\alpha$-leakage by Arimoto mutual information can be expressed as $\\tilde{f}$-mean measures by a scaled probability.","Further, Sibson mutual information is interpreted as the maximum $\\tilde{f}$-mean information gain over all estimation decisions applied to channel output.","This reveals that the exiting generalized Blahut-Arimoto method for computing R\\'{e}nyi capacity (or Gallager's error exponent) in fact maximizes a $\\tilde{f}$-mean information gain iteratively over estimation decision and channel input.","This paper also derives a decomposition of $\\tilde{f}$-mean information gain, analogous to the Sibson identity for R\\'{e}nyi divergence."],"url":"http://arxiv.org/abs/2405.00423v1","category":"cs.IT"}
{"created":"2024-05-01 10:00:08","title":"Bona-Smith-type systems in bounded domains with slip-wall boundary conditions: Theoretical justification and a conservative numerical scheme","abstract":"Considered herein is a class of Boussinesq systems of Bona-Smith type that describe water waves in bounded two-dimensional domains with slip-wall boundary conditions and variable bottom topography. Such boundary conditions are necessary in situations involving water waves in channels, ports, and generally in basins with solid boundaries. We prove that, given appropriate initial conditions, the corresponding initial-boundary value problems have unique solutions locally in time, which is a fundamental property of deterministic mathematical modeling. Moreover, we demonstrate that the systems under consideration adhere to three basic conservation laws for water waves: mass, vorticity, and energy conservation.   The theoretical analysis of these specific Boussinesq systems leads to a conservative mixed finite element formulation. Using explicit, relaxation Runge-Kutta methods for the discretization in time, we devise a fully discrete scheme for the numerical solution of initial-boundary value problems with slip-wall conditions, preserving mass, vorticity, and energy. Finally, we present a series of challenging numerical experiments to assess the applicability of the new numerical model.","sentences":["Considered herein is a class of Boussinesq systems of Bona-Smith type that describe water waves in bounded two-dimensional domains with slip-wall boundary conditions and variable bottom topography.","Such boundary conditions are necessary in situations involving water waves in channels, ports, and generally in basins with solid boundaries.","We prove that, given appropriate initial conditions, the corresponding initial-boundary value problems have unique solutions locally in time, which is a fundamental property of deterministic mathematical modeling.","Moreover, we demonstrate that the systems under consideration adhere to three basic conservation laws for water waves: mass, vorticity, and energy conservation.   ","The theoretical analysis of these specific Boussinesq systems leads to a conservative mixed finite element formulation.","Using explicit, relaxation Runge-Kutta methods for the discretization in time, we devise a fully discrete scheme for the numerical solution of initial-boundary value problems with slip-wall conditions, preserving mass, vorticity, and energy.","Finally, we present a series of challenging numerical experiments to assess the applicability of the new numerical model."],"url":"http://arxiv.org/abs/2405.00422v1","category":"math.NA"}
{"created":"2024-05-01 09:58:57","title":"Self-supervised Pre-training of Text Recognizers","abstract":"In this paper, we investigate self-supervised pre-training methods for document text recognition. Nowadays, large unlabeled datasets can be collected for many research tasks, including text recognition, but it is costly to annotate them. Therefore, methods utilizing unlabeled data are researched. We study self-supervised pre-training methods based on masked label prediction using three different approaches -- Feature Quantization, VQ-VAE, and Post-Quantized AE. We also investigate joint-embedding approaches with VICReg and NT-Xent objectives, for which we propose an image shifting technique to prevent model collapse where it relies solely on positional encoding while completely ignoring the input image. We perform our experiments on historical handwritten (Bentham) and historical printed datasets mainly to investigate the benefits of the self-supervised pre-training techniques with different amounts of annotated target domain data. We use transfer learning as strong baselines. The evaluation shows that the self-supervised pre-training on data from the target domain is very effective, but it struggles to outperform transfer learning from closely related domains. This paper is one of the first researches exploring self-supervised pre-training in document text recognition, and we believe that it will become a cornerstone for future research in this area. We made our implementation of the investigated methods publicly available at https://github.com/DCGM/pero-pretraining.","sentences":["In this paper, we investigate self-supervised pre-training methods for document text recognition.","Nowadays, large unlabeled datasets can be collected for many research tasks, including text recognition, but it is costly to annotate them.","Therefore, methods utilizing unlabeled data are researched.","We study self-supervised pre-training methods based on masked label prediction using three different approaches -- Feature Quantization, VQ-VAE, and Post-Quantized AE.","We also investigate joint-embedding approaches with VICReg and NT-Xent objectives, for which we propose an image shifting technique to prevent model collapse where it relies solely on positional encoding while completely ignoring the input image.","We perform our experiments on historical handwritten (Bentham) and historical printed datasets mainly to investigate the benefits of the self-supervised pre-training techniques with different amounts of annotated target domain data.","We use transfer learning as strong baselines.","The evaluation shows that the self-supervised pre-training on data from the target domain is very effective, but it struggles to outperform transfer learning from closely related domains.","This paper is one of the first researches exploring self-supervised pre-training in document text recognition, and we believe that it will become a cornerstone for future research in this area.","We made our implementation of the investigated methods publicly available at https://github.com/DCGM/pero-pretraining."],"url":"http://arxiv.org/abs/2405.00420v1","category":"cs.CV"}
{"created":"2024-05-01 09:57:34","title":"Detection of ransomware attacks using federated learning based on the CNN model","abstract":"Computing is still under a significant threat from ransomware, which necessitates prompt action to prevent it. Ransomware attacks can have a negative impact on how smart grids, particularly digital substations. In addition to examining a ransomware detection method using artificial intelligence (AI), this paper offers a ransomware attack modeling technique that targets the disrupted operation of a digital substation. The first, binary data is transformed into image data and fed into the convolution neural network model using federated learning. The experimental findings demonstrate that the suggested technique detects ransomware with a high accuracy rate.","sentences":["Computing is still under a significant threat from ransomware, which necessitates prompt action to prevent it.","Ransomware attacks can have a negative impact on how smart grids, particularly digital substations.","In addition to examining a ransomware detection method using artificial intelligence (AI), this paper offers a ransomware attack modeling technique that targets the disrupted operation of a digital substation.","The first, binary data is transformed into image data and fed into the convolution neural network model using federated learning.","The experimental findings demonstrate that the suggested technique detects ransomware with a high accuracy rate."],"url":"http://arxiv.org/abs/2405.00418v1","category":"cs.CR"}
{"created":"2024-05-01 09:55:31","title":"Conformal Risk Control for Ordinal Classification","abstract":"As a natural extension to the standard conformal prediction method, several conformal risk control methods have been recently developed and applied to various learning problems. In this work, we seek to control the conformal risk in expectation for ordinal classification tasks, which have broad applications to many real problems. For this purpose, we firstly formulated the ordinal classification task in the conformal risk control framework, and provided theoretic risk bounds of the risk control method. Then we proposed two types of loss functions specially designed for ordinal classification tasks, and developed corresponding algorithms to determine the prediction set for each case to control their risks at a desired level. We demonstrated the effectiveness of our proposed methods, and analyzed the difference between the two types of risks on three different datasets, including a simulated dataset, the UTKFace dataset and the diabetic retinopathy detection dataset.","sentences":["As a natural extension to the standard conformal prediction method, several conformal risk control methods have been recently developed and applied to various learning problems.","In this work, we seek to control the conformal risk in expectation for ordinal classification tasks, which have broad applications to many real problems.","For this purpose, we firstly formulated the ordinal classification task in the conformal risk control framework, and provided theoretic risk bounds of the risk control method.","Then we proposed two types of loss functions specially designed for ordinal classification tasks, and developed corresponding algorithms to determine the prediction set for each case to control their risks at a desired level.","We demonstrated the effectiveness of our proposed methods, and analyzed the difference between the two types of risks on three different datasets, including a simulated dataset, the UTKFace dataset and the diabetic retinopathy detection dataset."],"url":"http://arxiv.org/abs/2405.00417v1","category":"cs.LG"}
{"created":"2024-05-01 09:49:20","title":"Structure of a fourth-order dispersive flow equation through the generalized Hasimoto transformation","abstract":"This paper focuses on a one-dimensional fourth-order nonlinear dispersive partial differential equation for curve flows on a K\\\"ahler manifold. The equation arises as a fourth-order extension of the one-dimensional Schr\\\"odinger flow equation, with physical and geometrical backgrounds. First, this paper presents a framework that can transform the equation into a system of fourth-order nonlinear dispersive partial differential-integral equations for complex-valued functions. This is achieved by developing the so-called generalized Hasimoto transformation, which enables us to handle general higher-dimensional compact K\\\"ahler manifolds. Second, this paper demonstrates the computations to obtain the explicit expression of the derived system for three examples of the compact K\\\"ahler manifolds, dealing with the complex Grassmannian as an example in detail.","sentences":["This paper focuses on a one-dimensional fourth-order nonlinear dispersive partial differential equation for curve flows on a K\\\"ahler manifold.","The equation arises as a fourth-order extension of the one-dimensional Schr\\\"odinger flow equation, with physical and geometrical backgrounds.","First, this paper presents a framework that can transform the equation into a system of fourth-order nonlinear dispersive partial differential-integral equations for complex-valued functions.","This is achieved by developing the so-called generalized Hasimoto transformation, which enables us to handle general higher-dimensional compact K\\\"ahler manifolds.","Second, this paper demonstrates the computations to obtain the explicit expression of the derived system for three examples of the compact K\\\"ahler manifolds, dealing with the complex Grassmannian as an example in detail."],"url":"http://arxiv.org/abs/2405.00412v1","category":"math.DG"}
{"created":"2024-05-01 09:32:03","title":"Shallow vertex minors, stability, and dependence","abstract":"Stability and dependence are model-theoretic notions that have recently proved highly effective in the study of structural and algorithmic properties of hereditary graph classes, and are considered key notions for generalizing to hereditary graph classes the theory of sparsity developed for monotone graph classes (where an essential notion is that of nowhere dense class). The theory of sparsity was initially built on the notion of shallow minors and on the idea of excluding different sets of minors, depending on the depth at which these minors can appear.   In this paper, we follow a similar path, where shallow vertex minors replace shallow minors. In this setting, we provide a neat characterization of stable / dependent hereditary classes of graphs: A hereditary class of graphs $\\mathscr C$ is   (1) dependent if and only if it does not contain all permutation graphs and, for each integer $r$, it excludes some split interval graph as a depth-$r$ vertex minor;   (2) stable if and only if, for each integer $r$, it excludes some half-graph as a depth-$r$ vertex minor.   A key ingredient in proving these results is the preservation of stability and dependence of a class when taking bounded depth shallow vertex minors. We extend this preservation result to binary structures and get, as a direct consequence, that bounded depth shallow vertex minors of graphs with bounded twin-width have bounded twin-width.","sentences":["Stability and dependence are model-theoretic notions that have recently proved highly effective in the study of structural and algorithmic properties of hereditary graph classes, and are considered key notions for generalizing to hereditary graph classes the theory of sparsity developed for monotone graph classes (where an essential notion is that of nowhere dense class).","The theory of sparsity was initially built on the notion of shallow minors and on the idea of excluding different sets of minors, depending on the depth at which these minors can appear.   ","In this paper, we follow a similar path, where shallow vertex minors replace shallow minors.","In this setting, we provide a neat characterization of stable / dependent hereditary classes of graphs: A hereditary class of graphs $\\mathscr C$ is   (1) dependent if and only if it does not contain all permutation graphs and, for each integer $r$, it excludes some split interval graph as a depth-$r$ vertex minor;   (2) stable if and only if, for each integer $r$, it excludes some half-graph as a depth-$r$ vertex minor.   ","A key ingredient in proving these results is the preservation of stability and dependence of a class when taking bounded depth shallow vertex minors.","We extend this preservation result to binary structures and get, as a direct consequence, that bounded depth shallow vertex minors of graphs with bounded twin-width have bounded twin-width."],"url":"http://arxiv.org/abs/2405.00408v1","category":"math.CO"}
{"created":"2024-05-01 09:27:49","title":"Compressive Sensing Imaging Using Caustic Lens Mask Generated by Periodic Perturbation in a Ripple Tank","abstract":"Terahertz imaging shows significant potential across diverse fields, yet the cost-effectiveness of multi-pixel imaging equipment remains an obstacle for many researchers. To tackle this issue, the utilization of single-pixel imaging arises as a lower-cost option, however, the data collection process necessary for reconstructing images is time-consuming. Compressive Sensing offers a promising solution by enabling image generation with fewer measurements than required by Nyquist's theorem, yet long processing times remain an issue, especially for large-sized images. Our proposed solution to this issue involves using caustic lens effect induced by perturbations in a ripple tank as a sampling mask. The dynamic characteristics of the ripple tank introduce randomness into the sampling process, thereby reducing measurement time through exploitation of the inherent sparsity of THz band signals. In this study, a Convolutional Neural Network was used to conduct target classification, based on the distinctive signal patterns obtained via the caustic lens mask. The suggested classifier obtained a 95.16 % accuracy rate in differentiating targets resembling Latin letters.","sentences":["Terahertz imaging shows significant potential across diverse fields, yet the cost-effectiveness of multi-pixel imaging equipment remains an obstacle for many researchers.","To tackle this issue, the utilization of single-pixel imaging arises as a lower-cost option, however, the data collection process necessary for reconstructing images is time-consuming.","Compressive Sensing offers a promising solution by enabling image generation with fewer measurements than required by Nyquist's theorem, yet long processing times remain an issue, especially for large-sized images.","Our proposed solution to this issue involves using caustic lens effect induced by perturbations in a ripple tank as a sampling mask.","The dynamic characteristics of the ripple tank introduce randomness into the sampling process, thereby reducing measurement time through exploitation of the inherent sparsity of THz band signals.","In this study, a Convolutional Neural Network was used to conduct target classification, based on the distinctive signal patterns obtained via the caustic lens mask.","The suggested classifier obtained a 95.16 % accuracy rate in differentiating targets resembling Latin letters."],"url":"http://arxiv.org/abs/2405.00407v1","category":"eess.SP"}
{"created":"2024-05-01 09:21:06","title":"Pure State Inspired Lossless Post-selected Quantum Metrology of Mixed States","abstract":"Given an ensemble of identical pure quantum states that depend on an unknown parameter, recently it was shown that the quantum Fisher information can be losslessly compressed into a subensemble with a much smaller number of samples. However, generalization to mixed states leads to a technical challenge that is formidable to overcome directly. In this work, we avoid such technicality by unveiling the physics of a featured lossless post-selection measurement: while the post-selected quantum state is unchanged, the parametric derivative of the density operator is amplified by a large factor equal to the square root of the inverse of the post-selection success probability. This observation not only clarifies the intuition and essence of post-selected quantum metrology but also allows us to develop a mathematically compact theory for the lossless post-selection of mixed states. We find that if the parametric derivative of the density operator of a mixed state, or alternatively the symmetric logarithmic derivative, vanishes on the support of the density matrix, lossless post-selection can be achieved with an arbitrarily large amplification factor. We exemplify with the examples of superresolution imaging and unitary encoding of mixed initial states. Our results are useful for realistic post-selected quantum metrology in the presence of decoherence and of foundational interests to several problems in quantum information theory.","sentences":["Given an ensemble of identical pure quantum states that depend on an unknown parameter, recently it was shown that the quantum Fisher information can be losslessly compressed into a subensemble with a much smaller number of samples.","However, generalization to mixed states leads to a technical challenge that is formidable to overcome directly.","In this work, we avoid such technicality by unveiling the physics of a featured lossless post-selection measurement: while the post-selected quantum state is unchanged, the parametric derivative of the density operator is amplified by a large factor equal to the square root of the inverse of the post-selection success probability.","This observation not only clarifies the intuition and essence of post-selected quantum metrology but also allows us to develop a mathematically compact theory for the lossless post-selection of mixed states.","We find that if the parametric derivative of the density operator of a mixed state, or alternatively the symmetric logarithmic derivative, vanishes on the support of the density matrix, lossless post-selection can be achieved with an arbitrarily large amplification factor.","We exemplify with the examples of superresolution imaging and unitary encoding of mixed initial states.","Our results are useful for realistic post-selected quantum metrology in the presence of decoherence and of foundational interests to several problems in quantum information theory."],"url":"http://arxiv.org/abs/2405.00405v1","category":"quant-ph"}
{"created":"2024-05-01 09:19:20","title":"Rotations and boosts of Hermite functions","abstract":"We provide transformation matrices for arbitrary Lorentz transformations of multidimensional Hermite functions in any dimension. These serve as a valuable tool for analyzing spacetime properties of MHS fields, and aid in the description of the relativistic harmonic oscillator and digital image manipulation. We also focus on finite boosts and rotations around specific axes, enabling us to identify the Lorentz Lie algebra generators. As an application and to establish a contact with the literature we construct a basis in which the two dimensional rotation operator is diagonal. We comment on the use of hypergeometric functions, the Wigner d-functions, Kravchuk polynomials, Jacobi polynomials and generalized associated Legendre functions.","sentences":["We provide transformation matrices for arbitrary Lorentz transformations of multidimensional Hermite functions in any dimension.","These serve as a valuable tool for analyzing spacetime properties of MHS fields, and aid in the description of the relativistic harmonic oscillator and digital image manipulation.","We also focus on finite boosts and rotations around specific axes, enabling us to identify the Lorentz Lie algebra generators.","As an application and to establish a contact with the literature we construct a basis in which the two dimensional rotation operator is diagonal.","We comment on the use of hypergeometric functions, the Wigner d-functions, Kravchuk polynomials, Jacobi polynomials and generalized associated Legendre functions."],"url":"http://arxiv.org/abs/2405.00404v1","category":"hep-th"}
{"created":"2024-05-01 09:12:22","title":"Maximal acceleration in Rainbow gravity","abstract":"In this paper, we derive maximal acceleration of a massive particle in Rainbow gravity. Using eight-dimensional phase-space metric compatible with Rainbow gravity, we obtain the maximal acceleration, valid up to first order in the Rainbow gravity parameter $\\eta$. Using the positivity condition on maximal acceleration, we find the upper bound on the Rainbow gravity parameter is of the order of $~10^{22}$ for positron and $10^{-44}$ for a black hole. After obtaining the expression for maximal acceleration for different choices of Rainbow functions, we derive corresponding modifications to Unruh temperature. Comparing with the observational value of the Unruh temperature, we find the upper bound on $\\eta$ as $~10^{32}$ for positron radiation. %and of the order of $10^{-100}$ for radiation from a black hole. We then derive geodesic equations for different choices of Rainbow functions and also obtain Newtonian limit of these geodesic equations. We find that the changes in the value of maximum acceleration, maximum temperature and Newtonian force equation are dependent on the choices of Rainbow functions.","sentences":["In this paper, we derive maximal acceleration of a massive particle in Rainbow gravity.","Using eight-dimensional phase-space metric compatible with Rainbow gravity, we obtain the maximal acceleration, valid up to first order in the Rainbow gravity parameter $\\eta$. Using the positivity condition on maximal acceleration, we find the upper bound on the Rainbow gravity parameter is of the order of $~10^{22}$ for positron and $10^{-44}$ for a black hole.","After obtaining the expression for maximal acceleration for different choices of Rainbow functions, we derive corresponding modifications to Unruh temperature.","Comparing with the observational value of the Unruh temperature, we find the upper bound on $\\eta$ as $~10^{32}$ for positron radiation.","%and of the order of $10^{-100}$ for radiation from a black hole.","We then derive geodesic equations for different choices of Rainbow functions and also obtain Newtonian limit of these geodesic equations.","We find that the changes in the value of maximum acceleration, maximum temperature and Newtonian force equation are dependent on the choices of Rainbow functions."],"url":"http://arxiv.org/abs/2405.00403v1","category":"gr-qc"}
{"created":"2024-05-01 09:10:27","title":"Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models","abstract":"The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches deliver more performant models, they do not show sufficiently strong generalization ability as the training only relies on the provided demonstrations.   In this paper, we propose the Self-refine Instruction-tuning method that elicits Smaller Language Models to self-refine their abilities. Our approach is based on a two-stage process, where reasoning abilities are first transferred between LLMs and Small Language Models (SLMs) via Instruction-tuning on demonstrations provided by LLMs, and then the instructed models Self-refine their abilities through preference optimization strategies. In particular, the second phase operates refinement heuristics based on the Direct Preference Optimization algorithm, where the SLMs are elicited to deliver a series of reasoning paths by automatically sampling the generated responses and providing rewards using ground truths from the LLMs. Results obtained on commonsense and math reasoning tasks show that this approach significantly outperforms Instruction-tuning in both in-domain and out-domain scenarios, aligning the reasoning abilities of Smaller and Larger Language Models.","sentences":["The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs).","Although these approaches deliver more performant models, they do not show sufficiently strong generalization ability as the training only relies on the provided demonstrations.   ","In this paper, we propose the Self-refine Instruction-tuning method that elicits Smaller Language Models to self-refine their abilities.","Our approach is based on a two-stage process, where reasoning abilities are first transferred between LLMs and Small Language Models (SLMs) via Instruction-tuning on demonstrations provided by LLMs, and then the instructed models Self-refine their abilities through preference optimization strategies.","In particular, the second phase operates refinement heuristics based on the Direct Preference Optimization algorithm, where the SLMs are elicited to deliver a series of reasoning paths by automatically sampling the generated responses and providing rewards using ground truths from the LLMs.","Results obtained on commonsense and math reasoning tasks show that this approach significantly outperforms Instruction-tuning in both in-domain and out-domain scenarios, aligning the reasoning abilities of Smaller and Larger Language Models."],"url":"http://arxiv.org/abs/2405.00402v1","category":"cs.CL"}
{"created":"2024-05-01 08:50:08","title":"Trust Driven On-Demand Scheme for Client Deployment in Federated Learning","abstract":"Containerization technology plays a crucial role in Federated Learning (FL) setups, expanding the pool of potential clients and ensuring the availability of specific subsets for each learning iteration. However, doubts arise about the trustworthiness of devices deployed as clients in FL scenarios, especially when container deployment processes are involved. Addressing these challenges is important, particularly in managing potentially malicious clients capable of disrupting the learning process or compromising the entire model. In our research, we are motivated to integrate a trust element into the client selection and model deployment processes within our system architecture. This is a feature lacking in the initial client selection and deployment mechanism of the On-Demand architecture. We introduce a trust mechanism, named \"Trusted-On-Demand-FL\", which establishes a relationship of trust between the server and the pool of eligible clients. Utilizing Docker in our deployment strategy enables us to monitor and validate participant actions effectively, ensuring strict adherence to agreed-upon protocols while strengthening defenses against unauthorized data access or tampering. Our simulations rely on a continuous user behavior dataset, deploying an optimization model powered by a genetic algorithm to efficiently select clients for participation. By assigning trust values to individual clients and dynamically adjusting these values, combined with penalizing malicious clients through decreased trust scores, our proposed framework identifies and isolates harmful clients. This approach not only reduces disruptions to regular rounds but also minimizes instances of round dismissal, Consequently enhancing both system stability and security.","sentences":["Containerization technology plays a crucial role in Federated Learning (FL) setups, expanding the pool of potential clients and ensuring the availability of specific subsets for each learning iteration.","However, doubts arise about the trustworthiness of devices deployed as clients in FL scenarios, especially when container deployment processes are involved.","Addressing these challenges is important, particularly in managing potentially malicious clients capable of disrupting the learning process or compromising the entire model.","In our research, we are motivated to integrate a trust element into the client selection and model deployment processes within our system architecture.","This is a feature lacking in the initial client selection and deployment mechanism of the On-Demand architecture.","We introduce a trust mechanism, named \"Trusted-On-Demand-FL\", which establishes a relationship of trust between the server and the pool of eligible clients.","Utilizing Docker in our deployment strategy enables us to monitor and validate participant actions effectively, ensuring strict adherence to agreed-upon protocols while strengthening defenses against unauthorized data access or tampering.","Our simulations rely on a continuous user behavior dataset, deploying an optimization model powered by a genetic algorithm to efficiently select clients for participation.","By assigning trust values to individual clients and dynamically adjusting these values, combined with penalizing malicious clients through decreased trust scores, our proposed framework identifies and isolates harmful clients.","This approach not only reduces disruptions to regular rounds but also minimizes instances of round dismissal, Consequently enhancing both system stability and security."],"url":"http://arxiv.org/abs/2405.00395v1","category":"cs.CR"}
{"created":"2024-05-01 08:49:22","title":"Enhancing Mutual Trustworthiness in Federated Learning for Data-Rich Smart Cities","abstract":"Federated learning is a promising collaborative and privacy-preserving machine learning approach in data-rich smart cities. Nevertheless, the inherent heterogeneity of these urban environments presents a significant challenge in selecting trustworthy clients for collaborative model training. The usage of traditional approaches, such as the random client selection technique, poses several threats to the system's integrity due to the possibility of malicious client selection. Primarily, the existing literature focuses on assessing the trustworthiness of clients, neglecting the crucial aspect of trust in federated servers. To bridge this gap, in this work, we propose a novel framework that addresses the mutual trustworthiness in federated learning by considering the trust needs of both the client and the server. Our approach entails: (1) Creating preference functions for servers and clients, allowing them to rank each other based on trust scores, (2) Establishing a reputation-based recommendation system leveraging multiple clients to assess newly connected servers, (3) Assigning credibility scores to recommending devices for better server trustworthiness measurement, (4) Developing a trust assessment mechanism for smart devices using a statistical Interquartile Range (IQR) method, (5) Designing intelligent matching algorithms considering the preferences of both parties. Based on simulation and experimental results, our approach outperforms baseline methods by increasing trust levels, global model accuracy, and reducing non-trustworthy clients in the system.","sentences":["Federated learning is a promising collaborative and privacy-preserving machine learning approach in data-rich smart cities.","Nevertheless, the inherent heterogeneity of these urban environments presents a significant challenge in selecting trustworthy clients for collaborative model training.","The usage of traditional approaches, such as the random client selection technique, poses several threats to the system's integrity due to the possibility of malicious client selection.","Primarily, the existing literature focuses on assessing the trustworthiness of clients, neglecting the crucial aspect of trust in federated servers.","To bridge this gap, in this work, we propose a novel framework that addresses the mutual trustworthiness in federated learning by considering the trust needs of both the client and the server.","Our approach entails: (1) Creating preference functions for servers and clients, allowing them to rank each other based on trust scores, (2) Establishing a reputation-based recommendation system leveraging multiple clients to assess newly connected servers, (3)","Assigning credibility scores to recommending devices for better server trustworthiness measurement, (4) Developing a trust assessment mechanism for smart devices using a statistical Interquartile Range (IQR) method, (5) Designing intelligent matching algorithms considering the preferences of both parties.","Based on simulation and experimental results, our approach outperforms baseline methods by increasing trust levels, global model accuracy, and reducing non-trustworthy clients in the system."],"url":"http://arxiv.org/abs/2405.00394v1","category":"cs.GT"}
{"created":"2024-05-01 08:45:57","title":"Certified Adversarial Robustness of Machine Learning-based Malware Detectors via (De)Randomized Smoothing","abstract":"Deep learning-based malware detection systems are vulnerable to adversarial EXEmples - carefully-crafted malicious programs that evade detection with minimal perturbation. As such, the community is dedicating effort to develop mechanisms to defend against adversarial EXEmples. However, current randomized smoothing-based defenses are still vulnerable to attacks that inject blocks of adversarial content. In this paper, we introduce a certifiable defense against patch attacks that guarantees, for a given executable and an adversarial patch size, no adversarial EXEmple exist. Our method is inspired by (de)randomized smoothing which provides deterministic robustness certificates. During training, a base classifier is trained using subsets of continguous bytes. At inference time, our defense splits the executable into non-overlapping chunks, classifies each chunk independently, and computes the final prediction through majority voting to minimize the influence of injected content. Furthermore, we introduce a preprocessing step that fixes the size of the sections and headers to a multiple of the chunk size. As a consequence, the injected content is confined to an integer number of chunks without tampering the other chunks containing the real bytes of the input examples, allowing us to extend our certified robustness guarantees to content insertion attacks. We perform an extensive ablation study, by comparing our defense with randomized smoothing-based defenses against a plethora of content manipulation attacks and neural network architectures. Results show that our method exhibits unmatched robustness against strong content-insertion attacks, outperforming randomized smoothing-based defenses in the literature.","sentences":["Deep learning-based malware detection systems are vulnerable to adversarial EXEmples - carefully-crafted malicious programs that evade detection with minimal perturbation.","As such, the community is dedicating effort to develop mechanisms to defend against adversarial EXEmples.","However, current randomized smoothing-based defenses are still vulnerable to attacks that inject blocks of adversarial content.","In this paper, we introduce a certifiable defense against patch attacks that guarantees, for a given executable and an adversarial patch size, no adversarial EXEmple exist.","Our method is inspired by (de)randomized smoothing which provides deterministic robustness certificates.","During training, a base classifier is trained using subsets of continguous bytes.","At inference time, our defense splits the executable into non-overlapping chunks, classifies each chunk independently, and computes the final prediction through majority voting to minimize the influence of injected content.","Furthermore, we introduce a preprocessing step that fixes the size of the sections and headers to a multiple of the chunk size.","As a consequence, the injected content is confined to an integer number of chunks without tampering the other chunks containing the real bytes of the input examples, allowing us to extend our certified robustness guarantees to content insertion attacks.","We perform an extensive ablation study, by comparing our defense with randomized smoothing-based defenses against a plethora of content manipulation attacks and neural network architectures.","Results show that our method exhibits unmatched robustness against strong content-insertion attacks, outperforming randomized smoothing-based defenses in the literature."],"url":"http://arxiv.org/abs/2405.00392v1","category":"cs.CR"}
{"created":"2024-05-01 08:44:50","title":"Beamforming Inferring by Conditional WGAN-GP for Holographic Antenna Arrays","abstract":"The beamforming technology with large holographic antenna arrays is one of the key enablers for the next generation of wireless systems, which can significantly improve the spectral efficiency. However, the deployment of large antenna arrays implies high algorithm complexity and resource overhead at both receiver and transmitter ends. To address this issue, advanced technologies such as artificial intelligence have been developed to reduce beamforming overhead. Intuitively, if we can implement the near-optimal beamforming only using a tiny subset of the all channel information, the overhead for channel estimation and beamforming would be reduced significantly compared with the traditional beamforming methods that usually need full channel information and the inversion of large dimensional matrix. In light of this idea, we propose a novel scheme that utilizes Wasserstein generative adversarial network with gradient penalty to infer the full beamforming matrices based on very little of channel information. Simulation results confirm that it can accomplish comparable performance with the weighted minimum mean-square error algorithm, while reducing the overhead by over 50%.","sentences":["The beamforming technology with large holographic antenna arrays is one of the key enablers for the next generation of wireless systems, which can significantly improve the spectral efficiency.","However, the deployment of large antenna arrays implies high algorithm complexity and resource overhead at both receiver and transmitter ends.","To address this issue, advanced technologies such as artificial intelligence have been developed to reduce beamforming overhead.","Intuitively, if we can implement the near-optimal beamforming only using a tiny subset of the all channel information, the overhead for channel estimation and beamforming would be reduced significantly compared with the traditional beamforming methods that usually need full channel information and the inversion of large dimensional matrix.","In light of this idea, we propose a novel scheme that utilizes Wasserstein generative adversarial network with gradient penalty to infer the full beamforming matrices based on very little of channel information.","Simulation results confirm that it can accomplish comparable performance with the weighted minimum mean-square error algorithm, while reducing the overhead by over 50%."],"url":"http://arxiv.org/abs/2405.00391v1","category":"cs.IT"}
{"created":"2024-05-01 08:44:44","title":"CofiPara: A Coarse-to-fine Paradigm for Multimodal Sarcasm Target Identification with Large Multimodal Models","abstract":"Social media abounds with multimodal sarcasm, and identifying sarcasm targets is particularly challenging due to the implicit incongruity not directly evident in the text and image modalities. Current methods for Multimodal Sarcasm Target Identification (MSTI) predominantly focus on superficial indicators in an end-to-end manner, overlooking the nuanced understanding of multimodal sarcasm conveyed through both the text and image. This paper proposes a versatile MSTI framework with a coarse-to-fine paradigm, by augmenting sarcasm explainability with reasoning and pre-training knowledge. Inspired by the powerful capacity of Large Multimodal Models (LMMs) on multimodal reasoning, we first engage LMMs to generate competing rationales for coarser-grained pre-training of a small language model on multimodal sarcasm detection. We then propose fine-tuning the model for finer-grained sarcasm target identification. Our framework is thus empowered to adeptly unveil the intricate targets within multimodal sarcasm and mitigate the negative impact posed by potential noise inherently in LMMs. Experimental results demonstrate that our model far outperforms state-of-the-art MSTI methods, and markedly exhibits explainability in deciphering sarcasm as well.","sentences":["Social media abounds with multimodal sarcasm, and identifying sarcasm targets is particularly challenging due to the implicit incongruity not directly evident in the text and image modalities.","Current methods for Multimodal Sarcasm Target Identification (MSTI) predominantly focus on superficial indicators in an end-to-end manner, overlooking the nuanced understanding of multimodal sarcasm conveyed through both the text and image.","This paper proposes a versatile MSTI framework with a coarse-to-fine paradigm, by augmenting sarcasm explainability with reasoning and pre-training knowledge.","Inspired by the powerful capacity of Large Multimodal Models (LMMs) on multimodal reasoning, we first engage LMMs to generate competing rationales for coarser-grained pre-training of a small language model on multimodal sarcasm detection.","We then propose fine-tuning the model for finer-grained sarcasm target identification.","Our framework is thus empowered to adeptly unveil the intricate targets within multimodal sarcasm and mitigate the negative impact posed by potential noise inherently in LMMs.","Experimental results demonstrate that our model far outperforms state-of-the-art MSTI methods, and markedly exhibits explainability in deciphering sarcasm as well."],"url":"http://arxiv.org/abs/2405.00390v1","category":"cs.CL"}
{"created":"2024-05-01 08:42:22","title":"Employing Federated Learning for Training Autonomous HVAC Systems","abstract":"Buildings account for 40 % of global energy consumption. A considerable portion of building energy consumption stems from heating, ventilation, and air conditioning (HVAC), and thus implementing smart, energy-efficient HVAC systems has the potential to significantly impact the course of climate change. In recent years, model-free reinforcement learning algorithms have been increasingly assessed for this purpose due to their ability to learn and adapt purely from experience. They have been shown to outperform classical controllers in terms of energy cost and consumption, as well as thermal comfort. However, their weakness lies in their relatively poor data efficiency, requiring long periods of training to reach acceptable policies, making them inapplicable to real-world controllers directly. Hence, common research goals are to improve the learning speed, as well as to improve their ability to generalize, in order to facilitate transfer learning to unseen building environments. In this paper, we take a federated learning approach to training the reinforcement learning controller of an HVAC system. A global control policy is learned by aggregating local policies trained on multiple data centers located in different climate zones. The goal of the policy is to simultaneously minimize energy consumption and maximize thermal comfort. The federated optimization strategy indirectly increases both the rate at which experience data is collected and the variation in the data. We demonstrate through experimental evaluation that these effects lead to a faster learning speed, as well as greater generalization capabilities in the federated policy compared to any individually trained policy.","sentences":["Buildings account for 40 % of global energy consumption.","A considerable portion of building energy consumption stems from heating, ventilation, and air conditioning (HVAC), and thus implementing smart, energy-efficient HVAC systems has the potential to significantly impact the course of climate change.","In recent years, model-free reinforcement learning algorithms have been increasingly assessed for this purpose due to their ability to learn and adapt purely from experience.","They have been shown to outperform classical controllers in terms of energy cost and consumption, as well as thermal comfort.","However, their weakness lies in their relatively poor data efficiency, requiring long periods of training to reach acceptable policies, making them inapplicable to real-world controllers directly.","Hence, common research goals are to improve the learning speed, as well as to improve their ability to generalize, in order to facilitate transfer learning to unseen building environments.","In this paper, we take a federated learning approach to training the reinforcement learning controller of an HVAC system.","A global control policy is learned by aggregating local policies trained on multiple data centers located in different climate zones.","The goal of the policy is to simultaneously minimize energy consumption and maximize thermal comfort.","The federated optimization strategy indirectly increases both the rate at which experience data is collected and the variation in the data.","We demonstrate through experimental evaluation that these effects lead to a faster learning speed, as well as greater generalization capabilities in the federated policy compared to any individually trained policy."],"url":"http://arxiv.org/abs/2405.00389v1","category":"math.OC"}
{"created":"2024-05-01 08:38:45","title":"Delamination Detection in Layered Waveguides using Ostrovsky Wave Packets","abstract":"We examine the scattering of Ostrovsky wave packets, generated from an incident solitary wave, in a two layered waveguide with a delamination in the centre and soft (imperfect) bonding either side of the centre. The layers of the waveguide are assumed to consist of different materials, and the strains are described by a system of coupled Boussinesq equations. A semi-analytical approach consisting of matched asymptotic multiple-scale expansions is applied, leading to Ostrovsky equations in soft bonded regions and Korteweg-de Vries equations in the delaminated region. This semi-analytical method has good agreement with direct numerical simulations, validating the approach.   In the delaminated regions, Ostrovsky wave packets evolve into a train of solitary waves, which subsequently evolve into Ostrovsky wave packets in the second bonded region. Analysis of the phase shift in the wave packet, introduced from the delaminated region, allows us to predict both the position and the length of the delamination; the first time this has been achieved using nonlinear waves. These results motivate experiments to validate the theoretical results, with the aim of creating a tool to monitor the integrity of layered structures.","sentences":["We examine the scattering of Ostrovsky wave packets, generated from an incident solitary wave, in a two layered waveguide with a delamination in the centre and soft (imperfect) bonding either side of the centre.","The layers of the waveguide are assumed to consist of different materials, and the strains are described by a system of coupled Boussinesq equations.","A semi-analytical approach consisting of matched asymptotic multiple-scale expansions is applied, leading to Ostrovsky equations in soft bonded regions and Korteweg-de Vries equations in the delaminated region.","This semi-analytical method has good agreement with direct numerical simulations, validating the approach.   ","In the delaminated regions, Ostrovsky wave packets evolve into a train of solitary waves, which subsequently evolve into Ostrovsky wave packets in the second bonded region.","Analysis of the phase shift in the wave packet, introduced from the delaminated region, allows us to predict both the position and the length of the delamination; the first time this has been achieved using nonlinear waves.","These results motivate experiments to validate the theoretical results, with the aim of creating a tool to monitor the integrity of layered structures."],"url":"http://arxiv.org/abs/2405.00388v1","category":"nlin.PS"}
{"created":"2024-05-01 08:30:10","title":"Learning Tactile Insertion in the Real World","abstract":"Humans have exceptional tactile sensing capabilities, which they can leverage to solve challenging, partially observable tasks that cannot be solved from visual observation alone. Research in tactile sensing attempts to unlock this new input modality for robots. Lately, these sensors have become cheaper and, thus, widely available. At the same time, the question of how to integrate them into control loops is still an active area of research, with central challenges being partial observability and the contact-rich nature of manipulation tasks. In this study, we propose to use Reinforcement Learning to learn an end-to-end policy, mapping directly from tactile sensor readings to actions. Specifically, we use Dreamer-v3 on a challenging, partially observable robotic insertion task with a Franka Research 3, both in simulation and on a real system. For the real setup, we built a robotic platform capable of resetting itself fully autonomously, allowing for extensive training runs without human supervision. Our preliminary results indicate that Dreamer is capable of utilizing tactile inputs to solve robotic manipulation tasks in simulation and reality. Furthermore, we find that providing the robot with tactile feedback generally improves task performance, though, in our setup, we do not yet include other sensing modalities. In the future, we plan to utilize our platform to evaluate a wide range of other Reinforcement Learning algorithms on tactile tasks.","sentences":["Humans have exceptional tactile sensing capabilities, which they can leverage to solve challenging, partially observable tasks that cannot be solved from visual observation alone.","Research in tactile sensing attempts to unlock this new input modality for robots.","Lately, these sensors have become cheaper and, thus, widely available.","At the same time, the question of how to integrate them into control loops is still an active area of research, with central challenges being partial observability and the contact-rich nature of manipulation tasks.","In this study, we propose to use Reinforcement Learning to learn an end-to-end policy, mapping directly from tactile sensor readings to actions.","Specifically, we use Dreamer-v3 on a challenging, partially observable robotic insertion task with a Franka Research 3, both in simulation and on a real system.","For the real setup, we built a robotic platform capable of resetting itself fully autonomously, allowing for extensive training runs without human supervision.","Our preliminary results indicate that Dreamer is capable of utilizing tactile inputs to solve robotic manipulation tasks in simulation and reality.","Furthermore, we find that providing the robot with tactile feedback generally improves task performance, though, in our setup, we do not yet include other sensing modalities.","In the future, we plan to utilize our platform to evaluate a wide range of other Reinforcement Learning algorithms on tactile tasks."],"url":"http://arxiv.org/abs/2405.00383v1","category":"cs.RO"}
{"created":"2024-05-01 08:18:18","title":"Multi-Path and Multi-Particle Tests of the Dimensionality of Quantum Mechanics","abstract":"The axioms of quantum mechanics provide limited information regarding the structure of the Hilbert space, such as the underlying number system. The latter is generally regarded as complex, but generalizations of complex numbers, so-called hyper-complex numbers, cannot be ruled out in theory. Therefore, specialized experiments to test for hyper-complex quantum mechanics are needed. To date, experimental tests are limited to single-particle interference exploiting a closed phase relation in three-path interference called the Peres test. In this work, we reveal an elegant matrix formalism to derive the Peres test putting it on a solid mathematical ground. On this basis, we introduce multi-path and multi-particle interference tests, which provide a direct probe for the dimensionality of the number system of quantum mechanics.","sentences":["The axioms of quantum mechanics provide limited information regarding the structure of the Hilbert space, such as the underlying number system.","The latter is generally regarded as complex, but generalizations of complex numbers, so-called hyper-complex numbers, cannot be ruled out in theory.","Therefore, specialized experiments to test for hyper-complex quantum mechanics are needed.","To date, experimental tests are limited to single-particle interference exploiting a closed phase relation in three-path interference called the Peres test.","In this work, we reveal an elegant matrix formalism to derive the Peres test putting it on a solid mathematical ground.","On this basis, we introduce multi-path and multi-particle interference tests, which provide a direct probe for the dimensionality of the number system of quantum mechanics."],"url":"http://arxiv.org/abs/2405.00380v1","category":"quant-ph"}
{"created":"2024-05-01 08:17:43","title":"Adaptive Bidirectional Displacement for Semi-Supervised Medical Image Segmentation","abstract":"Consistency learning is a central strategy to tackle unlabeled data in semi-supervised medical image segmentation (SSMIS), which enforces the model to produce consistent predictions under the perturbation. However, most current approaches solely focus on utilizing a specific single perturbation, which can only cope with limited cases, while employing multiple perturbations simultaneously is hard to guarantee the quality of consistency learning. In this paper, we propose an Adaptive Bidirectional Displacement (ABD) approach to solve the above challenge. Specifically, we first design a bidirectional patch displacement based on reliable prediction confidence for unlabeled data to generate new samples, which can effectively suppress uncontrollable regions and still retain the influence of input perturbations. Meanwhile, to enforce the model to learn the potentially uncontrollable content, a bidirectional displacement operation with inverse confidence is proposed for the labeled images, which generates samples with more unreliable information to facilitate model learning. Extensive experiments show that ABD achieves new state-of-the-art performances for SSMIS, significantly improving different baselines. Source code is available at https://github.com/chy-upc/ABD.","sentences":["Consistency learning is a central strategy to tackle unlabeled data in semi-supervised medical image segmentation (SSMIS), which enforces the model to produce consistent predictions under the perturbation.","However, most current approaches solely focus on utilizing a specific single perturbation, which can only cope with limited cases, while employing multiple perturbations simultaneously is hard to guarantee the quality of consistency learning.","In this paper, we propose an Adaptive Bidirectional Displacement (ABD) approach to solve the above challenge.","Specifically, we first design a bidirectional patch displacement based on reliable prediction confidence for unlabeled data to generate new samples, which can effectively suppress uncontrollable regions and still retain the influence of input perturbations.","Meanwhile, to enforce the model to learn the potentially uncontrollable content, a bidirectional displacement operation with inverse confidence is proposed for the labeled images, which generates samples with more unreliable information to facilitate model learning.","Extensive experiments show that ABD achieves new state-of-the-art performances for SSMIS, significantly improving different baselines.","Source code is available at https://github.com/chy-upc/ABD."],"url":"http://arxiv.org/abs/2405.00378v1","category":"cs.CV"}
{"created":"2024-05-01 08:06:13","title":"EPOCHS Paper V. The dependence of galaxy formation on galaxy structure at z < 7 from JWST observations","abstract":"We measure the broad impact of galaxy structure on galaxy formation by examining the ongoing star formation and integrated star formation history as revealed through the stellar masses of galaxies at $z < 7$ based on JWST CEERS data from the Extended Groth Strip (EGS). Using the morphological catalog of 3965 visually classified JWST galaxies from Ferreira et al. (2023), we investigate the evolution of stars, and when they form, as a function of morphological type as well as galaxies classified as passive and starburst through spectral energy distributions. Although disk galaxies dominate the structures of galaxies at $z < 7$, we find that these disks are in general either `passive', or on the main-sequence of star formation, and do not contain a large population of starburst galaxies. We also find no significant correlation between morphological type and the star formation rate or colours of galaxies at $z < 7$. In fact, we find that the morphologically classified `spheroids' tend to be blue and are not found to be predominately passive systems at $z > 1.5$. We also find that the stellar mass function for disk galaxies does not evolve significantly during this time, whereas other galaxy types, such as the peculiar population, evolve dramatically, declining at lower redshifts. This indicates that massive peculiars are more common at higher redshifts. We further find that up to $z \\sim 7$, the specific star formation rate (sSFR) does not vary with visual morphology, but strongly depends on stellar mass and internal galaxy mass density. This demonstrates that at early epochs galaxy assembly is a mass-driven, rather than a morphologically-driven, process. Quenching of star formation is therefore a mass-dominated process throughout the universe's history, likely due to the presence of supermassive black holes.","sentences":["We measure the broad impact of galaxy structure on galaxy formation by examining the ongoing star formation and integrated star formation history as revealed through the stellar masses of galaxies at $z < 7$ based on JWST CEERS data from the Extended Groth Strip (EGS).","Using the morphological catalog of 3965 visually classified JWST galaxies from Ferreira et al. (2023), we investigate the evolution of stars, and when they form, as a function of morphological type as well as galaxies classified as passive and starburst through spectral energy distributions.","Although disk galaxies dominate the structures of galaxies at $z < 7$, we find that these disks are in general either `passive', or on the main-sequence of star formation, and do not contain a large population of starburst galaxies.","We also find no significant correlation between morphological type and the star formation rate or colours of galaxies at $z < 7$.","In fact, we find that the morphologically classified `spheroids' tend to be blue and are not found to be predominately passive systems at $z > 1.5$. We also find that the stellar mass function for disk galaxies does not evolve significantly during this time, whereas other galaxy types, such as the peculiar population, evolve dramatically, declining at lower redshifts.","This indicates that massive peculiars are more common at higher redshifts.","We further find that up to $z \\sim 7$, the specific star formation rate (sSFR) does not vary with visual morphology, but strongly depends on stellar mass and internal galaxy mass density.","This demonstrates that at early epochs galaxy assembly is a mass-driven, rather than a morphologically-driven, process.","Quenching of star formation is therefore a mass-dominated process throughout the universe's history, likely due to the presence of supermassive black holes."],"url":"http://arxiv.org/abs/2405.00376v1","category":"astro-ph.GA"}
{"created":"2024-05-01 08:03:50","title":"Study of Charged Cylindrical Collapse in $f(\\mathcal{R},\\mathcal{T},\\mathcal{Q})$ Gravity","abstract":"This paper investigates the effects of electromagnetic field on the gravitational collapse in $f(\\mathcal{R},\\mathcal{T},\\mathcal{Q})$ theory, where $\\mathcal{Q} = \\mathcal{R}_{\\varphi\\vartheta} \\mathcal{T}^{\\varphi\\vartheta}$. For this, we assume dynamical cylindrically symmetric self-gravitating geometry which is coupled with generalized anisotropic matter distribution as well as dissipation flux. We adopt the model $\\mathcal{R}+\\Phi\\sqrt{\\mathcal{T}}+\\Psi\\mathcal{Q}$ to formulate the corresponding dynamical and transport equations by employing the Misner-Sharp as well as M\\\"{u}ler-Israel Stewart formalisms, where $\\Phi$ and $\\Psi$ are real-valued coupling constants. The influence of state variables, heat dissipation, charge and the bulk viscosity on the collapsing phenomenon is then studied by establishing some relations between these evolution equations. Moreover, the Weyl scalar and the modified field equations are expressed in terms of each other. We apply some constraints on the considered modified model and the fluid configuration to obtain conformally flat spacetime. Finally, we address different cases to check how the modified corrections and charge affect the collapse rate of cylindrical matter source.","sentences":["This paper investigates the effects of electromagnetic field on the gravitational collapse in $f(\\mathcal{R},\\mathcal{T},\\mathcal{Q})$ theory, where $\\mathcal{Q} = \\mathcal{R}_{\\varphi\\vartheta} \\mathcal{T}^{\\varphi\\vartheta}$. For this, we assume dynamical cylindrically symmetric self-gravitating geometry which is coupled with generalized anisotropic matter distribution as well as dissipation flux.","We adopt the model $\\mathcal{R}+\\Phi\\sqrt{\\mathcal{T}}+\\Psi\\mathcal{Q}$ to formulate the corresponding dynamical and transport equations by employing the Misner-Sharp as well as M\\\"{u}ler-Israel Stewart formalisms, where $\\Phi$ and $\\Psi$ are real-valued coupling constants.","The influence of state variables, heat dissipation, charge and the bulk viscosity on the collapsing phenomenon is then studied by establishing some relations between these evolution equations.","Moreover, the Weyl scalar and the modified field equations are expressed in terms of each other.","We apply some constraints on the considered modified model and the fluid configuration to obtain conformally flat spacetime.","Finally, we address different cases to check how the modified corrections and charge affect the collapse rate of cylindrical matter source."],"url":"http://arxiv.org/abs/2405.00374v1","category":"gr-qc"}
{"created":"2024-05-01 08:01:18","title":"A Zero-Sum Differential Game with Exit Time","abstract":"The paper is concerned with a zero-sum differential game in the case where a payoff is determined by the exit time, that is, the first time when the system leaves the game domain. Additionally, we assume that a part of domain's boundary is a lifeline where the payoff is infinite. Hereby, the examined problem generalizes the well-known time-optimal problem as well as time-optimal problem with lifeline. The main result of the paper relies on the solution to the Direchlet problem for the Hamilton-Jacobi equation associated with the game with exit time. We prove the existence of the value function for examined problem and construct suboptimal feedback strategies under assumption that the associated Dirichlet problem for the Hamilton-Jacobi equation admits a viscosity/minimax solution. Additionally, we derive a sufficient condition of existence result to this Dirichlet problem.","sentences":["The paper is concerned with a zero-sum differential game in the case where a payoff is determined by the exit time, that is, the first time when the system leaves the game domain.","Additionally, we assume that a part of domain's boundary is a lifeline where the payoff is infinite.","Hereby, the examined problem generalizes the well-known time-optimal problem as well as time-optimal problem with lifeline.","The main result of the paper relies on the solution to the Direchlet problem for the Hamilton-Jacobi equation associated with the game with exit time.","We prove the existence of the value function for examined problem and construct suboptimal feedback strategies under assumption that the associated Dirichlet problem for the Hamilton-Jacobi equation admits a viscosity/minimax solution.","Additionally, we derive a sufficient condition of existence result to this Dirichlet problem."],"url":"http://arxiv.org/abs/2405.00371v1","category":"math.OC"}
{"created":"2024-05-01 07:44:28","title":"Distance Sampling-based Paraphraser Leveraging ChatGPT for Text Data Manipulation","abstract":"There has been growing interest in audio-language retrieval research, where the objective is to establish the correlation between audio and text modalities. However, most audio-text paired datasets often lack rich expression of the text data compared to the audio samples. One of the significant challenges facing audio-text datasets is the presence of similar or identical captions despite different audio samples. Therefore, under many-to-one mapping conditions, audio-text datasets lead to poor performance of retrieval tasks. In this paper, we propose a novel approach to tackle the data imbalance problem in audio-language retrieval task. To overcome the limitation, we introduce a method that employs a distance sampling-based paraphraser leveraging ChatGPT, utilizing distance function to generate a controllable distribution of manipulated text data. For a set of sentences with the same context, the distance is used to calculate a degree of manipulation for any two sentences, and ChatGPT's few-shot prompting is performed using a text cluster with a similar distance defined by the Jaccard similarity. Therefore, ChatGPT, when applied to few-shot prompting with text clusters, can adjust the diversity of the manipulated text based on the distance. The proposed approach is shown to significantly enhance performance in audio-text retrieval, outperforming conventional text augmentation techniques.","sentences":["There has been growing interest in audio-language retrieval research, where the objective is to establish the correlation between audio and text modalities.","However, most audio-text paired datasets often lack rich expression of the text data compared to the audio samples.","One of the significant challenges facing audio-text datasets is the presence of similar or identical captions despite different audio samples.","Therefore, under many-to-one mapping conditions, audio-text datasets lead to poor performance of retrieval tasks.","In this paper, we propose a novel approach to tackle the data imbalance problem in audio-language retrieval task.","To overcome the limitation, we introduce a method that employs a distance sampling-based paraphraser leveraging ChatGPT, utilizing distance function to generate a controllable distribution of manipulated text data.","For a set of sentences with the same context, the distance is used to calculate a degree of manipulation for any two sentences, and ChatGPT's few-shot prompting is performed using a text cluster with a similar distance defined by the Jaccard similarity.","Therefore, ChatGPT, when applied to few-shot prompting with text clusters, can adjust the diversity of the manipulated text based on the distance.","The proposed approach is shown to significantly enhance performance in audio-text retrieval, outperforming conventional text augmentation techniques."],"url":"http://arxiv.org/abs/2405.00367v1","category":"cs.IR"}
{"created":"2024-05-01 07:43:26","title":"L0-regularized compressed sensing with Mean-field Coherent Ising Machines","abstract":"Coherent Ising Machine (CIM) is a network of optical parametric oscillators that solves combinatorial optimization problems by finding the ground state of an Ising Hamiltonian. As a practical application of CIM, Aonishi et al. proposed a quantum-classical hybrid system to solve optimization problems of L0-regularization-based compressed sensing (L0RBCS). Gunathilaka et al. has further enhanced the accuracy of the system. However, the computationally expensive CIM's stochastic differential equations (SDEs) limit the use of digital hardware implementations. As an alternative to Gunathilaka et al.'s CIM SDEs used previously, we propose using the mean-field CIM (MF-CIM) model, which is a physics-inspired heuristic solver without quantum noise. MF-CIM surmounts the high computational cost due to the simple nature of the differential equations (DEs). Furthermore, our results indicate that the proposed model has similar performance to physically accurate SDEs in both artificial and magnetic resonance imaging data, paving the way for implementing CIM-based L0RBCS on digital hardware such as Field Programmable Gate Arrays (FPGAs).","sentences":["Coherent Ising Machine (CIM) is a network of optical parametric oscillators that solves combinatorial optimization problems by finding the ground state of an Ising Hamiltonian.","As a practical application of CIM, Aonishi et al. proposed a quantum-classical hybrid system to solve optimization problems of L0-regularization-based compressed sensing (L0RBCS).","Gunathilaka et al. has further enhanced the accuracy of the system.","However, the computationally expensive CIM's stochastic differential equations (SDEs) limit the use of digital hardware implementations.","As an alternative to Gunathilaka et al.'s CIM SDEs used previously, we propose using the mean-field CIM (MF-CIM) model, which is a physics-inspired heuristic solver without quantum noise.","MF-CIM surmounts the high computational cost due to the simple nature of the differential equations (DEs).","Furthermore, our results indicate that the proposed model has similar performance to physically accurate SDEs in both artificial and magnetic resonance imaging data, paving the way for implementing CIM-based L0RBCS on digital hardware such as Field Programmable Gate Arrays (FPGAs)."],"url":"http://arxiv.org/abs/2405.00366v1","category":"cs.ET"}
{"created":"2024-05-01 07:42:46","title":"Robust Continuous-Time Beam Tracking with Liquid Neural Network","abstract":"Millimeter-wave (mmWave) technology is increasingly recognized as a pivotal technology of the sixth-generation communication networks due to the large amounts of available spectrum at high frequencies. However, the huge overhead associated with beam training imposes a significant challenge in mmWave communications, particularly in urban environments with high background noise. To reduce this high overhead, we propose a novel solution for robust continuous-time beam tracking with liquid neural network, which dynamically adjust the narrow mmWave beams to ensure real-time beam alignment with mobile users. Through extensive simulations, we validate the effectiveness of our proposed method and demonstrate its superiority over existing state-of-the-art deep-learning-based approaches. Specifically, our scheme achieves at most 46.9% higher normalized spectral efficiency than the baselines when the user is moving at 5 m/s, demonstrating the potential of liquid neural networks to enhance mmWave mobile communication performance.","sentences":["Millimeter-wave (mmWave) technology is increasingly recognized as a pivotal technology of the sixth-generation communication networks due to the large amounts of available spectrum at high frequencies.","However, the huge overhead associated with beam training imposes a significant challenge in mmWave communications, particularly in urban environments with high background noise.","To reduce this high overhead, we propose a novel solution for robust continuous-time beam tracking with liquid neural network, which dynamically adjust the narrow mmWave beams to ensure real-time beam alignment with mobile users.","Through extensive simulations, we validate the effectiveness of our proposed method and demonstrate its superiority over existing state-of-the-art deep-learning-based approaches.","Specifically, our scheme achieves at most 46.9% higher normalized spectral efficiency than the baselines when the user is moving at 5 m/s, demonstrating the potential of liquid neural networks to enhance mmWave mobile communication performance."],"url":"http://arxiv.org/abs/2405.00365v1","category":"cs.IT"}
{"created":"2024-05-01 07:38:52","title":"Competing bootstrap processes on the random graph $G(n,p)$","abstract":"We consider a generalization of classic bootstrap percolation in which two competing processes concurrently evolve on the same graph $G(n,p)$. Nodes can be in one of three states, conveniently represented by different colors: red, black and white. Initially, a given number $a_R$ of active red nodes (red seeds) are selected uniformly at random among the $n$ nodes. Similarly, a given number $a_B$ of active black nodes (black seeds) are selected uniformly at random among the other $n-a_R$ nodes. All remaining nodes are initially white (inactive). White nodes wake up at times dictated by independent Poisson clocks of rate 1. When a white node wakes up, it checks the state of its neighbors: if the number of red (black) neighbors exceeds the number of black (red) neighbors by a fixed amount $r \\geq 2$, the node becomes an active red (black) node, and remains so forever. The parameters of the model are, besides $r$ (fixed) and $n$ (tending to $\\infty$), the numbers $a_R$ ($a_B$) of initial red (black) seeds, and the edge existence probability $p=p(n)$. We study the size $A^*_R$ ($A^*_B$) of the final set of active red (black) nodes, identifying different regimes which are analyzed under suitable time-scales, allowing us to obtain detailed (asymptotic) temporal dynamics of the two concurrent activation processes.","sentences":["We consider a generalization of classic bootstrap percolation in which two competing processes concurrently evolve on the same graph $G(n,p)$. Nodes can be in one of three states, conveniently represented by different colors: red, black and white.","Initially, a given number $a_R$ of active red nodes (red seeds) are selected uniformly at random among the $n$ nodes.","Similarly, a given number $a_B$ of active black nodes (black seeds) are selected uniformly at random among the other $n-a_R$ nodes.","All remaining nodes are initially white (inactive).","White nodes wake up at times dictated by independent Poisson clocks of rate 1.","When a white node wakes up, it checks the state of its neighbors: if the number of red (black) neighbors exceeds the number of black (red) neighbors by a fixed amount $r \\geq 2$, the node becomes an active red (black) node, and remains so forever.","The parameters of the model are, besides $r$ (fixed) and $n$ (tending to $\\infty$), the numbers $a_R$ ($a_B$) of initial red (black) seeds, and the edge existence probability $p=p(n)$. We study the size $A^*_R$ ($A^*_B$) of the final set of active red (black) nodes, identifying different regimes which are analyzed under suitable time-scales, allowing us to obtain detailed (asymptotic) temporal dynamics of the two concurrent activation processes."],"url":"http://arxiv.org/abs/2405.00363v1","category":"math.PR"}
{"created":"2024-05-01 07:38:41","title":"Implicit Swept Volume SDF: Enabling Continuous Collision-Free Trajectory Generation for Arbitrary Shapes","abstract":"In the field of trajectory generation for objects, ensuring continuous collision-free motion remains a huge challenge, especially for non-convex geometries and complex environments. Previous methods either oversimplify object shapes, which results in a sacrifice of feasible space or rely on discrete sampling, which suffers from the \"tunnel effect\". To address these limitations, we propose a novel hierarchical trajectory generation pipeline, which utilizes the Swept Volume Signed Distance Field (SVSDF) to guide trajectory optimization for Continuous Collision Avoidance (CCA). Our interdisciplinary approach, blending techniques from graphics and robotics, exhibits outstanding effectiveness in solving this problem. We formulate the computation of the SVSDF as a Generalized Semi-Infinite Programming model, and we solve for the numerical solutions at query points implicitly, thereby eliminating the need for explicit reconstruction of the surface. Our algorithm has been validated in a variety of complex scenarios and applies to robots of various dynamics, including both rigid and deformable shapes. It demonstrates exceptional universality and superior CCA performance compared to typical algorithms. The code will be released at https://github.com/ZJU-FAST-Lab/Implicit-SVSDF-Planner for the benefit of the community.","sentences":["In the field of trajectory generation for objects, ensuring continuous collision-free motion remains a huge challenge, especially for non-convex geometries and complex environments.","Previous methods either oversimplify object shapes, which results in a sacrifice of feasible space or rely on discrete sampling, which suffers from the \"tunnel effect\".","To address these limitations, we propose a novel hierarchical trajectory generation pipeline, which utilizes the Swept Volume Signed Distance Field (SVSDF) to guide trajectory optimization for Continuous Collision Avoidance (CCA).","Our interdisciplinary approach, blending techniques from graphics and robotics, exhibits outstanding effectiveness in solving this problem.","We formulate the computation of the SVSDF as a Generalized Semi-Infinite Programming model, and we solve for the numerical solutions at query points implicitly, thereby eliminating the need for explicit reconstruction of the surface.","Our algorithm has been validated in a variety of complex scenarios and applies to robots of various dynamics, including both rigid and deformable shapes.","It demonstrates exceptional universality and superior CCA performance compared to typical algorithms.","The code will be released at https://github.com/ZJU-FAST-Lab/Implicit-SVSDF-Planner for the benefit of the community."],"url":"http://arxiv.org/abs/2405.00362v1","category":"cs.RO"}
{"created":"2024-05-01 07:31:25","title":"Heat capacity and quantum compressibility of dynamical spacetimes with thermal particle creation","abstract":"This work continues the investigation in two recent papers on the quantum thermodynamics of spacetimes, 1) placing what was studied in [1] for thermal quantum fields in the context of early universe cosmology, and 2) extending the considerations of vacuum compressibility of dynamical spaces treated in [2] to dynamical spacetimes with thermal quantum fields. We begin with a warning that thermal equilibrium condition is not guaranteed to exist or maintained in a dynamical setting and thus finite temperature quantum field theory in cosmological spacetimes needs more careful considerations than what is often described in textbooks. A full description requires nonequilibrium quantum field theory in dynamical spacetimes using `in-in' techniques. A more manageable subclass of dynamics is where thermal equilibrium conditions are established at both the beginning and the end of evolution are both well defined. Here we shall assume an in-vacuum state. It has been shown that if the intervening dynamics has an initial period of exponential expansion, such as in inflationary cosmology, particles created from the parametric amplification of the vacuum fluctuations in the initial vacuum will have a thermal spectrum measured at the out-state. Under these conditions finite temperature field theory can be applied to calculate the quantum thermodynamic quantities. Here we consider a massive conformal scalar field in a closed four-dimensional Friedmann-Lemaitre-Robertson-Walker universe based on the simple analytically solvable Bernard-Duncan model. We calculate the energy density of particles created from an in-vacuum and derive the partition function. From the free energy we then derive the heat capacity and the quantum compressibility of the spacetimes with thermal particle creation. We end with some discussions and suggestions for further work in this program of studies.","sentences":["This work continues the investigation in two recent papers on the quantum thermodynamics of spacetimes, 1) placing what was studied in [1] for thermal quantum fields in the context of early universe cosmology, and 2) extending the considerations of vacuum compressibility of dynamical spaces treated in [2] to dynamical spacetimes with thermal quantum fields.","We begin with a warning that thermal equilibrium condition is not guaranteed to exist or maintained in a dynamical setting and thus finite temperature quantum field theory in cosmological spacetimes needs more careful considerations than what is often described in textbooks.","A full description requires nonequilibrium quantum field theory in dynamical spacetimes using `in-in' techniques.","A more manageable subclass of dynamics is where thermal equilibrium conditions are established at both the beginning and the end of evolution are both well defined.","Here we shall assume an in-vacuum state.","It has been shown that if the intervening dynamics has an initial period of exponential expansion, such as in inflationary cosmology, particles created from the parametric amplification of the vacuum fluctuations in the initial vacuum will have a thermal spectrum measured at the out-state.","Under these conditions finite temperature field theory can be applied to calculate the quantum thermodynamic quantities.","Here we consider a massive conformal scalar field in a closed four-dimensional Friedmann-Lemaitre-Robertson-Walker universe based on the simple analytically solvable Bernard-Duncan model.","We calculate the energy density of particles created from an in-vacuum and derive the partition function.","From the free energy we then derive the heat capacity and the quantum compressibility of the spacetimes with thermal particle creation.","We end with some discussions and suggestions for further work in this program of studies."],"url":"http://arxiv.org/abs/2405.00360v1","category":"hep-th"}
{"created":"2024-05-01 07:28:40","title":"Subquadratic Submodular Maximization with a General Matroid Constraint","abstract":"We consider fast algorithms for monotone submodular maximization with a general matroid constraint. We present a randomized $(1 - 1/e - \\epsilon)$-approximation algorithm that requires $\\tilde{O}_{\\epsilon}(\\sqrt{r} n)$ independence oracle and value oracle queries, where $n$ is the number of elements in the matroid and $r \\leq n$ is the rank of the matroid. This improves upon the previously best algorithm by Buchbinder-Feldman-Schwartz [Mathematics of Operations Research 2017] that requires $\\tilde{O}_{\\epsilon}(r^2 + \\sqrt{r}n)$ queries.   Our algorithm is based on continuous relaxation, as with other submodular maximization algorithms in the literature. To achieve subquadratic query complexity, we develop a new rounding algorithm, which is our main technical contribution. The rounding algorithm takes as input a point represented as a convex combination of $t$ bases of a matroid and rounds it to an integral solution. Our rounding algorithm requires $\\tilde{O}(r^{3/2} t)$ independence oracle queries, while the previously best rounding algorithm by Chekuri-Vondr\\'{a}k-Zenklusen [FOCS 2010] requires $O(r^2 t)$ independence oracle queries. A key idea in our rounding algorithm is to use a directed cycle of arbitrary length in an auxiliary graph, while the algorithm of Chekuri-Vondr\\'{a}k-Zenklusen focused on directed cycles of length two.","sentences":["We consider fast algorithms for monotone submodular maximization with a general matroid constraint.","We present a randomized $(1 - 1/e - \\epsilon)$-approximation algorithm that requires $\\tilde{O}_{\\epsilon}(\\sqrt{r} n)$ independence oracle and value oracle queries, where $n$ is the number of elements in the matroid and $r \\leq n$ is the rank of the matroid.","This improves upon the previously best algorithm by Buchbinder-Feldman-Schwartz [Mathematics of Operations Research 2017] that requires $\\tilde{O}_{\\epsilon}(r^2 + \\sqrt{r}n)$ queries.   ","Our algorithm is based on continuous relaxation, as with other submodular maximization algorithms in the literature.","To achieve subquadratic query complexity, we develop a new rounding algorithm, which is our main technical contribution.","The rounding algorithm takes as input a point represented as a convex combination of $t$ bases of a matroid and rounds it to an integral solution.","Our rounding algorithm requires $\\tilde{O}(r^{3/2} t)$ independence oracle queries, while the previously best rounding algorithm by Chekuri-Vondr\\'{a}k-Zenklusen [FOCS 2010] requires $O(r^2 t)$ independence oracle queries.","A key idea in our rounding algorithm is to use a directed cycle of arbitrary length in an auxiliary graph, while the algorithm of Chekuri-Vondr\\'{a}k-Zenklusen focused on directed cycles of length two."],"url":"http://arxiv.org/abs/2405.00359v1","category":"cs.DS"}
{"created":"2024-05-01 07:27:04","title":"Arbitrary Time Information Modeling via Polynomial Approximation for Temporal Knowledge Graph Embedding","abstract":"Distinguished from traditional knowledge graphs (KGs), temporal knowledge graphs (TKGs) must explore and reason over temporally evolving facts adequately. However, existing TKG approaches still face two main challenges, i.e., the limited capability to model arbitrary timestamps continuously and the lack of rich inference patterns under temporal constraints. In this paper, we propose an innovative TKGE method (PTBox) via polynomial decomposition-based temporal representation and box embedding-based entity representation to tackle the above-mentioned problems. Specifically, we decompose time information by polynomials and then enhance the model's capability to represent arbitrary timestamps flexibly by incorporating the learnable temporal basis tensor. In addition, we model every entity as a hyperrectangle box and define each relation as a transformation on the head and tail entity boxes. The entity boxes can capture complex geometric structures and learn robust representations, improving the model's inductive capability for rich inference patterns. Theoretically, our PTBox can encode arbitrary time information or even unseen timestamps while capturing rich inference patterns and higher-arity relations of the knowledge base. Extensive experiments on real-world datasets demonstrate the effectiveness of our method.","sentences":["Distinguished from traditional knowledge graphs (KGs), temporal knowledge graphs (TKGs) must explore and reason over temporally evolving facts adequately.","However, existing TKG approaches still face two main challenges, i.e., the limited capability to model arbitrary timestamps continuously and the lack of rich inference patterns under temporal constraints.","In this paper, we propose an innovative TKGE method (PTBox) via polynomial decomposition-based temporal representation and box embedding-based entity representation to tackle the above-mentioned problems.","Specifically, we decompose time information by polynomials and then enhance the model's capability to represent arbitrary timestamps flexibly by incorporating the learnable temporal basis tensor.","In addition, we model every entity as a hyperrectangle box and define each relation as a transformation on the head and tail entity boxes.","The entity boxes can capture complex geometric structures and learn robust representations, improving the model's inductive capability for rich inference patterns.","Theoretically, our PTBox can encode arbitrary time information or even unseen timestamps while capturing rich inference patterns and higher-arity relations of the knowledge base.","Extensive experiments on real-world datasets demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2405.00358v1","category":"cs.AI"}
{"created":"2024-05-01 07:16:49","title":"Exploring Self-Supervised Vision Transformers for Deepfake Detection: A Comparative Analysis","abstract":"This paper investigates the effectiveness of self-supervised pre-trained transformers compared to supervised pre-trained transformers and conventional neural networks (ConvNets) for detecting various types of deepfakes. We focus on their potential for improved generalization, particularly when training data is limited. Despite the notable success of large vision-language models utilizing transformer architectures in various tasks, including zero-shot and few-shot learning, the deepfake detection community has still shown some reluctance to adopt pre-trained vision transformers (ViTs), especially large ones, as feature extractors. One concern is their perceived excessive capacity, which often demands extensive data, and the resulting suboptimal generalization when training or fine-tuning data is small or less diverse. This contrasts poorly with ConvNets, which have already established themselves as robust feature extractors. Additionally, training and optimizing transformers from scratch requires significant computational resources, making this accessible primarily to large companies and hindering broader investigation within the academic community. Recent advancements in using self-supervised learning (SSL) in transformers, such as DINO and its derivatives, have showcased significant adaptability across diverse vision tasks and possess explicit semantic segmentation capabilities. By leveraging DINO for deepfake detection with modest training data and implementing partial fine-tuning, we observe comparable adaptability to the task and the natural explainability of the detection result via the attention mechanism. Moreover, partial fine-tuning of transformers for deepfake detection offers a more resource-efficient alternative, requiring significantly fewer computational resources.","sentences":["This paper investigates the effectiveness of self-supervised pre-trained transformers compared to supervised pre-trained transformers and conventional neural networks (ConvNets) for detecting various types of deepfakes.","We focus on their potential for improved generalization, particularly when training data is limited.","Despite the notable success of large vision-language models utilizing transformer architectures in various tasks, including zero-shot and few-shot learning, the deepfake detection community has still shown some reluctance to adopt pre-trained vision transformers (ViTs), especially large ones, as feature extractors.","One concern is their perceived excessive capacity, which often demands extensive data, and the resulting suboptimal generalization when training or fine-tuning data is small or less diverse.","This contrasts poorly with ConvNets, which have already established themselves as robust feature extractors.","Additionally, training and optimizing transformers from scratch requires significant computational resources, making this accessible primarily to large companies and hindering broader investigation within the academic community.","Recent advancements in using self-supervised learning (SSL) in transformers, such as DINO and its derivatives, have showcased significant adaptability across diverse vision tasks and possess explicit semantic segmentation capabilities.","By leveraging DINO for deepfake detection with modest training data and implementing partial fine-tuning, we observe comparable adaptability to the task and the natural explainability of the detection result via the attention mechanism.","Moreover, partial fine-tuning of transformers for deepfake detection offers a more resource-efficient alternative, requiring significantly fewer computational resources."],"url":"http://arxiv.org/abs/2405.00355v1","category":"cs.CV"}
{"created":"2024-05-01 07:16:03","title":"CrossMatch: Enhance Semi-Supervised Medical Image Segmentation with Perturbation Strategies and Knowledge Distillation","abstract":"Semi-supervised learning for medical image segmentation presents a unique challenge of efficiently using limited labeled data while leveraging abundant unlabeled data. Despite advancements, existing methods often do not fully exploit the potential of the unlabeled data for enhancing model robustness and accuracy. In this paper, we introduce CrossMatch, a novel framework that integrates knowledge distillation with dual perturbation strategies-image-level and feature-level-to improve the model's learning from both labeled and unlabeled data. CrossMatch employs multiple encoders and decoders to generate diverse data streams, which undergo self-knowledge distillation to enhance consistency and reliability of predictions across varied perturbations. Our method significantly surpasses other state-of-the-art techniques in standard benchmarks by effectively minimizing the gap between training on labeled and unlabeled data and improving edge accuracy and generalization in medical image segmentation. The efficacy of CrossMatch is demonstrated through extensive experimental validations, showing remarkable performance improvements without increasing computational costs. Code for this implementation is made available at https://github.com/AiEson/CrossMatch.git.","sentences":["Semi-supervised learning for medical image segmentation presents a unique challenge of efficiently using limited labeled data while leveraging abundant unlabeled data.","Despite advancements, existing methods often do not fully exploit the potential of the unlabeled data for enhancing model robustness and accuracy.","In this paper, we introduce CrossMatch, a novel framework that integrates knowledge distillation with dual perturbation strategies-image-level and feature-level-to improve the model's learning from both labeled and unlabeled data.","CrossMatch employs multiple encoders and decoders to generate diverse data streams, which undergo self-knowledge distillation to enhance consistency and reliability of predictions across varied perturbations.","Our method significantly surpasses other state-of-the-art techniques in standard benchmarks by effectively minimizing the gap between training on labeled and unlabeled data and improving edge accuracy and generalization in medical image segmentation.","The efficacy of CrossMatch is demonstrated through extensive experimental validations, showing remarkable performance improvements without increasing computational costs.","Code for this implementation is made available at https://github.com/AiEson/CrossMatch.git."],"url":"http://arxiv.org/abs/2405.00354v1","category":"cs.CV"}
{"created":"2024-05-01 07:12:16","title":"Transformer-based Reasoning for Learning Evolutionary Chain of Events on Temporal Knowledge Graph","abstract":"Temporal Knowledge Graph (TKG) reasoning often involves completing missing factual elements along the timeline. Although existing methods can learn good embeddings for each factual element in quadruples by integrating temporal information, they often fail to infer the evolution of temporal facts. This is mainly because of (1) insufficiently exploring the internal structure and semantic relationships within individual quadruples and (2) inadequately learning a unified representation of the contextual and temporal correlations among different quadruples. To overcome these limitations, we propose a novel Transformer-based reasoning model (dubbed ECEformer) for TKG to learn the Evolutionary Chain of Events (ECE). Specifically, we unfold the neighborhood subgraph of an entity node in chronological order, forming an evolutionary chain of events as the input for our model. Subsequently, we utilize a Transformer encoder to learn the embeddings of intra-quadruples for ECE. We then craft a mixed-context reasoning module based on the multi-layer perceptron (MLP) to learn the unified representations of inter-quadruples for ECE while accomplishing temporal knowledge reasoning. In addition, to enhance the timeliness of the events, we devise an additional time prediction task to complete effective temporal information within the learned unified representation. Extensive experiments on six benchmark datasets verify the state-of-the-art performance and the effectiveness of our method.","sentences":["Temporal Knowledge Graph (TKG) reasoning often involves completing missing factual elements along the timeline.","Although existing methods can learn good embeddings for each factual element in quadruples by integrating temporal information, they often fail to infer the evolution of temporal facts.","This is mainly because of (1) insufficiently exploring the internal structure and semantic relationships within individual quadruples and (2) inadequately learning a unified representation of the contextual and temporal correlations among different quadruples.","To overcome these limitations, we propose a novel Transformer-based reasoning model (dubbed ECEformer) for TKG to learn the Evolutionary Chain of Events (ECE).","Specifically, we unfold the neighborhood subgraph of an entity node in chronological order, forming an evolutionary chain of events as the input for our model.","Subsequently, we utilize a Transformer encoder to learn the embeddings of intra-quadruples for ECE.","We then craft a mixed-context reasoning module based on the multi-layer perceptron (MLP) to learn the unified representations of inter-quadruples for ECE while accomplishing temporal knowledge reasoning.","In addition, to enhance the timeliness of the events, we devise an additional time prediction task to complete effective temporal information within the learned unified representation.","Extensive experiments on six benchmark datasets verify the state-of-the-art performance and the effectiveness of our method."],"url":"http://arxiv.org/abs/2405.00352v1","category":"cs.AI"}
{"created":"2024-05-01 07:08:24","title":"Learning High-Quality Navigation and Zooming on Omnidirectional Images in Virtual Reality","abstract":"Viewing omnidirectional images (ODIs) in virtual reality (VR) represents a novel form of media that provides immersive experiences for users to navigate and interact with digital content. Nonetheless, this sense of immersion can be greatly compromised by a blur effect that masks details and hampers the user's ability to engage with objects of interest. In this paper, we present a novel system, called OmniVR, designed to enhance visual clarity during VR navigation. Our system enables users to effortlessly locate and zoom in on the objects of interest in VR. It captures user commands for navigation and zoom, converting these inputs into parameters for the Mobius transformation matrix. Leveraging these parameters, the ODI is refined using a learning-based algorithm. The resultant ODI is presented within the VR media, effectively reducing blur and increasing user engagement. To verify the effectiveness of our system, we first evaluate our algorithm with state-of-the-art methods on public datasets, which achieves the best performance. Furthermore, we undertake a comprehensive user study to evaluate viewer experiences across diverse scenarios and to gather their qualitative feedback from multiple perspectives. The outcomes reveal that our system enhances user engagement by improving the viewers' recognition, reducing discomfort, and improving the overall immersive experience. Our system makes the navigation and zoom more user-friendly.","sentences":["Viewing omnidirectional images (ODIs) in virtual reality (VR) represents a novel form of media that provides immersive experiences for users to navigate and interact with digital content.","Nonetheless, this sense of immersion can be greatly compromised by a blur effect that masks details and hampers the user's ability to engage with objects of interest.","In this paper, we present a novel system, called OmniVR, designed to enhance visual clarity during VR navigation.","Our system enables users to effortlessly locate and zoom in on the objects of interest in VR.","It captures user commands for navigation and zoom, converting these inputs into parameters for the Mobius transformation matrix.","Leveraging these parameters, the ODI is refined using a learning-based algorithm.","The resultant ODI is presented within the VR media, effectively reducing blur and increasing user engagement.","To verify the effectiveness of our system, we first evaluate our algorithm with state-of-the-art methods on public datasets, which achieves the best performance.","Furthermore, we undertake a comprehensive user study to evaluate viewer experiences across diverse scenarios and to gather their qualitative feedback from multiple perspectives.","The outcomes reveal that our system enhances user engagement by improving the viewers' recognition, reducing discomfort, and improving the overall immersive experience.","Our system makes the navigation and zoom more user-friendly."],"url":"http://arxiv.org/abs/2405.00351v1","category":"cs.HC"}
{"created":"2024-05-01 06:50:18","title":"A Self-explaining Neural Architecture for Generalizable Concept Learning","abstract":"With the wide proliferation of Deep Neural Networks in high-stake applications, there is a growing demand for explainability behind their decision-making process. Concept learning models attempt to learn high-level 'concepts' - abstract entities that align with human understanding, and thus provide interpretability to DNN architectures. However, in this paper, we demonstrate that present SOTA concept learning approaches suffer from two major problems - lack of concept fidelity wherein the models fail to learn consistent concepts among similar classes and limited concept interoperability wherein the models fail to generalize learned concepts to new domains for the same task. Keeping these in mind, we propose a novel self-explaining architecture for concept learning across domains which - i) incorporates a new concept saliency network for representative concept selection, ii) utilizes contrastive learning to capture representative domain invariant concepts, and iii) uses a novel prototype-based concept grounding regularization to improve concept alignment across domains. We demonstrate the efficacy of our proposed approach over current SOTA concept learning approaches on four widely used real-world datasets. Empirical results show that our method improves both concept fidelity measured through concept overlap and concept interoperability measured through domain adaptation performance.","sentences":["With the wide proliferation of Deep Neural Networks in high-stake applications, there is a growing demand for explainability behind their decision-making process.","Concept learning models attempt to learn high-level 'concepts' - abstract entities that align with human understanding, and thus provide interpretability to DNN architectures.","However, in this paper, we demonstrate that present SOTA concept learning approaches suffer from two major problems - lack of concept fidelity wherein the models fail to learn consistent concepts among similar classes and limited concept interoperability wherein the models fail to generalize learned concepts to new domains for the same task.","Keeping these in mind, we propose a novel self-explaining architecture for concept learning across domains which - i) incorporates a new concept saliency network for representative concept selection, ii) utilizes contrastive learning to capture representative domain invariant concepts, and iii) uses a novel prototype-based concept grounding regularization to improve concept alignment across domains.","We demonstrate the efficacy of our proposed approach over current SOTA concept learning approaches on four widely used real-world datasets.","Empirical results show that our method improves both concept fidelity measured through concept overlap and concept interoperability measured through domain adaptation performance."],"url":"http://arxiv.org/abs/2405.00349v1","category":"cs.LG"}
{"created":"2024-05-01 06:41:27","title":"Practical Dataset Distillation Based on Deep Support Vectors","abstract":"Conventional dataset distillation requires significant computational resources and assumes access to the entire dataset, an assumption impractical as it presumes all data resides on a central server. In this paper, we focus on dataset distillation in practical scenarios with access to only a fraction of the entire dataset. We introduce a novel distillation method that augments the conventional process by incorporating general model knowledge via the addition of Deep KKT (DKKT) loss. In practical settings, our approach showed improved performance compared to the baseline distribution matching distillation method on the CIFAR-10 dataset. Additionally, we present experimental evidence that Deep Support Vectors (DSVs) offer unique information to the original distillation, and their integration results in enhanced performance.","sentences":["Conventional dataset distillation requires significant computational resources and assumes access to the entire dataset, an assumption impractical as it presumes all data resides on a central server.","In this paper, we focus on dataset distillation in practical scenarios with access to only a fraction of the entire dataset.","We introduce a novel distillation method that augments the conventional process by incorporating general model knowledge via the addition of Deep KKT (DKKT) loss.","In practical settings, our approach showed improved performance compared to the baseline distribution matching distillation method on the CIFAR-10 dataset.","Additionally, we present experimental evidence that Deep Support Vectors (DSVs) offer unique information to the original distillation, and their integration results in enhanced performance."],"url":"http://arxiv.org/abs/2405.00348v1","category":"cs.LG"}
{"created":"2024-05-01 06:40:52","title":"Origin of the Very High Energy \u03b3-rays in the Low-luminosity Active Galactic Nucleus NGC 4278","abstract":"NGC 4278, a Low-luminosity active galactic nucleus (AGN), is generally classified as a low-ionization nuclear emission line region (LINER) type AGN. Recently, it is reported to be associated with a very high energy (VHE) $\\gamma$-ray source 1LHAASO J1219+2915 in the first Large High Altitude Air Shower Observatory (LHAASO) source catalog. However, no associated counterpart has been detected by Fermi-LAT. By analyzing its X-ray observation data from Swift-XRT, we find it is in a high-flux state on MJD 59546, with the X-ray flux more than one order of magnitude higher than that observed $\\sim$ 11.7 year earlier by Chandra. We propose that the detection of VHE $\\gamma$-rays from NGC 4278 may be attributed to the presence of an active nucleus displaying behavior similar to that of a BL Lac. To reproduce its spectral energy distribution (SED), we employ a one-zone leptonic model, typically used for fitting broadband SEDs of BL Lacs, and find that smaller values for both Doppler factor ($\\delta$) and magnetic field strength ($B$) are required than that of typical TeV BL Lacs. Furthermore, NGC 4278 exhibits significantly lower luminosity in both radio and TeV bands when compared with typical TeV BL Lacs. In the radio-luminosity vs. Eddington-ratio plane, NGC 4278 shows greater similarity to Seyfert galaxies and LINERs rather than BL Lacs; however, it still roughly follows the extension towards lower luminosity seen in BL Lacs.","sentences":["NGC 4278, a Low-luminosity active galactic nucleus (AGN), is generally classified as a low-ionization nuclear emission line region (LINER) type AGN.","Recently, it is reported to be associated with a very high energy (VHE) $\\gamma$-ray source 1LHAASO J1219+2915 in the first Large High Altitude Air Shower Observatory (LHAASO) source catalog.","However, no associated counterpart has been detected by Fermi-LAT.","By analyzing its X-ray observation data from Swift-XRT, we find it is in a high-flux state on MJD 59546, with the X-ray flux more than one order of magnitude higher than that observed $\\sim$ 11.7 year earlier by Chandra.","We propose that the detection of VHE $\\gamma$-rays from NGC 4278 may be attributed to the presence of an active nucleus displaying behavior similar to that of a BL Lac.","To reproduce its spectral energy distribution (SED), we employ a one-zone leptonic model, typically used for fitting broadband SEDs of BL Lacs, and find that smaller values for both Doppler factor ($\\delta$) and magnetic field strength ($B$) are required than that of typical TeV BL Lacs.","Furthermore, NGC 4278 exhibits significantly lower luminosity in both radio and TeV bands when compared with typical TeV BL Lacs.","In the radio-luminosity vs. Eddington-ratio plane, NGC 4278 shows greater similarity to Seyfert galaxies and LINERs rather than BL Lacs; however, it still roughly follows the extension towards lower luminosity seen in BL Lacs."],"url":"http://arxiv.org/abs/2405.00347v1","category":"astro-ph.HE"}
{"created":"2024-05-01 06:35:41","title":"Expert Insight-Enhanced Follow-up Chest X-Ray Summary Generation","abstract":"A chest X-ray radiology report describes abnormal findings not only from X-ray obtained at current examination, but also findings on disease progression or change in device placement with reference to the X-ray from previous examination. Majority of the efforts on automatic generation of radiology report pertain to reporting the former, but not the latter, type of findings. To the best of the authors' knowledge, there is only one work dedicated to generating summary of the latter findings, i.e., follow-up summary. In this study, we therefore propose a transformer-based framework to tackle this task. Motivated by our observations on the significance of medical lexicon on the fidelity of summary generation, we introduce two mechanisms to bestow expert insight to our model, namely expert soft guidance and masked entity modeling loss. The former mechanism employs a pretrained expert disease classifier to guide the presence level of specific abnormalities, while the latter directs the model's attention toward medical lexicon. Extensive experiments were conducted to demonstrate that the performance of our model is competitive with or exceeds the state-of-the-art.","sentences":["A chest X-ray radiology report describes abnormal findings not only from X-ray obtained at current examination, but also findings on disease progression or change in device placement with reference to the X-ray from previous examination.","Majority of the efforts on automatic generation of radiology report pertain to reporting the former, but not the latter, type of findings.","To the best of the authors' knowledge, there is only one work dedicated to generating summary of the latter findings, i.e., follow-up summary.","In this study, we therefore propose a transformer-based framework to tackle this task.","Motivated by our observations on the significance of medical lexicon on the fidelity of summary generation, we introduce two mechanisms to bestow expert insight to our model, namely expert soft guidance and masked entity modeling loss.","The former mechanism employs a pretrained expert disease classifier to guide the presence level of specific abnormalities, while the latter directs the model's attention toward medical lexicon.","Extensive experiments were conducted to demonstrate that the performance of our model is competitive with or exceeds the state-of-the-art."],"url":"http://arxiv.org/abs/2405.00344v1","category":"cs.MM"}
{"created":"2024-05-01 06:35:18","title":"From Compton Scattering of photons on targets to Inverse Compton Scattering of electron and photon beams","abstract":"We revisit the kinematics of Compton Scattering (electron-photon interactions producing electrons and photons in the exit channel) covering the full range of energy/momenta distribution between the two colliding particles, with a dedicated view to statistical properties of secondary beams that are generated in beam-beam collisions. Starting from the Thomson inverse scattering, where electrons do not recoil and photons are back-scattered to higher energies by a Lorentz boost effect (factor $4\\gamma^2$), we analyze three transition points, separating four regions. These are in sequence, given by increasing the electron recoil (numbers are for transition points, letters for regions): a) Thomson back-scattering, 1) equal sharing of total energy in the exit channel between electron and photon, b) deep recoil regime where the bandwidth/energy spread of the two interacting beams are exchanged in the exit channel, 2) electron is stopped, i.e. taken down at rest in the laboratory system by colliding with an incident photon of $mc^2/2$ energy, c) electron back-scattering region, where incident electron is back-scattered by the incident photon, 3) symmetric scattering, when the incident particles carry equal and opposite momenta, so that in the exit channel they are back-scattered with same energy/momenta, d) Compton scattering ($a'$ $la$ Arthur Compton, see ref.4), where photons carry an energy much larger than the colliding electron energy. For each region and/or transition point we discuss the potential effects of interest in diverse areas, like generating mono-chromatic gamma ray beams in deep recoil regions with spectral purification, or possible mechanisms of generation and propagation of very high energy photons in the cosmological domain.","sentences":["We revisit the kinematics of Compton Scattering (electron-photon interactions producing electrons and photons in the exit channel) covering the full range of energy/momenta distribution between the two colliding particles, with a dedicated view to statistical properties of secondary beams that are generated in beam-beam collisions.","Starting from the Thomson inverse scattering, where electrons do not recoil and photons are back-scattered to higher energies by a Lorentz boost effect (factor $4\\gamma^2$), we analyze three transition points, separating four regions.","These are in sequence, given by increasing the electron recoil (numbers are for transition points, letters for regions): a) Thomson back-scattering, 1) equal sharing of total energy in the exit channel between electron and photon, b) deep recoil regime where the bandwidth/energy spread of the two interacting beams are exchanged in the exit channel, 2) electron is stopped, i.e. taken down at rest in the laboratory system by colliding with an incident photon of $mc^2/2$ energy, c) electron back-scattering region, where incident electron is back-scattered by the incident photon, 3) symmetric scattering, when the incident particles carry equal and opposite momenta, so that in the exit channel they are back-scattered with same energy/momenta, d)","Compton scattering ($a'$ $la$ Arthur Compton, see ref.4), where photons carry an energy much larger than the colliding electron energy.","For each region and/or transition point we discuss the potential effects of interest in diverse areas, like generating mono-chromatic gamma ray beams in deep recoil regions with spectral purification, or possible mechanisms of generation and propagation of very high energy photons in the cosmological domain."],"url":"http://arxiv.org/abs/2405.00343v1","category":"physics.acc-ph"}
{"created":"2024-05-01 06:29:47","title":"The Set of Stable Matchings and the Core in a Matching Market with Ties and Matroid Constraints","abstract":"In this paper, we consider a many-to-one matching market where ties in the preferences of agents are allowed. For this market with capacity constraints, Bonifacio, Juarez, Neme, and Oviedo proved some relationship between the set of stable matchings and the core. In this paper, we consider a matroid constraint that is a generalization of a capacity constraint. We prove that the results proved by Bonifacio, Juarez, Neme, and Oviedo can be generalized to this setting.","sentences":["In this paper, we consider a many-to-one matching market where ties in the preferences of agents are allowed.","For this market with capacity constraints, Bonifacio, Juarez, Neme, and Oviedo proved some relationship between the set of stable matchings and the core.","In this paper, we consider a matroid constraint that is a generalization of a capacity constraint.","We prove that the results proved by Bonifacio, Juarez, Neme, and Oviedo can be generalized to this setting."],"url":"http://arxiv.org/abs/2405.00342v1","category":"cs.GT"}
{"created":"2024-05-01 06:28:58","title":"Google or ChatGPT: Who is the Better Helper for University Students","abstract":"Using information technology tools for academic help-seeking among college students has become a popular trend. In the evolutionary process between Generation Artificial Intelligence (GenAI) and traditional search engines, when students face academic challenges, do they tend to prefer Google, or are they more inclined to utilize ChatGPT? And what are the key factors influencing learners' preference to use ChatGPT for academic help-seeking? These relevant questions merit attention. The study employed a mixed-methods research design to investigate Taiwanese university students' online academic help-seeking preferences. The results indicated that students tend to prefer using ChatGPT to seek academic assistance, reflecting the potential popularity of GenAI in the educational field. Additionally, in comparing seven machine learning algorithms, the Random Forest and LightGBM algorithms exhibited superior performance. These two algorithms were employed to evaluate the predictive capability of 18 potential factors. It was found that GenAI fluency, GenAI distortions, and age were the core factors influencing how university students seek academic help. Overall, this study underscores that educators should prioritize the cultivation of students' critical thinking skills, while technical personnel should enhance the fluency and reliability of ChatGPT and Google searches and explore the integration of chat and search functions to achieve optimal balance.","sentences":["Using information technology tools for academic help-seeking among college students has become a popular trend.","In the evolutionary process between Generation Artificial Intelligence (GenAI) and traditional search engines, when students face academic challenges, do they tend to prefer Google, or are they more inclined to utilize ChatGPT?","And what are the key factors influencing learners' preference to use ChatGPT for academic help-seeking?","These relevant questions merit attention.","The study employed a mixed-methods research design to investigate Taiwanese university students' online academic help-seeking preferences.","The results indicated that students tend to prefer using ChatGPT to seek academic assistance, reflecting the potential popularity of GenAI in the educational field.","Additionally, in comparing seven machine learning algorithms, the Random Forest and LightGBM algorithms exhibited superior performance.","These two algorithms were employed to evaluate the predictive capability of 18 potential factors.","It was found that GenAI fluency, GenAI distortions, and age were the core factors influencing how university students seek academic help.","Overall, this study underscores that educators should prioritize the cultivation of students' critical thinking skills, while technical personnel should enhance the fluency and reliability of ChatGPT and Google searches and explore the integration of chat and search functions to achieve optimal balance."],"url":"http://arxiv.org/abs/2405.00341v1","category":"cs.HC"}
{"created":"2024-05-01 05:57:03","title":"Finding the white male: The prevalence and consequences of algorithmic gender and race bias in political Google searches","abstract":"Search engines like Google have become major information gatekeepers that use artificial intelligence (AI) to determine who and what voters find when searching for political information. This article proposes and tests a framework of algorithmic representation of minoritized groups in a series of four studies. First, two algorithm audits of political image searches delineate how search engines reflect and uphold structural inequalities by under- and misrepresenting women and non-white politicians. Second, two online experiments show that these biases in algorithmic representation in turn distort perceptions of the political reality and actively reinforce a white and masculinized view of politics. Together, the results have substantive implications for the scientific understanding of how AI technology amplifies biases in political perceptions and decision-making. The article contributes to ongoing public debates and cross-disciplinary research on algorithmic fairness and injustice.","sentences":["Search engines like Google have become major information gatekeepers that use artificial intelligence (AI) to determine who and what voters find when searching for political information.","This article proposes and tests a framework of algorithmic representation of minoritized groups in a series of four studies.","First, two algorithm audits of political image searches delineate how search engines reflect and uphold structural inequalities by under- and misrepresenting women and non-white politicians.","Second, two online experiments show that these biases in algorithmic representation in turn distort perceptions of the political reality and actively reinforce a white and masculinized view of politics.","Together, the results have substantive implications for the scientific understanding of how AI technology amplifies biases in political perceptions and decision-making.","The article contributes to ongoing public debates and cross-disciplinary research on algorithmic fairness and injustice."],"url":"http://arxiv.org/abs/2405.00335v1","category":"cs.CY"}
{"created":"2024-05-01 05:52:15","title":"Reevaluating coexistence and stability in ecosystem networks to address ecological transients: methods and implications","abstract":"Representing ecosystems at equilibrium has been foundational for building ecological theories, forecasting species populations and planning conservation actions. The equilibrium \"balance of nature\" ideal suggests that populations will eventually stabilise to a coexisting balance of species. However, a growing body of literature argues that the equilibrium ideal is inappropriate for ecosystems. Here, we develop and demonstrate a new framework for representing ecosystems without considering equilibrium dynamics. Instead, far more pragmatic ecosystem models are constructed by considering population trajectories, regardless of whether they exhibit equilibrium or transient (i.e. non-equilibrium) behaviour. This novel framework maximally utilises readily available, but often overlooked, knowledge from field observations and expert elicitation, rather than relying on theoretical ecosystem properties. We developed innovative Bayesian algorithms to generate ecosystem models in this new statistical framework, without excessive computational burden. Our results reveal that our pragmatic framework could have a dramatic impact on conservation decision-making and enhance the realism of ecosystem models and forecasts.","sentences":["Representing ecosystems at equilibrium has been foundational for building ecological theories, forecasting species populations and planning conservation actions.","The equilibrium \"balance of nature\" ideal suggests that populations will eventually stabilise to a coexisting balance of species.","However, a growing body of literature argues that the equilibrium ideal is inappropriate for ecosystems.","Here, we develop and demonstrate a new framework for representing ecosystems without considering equilibrium dynamics.","Instead, far more pragmatic ecosystem models are constructed by considering population trajectories, regardless of whether they exhibit equilibrium or transient (i.e. non-equilibrium) behaviour.","This novel framework maximally utilises readily available, but often overlooked, knowledge from field observations and expert elicitation, rather than relying on theoretical ecosystem properties.","We developed innovative Bayesian algorithms to generate ecosystem models in this new statistical framework, without excessive computational burden.","Our results reveal that our pragmatic framework could have a dramatic impact on conservation decision-making and enhance the realism of ecosystem models and forecasts."],"url":"http://arxiv.org/abs/2405.00333v1","category":"q-bio.PE"}
{"created":"2024-05-01 05:52:05","title":"A Careful Examination of Large Language Model Performance on Grade School Arithmetic","abstract":"Large language models (LLMs) have achieved impressive success on many benchmarks for mathematical reasoning. However, there is growing concern that some of this performance actually reflects dataset contamination, where data closely resembling benchmark questions leaks into the training data, instead of true reasoning ability. To investigate this claim rigorously, we commission Grade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and complexity of the established GSM8k benchmark, the gold standard for measuring elementary mathematical reasoning. We ensure that the two benchmarks are comparable across important metrics such as human solve rates, number of steps in solution, answer magnitude, and more. When evaluating leading open- and closed-source LLMs on GSM1k, we observe accuracy drops of up to 13%, with several families of models (e.g., Phi and Mistral) showing evidence of systematic overfitting across almost all model sizes. At the same time, many models, especially those on the frontier, (e.g., Gemini/GPT/Claude) show minimal signs of overfitting. Further analysis suggests a positive relationship (Spearman's r^2=0.32) between a model's probability of generating an example from GSM8k and its performance gap between GSM8k and GSM1k, suggesting that many models may have partially memorized GSM8k.","sentences":["Large language models (LLMs) have achieved impressive success on many benchmarks for mathematical reasoning.","However, there is growing concern that some of this performance actually reflects dataset contamination, where data closely resembling benchmark questions leaks into the training data, instead of true reasoning ability.","To investigate this claim rigorously, we commission Grade School Math 1000 (GSM1k).","GSM1k is designed to mirror the style and complexity of the established GSM8k benchmark, the gold standard for measuring elementary mathematical reasoning.","We ensure that the two benchmarks are comparable across important metrics such as human solve rates, number of steps in solution, answer magnitude, and more.","When evaluating leading open- and closed-source LLMs on GSM1k, we observe accuracy drops of up to 13%, with several families of models (e.g., Phi and Mistral) showing evidence of systematic overfitting across almost all model sizes.","At the same time, many models, especially those on the frontier, (e.g., Gemini/GPT/Claude) show minimal signs of overfitting.","Further analysis suggests a positive relationship (Spearman's r^2=0.32) between a model's probability of generating an example from GSM8k and its performance gap between GSM8k and GSM1k, suggesting that many models may have partially memorized GSM8k."],"url":"http://arxiv.org/abs/2405.00332v1","category":"cs.CL"}
{"created":"2024-05-01 05:43:18","title":"A Class of Numerical Semigroups Defined by Kunz and Waldo","abstract":"In this paper, we explore a class of numerical semigroups initiated by Kunz and Waldi containing two coprime numbers $p < q$, which we call KW semigroups. We characterize KW numerical semigroups by their principal matrices. We present a necessary and sufficient criterion for a matrix to be the principal matrix of a KW semigroup. An explicit description of the minimal resolutions of numerical semigroups in the same class with small embedding dimensions 3 and 4 is given. We give a generalization of this notion to three dimensions using lattice paths under a plane and present some preliminary results and questions.","sentences":["In this paper, we explore a class of numerical semigroups initiated by Kunz and Waldi containing two coprime numbers $p < q$, which we call KW semigroups.","We characterize KW numerical semigroups by their principal matrices.","We present a necessary and sufficient criterion for a matrix to be the principal matrix of a KW semigroup.","An explicit description of the minimal resolutions of numerical semigroups in the same class with small embedding dimensions 3 and 4 is given.","We give a generalization of this notion to three dimensions using lattice paths under a plane and present some preliminary results and questions."],"url":"http://arxiv.org/abs/2405.00331v1","category":"math.AC"}
{"created":"2024-05-01 05:39:07","title":"Integrating A.I. in Higher Education: Protocol for a Pilot Study with 'SAMCares: An Adaptive Learning Hub'","abstract":"Learning never ends, and there is no age limit to grow yourself. However, the educational landscape may face challenges in effectively catering to students' inclusion and diverse learning needs. These students should have access to state-of-the-art methods for lecture delivery, online resources, and technology needs. However, with all the diverse learning sources, it becomes harder for students to comprehend a large amount of knowledge in a short period of time. Traditional assistive technologies and learning aids often lack the dynamic adaptability required for individualized education plans. Large Language Models (LLM) have been used in language translation, text summarization, and content generation applications. With rapid growth in AI over the past years, AI-powered chatbots and virtual assistants have been developed. This research aims to bridge this gap by introducing an innovative study buddy we will be calling the 'SAMCares'. The system leverages a Large Language Model (LLM) (in our case, LLaMa-2 70B as the base model) and Retriever-Augmented Generation (RAG) to offer real-time, context-aware, and adaptive educational support. The context of the model will be limited to the knowledge base of Sam Houston State University (SHSU) course notes. The LLM component enables a chat-like environment to interact with it to meet the unique learning requirements of each student. For this, we will build a custom web-based GUI. At the same time, RAG enhances real-time information retrieval and text generation, in turn providing more accurate and context-specific assistance. An option to upload additional study materials in the web GUI is added in case additional knowledge support is required. The system's efficacy will be evaluated through controlled trials and iterative feedback mechanisms.","sentences":["Learning never ends, and there is no age limit to grow yourself.","However, the educational landscape may face challenges in effectively catering to students' inclusion and diverse learning needs.","These students should have access to state-of-the-art methods for lecture delivery, online resources, and technology needs.","However, with all the diverse learning sources, it becomes harder for students to comprehend a large amount of knowledge in a short period of time.","Traditional assistive technologies and learning aids often lack the dynamic adaptability required for individualized education plans.","Large Language Models (LLM) have been used in language translation, text summarization, and content generation applications.","With rapid growth in AI over the past years, AI-powered chatbots and virtual assistants have been developed.","This research aims to bridge this gap by introducing an innovative study buddy we will be calling the 'SAMCares'.","The system leverages a Large Language Model (LLM) (in our case, LLaMa-2 70B as the base model) and Retriever-Augmented Generation (RAG) to offer real-time, context-aware, and adaptive educational support.","The context of the model will be limited to the knowledge base of Sam Houston State University (SHSU) course notes.","The LLM component enables a chat-like environment to interact with it to meet the unique learning requirements of each student.","For this, we will build a custom web-based GUI.","At the same time, RAG enhances real-time information retrieval and text generation, in turn providing more accurate and context-specific assistance.","An option to upload additional study materials in the web GUI is added in case additional knowledge support is required.","The system's efficacy will be evaluated through controlled trials and iterative feedback mechanisms."],"url":"http://arxiv.org/abs/2405.00330v1","category":"cs.CY"}
{"created":"2024-05-01 05:31:53","title":"Metric geometry of the privacy-utility tradeoff","abstract":"Synthetic data are an attractive concept to enable privacy in data sharing. A fundamental question is how similar the privacy-preserving synthetic data are compared to the true data. Using metric privacy, an effective generalization of differential privacy beyond the discrete setting, we raise the problem of characterizing the optimal privacy-accuracy tradeoff by the metric geometry of the underlying space. We provide a partial solution to this problem in terms of the \"entropic scale\", a quantity that captures the multiscale geometry of a metric space via the behavior of its packing numbers. We illustrate the applicability of our privacy-accuracy tradeoff framework via a diverse set of examples of metric spaces.","sentences":["Synthetic data are an attractive concept to enable privacy in data sharing.","A fundamental question is how similar the privacy-preserving synthetic data are compared to the true data.","Using metric privacy, an effective generalization of differential privacy beyond the discrete setting, we raise the problem of characterizing the optimal privacy-accuracy tradeoff by the metric geometry of the underlying space.","We provide a partial solution to this problem in terms of the \"entropic scale\", a quantity that captures the multiscale geometry of a metric space via the behavior of its packing numbers.","We illustrate the applicability of our privacy-accuracy tradeoff framework via a diverse set of examples of metric spaces."],"url":"http://arxiv.org/abs/2405.00329v1","category":"cs.CR"}
{"created":"2024-05-01 05:30:04","title":"Discrete Time Crystal Phase as a Resource for Quantum Enhanced Sensing","abstract":"Discrete time crystals are a special phase of matter in which time translational symmetry is broken through a periodic driving pulse. Here, we first propose and characterize an effective mechanism to generate a stable discrete time crystal phase in a disorder-free many-body system with indefinite persistent oscillations even in finite-size systems. Then we explore the sensing capability of this system to measure the spin exchange coupling. The results show strong super-Heisenberg precision, in terms of system size, throughout the time crystal phase. As the spin exchange coupling varies, the system goes through a sharp phase transition and enters a non-time crystal phase in which the performance of the probe considerably decreases. We characterize this phase transition as a second-order type and determine its critical properties through a comprehensive finite-size scaling analysis. The performance of our probe is independent of the initial states and may even benefit from imperfections in the driving pulse.","sentences":["Discrete time crystals are a special phase of matter in which time translational symmetry is broken through a periodic driving pulse.","Here, we first propose and characterize an effective mechanism to generate a stable discrete time crystal phase in a disorder-free many-body system with indefinite persistent oscillations even in finite-size systems.","Then we explore the sensing capability of this system to measure the spin exchange coupling.","The results show strong super-Heisenberg precision, in terms of system size, throughout the time crystal phase.","As the spin exchange coupling varies, the system goes through a sharp phase transition and enters a non-time crystal phase in which the performance of the probe considerably decreases.","We characterize this phase transition as a second-order type and determine its critical properties through a comprehensive finite-size scaling analysis.","The performance of our probe is independent of the initial states and may even benefit from imperfections in the driving pulse."],"url":"http://arxiv.org/abs/2405.00328v1","category":"quant-ph"}
{"created":"2024-05-01 05:20:49","title":"A Smoothed Analysis of the Space Complexity of Computing a Chaotic Sequence","abstract":"This work is motivated by a question whether it is possible to calculate a chaotic sequence efficiently, e.g., is it possible to get the $n$-th bit of a bit sequence generated by a chaotic map, such as $\\beta$-expansion, tent map and logistic map in $\\mathrm{o}(n)$ time/space? This paper gives an affirmative answer to the question about the space complexity of a tent map. We show that the decision problem of whether a given bit sequence is a valid tent code is solved in $\\mathrm{O}(\\log^{2} n)$ space in a sense of the smoothed complexity.","sentences":["This work is motivated by a question whether it is possible to calculate a chaotic sequence efficiently, e.g., is it possible to get the $n$-th bit of a bit sequence generated by a chaotic map, such as $\\beta$-expansion, tent map and logistic map in $\\mathrm{o}(n)$ time/space?","This paper gives an affirmative answer to the question about the space complexity of a tent map.","We show that the decision problem of whether a given bit sequence is a valid tent code is solved in $\\mathrm{O}(\\log^{2} n)$ space in a sense of the smoothed complexity."],"url":"http://arxiv.org/abs/2405.00327v1","category":"cs.CC"}
{"created":"2024-05-01 04:55:51","title":"Data Augmentation Policy Search for Long-Term Forecasting","abstract":"Data augmentation serves as a popular regularization technique to combat overfitting challenges in neural networks. While automatic augmentation has demonstrated success in image classification tasks, its application to time-series problems, particularly in long-term forecasting, has received comparatively less attention. To address this gap, we introduce a time-series automatic augmentation approach named TSAA, which is both efficient and easy to implement. The solution involves tackling the associated bilevel optimization problem through a two-step process: initially training a non-augmented model for a limited number of epochs, followed by an iterative split procedure. During this iterative process, we alternate between identifying a robust augmentation policy through Bayesian optimization and refining the model while discarding suboptimal runs. Extensive evaluations on challenging univariate and multivariate forecasting benchmark problems demonstrate that TSAA consistently outperforms several robust baselines, suggesting its potential integration into prediction pipelines.","sentences":["Data augmentation serves as a popular regularization technique to combat overfitting challenges in neural networks.","While automatic augmentation has demonstrated success in image classification tasks, its application to time-series problems, particularly in long-term forecasting, has received comparatively less attention.","To address this gap, we introduce a time-series automatic augmentation approach named TSAA, which is both efficient and easy to implement.","The solution involves tackling the associated bilevel optimization problem through a two-step process: initially training a non-augmented model for a limited number of epochs, followed by an iterative split procedure.","During this iterative process, we alternate between identifying a robust augmentation policy through Bayesian optimization and refining the model while discarding suboptimal runs.","Extensive evaluations on challenging univariate and multivariate forecasting benchmark problems demonstrate that TSAA consistently outperforms several robust baselines, suggesting its potential integration into prediction pipelines."],"url":"http://arxiv.org/abs/2405.00319v1","category":"cs.LG"}
{"created":"2024-05-01 04:48:20","title":"Input gradient annealing neural network for solving low-temperature Fokker-Planck equations","abstract":"We present a novel yet simple deep learning approach, called input gradient annealing neural network (IGANN), for solving stationary Fokker-Planck equations. Traditional methods, such as finite difference and finite elements, suffer from the curse of dimensionality. Neural network based algorithms are meshless methods, which can avoid the curse of dimensionality. However, at low temperature, when directly solving a stationary Fokker-Planck equation with more than two metastable states in the generalized potential landscape, the small eigenvalue introduces numerical difficulties due to a large condition number. To overcome these problems, we introduce the IGANN method, which uses a penalty of negative input gradient annealing during the training. We demonstrate that the IGANN method can effectively solve high-dimensional and low-temperature Fokker-Planck equations through our numerical experiments.","sentences":["We present a novel yet simple deep learning approach, called input gradient annealing neural network (IGANN), for solving stationary Fokker-Planck equations.","Traditional methods, such as finite difference and finite elements, suffer from the curse of dimensionality.","Neural network based algorithms are meshless methods, which can avoid the curse of dimensionality.","However, at low temperature, when directly solving a stationary Fokker-Planck equation with more than two metastable states in the generalized potential landscape, the small eigenvalue introduces numerical difficulties due to a large condition number.","To overcome these problems, we introduce the IGANN method, which uses a penalty of negative input gradient annealing during the training.","We demonstrate that the IGANN method can effectively solve high-dimensional and low-temperature Fokker-Planck equations through our numerical experiments."],"url":"http://arxiv.org/abs/2405.00317v1","category":"math.DS"}
{"created":"2024-05-01 04:46:36","title":"Enhance Planning with Physics-informed Safety Controllor for End-to-end Autonomous Driving","abstract":"Recent years have seen a growing research interest in applications of Deep Neural Networks (DNN) on autonomous vehicle technology. The trend started with perception and prediction a few years ago and it is gradually being applied to motion planning tasks. Despite the performance of networks improve over time, DNN planners inherit the natural drawbacks of Deep Learning. Learning-based planners have limitations in achieving perfect accuracy on the training dataset and network performance can be affected by out-of-distribution problem. In this paper, we propose FusionAssurance, a novel trajectory-based end-to-end driving fusion framework which combines physics-informed control for safety assurance. By incorporating Potential Field into Model Predictive Control, FusionAssurance is capable of navigating through scenarios that are not included in the training dataset and scenarios where neural network fail to generalize. The effectiveness of the approach is demonstrated by extensive experiments under various scenarios on the CARLA benchmark.","sentences":["Recent years have seen a growing research interest in applications of Deep Neural Networks (DNN) on autonomous vehicle technology.","The trend started with perception and prediction a few years ago and it is gradually being applied to motion planning tasks.","Despite the performance of networks improve over time, DNN planners inherit the natural drawbacks of Deep Learning.","Learning-based planners have limitations in achieving perfect accuracy on the training dataset and network performance can be affected by out-of-distribution problem.","In this paper, we propose FusionAssurance, a novel trajectory-based end-to-end driving fusion framework which combines physics-informed control for safety assurance.","By incorporating Potential Field into Model Predictive Control, FusionAssurance is capable of navigating through scenarios that are not included in the training dataset and scenarios where neural network fail to generalize.","The effectiveness of the approach is demonstrated by extensive experiments under various scenarios on the CARLA benchmark."],"url":"http://arxiv.org/abs/2405.00316v1","category":"cs.RO"}
{"created":"2024-05-01 04:32:07","title":"Model Quantization and Hardware Acceleration for Vision Transformers: A Comprehensive Survey","abstract":"Vision Transformers (ViTs) have recently garnered considerable attention, emerging as a promising alternative to convolutional neural networks (CNNs) in several vision-related applications. However, their large model sizes and high computational and memory demands hinder deployment, especially on resource-constrained devices. This underscores the necessity of algorithm-hardware co-design specific to ViTs, aiming to optimize their performance by tailoring both the algorithmic structure and the underlying hardware accelerator to each other's strengths. Model quantization, by converting high-precision numbers to lower-precision, reduces the computational demands and memory needs of ViTs, allowing the creation of hardware specifically optimized for these quantized algorithms, boosting efficiency. This article provides a comprehensive survey of ViTs quantization and its hardware acceleration. We first delve into the unique architectural attributes of ViTs and their runtime characteristics. Subsequently, we examine the fundamental principles of model quantization, followed by a comparative analysis of the state-of-the-art quantization techniques for ViTs. Additionally, we explore the hardware acceleration of quantized ViTs, highlighting the importance of hardware-friendly algorithm design. In conclusion, this article will discuss ongoing challenges and future research paths. We consistently maintain the related open-source materials at https://github.com/DD-DuDa/awesome-vit-quantization-acceleration.","sentences":["Vision Transformers (ViTs) have recently garnered considerable attention, emerging as a promising alternative to convolutional neural networks (CNNs) in several vision-related applications.","However, their large model sizes and high computational and memory demands hinder deployment, especially on resource-constrained devices.","This underscores the necessity of algorithm-hardware co-design specific to ViTs, aiming to optimize their performance by tailoring both the algorithmic structure and the underlying hardware accelerator to each other's strengths.","Model quantization, by converting high-precision numbers to lower-precision, reduces the computational demands and memory needs of ViTs, allowing the creation of hardware specifically optimized for these quantized algorithms, boosting efficiency.","This article provides a comprehensive survey of ViTs quantization and its hardware acceleration.","We first delve into the unique architectural attributes of ViTs and their runtime characteristics.","Subsequently, we examine the fundamental principles of model quantization, followed by a comparative analysis of the state-of-the-art quantization techniques for ViTs.","Additionally, we explore the hardware acceleration of quantized ViTs, highlighting the importance of hardware-friendly algorithm design.","In conclusion, this article will discuss ongoing challenges and future research paths.","We consistently maintain the related open-source materials at https://github.com/DD-DuDa/awesome-vit-quantization-acceleration."],"url":"http://arxiv.org/abs/2405.00314v1","category":"cs.LG"}
{"created":"2024-05-01 04:30:03","title":"Streamlining Image Editing with Layered Diffusion Brushes","abstract":"Denoising diffusion models have recently gained prominence as powerful tools for a variety of image generation and manipulation tasks. Building on this, we propose a novel tool for real-time editing of images that provides users with fine-grained region-targeted supervision in addition to existing prompt-based controls. Our novel editing technique, termed Layered Diffusion Brushes, leverages prompt-guided and region-targeted alteration of intermediate denoising steps, enabling precise modifications while maintaining the integrity and context of the input image. We provide an editor based on Layered Diffusion Brushes modifications, which incorporates well-known image editing concepts such as layer masks, visibility toggles, and independent manipulation of layers; regardless of their order. Our system renders a single edit on a 512x512 image within 140 ms using a high-end consumer GPU, enabling real-time feedback and rapid exploration of candidate edits. We validated our method and editing system through a user study involving both natural images (using inversion) and generated images, showcasing its usability and effectiveness compared to existing techniques such as InstructPix2Pix and Stable Diffusion Inpainting for refining images. Our approach demonstrates efficacy across a range of tasks, including object attribute adjustments, error correction, and sequential prompt-based object placement and manipulation, demonstrating its versatility and potential for enhancing creative workflows.","sentences":["Denoising diffusion models have recently gained prominence as powerful tools for a variety of image generation and manipulation tasks.","Building on this, we propose a novel tool for real-time editing of images that provides users with fine-grained region-targeted supervision in addition to existing prompt-based controls.","Our novel editing technique, termed Layered Diffusion Brushes, leverages prompt-guided and region-targeted alteration of intermediate denoising steps, enabling precise modifications while maintaining the integrity and context of the input image.","We provide an editor based on Layered Diffusion Brushes modifications, which incorporates well-known image editing concepts such as layer masks, visibility toggles, and independent manipulation of layers; regardless of their order.","Our system renders a single edit on a 512x512 image within 140 ms using a high-end consumer GPU, enabling real-time feedback and rapid exploration of candidate edits.","We validated our method and editing system through a user study involving both natural images (using inversion) and generated images, showcasing its usability and effectiveness compared to existing techniques such as InstructPix2Pix and Stable Diffusion Inpainting for refining images.","Our approach demonstrates efficacy across a range of tasks, including object attribute adjustments, error correction, and sequential prompt-based object placement and manipulation, demonstrating its versatility and potential for enhancing creative workflows."],"url":"http://arxiv.org/abs/2405.00313v1","category":"cs.CV"}
{"created":"2024-05-01 04:29:05","title":"Compatible weak factorization systems and model structures","abstract":"In this paper the concept of compatible weak factorization systems in general categories is introduced as a counterpart of compatible complete cotorsion pairs in abelian categories. We describe a method to construct model structures on general categories via two compatible weak factorization systems satisfying certain conditions, and hence generalize a very useful result by Gillespie for abelian model structures. As particular examples, we show that weak factorizations systems associated to some classical model structures (for example, the Kan-Quillen model structure on $\\mathsf{sSet}$) satisfy these conditions.","sentences":["In this paper the concept of compatible weak factorization systems in general categories is introduced as a counterpart of compatible complete cotorsion pairs in abelian categories.","We describe a method to construct model structures on general categories via two compatible weak factorization systems satisfying certain conditions, and hence generalize a very useful result by Gillespie for abelian model structures.","As particular examples, we show that weak factorizations systems associated to some classical model structures (for example, the Kan-Quillen model structure on $\\mathsf{sSet}$) satisfy these conditions."],"url":"http://arxiv.org/abs/2405.00312v1","category":"math.CT"}
{"created":"2024-05-01 04:25:09","title":"New upper bounds on the number of non-zero weights of constacyclic codes","abstract":"For any simple-root constacyclic code $\\mathcal{C}$ over a finite field $\\mathbb{F}_q$, as far as we know, the group $\\mathcal{G}$ generated by the multiplier, the constacyclic shift and the scalar multiplications is the largest subgroup of the automorphism group ${\\rm Aut}(\\mathcal{C})$ of $\\mathcal{C}$. In this paper, by calculating the number of $\\mathcal{G}$-orbits of $\\mathcal{C}\\backslash\\{\\bf 0\\}$, we give an explicit upper bound on the number of non-zero weights of $\\mathcal{C}$ and present a necessary and sufficient condition for $\\mathcal{C}$ to meet the upper bound. Some examples in this paper show that our upper bound is tight and better than the upper bounds in [Zhang and Cao, FFA, 2024]. In particular, our main results provide a new method to construct few-weight constacyclic codes. Furthermore, for the constacyclic code $\\mathcal{C}$ belonging to two special types, we obtain a smaller upper bound on the number of non-zero weights of $\\mathcal{C}$ by substituting $\\mathcal{G}$ with a larger subgroup of ${\\rm Aut}(\\mathcal{C})$. The results derived in this paper generalize the main results in [Chen, Fu and Liu, IEEE-TIT, 2024]}.","sentences":["For any simple-root constacyclic code $\\mathcal{C}$ over a finite field $\\mathbb{F}_q$, as far as we know, the group $\\mathcal{G}$ generated by the multiplier, the constacyclic shift and the scalar multiplications is the largest subgroup of the automorphism group ${\\rm Aut}(\\mathcal{C})$ of $\\mathcal{C}$. In this paper, by calculating the number of $\\mathcal{G}$-orbits of $\\mathcal{C}\\backslash\\{\\bf 0\\}$, we give an explicit upper bound on the number of non-zero weights of $\\mathcal{C}$ and present a necessary and sufficient condition for $\\mathcal{C}$ to meet the upper bound.","Some examples in this paper show that our upper bound is tight and better than the upper bounds in [Zhang and Cao, FFA, 2024].","In particular, our main results provide a new method to construct few-weight constacyclic codes.","Furthermore, for the constacyclic code $\\mathcal{C}$ belonging to two special types, we obtain a smaller upper bound on the number of non-zero weights of $\\mathcal{C}$ by substituting $\\mathcal{G}$ with a larger subgroup of ${\\rm Aut}(\\mathcal{C})$.","The results derived in this paper generalize the main results in [Chen, Fu and Liu, IEEE-TIT, 2024]}."],"url":"http://arxiv.org/abs/2405.00309v1","category":"cs.IT"}
{"created":"2024-05-01 04:24:37","title":"FPGA Digital Dice using Pseudo Random Number Generator","abstract":"The goal of this project is to design a digital dice that displays dice numbers in real-time. The number is generated by a pseudo-random number generator (PRNG) using XORshift algorithm that is implemented in Verilog HDL on an FPGA. The digital dice is equipped with tilt sensor, display, power management circuit, and rechargeable battery hosted in a 3D printed dice casing. By shaking the digital dice, the tilt sensor signal produces a seed for the PRNG. This digital dice demonstrates a set of possible random numbers of 2, 4, 6, 8, 10, 12, 20, 100 that simulate the number of dice sides. The kit is named SUTDicey.","sentences":["The goal of this project is to design a digital dice that displays dice numbers in real-time.","The number is generated by a pseudo-random number generator (PRNG) using XORshift algorithm that is implemented in Verilog HDL on an FPGA.","The digital dice is equipped with tilt sensor, display, power management circuit, and rechargeable battery hosted in a 3D printed dice casing.","By shaking the digital dice, the tilt sensor signal produces a seed for the PRNG.","This digital dice demonstrates a set of possible random numbers of 2, 4, 6, 8, 10, 12, 20, 100 that simulate the number of dice sides.","The kit is named SUTDicey."],"url":"http://arxiv.org/abs/2405.00308v1","category":"cs.CR"}
{"created":"2024-05-01 04:05:29","title":"Active Learning with Task Adaptation Pre-training for Speech Emotion Recognition","abstract":"Speech emotion recognition (SER) has garnered increasing attention due to its wide range of applications in various fields, including human-machine interaction, virtual assistants, and mental health assistance. However, existing SER methods often overlook the information gap between the pre-training speech recognition task and the downstream SER task, resulting in sub-optimal performance. Moreover, current methods require much time for fine-tuning on each specific speech dataset, such as IEMOCAP, which limits their effectiveness in real-world scenarios with large-scale noisy data. To address these issues, we propose an active learning (AL)-based fine-tuning framework for SER, called \\textsc{After}, that leverages task adaptation pre-training (TAPT) and AL methods to enhance performance and efficiency. Specifically, we first use TAPT to minimize the information gap between the pre-training speech recognition task and the downstream speech emotion recognition task. Then, AL methods are employed to iteratively select a subset of the most informative and diverse samples for fine-tuning, thereby reducing time consumption. Experiments demonstrate that our proposed method \\textsc{After}, using only 20\\% of samples, improves accuracy by 8.45\\% and reduces time consumption by 79\\%. The additional extension of \\textsc{After} and ablation studies further confirm its effectiveness and applicability to various real-world scenarios. Our source code is available on Github for reproducibility. (https://github.com/Clearloveyuan/AFTER).","sentences":["Speech emotion recognition (SER) has garnered increasing attention due to its wide range of applications in various fields, including human-machine interaction, virtual assistants, and mental health assistance.","However, existing SER methods often overlook the information gap between the pre-training speech recognition task and the downstream SER task, resulting in sub-optimal performance.","Moreover, current methods require much time for fine-tuning on each specific speech dataset, such as IEMOCAP, which limits their effectiveness in real-world scenarios with large-scale noisy data.","To address these issues, we propose an active learning (AL)-based fine-tuning framework for SER, called \\textsc{After}, that leverages task adaptation pre-training (TAPT) and AL methods to enhance performance and efficiency.","Specifically, we first use TAPT to minimize the information gap between the pre-training speech recognition task and the downstream speech emotion recognition task.","Then, AL methods are employed to iteratively select a subset of the most informative and diverse samples for fine-tuning, thereby reducing time consumption.","Experiments demonstrate that our proposed method \\textsc{After}, using only 20\\% of samples, improves accuracy by 8.45\\% and reduces time consumption by 79\\%.","The additional extension of \\textsc{After} and ablation studies further confirm its effectiveness and applicability to various real-world scenarios.","Our source code is available on Github for reproducibility.","(https://github.com/Clearloveyuan/AFTER)."],"url":"http://arxiv.org/abs/2405.00307v1","category":"cs.SD"}
{"created":"2024-05-01 03:52:39","title":"Generating Feedback-Ladders for Logical Errors in Programming using Large Language Models","abstract":"In feedback generation for logical errors in programming assignments, large language model (LLM)-based methods have shown great promise. These methods ask the LLM to generate feedback given the problem statement and a student's (buggy) submission. There are several issues with these types of methods. First, the generated feedback messages are often too direct in revealing the error in the submission and thus diminish valuable opportunities for the student to learn. Second, they do not consider the student's learning context, i.e., their previous submissions, current knowledge, etc. Third, they are not layered since existing methods use a single, shared prompt for all student submissions. In this paper, we explore using LLMs to generate a \"feedback-ladder\", i.e., multiple levels of feedback for the same problem-submission pair. We evaluate the quality of the generated feedback-ladder via a user study with students, educators, and researchers. We have observed diminishing effectiveness for higher-level feedback and higher-scoring submissions overall in the study. In practice, our method enables teachers to select an appropriate level of feedback to show to a student based on their personal learning context, or in a progressive manner to go more detailed if a higher-level feedback fails to correct the student's error.","sentences":["In feedback generation for logical errors in programming assignments, large language model (LLM)-based methods have shown great promise.","These methods ask the LLM to generate feedback given the problem statement and a student's (buggy) submission.","There are several issues with these types of methods.","First, the generated feedback messages are often too direct in revealing the error in the submission and thus diminish valuable opportunities for the student to learn.","Second, they do not consider the student's learning context, i.e., their previous submissions, current knowledge, etc.","Third, they are not layered since existing methods use a single, shared prompt for all student submissions.","In this paper, we explore using LLMs to generate a \"feedback-ladder\", i.e., multiple levels of feedback for the same problem-submission pair.","We evaluate the quality of the generated feedback-ladder via a user study with students, educators, and researchers.","We have observed diminishing effectiveness for higher-level feedback and higher-scoring submissions overall in the study.","In practice, our method enables teachers to select an appropriate level of feedback to show to a student based on their personal learning context, or in a progressive manner to go more detailed if a higher-level feedback fails to correct the student's error."],"url":"http://arxiv.org/abs/2405.00302v1","category":"cs.CL"}
{"created":"2024-05-01 03:50:09","title":"LITO: Learnable Intervention for Truthfulness Optimization","abstract":"Large language models (LLMs) can generate long-form and coherent text, but they still frequently hallucinate facts, thus limiting their reliability. To address this issue, inference-time methods that elicit truthful responses have been proposed by shifting LLM representations towards learned \"truthful directions\". However, applying the truthful directions with the same intensity fails to generalize across different question contexts. We propose LITO, a Learnable Intervention method for Truthfulness Optimization that automatically identifies the optimal intervention intensity tailored to a specific context. LITO explores a sequence of model generations based on increasing levels of intervention intensities. It selects the most accurate response or refuses to answer when the predictions are highly uncertain. Experiments on multiple LLMs and question-answering datasets demonstrate that LITO improves truthfulness while preserving task accuracy. The adaptive nature of LITO counters issues with one-size-fits-all intervention-based solutions, maximizing model truthfulness by reflecting internal knowledge only when the model is confident.","sentences":["Large language models (LLMs) can generate long-form and coherent text, but they still frequently hallucinate facts, thus limiting their reliability.","To address this issue, inference-time methods that elicit truthful responses have been proposed by shifting LLM representations towards learned \"truthful directions\".","However, applying the truthful directions with the same intensity fails to generalize across different question contexts.","We propose LITO, a Learnable Intervention method for Truthfulness Optimization that automatically identifies the optimal intervention intensity tailored to a specific context.","LITO explores a sequence of model generations based on increasing levels of intervention intensities.","It selects the most accurate response or refuses to answer when the predictions are highly uncertain.","Experiments on multiple LLMs and question-answering datasets demonstrate that LITO improves truthfulness while preserving task accuracy.","The adaptive nature of LITO counters issues with one-size-fits-all intervention-based solutions, maximizing model truthfulness by reflecting internal knowledge only when the model is confident."],"url":"http://arxiv.org/abs/2405.00301v1","category":"cs.CL"}
{"created":"2024-05-01 03:46:12","title":"Generalized Cayley graphs of complete groups","abstract":"A group $G$ is complete group if it satisfies $Z(G) = e$ and Aut$(G)$ = Inn$(G)$. In this paper, we study the basic properties of generalized Cayley graphs and characterize several classes of isomorphic generalized Cayley graphs of complete group. As an application, we complete the classification of restricted GCI-groups for symmetric groups.","sentences":["A group $G$ is complete group if it satisfies $Z(G) = e$ and Aut$(G)$ = Inn$(G)$.","In this paper, we study the basic properties of generalized Cayley graphs and characterize several classes of isomorphic generalized Cayley graphs of complete group.","As an application, we complete the classification of restricted GCI-groups for symmetric groups."],"url":"http://arxiv.org/abs/2405.00297v1","category":"math.CO"}
{"created":"2024-05-01 03:27:58","title":"Proof of Sampling: A Nash Equilibrium-Secured Verification Protocol for Decentralized Systems","abstract":"This paper presents a secure and versatile sampling-based verification protocol, Proof of Sampling (PoSP) protocol, suitable for a wide range of decentralized applications. Our protocol has a pure strategy Nash Equilibrium, which compels rational participants to act honestly, thus fortifying the network's integrity. This design effectively eliminates the possibility of free-riding, achieving this with manageable computational overhead. When applied to decentralized inference for AI applications, we design spML based on PoSP protocol, which ingeniously amalgamates the strengths of optimistic fraud proof and zero knowledge proof based approaches, the foremost approaches in the domain at present. Within the realm of Layer 2 solutions, our protocol sp-rollups addresses the security vulnerabilities of current optimistic rollups, which include a risk of undetected fraud due to reliance on mixed strategy equilibria, all the while keeping the computational overhead within reasonable bounds. Moreover, the PoSP protocol can be effectively utilized for designing verification mechanisms within Actively Validated Services (AVS) in EigenLayer, further broadening its applicability. This innovative approach not only enhances the security and efficiency of decentralized systems but also paves the way for a new generation of scalable and reliable decentralized applications.","sentences":["This paper presents a secure and versatile sampling-based verification protocol, Proof of Sampling (PoSP) protocol, suitable for a wide range of decentralized applications.","Our protocol has a pure strategy Nash Equilibrium, which compels rational participants to act honestly, thus fortifying the network's integrity.","This design effectively eliminates the possibility of free-riding, achieving this with manageable computational overhead.","When applied to decentralized inference for AI applications, we design spML based on PoSP protocol, which ingeniously amalgamates the strengths of optimistic fraud proof and zero knowledge proof based approaches, the foremost approaches in the domain at present.","Within the realm of Layer 2 solutions, our protocol sp-rollups addresses the security vulnerabilities of current optimistic rollups, which include a risk of undetected fraud due to reliance on mixed strategy equilibria, all the while keeping the computational overhead within reasonable bounds.","Moreover, the PoSP protocol can be effectively utilized for designing verification mechanisms within Actively Validated Services (AVS) in EigenLayer, further broadening its applicability.","This innovative approach not only enhances the security and efficiency of decentralized systems but also paves the way for a new generation of scalable and reliable decentralized applications."],"url":"http://arxiv.org/abs/2405.00295v1","category":"cs.GT"}
{"created":"2024-05-01 03:25:31","title":"Conformal inference for random objects","abstract":"We develop an inferential toolkit for analyzing object-valued responses, which correspond to data situated in general metric spaces, paired with Euclidean predictors within the conformal framework. To this end we introduce conditional profile average transport costs, where we compare distance profiles that correspond to one-dimensional distributions of probability mass falling into balls of increasing radius through the optimal transport cost when moving from one distance profile to another. The average transport cost to transport a given distance profile to all others is crucial for statistical inference in metric spaces and underpins the proposed conditional profile scores. A key feature of the proposed approach is to utilize the distribution of conditional profile average transport costs as conformity score for general metric space-valued responses, which facilitates the construction of prediction sets by the split conformal algorithm. We derive the uniform convergence rate of the proposed conformity score estimators and establish asymptotic conditional validity for the prediction sets. The finite sample performance for synthetic data in various metric spaces demonstrates that the proposed conditional profile score outperforms existing methods in terms of both coverage level and size of the resulting prediction sets, even in the special case of scalar and thus Euclidean responses. We also demonstrate the practical utility of conditional profile scores for network data from New York taxi trips and for compositional data reflecting energy sourcing of U.S. states.","sentences":["We develop an inferential toolkit for analyzing object-valued responses, which correspond to data situated in general metric spaces, paired with Euclidean predictors within the conformal framework.","To this end we introduce conditional profile average transport costs, where we compare distance profiles that correspond to one-dimensional distributions of probability mass falling into balls of increasing radius through the optimal transport cost when moving from one distance profile to another.","The average transport cost to transport a given distance profile to all others is crucial for statistical inference in metric spaces and underpins the proposed conditional profile scores.","A key feature of the proposed approach is to utilize the distribution of conditional profile average transport costs as conformity score for general metric space-valued responses, which facilitates the construction of prediction sets by the split conformal algorithm.","We derive the uniform convergence rate of the proposed conformity score estimators and establish asymptotic conditional validity for the prediction sets.","The finite sample performance for synthetic data in various metric spaces demonstrates that the proposed conditional profile score outperforms existing methods in terms of both coverage level and size of the resulting prediction sets, even in the special case of scalar and thus Euclidean responses.","We also demonstrate the practical utility of conditional profile scores for network data from New York taxi trips and for compositional data reflecting energy sourcing of U.S. states."],"url":"http://arxiv.org/abs/2405.00294v1","category":"stat.ME"}
{"created":"2024-05-01 03:05:59","title":"Phase shifts, band geometry and responses in triple-Q charge and spin density waves","abstract":"Recently, there has been growing interest in the impacts of phase shifts within the triple-Q spin density wave (SDW) order parameters. Concurrently, it is widely recognized that incommensurate triple-Q charge density waves (CDW) are also prevalent in low-dimensional materials, where the phase degrees of freedom in the order parameters are generally allowed. In this study, we systematically investigate the pivotal effects arising from both triple-Q CDW and SDW order parameters, with particular consideration given to possible phase shifts. We show that the phase shifts play a crucial role in determining the real-space topology of triple-Q density waves. More importantly, we show that the triple-Q CDW and SDW order parameters would influence the band geometry in the momentum space, where multiband Dirac-like fermions are induced by the triple-Q density wave order parameters near the Fermi energy. Furthermore, we explicitly establish that such nontrivial band geometry, combined with symmetry-breaking induced by phase shifts, leads to a variety of intriguing linear and nonlinear responses.","sentences":["Recently, there has been growing interest in the impacts of phase shifts within the triple-Q spin density wave (SDW) order parameters.","Concurrently, it is widely recognized that incommensurate triple-Q charge density waves (CDW) are also prevalent in low-dimensional materials, where the phase degrees of freedom in the order parameters are generally allowed.","In this study, we systematically investigate the pivotal effects arising from both triple-Q CDW and SDW order parameters, with particular consideration given to possible phase shifts.","We show that the phase shifts play a crucial role in determining the real-space topology of triple-Q density waves.","More importantly, we show that the triple-Q CDW and SDW order parameters would influence the band geometry in the momentum space, where multiband Dirac-like fermions are induced by the triple-Q density wave order parameters near the Fermi energy.","Furthermore, we explicitly establish that such nontrivial band geometry, combined with symmetry-breaking induced by phase shifts, leads to a variety of intriguing linear and nonlinear responses."],"url":"http://arxiv.org/abs/2405.00292v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-01 02:59:10","title":"How Can I Improve? Using GPT to Highlight the Desired and Undesired Parts of Open-ended Responses","abstract":"Automated explanatory feedback systems play a crucial role in facilitating learning for a large cohort of learners by offering feedback that incorporates explanations, significantly enhancing the learning process. However, delivering such explanatory feedback in real-time poses challenges, particularly when high classification accuracy for domain-specific, nuanced responses is essential. Our study leverages the capabilities of large language models, specifically Generative Pre-Trained Transformers (GPT), to explore a sequence labeling approach focused on identifying components of desired and less desired praise for providing explanatory feedback within a tutor training dataset. Our aim is to equip tutors with actionable, explanatory feedback during online training lessons. To investigate the potential of GPT models for providing the explanatory feedback, we employed two commonly-used approaches: prompting and fine-tuning. To quantify the quality of highlighted praise components identified by GPT models, we introduced a Modified Intersection over Union (M-IoU) score. Our findings demonstrate that: (1) the M-IoU score effectively correlates with human judgment in evaluating sequence quality; (2) using two-shot prompting on GPT-3.5 resulted in decent performance in recognizing effort-based (M-IoU of 0.46) and outcome-based praise (M-IoU of 0.68); and (3) our optimally fine-tuned GPT-3.5 model achieved M-IoU scores of 0.64 for effort-based praise and 0.84 for outcome-based praise, aligning with the satisfaction levels evaluated by human coders. Our results show promise for using GPT models to provide feedback that focuses on specific elements in their open-ended responses that are desirable or could use improvement.","sentences":["Automated explanatory feedback systems play a crucial role in facilitating learning for a large cohort of learners by offering feedback that incorporates explanations, significantly enhancing the learning process.","However, delivering such explanatory feedback in real-time poses challenges, particularly when high classification accuracy for domain-specific, nuanced responses is essential.","Our study leverages the capabilities of large language models, specifically Generative Pre-Trained Transformers (GPT), to explore a sequence labeling approach focused on identifying components of desired and less desired praise for providing explanatory feedback within a tutor training dataset.","Our aim is to equip tutors with actionable, explanatory feedback during online training lessons.","To investigate the potential of GPT models for providing the explanatory feedback, we employed two commonly-used approaches: prompting and fine-tuning.","To quantify the quality of highlighted praise components identified by GPT models, we introduced a Modified Intersection over Union (M-IoU) score.","Our findings demonstrate that: (1) the M-IoU score effectively correlates with human judgment in evaluating sequence quality; (2) using two-shot prompting on GPT-3.5 resulted in decent performance in recognizing effort-based (M-IoU of 0.46) and outcome-based praise (M-IoU of 0.68); and (3) our optimally fine-tuned GPT-3.5 model achieved M-IoU scores of 0.64 for effort-based praise and 0.84 for outcome-based praise, aligning with the satisfaction levels evaluated by human coders.","Our results show promise for using GPT models to provide feedback that focuses on specific elements in their open-ended responses that are desirable or could use improvement."],"url":"http://arxiv.org/abs/2405.00291v1","category":"cs.CL"}
{"created":"2024-05-01 02:49:18","title":"Adversarial Attacks and Defense for Conversation Entailment Task","abstract":"Large language models (LLMs) that are proved to be very powerful on different NLP tasks. However, there are still many ways to attack the model with very low costs. How to defend the model becomes an important problem. In our work, we treat adversarial attack results as a new (unseen) domain of the model, and we frame the defending problem into how to improve the robustness of the model on the new domain. We focus on the task of conversation entailment, where multi-turn natural language dialogues are the premise, and the transformer model is fine-tuned to predict whether a given hypothesis about the given dialogue is true or false. The adversary would attack the hypothesis to fool the model to make the wrong predictions. We apply synonym-swapping as the attack method. To show the robustness of the model, we implement some fine-tuning strategies and propose the embedding perturbation loss as a method to improve the robustness of the model. Finally, we show the importance of our work by discussing the adversarial attacks in NLP in the real world.","sentences":["Large language models (LLMs) that are proved to be very powerful on different NLP tasks.","However, there are still many ways to attack the model with very low costs.","How to defend the model becomes an important problem.","In our work, we treat adversarial attack results as a new (unseen) domain of the model, and we frame the defending problem into how to improve the robustness of the model on the new domain.","We focus on the task of conversation entailment, where multi-turn natural language dialogues are the premise, and the transformer model is fine-tuned to predict whether a given hypothesis about the given dialogue is true or false.","The adversary would attack the hypothesis to fool the model to make the wrong predictions.","We apply synonym-swapping as the attack method.","To show the robustness of the model, we implement some fine-tuning strategies and propose the embedding perturbation loss as a method to improve the robustness of the model.","Finally, we show the importance of our work by discussing the adversarial attacks in NLP in the real world."],"url":"http://arxiv.org/abs/2405.00289v1","category":"cs.CL"}
{"created":"2024-05-01 02:28:21","title":"Intrinsic phase based proper orthogonal decomposition (IPhaB POD): a method for physically interpretable modes in near-periodic systems","abstract":"Fluid dynamics systems driven by dominant, nearly periodic large-scale dynamics are common across wakes, jets, rotating machinery, and high-speed flows. Traditional decomposition techniques such as proper orthogonal decomposition and dynamic mode decomposition have been used to gain insight into these flows, but can require many modes to represent physical processes. With the aim of generating modes that intuitively convey the underlying physical mechanisms, we propose an intrinsic phase-based proper orthogonal decomposition (IPhaB POD) method. IPhaB POD creates energetically ranked modes that evolve along a characteristic cycle of a driving near-periodic large scale. Our proposed formulation is set in the time domain, which is particularly useful in cases where the large-scale dynamics are imperfectly periodic. We formally derive IPhaB POD within a proper orthogonal decomposition framework, and it inherits the optimal representation inherent to POD. As part of this derivation, a dynamical systems representation of the large scale is utilized, facilitating a definition of phase relative to the large scale within the time domain. An expectation operator and inner product are also constructed relative to this definition of phase in a manner that allows for the various cycles within the data to demonstrate imperfect periodicity. The formulation is tested on two sample problems: a simple, low Reynolds number airfoil wake, and a complex, high-speed pulsating shock wave problem. The resulting modes are shown to better isolate the large-scale dynamics in the first mode than space-only proper orthogonal decomposition, and to highlight meaningful small-scale dynamics in higher modes for the shock flow problem.","sentences":["Fluid dynamics systems driven by dominant, nearly periodic large-scale dynamics are common across wakes, jets, rotating machinery, and high-speed flows.","Traditional decomposition techniques such as proper orthogonal decomposition and dynamic mode decomposition have been used to gain insight into these flows, but can require many modes to represent physical processes.","With the aim of generating modes that intuitively convey the underlying physical mechanisms, we propose an intrinsic phase-based proper orthogonal decomposition (IPhaB POD) method.","IPhaB POD creates energetically ranked modes that evolve along a characteristic cycle of a driving near-periodic large scale.","Our proposed formulation is set in the time domain, which is particularly useful in cases where the large-scale dynamics are imperfectly periodic.","We formally derive IPhaB POD within a proper orthogonal decomposition framework, and it inherits the optimal representation inherent to POD.","As part of this derivation, a dynamical systems representation of the large scale is utilized, facilitating a definition of phase relative to the large scale within the time domain.","An expectation operator and inner product are also constructed relative to this definition of phase in a manner that allows for the various cycles within the data to demonstrate imperfect periodicity.","The formulation is tested on two sample problems: a simple, low Reynolds number airfoil wake, and a complex, high-speed pulsating shock wave problem.","The resulting modes are shown to better isolate the large-scale dynamics in the first mode than space-only proper orthogonal decomposition, and to highlight meaningful small-scale dynamics in higher modes for the shock flow problem."],"url":"http://arxiv.org/abs/2405.00288v1","category":"physics.flu-dyn"}
{"created":"2024-05-01 02:27:59","title":"Stochastic Sampling for Contrastive Views and Hard Negative Samples in Graph-based Collaborative Filtering","abstract":"Graph-based collaborative filtering (CF) has emerged as a promising approach in recommendation systems. Despite its achievements, graph-based CF models face challenges due to data sparsity and negative sampling. In this paper, we propose a novel Stochastic sampling for i) COntrastive views and ii) hard NEgative samples (SCONE) to overcome these issues. By considering that they are both sampling tasks, we generate dynamic augmented views and diverse hard negative samples via our unified stochastic sampling framework based on score-based generative models. In our comprehensive evaluations with 6 benchmark datasets, our proposed SCONE significantly improves recommendation accuracy and robustness, and demonstrates the superiority of our approach over existing CF models. Furthermore, we prove the efficacy of user-item specific stochastic sampling for addressing the user sparsity and item popularity issues. The integration of the stochastic sampling and graph-based CF obtains the state-of-the-art in personalized recommendation systems, making significant strides in information-rich environments.","sentences":["Graph-based collaborative filtering (CF) has emerged as a promising approach in recommendation systems.","Despite its achievements, graph-based CF models face challenges due to data sparsity and negative sampling.","In this paper, we propose a novel Stochastic sampling for i) COntrastive views and ii) hard NEgative samples (SCONE) to overcome these issues.","By considering that they are both sampling tasks, we generate dynamic augmented views and diverse hard negative samples via our unified stochastic sampling framework based on score-based generative models.","In our comprehensive evaluations with 6 benchmark datasets, our proposed SCONE significantly improves recommendation accuracy and robustness, and demonstrates the superiority of our approach over existing CF models.","Furthermore, we prove the efficacy of user-item specific stochastic sampling for addressing the user sparsity and item popularity issues.","The integration of the stochastic sampling and graph-based CF obtains the state-of-the-art in personalized recommendation systems, making significant strides in information-rich environments."],"url":"http://arxiv.org/abs/2405.00287v1","category":"cs.IR"}
{"created":"2024-05-01 02:26:20","title":"Ultrafast Photocurrent Hysteresis in Photoferroelectric \u03b1-In2Se3","abstract":"The photon-electron interactions are generally volatile and the intricate multiphysics details of photoexcited carrier dynamics are not yet distinguished. How to nonvolatile control the physical state through all-optical means and clarify the intricate physical processes has been a long-term goal pursued in polar materials. Photoferroelectric {\\alpha}-In2Se3 holds the great potential for capturing multimodal nonvolatile states due to the spontaneous reversible in-plane and out-of-plane polarizations and its tunable light-matter interactions arising from the electronic degree of freedom. Here we uncover a nonvolatile zero-bias ultrafast photocurrent hysteresis response with an all-optical scheme, diagnosed by in-plane and out-of-plane terahertz waves emitted from the photoferroelectric {\\alpha}-In2Se3. The mechanism of such ultrafast photocurrent hysteresis emerges as a result of anomalous bulk linear and circular photovoltaic effect synchronously driven by local polarization rearrangement. Utilizing anisotropic ferroelectric kinetics-induced relative phase between the in-plane and out-of-plane directions, we further show flexibly selective chirality, tunable rotational angle, and optimizable ellipticity of terahertz wave polarizations. Our finding offers a promising avenue towards direct ultrafast nonvolatile processing of photocurrent signals through an all-optical scheme.","sentences":["The photon-electron interactions are generally volatile and the intricate multiphysics details of photoexcited carrier dynamics are not yet distinguished.","How to nonvolatile control the physical state through all-optical means and clarify the intricate physical processes has been a long-term goal pursued in polar materials.","Photoferroelectric {\\alpha}-In2Se3 holds the great potential for capturing multimodal nonvolatile states due to the spontaneous reversible in-plane and out-of-plane polarizations and its tunable light-matter interactions arising from the electronic degree of freedom.","Here we uncover a nonvolatile zero-bias ultrafast photocurrent hysteresis response with an all-optical scheme, diagnosed by in-plane and out-of-plane terahertz waves emitted from the photoferroelectric {\\alpha}-In2Se3.","The mechanism of such ultrafast photocurrent hysteresis emerges as a result of anomalous bulk linear and circular photovoltaic effect synchronously driven by local polarization rearrangement.","Utilizing anisotropic ferroelectric kinetics-induced relative phase between the in-plane and out-of-plane directions, we further show flexibly selective chirality, tunable rotational angle, and optimizable ellipticity of terahertz wave polarizations.","Our finding offers a promising avenue towards direct ultrafast nonvolatile processing of photocurrent signals through an all-optical scheme."],"url":"http://arxiv.org/abs/2405.00286v1","category":"physics.optics"}
{"created":"2024-05-01 02:26:13","title":"iMTSP: Solving Min-Max Multiple Traveling Salesman Problem with Imperative Learning","abstract":"This paper considers a Min-Max Multiple Traveling Salesman Problem (MTSP), where the goal is to find a set of tours, one for each agent, to collectively visit all the cities while minimizing the length of the longest tour. Though MTSP has been widely studied, obtaining near-optimal solutions for large-scale problems is still challenging due to its NP-hardness. Recent efforts in data-driven methods face challenges of the need for hard-to-obtain supervision and issues with high variance in gradient estimations, leading to slow convergence and highly suboptimal solutions. We address these issues by reformulating MTSP as a bilevel optimization problem, using the concept of imperative learning (IL). This involves introducing an allocation network that decomposes the MTSP into multiple single-agent traveling salesman problems (TSPs). The longest tour from these TSP solutions is then used to self-supervise the allocation network, resulting in a new self-supervised, bilevel, end-to-end learning framework, which we refer to as imperative MTSP (iMTSP). Additionally, to tackle the high-variance gradient issues during the optimization, we introduce a control variate-based gradient estimation algorithm. Our experiments showed that these innovative designs enable our gradient estimator to converge 20% faster than the advanced reinforcement learning baseline and find up to 80% shorter tour length compared with Google OR-Tools MTSP solver, especially in large-scale problems (e.g. 1000 cities and 15 agents).","sentences":["This paper considers a Min-Max Multiple Traveling Salesman Problem (MTSP), where the goal is to find a set of tours, one for each agent, to collectively visit all the cities while minimizing the length of the longest tour.","Though MTSP has been widely studied, obtaining near-optimal solutions for large-scale problems is still challenging due to its NP-hardness.","Recent efforts in data-driven methods face challenges of the need for hard-to-obtain supervision and issues with high variance in gradient estimations, leading to slow convergence and highly suboptimal solutions.","We address these issues by reformulating MTSP as a bilevel optimization problem, using the concept of imperative learning (IL).","This involves introducing an allocation network that decomposes the MTSP into multiple single-agent traveling salesman problems (TSPs).","The longest tour from these TSP solutions is then used to self-supervise the allocation network, resulting in a new self-supervised, bilevel, end-to-end learning framework, which we refer to as imperative MTSP (iMTSP).","Additionally, to tackle the high-variance gradient issues during the optimization, we introduce a control variate-based gradient estimation algorithm.","Our experiments showed that these innovative designs enable our gradient estimator to converge 20% faster than the advanced reinforcement learning baseline and find up to 80% shorter tour length compared with Google OR-Tools MTSP solver, especially in large-scale problems (e.g. 1000 cities and 15 agents)."],"url":"http://arxiv.org/abs/2405.00285v1","category":"cs.AI"}
{"created":"2024-05-01 02:23:58","title":"An Unstructured Mesh Reaction-Drift-Diffusion Master Equation with Reversible Reactions","abstract":"We develop a convergent reaction-drift-diffusion master equation (CRDDME) to facilitate the study of reaction processes in which spatial transport is influenced by drift due to one-body potential fields within general domain geometries. The generalized CRDDME is obtained through two steps. We first derive an unstructured grid jump process approximation for reversible diffusions, enabling the simulation of drift-diffusion processes where the drift arises due to a conservative field that biases particle motion. Leveraging the Edge-Averaged Finite Element method, our approach preserves detailed balance of drift-diffusion fluxes at equilibrium, and preserves an equilibrium Gibbs-Boltzmann distribution for particles undergoing drift-diffusion on the unstructured mesh. We next formulate a spatially-continuous volume reactivity particle-based reaction-drift-diffusion model for reversible reactions of the form $\\textrm{A} + \\textrm{B} \\leftrightarrow \\textrm{C}$. A finite volume discretization is used to generate jump process approximations to reaction terms in this model. The discretization is developed to ensure the combined reaction-drift-diffusion jump process approximation is consistent with detailed balance of reaction fluxes holding at equilibrium, along with supporting a discrete version of the continuous equilibrium state. The new CRDDME model represents a continuous-time discrete-space jump process approximation to the underlying volume reactivity model. We demonstrate the convergence and accuracy of the new CRDDME through a number of numerical examples, and illustrate its use on an idealized model for membrane protein receptor dynamics in T cell signaling.","sentences":["We develop a convergent reaction-drift-diffusion master equation (CRDDME) to facilitate the study of reaction processes in which spatial transport is influenced by drift due to one-body potential fields within general domain geometries.","The generalized CRDDME is obtained through two steps.","We first derive an unstructured grid jump process approximation for reversible diffusions, enabling the simulation of drift-diffusion processes where the drift arises due to a conservative field that biases particle motion.","Leveraging the Edge-Averaged Finite Element method, our approach preserves detailed balance of drift-diffusion fluxes at equilibrium, and preserves an equilibrium Gibbs-Boltzmann distribution for particles undergoing drift-diffusion on the unstructured mesh.","We next formulate a spatially-continuous volume reactivity particle-based reaction-drift-diffusion model for reversible reactions of the form $\\textrm{A} + \\textrm{B} \\leftrightarrow \\textrm{C}$.","A finite volume discretization is used to generate jump process approximations to reaction terms in this model.","The discretization is developed to ensure the combined reaction-drift-diffusion jump process approximation is consistent with detailed balance of reaction fluxes holding at equilibrium, along with supporting a discrete version of the continuous equilibrium state.","The new CRDDME model represents a continuous-time discrete-space jump process approximation to the underlying volume reactivity model.","We demonstrate the convergence and accuracy of the new CRDDME through a number of numerical examples, and illustrate its use on an idealized model for membrane protein receptor dynamics in T cell signaling."],"url":"http://arxiv.org/abs/2405.00283v1","category":"math.NA"}
{"created":"2024-05-01 02:19:31","title":"MF-OML: Online Mean-Field Reinforcement Learning with Occupation Measures for Large Population Games","abstract":"Reinforcement learning for multi-agent games has attracted lots of attention recently. However, given the challenge of solving Nash equilibria for large population games, existing works with guaranteed polynomial complexities either focus on variants of zero-sum and potential games, or aim at solving (coarse) correlated equilibria, or require access to simulators, or rely on certain assumptions that are hard to verify. This work proposes MF-OML (Mean-Field Occupation-Measure Learning), an online mean-field reinforcement learning algorithm for computing approximate Nash equilibria of large population sequential symmetric games. MF-OML is the first fully polynomial multi-agent reinforcement learning algorithm for provably solving Nash equilibria (up to mean-field approximation gaps that vanish as the number of players $N$ goes to infinity) beyond variants of zero-sum and potential games. When evaluated by the cumulative deviation from Nash equilibria, the algorithm is shown to achieve a high probability regret bound of $\\tilde{O}(M^{3/4}+N^{-1/2}M)$ for games with the strong Lasry-Lions monotonicity condition, and a regret bound of $\\tilde{O}(M^{11/12}+N^{- 1/6}M)$ for games with only the Lasry-Lions monotonicity condition, where $M$ is the total number of episodes and $N$ is the number of agents of the game. As a byproduct, we also obtain the first tractable globally convergent computational algorithm for computing approximate Nash equilibria of monotone mean-field games.","sentences":["Reinforcement learning for multi-agent games has attracted lots of attention recently.","However, given the challenge of solving Nash equilibria for large population games, existing works with guaranteed polynomial complexities either focus on variants of zero-sum and potential games, or aim at solving (coarse) correlated equilibria, or require access to simulators, or rely on certain assumptions that are hard to verify.","This work proposes MF-OML (Mean-Field Occupation-Measure Learning), an online mean-field reinforcement learning algorithm for computing approximate Nash equilibria of large population sequential symmetric games.","MF-OML is the first fully polynomial multi-agent reinforcement learning algorithm for provably solving Nash equilibria (up to mean-field approximation gaps that vanish as the number of players $N$ goes to infinity) beyond variants of zero-sum and potential games.","When evaluated by the cumulative deviation from Nash equilibria, the algorithm is shown to achieve a high probability regret bound of $\\tilde{O}(M^{3/4}+N^{-1/2}M)$ for games with the strong Lasry-Lions monotonicity condition, and a regret bound of $\\tilde{O}(M^{11/12}+N^{- 1/6}M)$ for games with only the Lasry-Lions monotonicity condition, where $M$ is the total number of episodes and $N$ is the number of agents of the game.","As a byproduct, we also obtain the first tractable globally convergent computational algorithm for computing approximate Nash equilibria of monotone mean-field games."],"url":"http://arxiv.org/abs/2405.00282v1","category":"math.OC"}
{"created":"2024-05-01 02:05:21","title":"Beyond Schwarzschild: New Pulsating Coordinates for Spherically Symmetric Metrics","abstract":"Starting from a general transformation for spherically symmetric metrics where g\\_11=-1/g\\_00, we analyze coordinates with the common property of conformal flatness at constant solid angle element. Three general possibilities arise: one where tortoise coordinate appears as the unique solution, other that includes Kruskal-Szekeres coordinates as a very specific case, but that also allows other similar transformations, and finally a new set of coordinates with very different properties than the other two. In particular, this represents any causal patch of the spherically symmetric metrics in a compactified form. We analyze some relations, taking the Schwarzschild case as prototype, but also contrasting the cosmological de-Sitter and Anti-de-Sitter solutions for the new proposed pulsating coordinates.","sentences":["Starting from a general transformation for spherically symmetric metrics where g\\_11=-1/g\\_00, we analyze coordinates with the common property of conformal flatness at constant solid angle element.","Three general possibilities arise: one where tortoise coordinate appears as the unique solution, other that includes Kruskal-Szekeres coordinates as a very specific case, but that also allows other similar transformations, and finally a new set of coordinates with very different properties than the other two.","In particular, this represents any causal patch of the spherically symmetric metrics in a compactified form.","We analyze some relations, taking the Schwarzschild case as prototype, but also contrasting the cosmological de-Sitter and Anti-de-Sitter solutions for the new proposed pulsating coordinates."],"url":"http://arxiv.org/abs/2405.00278v1","category":"gr-qc"}
{"created":"2024-05-01 02:00:32","title":"The strong-coupling quantum thermodynamics of quantum Brownian motion based on the exact solution of its reduced density matrix","abstract":"We derive the quantum thermodynamics of quantum Brownian motion from the exact solution of its reduced density matrix. By exactly traced over all the reservoir states, we solve analytically and exactly the reduced density matrix of the Brownian particle from the total equilibrium thermal state of the system strongly entangling with its reservoir. We find that the reduced Hamiltonian and the reduced partition function of the Brownian particle must be renormalized significantly, as be generally shown in the nonperturbative renormalization theory of quantum thermodynamics we developed recently. A momentum-dependent potential is generated naturally from the linear coupling between the Brownian particle and the reservoir particles, after all the reservoir states are completely traced out. Moreover, beyond the weak coupling limit, it is imperative to take into account the non-negligible changes of the reservoir state induced by the system-reservoir coupling, in order to obtain the correctly reduced partition function of the Brownian particle. Using the exact solutions of the reduced density matrix, the renormalized Hamiltonian and the reduced partition function for the Brownian particle, we show that the controversial results from the different definitions of internal energy and the issue of the negative heat capacity in the previous studies of strong-coupling quantum thermodynamics are resolved.","sentences":["We derive the quantum thermodynamics of quantum Brownian motion from the exact solution of its reduced density matrix.","By exactly traced over all the reservoir states, we solve analytically and exactly the reduced density matrix of the Brownian particle from the total equilibrium thermal state of the system strongly entangling with its reservoir.","We find that the reduced Hamiltonian and the reduced partition function of the Brownian particle must be renormalized significantly, as be generally shown in the nonperturbative renormalization theory of quantum thermodynamics we developed recently.","A momentum-dependent potential is generated naturally from the linear coupling between the Brownian particle and the reservoir particles, after all the reservoir states are completely traced out.","Moreover, beyond the weak coupling limit, it is imperative to take into account the non-negligible changes of the reservoir state induced by the system-reservoir coupling, in order to obtain the correctly reduced partition function of the Brownian particle.","Using the exact solutions of the reduced density matrix, the renormalized Hamiltonian and the reduced partition function for the Brownian particle, we show that the controversial results from the different definitions of internal energy and the issue of the negative heat capacity in the previous studies of strong-coupling quantum thermodynamics are resolved."],"url":"http://arxiv.org/abs/2405.00277v1","category":"quant-ph"}
{"created":"2024-05-01 01:51:09","title":"Large Values of Newform Dedekind Sums","abstract":"We study a generalized Dedekind sum $S_{\\chi_1,\\chi_2}(a,c)$ attached to newform Eisenstein series $E_{\\chi_1,\\chi_2}(z,s)$. Our work shows the Dedekind sum is rarely substantially larger than $\\log^3 c$. The method of proof first relates the size of the Dedekind sum to continued fractions. A result of Hensley from 1991 then controls the average size of the maximal partial quotient in the continued fraction expansion of $a/c$.   We complement this result by computing approximate values of the Dedekind sum in some special cases, which in particular produces examples of large values of the Dedekind sum.","sentences":["We study a generalized Dedekind sum $S_{\\chi_1,\\chi_2}(a,c)$ attached to newform Eisenstein series $E_{\\chi_1,\\chi_2}(z,s)$.","Our work shows the Dedekind sum is rarely substantially larger than $\\log^3 c$.","The method of proof first relates the size of the Dedekind sum to continued fractions.","A result of Hensley from 1991 then controls the average size of the maximal partial quotient in the continued fraction expansion of $a/c$.   We complement this result by computing approximate values of the Dedekind sum in some special cases, which in particular produces examples of large values of the Dedekind sum."],"url":"http://arxiv.org/abs/2405.00274v1","category":"math.NT"}
{"created":"2024-05-01 01:45:50","title":"Social Life Simulation for Non-Cognitive Skills Learning","abstract":"Non-cognitive skills are crucial for personal and social life well-being, and such skill development can be supported by narrative-based (e.g., storytelling) technologies. While generative AI enables interactive and role-playing storytelling, little is known about how users engage with and perceive the use of AI in social life simulation for non-cognitive skills learning. To this end, we introduced SimuLife++, an interactive platform enabled by a large language model (LLM). The system allows users to act as protagonists, creating stories with one or multiple AI-based characters in diverse social scenarios. In particular, we expanded the Human-AI interaction to a Human-AI-AI collaboration by including a sage agent, who acts as a bystander to provide users with more insightful perspectives on their choices and conversations. Through a within-subject user study, we found that the inclusion of the sage agent significantly enhanced narrative immersion, according to the narrative transportation scale, leading to more messages, particularly in group chats. Participants' interactions with the sage agent were also associated with significantly higher scores in their perceived motivation, self-perceptions, and resilience and coping, indicating positive impacts on non-cognitive skills reflection. Participants' interview results further explained the sage agent's aid in decision-making, solving ethical dilemmas, and problem-solving; on the other hand, they suggested improvements in user control and balanced responses from multiple characters. We provide design implications on the application of generative AI in narrative solutions for non-cognitive skill development in broader social contexts.","sentences":["Non-cognitive skills are crucial for personal and social life well-being, and such skill development can be supported by narrative-based (e.g., storytelling) technologies.","While generative AI enables interactive and role-playing storytelling, little is known about how users engage with and perceive the use of AI in social life simulation for non-cognitive skills learning.","To this end, we introduced SimuLife++, an interactive platform enabled by a large language model (LLM).","The system allows users to act as protagonists, creating stories with one or multiple AI-based characters in diverse social scenarios.","In particular, we expanded the Human-AI interaction to a Human-AI-AI collaboration by including a sage agent, who acts as a bystander to provide users with more insightful perspectives on their choices and conversations.","Through a within-subject user study, we found that the inclusion of the sage agent significantly enhanced narrative immersion, according to the narrative transportation scale, leading to more messages, particularly in group chats.","Participants' interactions with the sage agent were also associated with significantly higher scores in their perceived motivation, self-perceptions, and resilience and coping, indicating positive impacts on non-cognitive skills reflection.","Participants' interview results further explained the sage agent's aid in decision-making, solving ethical dilemmas, and problem-solving; on the other hand, they suggested improvements in user control and balanced responses from multiple characters.","We provide design implications on the application of generative AI in narrative solutions for non-cognitive skill development in broader social contexts."],"url":"http://arxiv.org/abs/2405.00273v1","category":"cs.CL"}
{"created":"2024-05-01 01:33:57","title":"Exact solution of the propagation of ON-OFF signals by dispersive waves","abstract":"The propagation of ON-OFF signals with dispersive waves is examined in this study. An integral-form exact solution for a simple ON-OFF switching event is derived, which holds for any dispersion relation. The integral can be exactly calculated for two types of dispersion relations. Further, the analysis of these solutions shows that the ON-OFF signal propagates with the group velocity and that the boundary thickness of the signal increases with time, typically at a rate proportional to the square root of time, owing to dispersion. Additionally, an approximate solution for a general dispersion relation is derived, and a for a higher-complexity ON-OFF switching pattern is constructed.","sentences":["The propagation of ON-OFF signals with dispersive waves is examined in this study.","An integral-form exact solution for a simple ON-OFF switching event is derived, which holds for any dispersion relation.","The integral can be exactly calculated for two types of dispersion relations.","Further, the analysis of these solutions shows that the ON-OFF signal propagates with the group velocity and that the boundary thickness of the signal increases with time, typically at a rate proportional to the square root of time, owing to dispersion.","Additionally, an approximate solution for a general dispersion relation is derived, and a for a higher-complexity ON-OFF switching pattern is constructed."],"url":"http://arxiv.org/abs/2405.00271v1","category":"math-ph"}
{"created":"2024-05-01 01:25:16","title":"A biased random-key genetic algorithm with variable mutants to solve a vehicle routing problem","abstract":"The paper explores the Biased Random-Key Genetic Algorithm (BRKGA) in the domain of logistics and vehicle routing. Specifically, the application of the algorithm is contextualized within the framework of the Vehicle Routing Problem with Occasional Drivers and Time Window (VRPODTW) that represents a critical challenge in contemporary delivery systems. Within this context, BRKGA emerges as an innovative solution approach to optimize routing plans, balancing cost-efficiency with operational constraints. This research introduces a new BRKGA, characterized by a variable mutant population which can vary from generation to generation, named BRKGA-VM. This novel variant was tested to solve a VRPODTW. For this purpose, an innovative specific decoder procedure was proposed and implemented. Furthermore, a hybridization of the algorithm with a Variable Neighborhood Descent (VND) algorithm has also been considered, showing an improvement of problem-solving capabilities. Computational results show a better performances in term of effectiveness over a previous version of BRKGA, denoted as MP. The improved performance of BRKGA-VM is evident from its ability to optimize solutions across a wide range of scenarios, with significant improvements observed for each type of instance considered. The analysis also reveals that VM achieves preset goals more quickly compared to MP, thanks to the increased variability induced in the mutant population which facilitates the exploration of new regions of the solution space. Furthermore, the integration of VND has shown an additional positive impact on the quality of the solutions found.","sentences":["The paper explores the Biased Random-Key Genetic Algorithm (BRKGA) in the domain of logistics and vehicle routing.","Specifically, the application of the algorithm is contextualized within the framework of the Vehicle Routing Problem with Occasional Drivers and Time Window (VRPODTW) that represents a critical challenge in contemporary delivery systems.","Within this context, BRKGA emerges as an innovative solution approach to optimize routing plans, balancing cost-efficiency with operational constraints.","This research introduces a new BRKGA, characterized by a variable mutant population which can vary from generation to generation, named BRKGA-VM.","This novel variant was tested to solve a VRPODTW.","For this purpose, an innovative specific decoder procedure was proposed and implemented.","Furthermore, a hybridization of the algorithm with a Variable Neighborhood Descent (VND) algorithm has also been considered, showing an improvement of problem-solving capabilities.","Computational results show a better performances in term of effectiveness over a previous version of BRKGA, denoted as MP.","The improved performance of BRKGA-VM is evident from its ability to optimize solutions across a wide range of scenarios, with significant improvements observed for each type of instance considered.","The analysis also reveals that VM achieves preset goals more quickly compared to MP, thanks to the increased variability induced in the mutant population which facilitates the exploration of new regions of the solution space.","Furthermore, the integration of VND has shown an additional positive impact on the quality of the solutions found."],"url":"http://arxiv.org/abs/2405.00268v1","category":"cs.NE"}
{"created":"2024-05-01 01:20:25","title":"Differentially Private Release of Israel's National Registry of Live Births","abstract":"In February 2024, Israel's Ministry of Health released microdata of live births in Israel in 2014. The dataset is based on Israel's National Registry of Live Births and offers substantial value in multiple areas, such as scientific research and policy-making. At the same time, the data was processed so as to protect the privacy of 2014's mothers and newborns. The release was co-designed by the authors together with stakeholders from both inside and outside the Ministry of Health. This paper presents the methodology used to obtain that release. It also describes the considerations involved in choosing the methodology and the process followed.   We used differential privacy as our formal measure of the privacy loss incurred by the released dataset. More concretely, we prove that the released dataset is differentially private with privacy loss budget \\varepsilon = 9.98. We extensively used the private selection algorithm of Liu and Talwar (STOC 2019) to bundle together multiple steps such as data transformation, model generation algorithm, hyperparameter selection, and evaluation. The model generation algorithm selected was PrivBayes (Zhang et al., SIGMOD 2014). The evaluation was based on a list of acceptance criteria, which were also disclosed only approximately so as to provide an overall differential privacy guarantee. We also discuss concrete challenges and barriers that appear relevant to the next steps of this pilot project, as well as to future differentially private releases.","sentences":["In February 2024, Israel's Ministry of Health released microdata of live births in Israel in 2014.","The dataset is based on Israel's National Registry of Live Births and offers substantial value in multiple areas, such as scientific research and policy-making.","At the same time, the data was processed so as to protect the privacy of 2014's mothers and newborns.","The release was co-designed by the authors together with stakeholders from both inside and outside the Ministry of Health.","This paper presents the methodology used to obtain that release.","It also describes the considerations involved in choosing the methodology and the process followed.   ","We used differential privacy as our formal measure of the privacy loss incurred by the released dataset.","More concretely, we prove that the released dataset is differentially private with privacy loss budget \\varepsilon = 9.98.","We extensively used the private selection algorithm of Liu and Talwar (STOC 2019) to bundle together multiple steps such as data transformation, model generation algorithm, hyperparameter selection, and evaluation.","The model generation algorithm selected was PrivBayes (Zhang et al., SIGMOD 2014).","The evaluation was based on a list of acceptance criteria, which were also disclosed only approximately so as to provide an overall differential privacy guarantee.","We also discuss concrete challenges and barriers that appear relevant to the next steps of this pilot project, as well as to future differentially private releases."],"url":"http://arxiv.org/abs/2405.00267v1","category":"cs.CR"}
{"created":"2024-05-01 01:04:31","title":"Robot-As-A-Sensor: Forming a Sensing Network with Robots for Underground Mining Missions","abstract":"Nowadays, robots are deployed as mobile platforms equipped with sensing, communication and computing capabilities, especially in the mining industry, where they perform tasks in hazardous and repetitive environments. Despite their potential, individual robots face significant limitations when completing complex tasks that require the collaboration of multiple robots. This collaboration requires a robust wireless network to ensure operational efficiency and reliability. This paper introduces the concept of \"Robot-As-A-Sensor\" (RAAS), which treats the robots as mobile sensors within structures similar to Wireless Sensor Networks (WSNs). We later identify specific challenges in integrating RAAS technology and propose technological advancements to address these challenges. Finally, we provide an outlook about the technologies that can contribute to realising RAAS, suggesting that this approach could catalyse a shift towards safer, more intelligent, and sustainable industry practices. We believe that this innovative RAAS framework could significantly transform industries requiring advanced technological integration.","sentences":["Nowadays, robots are deployed as mobile platforms equipped with sensing, communication and computing capabilities, especially in the mining industry, where they perform tasks in hazardous and repetitive environments.","Despite their potential, individual robots face significant limitations when completing complex tasks that require the collaboration of multiple robots.","This collaboration requires a robust wireless network to ensure operational efficiency and reliability.","This paper introduces the concept of \"Robot-As-A-Sensor\" (RAAS), which treats the robots as mobile sensors within structures similar to Wireless Sensor Networks (WSNs).","We later identify specific challenges in integrating RAAS technology and propose technological advancements to address these challenges.","Finally, we provide an outlook about the technologies that can contribute to realising RAAS, suggesting that this approach could catalyse a shift towards safer, more intelligent, and sustainable industry practices.","We believe that this innovative RAAS framework could significantly transform industries requiring advanced technological integration."],"url":"http://arxiv.org/abs/2405.00266v1","category":"cs.NI"}
{"created":"2024-05-01 00:49:10","title":"Tree independence number II. Three-path-configurations","abstract":"A \\textit{three-paths-configuration} is a graph consisting of three pairwise internally-disjoint paths the union of every two of which is an induced cycle of length at least four. A graph is \\textit{3PC-free} if no induced subgraph of it is a three-paths-configuration. We prove that 3PC-free graphs have poly-logarithmic tree-independence number. More explicitly, we show that there exists a constant $c$ such that every $n$-vertex 3PC-free graph graph has a tree decomposition in which every bag has stability number at most $c (\\log n)^2$. This implies that the \\textsc{Maximum Weight Independent Set} problem, as well as several other natural algorithmic problems, that are known to be \\textsf{NP}-hard in general, can be solved in quasi-polynomial time if the input graph is 3PC-free.","sentences":["A \\textit{three-paths-configuration} is a graph consisting of three pairwise internally-disjoint paths the union of every two of which is an induced cycle of length at least four.","A graph is \\textit{3PC-free} if no induced subgraph of it is a three-paths-configuration.","We prove that 3PC-free graphs have poly-logarithmic tree-independence number.","More explicitly, we show that there exists a constant $c$ such that every $n$-vertex 3PC-free graph graph has a tree decomposition in which every bag has stability number at most $c (\\log n)^2$.","This implies that the \\textsc{Maximum Weight Independent Set} problem, as well as several other natural algorithmic problems, that are known to be \\textsf{NP}-hard in general, can be solved in quasi-polynomial time if the input graph is 3PC-free."],"url":"http://arxiv.org/abs/2405.00265v1","category":"math.CO"}
{"created":"2024-05-01 17:32:17","title":"Particle scale anisotropy controls bulk properties in sheared granular materials","abstract":"The bulk dynamics of dense granular materials arise through a combination of particle-scale and mesoscale effects. Theoretical and numerical studies have shown that collective effects are created by particle-scale anisotropic structures such as grain connectivity (fabric), force transmission, and frictional mobilization, all of which influence bulk properties like bulk friction and the stress tensor through the Stress-Force-Fabric (SFF) relationship. To date, establishing the relevance of these effects to laboratory systems has remained elusive due to the challenge of measuring both normal and frictional contact forces at the particle scale. In this study, we perform experiments on a sheared photoelastic granular system in an quasi-2D annular (Couette) cell. During these experiments, we measure particle locations, contacts, and normal and frictional forces vectors during loading. We reconstruct the angular distributions of the contact and force vectors, and extract the corresponding emergent anisotropies for each of these metrics. Finally, we show that the SFF relation quantitatively predicts the relationship between particle scale anisotropies, the stress tensor components, and the bulk friction coefficient, capturing even transient behaviors. As such, this method shows promise for application to other dense particulate systems where fabric anisotropy can provide a useful measure of bulk friction.","sentences":["The bulk dynamics of dense granular materials arise through a combination of particle-scale and mesoscale effects.","Theoretical and numerical studies have shown that collective effects are created by particle-scale anisotropic structures such as grain connectivity (fabric), force transmission, and frictional mobilization, all of which influence bulk properties like bulk friction and the stress tensor through the Stress-Force-Fabric (SFF) relationship.","To date, establishing the relevance of these effects to laboratory systems has remained elusive due to the challenge of measuring both normal and frictional contact forces at the particle scale.","In this study, we perform experiments on a sheared photoelastic granular system in an quasi-2D annular (Couette) cell.","During these experiments, we measure particle locations, contacts, and normal and frictional forces vectors during loading.","We reconstruct the angular distributions of the contact and force vectors, and extract the corresponding emergent anisotropies for each of these metrics.","Finally, we show that the SFF relation quantitatively predicts the relationship between particle scale anisotropies, the stress tensor components, and the bulk friction coefficient, capturing even transient behaviors.","As such, this method shows promise for application to other dense particulate systems where fabric anisotropy can provide a useful measure of bulk friction."],"url":"http://arxiv.org/abs/2405.00653v1","category":"cond-mat.soft"}
{"created":"2024-05-01 16:35:04","title":"Multigroup Robustness","abstract":"To address the shortcomings of real-world datasets, robust learning algorithms have been designed to overcome arbitrary and indiscriminate data corruption. However, practical processes of gathering data may lead to patterns of data corruption that are localized to specific partitions of the training dataset. Motivated by critical applications where the learned model is deployed to make predictions about people from a rich collection of overlapping subpopulations, we initiate the study of multigroup robust algorithms whose robustness guarantees for each subpopulation only degrade with the amount of data corruption inside that subpopulation. When the data corruption is not distributed uniformly over subpopulations, our algorithms provide more meaningful robustness guarantees than standard guarantees that are oblivious to how the data corruption and the affected subpopulations are related. Our techniques establish a new connection between multigroup fairness and robustness.","sentences":["To address the shortcomings of real-world datasets, robust learning algorithms have been designed to overcome arbitrary and indiscriminate data corruption.","However, practical processes of gathering data may lead to patterns of data corruption that are localized to specific partitions of the training dataset.","Motivated by critical applications where the learned model is deployed to make predictions about people from a rich collection of overlapping subpopulations, we initiate the study of multigroup robust algorithms whose robustness guarantees for each subpopulation only degrade with the amount of data corruption inside that subpopulation.","When the data corruption is not distributed uniformly over subpopulations, our algorithms provide more meaningful robustness guarantees than standard guarantees that are oblivious to how the data corruption and the affected subpopulations are related.","Our techniques establish a new connection between multigroup fairness and robustness."],"url":"http://arxiv.org/abs/2405.00614v1","category":"cs.LG"}
{"created":"2024-05-01 15:25:54","title":"EALD-MLLM: Emotion Analysis in Long-sequential and De-identity videos with Multi-modal Large Language Model","abstract":"Emotion AI is the ability of computers to understand human emotional states. Existing works have achieved promising progress, but two limitations remain to be solved: 1) Previous studies have been more focused on short sequential video emotion analysis while overlooking long sequential video. However, the emotions in short sequential videos only reflect instantaneous emotions, which may be deliberately guided or hidden. In contrast, long sequential videos can reveal authentic emotions; 2) Previous studies commonly utilize various signals such as facial, speech, and even sensitive biological signals (e.g., electrocardiogram). However, due to the increasing demand for privacy, developing Emotion AI without relying on sensitive signals is becoming important. To address the aforementioned limitations, in this paper, we construct a dataset for Emotion Analysis in Long-sequential and De-identity videos called EALD by collecting and processing the sequences of athletes' post-match interviews. In addition to providing annotations of the overall emotional state of each video, we also provide the Non-Facial Body Language (NFBL) annotations for each player. NFBL is an inner-driven emotional expression and can serve as an identity-free clue to understanding the emotional state. Moreover, we provide a simple but effective baseline for further research. More precisely, we evaluate the Multimodal Large Language Models (MLLMs) with de-identification signals (e.g., visual, speech, and NFBLs) to perform emotion analysis. Our experimental results demonstrate that: 1) MLLMs can achieve comparable, even better performance than the supervised single-modal models, even in a zero-shot scenario; 2) NFBL is an important cue in long sequential emotion analysis. EALD will be available on the open-source platform.","sentences":["Emotion AI is the ability of computers to understand human emotional states.","Existing works have achieved promising progress, but two limitations remain to be solved: 1) Previous studies have been more focused on short sequential video emotion analysis while overlooking long sequential video.","However, the emotions in short sequential videos only reflect instantaneous emotions, which may be deliberately guided or hidden.","In contrast, long sequential videos can reveal authentic emotions; 2) Previous studies commonly utilize various signals such as facial, speech, and even sensitive biological signals (e.g., electrocardiogram).","However, due to the increasing demand for privacy, developing Emotion AI without relying on sensitive signals is becoming important.","To address the aforementioned limitations, in this paper, we construct a dataset for Emotion Analysis in Long-sequential and De-identity videos called EALD by collecting and processing the sequences of athletes' post-match interviews.","In addition to providing annotations of the overall emotional state of each video, we also provide the Non-Facial Body Language (NFBL) annotations for each player.","NFBL is an inner-driven emotional expression and can serve as an identity-free clue to understanding the emotional state.","Moreover, we provide a simple but effective baseline for further research.","More precisely, we evaluate the Multimodal Large Language Models (MLLMs) with de-identification signals (e.g., visual, speech, and NFBLs) to perform emotion analysis.","Our experimental results demonstrate that: 1) MLLMs can achieve comparable, even better performance than the supervised single-modal models, even in a zero-shot scenario; 2) NFBL is an important cue in long sequential emotion analysis.","EALD will be available on the open-source platform."],"url":"http://arxiv.org/abs/2405.00574v1","category":"cs.CV"}
{"created":"2024-05-01 14:38:39","title":"Transport of topological defects in a biphasic mixture of active and passive nematic fluids","abstract":"Collectively moving cellular systems often contain a proportion of dead cells or non-motile genotypes. When mixed, nematically aligning motile and non-motile agents are known to segregate spontaneously. However, the role that topological defects and active stresses play in shaping the distribution of the two phases remains unresolved. In this study, we investigate the behaviour of a two-dimensional binary mixture of active and passive nematic fluids to understand how topological defects are transported between the two phases and, ultimately, how this leads to the segregation of topological charges. When the activity of the motile phase is large, and the tension at the interface of motile and non-motile phases is weak, we find that the active phase tends to accumulate $+1/2$ defects and expel $-1/2$ defects so that the motile phase develops a net positive charge. Conversely, when the activity of the motile phase is comparatively small and interfacial tension is strong, the opposite occurs so that the active phase develops a net negative charge. We then use these simulations to develop a physical intuition of the underlying processes that drive the charge segregation. Lastly, we quantify the sensitivity of this process on the other model parameters, by exploring the effect that anchoring strength, orientational elasticity, friction, and volume fraction of the motile phase have on topological charge segregation. As $+1/2$ and $-1/2$ defects have very different effects on interface morphology and fluid transport, this study offers new insights into the spontaneous pattern formation that occurs when motile and non-motile cells interact.","sentences":["Collectively moving cellular systems often contain a proportion of dead cells or non-motile genotypes.","When mixed, nematically aligning motile and non-motile agents are known to segregate spontaneously.","However, the role that topological defects and active stresses play in shaping the distribution of the two phases remains unresolved.","In this study, we investigate the behaviour of a two-dimensional binary mixture of active and passive nematic fluids to understand how topological defects are transported between the two phases and, ultimately, how this leads to the segregation of topological charges.","When the activity of the motile phase is large, and the tension at the interface of motile and non-motile phases is weak, we find that the active phase tends to accumulate $+1/2$ defects and expel $-1/2$ defects so that the motile phase develops a net positive charge.","Conversely, when the activity of the motile phase is comparatively small and interfacial tension is strong, the opposite occurs so that the active phase develops a net negative charge.","We then use these simulations to develop a physical intuition of the underlying processes that drive the charge segregation.","Lastly, we quantify the sensitivity of this process on the other model parameters, by exploring the effect that anchoring strength, orientational elasticity, friction, and volume fraction of the motile phase have on topological charge segregation.","As $+1/2$ and $-1/2$ defects have very different effects on interface morphology and fluid transport, this study offers new insights into the spontaneous pattern formation that occurs when motile and non-motile cells interact."],"url":"http://arxiv.org/abs/2405.00547v1","category":"physics.bio-ph"}
{"created":"2024-05-01 11:36:08","title":"U.S. Election Hardens Hate Universe","abstract":"Local or national politics can trigger potentially dangerous hate in someone. But with a third of the world's population eligible to vote in elections in 2024 alone, we lack understanding of how individual-level hate multiplies up to hate behavior at the collective global scale. Here we show, based on the most recent U.S. election, that offline events are associated with a rapid adaptation of the global online hate universe that hardens (strengthens) both its network-of-networks structure and the 'flavors' of hate content that it collectively produces. Approximately 50 million potential voters in hate communities are drawn closer to each other and to the broad mainstream of approximately 2 billion others. It triggers new hate content at scale around immigration, ethnicity, and antisemitism that aligns with conspiracy theories about Jewish-led replacement before blending in hate around gender identity/sexual orientation, and religion. Telegram acts as a key hardening agent - yet is overlooked by U.S. Congressional hearings and new E.U. legislation. Because the hate universe has remained robust since 2020, anti-hate messaging surrounding not only upcoming elections but also other events like the war in Gaza, should pivot to blending multiple hate 'flavors' while targeting previously untouched social media structures.","sentences":["Local or national politics can trigger potentially dangerous hate in someone.","But with a third of the world's population eligible to vote in elections in 2024 alone, we lack understanding of how individual-level hate multiplies up to hate behavior at the collective global scale.","Here we show, based on the most recent U.S. election, that offline events are associated with a rapid adaptation of the global online hate universe that hardens (strengthens) both its network-of-networks structure and the 'flavors' of hate content that it collectively produces.","Approximately 50 million potential voters in hate communities are drawn closer to each other and to the broad mainstream of approximately 2 billion others.","It triggers new hate content at scale around immigration, ethnicity, and antisemitism that aligns with conspiracy theories about Jewish-led replacement before blending in hate around gender identity/sexual orientation, and religion.","Telegram acts as a key hardening agent - yet is overlooked by U.S. Congressional hearings and new E.U. legislation.","Because the hate universe has remained robust since 2020, anti-hate messaging surrounding not only upcoming elections but also other events like the war in Gaza, should pivot to blending multiple hate 'flavors' while targeting previously untouched social media structures."],"url":"http://arxiv.org/abs/2405.00459v1","category":"cs.SI"}
{"created":"2024-05-01 05:54:33","title":"A Survey on Deep Active Learning: Recent Advances and New Frontiers","abstract":"Active learning seeks to achieve strong performance with fewer training samples. It does this by iteratively asking an oracle to label new selected samples in a human-in-the-loop manner. This technique has gained increasing popularity due to its broad applicability, yet its survey papers, especially for deep learning-based active learning (DAL), remain scarce. Therefore, we conduct an advanced and comprehensive survey on DAL. We first introduce reviewed paper collection and filtering. Second, we formally define the DAL task and summarize the most influential baselines and widely used datasets. Third, we systematically provide a taxonomy of DAL methods from five perspectives, including annotation types, query strategies, deep model architectures, learning paradigms, and training processes, and objectively analyze their strengths and weaknesses. Then, we comprehensively summarize main applications of DAL in Natural Language Processing (NLP), Computer Vision (CV), and Data Mining (DM), etc. Finally, we discuss challenges and perspectives after a detailed analysis of current studies. This work aims to serve as a useful and quick guide for researchers in overcoming difficulties in DAL. We hope that this survey will spur further progress in this burgeoning field.","sentences":["Active learning seeks to achieve strong performance with fewer training samples.","It does this by iteratively asking an oracle to label new selected samples in a human-in-the-loop manner.","This technique has gained increasing popularity due to its broad applicability, yet its survey papers, especially for deep learning-based active learning (DAL), remain scarce.","Therefore, we conduct an advanced and comprehensive survey on DAL.","We first introduce reviewed paper collection and filtering.","Second, we formally define the DAL task and summarize the most influential baselines and widely used datasets.","Third, we systematically provide a taxonomy of DAL methods from five perspectives, including annotation types, query strategies, deep model architectures, learning paradigms, and training processes, and objectively analyze their strengths and weaknesses.","Then, we comprehensively summarize main applications of DAL in Natural Language Processing (NLP), Computer Vision (CV), and Data Mining (DM), etc.","Finally, we discuss challenges and perspectives after a detailed analysis of current studies.","This work aims to serve as a useful and quick guide for researchers in overcoming difficulties in DAL.","We hope that this survey will spur further progress in this burgeoning field."],"url":"http://arxiv.org/abs/2405.00334v1","category":"cs.LG"}
{"created":"2024-05-01 05:15:00","title":"Characterizing Information Seeking Processes with Multiple Physiological Signals","abstract":"Information access systems are getting complex, and our understanding of user behavior during information seeking processes is mainly drawn from qualitative methods, such as observational studies or surveys. Leveraging the advances in sensing technologies, our study aims to characterize user behaviors with physiological signals, particularly in relation to cognitive load, affective arousal, and valence. We conduct a controlled lab study with 26 participants, and collect data including Electrodermal Activities, Photoplethysmogram, Electroencephalogram, and Pupillary Responses. This study examines informational search with four stages: the realization of Information Need (IN), Query Formulation (QF), Query Submission (QS), and Relevance Judgment (RJ). We also include different interaction modalities to represent modern systems, e.g., QS by text-typing or verbalizing, and RJ with text or audio information. We analyze the physiological signals across these stages and report outcomes of pairwise non-parametric repeated-measure statistical tests. The results show that participants experience significantly higher cognitive loads at IN with a subtle increase in alertness, while QF requires higher attention. QS involves demanding cognitive loads than QF. Affective responses are more pronounced at RJ than QS or IN, suggesting greater interest and engagement as knowledge gaps are resolved. To the best of our knowledge, this is the first study that explores user behaviors in a search process employing a more nuanced quantitative analysis of physiological signals. Our findings offer valuable insights into user behavior and emotional responses in information seeking processes. We believe our proposed methodology can inform the characterization of more complex processes, such as conversational information seeking.","sentences":["Information access systems are getting complex, and our understanding of user behavior during information seeking processes is mainly drawn from qualitative methods, such as observational studies or surveys.","Leveraging the advances in sensing technologies, our study aims to characterize user behaviors with physiological signals, particularly in relation to cognitive load, affective arousal, and valence.","We conduct a controlled lab study with 26 participants, and collect data including Electrodermal Activities, Photoplethysmogram, Electroencephalogram, and Pupillary Responses.","This study examines informational search with four stages: the realization of Information Need (IN), Query Formulation (QF), Query Submission (QS), and Relevance Judgment (RJ).","We also include different interaction modalities to represent modern systems, e.g., QS by text-typing or verbalizing, and RJ with text or audio information.","We analyze the physiological signals across these stages and report outcomes of pairwise non-parametric repeated-measure statistical tests.","The results show that participants experience significantly higher cognitive loads at IN with a subtle increase in alertness, while QF requires higher attention.","QS involves demanding cognitive loads than QF.","Affective responses are more pronounced at RJ than QS or IN, suggesting greater interest and engagement as knowledge gaps are resolved.","To the best of our knowledge, this is the first study that explores user behaviors in a search process employing a more nuanced quantitative analysis of physiological signals.","Our findings offer valuable insights into user behavior and emotional responses in information seeking processes.","We believe our proposed methodology can inform the characterization of more complex processes, such as conversational information seeking."],"url":"http://arxiv.org/abs/2405.00322v1","category":"cs.IR"}
{"created":"2024-05-01 04:01:28","title":"Environment-adaptive machine learning potentials","abstract":"The development of interatomic potentials that can effectively capture a wide range of atomic environments is a complex challenge due to several reasons. Materials can exist in numerous structural forms (e.g., crystalline, amorphous, defects, interfaces) and phases (solid, liquid, gas, plasma). Each form may require different treatment in potential modeling to reflect the real physical behavior correctly. Atoms interact through various forces such as electrostatic, van der Waals, ionic bonding, covalent bonding, and metallic bonding, which manifest differently depending on the chemical elements and their electronic structures. Furthermore, the effective interaction among atoms can change with external conditions like temperature, pressure, and chemical environment. Consequently, creating an interatomic potential that performs well across diverse conditions is difficult since optimizing the potential for one set of conditions can lead to a trade-off in the accuracy of predicted properties associated with other conditions. In this paper, we present a method to construct accurate, efficient and transferable interatomic potentials by adapting to the local atomic environment of each atom within a system. The collection of atomic environments of interest is partitioned into several clusters of atomic environments. Each cluster represents a distinctive local environment and is used to define a corresponding local potential. We introduce a many-body many-potential expansion to smoothly blend these local potentials to ensure global continuity of the potential energy surface. This is achieved by computing the probability functions that determine the likelihood of an atom belonging to each cluster. We apply the environment-adaptive machine learning potentials to predict observable properties for Ta element and InP compound, and compare them with density functional theory calculations.","sentences":["The development of interatomic potentials that can effectively capture a wide range of atomic environments is a complex challenge due to several reasons.","Materials can exist in numerous structural forms (e.g., crystalline, amorphous, defects, interfaces) and phases (solid, liquid, gas, plasma).","Each form may require different treatment in potential modeling to reflect the real physical behavior correctly.","Atoms interact through various forces such as electrostatic, van der Waals, ionic bonding, covalent bonding, and metallic bonding, which manifest differently depending on the chemical elements and their electronic structures.","Furthermore, the effective interaction among atoms can change with external conditions like temperature, pressure, and chemical environment.","Consequently, creating an interatomic potential that performs well across diverse conditions is difficult since optimizing the potential for one set of conditions can lead to a trade-off in the accuracy of predicted properties associated with other conditions.","In this paper, we present a method to construct accurate, efficient and transferable interatomic potentials by adapting to the local atomic environment of each atom within a system.","The collection of atomic environments of interest is partitioned into several clusters of atomic environments.","Each cluster represents a distinctive local environment and is used to define a corresponding local potential.","We introduce a many-body many-potential expansion to smoothly blend these local potentials to ensure global continuity of the potential energy surface.","This is achieved by computing the probability functions that determine the likelihood of an atom belonging to each cluster.","We apply the environment-adaptive machine learning potentials to predict observable properties for Ta element and InP compound, and compare them with density functional theory calculations."],"url":"http://arxiv.org/abs/2405.00306v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-01 01:40:14","title":"Search for charged lepton flavor violation in $J/\u03c8$ decays at BESIII","abstract":"In the Standard Model, charged lepton flavor violation (CLFV) is heavily suppressed by tiny neutrino mass, while many theoretical models can enhance CLFV effects up to detectable level. The observation of any CLFV process would be a clear signal of new physics beyond SM. BESIII experiment collected 10 billion $J/\\psi$ data and searched for CLFV processes $J/\\psi\\to e\\tau$ and $e\\mu$. The upper limits at the 90% confidence level are determined to be $\\mathcal{B}(J/\\psi\\to e\\tau)<7.5\\times 10^{-8}$ and $\\mathcal{B}(J/\\psi\\to e\\mu)<4.5\\times 10^{-9}$, respectively. Improving the previously experimental limits by two orders of magnitudes, the results are the most stringent CLFV searches in heavy quarkonium system.","sentences":["In the Standard Model, charged lepton flavor violation (CLFV) is heavily suppressed by tiny neutrino mass, while many theoretical models can enhance CLFV effects up to detectable level.","The observation of any CLFV process would be a clear signal of new physics beyond SM.","BESIII experiment collected 10 billion $J/\\psi$ data and searched for CLFV processes $J/\\psi\\to e\\tau$ and $e\\mu$. The upper limits at the 90% confidence level are determined to be $\\mathcal{B}(J/\\psi\\to e\\tau)<7.5\\times 10^{-8}$ and $\\mathcal{B}(J/\\psi\\to e\\mu)<4.5\\times 10^{-9}$, respectively.","Improving the previously experimental limits by two orders of magnitudes, the results are the most stringent CLFV searches in heavy quarkonium system."],"url":"http://arxiv.org/abs/2405.00272v1","category":"hep-ex"}
{"created":"2024-05-01 00:46:22","title":"Clover: Regressive Lightweight Speculative Decoding with Sequential Knowledge","abstract":"Large language models (LLMs) suffer from low efficiency as the mismatch between the requirement of auto-regressive decoding and the design of most contemporary GPUs. Specifically, billions to trillions of parameters must be loaded to the GPU cache through its limited memory bandwidth for computation, but only a small batch of tokens is actually computed. Consequently, the GPU spends most of its time on memory transfer instead of computation. Recently, parallel decoding, a type of speculative decoding algorithms, is becoming more popular and has demonstrated impressive efficiency improvement in generation. It introduces extra decoding heads to large models, enabling them to predict multiple subsequent tokens simultaneously and verify these candidate continuations in a single decoding step. However, this approach deviates from the training objective of next token prediction used during pre-training, resulting in a low hit rate for candidate tokens. In this paper, we propose a new speculative decoding algorithm, Clover, which integrates sequential knowledge into the parallel decoding process. This enhancement improves the hit rate of speculators and thus boosts the overall efficiency. Clover transmits the sequential knowledge from pre-speculated tokens via the Regressive Connection, then employs an Attention Decoder to integrate these speculated tokens. Additionally, Clover incorporates an Augmenting Block that modifies the hidden states to better align with the purpose of speculative generation rather than next token prediction. The experiment results demonstrate that Clover outperforms the baseline by up to 91% on Baichuan-Small and 146% on Baichuan-Large, respectively, and exceeds the performance of the previously top-performing method, Medusa, by up to 37% on Baichuan-Small and 57% on Baichuan-Large, respectively.","sentences":["Large language models (LLMs) suffer from low efficiency as the mismatch between the requirement of auto-regressive decoding and the design of most contemporary GPUs.","Specifically, billions to trillions of parameters must be loaded to the GPU cache through its limited memory bandwidth for computation, but only a small batch of tokens is actually computed.","Consequently, the GPU spends most of its time on memory transfer instead of computation.","Recently, parallel decoding, a type of speculative decoding algorithms, is becoming more popular and has demonstrated impressive efficiency improvement in generation.","It introduces extra decoding heads to large models, enabling them to predict multiple subsequent tokens simultaneously and verify these candidate continuations in a single decoding step.","However, this approach deviates from the training objective of next token prediction used during pre-training, resulting in a low hit rate for candidate tokens.","In this paper, we propose a new speculative decoding algorithm, Clover, which integrates sequential knowledge into the parallel decoding process.","This enhancement improves the hit rate of speculators and thus boosts the overall efficiency.","Clover transmits the sequential knowledge from pre-speculated tokens via the Regressive Connection, then employs an Attention Decoder to integrate these speculated tokens.","Additionally, Clover incorporates an Augmenting Block that modifies the hidden states to better align with the purpose of speculative generation rather than next token prediction.","The experiment results demonstrate that Clover outperforms the baseline by up to 91% on Baichuan-Small and 146% on Baichuan-Large, respectively, and exceeds the performance of the previously top-performing method, Medusa, by up to 37% on Baichuan-Small and 57% on Baichuan-Large, respectively."],"url":"http://arxiv.org/abs/2405.00263v1","category":"cs.CL"}
{"created":"2024-04-30 23:57:23","title":"Principled RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation","abstract":"Reinforcement learning from human feedback (RLHF) has been an effective technique for aligning AI systems with human values, with remarkable successes in fine-tuning large-language models recently. Most existing RLHF paradigms make the underlying assumption that human preferences are relatively homogeneous, and can be encoded by a single reward model. In this paper, we focus on addressing the issues due to the inherent heterogeneity in human preferences, as well as their potential strategic behavior in providing feedback. Specifically, we propose two frameworks to address heterogeneous human feedback in principled ways: personalization-based one and aggregation-based one. For the former, we propose two approaches based on representation learning and clustering, respectively, for learning multiple reward models that trades off the bias (due to preference heterogeneity) and variance (due to the use of fewer data for learning each model by personalization). We then establish sample complexity guarantees for both approaches. For the latter, we aim to adhere to the single-model framework, as already deployed in the current RLHF paradigm, by carefully aggregating diverse and truthful preferences from humans. We propose two approaches based on reward and preference aggregation, respectively: the former utilizes both utilitarianism and Leximin approaches to aggregate individual reward models, with sample complexity guarantees; the latter directly aggregates the human feedback in the form of probabilistic opinions. Under the probabilistic-opinion-feedback model, we also develop an approach to handle strategic human labelers who may bias and manipulate the aggregated preferences with untruthful feedback. Based on the ideas in mechanism design, our approach ensures truthful preference reporting, with the induced aggregation rule maximizing social welfare functions.","sentences":["Reinforcement learning from human feedback (RLHF) has been an effective technique for aligning AI systems with human values, with remarkable successes in fine-tuning large-language models recently.","Most existing RLHF paradigms make the underlying assumption that human preferences are relatively homogeneous, and can be encoded by a single reward model.","In this paper, we focus on addressing the issues due to the inherent heterogeneity in human preferences, as well as their potential strategic behavior in providing feedback.","Specifically, we propose two frameworks to address heterogeneous human feedback in principled ways: personalization-based one and aggregation-based one.","For the former, we propose two approaches based on representation learning and clustering, respectively, for learning multiple reward models that trades off the bias (due to preference heterogeneity) and variance (due to the use of fewer data for learning each model by personalization).","We then establish sample complexity guarantees for both approaches.","For the latter, we aim to adhere to the single-model framework, as already deployed in the current RLHF paradigm, by carefully aggregating diverse and truthful preferences from humans.","We propose two approaches based on reward and preference aggregation, respectively: the former utilizes both utilitarianism and Leximin approaches to aggregate individual reward models, with sample complexity guarantees; the latter directly aggregates the human feedback in the form of probabilistic opinions.","Under the probabilistic-opinion-feedback model, we also develop an approach to handle strategic human labelers who may bias and manipulate the aggregated preferences with untruthful feedback.","Based on the ideas in mechanism design, our approach ensures truthful preference reporting, with the induced aggregation rule maximizing social welfare functions."],"url":"http://arxiv.org/abs/2405.00254v1","category":"cs.AI"}
{"created":"2024-04-30 23:55:03","title":"Hybrid Quantum-Classical Scheduling for Accelerating Neural Network Training with Newton's Gradient Descent","abstract":"Optimization techniques in deep learning are predominantly led by first-order gradient methodologies, such as SGD. However, neural network training can greatly benefit from the rapid convergence characteristics of second-order optimization. Newton's GD stands out in this category, by rescaling the gradient using the inverse Hessian. Nevertheless, one of its major bottlenecks is matrix inversion, which is notably time-consuming in $O(N^3)$ time with weak scalability.   Matrix inversion can be translated into solving a series of linear equations. Given that quantum linear solver algorithms (QLSAs), leveraging the principles of quantum superposition and entanglement, can operate within a $\\text{polylog}(N)$ time frame, they present a promising approach with exponential acceleration. Specifically, one of the most recent QLSAs demonstrates a complexity scaling of $O(d\\cdot\\kappa \\log(N\\cdot\\kappa/\\epsilon))$, depending on: {size~$N$, condition number~$\\kappa$, error tolerance~$\\epsilon$, quantum oracle sparsity~$d$} of the matrix. However, this also implies that their potential exponential advantage may be hindered by certain properties (i.e. $\\kappa$ and $d$).   We propose Q-Newton, a hybrid quantum-classical scheduler for accelerating neural network training with Newton's GD. Q-Newton utilizes a streamlined scheduling module that coordinates between quantum and classical linear solvers, by estimating & reducing $\\kappa$ and constructing $d$ for the quantum solver.   Our evaluation showcases the potential for Q-Newton to significantly reduce the total training time compared to commonly used optimizers like SGD. We hypothesize a future scenario where the gate time of quantum machines is reduced, possibly realized by attoseconds physics. Our evaluation establishes an ambitious and promising target for the evolution of quantum computing.","sentences":["Optimization techniques in deep learning are predominantly led by first-order gradient methodologies, such as SGD.","However, neural network training can greatly benefit from the rapid convergence characteristics of second-order optimization.","Newton's GD stands out in this category, by rescaling the gradient using the inverse Hessian.","Nevertheless, one of its major bottlenecks is matrix inversion, which is notably time-consuming in $O(N^3)$ time with weak scalability.   ","Matrix inversion can be translated into solving a series of linear equations.","Given that quantum linear solver algorithms (QLSAs), leveraging the principles of quantum superposition and entanglement, can operate within a $\\text{polylog}(N)$ time frame, they present a promising approach with exponential acceleration.","Specifically, one of the most recent QLSAs demonstrates a complexity scaling of $O(d\\cdot\\kappa \\log(N\\cdot\\kappa/\\epsilon))$, depending on: {size~$N$, condition number~$\\kappa$, error tolerance~$\\epsilon$, quantum oracle sparsity~$d$} of the matrix.","However, this also implies that their potential exponential advantage may be hindered by certain properties (i.e. $\\kappa$ and $d$).   ","We propose Q-Newton, a hybrid quantum-classical scheduler for accelerating neural network training with Newton's GD.","Q-Newton utilizes a streamlined scheduling module that coordinates between quantum and classical linear solvers, by estimating & reducing $\\kappa$ and constructing $d$ for the quantum solver.   ","Our evaluation showcases the potential for Q-Newton to significantly reduce the total training time compared to commonly used optimizers like SGD.","We hypothesize a future scenario where the gate time of quantum machines is reduced, possibly realized by attoseconds physics.","Our evaluation establishes an ambitious and promising target for the evolution of quantum computing."],"url":"http://arxiv.org/abs/2405.00252v1","category":"quant-ph"}
{"created":"2024-04-30 23:45:16","title":"SemVecNet: Generalizable Vector Map Generation for Arbitrary Sensor Configurations","abstract":"Vector maps are essential in autonomous driving for tasks like localization and planning, yet their creation and maintenance are notably costly. While recent advances in online vector map generation for autonomous vehicles are promising, current models lack adaptability to different sensor configurations. They tend to overfit to specific sensor poses, leading to decreased performance and higher retraining costs. This limitation hampers their practical use in real-world applications. In response to this challenge, we propose a modular pipeline for vector map generation with improved generalization to sensor configurations. The pipeline leverages probabilistic semantic mapping to generate a bird's-eye-view (BEV) semantic map as an intermediate representation. This intermediate representation is then converted to a vector map using the MapTRv2 decoder. By adopting a BEV semantic map robust to different sensor configurations, our proposed approach significantly improves the generalization performance. We evaluate the model on datasets with sensor configurations not used during training. Our evaluation sets includes larger public datasets, and smaller scale private data collected on our platform. Our model generalizes significantly better than the state-of-the-art methods.","sentences":["Vector maps are essential in autonomous driving for tasks like localization and planning, yet their creation and maintenance are notably costly.","While recent advances in online vector map generation for autonomous vehicles are promising, current models lack adaptability to different sensor configurations.","They tend to overfit to specific sensor poses, leading to decreased performance and higher retraining costs.","This limitation hampers their practical use in real-world applications.","In response to this challenge, we propose a modular pipeline for vector map generation with improved generalization to sensor configurations.","The pipeline leverages probabilistic semantic mapping to generate a bird's-eye-view (BEV) semantic map as an intermediate representation.","This intermediate representation is then converted to a vector map using the MapTRv2 decoder.","By adopting a BEV semantic map robust to different sensor configurations, our proposed approach significantly improves the generalization performance.","We evaluate the model on datasets with sensor configurations not used during training.","Our evaluation sets includes larger public datasets, and smaller scale private data collected on our platform.","Our model generalizes significantly better than the state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.00250v1","category":"cs.CV"}
{"created":"2024-04-30 23:41:00","title":"Who is Authentic Speaker","abstract":"Voice conversion (VC) using deep learning technologies can now generate high quality one-to-many voices and thus has been used in some practical application fields, such as entertainment and healthcare. However, voice conversion can pose potential social issues when manipulated voices are employed for deceptive purposes. Moreover, it is a big challenge to find who are real speakers from the converted voices as the acoustic characteristics of source speakers are changed greatly. In this paper we attempt to explore the feasibility of identifying authentic speakers from converted voices. This study is conducted with the assumption that certain information from the source speakers persists, even when their voices undergo conversion into different target voices. Therefore our experiments are geared towards recognising the source speakers given the converted voices, which are generated by using FragmentVC on the randomly paired utterances from source and target speakers. To improve the robustness against converted voices, our recognition model is constructed by using hierarchical vector of locally aggregated descriptors (VLAD) in deep neural networks. The authentic speaker recognition system is mainly tested in two aspects, including the impact of quality of converted voices and the variations of VLAD. The dataset used in this work is VCTK corpus, where source and target speakers are randomly paired. The results obtained on the converted utterances show promising performances in recognising authentic speakers from converted voices.","sentences":["Voice conversion (VC) using deep learning technologies can now generate high quality one-to-many voices and thus has been used in some practical application fields, such as entertainment and healthcare.","However, voice conversion can pose potential social issues when manipulated voices are employed for deceptive purposes.","Moreover, it is a big challenge to find who are real speakers from the converted voices as the acoustic characteristics of source speakers are changed greatly.","In this paper we attempt to explore the feasibility of identifying authentic speakers from converted voices.","This study is conducted with the assumption that certain information from the source speakers persists, even when their voices undergo conversion into different target voices.","Therefore our experiments are geared towards recognising the source speakers given the converted voices, which are generated by using FragmentVC on the randomly paired utterances from source and target speakers.","To improve the robustness against converted voices, our recognition model is constructed by using hierarchical vector of locally aggregated descriptors (VLAD) in deep neural networks.","The authentic speaker recognition system is mainly tested in two aspects, including the impact of quality of converted voices and the variations of VLAD.","The dataset used in this work is VCTK corpus, where source and target speakers are randomly paired.","The results obtained on the converted utterances show promising performances in recognising authentic speakers from converted voices."],"url":"http://arxiv.org/abs/2405.00248v1","category":"cs.SD"}
{"created":"2024-04-30 23:38:00","title":"The value of non-traditional credentials in the labor market","abstract":"This study investigates the labor market value of credentials obtained from Massive Open Online Courses (MOOCs) and shared on business networking platforms. We conducted a randomized experiment involving more than 800,000 learners, primarily from developing countries and without college degrees, who completed technology or business-related courses on the Coursera platform between September 2022 and March 2023. The intervention targeted learners who had recently completed their courses, encouraging them to share their credentials and simplifying the sharing process. One year after the intervention, we collected data from LinkedIn profiles of approximately 40,000 experimental subjects. We find that the intervention leads to an increase of 17 percentage points for credential sharing. Further, learners in the treatment group were 6\\% more likely to report new employment within a year, with an 8\\% increase in jobs related to their certificates. This effect was more pronounced among LinkedIn users with lower baseline employability. Across the entire sample, the treated group received a higher number of certificate views, indicating an increased interest in their profiles. These results suggest that facilitating credential sharing and reminding learners of the value of skill signaling can yield significant gains. When the experiment is viewed as an encouragement design for credential sharing, we can estimate the local average treatment effect (LATE) of credential sharing (that is, the impact of credential sharing on the workers induced to share by the intervention) for the outcome of getting a job. The LATE estimates are imprecise but large in magnitude; they suggest that credential sharing more than doubles the baseline probability of getting a new job in scope for the credential.","sentences":["This study investigates the labor market value of credentials obtained from Massive Open Online Courses (MOOCs) and shared on business networking platforms.","We conducted a randomized experiment involving more than 800,000 learners, primarily from developing countries and without college degrees, who completed technology or business-related courses on the Coursera platform between September 2022 and March 2023.","The intervention targeted learners who had recently completed their courses, encouraging them to share their credentials and simplifying the sharing process.","One year after the intervention, we collected data from LinkedIn profiles of approximately 40,000 experimental subjects.","We find that the intervention leads to an increase of 17 percentage points for credential sharing.","Further, learners in the treatment group were 6\\% more likely to report new employment within a year, with an 8\\% increase in jobs related to their certificates.","This effect was more pronounced among LinkedIn users with lower baseline employability.","Across the entire sample, the treated group received a higher number of certificate views, indicating an increased interest in their profiles.","These results suggest that facilitating credential sharing and reminding learners of the value of skill signaling can yield significant gains.","When the experiment is viewed as an encouragement design for credential sharing, we can estimate the local average treatment effect (LATE) of credential sharing (that is, the impact of credential sharing on the workers induced to share by the intervention) for the outcome of getting a job.","The LATE estimates are imprecise but large in magnitude; they suggest that credential sharing more than doubles the baseline probability of getting a new job in scope for the credential."],"url":"http://arxiv.org/abs/2405.00247v1","category":"econ.GN"}
{"created":"2024-04-30 23:18:51","title":"Guiding Attention in End-to-End Driving Models","abstract":"Vision-based end-to-end driving models trained by imitation learning can lead to affordable solutions for autonomous driving. However, training these well-performing models usually requires a huge amount of data, while still lacking explicit and intuitive activation maps to reveal the inner workings of these models while driving. In this paper, we study how to guide the attention of these models to improve their driving quality and obtain more intuitive activation maps by adding a loss term during training using salient semantic maps. In contrast to previous work, our method does not require these salient semantic maps to be available during testing time, as well as removing the need to modify the model's architecture to which it is applied. We perform tests using perfect and noisy salient semantic maps with encouraging results in both, the latter of which is inspired by possible errors encountered with real data. Using CIL++ as a representative state-of-the-art model and the CARLA simulator with its standard benchmarks, we conduct experiments that show the effectiveness of our method in training better autonomous driving models, especially when data and computational resources are scarce.","sentences":["Vision-based end-to-end driving models trained by imitation learning can lead to affordable solutions for autonomous driving.","However, training these well-performing models usually requires a huge amount of data, while still lacking explicit and intuitive activation maps to reveal the inner workings of these models while driving.","In this paper, we study how to guide the attention of these models to improve their driving quality and obtain more intuitive activation maps by adding a loss term during training using salient semantic maps.","In contrast to previous work, our method does not require these salient semantic maps to be available during testing time, as well as removing the need to modify the model's architecture to which it is applied.","We perform tests using perfect and noisy salient semantic maps with encouraging results in both, the latter of which is inspired by possible errors encountered with real data.","Using CIL++ as a representative state-of-the-art model and the CARLA simulator with its standard benchmarks, we conduct experiments that show the effectiveness of our method in training better autonomous driving models, especially when data and computational resources are scarce."],"url":"http://arxiv.org/abs/2405.00242v1","category":"cs.CV"}
{"created":"2024-04-30 23:04:36","title":"STT: Stateful Tracking with Transformers for Autonomous Driving","abstract":"Tracking objects in three-dimensional space is critical for autonomous driving. To ensure safety while driving, the tracker must be able to reliably track objects across frames and accurately estimate their states such as velocity and acceleration in the present. Existing works frequently focus on the association task while either neglecting the model performance on state estimation or deploying complex heuristics to predict the states. In this paper, we propose STT, a Stateful Tracking model built with Transformers, that can consistently track objects in the scenes while also predicting their states accurately. STT consumes rich appearance, geometry, and motion signals through long term history of detections and is jointly optimized for both data association and state estimation tasks. Since the standard tracking metrics like MOTA and MOTP do not capture the combined performance of the two tasks in the wider spectrum of object states, we extend them with new metrics called S-MOTA and MOTPS that address this limitation. STT achieves competitive real-time performance on the Waymo Open Dataset.","sentences":["Tracking objects in three-dimensional space is critical for autonomous driving.","To ensure safety while driving, the tracker must be able to reliably track objects across frames and accurately estimate their states such as velocity and acceleration in the present.","Existing works frequently focus on the association task while either neglecting the model performance on state estimation or deploying complex heuristics to predict the states.","In this paper, we propose STT, a Stateful Tracking model built with Transformers, that can consistently track objects in the scenes while also predicting their states accurately.","STT consumes rich appearance, geometry, and motion signals through long term history of detections and is jointly optimized for both data association and state estimation tasks.","Since the standard tracking metrics like MOTA and MOTP do not capture the combined performance of the two tasks in the wider spectrum of object states, we extend them with new metrics called S-MOTA and MOTPS that address this limitation.","STT achieves competitive real-time performance on the Waymo Open Dataset."],"url":"http://arxiv.org/abs/2405.00236v1","category":"cs.RO"}
{"created":"2024-04-30 22:51:36","title":"SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound","abstract":"Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modelling techniques to audio data. However, traditional codecs often operate at high bitrates or within narrow domains such as speech and lack the semantic clues required for efficient language modelling. Addressing these challenges, we introduce SemantiCodec, a novel codec designed to compress audio into fewer than a hundred tokens per second across diverse audio types, including speech, general audio, and music, without compromising quality. SemantiCodec features a dual-encoder architecture: a semantic encoder using a self-supervised AudioMAE, discretized using k-means clustering on extensive audio data, and an acoustic encoder to capture the remaining details. The semantic and acoustic encoder outputs are used to reconstruct audio via a diffusion-model-based decoder. SemantiCodec is presented in three variants with token rates of 25, 50, and 100 per second, supporting a range of ultra-low bit rates between 0.31 kbps and 1.43 kbps. Experimental results demonstrate that SemantiCodec significantly outperforms the state-of-the-art Descript codec on reconstruction quality. Our results also suggest that SemantiCodec contains significantly richer semantic information than all evaluated audio codecs, even at significantly lower bitrates. Our code and demos are available at https://haoheliu.github.io/SemantiCodec/.","sentences":["Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modelling techniques to audio data.","However, traditional codecs often operate at high bitrates or within narrow domains such as speech and lack the semantic clues required for efficient language modelling.","Addressing these challenges, we introduce SemantiCodec, a novel codec designed to compress audio into fewer than a hundred tokens per second across diverse audio types, including speech, general audio, and music, without compromising quality.","SemantiCodec features a dual-encoder architecture: a semantic encoder using a self-supervised AudioMAE, discretized using k-means clustering on extensive audio data, and an acoustic encoder to capture the remaining details.","The semantic and acoustic encoder outputs are used to reconstruct audio via a diffusion-model-based decoder.","SemantiCodec is presented in three variants with token rates of 25, 50, and 100 per second, supporting a range of ultra-low bit rates between 0.31 kbps and 1.43 kbps.","Experimental results demonstrate that SemantiCodec significantly outperforms the state-of-the-art Descript codec on reconstruction quality.","Our results also suggest that SemantiCodec contains significantly richer semantic information than all evaluated audio codecs, even at significantly lower bitrates.","Our code and demos are available at https://haoheliu.github.io/SemantiCodec/."],"url":"http://arxiv.org/abs/2405.00233v1","category":"cs.SD"}
{"created":"2024-04-30 22:33:34","title":"Aptly: Making Mobile Apps from Natural Language","abstract":"We present Aptly, an extension of the MIT App Inventor platform enabling mobile app development via natural language powered by code-generating large language models (LLMs). Aptly complements App Inventor's block language with a text language designed to allow visual code generation via text-based LLMs. We detail the technical aspects of how the Aptly server integrates LLMs with a realtime collaboration function to facilitate the automated creation and editing of mobile apps given user instructions. The paper concludes with insights from a study of a pilot implementation involving high school students, which examines Aptly's practicality and user experience. The findings underscore Aptly's potential as a tool that democratizes app development and fosters technological creativity.","sentences":["We present Aptly, an extension of the MIT App Inventor platform enabling mobile app development via natural language powered by code-generating large language models (LLMs).","Aptly complements App Inventor's block language with a text language designed to allow visual code generation via text-based LLMs.","We detail the technical aspects of how the Aptly server integrates LLMs with a realtime collaboration function to facilitate the automated creation and editing of mobile apps given user instructions.","The paper concludes with insights from a study of a pilot implementation involving high school students, which examines Aptly's practicality and user experience.","The findings underscore Aptly's potential as a tool that democratizes app development and fosters technological creativity."],"url":"http://arxiv.org/abs/2405.00229v1","category":"cs.HC"}
{"created":"2024-04-30 22:11:05","title":"Hacia una implementaci\u00f3n \u00e9tica e inclusiva de la Inteligencia Artificial en las organizaciones: un marco multidimensional","abstract":"The article analyzes the impact of artificial intelligence (AI) on contemporary society and the importance of adopting an ethical approach to its development and implementation within organizations. It examines the critical perspective of French philosopher \\'Eric Sadin and others, who warn of the risks of unbridled technologization that can erode human autonomy. However, the article also recognizes the active role that various actors, such as governments, academics and civil society, can play in shaping the development of AI aligned with human and social values. A multidimensional approach is proposed that combines ethics with regulation, innovation and education. It highlights the importance of developing detailed ethical frameworks, incorporating ethics in the training of professionals, conducting ethical impact audits, and encouraging stakeholder participation in AI design. In addition, four fundamental pillars for the ethical implementation of AI in organizations are presented: 1) Integrated values, 2) Trust and transparency, 3) Empowering human growth, and 4) Identifying strategic factors. These pillars cover aspects such as alignment with the company's ethical identity, governance and accountability, human-centered design, continuous training and adaptability in the face of technological and market changes. It concludes by emphasizing that ethics must be the cornerstone of the strategy of any organization that aspires to incorporate AI, establishing a solid framework to ensure that the technology is developed and used in a way that respects and promotes human values.","sentences":["The article analyzes the impact of artificial intelligence (AI) on contemporary society and the importance of adopting an ethical approach to its development and implementation within organizations.","It examines the critical perspective of French philosopher \\'Eric Sadin and others, who warn of the risks of unbridled technologization that can erode human autonomy.","However, the article also recognizes the active role that various actors, such as governments, academics and civil society, can play in shaping the development of AI aligned with human and social values.","A multidimensional approach is proposed that combines ethics with regulation, innovation and education.","It highlights the importance of developing detailed ethical frameworks, incorporating ethics in the training of professionals, conducting ethical impact audits, and encouraging stakeholder participation in AI design.","In addition, four fundamental pillars for the ethical implementation of AI in organizations are presented: 1) Integrated values, 2) Trust and transparency, 3) Empowering human growth, and 4) Identifying strategic factors.","These pillars cover aspects such as alignment with the company's ethical identity, governance and accountability, human-centered design, continuous training and adaptability in the face of technological and market changes.","It concludes by emphasizing that ethics must be the cornerstone of the strategy of any organization that aspires to incorporate AI, establishing a solid framework to ensure that the technology is developed and used in a way that respects and promotes human values."],"url":"http://arxiv.org/abs/2405.00225v1","category":"cs.CY"}
{"created":"2024-04-30 22:03:17","title":"ConFides: A Visual Analytics Solution for Automated Speech Recognition Analysis and Exploration","abstract":"Confidence scores of automatic speech recognition (ASR) outputs are often inadequately communicated, preventing its seamless integration into analytical workflows. In this paper, we introduce ConFides, a visual analytic system developed in collaboration with intelligence analysts to address this issue. ConFides aims to aid exploration and post-AI-transcription editing by visually representing the confidence associated with the transcription. We demonstrate how our tool can assist intelligence analysts who use ASR outputs in their analytical and exploratory tasks and how it can help mitigate misinterpretation of crucial information. We also discuss opportunities for improving textual data cleaning and model transparency for human-machine collaboration.","sentences":["Confidence scores of automatic speech recognition (ASR) outputs are often inadequately communicated, preventing its seamless integration into analytical workflows.","In this paper, we introduce ConFides, a visual analytic system developed in collaboration with intelligence analysts to address this issue.","ConFides aims to aid exploration and post-AI-transcription editing by visually representing the confidence associated with the transcription.","We demonstrate how our tool can assist intelligence analysts who use ASR outputs in their analytical and exploratory tasks and how it can help mitigate misinterpretation of crucial information.","We also discuss opportunities for improving textual data cleaning and model transparency for human-machine collaboration."],"url":"http://arxiv.org/abs/2405.00223v1","category":"cs.HC"}
{"created":"2024-04-30 21:55:49","title":"Context-Aware Mobile Network Performance Prediction Using Network & Remote Sensing Data","abstract":"Accurate estimation of Network Performance is crucial for several tasks in telecom networks. Telecom networks regularly serve a vast number of radio nodes. Each radio node provides services to end-users in the associated coverage areas. The task of predicting Network Performance for telecom networks necessitates considering complex spatio-temporal interactions and incorporating geospatial information where the radio nodes are deployed. Instead of relying on historical data alone, our approach augments network historical performance datasets with satellite imagery data. Our comprehensive experiments, using real-world data collected from multiple different regions of an operational network, show that the model is robust and can generalize across different scenarios. The results indicate that the model, utilizing satellite imagery, performs very well across the tested regions. Additionally, the model demonstrates a robust approach to the cold-start problem, offering a promising alternative for initial performance estimation in newly deployed sites.","sentences":["Accurate estimation of Network Performance is crucial for several tasks in telecom networks.","Telecom networks regularly serve a vast number of radio nodes.","Each radio node provides services to end-users in the associated coverage areas.","The task of predicting Network Performance for telecom networks necessitates considering complex spatio-temporal interactions and incorporating geospatial information where the radio nodes are deployed.","Instead of relying on historical data alone, our approach augments network historical performance datasets with satellite imagery data.","Our comprehensive experiments, using real-world data collected from multiple different regions of an operational network, show that the model is robust and can generalize across different scenarios.","The results indicate that the model, utilizing satellite imagery, performs very well across the tested regions.","Additionally, the model demonstrates a robust approach to the cold-start problem, offering a promising alternative for initial performance estimation in newly deployed sites."],"url":"http://arxiv.org/abs/2405.00220v1","category":"cs.LG"}
{"created":"2024-04-30 21:52:19","title":"Constrained Decoding for Secure Code Generation","abstract":"Code Large Language Models (Code LLMs) have been increasingly used by developers to boost productivity, but they often generate vulnerable code. Thus, there is an urgent need to ensure that code generated by Code LLMs is correct and secure. Previous research has primarily focused on generating secure code, overlooking the fact that secure code also needs to be correct. This oversight can lead to a false sense of security. Currently, the community lacks a method to measure actual progress in this area, and we need solutions that address both security and correctness of code generation.   This paper introduces a new benchmark, CodeGuard+, along with two new metrics, secure-pass@k and secure@$k_{\\text{pass}}$, to measure Code LLMs' ability to generate both secure and correct code. Using our new evaluation methods, we show that the state-of-the-art defense technique, prefix tuning, may not be as strong as previously believed, since it generates secure code but sacrifices functional correctness. We also demonstrate that different decoding methods significantly affect the security of Code LLMs.   Furthermore, we explore a new defense direction: constrained decoding for secure code generation. We propose new constrained decoding techniques to generate code that satisfies security and correctness constraints simultaneously. Our results reveal that constrained decoding is more effective than prefix tuning to improve the security of Code LLMs, without requiring a specialized training dataset. Moreover, constrained decoding can be used together with prefix tuning to further improve the security of Code LLMs.","sentences":["Code Large Language Models (Code LLMs) have been increasingly used by developers to boost productivity, but they often generate vulnerable code.","Thus, there is an urgent need to ensure that code generated by Code LLMs is correct and secure.","Previous research has primarily focused on generating secure code, overlooking the fact that secure code also needs to be correct.","This oversight can lead to a false sense of security.","Currently, the community lacks a method to measure actual progress in this area, and we need solutions that address both security and correctness of code generation.   ","This paper introduces a new benchmark, CodeGuard+, along with two new metrics, secure-pass@k and secure@$k_{\\text{pass}}$, to measure Code LLMs' ability to generate both secure and correct code.","Using our new evaluation methods, we show that the state-of-the-art defense technique, prefix tuning, may not be as strong as previously believed, since it generates secure code but sacrifices functional correctness.","We also demonstrate that different decoding methods significantly affect the security of Code LLMs.   ","Furthermore, we explore a new defense direction: constrained decoding for secure code generation.","We propose new constrained decoding techniques to generate code that satisfies security and correctness constraints simultaneously.","Our results reveal that constrained decoding is more effective than prefix tuning to improve the security of Code LLMs, without requiring a specialized training dataset.","Moreover, constrained decoding can be used together with prefix tuning to further improve the security of Code LLMs."],"url":"http://arxiv.org/abs/2405.00218v1","category":"cs.CR"}
{"created":"2024-04-30 21:41:53","title":"Graphical Reasoning: LLM-based Semi-Open Relation Extraction","abstract":"This paper presents a comprehensive exploration of relation extraction utilizing advanced language models, specifically Chain of Thought (CoT) and Graphical Reasoning (GRE) techniques. We demonstrate how leveraging in-context learning with GPT-3.5 can significantly enhance the extraction process, particularly through detailed example-based reasoning. Additionally, we introduce a novel graphical reasoning approach that dissects relation extraction into sequential sub-tasks, improving precision and adaptability in processing complex relational data. Our experiments, conducted on multiple datasets, including manually annotated data, show considerable improvements in performance metrics, underscoring the effectiveness of our methodologies.","sentences":["This paper presents a comprehensive exploration of relation extraction utilizing advanced language models, specifically Chain of Thought (CoT) and Graphical Reasoning (GRE) techniques.","We demonstrate how leveraging in-context learning with GPT-3.5 can significantly enhance the extraction process, particularly through detailed example-based reasoning.","Additionally, we introduce a novel graphical reasoning approach that dissects relation extraction into sequential sub-tasks, improving precision and adaptability in processing complex relational data.","Our experiments, conducted on multiple datasets, including manually annotated data, show considerable improvements in performance metrics, underscoring the effectiveness of our methodologies."],"url":"http://arxiv.org/abs/2405.00216v1","category":"cs.CL"}
{"created":"2024-04-30 21:41:34","title":"Quantum thermodynamics of the Caldeira-Leggett model with non-equilibrium Gaussian reservoirs","abstract":"We introduce a non-equilibrium version of the Caldeira-Leggett model in which a quantum particle is strongly coupled to a set of engineered reservoirs. The reservoirs are composed by collections of squeezed and displaced thermal modes, in contrast to the standard case in which the modes are assumed to be at equilibrium. The model proves to be very versatile. Strongly displaced/squeezed reservoirs can be used to generate an effective time dependence in the system Hamiltonian and can be identified as sources of pure work. In the case of squeezing, the time dependence is stochastic and breaks the fluctuation-dissipation relation, this can be reconciled with the second law of thermodynamics by correctly accounting for the energy used to generate the initial non-equilibrium conditions. To go beyond the average description and compute the full heat statistics, we treat squeezing and displacement as generalized Hamiltonians on a modified Keldysh contour. As an application of this technique, we show the quantum-classical correspondence between the heat statistics in the non-equilibrium Caldeira-Leggett model and the statistics of a classical Langevin particle under the action of squeezed and displaced colored noises. Finally, we discuss thermodynamic symmetries of the heat generating function, proving a fluctuation theorem for the energy balance and showing that the conservation of energy at the trajectory level emerges in the classical limit.","sentences":["We introduce a non-equilibrium version of the Caldeira-Leggett model in which a quantum particle is strongly coupled to a set of engineered reservoirs.","The reservoirs are composed by collections of squeezed and displaced thermal modes, in contrast to the standard case in which the modes are assumed to be at equilibrium.","The model proves to be very versatile.","Strongly displaced/squeezed reservoirs can be used to generate an effective time dependence in the system Hamiltonian and can be identified as sources of pure work.","In the case of squeezing, the time dependence is stochastic and breaks the fluctuation-dissipation relation, this can be reconciled with the second law of thermodynamics by correctly accounting for the energy used to generate the initial non-equilibrium conditions.","To go beyond the average description and compute the full heat statistics, we treat squeezing and displacement as generalized Hamiltonians on a modified Keldysh contour.","As an application of this technique, we show the quantum-classical correspondence between the heat statistics in the non-equilibrium Caldeira-Leggett model and the statistics of a classical Langevin particle under the action of squeezed and displaced colored noises.","Finally, we discuss thermodynamic symmetries of the heat generating function, proving a fluctuation theorem for the energy balance and showing that the conservation of energy at the trajectory level emerges in the classical limit."],"url":"http://arxiv.org/abs/2405.00215v1","category":"quant-ph"}
{"created":"2024-04-30 21:37:08","title":"Block-As-Domain Adaptation for Workload Prediction from fNIRS Data","abstract":"Functional near-infrared spectroscopy (fNIRS) is a non-intrusive way to measure cortical hemodynamic activity. Predicting cognitive workload from fNIRS data has taken on a diffuse set of methods. To be applicable in real-world settings, models are needed, which can perform well across different sessions as well as different subjects. However, most existing works assume that training and testing data come from the same subjects and/or cannot generalize well across never-before-seen subjects. Additional challenges imposed by fNIRS data include the high variations in inter-subject fNIRS data and also in intra-subject data collected across different blocks of sessions. To address these issues, we propose an effective method, referred to as the class-aware-block-aware domain adaptation (CABA-DA) which explicitly minimize intra-session variance by viewing different blocks from the same subject same session as different domains. We minimize the intra-class domain discrepancy and maximize the inter-class domain discrepancy accordingly. In addition, we propose an MLPMixer-based model for cognitive load classification. Experimental results demonstrate the proposed model has better performance compared with three different baseline models on three public-available datasets of cognitive workload. Two of them are collected from n-back tasks and one of them is from finger tapping. From our experiments, we also show the proposed contrastive learning method can also improve baseline models we compared with.","sentences":["Functional near-infrared spectroscopy (fNIRS) is a non-intrusive way to measure cortical hemodynamic activity.","Predicting cognitive workload from fNIRS data has taken on a diffuse set of methods.","To be applicable in real-world settings, models are needed, which can perform well across different sessions as well as different subjects.","However, most existing works assume that training and testing data come from the same subjects and/or cannot generalize well across never-before-seen subjects.","Additional challenges imposed by fNIRS data include the high variations in inter-subject fNIRS data and also in intra-subject data collected across different blocks of sessions.","To address these issues, we propose an effective method, referred to as the class-aware-block-aware domain adaptation (CABA-DA) which explicitly minimize intra-session variance by viewing different blocks from the same subject same session as different domains.","We minimize the intra-class domain discrepancy and maximize the inter-class domain discrepancy accordingly.","In addition, we propose an MLPMixer-based model for cognitive load classification.","Experimental results demonstrate the proposed model has better performance compared with three different baseline models on three public-available datasets of cognitive workload.","Two of them are collected from n-back tasks and one of them is from finger tapping.","From our experiments, we also show the proposed contrastive learning method can also improve baseline models we compared with."],"url":"http://arxiv.org/abs/2405.00213v1","category":"cs.LG"}
{"created":"2024-04-30 21:16:38","title":"A Logic for Reasoning About Aggregate-Combine Graph Neural Networks","abstract":"We propose a modal logic in which counting modalities appear in linear inequalities. We show that each formula can be transformed into an equivalent graph neural network (GNN). We also show that a broad class of GNNs can be transformed efficiently into a formula, thus significantly improving upon the literature about the logical expressiveness of GNNs. We also show that the satisfiability problem is PSPACE-complete. These results bring together the promise of using standard logical methods for reasoning about GNNs and their properties, particularly in applications such as GNN querying, equivalence checking, etc. We prove that such natural problems can be solved in polynomial space.","sentences":["We propose a modal logic in which counting modalities appear in linear inequalities.","We show that each formula can be transformed into an equivalent graph neural network (GNN).","We also show that a broad class of GNNs can be transformed efficiently into a formula, thus significantly improving upon the literature about the logical expressiveness of GNNs.","We also show that the satisfiability problem is PSPACE-complete.","These results bring together the promise of using standard logical methods for reasoning about GNNs and their properties, particularly in applications such as GNN querying, equivalence checking, etc.","We prove that such natural problems can be solved in polynomial space."],"url":"http://arxiv.org/abs/2405.00205v1","category":"cs.AI"}
{"created":"2024-04-30 21:15:17","title":"General Purpose Verification for Chain of Thought Prompting","abstract":"Many of the recent capabilities demonstrated by Large Language Models (LLMs) arise primarily from their ability to exploit contextual information. In this paper, we explore ways to improve reasoning capabilities of LLMs through (1) exploration of different chains of thought and (2) validation of the individual steps of the reasoning process. We propose three general principles that a model should adhere to while reasoning: (i) Relevance, (ii) Mathematical Accuracy, and (iii) Logical Consistency. We apply these constraints to the reasoning steps generated by the LLM to improve the accuracy of the final generation. The constraints are applied in the form of verifiers: the model itself is asked to verify if the generated steps satisfy each constraint. To further steer the generations towards high-quality solutions, we use the perplexity of the reasoning steps as an additional verifier. We evaluate our method on 4 distinct types of reasoning tasks, spanning a total of 9 different datasets. Experiments show that our method is always better than vanilla generation, and, in 6 out of the 9 datasets, it is better than best-of N sampling which samples N reasoning chains and picks the lowest perplexity generation.","sentences":["Many of the recent capabilities demonstrated by Large Language Models (LLMs) arise primarily from their ability to exploit contextual information.","In this paper, we explore ways to improve reasoning capabilities of LLMs through (1) exploration of different chains of thought and (2) validation of the individual steps of the reasoning process.","We propose three general principles that a model should adhere to while reasoning: (i) Relevance, (ii) Mathematical Accuracy, and (iii) Logical Consistency.","We apply these constraints to the reasoning steps generated by the LLM to improve the accuracy of the final generation.","The constraints are applied in the form of verifiers: the model itself is asked to verify if the generated steps satisfy each constraint.","To further steer the generations towards high-quality solutions, we use the perplexity of the reasoning steps as an additional verifier.","We evaluate our method on 4 distinct types of reasoning tasks, spanning a total of 9 different datasets.","Experiments show that our method is always better than vanilla generation, and, in 6 out of the 9 datasets, it is better than best-of N sampling which samples N reasoning chains and picks the lowest perplexity generation."],"url":"http://arxiv.org/abs/2405.00204v1","category":"cs.CL"}
{"created":"2024-04-30 21:07:32","title":"SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained Large Language Models","abstract":"Full fine-tuning is a popular approach to adapt Transformer-based pre-trained large language models to a specific downstream task. However, the substantial requirements for computational power and storage have discouraged its widespread use. Moreover, increasing evidence of catastrophic forgetting and overparameterization in the Transformer architecture has motivated researchers to seek more efficient fine-tuning (PEFT) methods. Commonly known parameter-efficient fine-tuning methods like LoRA and BitFit are typically applied across all layers of the model. We propose a PEFT method, called Stratified Progressive Adaptation Fine-tuning (SPAFIT), based on the localization of different types of linguistic knowledge to specific layers of the model. Our experiments, conducted on nine tasks from the GLUE benchmark, show that our proposed SPAFIT method outperforms other PEFT methods while fine-tuning only a fraction of the parameters adjusted by other methods.","sentences":["Full fine-tuning is a popular approach to adapt Transformer-based pre-trained large language models to a specific downstream task.","However, the substantial requirements for computational power and storage have discouraged its widespread use.","Moreover, increasing evidence of catastrophic forgetting and overparameterization in the Transformer architecture has motivated researchers to seek more efficient fine-tuning (PEFT) methods.","Commonly known parameter-efficient fine-tuning methods like LoRA and BitFit are typically applied across all layers of the model.","We propose a PEFT method, called Stratified Progressive Adaptation Fine-tuning (SPAFIT), based on the localization of different types of linguistic knowledge to specific layers of the model.","Our experiments, conducted on nine tasks from the GLUE benchmark, show that our proposed SPAFIT method outperforms other PEFT methods while fine-tuning only a fraction of the parameters adjusted by other methods."],"url":"http://arxiv.org/abs/2405.00201v1","category":"cs.CL"}
{"created":"2024-04-30 21:01:34","title":"Grounding Realizable Entities","abstract":"Ontological representations of qualities, dispositions, and roles have been refined over the past decade, clarifying subtle distinctions in life science research. After articulating a widely-used characterization of these entities within the context of Basic Formal Ontology (BFO), we identify gaps in this treatment and motivate the need for supplementing the BFO characterization. By way of supplement, we propose definitions for grounding relations holding between qualities and dispositions, and dispositions and roles, illustrating our proposal by representing subtle aspects of host-pathogen interactions.","sentences":["Ontological representations of qualities, dispositions, and roles have been refined over the past decade, clarifying subtle distinctions in life science research.","After articulating a widely-used characterization of these entities within the context of Basic Formal Ontology (BFO), we identify gaps in this treatment and motivate the need for supplementing the BFO characterization.","By way of supplement, we propose definitions for grounding relations holding between qualities and dispositions, and dispositions and roles, illustrating our proposal by representing subtle aspects of host-pathogen interactions."],"url":"http://arxiv.org/abs/2405.00197v1","category":"cs.AI"}
{"created":"2024-04-30 20:23:18","title":"Credentials in the Occupation Ontology","abstract":"The term credential encompasses educational certificates, degrees, certifications, and government-issued licenses. An occupational credential is a verification of an individuals qualification or competence issued by a third party with relevant authority. Job seekers often leverage such credentials as evidence that desired qualifications are satisfied by their holders. Many U.S. education and workforce development organizations have recognized the importance of credentials for employment and the challenges of understanding the value of credentials. In this study, we identified and ontologically defined credential and credential-related terms at the textual and semantic levels based on the Occupation Ontology (OccO), a BFO-based ontology. Different credential types and their authorization logic are modeled. We additionally defined a high-level hierarchy of credential related terms and relations among many terms, which were initiated in concert with the Alabama Talent Triad (ATT) program, which aims to connect learners, earners, employers and education/training providers through credentials and skills. To our knowledge, our research provides for the first time systematic ontological modeling of the important domain of credentials and related contents, supporting enhanced credential data and knowledge integration in the future.","sentences":["The term credential encompasses educational certificates, degrees, certifications, and government-issued licenses.","An occupational credential is a verification of an individuals qualification or competence issued by a third party with relevant authority.","Job seekers often leverage such credentials as evidence that desired qualifications are satisfied by their holders.","Many U.S. education and workforce development organizations have recognized the importance of credentials for employment and the challenges of understanding the value of credentials.","In this study, we identified and ontologically defined credential and credential-related terms at the textual and semantic levels based on the Occupation Ontology (OccO), a BFO-based ontology.","Different credential types and their authorization logic are modeled.","We additionally defined a high-level hierarchy of credential related terms and relations among many terms, which were initiated in concert with the Alabama Talent Triad (ATT) program, which aims to connect learners, earners, employers and education/training providers through credentials and skills.","To our knowledge, our research provides for the first time systematic ontological modeling of the important domain of credentials and related contents, supporting enhanced credential data and knowledge integration in the future."],"url":"http://arxiv.org/abs/2405.00186v1","category":"cs.AI"}
{"created":"2024-04-30 20:16:14","title":"Capabilities","abstract":"In our daily lives, as in science and in all other domains, we encounter huge numbers of dispositions (tendencies, potentials, powers) which are realized in processes such as sneezing, sweating, shedding dandruff, and on and on. Among this plethora of what we can think of as mere dispositions is a subset of dispositions in whose realizations we have an interest a car responding well when driven on ice, a rabbits lungs responding well when it is chased by a wolf, and so on. We call the latter capabilities and we attempt to provide a robust ontological account of what capabilities are that is of sufficient generality to serve a variety of purposes, for example by providing a useful extension to ontology-based research in areas where capabilities data are currently being collected in siloed fashion.","sentences":["In our daily lives, as in science and in all other domains, we encounter huge numbers of dispositions (tendencies, potentials, powers) which are realized in processes such as sneezing, sweating, shedding dandruff, and on and on.","Among this plethora of what we can think of as mere dispositions is a subset of dispositions in whose realizations we have an interest a car responding well when driven on ice, a rabbits lungs responding well when it is chased by a wolf, and so on.","We call the latter capabilities and we attempt to provide a robust ontological account of what capabilities are that is of sufficient generality to serve a variety of purposes, for example by providing a useful extension to ontology-based research in areas where capabilities data are currently being collected in siloed fashion."],"url":"http://arxiv.org/abs/2405.00183v1","category":"cs.AI"}
{"created":"2024-04-30 20:13:18","title":"M-DEW: Extending Dynamic Ensemble Weighting to Handle Missing Values","abstract":"Missing value imputation is a crucial preprocessing step for many machine learning problems. However, it is often considered as a separate subtask from downstream applications such as classification, regression, or clustering, and thus is not optimized together with them. We hypothesize that treating the imputation model and downstream task model together and optimizing over full pipelines will yield better results than treating them separately. Our work describes a novel AutoML technique for making downstream predictions with missing data that automatically handles preprocessing, model weighting, and selection during inference time, with minimal compute overhead. Specifically we develop M-DEW, a Dynamic missingness-aware Ensemble Weighting (DEW) approach, that constructs a set of two-stage imputation-prediction pipelines, trains each component separately, and dynamically calculates a set of pipeline weights for each sample during inference time. We thus extend previous work on dynamic ensemble weighting to handle missing data at the level of full imputation-prediction pipelines, improving performance and calibration on downstream machine learning tasks over standard model averaging techniques. M-DEW is shown to outperform the state-of-the-art in that it produces statistically significant reductions in model perplexity in 17 out of 18 experiments, while improving average precision in 13 out of 18 experiments.","sentences":["Missing value imputation is a crucial preprocessing step for many machine learning problems.","However, it is often considered as a separate subtask from downstream applications such as classification, regression, or clustering, and thus is not optimized together with them.","We hypothesize that treating the imputation model and downstream task model together and optimizing over full pipelines will yield better results than treating them separately.","Our work describes a novel AutoML technique for making downstream predictions with missing data that automatically handles preprocessing, model weighting, and selection during inference time, with minimal compute overhead.","Specifically we develop M-DEW, a Dynamic missingness-aware Ensemble Weighting (DEW) approach, that constructs a set of two-stage imputation-prediction pipelines, trains each component separately, and dynamically calculates a set of pipeline weights for each sample during inference time.","We thus extend previous work on dynamic ensemble weighting to handle missing data at the level of full imputation-prediction pipelines, improving performance and calibration on downstream machine learning tasks over standard model averaging techniques.","M-DEW is shown to outperform the state-of-the-art in that it produces statistically significant reductions in model perplexity in 17 out of 18 experiments, while improving average precision in 13 out of 18 experiments."],"url":"http://arxiv.org/abs/2405.00182v1","category":"cs.LG"}
{"created":"2024-04-30 20:11:49","title":"Uncovering What, Why and How: A Comprehensive Benchmark for Causation Understanding of Video Anomaly","abstract":"Video anomaly understanding (VAU) aims to automatically comprehend unusual occurrences in videos, thereby enabling various applications such as traffic surveillance and industrial manufacturing. While existing VAU benchmarks primarily concentrate on anomaly detection and localization, our focus is on more practicality, prompting us to raise the following crucial questions: \"what anomaly occurred?\", \"why did it happen?\", and \"how severe is this abnormal event?\". In pursuit of these answers, we present a comprehensive benchmark for Causation Understanding of Video Anomaly (CUVA). Specifically, each instance of the proposed benchmark involves three sets of human annotations to indicate the \"what\", \"why\" and \"how\" of an anomaly, including 1) anomaly type, start and end times, and event descriptions, 2) natural language explanations for the cause of an anomaly, and 3) free text reflecting the effect of the abnormality. In addition, we also introduce MMEval, a novel evaluation metric designed to better align with human preferences for CUVA, facilitating the measurement of existing LLMs in comprehending the underlying cause and corresponding effect of video anomalies. Finally, we propose a novel prompt-based method that can serve as a baseline approach for the challenging CUVA. We conduct extensive experiments to show the superiority of our evaluation metric and the prompt-based approach. Our code and dataset are available at https://github.com/fesvhtr/CUVA.","sentences":["Video anomaly understanding (VAU) aims to automatically comprehend unusual occurrences in videos, thereby enabling various applications such as traffic surveillance and industrial manufacturing.","While existing VAU benchmarks primarily concentrate on anomaly detection and localization, our focus is on more practicality, prompting us to raise the following crucial questions: \"what anomaly occurred?\", \"why did it happen?\", and \"how severe is this abnormal event?\".","In pursuit of these answers, we present a comprehensive benchmark for Causation Understanding of Video Anomaly (CUVA).","Specifically, each instance of the proposed benchmark involves three sets of human annotations to indicate the \"what\", \"why\" and \"how\" of an anomaly, including 1) anomaly type, start and end times, and event descriptions, 2) natural language explanations for the cause of an anomaly, and 3) free text reflecting the effect of the abnormality.","In addition, we also introduce MMEval, a novel evaluation metric designed to better align with human preferences for CUVA, facilitating the measurement of existing LLMs in comprehending the underlying cause and corresponding effect of video anomalies.","Finally, we propose a novel prompt-based method that can serve as a baseline approach for the challenging CUVA.","We conduct extensive experiments to show the superiority of our evaluation metric and the prompt-based approach.","Our code and dataset are available at https://github.com/fesvhtr/CUVA."],"url":"http://arxiv.org/abs/2405.00181v1","category":"cs.CV"}
{"created":"2024-04-30 20:04:39","title":"A Bayesian joint longitudinal-survival model with a latent stochastic process for intensive longitudinal data","abstract":"The availability of mobile health (mHealth) technology has enabled increased collection of intensive longitudinal data (ILD). ILD have potential to capture rapid fluctuations in outcomes that may be associated with changes in the risk of an event. However, existing methods for jointly modeling longitudinal and event-time outcomes are not well-equipped to handle ILD due to the high computational cost. We propose a joint longitudinal and time-to-event model suitable for analyzing ILD. In this model, we summarize a multivariate longitudinal outcome as a smaller number of time-varying latent factors. These latent factors, which are modeled using an Ornstein-Uhlenbeck stochastic process, capture the risk of a time-to-event outcome in a parametric hazard model. We take a Bayesian approach to fit our joint model and conduct simulations to assess its performance. We use it to analyze data from an mHealth study of smoking cessation. We summarize the longitudinal self-reported intensity of nine emotions as the psychological states of positive and negative affect. These time-varying latent states capture the risk of the first smoking lapse after attempted quit. Understanding factors associated with smoking lapse is of keen interest to smoking cessation researchers.","sentences":["The availability of mobile health (mHealth) technology has enabled increased collection of intensive longitudinal data (ILD).","ILD have potential to capture rapid fluctuations in outcomes that may be associated with changes in the risk of an event.","However, existing methods for jointly modeling longitudinal and event-time outcomes are not well-equipped to handle ILD due to the high computational cost.","We propose a joint longitudinal and time-to-event model suitable for analyzing ILD.","In this model, we summarize a multivariate longitudinal outcome as a smaller number of time-varying latent factors.","These latent factors, which are modeled using an Ornstein-Uhlenbeck stochastic process, capture the risk of a time-to-event outcome in a parametric hazard model.","We take a Bayesian approach to fit our joint model and conduct simulations to assess its performance.","We use it to analyze data from an mHealth study of smoking cessation.","We summarize the longitudinal self-reported intensity of nine emotions as the psychological states of positive and negative affect.","These time-varying latent states capture the risk of the first smoking lapse after attempted quit.","Understanding factors associated with smoking lapse is of keen interest to smoking cessation researchers."],"url":"http://arxiv.org/abs/2405.00179v1","category":"stat.ME"}
{"created":"2024-04-30 19:37:58","title":"Revisiting RGBT Tracking Benchmarks from the Perspective of Modality Validity: A New Benchmark, Problem, and Method","abstract":"RGBT tracking draws increasing attention due to its robustness in multi-modality warranting (MMW) scenarios, such as nighttime and bad weather, where relying on a single sensing modality fails to ensure stable tracking results. However, the existing benchmarks predominantly consist of videos collected in common scenarios where both RGB and thermal infrared (TIR) information are of sufficient quality. This makes the data unrepresentative of severe imaging conditions, leading to tracking failures in MMW scenarios. To bridge this gap, we present a new benchmark, MV-RGBT, captured specifically in MMW scenarios. In contrast with the existing datasets, MV-RGBT comprises more object categories and scenes, providing a diverse and challenging benchmark. Furthermore, for severe imaging conditions of MMW scenarios, a new problem is posed, namely \\textit{when to fuse}, to stimulate the development of fusion strategies for such data. We propose a new method based on a mixture of experts, namely MoETrack, as a baseline fusion strategy. In MoETrack, each expert generates independent tracking results along with the corresponding confidence score, which is used to control the fusion process. Extensive experimental results demonstrate the significant potential of MV-RGBT in advancing RGBT tracking and elicit the conclusion that fusion is not always beneficial, especially in MMW scenarios. Significantly, the proposed MoETrack method achieves new state-of-the-art results not only on MV-RGBT, but also on standard benchmarks, such as RGBT234, LasHeR, and the short-term split of VTUAV (VTUAV-ST). More information of MV-RGBT and the source code of MoETrack will be released at https://github.com/Zhangyong-Tang/MoETrack.","sentences":["RGBT tracking draws increasing attention due to its robustness in multi-modality warranting (MMW) scenarios, such as nighttime and bad weather, where relying on a single sensing modality fails to ensure stable tracking results.","However, the existing benchmarks predominantly consist of videos collected in common scenarios where both RGB and thermal infrared (TIR) information are of sufficient quality.","This makes the data unrepresentative of severe imaging conditions, leading to tracking failures in MMW scenarios.","To bridge this gap, we present a new benchmark, MV-RGBT, captured specifically in MMW scenarios.","In contrast with the existing datasets, MV-RGBT comprises more object categories and scenes, providing a diverse and challenging benchmark.","Furthermore, for severe imaging conditions of MMW scenarios, a new problem is posed, namely \\textit{when to fuse}, to stimulate the development of fusion strategies for such data.","We propose a new method based on a mixture of experts, namely MoETrack, as a baseline fusion strategy.","In MoETrack, each expert generates independent tracking results along with the corresponding confidence score, which is used to control the fusion process.","Extensive experimental results demonstrate the significant potential of MV-RGBT in advancing RGBT tracking and elicit the conclusion that fusion is not always beneficial, especially in MMW scenarios.","Significantly, the proposed MoETrack method achieves new state-of-the-art results not only on MV-RGBT, but also on standard benchmarks, such as RGBT234, LasHeR, and the short-term split of VTUAV (VTUAV-ST).","More information of MV-RGBT and the source code of MoETrack will be released at https://github.com/Zhangyong-Tang/MoETrack."],"url":"http://arxiv.org/abs/2405.00168v1","category":"cs.CV"}
{"created":"2024-04-30 19:31:31","title":"Discovering intrinsic multi-compartment pharmacometric models using Physics Informed Neural Networks","abstract":"Pharmacometric models are pivotal across drug discovery and development, playing a decisive role in determining the progression of candidate molecules. However, the derivation of mathematical equations governing the system is a labor-intensive trial-and-error process, often constrained by tight timelines. In this study, we introduce PKINNs, a novel purely data-driven pharmacokinetic-informed neural network model. PKINNs efficiently discovers and models intrinsic multi-compartment-based pharmacometric structures, reliably forecasting their derivatives. The resulting models are both interpretable and explainable through Symbolic Regression methods. Our computational framework demonstrates the potential for closed-form model discovery in pharmacometric applications, addressing the labor-intensive nature of traditional model derivation. With the increasing availability of large datasets, this framework holds the potential to significantly enhance model-informed drug discovery.","sentences":["Pharmacometric models are pivotal across drug discovery and development, playing a decisive role in determining the progression of candidate molecules.","However, the derivation of mathematical equations governing the system is a labor-intensive trial-and-error process, often constrained by tight timelines.","In this study, we introduce PKINNs, a novel purely data-driven pharmacokinetic-informed neural network model.","PKINNs efficiently discovers and models intrinsic multi-compartment-based pharmacometric structures, reliably forecasting their derivatives.","The resulting models are both interpretable and explainable through Symbolic Regression methods.","Our computational framework demonstrates the potential for closed-form model discovery in pharmacometric applications, addressing the labor-intensive nature of traditional model derivation.","With the increasing availability of large datasets, this framework holds the potential to significantly enhance model-informed drug discovery."],"url":"http://arxiv.org/abs/2405.00166v1","category":"cs.LG"}
{"created":"2024-04-30 19:06:37","title":"Expanding the Horizon: Enabling Hybrid Quantum Transfer Learning for Long-Tailed Chest X-Ray Classification","abstract":"Quantum machine learning (QML) has the potential for improving the multi-label classification of rare, albeit critical, diseases in large-scale chest x-ray (CXR) datasets due to theoretical quantum advantages over classical machine learning (CML) in sample efficiency and generalizability. While prior literature has explored QML with CXRs, it has focused on binary classification tasks with small datasets due to limited access to quantum hardware and computationally expensive simulations. To that end, we implemented a Jax-based framework that enables the simulation of medium-sized qubit architectures with significant improvements in wall-clock time over current software offerings. We evaluated the performance of our Jax-based framework in terms of efficiency and performance for hybrid quantum transfer learning for long-tailed classification across 8, 14, and 19 disease labels using large-scale CXR datasets. The Jax-based framework resulted in up to a 58% and 95% speed-up compared to PyTorch and TensorFlow implementations, respectively. However, compared to CML, QML demonstrated slower convergence and an average AUROC of 0.70, 0.73, and 0.74 for the classification of 8, 14, and 19 CXR disease labels. In comparison, the CML models had an average AUROC of 0.77, 0.78, and 0.80 respectively. In conclusion, our work presents an accessible implementation of hybrid quantum transfer learning for long-tailed CXR classification with a computationally efficient Jax-based framework.","sentences":["Quantum machine learning (QML) has the potential for improving the multi-label classification of rare, albeit critical, diseases in large-scale chest x-ray (CXR) datasets due to theoretical quantum advantages over classical machine learning (CML) in sample efficiency and generalizability.","While prior literature has explored QML with CXRs, it has focused on binary classification tasks with small datasets due to limited access to quantum hardware and computationally expensive simulations.","To that end, we implemented a Jax-based framework that enables the simulation of medium-sized qubit architectures with significant improvements in wall-clock time over current software offerings.","We evaluated the performance of our Jax-based framework in terms of efficiency and performance for hybrid quantum transfer learning for long-tailed classification across 8, 14, and 19 disease labels using large-scale CXR datasets.","The Jax-based framework resulted in up to a 58% and 95% speed-up compared to PyTorch and TensorFlow implementations, respectively.","However, compared to CML, QML demonstrated slower convergence and an average AUROC of 0.70, 0.73, and 0.74 for the classification of 8, 14, and 19 CXR disease labels.","In comparison, the CML models had an average AUROC of 0.77, 0.78, and 0.80 respectively.","In conclusion, our work presents an accessible implementation of hybrid quantum transfer learning for long-tailed CXR classification with a computationally efficient Jax-based framework."],"url":"http://arxiv.org/abs/2405.00156v1","category":"cs.CV"}
{"created":"2024-04-30 18:42:18","title":"GUing: A Mobile GUI Search Engine using a Vision-Language Model","abstract":"App developers use the Graphical User Interface (GUI) of other apps as an important source of inspiration to design and improve their own apps. In recent years, research suggested various approaches to retrieve GUI designs that fit a certain text query from screenshot datasets acquired through automated GUI exploration. However, such text-to-GUI retrieval approaches only leverage the textual information of the GUI elements in the screenshots, neglecting visual information such as icons or background images. In addition, the retrieved screenshots are not steered by app developers and often lack important app features, e.g. whose UI pages require user authentication. To overcome these limitations, this paper proposes GUing, a GUI search engine based on a vision-language model called UIClip, which we trained specifically for the app GUI domain. For this, we first collected app introduction images from Google Play, which usually display the most representative screenshots selected and often captioned (i.e. labeled) by app vendors. Then, we developed an automated pipeline to classify, crop, and extract the captions from these images. This finally results in a large dataset which we share with this paper: including 303k app screenshots, out of which 135k have captions. We used this dataset to train a novel vision-language model, which is, to the best of our knowledge, the first of its kind in GUI retrieval. We evaluated our approach on various datasets from related work and in manual experiment. The results demonstrate that our model outperforms previous approaches in text-to-GUI retrieval achieving a Recall@10 of up to 0.69 and a HIT@10 of 0.91. We also explored the performance of UIClip for other GUI tasks including GUI classification and Sketch-to-GUI retrieval with encouraging results.","sentences":["App developers use the Graphical User Interface (GUI) of other apps as an important source of inspiration to design and improve their own apps.","In recent years, research suggested various approaches to retrieve GUI designs that fit a certain text query from screenshot datasets acquired through automated GUI exploration.","However, such text-to-GUI retrieval approaches only leverage the textual information of the GUI elements in the screenshots, neglecting visual information such as icons or background images.","In addition, the retrieved screenshots are not steered by app developers and often lack important app features, e.g. whose UI pages require user authentication.","To overcome these limitations, this paper proposes GUing, a GUI search engine based on a vision-language model called UIClip, which we trained specifically for the app GUI domain.","For this, we first collected app introduction images from Google Play, which usually display the most representative screenshots selected and often captioned (i.e. labeled) by app vendors.","Then, we developed an automated pipeline to classify, crop, and extract the captions from these images.","This finally results in a large dataset which we share with this paper: including 303k app screenshots, out of which 135k have captions.","We used this dataset to train a novel vision-language model, which is, to the best of our knowledge, the first of its kind in GUI retrieval.","We evaluated our approach on various datasets from related work and in manual experiment.","The results demonstrate that our model outperforms previous approaches in text-to-GUI retrieval achieving a Recall@10 of up to 0.69 and a HIT@10 of 0.91.","We also explored the performance of UIClip for other GUI tasks including GUI classification and Sketch-to-GUI retrieval with encouraging results."],"url":"http://arxiv.org/abs/2405.00145v1","category":"cs.SE"}
{"created":"2024-04-30 18:39:25","title":"RIS-aided Wireless Communication with Movable Elements Geometry Impact on Performance","abstract":"Reconfigurable Intelligent Surfaces (RIS) are known as a promising technology to improve the performance of wireless communication networks, and have been extensively studied. Movable Antennas (MA) are a novel technology that fully exploits the antenna placement for enhancing the system performance. This article aims at evaluating the impact of transmit power and number of antenna elements on the outage probability performance of an MA-enabled RIS structure (MA-RIS), compared to existing Fixed-Position Antenna RIS (FPA-RIS). The change in geometry caused by the movement of antennas and its implications for the effective number of illuminated elements, are studied for 1D and 2D array structures. Our numerical results confirm the performance advantage provided by MA-RIS, achieving 24\\% improvement in outage probability, and 2 dB gain in Signal-to-Noise Ratio (SNR), as compared to FPA-RIS.","sentences":["Reconfigurable Intelligent Surfaces (RIS) are known as a promising technology to improve the performance of wireless communication networks, and have been extensively studied.","Movable Antennas (MA) are a novel technology that fully exploits the antenna placement for enhancing the system performance.","This article aims at evaluating the impact of transmit power and number of antenna elements on the outage probability performance of an MA-enabled RIS structure (MA-RIS), compared to existing Fixed-Position Antenna RIS (FPA-RIS).","The change in geometry caused by the movement of antennas and its implications for the effective number of illuminated elements, are studied for 1D and 2D array structures.","Our numerical results confirm the performance advantage provided by MA-RIS, achieving 24\\% improvement in outage probability, and 2 dB gain in Signal-to-Noise Ratio (SNR), as compared to FPA-RIS."],"url":"http://arxiv.org/abs/2405.00141v1","category":"eess.SY"}
{"created":"2024-04-30 18:36:45","title":"AfricAIED 2024: 2nd Workshop on Artificial Intelligence in Education in Africa","abstract":"Recent AI advancements offer transformative potential for global education, yet their application often overlooks Africa's unique educational landscape. AfricAIED 2024 will address this gap, spotlighting efforts to develop AI in Education (AIED) systems tailored to Africa's needs. Building on the success of the inaugural workshop, AfricAIED 2024 will feature an online AI Hackathon focused on democratizing preparation for Ghana's National Science & Maths Quiz (NSMQ). Participants will create open-source AI tools leveraging resources from the Brilla AI project to level the academic playing field and enhance science and math education across Africa. The workshop will showcase top competitors' solutions, invite discussions on AIED opportunities and challenges in Africa, and highlight the latest advancements in AI education integration. AfricAIED 2024 aims to foster collaboration and innovation, amplifying African voices in the AIED community and driving positive change in African education through AI.","sentences":["Recent AI advancements offer transformative potential for global education, yet their application often overlooks Africa's unique educational landscape.","AfricAIED 2024 will address this gap, spotlighting efforts to develop AI in Education (AIED) systems tailored to Africa's needs.","Building on the success of the inaugural workshop, AfricAIED 2024 will feature an online AI Hackathon focused on democratizing preparation for Ghana's National Science & Maths Quiz (NSMQ).","Participants will create open-source AI tools leveraging resources from the Brilla AI project to level the academic playing field and enhance science and math education across Africa.","The workshop will showcase top competitors' solutions, invite discussions on AIED opportunities and challenges in Africa, and highlight the latest advancements in AI education integration.","AfricAIED 2024 aims to foster collaboration and innovation, amplifying African voices in the AIED community and driving positive change in African education through AI."],"url":"http://arxiv.org/abs/2405.00139v1","category":"cs.CY"}
{"created":"2024-04-30 18:32:24","title":"Data-Driven Permissible Safe Control with Barrier Certificates","abstract":"This paper introduces a method of identifying a maximal set of safe strategies from data for stochastic systems with unknown dynamics using barrier certificates. The first step is learning the dynamics of the system via Gaussian process (GP) regression and obtaining probabilistic errors for this estimate. Then, we develop an algorithm for constructing piecewise stochastic barrier functions to find a maximal permissible strategy set using the learned GP model, which is based on sequentially pruning the worst controls until a maximal set is identified. The permissible strategies are guaranteed to maintain probabilistic safety for the true system. This is especially important for learning-enabled systems, because a rich strategy space enables additional data collection and complex behaviors while remaining safe. Case studies on linear and nonlinear systems demonstrate that increasing the size of the dataset for learning the system grows the permissible strategy set.","sentences":["This paper introduces a method of identifying a maximal set of safe strategies from data for stochastic systems with unknown dynamics using barrier certificates.","The first step is learning the dynamics of the system via Gaussian process (GP) regression and obtaining probabilistic errors for this estimate.","Then, we develop an algorithm for constructing piecewise stochastic barrier functions to find a maximal permissible strategy set using the learned GP model, which is based on sequentially pruning the worst controls until a maximal set is identified.","The permissible strategies are guaranteed to maintain probabilistic safety for the true system.","This is especially important for learning-enabled systems, because a rich strategy space enables additional data collection and complex behaviors while remaining safe.","Case studies on linear and nonlinear systems demonstrate that increasing the size of the dataset for learning the system grows the permissible strategy set."],"url":"http://arxiv.org/abs/2405.00136v1","category":"cs.LG"}
{"created":"2024-04-30 18:31:19","title":"Transforming Dutch: Debiasing Dutch Coreference Resolution Systems for Non-binary Pronouns","abstract":"Gender-neutral pronouns are increasingly being introduced across Western languages. Recent evaluations have however demonstrated that English NLP systems are unable to correctly process gender-neutral pronouns, with the risk of erasing and misgendering non-binary individuals. This paper examines a Dutch coreference resolution system's performance on gender-neutral pronouns, specifically hen and die. In Dutch, these pronouns were only introduced in 2016, compared to the longstanding existence of singular they in English. We additionally compare two debiasing techniques for coreference resolution systems in non-binary contexts: Counterfactual Data Augmentation (CDA) and delexicalisation. Moreover, because pronoun performance can be hard to interpret from a general evaluation metric like LEA, we introduce an innovative evaluation metric, the pronoun score, which directly represents the portion of correctly processed pronouns. Our results reveal diminished performance on gender-neutral pronouns compared to gendered counterparts. Nevertheless, although delexicalisation fails to yield improvements, CDA substantially reduces the performance gap between gendered and gender-neutral pronouns. We further show that CDA remains effective in low-resource settings, in which a limited set of debiasing documents is used. This efficacy extends to previously unseen neopronouns, which are currently infrequently used but may gain popularity in the future, underscoring the viability of effective debiasing with minimal resources and low computational costs.","sentences":["Gender-neutral pronouns are increasingly being introduced across Western languages.","Recent evaluations have however demonstrated that English NLP systems are unable to correctly process gender-neutral pronouns, with the risk of erasing and misgendering non-binary individuals.","This paper examines a Dutch coreference resolution system's performance on gender-neutral pronouns, specifically hen and die.","In Dutch, these pronouns were only introduced in 2016, compared to the longstanding existence of singular they in English.","We additionally compare two debiasing techniques for coreference resolution systems in non-binary contexts: Counterfactual Data Augmentation (CDA) and delexicalisation.","Moreover, because pronoun performance can be hard to interpret from a general evaluation metric like LEA, we introduce an innovative evaluation metric, the pronoun score, which directly represents the portion of correctly processed pronouns.","Our results reveal diminished performance on gender-neutral pronouns compared to gendered counterparts.","Nevertheless, although delexicalisation fails to yield improvements, CDA substantially reduces the performance gap between gendered and gender-neutral pronouns.","We further show that CDA remains effective in low-resource settings, in which a limited set of debiasing documents is used.","This efficacy extends to previously unseen neopronouns, which are currently infrequently used but may gain popularity in the future, underscoring the viability of effective debiasing with minimal resources and low computational costs."],"url":"http://arxiv.org/abs/2405.00134v1","category":"cs.CL"}
{"created":"2024-04-30 18:08:08","title":"Training a high-performance retinal foundation model with half-the-data and 400 times less compute","abstract":"Artificial Intelligence holds tremendous potential in medicine, but is traditionally limited by the lack of massive datasets to train models on. Foundation models, pre-trained models that can be adapted to downstream tasks with small datasets, could alleviate this problem. Researchers at Moorfields Eye Hospital (MEH) proposed RETFound-MEH, a foundation model for retinal imaging that was trained on 900,000 images, including private hospital data. Recently, data-efficient DERETFound was proposed that provides comparable performance while being trained on only 150,000 images that are all publicly available. However, both these models required very substantial resources to train initially and are resource-intensive in downstream use. We propose a novel Token Reconstruction objective that we use to train RETFound-Green, a retinal foundation model trained using only 75,000 publicly available images and 400 times less compute. We estimate the cost of training RETFound-MEH and DERETFound at $10,000 and $14,000, respectively, while RETFound-Green could be trained for less than $100, with equally reduced environmental impact. RETFound-Green is also far more efficient in downstream use: it can be downloaded 14 times faster, computes vector embeddings 2.7 times faster which then require 2.6 times less storage space. Despite this, RETFound-Green does not perform systematically worse. In fact, it performs best on 14 tasks, compared to six for DERETFound and two for RETFound-MEH. Our results suggest that RETFound-Green is a very efficient, high-performance retinal foundation model. We anticipate that our Token Reconstruction objective could be scaled up for even higher performance and be applied to other domains beyond retinal imaging.","sentences":["Artificial Intelligence holds tremendous potential in medicine, but is traditionally limited by the lack of massive datasets to train models on.","Foundation models, pre-trained models that can be adapted to downstream tasks with small datasets, could alleviate this problem.","Researchers at Moorfields Eye Hospital (MEH) proposed RETFound-MEH, a foundation model for retinal imaging that was trained on 900,000 images, including private hospital data.","Recently, data-efficient DERETFound was proposed that provides comparable performance while being trained on only 150,000 images that are all publicly available.","However, both these models required very substantial resources to train initially and are resource-intensive in downstream use.","We propose a novel Token Reconstruction objective that we use to train RETFound-Green, a retinal foundation model trained using only 75,000 publicly available images and 400 times less compute.","We estimate the cost of training RETFound-MEH and DERETFound at $10,000 and $14,000, respectively, while RETFound-Green could be trained for less than $100, with equally reduced environmental impact.","RETFound-Green is also far more efficient in downstream use: it can be downloaded 14 times faster, computes vector embeddings 2.7 times faster which then require 2.6 times less storage space.","Despite this, RETFound-Green does not perform systematically worse.","In fact, it performs best on 14 tasks, compared to six for DERETFound and two for RETFound-MEH.","Our results suggest that RETFound-Green is a very efficient, high-performance retinal foundation model.","We anticipate that our Token Reconstruction objective could be scaled up for even higher performance and be applied to other domains beyond retinal imaging."],"url":"http://arxiv.org/abs/2405.00117v1","category":"cs.CV"}
{"created":"2024-04-30 18:03:22","title":"Gravitational entropy is observer-dependent","abstract":"In quantum gravity, it has been argued that a proper accounting of the role played by an observer promotes the von Neumann algebra of observables in a given spacetime subregion from Type III to Type II. While this allows for a mathematically precise definition of its entropy, we show that this procedure depends on which observer is employed. We make this precise by considering a setup in which many possible observers are present; by generalising previous approaches, we derive density operators for the subregion relative to different observers (and relative to arbitrary collections of observers), and we compute the associated entropies in a semiclassical regime, as well as in some specific examples that go beyond this regime. We find that the entropies seen by distinct observers can drastically differ. Our work makes extensive use of the formalism of quantum reference frames (QRF); indeed, as we point out, the 'observers' considered here and in the previous works are nothing but QRFs. In the process, we demonstrate that the description of physical states and observables invoked by Chandrasekaran et al. [arXiv:2206.10780] is equivalent to the Page-Wootters formalism, leading to the informal slogan \"PW=CLPW\". It is our hope that this paper will help motivate a long overdue union between the QRF and quantum gravity communities. Further details will appear in a companion paper.","sentences":["In quantum gravity, it has been argued that a proper accounting of the role played by an observer promotes the von Neumann algebra of observables in a given spacetime subregion from Type III to Type II.","While this allows for a mathematically precise definition of its entropy, we show that this procedure depends on which observer is employed.","We make this precise by considering a setup in which many possible observers are present; by generalising previous approaches, we derive density operators for the subregion relative to different observers (and relative to arbitrary collections of observers), and we compute the associated entropies in a semiclassical regime, as well as in some specific examples that go beyond this regime.","We find that the entropies seen by distinct observers can drastically differ.","Our work makes extensive use of the formalism of quantum reference frames (QRF); indeed, as we point out, the 'observers' considered here and in the previous works are nothing but QRFs.","In the process, we demonstrate that the description of physical states and observables invoked by Chandrasekaran et al.","[arXiv:2206.10780] is equivalent to the Page-Wootters formalism, leading to the informal slogan \"PW=CLPW\".","It is our hope that this paper will help motivate a long overdue union between the QRF and quantum gravity communities.","Further details will appear in a companion paper."],"url":"http://arxiv.org/abs/2405.00114v1","category":"hep-th"}
{"created":"2024-04-30 18:00:02","title":"Creative Beam Search","abstract":"Large language models are revolutionizing several areas, including artificial creativity. However, the process of generation in machines profoundly diverges from that observed in humans. In particular, machine generation is characterized by a lack of intentionality and an underlying creative process. We propose a method called Creative Beam Search that uses Diverse Beam Search and LLM-as-a-Judge to perform response generation and response validation. The results of a qualitative experiment show how our approach can provide better output than standard sampling techniques. We also show that the response validation step is a necessary complement to the response generation step.","sentences":["Large language models are revolutionizing several areas, including artificial creativity.","However, the process of generation in machines profoundly diverges from that observed in humans.","In particular, machine generation is characterized by a lack of intentionality and an underlying creative process.","We propose a method called Creative Beam Search that uses Diverse Beam Search and LLM-as-a-Judge to perform response generation and response validation.","The results of a qualitative experiment show how our approach can provide better output than standard sampling techniques.","We also show that the response validation step is a necessary complement to the response generation step."],"url":"http://arxiv.org/abs/2405.00099v1","category":"cs.AI"}
{"created":"2024-04-30 18:00:01","title":"Amplitude analysis and branching fraction measurement of $B^{+}\\to D^{*-}D^{+}_{s}\u03c0^{+}$ decays","abstract":"The decays of the $B^{+}$ meson to the final state $D^{*-}D^{+}_{s}\\pi^{+}$ are studied in proton-proton collision data collected with the LHCb detector at centre-of-mass energies of 7, 8, and 13 TeV, corresponding to a total integrated luminosity of 9 fb$^{-1}$. The ratio of branching fractions of the $B^{+}\\to D^{*-}D^{+}_{s}\\pi^{+}$ and $B^{0}\\to D^{*-}D^{+}_{s}$ decays is measured to be $0.173\\pm 0.006\\pm 0.010$, where the first uncertainty is statistical and the second is systematic. Using partially reconstructed $D^{*+}_{s}\\to D^{+}_{s}\\gamma$ and $D^{+}_{s}\\pi^{0}$ decays, the ratio of branching fractions between the $B^{+}\\to D^{*-}D^{*+}_{s}\\pi^{+}$ and $B^{+}\\to D^{*-}D^{+}_{s}\\pi^{+}$ decays is determined as $1.31\\pm 0.07\\pm 0.14$. An amplitude analysis of the $B^{+}\\to D^{*-}D^{+}_{s}\\pi^{+}$ decay is performed for the first time, revealing dominant contributions from known excited charm resonances decaying to the $D^{*-}\\pi^{+}$ final state. No significant evidence of exotic contributions in the $D^{+}_{s}\\pi^{+}$ or $D^{*-}D^{+}_{s}$ channels is found. The fit fraction of the scalar state $T_{c\\bar{s} 0}^{\\ast}(2900)^{++}$ observed in the $B^{+}\\to D^{-}D^{+}_{s}\\pi^{+}$ decay is determined to be less than 2.3% at a 90% confidence level.","sentences":["The decays of the $B^{+}$ meson to the final state $D^{*-}D^{+}_{s}\\pi^{+}$ are studied in proton-proton collision data collected with the LHCb detector at centre-of-mass energies of 7, 8, and 13 TeV, corresponding to a total integrated luminosity of 9 fb$^{-1}$. The ratio of branching fractions of the $B^{+}\\to D^{*-}D^{+}_{s}\\pi^{+}$ and $B^{0}\\to D^{*-}D^{+}_{s}$ decays is measured to be $0.173\\pm 0.006\\pm 0.010$, where the first uncertainty is statistical and the second is systematic.","Using partially reconstructed $D^{*+}_{s}\\to D^{+}_{s}\\gamma$ and $D^{+}_{s}\\pi^{0}$ decays, the ratio of branching fractions between the $B^{+}\\to D^{*-}D^{*+}_{s}\\pi^{+}$ and $B^{+}\\to D^{*-}D^{+}_{s}\\pi^{+}$ decays is determined as $1.31\\pm 0.07\\pm 0.14$.","An amplitude analysis of the $B^{+}\\to D^{*-}D^{+}_{s}\\pi^{+}$ decay is performed for the first time, revealing dominant contributions from known excited charm resonances decaying to the $D^{*-}\\pi^{+}$ final state.","No significant evidence of exotic contributions in the $D^{+}_{s}\\pi^{+}$ or $D^{*-}D^{+}_{s}$ channels is found.","The fit fraction of the scalar state $T_{c\\bar{s} 0}^{\\ast}(2900)^{++}$ observed in the $B^{+}\\to D^{-}D^{+}_{s}\\pi^{+}$ decay is determined to be less than 2.3% at a 90% confidence level."],"url":"http://arxiv.org/abs/2405.00098v1","category":"hep-ex"}
{"created":"2024-04-30 17:08:43","title":"Imprecise Markov Semigroups and their Ergodicity","abstract":"We introduce the concept of imprecise Markov semigroup. It allows us to see Markov chains and processes with imprecise transition probabilities as (a collection of diffusion) operators, and thus to unlock techniques from geometry, functional analysis, and (high dimensional) probability to study their ergodic behavior. We show that, if the initial distribution of an imprecise Markov semigroup is known and invariant, under some conditions that also involve the geometry of the state space, eventually the ambiguity around the transition probability fades. We call this property ergodicity of the imprecise Markov semigroup, and we relate it to the classical (Birkhoff's) notion of ergodicity. We prove ergodicity both when the state space is Euclidean or a Riemannian manifold, and when it is an arbitrary measurable space. The importance of our findings for the fields of machine learning and computer vision is also discussed.","sentences":["We introduce the concept of imprecise Markov semigroup.","It allows us to see Markov chains and processes with imprecise transition probabilities as (a collection of diffusion) operators, and thus to unlock techniques from geometry, functional analysis, and (high dimensional) probability to study their ergodic behavior.","We show that, if the initial distribution of an imprecise Markov semigroup is known and invariant, under some conditions that also involve the geometry of the state space, eventually the ambiguity around the transition probability fades.","We call this property ergodicity of the imprecise Markov semigroup, and we relate it to the classical (Birkhoff's) notion of ergodicity.","We prove ergodicity both when the state space is Euclidean or a Riemannian manifold, and when it is an arbitrary measurable space.","The importance of our findings for the fields of machine learning and computer vision is also discussed."],"url":"http://arxiv.org/abs/2405.00081v1","category":"math.PR"}
{"created":"2024-04-30 10:39:20","title":"On Correcting SHAP Scores","abstract":"Recent work uncovered examples of classifiers for which SHAP scores yield misleading feature attributions. While such examples might be perceived as suggesting the inadequacy of Shapley values for explainability, this paper shows that the source of the identified shortcomings of SHAP scores resides elsewhere. Concretely, the paper makes the case that the failings of SHAP scores result from the characteristic functions used in earlier works. Furthermore, the paper identifies a number of properties that characteristic functions ought to respect, and proposes several novel characteristic functions, each exhibiting one or more of the desired properties. More importantly, some of the characteristic functions proposed in this paper are guaranteed not to exhibit any of the shortcomings uncovered by earlier work. The paper also investigates the impact of the new characteristic functions on the complexity of computing SHAP scores. Finally, the paper proposes modifications to the tool SHAP to use instead one of our novel characteristic functions, thereby eliminating some of the limitations reported for SHAP scores.","sentences":["Recent work uncovered examples of classifiers for which SHAP scores yield misleading feature attributions.","While such examples might be perceived as suggesting the inadequacy of Shapley values for explainability, this paper shows that the source of the identified shortcomings of SHAP scores resides elsewhere.","Concretely, the paper makes the case that the failings of SHAP scores result from the characteristic functions used in earlier works.","Furthermore, the paper identifies a number of properties that characteristic functions ought to respect, and proposes several novel characteristic functions, each exhibiting one or more of the desired properties.","More importantly, some of the characteristic functions proposed in this paper are guaranteed not to exhibit any of the shortcomings uncovered by earlier work.","The paper also investigates the impact of the new characteristic functions on the complexity of computing SHAP scores.","Finally, the paper proposes modifications to the tool SHAP to use instead one of our novel characteristic functions, thereby eliminating some of the limitations reported for SHAP scores."],"url":"http://arxiv.org/abs/2405.00076v1","category":"cs.LG"}
{"created":"2024-04-30 08:48:36","title":"Charting the Path Forward: CT Image Quality Assessment -- An In-Depth Review","abstract":"Computed Tomography (CT) is a frequently utilized imaging technology that is employed in the clinical diagnosis of many disorders. However, clinical diagnosis, data storage, and management are posed huge challenges by a huge volume of non-homogeneous CT data in terms of imaging quality. As a result, the quality assessment of CT images is a crucial problem that demands consideration. The history, advancements in research, and current developments in CT image quality assessment (IQA) are examined in this paper. In this review, we collected and researched more than 500 CT-IQA publications published before August 2023. And we provide the visualization analysis of keywords and co-citations in the knowledge graph of these papers. Prospects and obstacles for the continued development of CT-IQA are also covered. At present, significant research branches in the CT-IQA domain include Phantom study, Artificial intelligence deep-learning reconstruction algorithm, Dose reduction opportunity, and Virtual monoenergetic reconstruction. Artificial intelligence (AI)-based CT-IQA also becomes a trend. It increases the accuracy of the CT scanning apparatus, amplifies the impact of the CT system reconstruction algorithm, and creates an effective algorithm for post-processing CT images. AI-based medical IQA offers excellent application opportunities in clinical work. AI can provide uniform quality assessment criteria and more comprehensive guidance amongst various healthcare facilities, and encourage them to identify one another's images. It will help lower the number of unnecessary tests and associated costs, and enhance the quality of medical imaging and assessment efficiency.","sentences":["Computed Tomography (CT) is a frequently utilized imaging technology that is employed in the clinical diagnosis of many disorders.","However, clinical diagnosis, data storage, and management are posed huge challenges by a huge volume of non-homogeneous CT data in terms of imaging quality.","As a result, the quality assessment of CT images is a crucial problem that demands consideration.","The history, advancements in research, and current developments in CT image quality assessment (IQA) are examined in this paper.","In this review, we collected and researched more than 500 CT-IQA publications published before August 2023.","And we provide the visualization analysis of keywords and co-citations in the knowledge graph of these papers.","Prospects and obstacles for the continued development of CT-IQA are also covered.","At present, significant research branches in the CT-IQA domain include Phantom study, Artificial intelligence deep-learning reconstruction algorithm, Dose reduction opportunity, and Virtual monoenergetic reconstruction.","Artificial intelligence (AI)-based CT-IQA also becomes a trend.","It increases the accuracy of the CT scanning apparatus, amplifies the impact of the CT system reconstruction algorithm, and creates an effective algorithm for post-processing CT images.","AI-based medical IQA offers excellent application opportunities in clinical work.","AI can provide uniform quality assessment criteria and more comprehensive guidance amongst various healthcare facilities, and encourage them to identify one another's images.","It will help lower the number of unnecessary tests and associated costs, and enhance the quality of medical imaging and assessment efficiency."],"url":"http://arxiv.org/abs/2405.00075v1","category":"eess.IV"}
{"created":"2024-05-01 17:58:11","title":"Quantum algorithms for matrix geometric means","abstract":"Matrix geometric means between two positive definite matrices can be defined equivalently from distinct perspectives - as solutions to certain nonlinear systems of equations, as points along geodesics in Riemannian geometry, and as solutions to certain optimisation problems. This diversity already suggests the potential for varied applications, as well as acting as a bridge between different domains. Here we devise new quantum subroutines to efficiently prepare quantum unitary operators that embed the standard matrix geometric mean and its generalisations called the weighted matrix geometric mean. This enables the construction of solutions to the algebraic Riccati equation, which is an important class of nonlinear systems of equations that appears in machine learning, optimal control, estimation, and filtering. Using these subroutines, we present a new class of quantum learning algorithms called quantum geometric mean metric learning. This has applications in efficiently finding the best distance measure and solving classification problems in the weakly supervised limit and for anomaly detection, for both classical and quantum problems. We also show how our method can be generalised to a particular p^th-order system of nonlinear equations. These quantum subroutines for matrix geometric means are also useful in other areas of quantum information. For example, we show how to use them in the estimation of geometric Renyi relative entropies and the Uhlmann fidelity by means of the Fuchs-Caves observable. In particular, our quantum algorithms for estimating the Uhlmann and Matsumoto fidelities have optimal dependence on the precision. Finally, we provide a BQP-complete problem based on matrix geometric means that can be solved by our subroutines, thus characterising their computational capability.","sentences":["Matrix geometric means between two positive definite matrices can be defined equivalently from distinct perspectives - as solutions to certain nonlinear systems of equations, as points along geodesics in Riemannian geometry, and as solutions to certain optimisation problems.","This diversity already suggests the potential for varied applications, as well as acting as a bridge between different domains.","Here we devise new quantum subroutines to efficiently prepare quantum unitary operators that embed the standard matrix geometric mean and its generalisations called the weighted matrix geometric mean.","This enables the construction of solutions to the algebraic Riccati equation, which is an important class of nonlinear systems of equations that appears in machine learning, optimal control, estimation, and filtering.","Using these subroutines, we present a new class of quantum learning algorithms called quantum geometric mean metric learning.","This has applications in efficiently finding the best distance measure and solving classification problems in the weakly supervised limit and for anomaly detection, for both classical and quantum problems.","We also show how our method can be generalised to a particular p^th-order system of nonlinear equations.","These quantum subroutines for matrix geometric means are also useful in other areas of quantum information.","For example, we show how to use them in the estimation of geometric Renyi relative entropies and the Uhlmann fidelity by means of the Fuchs-Caves observable.","In particular, our quantum algorithms for estimating the Uhlmann and Matsumoto fidelities have optimal dependence on the precision.","Finally, we provide a BQP-complete problem based on matrix geometric means that can be solved by our subroutines, thus characterising their computational capability."],"url":"http://arxiv.org/abs/2405.00673v1","category":"quant-ph"}
{"created":"2024-05-01 17:52:29","title":"Optimizing Profitability in Timely Gossip Networks","abstract":"We consider a communication system where a group of users, interconnected in a bidirectional gossip network, wishes to follow a time-varying source, e.g., updates on an event, in real-time. The users wish to maintain their expected version ages below a threshold, and can either rely on gossip from their neighbors or directly subscribe to a server publishing about the event, if the former option does not meet the timeliness requirements. The server wishes to maximize its profit by increasing subscriptions from users and minimizing event sampling frequency to reduce costs. This leads to a Stackelberg game between the server and the users where the sender is the leader deciding its sampling frequency and the users are the followers deciding their subscription strategies. We investigate equilibrium strategies for low-connectivity and high-connectivity topologies.","sentences":["We consider a communication system where a group of users, interconnected in a bidirectional gossip network, wishes to follow a time-varying source, e.g., updates on an event, in real-time.","The users wish to maintain their expected version ages below a threshold, and can either rely on gossip from their neighbors or directly subscribe to a server publishing about the event, if the former option does not meet the timeliness requirements.","The server wishes to maximize its profit by increasing subscriptions from users and minimizing event sampling frequency to reduce costs.","This leads to a Stackelberg game between the server and the users where the sender is the leader deciding its sampling frequency and the users are the followers deciding their subscription strategies.","We investigate equilibrium strategies for low-connectivity and high-connectivity topologies."],"url":"http://arxiv.org/abs/2405.00665v1","category":"cs.IT"}
{"created":"2024-05-01 17:10:55","title":"From Empirical Observations to Universality: Dynamics of Deep Learning with Inputs Built on Gaussian mixture","abstract":"This study broadens the scope of theoretical frameworks in deep learning by delving into the dynamics of neural networks with inputs that demonstrate the structural characteristics to Gaussian Mixture (GM). We analyzed how the dynamics of neural networks under GM-structured inputs diverge from the predictions of conventional theories based on simple Gaussian structures. A revelation of our work is the observed convergence of neural network dynamics towards conventional theory even with standardized GM inputs, highlighting an unexpected universality. We found that standardization, especially in conjunction with certain nonlinear functions, plays a critical role in this phenomena. Consequently, despite the complex and varied nature of GM distributions, we demonstrate that neural networks exhibit asymptotic behaviors in line with predictions under simple Gaussian frameworks.","sentences":["This study broadens the scope of theoretical frameworks in deep learning by delving into the dynamics of neural networks with inputs that demonstrate the structural characteristics to Gaussian Mixture (GM).","We analyzed how the dynamics of neural networks under GM-structured inputs diverge from the predictions of conventional theories based on simple Gaussian structures.","A revelation of our work is the observed convergence of neural network dynamics towards conventional theory even with standardized GM inputs, highlighting an unexpected universality.","We found that standardization, especially in conjunction with certain nonlinear functions, plays a critical role in this phenomena.","Consequently, despite the complex and varied nature of GM distributions, we demonstrate that neural networks exhibit asymptotic behaviors in line with predictions under simple Gaussian frameworks."],"url":"http://arxiv.org/abs/2405.00642v1","category":"stat.ML"}
{"created":"2024-05-01 17:05:22","title":"A Distributed Model Identification Algorithm for Multi-Agent Systems","abstract":"In this study, we investigate agent-based approach for system model identification with an emphasis on power distribution system applications. Departing from conventional practices of relying on historical data for offline model identification, we adopt an online update approach utilizing real-time data by employing the latest data points for gradient computation. This methodology offers advantages including a large reduction in the communication network's bandwidth requirements by minimizing the data exchanged at each iteration and enabling the model to adapt in real-time to disturbances. Furthermore, we extend our model identification process from linear frameworks to more complex non-linear convex models. This extension is validated through numerical studies demonstrating improved control performance for a synthetic IEEE test case.","sentences":["In this study, we investigate agent-based approach for system model identification with an emphasis on power distribution system applications.","Departing from conventional practices of relying on historical data for offline model identification, we adopt an online update approach utilizing real-time data by employing the latest data points for gradient computation.","This methodology offers advantages including a large reduction in the communication network's bandwidth requirements by minimizing the data exchanged at each iteration and enabling the model to adapt in real-time to disturbances.","Furthermore, we extend our model identification process from linear frameworks to more complex non-linear convex models.","This extension is validated through numerical studies demonstrating improved control performance for a synthetic IEEE test case."],"url":"http://arxiv.org/abs/2405.00637v1","category":"eess.SY"}
{"created":"2024-05-01 17:01:26","title":"Bye bye, local bias: the statistics of the halo field are not determined by the local mass density","abstract":"Bias models relating the dark matter field to the spatial distribution of halos are widely used in cosmological analyses. Many such models predict halos purely from the local matter density, an assumption which has not been verified in a model-agnostic setting. Bias models in perturbation theory require the inclusion of other local properties, but it is not clear whether this extends to non-perturbative approaches. We assess the validity of the assumption that only the local dark matter density can be used to predict the number density of halos in a model-independent way and in the non-perturbative regime. Utilising $N$-body simulations, we introduce a test wherein we study the properties of the halo counts field after spatial voxels with near-equal dark matter density have been permuted. If local-in-matter-density (LIMD) biasing were valid, the statistical properties of the permuted and un-permuted fields would be indistinguishable since both are equally fair draws of the stochastic biasing model. For voxels of side length $\\sim4-60\\,h^{-1}{\\rm\\,Mpc}$ and for halos less massive than $\\sim10^{15}\\,h^{-1}{\\rm\\,M_\\odot}$, the permuted halo field has significantly too much power on large scales compared to the truth. We interpret this as due to LIMD models removing small-scale power by not modelling correlations between neighbouring voxels. Since the permutation conserves the total variance of the halo counts field, large-scale power is substantially boosted to compensate. This conclusion is robust to the choice of initial conditions and cosmology. LIMD halo biasing cannot, therefore, reproduce the distribution of halos across a large range of scales and halo masses, no matter how complex the model. To reproduce this distribution accurately, one must allow the biasing to be a function of other quantities and/or remove the assumption that neighbouring voxels are statistically independent.","sentences":["Bias models relating the dark matter field to the spatial distribution of halos are widely used in cosmological analyses.","Many such models predict halos purely from the local matter density, an assumption which has not been verified in a model-agnostic setting.","Bias models in perturbation theory require the inclusion of other local properties, but it is not clear whether this extends to non-perturbative approaches.","We assess the validity of the assumption that only the local dark matter density can be used to predict the number density of halos in a model-independent way and in the non-perturbative regime.","Utilising $N$-body simulations, we introduce a test wherein we study the properties of the halo counts field after spatial voxels with near-equal dark matter density have been permuted.","If local-in-matter-density (LIMD) biasing were valid, the statistical properties of the permuted and un-permuted fields would be indistinguishable since both are equally fair draws of the stochastic biasing model.","For voxels of side length $\\sim4-60\\,h^{-1}{\\rm\\,Mpc}$ and for halos less massive than $\\sim10^{15}\\,h^{-1}{\\rm\\,M_\\odot}$, the permuted halo field has significantly too much power on large scales compared to the truth.","We interpret this as due to LIMD models removing small-scale power by not modelling correlations between neighbouring voxels.","Since the permutation conserves the total variance of the halo counts field, large-scale power is substantially boosted to compensate.","This conclusion is robust to the choice of initial conditions and cosmology.","LIMD halo biasing cannot, therefore, reproduce the distribution of halos across a large range of scales and halo masses, no matter how complex the model.","To reproduce this distribution accurately, one must allow the biasing to be a function of other quantities and/or remove the assumption that neighbouring voxels are statistically independent."],"url":"http://arxiv.org/abs/2405.00635v1","category":"astro-ph.CO"}
{"created":"2024-05-01 16:49:54","title":"Koopman-based Deep Learning for Nonlinear System Estimation","abstract":"Nonlinear differential equations are encountered as models of fluid flow, spiking neurons, and many other systems of interest in the real world. Common features of these systems are that their behaviors are difficult to describe exactly and invariably unmodeled dynamics present challenges in making precise predictions. In many cases the models exhibit extremely complicated behavior due to bifurcations and chaotic regimes. In this paper, we present a novel data-driven linear estimator that uses Koopman operator theory to extract finite-dimensional representations of complex nonlinear systems. The extracted model is used together with a deep reinforcement learning network that learns the optimal stepwise actions to predict future states of the original nonlinear system. Our estimator is also adaptive to a diffeomorphic transformation of the nonlinear system which enables transfer learning to compute state estimates of the transformed system without relearning from scratch.","sentences":["Nonlinear differential equations are encountered as models of fluid flow, spiking neurons, and many other systems of interest in the real world.","Common features of these systems are that their behaviors are difficult to describe exactly and invariably unmodeled dynamics present challenges in making precise predictions.","In many cases the models exhibit extremely complicated behavior due to bifurcations and chaotic regimes.","In this paper, we present a novel data-driven linear estimator that uses Koopman operator theory to extract finite-dimensional representations of complex nonlinear systems.","The extracted model is used together with a deep reinforcement learning network that learns the optimal stepwise actions to predict future states of the original nonlinear system.","Our estimator is also adaptive to a diffeomorphic transformation of the nonlinear system which enables transfer learning to compute state estimates of the transformed system without relearning from scratch."],"url":"http://arxiv.org/abs/2405.00627v1","category":"eess.SY"}
{"created":"2024-05-01 16:48:28","title":"Queue-based Eco-Driving at Roundabouts with Reinforcement Learning","abstract":"We address eco-driving at roundabouts in mixed traffic to enhance traffic flow and traffic efficiency in urban areas. The aim is to proactively optimize speed of automated or non-automated connected vehicles (CVs), ensuring both an efficient approach and smooth entry into roundabouts. We incorporate the traffic situation ahead, i.e. preceding vehicles and waiting queues. Further, we develop two approaches: a rule-based and an Reinforcement Learning (RL) based eco-driving system, with both using the approach link and information from conflicting CVs for speed optimization. A fair comparison of rule-based and RL-based approaches is performed to explore RL as a viable alternative to classical optimization. Results show that both approaches outperform the baseline. Improvements significantly increase with growing traffic volumes, leading to best results on average being obtained at high volumes. Near capacity, performance deteriorates, indicating limited applicability at capacity limits. Examining different CV penetration rates, a decline in performance is observed, but with substantial results still being achieved at lower CV rates. RL agents can discover effective policies for speed optimization in dynamic roundabout settings, but they do not offer a substantial advantage over classical approaches, especially at higher traffic volumes or lower CV penetration rates.","sentences":["We address eco-driving at roundabouts in mixed traffic to enhance traffic flow and traffic efficiency in urban areas.","The aim is to proactively optimize speed of automated or non-automated connected vehicles (CVs), ensuring both an efficient approach and smooth entry into roundabouts.","We incorporate the traffic situation ahead, i.e. preceding vehicles and waiting queues.","Further, we develop two approaches: a rule-based and an Reinforcement Learning (RL) based eco-driving system, with both using the approach link and information from conflicting CVs for speed optimization.","A fair comparison of rule-based and RL-based approaches is performed to explore RL as a viable alternative to classical optimization.","Results show that both approaches outperform the baseline.","Improvements significantly increase with growing traffic volumes, leading to best results on average being obtained at high volumes.","Near capacity, performance deteriorates, indicating limited applicability at capacity limits.","Examining different CV penetration rates, a decline in performance is observed, but with substantial results still being achieved at lower CV rates.","RL agents can discover effective policies for speed optimization in dynamic roundabout settings, but they do not offer a substantial advantage over classical approaches, especially at higher traffic volumes or lower CV penetration rates."],"url":"http://arxiv.org/abs/2405.00625v1","category":"cs.LG"}
{"created":"2024-05-01 16:35:16","title":"Chemistry in externally FUV irradiated disks in the outskirts of the Orion Nebula","abstract":"Most stars are born in stellar clusters and their protoplanetary disks, which are the birthplaces of planets, can therefore be affected by the radiation of nearby massive stars. However, little is known about the chemistry of externally irradiated disks, including whether or not their properties are similar to the so-far better-studied isolated disks. Motivated by this question, we present ALMA Band 6 observations of two irradiated Class II protoplanetary disks in the outskirts of the Orion Nebula Cluster (ONC) to explore the chemical composition of disks exposed to (external) FUV radiation fields: the 216-0939 disk and the binary system 253-1536A/B, which are exposed to radiation fields of $10^2-10^3$ times the average interstellar radiation field. We detect lines from CO isotopologues, HCN, H$_2$CO, and C$_2$H toward both protoplanetary disks. Based on the observed disk-integrated line fluxes and flux ratios, we do not find significant differences between isolated and irradiated disks. The observed differences seem to be more closely related to the different stellar masses than to the external radiation field. This suggests that these disks are far enough away from the massive Trapezium stars, that their chemistry is no longer affected by external FUV radiation. Additional observations towards lower-mass disks and disks closer to the massive Trapezium stars are required to elucidate the level of external radiation required to make an impact on the chemistry of planet formation in different kinds of disks.","sentences":["Most stars are born in stellar clusters and their protoplanetary disks, which are the birthplaces of planets, can therefore be affected by the radiation of nearby massive stars.","However, little is known about the chemistry of externally irradiated disks, including whether or not their properties are similar to the so-far better-studied isolated disks.","Motivated by this question, we present ALMA Band 6 observations of two irradiated Class II protoplanetary disks in the outskirts of the Orion Nebula Cluster (ONC) to explore the chemical composition of disks exposed to (external) FUV radiation fields: the 216-0939 disk and the binary system 253-1536A/B, which are exposed to radiation fields of $10^2-10^3$ times the average interstellar radiation field.","We detect lines from CO isotopologues, HCN, H$_2$CO, and C$_2$H toward both protoplanetary disks.","Based on the observed disk-integrated line fluxes and flux ratios, we do not find significant differences between isolated and irradiated disks.","The observed differences seem to be more closely related to the different stellar masses than to the external radiation field.","This suggests that these disks are far enough away from the massive Trapezium stars, that their chemistry is no longer affected by external FUV radiation.","Additional observations towards lower-mass disks and disks closer to the massive Trapezium stars are required to elucidate the level of external radiation required to make an impact on the chemistry of planet formation in different kinds of disks."],"url":"http://arxiv.org/abs/2405.00615v1","category":"astro-ph.EP"}
{"created":"2024-05-01 16:34:09","title":"Two-loop mixed QCD-EW corrections to charged current Drell-Yan","abstract":"We present the two-loop mixed strong-electroweak virtual corrections to the charged current Drell-Yan process. The final-state collinear singularities are regularised by the lepton mass. The evaluation of all the relevant Feynman integrals, including those with up to two different internal massive lines, has been worked out relying on semi-analytical techniques, using complex-valued masses. We can provide, at any arbitrary phase-space point, the solution as a power series in the $W$-boson mass, around a reference value. Starting from these expansions, we can prepare a numerical grid for any value of the $W$-boson mass within their radius of convergence in a negligible amount of time.","sentences":["We present the two-loop mixed strong-electroweak virtual corrections to the charged current Drell-Yan process.","The final-state collinear singularities are regularised by the lepton mass.","The evaluation of all the relevant Feynman integrals, including those with up to two different internal massive lines, has been worked out relying on semi-analytical techniques, using complex-valued masses.","We can provide, at any arbitrary phase-space point, the solution as a power series in the $W$-boson mass, around a reference value.","Starting from these expansions, we can prepare a numerical grid for any value of the $W$-boson mass within their radius of convergence in a negligible amount of time."],"url":"http://arxiv.org/abs/2405.00612v1","category":"hep-ph"}
{"created":"2024-05-01 16:28:22","title":"Inverse images of a positive closed current for a holomorphic endomorphism of a compact K\u00e4hler manifold","abstract":"In this paper, we prove that for a given surjective holomorphic endomorphism $f$ of a compact K\\\"ahler manifold $X$ and for some integer $p$ with $1\\le p\\le k$, there exists a proper invariant analytic subset $E$ for $f$ such that if $S$ is smooth in a neighborhood of $E$, the sequence $d_p^{-n}(f^n)^*(S-\\alpha_S)$ converges to $0$ exponentially fast in the sense of currents where $d_p$ denotes the dynamical degree of order $p$ and $\\alpha_S$ is a closed smooth form in the de Rham cohomology class of $S$.","sentences":["In this paper, we prove that for a given surjective holomorphic endomorphism $f$ of a compact K\\\"ahler manifold $X$ and for some integer $p$ with $1\\le p\\le k$, there exists a proper invariant analytic subset $E$ for $f$ such that if $S$ is smooth in a neighborhood of $E$, the sequence $d_p^{-n}(f^n)^*(S-\\alpha_S)$ converges to $0$ exponentially fast in the sense of currents where $d_p$ denotes the dynamical degree of order $p$ and $\\alpha_S$ is a closed smooth form in the de Rham cohomology class of $S$."],"url":"http://arxiv.org/abs/2405.00607v1","category":"math.CV"}
{"created":"2024-05-01 16:13:09","title":"Radar-Based Localization For Autonomous Ground Vehicles In Suburban Neighborhoods","abstract":"For autonomous ground vehicles (AGVs) deployed in suburban neighborhoods and other human-centric environments the problem of localization remains a fundamental challenge. There are well established methods for localization with GPS, lidar, and cameras. But even in ideal conditions these have limitations. GPS is not always available and is often not accurate enough on its own, visual methods have difficulty coping with appearance changes due to weather and other factors, and lidar methods are prone to defective solutions due to ambiguous scene geometry. Radar on the other hand is not highly susceptible to these problems, owing in part to its longer range. Further, radar is also robust to challenging conditions that interfere with vision and lidar including fog, smoke, rain, and darkness. We present a radar-based localization system that includes a novel method for highly-accurate radar odometry for smooth, high-frequency relative pose estimation and a novel method for radar-based place recognition and relocalization. We present experiments demonstrating our methods' accuracy and reliability, which are comparable with \\new{other methods' published results for radar localization and we find outperform a similar method as ours applied to lidar measurements}. Further, we show our methods are lightweight enough to run on common low-power embedded hardware with ample headroom for other autonomy functions.","sentences":["For autonomous ground vehicles (AGVs) deployed in suburban neighborhoods and other human-centric environments the problem of localization remains a fundamental challenge.","There are well established methods for localization with GPS, lidar, and cameras.","But even in ideal conditions these have limitations.","GPS is not always available and is often not accurate enough on its own, visual methods have difficulty coping with appearance changes due to weather and other factors, and lidar methods are prone to defective solutions due to ambiguous scene geometry.","Radar on the other hand is not highly susceptible to these problems, owing in part to its longer range.","Further, radar is also robust to challenging conditions that interfere with vision and lidar including fog, smoke, rain, and darkness.","We present a radar-based localization system that includes a novel method for highly-accurate radar odometry for smooth, high-frequency relative pose estimation and a novel method for radar-based place recognition and relocalization.","We present experiments demonstrating our methods' accuracy and reliability, which are comparable with \\new{other methods' published results for radar localization and we find outperform a similar method as ours applied to lidar measurements}.","Further, we show our methods are lightweight enough to run on common low-power embedded hardware with ample headroom for other autonomy functions."],"url":"http://arxiv.org/abs/2405.00600v1","category":"cs.RO"}
{"created":"2024-05-01 15:45:06","title":"Multi-Robot Strategies for Communication-Constrained Exploration and Electrostatic Anomaly Characterization","abstract":"Exploration of extreme or remote environments such as Mars is often recognized as an opportunity for multi-robot systems. However, this poses challenges for maintaining robust inter-robot communication without preexisting infrastructure. It may be that robots can only share information when they are physically in close proximity with each other. At the same time, atmospheric phenomena such as dust devils are poorly understood and characterization of their electrostatic properties is of scientific interest. We perform a comparative analysis of two multi-robot communication strategies: a distributed approach, with pairwise intermittent rendezvous, and a centralized, fixed base station approach. We also introduce and evaluate the effectiveness of an algorithm designed to predict the location and strength of electrostatic anomalies, assuming robot proximity. Using an agent-based simulation, we assess the performance of these strategies in a 2D grid cell representation of a Martian environment. Results indicate that a decentralized rendezvous system consistently outperforms a fixed base station system in terms of exploration speed and in reducing the risk of data loss. We also find that inter-robot data sharing improves performance when trying to predict the location and strength of an electrostatic anomaly. These findings indicate the importance of appropriate communication strategies for efficient multi-robot science missions.","sentences":["Exploration of extreme or remote environments such as Mars is often recognized as an opportunity for multi-robot systems.","However, this poses challenges for maintaining robust inter-robot communication without preexisting infrastructure.","It may be that robots can only share information when they are physically in close proximity with each other.","At the same time, atmospheric phenomena such as dust devils are poorly understood and characterization of their electrostatic properties is of scientific interest.","We perform a comparative analysis of two multi-robot communication strategies: a distributed approach, with pairwise intermittent rendezvous, and a centralized, fixed base station approach.","We also introduce and evaluate the effectiveness of an algorithm designed to predict the location and strength of electrostatic anomalies, assuming robot proximity.","Using an agent-based simulation, we assess the performance of these strategies in a 2D grid cell representation of a Martian environment.","Results indicate that a decentralized rendezvous system consistently outperforms a fixed base station system in terms of exploration speed and in reducing the risk of data loss.","We also find that inter-robot data sharing improves performance when trying to predict the location and strength of an electrostatic anomaly.","These findings indicate the importance of appropriate communication strategies for efficient multi-robot science missions."],"url":"http://arxiv.org/abs/2405.00586v1","category":"cs.RO"}
{"created":"2024-05-01 15:33:14","title":"Conformalized Tensor Completion with Riemannian Optimization","abstract":"Tensor data, or multi-dimensional array, is a data format popular in multiple fields such as social network analysis, recommender systems, and brain imaging. It is not uncommon to observe tensor data containing missing values and tensor completion aims at estimating the missing values given the partially observed tensor. Sufficient efforts have been spared on devising scalable tensor completion algorithms but few on quantifying the uncertainty of the estimator. In this paper, we nest the uncertainty quantification (UQ) of tensor completion under a split conformal prediction framework and establish the connection of the UQ problem to a problem of estimating the missing propensity of each tensor entry. We model the data missingness of the tensor with a tensor Ising model parameterized by a low-rank tensor parameter. We propose to estimate the tensor parameter by maximum pseudo-likelihood estimation (MPLE) with a Riemannian gradient descent algorithm. Extensive simulation studies have been conducted to justify the validity of the resulting conformal interval. We apply our method to the regional total electron content (TEC) reconstruction problem.","sentences":["Tensor data, or multi-dimensional array, is a data format popular in multiple fields such as social network analysis, recommender systems, and brain imaging.","It is not uncommon to observe tensor data containing missing values and tensor completion aims at estimating the missing values given the partially observed tensor.","Sufficient efforts have been spared on devising scalable tensor completion algorithms but few on quantifying the uncertainty of the estimator.","In this paper, we nest the uncertainty quantification (UQ) of tensor completion under a split conformal prediction framework and establish the connection of the UQ problem to a problem of estimating the missing propensity of each tensor entry.","We model the data missingness of the tensor with a tensor Ising model parameterized by a low-rank tensor parameter.","We propose to estimate the tensor parameter by maximum pseudo-likelihood estimation (MPLE) with a Riemannian gradient descent algorithm.","Extensive simulation studies have been conducted to justify the validity of the resulting conformal interval.","We apply our method to the regional total electron content (TEC) reconstruction problem."],"url":"http://arxiv.org/abs/2405.00581v1","category":"stat.ME"}
{"created":"2024-05-01 15:22:05","title":"A Modular Pragmatic Architecture for Multiuser MIMO with Array-Fed RIS","abstract":"We propose a power- and hardware-efficient, pragmatic, modular, multiuser/multibeam array-fed RIS architecture particularly suited to operate in very high frequency bands (high mmWave and sub-THz), where channels are typically sparse in the beamspace and line-of-sight (LOS) is required to achieve an acceptable received signal level. The key module is an active multi-antenna feeder (AMAF) with a small number of active antennas placed in the near field of a RIS with a much larger number of passive controllable reflecting elements. We propose a pragmatic approach to obtain a steerable beam with high gain and very low sidelobes. Then, $K$ independently controlled beams can be achieved by stacking $K$ of such AMAF-RIS modules. Our analysis takes in full account: 1) the near-end crosstalk (NEXT) between the modules, 2) the far-end crosstalk (FEXT) due to the sidelobes; 3) a thorough energy efficiency comparison with respect to conventional {\\em active arrays} with the same beamforming performance. Overall, we show that the proposed architecture is very attractive in terms of spectral efficiency, ease of implementation (hardware complexity), and energy efficiency.","sentences":["We propose a power- and hardware-efficient, pragmatic, modular, multiuser/multibeam array-fed RIS architecture particularly suited to operate in very high frequency bands (high mmWave and sub-THz), where channels are typically sparse in the beamspace and line-of-sight (LOS) is required to achieve an acceptable received signal level.","The key module is an active multi-antenna feeder (AMAF) with a small number of active antennas placed in the near field of a RIS with a much larger number of passive controllable reflecting elements.","We propose a pragmatic approach to obtain a steerable beam with high gain and very low sidelobes.","Then, $K$ independently controlled beams can be achieved by stacking $K$ of such AMAF-RIS modules.","Our analysis takes in full account: 1) the near-end crosstalk (NEXT) between the modules, 2) the far-end crosstalk (FEXT) due to the sidelobes; 3) a thorough energy efficiency comparison with respect to conventional {\\em active arrays} with the same beamforming performance.","Overall, we show that the proposed architecture is very attractive in terms of spectral efficiency, ease of implementation (hardware complexity), and energy efficiency."],"url":"http://arxiv.org/abs/2405.00572v1","category":"eess.SP"}
{"created":"2024-05-01 15:15:52","title":"Leveraging Stack Traces for Spectrum-based Fault Localization in the Absence of Failing Tests","abstract":"Bug fixing is a crucial task in software maintenance to hold user trust. Although various automated fault localization techniques exist, they often require specific conditions to be effective. For example, Spectrum-Based Fault Localization (SBFL) techniques need at least one failing test to identify bugs, which may not always be available. Bug reports, particularly those with stack traces, provide detailed information on system execution failures and are invaluable for developers. This study focuses on utilizing stack traces from crash reports as fault-triggering tests for SBFL. Our findings indicate that only 3.33% of bugs have fault-triggering tests, limiting traditional SBFL efficiency. However, 98.3% of bugfix intentions align directly with exceptions in stack traces, and 78.3% of buggy methods are reachable within an average of 0.34 method calls, proving stack traces as a reliable source for locating bugs. We introduce a new approach, SBEST, that integrates stack trace data with test coverage to enhance fault localization. Our approach shows a significant improvement, increasing Mean Average Precision (MAP) by 32.22% and Mean Reciprocal Rank (MRR) by 17.43% over traditional stack trace ranking methods.","sentences":["Bug fixing is a crucial task in software maintenance to hold user trust.","Although various automated fault localization techniques exist, they often require specific conditions to be effective.","For example, Spectrum-Based Fault Localization (SBFL) techniques need at least one failing test to identify bugs, which may not always be available.","Bug reports, particularly those with stack traces, provide detailed information on system execution failures and are invaluable for developers.","This study focuses on utilizing stack traces from crash reports as fault-triggering tests for SBFL.","Our findings indicate that only 3.33% of bugs have fault-triggering tests, limiting traditional SBFL efficiency.","However, 98.3% of bugfix intentions align directly with exceptions in stack traces, and 78.3% of buggy methods are reachable within an average of 0.34 method calls, proving stack traces as a reliable source for locating bugs.","We introduce a new approach, SBEST, that integrates stack trace data with test coverage to enhance fault localization.","Our approach shows a significant improvement, increasing Mean Average Precision (MAP) by 32.22% and Mean Reciprocal Rank (MRR) by 17.43% over traditional stack trace ranking methods."],"url":"http://arxiv.org/abs/2405.00565v1","category":"cs.SE"}
{"created":"2024-05-01 15:12:19","title":"Contribution of PRIDE VLBI products to the joint JUICE-Europa Clipper moons' ephemerides solution","abstract":"In the coming decade, JUICE and Europa Clipper radio-science will yield the most accurate estimation to date of the Galilean moons' physical parameters and ephemerides. JUICE's PRIDE (Planetary Radio Interferometry and Doppler Experiment) will help achieve such a solution by providing VLBI (Very Long Baseline Interferometry) observations of the spacecraft's lateral position, complementing nominal radio-science measurements. We quantify how PRIDE VLBI can contribute to the moons' ephemerides determination, in terms of attainable solution improvement and validation opportunities. To this end, we simulated both single- and dual-spacecraft VLBI, exploiting the potential simultaneous tracking of JUICE and Europa Clipper. We considered various tracking and data quality scenarios, and compared the formal uncertainties obtained with and without VLBI. This was performed for both global and local (i.e., per-flyby) estimations of the moons' states, as achieving a global solution first requires proceeding arc-per-arc. We showed that both single- and dual-spacecraft VLBI only bring limited improvement to the global state estimation, but significantly contribute to the moons' normal points (i.e., local states at flyby times), most notably in the out-of-plane direction. Additionally, we designed a validation plan exploiting PRIDE VLBI to progressively validate the classical radio-science solution. By improving the local state estimations and offering various validation opportunities, PRIDE will be invaluable in overcoming possible dynamical modelling challenges. It can therefore play a key role in reconstructing a global solution for the Galilean moons' dynamics with the uncertainty levels promised by JUICE-Europa Clipper analyses. This, in turn, is critical to the accurate characterisation of tidal dissipation in the Jovian system, holding the key to the long-term evolution of the Galilean moons.","sentences":["In the coming decade, JUICE and Europa Clipper radio-science will yield the most accurate estimation to date of the Galilean moons' physical parameters and ephemerides.","JUICE's PRIDE (Planetary Radio Interferometry and Doppler Experiment) will help achieve such a solution by providing VLBI (Very Long Baseline Interferometry) observations of the spacecraft's lateral position, complementing nominal radio-science measurements.","We quantify how PRIDE VLBI can contribute to the moons' ephemerides determination, in terms of attainable solution improvement and validation opportunities.","To this end, we simulated both single- and dual-spacecraft VLBI, exploiting the potential simultaneous tracking of JUICE and Europa Clipper.","We considered various tracking and data quality scenarios, and compared the formal uncertainties obtained with and without VLBI.","This was performed for both global and local (i.e., per-flyby) estimations of the moons' states, as achieving a global solution first requires proceeding arc-per-arc.","We showed that both single- and dual-spacecraft VLBI only bring limited improvement to the global state estimation, but significantly contribute to the moons' normal points (i.e., local states at flyby times), most notably in the out-of-plane direction.","Additionally, we designed a validation plan exploiting PRIDE VLBI to progressively validate the classical radio-science solution.","By improving the local state estimations and offering various validation opportunities, PRIDE will be invaluable in overcoming possible dynamical modelling challenges.","It can therefore play a key role in reconstructing a global solution for the Galilean moons' dynamics with the uncertainty levels promised by JUICE-Europa Clipper analyses.","This, in turn, is critical to the accurate characterisation of tidal dissipation in the Jovian system, holding the key to the long-term evolution of the Galilean moons."],"url":"http://arxiv.org/abs/2405.00562v1","category":"astro-ph.EP"}
{"created":"2024-05-01 15:06:58","title":"An Energy Stable Well-balanced Scheme for the Barotropic Euler System with Gravity under the Anelastic Scaling","abstract":"We design and analyse an energy stable, structure preserving, well-balanced and asymptotic preserving (AP) scheme for the barotropic Euler system with gravity in the anelastic limit. The key to energy stability is the introduction of appropriate velocity shifts in the convective fluxes of mass and momenta. The semi-implicit in time and finite volume in space fully-discrete scheme supports the positivity of density and yields the consistency with the weak solutions of the Euler system upon mesh refinement. The numerical scheme admits the discrete hydrostatic states as solutions and the stability of numerical solutions in terms of the relative energy leads to well-balancing. The AP property of the scheme, i.e. the boundedness of the mesh parameters with respect to the Mach/Froude numbers and the scheme's asymptotic consistency with the anelastic Euler system is rigorously shown on the basis of apriori energy estimates. The numerical scheme is resolved in two steps: by solving a non-linear elliptic problem for the density and a subsequent explicit computation of the velocity. Results from several benchmark case studies are presented to corroborate the proposed claims.","sentences":["We design and analyse an energy stable, structure preserving, well-balanced and asymptotic preserving (AP) scheme for the barotropic Euler system with gravity in the anelastic limit.","The key to energy stability is the introduction of appropriate velocity shifts in the convective fluxes of mass and momenta.","The semi-implicit in time and finite volume in space fully-discrete scheme supports the positivity of density and yields the consistency with the weak solutions of the Euler system upon mesh refinement.","The numerical scheme admits the discrete hydrostatic states as solutions and the stability of numerical solutions in terms of the relative energy leads to well-balancing.","The AP property of the scheme, i.e. the boundedness of the mesh parameters with respect to the Mach/Froude numbers and the scheme's asymptotic consistency with the anelastic Euler system is rigorously shown on the basis of apriori energy estimates.","The numerical scheme is resolved in two steps: by solving a non-linear elliptic problem for the density and a subsequent explicit computation of the velocity.","Results from several benchmark case studies are presented to corroborate the proposed claims."],"url":"http://arxiv.org/abs/2405.00559v1","category":"math.NA"}
{"created":"2024-05-01 14:56:56","title":"A First Look at Selection Bias in Preference Elicitation for Recommendation","abstract":"Preference elicitation explicitly asks users what kind of recommendations they would like to receive. It is a popular technique for conversational recommender systems to deal with cold-starts. Previous work has studied selection bias in implicit feedback, e.g., clicks, and in some forms of explicit feedback, i.e., ratings on items. Despite the fact that the extreme sparsity of preference elicitation interactions make them severely more prone to selection bias than natural interactions, the effect of selection bias in preference elicitation on the resulting recommendations has not been studied yet. To address this gap, we take a first look at the effects of selection bias in preference elicitation and how they may be further investigated in the future. We find that a big hurdle is the current lack of any publicly available dataset that has preference elicitation interactions. As a solution, we propose a simulation of a topic-based preference elicitation process. The results from our simulation-based experiments indicate (i) that ignoring the effect of selection bias early in preference elicitation can lead to an exacerbation of overrepresentation in subsequent item recommendations, and (ii) that debiasing methods can alleviate this effect, which leads to significant improvements in subsequent item recommendation performance. Our aim is for the proposed simulator and initial results to provide a starting point and motivation for future research into this important but overlooked problem setting.","sentences":["Preference elicitation explicitly asks users what kind of recommendations they would like to receive.","It is a popular technique for conversational recommender systems to deal with cold-starts.","Previous work has studied selection bias in implicit feedback, e.g., clicks, and in some forms of explicit feedback, i.e., ratings on items.","Despite the fact that the extreme sparsity of preference elicitation interactions make them severely more prone to selection bias than natural interactions, the effect of selection bias in preference elicitation on the resulting recommendations has not been studied yet.","To address this gap, we take a first look at the effects of selection bias in preference elicitation and how they may be further investigated in the future.","We find that a big hurdle is the current lack of any publicly available dataset that has preference elicitation interactions.","As a solution, we propose a simulation of a topic-based preference elicitation process.","The results from our simulation-based experiments indicate (i) that ignoring the effect of selection bias early in preference elicitation can lead to an exacerbation of overrepresentation in subsequent item recommendations, and (ii) that debiasing methods can alleviate this effect, which leads to significant improvements in subsequent item recommendation performance.","Our aim is for the proposed simulator and initial results to provide a starting point and motivation for future research into this important but overlooked problem setting."],"url":"http://arxiv.org/abs/2405.00554v1","category":"cs.IR"}
{"created":"2024-05-01 14:50:58","title":"Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs","abstract":"We present a novel approach for long-term human trajectory prediction, which is essential for long-horizon robot planning in human-populated environments. State-of-the-art human trajectory prediction methods are limited by their focus on collision avoidance and short-term planning, and their inability to model complex interactions of humans with the environment. In contrast, our approach overcomes these limitations by predicting sequences of human interactions with the environment and using this information to guide trajectory predictions over a horizon of up to 60s. We leverage Large Language Models (LLMs) to predict interactions with the environment by conditioning the LLM prediction on rich contextual information about the scene. This information is given as a 3D Dynamic Scene Graph that encodes the geometry, semantics, and traversability of the environment into a hierarchical representation. We then ground these interaction sequences into multi-modal spatio-temporal distributions over human positions using a probabilistic approach based on continuous-time Markov Chains. To evaluate our approach, we introduce a new semi-synthetic dataset of long-term human trajectories in complex indoor environments, which also includes annotations of human-object interactions. We show in thorough experimental evaluations that our approach achieves a 54% lower average negative log-likelihood (NLL) and a 26.5% lower Best-of-20 displacement error compared to the best non-privileged baselines for a time horizon of 60s.","sentences":["We present a novel approach for long-term human trajectory prediction, which is essential for long-horizon robot planning in human-populated environments.","State-of-the-art human trajectory prediction methods are limited by their focus on collision avoidance and short-term planning, and their inability to model complex interactions of humans with the environment.","In contrast, our approach overcomes these limitations by predicting sequences of human interactions with the environment and using this information to guide trajectory predictions over a horizon of up to 60s.","We leverage Large Language Models (LLMs) to predict interactions with the environment by conditioning the LLM prediction on rich contextual information about the scene.","This information is given as a 3D Dynamic Scene Graph that encodes the geometry, semantics, and traversability of the environment into a hierarchical representation.","We then ground these interaction sequences into multi-modal spatio-temporal distributions over human positions using a probabilistic approach based on continuous-time Markov Chains.","To evaluate our approach, we introduce a new semi-synthetic dataset of long-term human trajectories in complex indoor environments, which also includes annotations of human-object interactions.","We show in thorough experimental evaluations that our approach achieves a 54% lower average negative log-likelihood (NLL) and a 26.5% lower Best-of-20 displacement error compared to the best non-privileged baselines for a time horizon of 60s."],"url":"http://arxiv.org/abs/2405.00552v1","category":"cs.RO"}
{"created":"2024-05-01 14:43:20","title":"Digital-analog quantum convolutional neural networks for image classification","abstract":"We propose digital-analog quantum kernels for enhancing the detection of complex features in the classification of images. We consider multipartite-entangled analog blocks, stemming from native Ising interactions in neutral-atom quantum processors, and individual operations as digital steps to implement the protocol. To further improving the detection of complex features, we apply multiple quantum kernels by varying the qubit connectivity according to the hardware constraints. An architecture that combines non-trainable quantum kernels and standard convolutional neural networks is used to classify realistic medical images, from breast cancer and pneumonia diseases, with a significantly reduced number of parameters. Despite this fact, the model exhibits better performance than its classical counterparts and achieves comparable metrics according to public benchmarks. These findings demonstrate the relevance of digital-analog encoding, paving the way for surpassing classical models in image recognition approaching us to quantum-advantage regimes.","sentences":["We propose digital-analog quantum kernels for enhancing the detection of complex features in the classification of images.","We consider multipartite-entangled analog blocks, stemming from native Ising interactions in neutral-atom quantum processors, and individual operations as digital steps to implement the protocol.","To further improving the detection of complex features, we apply multiple quantum kernels by varying the qubit connectivity according to the hardware constraints.","An architecture that combines non-trainable quantum kernels and standard convolutional neural networks is used to classify realistic medical images, from breast cancer and pneumonia diseases, with a significantly reduced number of parameters.","Despite this fact, the model exhibits better performance than its classical counterparts and achieves comparable metrics according to public benchmarks.","These findings demonstrate the relevance of digital-analog encoding, paving the way for surpassing classical models in image recognition approaching us to quantum-advantage regimes."],"url":"http://arxiv.org/abs/2405.00548v1","category":"quant-ph"}
{"created":"2024-05-01 14:27:42","title":"New Trends on the Systems Approach to Modeling SARS-CoV-2 Pandemics in a Globally Connected Planet","abstract":"This paper presents a critical analysis of the literature and perspective research ideas for modeling the epidemics caused by the SARS-CoV-2 virus. It goes beyond deterministic population dynamics to consider several key complexity features of the system under consideration. In particular, the multiscale features of the dynamics from contagion to the subsequent dynamics of competition between the immune system and the proliferating virus. Other topics addressed in this work include the propagation of epidemics in a territory, taking into account local transportation networks, the heterogeneity of the population, and the study of social and economic problems in populations involved in the spread of epidemics. The overall content aims to show how new mathematical tools can be developed to address the above topics and how mathematical models and simulations can contribute to the decision making of crisis managers.","sentences":["This paper presents a critical analysis of the literature and perspective research ideas for modeling the epidemics caused by the SARS-CoV-2 virus.","It goes beyond deterministic population dynamics to consider several key complexity features of the system under consideration.","In particular, the multiscale features of the dynamics from contagion to the subsequent dynamics of competition between the immune system and the proliferating virus.","Other topics addressed in this work include the propagation of epidemics in a territory, taking into account local transportation networks, the heterogeneity of the population, and the study of social and economic problems in populations involved in the spread of epidemics.","The overall content aims to show how new mathematical tools can be developed to address the above topics and how mathematical models and simulations can contribute to the decision making of crisis managers."],"url":"http://arxiv.org/abs/2405.00541v1","category":"q-bio.PE"}
{"created":"2024-05-01 14:19:30","title":"Quantifying Price Improvement in Order Flow Auctions","abstract":"This work introduces a framework for evaluating onchain order flow auctions (OFAs), emphasizing the metric of price improvement. Utilizing a set of open-source tools, our methodology systematically attributes price improvements to specific modifiable inputs of the system such as routing efficiency, gas optimization, and priority fee settings. When applied to leading Ethereum-based trading interfaces such as 1Inch and Uniswap, the results reveal that auction-enhanced interfaces can provide statistically significant improvements in trading outcomes, averaging 4-5 basis points in our sample. We further identify the sources of such price improvements to be added liquidity for large swaps. This research lays a foundation for future innovations in blockchain based trading platforms.","sentences":["This work introduces a framework for evaluating onchain order flow auctions (OFAs), emphasizing the metric of price improvement.","Utilizing a set of open-source tools, our methodology systematically attributes price improvements to specific modifiable inputs of the system such as routing efficiency, gas optimization, and priority fee settings.","When applied to leading Ethereum-based trading interfaces such as 1Inch and Uniswap, the results reveal that auction-enhanced interfaces can provide statistically significant improvements in trading outcomes, averaging 4-5 basis points in our sample.","We further identify the sources of such price improvements to be added liquidity for large swaps.","This research lays a foundation for future innovations in blockchain based trading platforms."],"url":"http://arxiv.org/abs/2405.00537v1","category":"q-fin.TR"}
{"created":"2024-05-01 14:06:11","title":"On the damping of spin waves","abstract":"We show that, in ideal-spin hydrodynamics, the components of the spin tensor follow damped wave equations. The damping rate is related to nonlocal collisions of the particles in the fluid, which enter at first order in $\\hbar$ in a semi-classical expansion. This rate provides an estimate for the timescale of spin equilibration and is computed by considering a system of spin-1/2 fermions subject to a quartic self-interaction. It is found that the relaxation times of the components of the spin tensor can become very large compared to the usual dissipative timescales of the system. Our results suggest that the spin degrees of freedom in a heavy-ion collision may not be in equilibrium by the time of freeze-out, and thus should be treated dynamically.","sentences":["We show that, in ideal-spin hydrodynamics, the components of the spin tensor follow damped wave equations.","The damping rate is related to nonlocal collisions of the particles in the fluid, which enter at first order in $\\hbar$ in a semi-classical expansion.","This rate provides an estimate for the timescale of spin equilibration and is computed by considering a system of spin-1/2 fermions subject to a quartic self-interaction.","It is found that the relaxation times of the components of the spin tensor can become very large compared to the usual dissipative timescales of the system.","Our results suggest that the spin degrees of freedom in a heavy-ion collision may not be in equilibrium by the time of freeze-out, and thus should be treated dynamically."],"url":"http://arxiv.org/abs/2405.00533v1","category":"nucl-th"}
{"created":"2024-05-01 14:02:08","title":"The Highly Durable Antibacterial Gel-like Coatings for Textiles","abstract":"Hospital-acquired infections are considered a priority for public health systems, which poses a significant burden for society. High-touch surfaces of healthcare centers, including textiles, provide a suitable environment for pathogenic bacteria to grow, necessitating incorporating effective antibacterial agents into textiles. This paper introduces a highly durable antibacterial gel-like solution, Silver Shell finish, which contains chitosan-bound silver chloride microparticles. The study investigates the coating's environmental impact, health risks, and durability during repeated washing. The structure of the Silver Shell finish was studied using Transmission Electron Microscopy (TEM) and Energy-Dispersive X-ray Spectroscopy (EDX). TEM images showed a core-shell structure, with chitosan forming a protective shell around groupings of silver micro-particles. Field Emission Scanning Electron Microscopy (FESEM) demonstrated the uniform deposition of Silver Shell on the surface of fabrics. AATCC Test Method 100 was employed to quantitatively analyze the antibacterial properties of fabrics coated with silver microparticles. Two types of bacteria, Staphylococcus aureus (S. aureus) and Escherichia coli (E. coli) were used in this study. The antibacterial results showed that after 75 wash cycles, a 100% reduction for both S. aureus and E. coli in the coated samples using crosslinking agents was observed. The coated samples without a crosslinking agent exhibited a 99.88% and 99.81% reduction for S. aureus and E. coli after 50 washing cycles. AATCC-147 was performed to investigate the coated samples' leaching properties and the crosslinking agent's effect against S. aureus and E. coli. All coated samples demonstrated remarkable antibacterial efficacy even after 75 wash cycles.","sentences":["Hospital-acquired infections are considered a priority for public health systems, which poses a significant burden for society.","High-touch surfaces of healthcare centers, including textiles, provide a suitable environment for pathogenic bacteria to grow, necessitating incorporating effective antibacterial agents into textiles.","This paper introduces a highly durable antibacterial gel-like solution, Silver Shell finish, which contains chitosan-bound silver chloride microparticles.","The study investigates the coating's environmental impact, health risks, and durability during repeated washing.","The structure of the Silver Shell finish was studied using Transmission Electron Microscopy (TEM) and Energy-Dispersive X-ray Spectroscopy (EDX).","TEM images showed a core-shell structure, with chitosan forming a protective shell around groupings of silver micro-particles.","Field Emission Scanning Electron Microscopy (FESEM) demonstrated the uniform deposition of Silver Shell on the surface of fabrics.","AATCC Test Method 100 was employed to quantitatively analyze the antibacterial properties of fabrics coated with silver microparticles.","Two types of bacteria, Staphylococcus aureus (S. aureus) and Escherichia coli (E. coli) were used in this study.","The antibacterial results showed that after 75 wash cycles, a 100% reduction for both S. aureus and E. coli in the coated samples using crosslinking agents was observed.","The coated samples without a crosslinking agent exhibited a 99.88% and 99.81% reduction for S. aureus and E. coli after 50 washing cycles.","AATCC-147 was performed to investigate the coated samples' leaching properties and the crosslinking agent's effect against S. aureus and E. coli.","All coated samples demonstrated remarkable antibacterial efficacy even after 75 wash cycles."],"url":"http://arxiv.org/abs/2405.00530v1","category":"q-bio.TO"}
{"created":"2024-05-01 14:01:16","title":"JNI Global References Are Still Vulnerable: Attacks and Defenses","abstract":"System services and resources in Android are accessed through IPC based mechanisms. Previous research has demonstrated that they are vulnerable to the denial-of-service attack (DoS attack). For instance, the JNI global reference (JGR), which is widely used by system services, can be exhausted to cause the system reboot (hence the name JGRE attack). Even though the Android team tries to fix the problem by enforcing security checks, we find that it is still possible to construct a JGR exhaustion DoS attack in the latest Android system.   In this paper, we propose a new JGR exhaustion DoS attack, which is effective in different Android versions, including the latest one (i.e., Android 10). Specifically, we developed JGREAnalyzer, a tool that can systematically detect JGR vulnerable services APIs via a call graph analysis and a forwarding reachability analysis. We applied this tool to different Android versions and found multiple vulnerabilities. In particular, among 148 system services in Android 10, 12 of them have 21 vulnerabilities. Among them, 9 can be successfully exploited without any permissions. We further analyze the root cause of the vulnerabilities and propose a new defense to mitigate the JGRE attack by restricting resource consumption via global reference counting.","sentences":["System services and resources in Android are accessed through IPC based mechanisms.","Previous research has demonstrated that they are vulnerable to the denial-of-service attack (DoS attack).","For instance, the JNI global reference (JGR), which is widely used by system services, can be exhausted to cause the system reboot (hence the name JGRE attack).","Even though the Android team tries to fix the problem by enforcing security checks, we find that it is still possible to construct a JGR exhaustion DoS attack in the latest Android system.   ","In this paper, we propose a new JGR exhaustion DoS attack, which is effective in different Android versions, including the latest one (i.e., Android 10).","Specifically, we developed JGREAnalyzer, a tool that can systematically detect JGR vulnerable services APIs via a call graph analysis and a forwarding reachability analysis.","We applied this tool to different Android versions and found multiple vulnerabilities.","In particular, among 148 system services in Android 10, 12 of them have 21 vulnerabilities.","Among them, 9 can be successfully exploited without any permissions.","We further analyze the root cause of the vulnerabilities and propose a new defense to mitigate the JGRE attack by restricting resource consumption via global reference counting."],"url":"http://arxiv.org/abs/2405.00526v1","category":"cs.CR"}
{"created":"2024-05-01 13:55:38","title":"Graph-Based Multivariate Multiscale Dispersion Entropy: Efficient Implementation and Applications to Real-World Network Data","abstract":"We introduce Multivariate Multiscale Graph-based Dispersion Entropy (mvDEG), a novel, computationally efficient method for analyzing multivariate time series data in graph and complex network frameworks, and demonstrate its application in real-world data. mvDEG effectively combines temporal dynamics with topological relationships, offering enhanced analysis compared to traditional nonlinear entropy methods. Its efficacy is established through testing on synthetic signals, such as uncorrelated and correlated noise, showcasing its adeptness in discerning various levels of dependency and complexity.   The robustness of mvDEG is further validated with real-world datasets, effectively differentiating various two-phase flow regimes and capturing distinct dynamics in weather data analysis. An important advancement of mvDEG is its computational efficiency. Our optimized algorithm displays a computational time that grows linearly with the number of vertices or nodes, in contrast to the exponential growth observed in classical methods. This efficiency is achieved through refined matrix power calculations that exploit matrix and Kronecker product properties, making our method faster than the state of the art. The significant acceleration in computational time positions mvDEG as a transformative tool for extensive and real-time applications, setting a new benchmark in the analysis of time series recorded at distributed locations and opening avenues for innovative applications.","sentences":["We introduce Multivariate Multiscale Graph-based Dispersion Entropy (mvDEG), a novel, computationally efficient method for analyzing multivariate time series data in graph and complex network frameworks, and demonstrate its application in real-world data.","mvDEG effectively combines temporal dynamics with topological relationships, offering enhanced analysis compared to traditional nonlinear entropy methods.","Its efficacy is established through testing on synthetic signals, such as uncorrelated and correlated noise, showcasing its adeptness in discerning various levels of dependency and complexity.   ","The robustness of mvDEG is further validated with real-world datasets, effectively differentiating various two-phase flow regimes and capturing distinct dynamics in weather data analysis.","An important advancement of mvDEG is its computational efficiency.","Our optimized algorithm displays a computational time that grows linearly with the number of vertices or nodes, in contrast to the exponential growth observed in classical methods.","This efficiency is achieved through refined matrix power calculations that exploit matrix and Kronecker product properties, making our method faster than the state of the art.","The significant acceleration in computational time positions mvDEG as a transformative tool for extensive and real-time applications, setting a new benchmark in the analysis of time series recorded at distributed locations and opening avenues for innovative applications."],"url":"http://arxiv.org/abs/2405.00518v1","category":"math.CO"}
{"created":"2024-05-01 13:45:59","title":"Quantum rates in dissipative systems with spatially varying friction","abstract":"We investigate whether making the friction spatially dependent introduces quantum effects into the thermal reaction rates for dissipative reactions. We calculate the quantum rates using the numerically exact multi-configuration time-dependent Hartree (MCTDH) method, as well as the approximate ring-polymer molecular dynamics (RPMD), ring-polymer instanton (RPI) methods, and classical mechanics. By conducting simulations across a wide range of temperatures and friction strengths, we can identify the various regimes that govern the reactive dynamics. At high temperatures, in addition to the spatial-diffusion and energy-diffusion regimes predicted by Kramer's rate theory, a (coherent) tunnelling-dominated regime is identified at low friction. At low temperatures, incoherent tunnelling dominates most of Kramer's curve, except at very low friction when coherent tunnelling becomes dominant. Unlike in classical mechanics, the bath's influence changes the equilibrium time-independent properties of the system, leading to a complex interplay between spatially dependent friction and nuclear quantum effects even at high temperatures. More specifically, we show that a realistic friction profile can lead to an increase (decrease) of the quantum (classical) rates with friction within the spatial-diffusion regime, showing that classical and quantum rates display qualitatively different behaviours. Except at very low frictions, we find that RPMD captures most of the quantum effects in the thermal reaction rates.","sentences":["We investigate whether making the friction spatially dependent introduces quantum effects into the thermal reaction rates for dissipative reactions.","We calculate the quantum rates using the numerically exact multi-configuration time-dependent Hartree (MCTDH) method, as well as the approximate ring-polymer molecular dynamics (RPMD), ring-polymer instanton (RPI) methods, and classical mechanics.","By conducting simulations across a wide range of temperatures and friction strengths, we can identify the various regimes that govern the reactive dynamics.","At high temperatures, in addition to the spatial-diffusion and energy-diffusion regimes predicted by Kramer's rate theory, a (coherent) tunnelling-dominated regime is identified at low friction.","At low temperatures, incoherent tunnelling dominates most of Kramer's curve, except at very low friction when coherent tunnelling becomes dominant.","Unlike in classical mechanics, the bath's influence changes the equilibrium time-independent properties of the system, leading to a complex interplay between spatially dependent friction and nuclear quantum effects even at high temperatures.","More specifically, we show that a realistic friction profile can lead to an increase (decrease) of the quantum (classical) rates with friction within the spatial-diffusion regime, showing that classical and quantum rates display qualitatively different behaviours.","Except at very low frictions, we find that RPMD captures most of the quantum effects in the thermal reaction rates."],"url":"http://arxiv.org/abs/2405.00512v1","category":"physics.chem-ph"}
{"created":"2024-05-01 13:37:47","title":"Significant tuning of internal mode coupling in doubly clamped MEMS beam resonators by thermal effect","abstract":"Intermodal coupling has been demonstrated to be a promising mechanism for the development of advanced micro/nanoelectromechanical devices. However, strong mode coupling remains a key challenge limiting the practical application of intermodal coupling. Furthermore, the insight into physical mechanisms underlying mode coupling and the capability to quantitatively tune the mode coupling is also limited. Here, we experimentally and theoretically demonstrate the significant tunability of mode coupling by using the thermal tuning effect, yet in an asymmetric doubly-clamped MEMS beam resonator, enabling various coupling strength to be implemented for practical applications. In this system, two out-of-plane vibrational modes are mechanically coupled through displacement-induced tension, and their mode coupling strength arises from both hardening and softening nonlinearities of the two modes, thus allowing for the tuning of mode coupling strength by thermally enhancing the softening nonlinearity of the MEMS beam. Our results demonstrate a feasible approach to tune the mode coupling and offer insights into fundamental mechanism of mode coupling in MEMS beam resonators, paving the way for the development of MEMS resonators with enhanced performance and application-specific tunability.","sentences":["Intermodal coupling has been demonstrated to be a promising mechanism for the development of advanced micro/nanoelectromechanical devices.","However, strong mode coupling remains a key challenge limiting the practical application of intermodal coupling.","Furthermore, the insight into physical mechanisms underlying mode coupling and the capability to quantitatively tune the mode coupling is also limited.","Here, we experimentally and theoretically demonstrate the significant tunability of mode coupling by using the thermal tuning effect, yet in an asymmetric doubly-clamped MEMS beam resonator, enabling various coupling strength to be implemented for practical applications.","In this system, two out-of-plane vibrational modes are mechanically coupled through displacement-induced tension, and their mode coupling strength arises from both hardening and softening nonlinearities of the two modes, thus allowing for the tuning of mode coupling strength by thermally enhancing the softening nonlinearity of the MEMS beam.","Our results demonstrate a feasible approach to tune the mode coupling and offer insights into fundamental mechanism of mode coupling in MEMS beam resonators, paving the way for the development of MEMS resonators with enhanced performance and application-specific tunability."],"url":"http://arxiv.org/abs/2405.00506v1","category":"physics.app-ph"}
{"created":"2024-05-01 13:37:27","title":"KVP10k : A Comprehensive Dataset for Key-Value Pair Extraction in Business Documents","abstract":"In recent years, the challenge of extracting information from business documents has emerged as a critical task, finding applications across numerous domains. This effort has attracted substantial interest from both industry and academy, highlighting its significance in the current technological landscape. Most datasets in this area are primarily focused on Key Information Extraction (KIE), where the extraction process revolves around extracting information using a specific, predefined set of keys. Unlike most existing datasets and benchmarks, our focus is on discovering key-value pairs (KVPs) without relying on predefined keys, navigating through an array of diverse templates and complex layouts. This task presents unique challenges, primarily due to the absence of comprehensive datasets and benchmarks tailored for non-predetermined KVP extraction. To address this gap, we introduce KVP10k , a new dataset and benchmark specifically designed for KVP extraction. The dataset contains 10707 richly annotated images. In our benchmark, we also introduce a new challenging task that combines elements of KIE as well as KVP in a single task. KVP10k sets itself apart with its extensive diversity in data and richly detailed annotations, paving the way for advancements in the field of information extraction from complex business documents.","sentences":["In recent years, the challenge of extracting information from business documents has emerged as a critical task, finding applications across numerous domains.","This effort has attracted substantial interest from both industry and academy, highlighting its significance in the current technological landscape.","Most datasets in this area are primarily focused on Key Information Extraction (KIE), where the extraction process revolves around extracting information using a specific, predefined set of keys.","Unlike most existing datasets and benchmarks, our focus is on discovering key-value pairs (KVPs) without relying on predefined keys, navigating through an array of diverse templates and complex layouts.","This task presents unique challenges, primarily due to the absence of comprehensive datasets and benchmarks tailored for non-predetermined KVP extraction.","To address this gap, we introduce KVP10k , a new dataset and benchmark specifically designed for KVP extraction.","The dataset contains 10707 richly annotated images.","In our benchmark, we also introduce a new challenging task that combines elements of KIE as well as KVP in a single task.","KVP10k sets itself apart with its extensive diversity in data and richly detailed annotations, paving the way for advancements in the field of information extraction from complex business documents."],"url":"http://arxiv.org/abs/2405.00505v1","category":"cs.IR"}
{"created":"2024-05-01 12:52:58","title":"Recoverable strain from reverse plasticity","abstract":"Recoverable strain is the strain recovered once a stress is removed from a body, in the direction opposite to that in which the stress had acted. To date, the phenomenon has been understood as being elastic in origin: polymer chains stretched in the direction of an imposed stress will recoil after the stress is removed, for example. Any unrecoverable strain is instead attributed to irreversible plastic deformations. Here we study theoretically recoverable and unrecoverable strain within the soft glassy rheology model, aimed at describing the rheology of elastoplastic yield stress fluids and amorphous soft solids. We consider a material subject to the switch-on of a shear stress that is held constant before later being set back to zero, after which the strain recovery is observed. After an initially fast recoil that is indeed elastic in nature, consistent with the usual intuition, we find that significant subsequent strain recovery then arises not via further recoil of elasticity but instead by `reverse plasticity', in a sense that we discuss carefully. Also unexpectedly, although in rare parameter regimes, this plastic part of the strain post switch-off does not always in fact recover in the negative direction, counter to that of the previously imposed stress, but can sometimes continue to accumulate in the forward direction. The recovery is then non-monotonic overall, reminiscent of recent observations of non-monotonic stress relaxation after straining, which have been attributed to complex material memory effects. We elucidate the mechanisms underlying these behaviours in terms of the evolution of the SGR model's population of elastoplastic elements, and suggest connections with the notion of `reversible plasticity', as discussed in the recent physics literature.","sentences":["Recoverable strain is the strain recovered once a stress is removed from a body, in the direction opposite to that in which the stress had acted.","To date, the phenomenon has been understood as being elastic in origin: polymer chains stretched in the direction of an imposed stress will recoil after the stress is removed, for example.","Any unrecoverable strain is instead attributed to irreversible plastic deformations.","Here we study theoretically recoverable and unrecoverable strain within the soft glassy rheology model, aimed at describing the rheology of elastoplastic yield stress fluids and amorphous soft solids.","We consider a material subject to the switch-on of a shear stress that is held constant before later being set back to zero, after which the strain recovery is observed.","After an initially fast recoil that is indeed elastic in nature, consistent with the usual intuition, we find that significant subsequent strain recovery then arises not via further recoil of elasticity but instead by `reverse plasticity', in a sense that we discuss carefully.","Also unexpectedly, although in rare parameter regimes, this plastic part of the strain post switch-off does not always in fact recover in the negative direction, counter to that of the previously imposed stress, but can sometimes continue to accumulate in the forward direction.","The recovery is then non-monotonic overall, reminiscent of recent observations of non-monotonic stress relaxation after straining, which have been attributed to complex material memory effects.","We elucidate the mechanisms underlying these behaviours in terms of the evolution of the SGR model's population of elastoplastic elements, and suggest connections with the notion of `reversible plasticity', as discussed in the recent physics literature."],"url":"http://arxiv.org/abs/2405.00487v1","category":"cond-mat.soft"}
{"created":"2024-05-01 12:50:32","title":"Time and frequency domain low order, low frequency approximation of mechanical systems","abstract":"Control design for linear, time-invariant mechanical systems typically requires an accurate low-order approximation in the low frequency range. For example a series expansion of the transfer function around zero consisting of a mass, velocity, and compliance term. Because computing such a series expansion of the transfer function can be cumbersome, a new method to compute low-order approximations of mechanical systems is developed in this paper. The method does not require an explicit expression for the transfer function, which is not always available for infinite-dimensional systems. The advantages of the proposed method is demonstrated in three examples.","sentences":["Control design for linear, time-invariant mechanical systems typically requires an accurate low-order approximation in the low frequency range.","For example a series expansion of the transfer function around zero consisting of a mass, velocity, and compliance term.","Because computing such a series expansion of the transfer function can be cumbersome, a new method to compute low-order approximations of mechanical systems is developed in this paper.","The method does not require an explicit expression for the transfer function, which is not always available for infinite-dimensional systems.","The advantages of the proposed method is demonstrated in three examples."],"url":"http://arxiv.org/abs/2405.00486v1","category":"math.OC"}
{"created":"2024-05-01 12:46:57","title":"PackVFL: Efficient HE Packing for Vertical Federated Learning","abstract":"As an essential tool of secure distributed machine learning, vertical federated learning (VFL) based on homomorphic encryption (HE) suffers from severe efficiency problems due to data inflation and time-consuming operations. To this core, we propose PackVFL, an efficient VFL framework based on packed HE (PackedHE), to accelerate the existing HE-based VFL algorithms. PackVFL packs multiple cleartexts into one ciphertext and supports single-instruction-multiple-data (SIMD)-style parallelism. We focus on designing a high-performant matrix multiplication (MatMult) method since it takes up most of the ciphertext computation time in HE-based VFL. Besides, devising the MatMult method is also challenging for PackedHE because a slight difference in the packing way could predominantly affect its computation and communication costs. Without domain-specific design, directly applying SOTA MatMult methods is hard to achieve optimal.   Therefore, we make a three-fold design: 1) we systematically explore the current design space of MatMult and quantify the complexity of existing approaches to provide guidance; 2) we propose a hybrid MatMult method according to the unique characteristics of VFL; 3) we adaptively apply our hybrid method in representative VFL algorithms, leveraging distinctive algorithmic properties to further improve efficiency. As the batch size, feature dimension and model size of VFL scale up to large sizes, PackVFL consistently delivers enhanced performance. Empirically, PackVFL propels existing VFL algorithms to new heights, achieving up to a 51.52X end-to-end speedup. This represents a substantial 34.51X greater speedup compared to the direct application of SOTA MatMult methods.","sentences":["As an essential tool of secure distributed machine learning, vertical federated learning (VFL) based on homomorphic encryption (HE) suffers from severe efficiency problems due to data inflation and time-consuming operations.","To this core, we propose PackVFL, an efficient VFL framework based on packed HE (PackedHE), to accelerate the existing HE-based VFL algorithms.","PackVFL packs multiple cleartexts into one ciphertext and supports single-instruction-multiple-data (SIMD)-style parallelism.","We focus on designing a high-performant matrix multiplication (MatMult) method since it takes up most of the ciphertext computation time in HE-based VFL.","Besides, devising the MatMult method is also challenging for PackedHE because a slight difference in the packing way could predominantly affect its computation and communication costs.","Without domain-specific design, directly applying SOTA MatMult methods is hard to achieve optimal.   ","Therefore, we make a three-fold design: 1) we systematically explore the current design space of MatMult and quantify the complexity of existing approaches to provide guidance; 2) we propose a hybrid MatMult method according to the unique characteristics of VFL; 3) we adaptively apply our hybrid method in representative VFL algorithms, leveraging distinctive algorithmic properties to further improve efficiency.","As the batch size, feature dimension and model size of VFL scale up to large sizes, PackVFL consistently delivers enhanced performance.","Empirically, PackVFL propels existing VFL algorithms to new heights, achieving up to a 51.52X end-to-end speedup.","This represents a substantial 34.51X greater speedup compared to the direct application of SOTA MatMult methods."],"url":"http://arxiv.org/abs/2405.00482v1","category":"cs.CR"}
{"created":"2024-05-01 12:39:35","title":"Enhanced Visual Question Answering: A Comparative Analysis and Textual Feature Extraction Via Convolutions","abstract":"Visual Question Answering (VQA) has emerged as a highly engaging field in recent years, attracting increasing research efforts aiming to enhance VQA accuracy through the deployment of advanced models such as Transformers. Despite this growing interest, there has been limited exploration into the comparative analysis and impact of textual modalities within VQA, particularly in terms of model complexity and its effect on performance. In this work, we conduct a comprehensive comparison between complex textual models that leverage long dependency mechanisms and simpler models focusing on local textual features within a well-established VQA framework. Our findings reveal that employing complex textual encoders is not invariably the optimal approach for the VQA-v2 dataset. Motivated by this insight, we introduce an improved model, ConvGRU, which incorporates convolutional layers to enhance the representation of question text. Tested on the VQA-v2 dataset, ConvGRU achieves better performance without substantially increasing parameter complexity.","sentences":["Visual Question Answering (VQA) has emerged as a highly engaging field in recent years, attracting increasing research efforts aiming to enhance VQA accuracy through the deployment of advanced models such as Transformers.","Despite this growing interest, there has been limited exploration into the comparative analysis and impact of textual modalities within VQA, particularly in terms of model complexity and its effect on performance.","In this work, we conduct a comprehensive comparison between complex textual models that leverage long dependency mechanisms and simpler models focusing on local textual features within a well-established VQA framework.","Our findings reveal that employing complex textual encoders is not invariably the optimal approach for the VQA-v2 dataset.","Motivated by this insight, we introduce an improved model, ConvGRU, which incorporates convolutional layers to enhance the representation of question text.","Tested on the VQA-v2 dataset, ConvGRU achieves better performance without substantially increasing parameter complexity."],"url":"http://arxiv.org/abs/2405.00479v1","category":"cs.CV"}
{"created":"2024-05-01 12:28:36","title":"Beyond the random phase approximation for calculating Curie temperatures in ferromagnets: application to Fe, Ni, Co and monolayer CrI3","abstract":"The magnetic properties of solids are typically analyzed in terms of Heisenberg models where the electronic structure is approximated by interacting localized spins. However, even in such models the evaluation of thermodynamic properties constitutes a major challenge and is usually handled by a mean field decoupling scheme. The random phase approximation (RPA) comprises a common approach and is often applied to evaluate critical temperatures although it is well known that the method is only accurate well below the critical temperature. In the present work we compare the performance of the RPA with a different decoupling scheme proposed by Callen as well as the mean field decoupling of interacting Holstein-Primakoff (HP) magnons. We consider three-dimensional (3D) as well as two-dimensional (2D) model systems where the Curie temperature is governed by anisotropy. In 3D, the Callen method is the most accurate in the classical limit, and we show that the Callen decoupling produces the best agreement with experiments for bcc Fe, fcc Ni and fcc Co with exchange interactions obtained from first principles. In contrast, for low spin systems where a quantum mechanical treatment in pertinent, the HP and RPA methods appear are superior to the Callen decoupling. In 2D systems with magnetic order driven by single-ion anisotropy, it is shown that HP fails rather dramatically and both RPA and Callen approaches severely overestimates Curie temperatures. The most accurate approach is then constructed by combining RPA with the Callen decoupling of single-ion anisotropy, which yields the correct lack of order for S=1/2. We exemplify this by the case of monolayer CrI3 using exchange constant extracted from experiments.","sentences":["The magnetic properties of solids are typically analyzed in terms of Heisenberg models where the electronic structure is approximated by interacting localized spins.","However, even in such models the evaluation of thermodynamic properties constitutes a major challenge and is usually handled by a mean field decoupling scheme.","The random phase approximation (RPA) comprises a common approach and is often applied to evaluate critical temperatures although it is well known that the method is only accurate well below the critical temperature.","In the present work we compare the performance of the RPA with a different decoupling scheme proposed by Callen as well as the mean field decoupling of interacting Holstein-Primakoff (HP) magnons.","We consider three-dimensional (3D) as well as two-dimensional (2D) model systems where the Curie temperature is governed by anisotropy.","In 3D, the Callen method is the most accurate in the classical limit, and we show that the Callen decoupling produces the best agreement with experiments for bcc Fe, fcc Ni and fcc Co with exchange interactions obtained from first principles.","In contrast, for low spin systems where a quantum mechanical treatment in pertinent, the HP and RPA methods appear are superior to the Callen decoupling.","In 2D systems with magnetic order driven by single-ion anisotropy, it is shown that HP fails rather dramatically and both RPA and Callen approaches severely overestimates Curie temperatures.","The most accurate approach is then constructed by combining RPA with the Callen decoupling of single-ion anisotropy, which yields the correct lack of order for S=1/2.","We exemplify this by the case of monolayer CrI3 using exchange constant extracted from experiments."],"url":"http://arxiv.org/abs/2405.00477v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-01 12:15:58","title":"DmADs-Net: Dense multiscale attention and depth-supervised network for medical image segmentation","abstract":"Deep learning has made important contributions to the development of medical image segmentation. Convolutional neural networks, as a crucial branch, have attracted strong attention from researchers. Through the tireless efforts of numerous researchers, convolutional neural networks have yielded numerous outstanding algorithms for processing medical images. The ideas and architectures of these algorithms have also provided important inspiration for the development of later technologies.Through extensive experimentation, we have found that currently mainstream deep learning algorithms are not always able to achieve ideal results when processing complex datasets and different types of datasets. These networks still have room for improvement in lesion localization and feature extraction. Therefore, we have created the Dense Multiscale Attention and Depth-Supervised Network (DmADs-Net).We use ResNet for feature extraction at different depths and create a Multi-scale Convolutional Feature Attention Block to improve the network's attention to weak feature information. The Local Feature Attention Block is created to enable enhanced local feature attention for high-level semantic information. In addition, in the feature fusion phase, a Feature Refinement and Fusion Block is created to enhance the fusion of different semantic information.We validated the performance of the network using five datasets of varying sizes and types. Results from comparative experiments show that DmADs-Net outperformed mainstream networks. Ablation experiments further demonstrated the effectiveness of the created modules and the rationality of the network architecture.","sentences":["Deep learning has made important contributions to the development of medical image segmentation.","Convolutional neural networks, as a crucial branch, have attracted strong attention from researchers.","Through the tireless efforts of numerous researchers, convolutional neural networks have yielded numerous outstanding algorithms for processing medical images.","The ideas and architectures of these algorithms have also provided important inspiration for the development of later technologies.","Through extensive experimentation, we have found that currently mainstream deep learning algorithms are not always able to achieve ideal results when processing complex datasets and different types of datasets.","These networks still have room for improvement in lesion localization and feature extraction.","Therefore, we have created the Dense Multiscale Attention and Depth-Supervised Network (DmADs-Net).We use ResNet for feature extraction at different depths and create a Multi-scale Convolutional Feature Attention Block to improve the network's attention to weak feature information.","The Local Feature Attention Block is created to enable enhanced local feature attention for high-level semantic information.","In addition, in the feature fusion phase, a Feature Refinement and Fusion Block is created to enhance the fusion of different semantic information.","We validated the performance of the network using five datasets of varying sizes and types.","Results from comparative experiments show that DmADs-Net outperformed mainstream networks.","Ablation experiments further demonstrated the effectiveness of the created modules and the rationality of the network architecture."],"url":"http://arxiv.org/abs/2405.00472v1","category":"eess.IV"}
{"created":"2024-05-01 11:55:50","title":"On the class of systems which are disjoint from every ergodic system","abstract":"In this note we give a fairly direct proof of a recent theorem of Gorska, Lemanczyk and de la Rue which characterises the class of measure preserving transformations that are disjoint from every ergodic measure preserving transformation. Our proof works just as well for any countable acting group.","sentences":["In this note we give a fairly direct proof of a recent theorem of Gorska, Lemanczyk and de la Rue which characterises the class of measure preserving transformations that are disjoint from every ergodic measure preserving transformation.","Our proof works just as well for any countable acting group."],"url":"http://arxiv.org/abs/2405.00463v1","category":"math.DS"}
{"created":"2024-05-01 11:03:11","title":"A Modelling Framework for Energy-Management and Eco-Driving Problems using Convex Relaxations","abstract":"This paper presents a convex optimization framework for eco-driving and vehicle energy management problems. We will first show that several types of eco-driving and vehicle energy management problems can be modelled using the same notions of energy storage buffers and energy storage converters that are connected to a power network. It will be shown that these problems can be formulated as optimization problems with linear cost functions and linear dynamics, and nonlinear constraints representing the power converters. We will show that under some mild conditions, the (non-convex) optimization problem has the same (globally) optimal solution as a convex relaxation. This means that the problems can be solved efficiently and that the solution is guaranteed to be globally optimal. Finally, a numerical example of the eco-driving problem is used to illustrate this claim.","sentences":["This paper presents a convex optimization framework for eco-driving and vehicle energy management problems.","We will first show that several types of eco-driving and vehicle energy management problems can be modelled using the same notions of energy storage buffers and energy storage converters that are connected to a power network.","It will be shown that these problems can be formulated as optimization problems with linear cost functions and linear dynamics, and nonlinear constraints representing the power converters.","We will show that under some mild conditions, the (non-convex) optimization problem has the same (globally) optimal solution as a convex relaxation.","This means that the problems can be solved efficiently and that the solution is guaranteed to be globally optimal.","Finally, a numerical example of the eco-driving problem is used to illustrate this claim."],"url":"http://arxiv.org/abs/2405.00447v1","category":"math.OC"}
{"created":"2024-05-01 10:48:23","title":"Modeling Linear and Non-linear Layers: An MILP Approach Towards Finding Differential and Impossible Differential Propagations","abstract":"Symmetric key cryptography stands as a fundamental cornerstone in ensuring security within contemporary electronic communication frameworks. The cryptanalysis of classical symmetric key ciphers involves traditional methods and techniques aimed at breaking or analyzing these cryptographic systems. In the evaluation of new ciphers, the resistance against linear and differential cryptanalysis is commonly a key design criterion. The wide trail design technique for block ciphers facilitates the demonstration of security against linear and differential cryptanalysis. Assessing the scheme's security against differential attacks often involves determining the minimum number of active SBoxes for all rounds of a cipher. The propagation characteristics of a cryptographic component, such as an SBox, can be expressed using Boolean functions. Mixed Integer Linear Programming (MILP) proves to be a valuable technique for solving Boolean functions. We formulate a set of inequalities to model a Boolean function, which is subsequently solved by an MILP solver. To efficiently model a Boolean function and select a minimal set of inequalities, two key challenges must be addressed. We propose algorithms to address the second challenge, aiming to find more optimized linear and non-linear components. Our approaches are applied to modeling SBoxes (up to six bits) and EXOR operations with any number of inputs. Additionally, we introduce an MILP-based automatic tool for exploring differential and impossible differential propagations within a cipher. The tool is successfully applied to five lightweight block ciphers: Lilliput, GIFT64, SKINNY64, Klein, and MIBS.","sentences":["Symmetric key cryptography stands as a fundamental cornerstone in ensuring security within contemporary electronic communication frameworks.","The cryptanalysis of classical symmetric key ciphers involves traditional methods and techniques aimed at breaking or analyzing these cryptographic systems.","In the evaluation of new ciphers, the resistance against linear and differential cryptanalysis is commonly a key design criterion.","The wide trail design technique for block ciphers facilitates the demonstration of security against linear and differential cryptanalysis.","Assessing the scheme's security against differential attacks often involves determining the minimum number of active SBoxes for all rounds of a cipher.","The propagation characteristics of a cryptographic component, such as an SBox, can be expressed using Boolean functions.","Mixed Integer Linear Programming (MILP) proves to be a valuable technique for solving Boolean functions.","We formulate a set of inequalities to model a Boolean function, which is subsequently solved by an MILP solver.","To efficiently model a Boolean function and select a minimal set of inequalities, two key challenges must be addressed.","We propose algorithms to address the second challenge, aiming to find more optimized linear and non-linear components.","Our approaches are applied to modeling SBoxes (up to six bits) and EXOR operations with any number of inputs.","Additionally, we introduce an MILP-based automatic tool for exploring differential and impossible differential propagations within a cipher.","The tool is successfully applied to five lightweight block ciphers: Lilliput, GIFT64, SKINNY64, Klein, and MIBS."],"url":"http://arxiv.org/abs/2405.00441v1","category":"cs.CR"}
{"created":"2024-05-01 10:48:02","title":"Intersection Types via Finite-Set Declarations","abstract":"The lambda-cube is a famous pure type system (PTS) cube of eight powerful explicit type systems that include the simple, polymorphic and dependent type theories. The lambda-cube only types Strongly Normalising (SN) terms but not all of them. It is well known that even the most powerful system of the lambda-cube can only type the same pure untyped lambda-terms that are typable by the higher-order polymorphic implicitly typed lambda-calculus Fomega, and that there is an untyped {\\lambda}-term U' that is SN but is not typable in Fomega or the lambda-cube. Hence, neither system can type all the SN terms it expresses. In this paper, we present the f-cube, an extension of the lambda-cube with finite-set declarations (FSDs) like y\\in{C1,...,Cn} : B which means that y is of type B and can only be one of C1,..., Cn. The novelty of our FSDs is that they allow to represent intersection types as Pi-types. We show how to translate and type the term U' in the f-cube using an encoding of intersection types based on FSDs. Notably, our translation works without needing anything like the usual troublesome intersection-introduction rule that proves a pure untyped lambda-term M has an intersection of k types using k independent sub-derivations. As such, our approach is useful for language implementers who want the power of intersection types without the pain of the intersection-introduction rule.","sentences":["The lambda-cube is a famous pure type system (PTS) cube of eight powerful explicit type systems that include the simple, polymorphic and dependent type theories.","The lambda-cube only types Strongly Normalising (SN) terms but not all of them.","It is well known that even the most powerful system of the lambda-cube can only type the same pure untyped lambda-terms that are typable by the higher-order polymorphic implicitly typed lambda-calculus Fomega, and that there is an untyped {\\lambda}-term U' that is SN but is not typable in Fomega or the lambda-cube.","Hence, neither system can type all the SN terms it expresses.","In this paper, we present the f-cube, an extension of the lambda-cube with finite-set declarations (FSDs) like y\\in{C1,...,Cn} : B which means that y is of type B and can only be one of C1,..., Cn.","The novelty of our FSDs is that they allow to represent intersection types as Pi-types.","We show how to translate and type the term U' in the f-cube using an encoding of intersection types based on FSDs.","Notably, our translation works without needing anything like the usual troublesome intersection-introduction rule that proves a pure untyped lambda-term M has an intersection of k types using k independent sub-derivations.","As such, our approach is useful for language implementers who want the power of intersection types without the pain of the intersection-introduction rule."],"url":"http://arxiv.org/abs/2405.00440v1","category":"cs.LO"}
{"created":"2024-05-01 10:45:01","title":"Fractional domain wall statistics in spin chains with anomalous symmetries","abstract":"We study the statistics of domain wall excitations in quantum spin chains. We focus on systems with finite symmetry groups represented by matrix product unitaries (MPUs), i.e. finite depth quantum circuits. Such symmetries can be anomalous, in which case gapped phases which they support must break the symmetry. The lowest lying excitations of those systems are thus domain wall excitations. We investigate the behavior of these domain walls under exchange, and find that they can exhibit non-trivial exchange statistics. This statistics is completely determined by the anomaly of the symmetry, and we provide a direct relation between the known classification of MPU symmetry actions on ground states and the domain wall statistics. Already for the simplest case of a $\\mathbb Z_2$ symmetry, we obtain that the presence of an anomalous MPU symmetry gives rise to domain wall excitations which behave neither as bosons nor as fermions, but rather exhibit fractional statistics. Finally, we show that the exchange statistics of domain walls is a physically accessible quantity, by devising explicit measurement operators through which it can be determined.","sentences":["We study the statistics of domain wall excitations in quantum spin chains.","We focus on systems with finite symmetry groups represented by matrix product unitaries (MPUs), i.e. finite depth quantum circuits.","Such symmetries can be anomalous, in which case gapped phases which they support must break the symmetry.","The lowest lying excitations of those systems are thus domain wall excitations.","We investigate the behavior of these domain walls under exchange, and find that they can exhibit non-trivial exchange statistics.","This statistics is completely determined by the anomaly of the symmetry, and we provide a direct relation between the known classification of MPU symmetry actions on ground states and the domain wall statistics.","Already for the simplest case of a $\\mathbb Z_2$ symmetry, we obtain that the presence of an anomalous MPU symmetry gives rise to domain wall excitations which behave neither as bosons nor as fermions, but rather exhibit fractional statistics.","Finally, we show that the exchange statistics of domain walls is a physically accessible quantity, by devising explicit measurement operators through which it can be determined."],"url":"http://arxiv.org/abs/2405.00439v1","category":"cond-mat.str-el"}
{"created":"2024-05-01 10:42:09","title":"Reduced-order modeling for second-order computational homogenization with applications to geometrically parameterized elastomeric metamaterials","abstract":"The structural properties of mechanical metamaterials are typically studied with two-scale methods based on computational homogenization. Because such materials have a complex microstructure, enriched schemes such as second-order computational homogenization are required to fully capture their non-linear behavior, which arises from non-local interactions due to the buckling or patterning of the microstructure. In the two-scale formulation, the effective behavior of the microstructure is captured with a representative volume element (RVE), and a homogenized effective continuum is considered on the macroscale.   Although an effective continuum formulation is introduced, solving such two-scale models concurrently is still computationally demanding due to the many repeated solutions for each RVE at the microscale level. In this work, we propose a reduced-order model for the microscopic problem arising in second-order computational homogenization, using proper orthogonal decomposition and a novel hyperreduction method that is specifically tailored for this problem and inspired by the empirical cubature method. Two numerical examples are considered, in which the performance of the reduced-order model is carefully assessed by comparing its solutions with direct numerical simulations (entirely resolving the underlying microstructure) and the full second-order computational homogenization model. The reduced-order model is able to approximate the result of the full computational homogenization well, provided that the training data is representative for the problem at hand. Any remaining errors, when compared with the direct numerical simulation, can be attributed to the inherent approximation errors in the computational homogenization scheme. Regarding run times for one thread, speed-ups on the order of 100 are achieved with the reduced-order model as compared to direct numerical simulations.","sentences":["The structural properties of mechanical metamaterials are typically studied with two-scale methods based on computational homogenization.","Because such materials have a complex microstructure, enriched schemes such as second-order computational homogenization are required to fully capture their non-linear behavior, which arises from non-local interactions due to the buckling or patterning of the microstructure.","In the two-scale formulation, the effective behavior of the microstructure is captured with a representative volume element (RVE), and a homogenized effective continuum is considered on the macroscale.   ","Although an effective continuum formulation is introduced, solving such two-scale models concurrently is still computationally demanding due to the many repeated solutions for each RVE at the microscale level.","In this work, we propose a reduced-order model for the microscopic problem arising in second-order computational homogenization, using proper orthogonal decomposition and a novel hyperreduction method that is specifically tailored for this problem and inspired by the empirical cubature method.","Two numerical examples are considered, in which the performance of the reduced-order model is carefully assessed by comparing its solutions with direct numerical simulations (entirely resolving the underlying microstructure) and the full second-order computational homogenization model.","The reduced-order model is able to approximate the result of the full computational homogenization well, provided that the training data is representative for the problem at hand.","Any remaining errors, when compared with the direct numerical simulation, can be attributed to the inherent approximation errors in the computational homogenization scheme.","Regarding run times for one thread, speed-ups on the order of 100 are achieved with the reduced-order model as compared to direct numerical simulations."],"url":"http://arxiv.org/abs/2405.00437v1","category":"cs.CE"}
{"created":"2024-05-01 10:35:08","title":"CultiVerse: Towards Cross-Cultural Understanding for Paintings with Large Language Model","abstract":"The integration of new technology with cultural studies enhances our understanding of cultural heritage but often struggles to connect with diverse audiences. It is challenging to align personal interpretations with the intended meanings across different cultures. Our study investigates the important factors in appreciating art from a cross-cultural perspective. We explore the application of Large Language Models (LLMs) to bridge the cultural and language barriers in understanding Traditional Chinese Paintings (TCPs). We present CultiVerse, a visual analytics system that utilizes LLMs within a mixed-initiative framework, enhancing interpretative appreciation of TCP in a cross-cultural dialogue. CultiVerse addresses the challenge of translating the nuanced symbolism in art, which involves interpreting complex cultural contexts, aligning cross-cultural symbols, and validating cultural acceptance. CultiVerse integrates an interactive interface with the analytical capability of LLMs to explore a curated TCP dataset, facilitating the analysis of multifaceted symbolic meanings and the exploration of cross-cultural serendipitous discoveries. Empirical evaluations affirm that CultiVerse significantly improves cross-cultural understanding, offering deeper insights and engaging art appreciation.","sentences":["The integration of new technology with cultural studies enhances our understanding of cultural heritage but often struggles to connect with diverse audiences.","It is challenging to align personal interpretations with the intended meanings across different cultures.","Our study investigates the important factors in appreciating art from a cross-cultural perspective.","We explore the application of Large Language Models (LLMs) to bridge the cultural and language barriers in understanding Traditional Chinese Paintings (TCPs).","We present CultiVerse, a visual analytics system that utilizes LLMs within a mixed-initiative framework, enhancing interpretative appreciation of TCP in a cross-cultural dialogue.","CultiVerse addresses the challenge of translating the nuanced symbolism in art, which involves interpreting complex cultural contexts, aligning cross-cultural symbols, and validating cultural acceptance.","CultiVerse integrates an interactive interface with the analytical capability of LLMs to explore a curated TCP dataset, facilitating the analysis of multifaceted symbolic meanings and the exploration of cross-cultural serendipitous discoveries.","Empirical evaluations affirm that CultiVerse significantly improves cross-cultural understanding, offering deeper insights and engaging art appreciation."],"url":"http://arxiv.org/abs/2405.00435v1","category":"cs.HC"}
{"created":"2024-05-01 10:18:31","title":"CC2Vec: Combining Typed Tokens with Contrastive Learning for Effective Code Clone Detection","abstract":"With the development of the open source community, the code is often copied, spread, and evolved in multiple software systems, which brings uncertainty and risk to the software system (e.g., bug propagation and copyright infringement). Therefore, it is important to conduct code clone detection to discover similar code pairs. Many approaches have been proposed to detect code clones where token-based tools can scale to big code. However, due to the lack of program details, they cannot handle more complicated code clones, i.e., semantic code clones. In this paper, we introduce CC2Vec, a novel code encoding method designed to swiftly identify simple code clones while also enhancing the capability for semantic code clone detection. To retain the program details between tokens, CC2Vec divides them into different categories (i.e., typed tokens) according to the syntactic types and then applies two self-attention mechanism layers to encode them. To resist changes in the code structure of semantic code clones, CC2Vec performs contrastive learning to reduce the differences introduced by different code implementations. We evaluate CC2Vec on two widely used datasets (i.e., BigCloneBench and Google Code Jam) and the results report that our method can effectively detect simple code clones. In addition, CC2Vec not only attains comparable performance to widely used semantic code clone detection systems such as ASTNN, SCDetector, and FCCA by simply fine-tuning, but also significantly surpasses these methods in both detection efficiency.","sentences":["With the development of the open source community, the code is often copied, spread, and evolved in multiple software systems, which brings uncertainty and risk to the software system (e.g., bug propagation and copyright infringement).","Therefore, it is important to conduct code clone detection to discover similar code pairs.","Many approaches have been proposed to detect code clones where token-based tools can scale to big code.","However, due to the lack of program details, they cannot handle more complicated code clones, i.e., semantic code clones.","In this paper, we introduce CC2Vec, a novel code encoding method designed to swiftly identify simple code clones while also enhancing the capability for semantic code clone detection.","To retain the program details between tokens, CC2Vec divides them into different categories (i.e., typed tokens) according to the syntactic types and then applies two self-attention mechanism layers to encode them.","To resist changes in the code structure of semantic code clones, CC2Vec performs contrastive learning to reduce the differences introduced by different code implementations.","We evaluate CC2Vec on two widely used datasets (i.e., BigCloneBench and Google Code Jam) and the results report that our method can effectively detect simple code clones.","In addition, CC2Vec not only attains comparable performance to widely used semantic code clone detection systems such as ASTNN, SCDetector, and FCCA by simply fine-tuning, but also significantly surpasses these methods in both detection efficiency."],"url":"http://arxiv.org/abs/2405.00428v1","category":"cs.SE"}
{"created":"2024-05-01 10:15:45","title":"Quantum Monte Carlo study of the phase diagram of the two-dimensional uniform electron liquid","abstract":"We present a study of spin-unpolarized and spin-polarized two-dimensional uniform electron liquids using variational and diffusion quantum Monte Carlo (VMC and DMC) methods with Slater-Jastrow-backflow trial wave functions. Ground-state VMC and DMC energies are obtained in the density range $1 \\leq r_\\text{s} \\leq 40$. Single-particle and many-body finite-size errors are corrected using canonical-ensemble twist-averaged boundary conditions and extrapolation of twist-averaged energies to the thermodynamic limit of infinite system size. System-size-dependent errors in Slater-Jastrow-backflow DMC energies caused by partially converged VMC energy minimization calculations are discussed. We find that, for $1 \\leq r_\\text{s} \\leq 5$, optimizing the backflow function at each twist lowers the twist-averaged DMC energy at finite system size. However, nonsystematic system-size-dependent effects remain in the DMC energies, which can be partially removed by extrapolation from multiple finite system sizes to infinite system size. We attribute these nonsystematic effects to the close competition between fluid and defected crystal phases at different system sizes at low density. The DMC energies in the thermodynamic limit are used to parameterize a local spin density approximation correlation functional for inhomogeneous electron systems. Our zero-temperature phase diagram shows a single transition from a paramagnetic fluid to a hexagonal Wigner crystal at $r_\\text{s}=35(1)$, with no region of stability for a ferromagnetic fluid.","sentences":["We present a study of spin-unpolarized and spin-polarized two-dimensional uniform electron liquids using variational and diffusion quantum Monte Carlo (VMC and DMC) methods with Slater-Jastrow-backflow trial wave functions.","Ground-state VMC and DMC energies are obtained in the density range $1 \\leq r_\\text{s} \\leq 40$.","Single-particle and many-body finite-size errors are corrected using canonical-ensemble twist-averaged boundary conditions and extrapolation of twist-averaged energies to the thermodynamic limit of infinite system size.","System-size-dependent errors in Slater-Jastrow-backflow DMC energies caused by partially converged VMC energy minimization calculations are discussed.","We find that, for $1 \\leq r_\\text{s} \\leq 5$, optimizing the backflow function at each twist lowers the twist-averaged DMC energy at finite system size.","However, nonsystematic system-size-dependent effects remain in the DMC energies, which can be partially removed by extrapolation from multiple finite system sizes to infinite system size.","We attribute these nonsystematic effects to the close competition between fluid and defected crystal phases at different system sizes at low density.","The DMC energies in the thermodynamic limit are used to parameterize a local spin density approximation correlation functional for inhomogeneous electron systems.","Our zero-temperature phase diagram shows a single transition from a paramagnetic fluid to a hexagonal Wigner crystal at $r_\\text{s}=35(1)$, with no region of stability for a ferromagnetic fluid."],"url":"http://arxiv.org/abs/2405.00425v1","category":"cond-mat.str-el"}
{"created":"2024-05-01 09:52:39","title":"Entanglement and fidelity across quantum phase transitions in locally perturbed topological codes with open boundaries","abstract":"We investigate the topological-to-non-topological quantum phase transitions (QPTs) occurring in the Kitaev code under local perturbations in the form of local magnetic field and spin-spin interactions of the Ising-type using fidelity susceptibility (FS) and entanglement as the probes. We assume the code to be embedded on the surface of a wide cylinder of height $M$ and circumference $D$ with $M\\ll D$. We demonstrate a power-law divergence of FS across the QPT, and determine the quantum critical points (QCPs) via a finite-size scaling analysis. We verify these results by mapping the perturbed Kitaev code to the 2D Ising model with nearest- and next-nearest-neighbor interactions, and computing the single-site magnetization as order parameter using quantum Monte-Carlo technique. We also point out an odd-even dichotomy in the occurrence of the QPT in the Kitaev ladder with respect to the odd and even values of $D$, when the system is perturbed with only Ising interaction. Our results also indicate a higher robustness of the topological phase of the Kitaev code against local perturbations if the boundary is made open along one direction. We further consider a local entanglement witness operator designed specifically to capture a lower bound to the localizable entanglement on the vertical non-trivial loop of the code. We show that the first derivative of the expectation value of the witness operator exhibits a logarithmic divergence across the QPT, and perform the finite-size scaling analysis. We demonstrate similar behaviour of the expectation value of the appropriately constructed witness operator also in the case of locally perturbed color code with open boundaries.","sentences":["We investigate the topological-to-non-topological quantum phase transitions (QPTs) occurring in the Kitaev code under local perturbations in the form of local magnetic field and spin-spin interactions of the Ising-type using fidelity susceptibility (FS) and entanglement as the probes.","We assume the code to be embedded on the surface of a wide cylinder of height $M$ and circumference $D$ with $M\\ll D$. We demonstrate a power-law divergence of FS across the QPT, and determine the quantum critical points (QCPs) via a finite-size scaling analysis.","We verify these results by mapping the perturbed Kitaev code to the 2D Ising model with nearest- and next-nearest-neighbor interactions, and computing the single-site magnetization as order parameter using quantum Monte-Carlo technique.","We also point out an odd-even dichotomy in the occurrence of the QPT in the Kitaev ladder with respect to the odd and even values of $D$, when the system is perturbed with only Ising interaction.","Our results also indicate a higher robustness of the topological phase of the Kitaev code against local perturbations if the boundary is made open along one direction.","We further consider a local entanglement witness operator designed specifically to capture a lower bound to the localizable entanglement on the vertical non-trivial loop of the code.","We show that the first derivative of the expectation value of the witness operator exhibits a logarithmic divergence across the QPT, and perform the finite-size scaling analysis.","We demonstrate similar behaviour of the expectation value of the appropriately constructed witness operator also in the case of locally perturbed color code with open boundaries."],"url":"http://arxiv.org/abs/2405.00416v1","category":"quant-ph"}
{"created":"2024-05-01 09:49:03","title":"Direct detection of down-converted photons spontaneously produced at a single Josephson junction","abstract":"We study spontaneous photon decay into multiple photons triggered by strong non-linearities in a superconducting quantum simulator of the boundary sine-Gordon impurity model. Previously, spectroscopic signatures of photon-conversion were reported and evidenced as resonances in the many-body spectrum of these systems. Here, we report on the observation of multi-mode fluorescence of a small Josephson junction embedded in a high impedance superconducting transmission line. Measurement of the down-converted photons is achieved using state-of-the-art broadband parametric amplifiers. Photon triplet emission is explicitly demonstrated at a given frequency as the counterpart of inelastic photon decay at three-times the emission frequency. These results open exciting prospects for the burgeoning field of many-body quantum optics and offer a direct signature of the ultra-strong light-matter coupling.","sentences":["We study spontaneous photon decay into multiple photons triggered by strong non-linearities in a superconducting quantum simulator of the boundary sine-Gordon impurity model.","Previously, spectroscopic signatures of photon-conversion were reported and evidenced as resonances in the many-body spectrum of these systems.","Here, we report on the observation of multi-mode fluorescence of a small Josephson junction embedded in a high impedance superconducting transmission line.","Measurement of the down-converted photons is achieved using state-of-the-art broadband parametric amplifiers.","Photon triplet emission is explicitly demonstrated at a given frequency as the counterpart of inelastic photon decay at three-times the emission frequency.","These results open exciting prospects for the burgeoning field of many-body quantum optics and offer a direct signature of the ultra-strong light-matter coupling."],"url":"http://arxiv.org/abs/2405.00411v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-01 09:25:54","title":"Low-temperature thermal transport in moir\u00e9 superlattices","abstract":"We calculate the thermal conductivity of various moir\\'e bilayer systems using a continuum approach and the semiclassical transport theory. When the twist angle is close to 0, we observe a significant reduction of thermal conductivity in a particular low-temperature regime. This reduction is attributed to a moir\\'e-induced reconstruction of acoustic phonon bands and associated decrease of the group velocity. Conversely, in the zero temperature limit, the thermal conductivity is enhanced by moir\\'e effect, surpassing the original values in non-moir\\'e counterparts. These changes result in a characteristic temperature dependence which deviates from the quadratic behavior in intrinsic two-dimensional systems.","sentences":["We calculate the thermal conductivity of various moir\\'e bilayer systems using a continuum approach and the semiclassical transport theory.","When the twist angle is close to 0, we observe a significant reduction of thermal conductivity in a particular low-temperature regime.","This reduction is attributed to a moir\\'e-induced reconstruction of acoustic phonon bands and associated decrease of the group velocity.","Conversely, in the zero temperature limit, the thermal conductivity is enhanced by moir\\'e effect, surpassing the original values in non-moir\\'e counterparts.","These changes result in a characteristic temperature dependence which deviates from the quadratic behavior in intrinsic two-dimensional systems."],"url":"http://arxiv.org/abs/2405.00406v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-01 08:46:36","title":"Inferring State Machine from the Protocol Implementation via Large Langeuage Model","abstract":"State machines play a pivotal role in augmenting the efficacy of protocol analyzing to unveil more vulnerabilities. However, the task of inferring state machines from network protocol implementations presents significant challenges. Traditional methods based on dynamic analysis often overlook crucial state transitions due to limited coverage, while static analysis faces difficulties with complex code structures and behaviors. To address these limitations, we propose an innovative state machine inference approach powered by Large Language Models (LLMs). Utilizing text-embedding technology, this method allows LLMs to dissect and analyze the intricacies of protocol implementation code. Through targeted prompt engineering, we systematically identify and infer the underlying state machines. Our evaluation across six protocol implementations demonstrates the method's high efficacy, achieving an accuracy rate exceeding 90% and successfully delineating differences on state machines among various implementations of the same protocol. Importantly, integrating this approach with protocol fuzzing has notably enhanced AFLNet's code coverage by 10% over RFCNLP, showcasing the considerable potential of LLMs in advancing network protocol security analysis. Our proposed method not only marks a significant step forward in accurate state machine inference but also opens new avenues for improving the security and reliability of protocol implementations.","sentences":["State machines play a pivotal role in augmenting the efficacy of protocol analyzing to unveil more vulnerabilities.","However, the task of inferring state machines from network protocol implementations presents significant challenges.","Traditional methods based on dynamic analysis often overlook crucial state transitions due to limited coverage, while static analysis faces difficulties with complex code structures and behaviors.","To address these limitations, we propose an innovative state machine inference approach powered by Large Language Models (LLMs).","Utilizing text-embedding technology, this method allows LLMs to dissect and analyze the intricacies of protocol implementation code.","Through targeted prompt engineering, we systematically identify and infer the underlying state machines.","Our evaluation across six protocol implementations demonstrates the method's high efficacy, achieving an accuracy rate exceeding 90% and successfully delineating differences on state machines among various implementations of the same protocol.","Importantly, integrating this approach with protocol fuzzing has notably enhanced AFLNet's code coverage by 10% over RFCNLP, showcasing the considerable potential of LLMs in advancing network protocol security analysis.","Our proposed method not only marks a significant step forward in accurate state machine inference but also opens new avenues for improving the security and reliability of protocol implementations."],"url":"http://arxiv.org/abs/2405.00393v1","category":"cs.CR"}
{"created":"2024-05-01 08:38:07","title":"Cell Switching in HAPS-Aided Networking: How the Obscurity of Traffic Loads Affects the Decision","abstract":"This study aims to introduce the cell load estimation problem of cell switching approaches in cellular networks specially-presented in a high-altitude platform station (HAPS)-assisted network. The problem arises from the fact that the traffic loads of sleeping base stations for the next time slot cannot be perfectly known, but they can rather be estimated, and any estimation error could result in divergence from the optimal decision, which subsequently affects the performance of energy efficiency. The traffic loads of the sleeping base stations for the next time slot are required because the switching decisions are made proactively in the current time slot. Two different Q-learning algorithms are developed; one is full-scale, focusing solely on the performance, while the other one is lightweight and addresses the computational cost. Results confirm that the estimation error is capable of changing cell switching decisions that yields performance divergence compared to no-error scenarios. Moreover, the developed Q-learning algorithms perform well since an insignificant difference (i.e., 0.3%) is observed between them and the optimum algorithm.","sentences":["This study aims to introduce the cell load estimation problem of cell switching approaches in cellular networks specially-presented in a high-altitude platform station (HAPS)-assisted network.","The problem arises from the fact that the traffic loads of sleeping base stations for the next time slot cannot be perfectly known, but they can rather be estimated, and any estimation error could result in divergence from the optimal decision, which subsequently affects the performance of energy efficiency.","The traffic loads of the sleeping base stations for the next time slot are required because the switching decisions are made proactively in the current time slot.","Two different Q-learning algorithms are developed; one is full-scale, focusing solely on the performance, while the other one is lightweight and addresses the computational cost.","Results confirm that the estimation error is capable of changing cell switching decisions that yields performance divergence compared to no-error scenarios.","Moreover, the developed Q-learning algorithms perform well since an insignificant difference (i.e., 0.3%) is observed between them and the optimum algorithm."],"url":"http://arxiv.org/abs/2405.00387v1","category":"cs.NI"}
{"created":"2024-05-01 08:36:31","title":"Large gap probabilities of complex and symplectic spherical ensembles with point charges","abstract":"We consider $n$ eigenvalues of complex and symplectic induced spherical ensembles, which can be realised as two-dimensional determinantal and Pfaffian Coulomb gases on the Riemann sphere under the insertion of point charges. For both cases, we show that the probability that there are no eigenvalues in a spherical cap around the poles has an asymptotic behaviour as $n\\to \\infty$ of the form $$ \\exp\\Big( c_1 n^2 + c_2 n\\log n + c_3 n + c_4 \\sqrt n + c_5 \\log n + c_6 + \\mathcal{O}(n^{-\\frac1{12}}) \\Big) $$ and determine the coefficients explicitly. Our results provide the second example of precise (up to and including the constant term) large gap asymptotic behaviours for two-dimensional point processes, following a recent breakthrough by Charlier.","sentences":["We consider $n$ eigenvalues of complex and symplectic induced spherical ensembles, which can be realised as two-dimensional determinantal and Pfaffian Coulomb gases on the Riemann sphere under the insertion of point charges.","For both cases, we show that the probability that there are no eigenvalues in a spherical cap around the poles has an asymptotic behaviour as $n\\to \\infty$ of the form $$ \\exp\\Big( c_1 n^2 + c_2 n\\log n + c_3 n + c_4 \\sqrt n + c_5 \\log n + c_6 + \\mathcal{O}(n^{-\\frac1{12}}) \\Big) $$ and determine the coefficients explicitly.","Our results provide the second example of precise (up to and including the constant term) large gap asymptotic behaviours for two-dimensional point processes, following a recent breakthrough by Charlier."],"url":"http://arxiv.org/abs/2405.00386v1","category":"math-ph"}
{"created":"2024-05-01 08:18:04","title":"Planar Hall Effect in Quasi-Two-Dimensional Materials","abstract":"The planar Hall effect in 3D systems is an effective probe for their Berry curvature, topology, and electronic properties. However, the Berry curvature-induced conventional planar Hall effect is forbidden in 2D systems as the out-of-plane Berry curvature cannot couple to the band velocity of the electrons moving in the 2D plane. Here, we demonstrate a unique 2D planar Hall effect (2DPHE) originating from the hidden planar components of the Berry curvature and orbital magnetic moment in quasi-2D materials. We identify all planar band geometric contributions to 2DPHE and classify their crystalline symmetry restrictions. Using gated bilayer graphene as an example, we show that in addition to capturing the hidden band geometric effects, 2DPHE is also sensitive to the Lifshitz transitions. Our work motivates further exploration of hidden planar band geometry-induced 2DPHE and related transport phenomena for innovative applications.","sentences":["The planar Hall effect in 3D systems is an effective probe for their Berry curvature, topology, and electronic properties.","However, the Berry curvature-induced conventional planar Hall effect is forbidden in 2D systems as the out-of-plane Berry curvature cannot couple to the band velocity of the electrons moving in the 2D plane.","Here, we demonstrate a unique 2D planar Hall effect (2DPHE) originating from the hidden planar components of the Berry curvature and orbital magnetic moment in quasi-2D materials.","We identify all planar band geometric contributions to 2DPHE and classify their crystalline symmetry restrictions.","Using gated bilayer graphene as an example, we show that in addition to capturing the hidden band geometric effects, 2DPHE is also sensitive to the Lifshitz transitions.","Our work motivates further exploration of hidden planar band geometry-induced 2DPHE and related transport phenomena for innovative applications."],"url":"http://arxiv.org/abs/2405.00379v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-01 08:09:04","title":"Thread review sentimental analysis with tkinter GUI & tableau dashboard","abstract":"This project focuses on utilizing a combination of Tkinter for GUI development and Tableauf for data visualization to do sentiment analysis on thread reviews.The main goal is to evaluate and visualize consumer sentiments as they are expressed in thread reviews in order to provide insights into areas for improvement, preferences, and customer satisfaction.The procedure starts with gathering thread reviews from many sources, which are then cleaned and prepared for analysis through preprocessing.Sentiment analysis classifies opinions as good, negative, or neutral based on the expressed sentiment by applying natural language processing techniques.The standard Python GUI package Tkinter is used to create an interactive user interface that allows users to enter thread reviews, start the sentiment analysis process, and see the analysis's outcomes.With the help of the user-friendly GUI, users may interact with the system and acquire insightful information with ease.Additionally, Tableau is used to produce a dynamic and eye-catching dashboard that displays the findings of the sentiment analysis using a variety of charts and graphs.Stakeholders may make educated decisions based on the studied data by using the dashboard, which provides a thorough overview of the sentiment distribution, frequency of positive and negative reviews, trending topics, and other pertinent indicators.Overall, this project offers a solid method for analyzing and comprehending customers' sentiments from thread reviews by integrating Tableauf for GUI development with Tkinter for sentiment analysis and data visualization. This allows for the creation of meaningful dashboards.","sentences":["This project focuses on utilizing a combination of Tkinter for GUI development and Tableauf for data visualization to do sentiment analysis on thread reviews.","The main goal is to evaluate and visualize consumer sentiments as they are expressed in thread reviews in order to provide insights into areas for improvement, preferences, and customer satisfaction.","The procedure starts with gathering thread reviews from many sources, which are then cleaned and prepared for analysis through preprocessing.","Sentiment analysis classifies opinions as good, negative, or neutral based on the expressed sentiment by applying natural language processing techniques.","The standard Python GUI package Tkinter is used to create an interactive user interface that allows users to enter thread reviews, start the sentiment analysis process, and see the analysis's outcomes.","With the help of the user-friendly GUI, users may interact with the system and acquire insightful information with ease.","Additionally, Tableau is used to produce a dynamic and eye-catching dashboard that displays the findings of the sentiment analysis using a variety of charts and graphs.","Stakeholders may make educated decisions based on the studied data by using the dashboard, which provides a thorough overview of the sentiment distribution, frequency of positive and negative reviews, trending topics, and other pertinent indicators.","Overall, this project offers a solid method for analyzing and comprehending customers' sentiments from thread reviews by integrating Tableauf for GUI development with Tkinter for sentiment analysis and data visualization.","This allows for the creation of meaningful dashboards."],"url":"http://arxiv.org/abs/2405.00377v1","category":"cs.GT"}
{"created":"2024-05-01 08:02:04","title":"An elliptic fibration arising from the Lagrange top and its monodromy","abstract":"This paper is to investigate an elliptic fibration over $\\mathbb{CP}^2$ arising from the Lagrange top from the viewpoint of complex algebraic geometry. The description of the discriminant locus of this elliptic fibration is given in detail. Moreover, the concrete description of the discriminant locus and the complete classification of singular fibres of the elliptic fibration are obtained according to Miranda's theory of elliptic threefolds after suitable modifications of the base and total spaces. Furthermore, the monodromy of the elliptic fibration is described.","sentences":["This paper is to investigate an elliptic fibration over $\\mathbb{CP}^2$ arising from the Lagrange top from the viewpoint of complex algebraic geometry.","The description of the discriminant locus of this elliptic fibration is given in detail.","Moreover, the concrete description of the discriminant locus and the complete classification of singular fibres of the elliptic fibration are obtained according to Miranda's theory of elliptic threefolds after suitable modifications of the base and total spaces.","Furthermore, the monodromy of the elliptic fibration is described."],"url":"http://arxiv.org/abs/2405.00373v1","category":"math-ph"}
{"created":"2024-05-01 08:01:41","title":"High-Precision Positioning with Continuous Delay and Doppler Shift using AFT-MC Waveforms","abstract":"This paper explores a novel integrated localization and communication (ILAC) system using the affine Fourier transform multicarrier (AFT-MC) waveform. Specifically, we consider a multiple-input multiple-output (MIMO) AFT-MC system with ILAC and derive a continuous delay and Doppler shift channel matrix model. Based on the derived signal model, we develop a two-step algorithm with low complexity for estimating channel parameters. Furthermore, we derive the Cram\\'er-Rao lower bound (CRLB) of location estimation as the fundamental limit of localization. Finally, we provide some insights about the AFT-MC parameters by explaining the impact of the parameters on localization performance. Simulation results demonstrate that the AFT-MC waveform is able to provide significant localization performance improvement compared to orthogonal frequency division multiplexing (OFDM) while achieving the CRLB of location estimation.","sentences":["This paper explores a novel integrated localization and communication (ILAC) system using the affine Fourier transform multicarrier (AFT-MC) waveform.","Specifically, we consider a multiple-input multiple-output (MIMO) AFT-MC system with ILAC and derive a continuous delay and Doppler shift channel matrix model.","Based on the derived signal model, we develop a two-step algorithm with low complexity for estimating channel parameters.","Furthermore, we derive the Cram\\'er-Rao lower bound (CRLB) of location estimation as the fundamental limit of localization.","Finally, we provide some insights about the AFT-MC parameters by explaining the impact of the parameters on localization performance.","Simulation results demonstrate that the AFT-MC waveform is able to provide significant localization performance improvement compared to orthogonal frequency division multiplexing (OFDM) while achieving the CRLB of location estimation."],"url":"http://arxiv.org/abs/2405.00372v1","category":"eess.SP"}
{"created":"2024-05-01 07:53:08","title":"Simple recursions displaying interesting evolutions","abstract":"Some simple nonlinear recursions which can be completely managed are identified and the behaviour of all their solutions is ascertained.","sentences":["Some simple nonlinear recursions which can be completely managed are identified and the behaviour of all their solutions is ascertained."],"url":"http://arxiv.org/abs/2405.00370v1","category":"nlin.SI"}
{"created":"2024-05-01 07:33:43","title":"AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts","abstract":"We introduce AdaMoLE, a novel method for fine-tuning large language models (LLMs) through an Adaptive Mixture of Low-Rank Adaptation (LoRA) Experts. Moving beyond conventional methods that employ a static top-k strategy for activating experts, AdaMoLE dynamically adjusts the activation threshold using a dedicated threshold network, adaptively responding to the varying complexities of different tasks. By replacing a single LoRA in a layer with multiple LoRA experts and integrating a gating function with the threshold mechanism, AdaMoLE effectively selects and activates the most appropriate experts based on the input context. Our extensive evaluations across a variety of commonsense reasoning and natural language processing tasks show that AdaMoLE exceeds baseline performance. This enhancement highlights the advantages of AdaMoLE's adaptive selection of LoRA experts, improving model effectiveness without a corresponding increase in the expert count. The experimental validation not only confirms AdaMoLE as a robust approach for enhancing LLMs but also suggests valuable directions for future research in adaptive expert selection mechanisms, potentially broadening the scope for optimizing model performance across diverse language processing tasks.","sentences":["We introduce AdaMoLE, a novel method for fine-tuning large language models (LLMs) through an Adaptive Mixture of Low-Rank Adaptation (LoRA) Experts.","Moving beyond conventional methods that employ a static top-k strategy for activating experts, AdaMoLE dynamically adjusts the activation threshold using a dedicated threshold network, adaptively responding to the varying complexities of different tasks.","By replacing a single LoRA in a layer with multiple LoRA experts and integrating a gating function with the threshold mechanism, AdaMoLE effectively selects and activates the most appropriate experts based on the input context.","Our extensive evaluations across a variety of commonsense reasoning and natural language processing tasks show that AdaMoLE exceeds baseline performance.","This enhancement highlights the advantages of AdaMoLE's adaptive selection of LoRA experts, improving model effectiveness without a corresponding increase in the expert count.","The experimental validation not only confirms AdaMoLE as a robust approach for enhancing LLMs but also suggests valuable directions for future research in adaptive expert selection mechanisms, potentially broadening the scope for optimizing model performance across diverse language processing tasks."],"url":"http://arxiv.org/abs/2405.00361v1","category":"cs.CL"}
{"created":"2024-05-01 06:38:40","title":"Glass clarified as the self-organizing system","abstract":"The synergetic approach proposed here is based on characteristic instability of chemical bonding in the form of the bond wave.The model explains characteristic hierarchical structure, including non-crystalline long-range order, the semi-deterministic behavior of properties, and a role of information fields for adaptation of glassy material to a current medium.","sentences":["The synergetic approach proposed here is based on characteristic instability of chemical bonding in the form of the bond wave.","The model explains characteristic hierarchical structure, including non-crystalline long-range order, the semi-deterministic behavior of properties, and a role of information fields for adaptation of glassy material to a current medium."],"url":"http://arxiv.org/abs/2405.00346v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-01 06:25:42","title":"Canonized then Minimized RMSD for Three-Dimensional Structures","abstract":"Existing molecular canonization algorithms typically operate on one-dimensional (1D) string representations or two-dimensional (2D) connectivity graphs of a molecule and are not able to differentiate equivalent atoms based on three-dimensional (3D) structures. The stereochemical tags on each atom are in fact determined according to established Cahn-Ingold-Prelog (CIP) rules for comparing grades, which can help to further differentiate atoms with similar environment. Therefore, a stereochemical-rule-based canonization algorithm that is capable of assigning canonical indices using 3D structural information is of great value. On top of the Schneider-Sayle-Landrum (SSL) partition-based canonization algorithm, we propose an enhanced canonization algorithm to expand its applicability. The initial index assignment rules are redesigned, so that the obtained canonical indices are compatible with the most of the common CIP Sequence Rules, which greatly eases the stereochemical assignment. Furthermore, a branching tiebreaking step is added to secure an accurate evaluation of the structural difference through the minimized root-mean-square deviation (RMSD) between structures, with an option to include hydrogen atoms or not. Our algorithm is implemented with Python and can efficiently obtain minimized RMSD taking into account of the symmetry of molecular systems , contributing to the fields of drug design, molecular docking, and data analysis of molecular dynamics simulation.","sentences":["Existing molecular canonization algorithms typically operate on one-dimensional (1D) string representations or two-dimensional (2D) connectivity graphs of a molecule and are not able to differentiate equivalent atoms based on three-dimensional (3D) structures.","The stereochemical tags on each atom are in fact determined according to established Cahn-Ingold-Prelog (CIP) rules for comparing grades, which can help to further differentiate atoms with similar environment.","Therefore, a stereochemical-rule-based canonization algorithm that is capable of assigning canonical indices using 3D structural information is of great value.","On top of the Schneider-Sayle-Landrum (SSL) partition-based canonization algorithm, we propose an enhanced canonization algorithm to expand its applicability.","The initial index assignment rules are redesigned, so that the obtained canonical indices are compatible with the most of the common CIP Sequence Rules, which greatly eases the stereochemical assignment.","Furthermore, a branching tiebreaking step is added to secure an accurate evaluation of the structural difference through the minimized root-mean-square deviation (RMSD) between structures, with an option to include hydrogen atoms or not.","Our algorithm is implemented with Python and can efficiently obtain minimized RMSD taking into account of the symmetry of molecular systems , contributing to the fields of drug design, molecular docking, and data analysis of molecular dynamics simulation."],"url":"http://arxiv.org/abs/2405.00339v1","category":"physics.chem-ph"}
{"created":"2024-05-01 05:19:49","title":"A Communication Avoiding and Reducing Algorithm for Symmetric Eigenproblem for Very Small Matrices","abstract":"In this paper, a parallel symmetric eigensolver with very small matrices in massively parallel processing is considered. We define very small matrices that fit the sizes of caches per node in a supercomputer. We assume that the sizes also fit the exa-scale computing requirements of current production runs of an application. To minimize communication time, we added several communication avoiding and communication reducing algorithms based on Message Passing Interface (MPI) non-blocking implementations. A performance evaluation with up to full nodes of the FX10 system indicates that (1) the MPI non-blocking implementation is 3x as efficient as the baseline implementation, (2) the hybrid MPI execution is 1.9x faster than the pure MPI execution, (3) our proposed solver is 2.3x and 22x faster than a ScaLAPACK routine with optimized blocking size and cyclic-cyclic distribution, respectively.","sentences":["In this paper, a parallel symmetric eigensolver with very small matrices in massively parallel processing is considered.","We define very small matrices that fit the sizes of caches per node in a supercomputer.","We assume that the sizes also fit the exa-scale computing requirements of current production runs of an application.","To minimize communication time, we added several communication avoiding and communication reducing algorithms based on Message Passing Interface (MPI) non-blocking implementations.","A performance evaluation with up to full nodes of the FX10 system indicates that (1) the MPI non-blocking implementation is 3x as efficient as the baseline implementation, (2) the hybrid MPI execution is 1.9x faster than the pure MPI execution, (3) our proposed solver is 2.3x and 22x faster than a ScaLAPACK routine with optimized blocking size and cyclic-cyclic distribution, respectively."],"url":"http://arxiv.org/abs/2405.00326v1","category":"cs.DC"}
{"created":"2024-05-01 05:15:26","title":"Stability analysis of a three-dimensional system of Topp model with diabetes","abstract":"Mathematical models of glucose, insulin, and pancreatic $\\beta$-cell mass dynamics are essential for understanding the physiological basis of type 2 diabetes. This paper investigates the Topp model's discrete-time dynamics to represent these interactions. We perform a comprehensive analysis of the system's trajectory, examining both local and global behavior. First, we establish the invariance of the positive trajectory and analyze the existence of fixed points. Then, we conduct a complete stability analysis, determining the local and global asymptotic stability of these fixed points. Finally, numerical examples validate the effectiveness and applicability of our theoretical findings. Additionally, we provide biological interpretations of our results.","sentences":["Mathematical models of glucose, insulin, and pancreatic $\\beta$-cell mass dynamics are essential for understanding the physiological basis of type 2 diabetes.","This paper investigates the Topp model's discrete-time dynamics to represent these interactions.","We perform a comprehensive analysis of the system's trajectory, examining both local and global behavior.","First, we establish the invariance of the positive trajectory and analyze the existence of fixed points.","Then, we conduct a complete stability analysis, determining the local and global asymptotic stability of these fixed points.","Finally, numerical examples validate the effectiveness and applicability of our theoretical findings.","Additionally, we provide biological interpretations of our results."],"url":"http://arxiv.org/abs/2405.00323v1","category":"math.DS"}
{"created":"2024-05-01 05:03:08","title":"DFKI-NLP at SemEval-2024 Task 2: Towards Robust LLMs Using Data Perturbations and MinMax Training","abstract":"The NLI4CT task at SemEval-2024 emphasizes the development of robust models for Natural Language Inference on Clinical Trial Reports (CTRs) using large language models (LLMs). This edition introduces interventions specifically targeting the numerical, vocabulary, and semantic aspects of CTRs. Our proposed system harnesses the capabilities of the state-of-the-art Mistral model, complemented by an auxiliary model, to focus on the intricate input space of the NLI4CT dataset. Through the incorporation of numerical and acronym-based perturbations to the data, we train a robust system capable of handling both semantic-altering and numerical contradiction interventions. Our analysis on the dataset sheds light on the challenging sections of the CTRs for reasoning.","sentences":["The NLI4CT task at SemEval-2024 emphasizes the development of robust models for Natural Language Inference on Clinical Trial Reports (CTRs) using large language models (LLMs).","This edition introduces interventions specifically targeting the numerical, vocabulary, and semantic aspects of CTRs.","Our proposed system harnesses the capabilities of the state-of-the-art Mistral model, complemented by an auxiliary model, to focus on the intricate input space of the NLI4CT dataset.","Through the incorporation of numerical and acronym-based perturbations to the data, we train a robust system capable of handling both semantic-altering and numerical contradiction interventions.","Our analysis on the dataset sheds light on the challenging sections of the CTRs for reasoning."],"url":"http://arxiv.org/abs/2405.00321v1","category":"cs.CL"}
{"created":"2024-05-01 04:57:25","title":"Web3 and the State: Indian state's redescription of blockchain","abstract":"The article does a close reading of a discussion paper by NITI Aayog and a strategy paper by the Ministry of Electronics and Information Technology (MeitY) advocating non-financial use cases of blockchain in India. By noting the discursive shift from transparency to trust that grounds these two documents and consequently Indian state's redescription of blockchain, the paper foregrounds how governance by infrastructure is at the heart of new forms of governance and how blockchain systems are being designated as decentral by states to have recentralizing effects. The papers highlight how a mapping of discursive shifts of notions such as trust, transparency, (de)centralization and (dis)intermediation can be a potent site to investigate redescriptions of emerging sociotechnical systems.","sentences":["The article does a close reading of a discussion paper by NITI Aayog and a strategy paper by the Ministry of Electronics and Information Technology (MeitY) advocating non-financial use cases of blockchain in India.","By noting the discursive shift from transparency to trust that grounds these two documents and consequently Indian state's redescription of blockchain, the paper foregrounds how governance by infrastructure is at the heart of new forms of governance and how blockchain systems are being designated as decentral by states to have recentralizing effects.","The papers highlight how a mapping of discursive shifts of notions such as trust, transparency, (de)centralization and (dis)intermediation can be a potent site to investigate redescriptions of emerging sociotechnical systems."],"url":"http://arxiv.org/abs/2405.00320v1","category":"cs.HC"}
{"created":"2024-05-01 04:51:10","title":"Covariant spatio-temporal receptive fields for neuromorphic computing","abstract":"Biological nervous systems constitute important sources of inspiration towards computers that are faster, cheaper, and more energy efficient. Neuromorphic disciplines view the brain as a coevolved system, simultaneously optimizing the hardware and the algorithms running on it. There are clear efficiency gains when bringing the computations into a physical substrate, but we presently lack theories to guide efficient implementations. Here, we present a principled computational model for neuromorphic systems in terms of spatio-temporal receptive fields, based on affine Gaussian kernels over space and leaky-integrator and leaky integrate-and-fire models over time. Our theory is provably covariant to spatial affine and temporal scaling transformations, and with close similarities to the visual processing in mammalian brains. We use these spatio-temporal receptive fields as a prior in an event-based vision task, and show that this improves the training of spiking networks, which otherwise is known as problematic for event-based vision. This work combines efforts within scale-space theory and computational neuroscience to identify theoretically well-founded ways to process spatio-temporal signals in neuromorphic systems. Our contributions are immediately relevant for signal processing and event-based vision, and can be extended to other processing tasks over space and time, such as memory and control.","sentences":["Biological nervous systems constitute important sources of inspiration towards computers that are faster, cheaper, and more energy efficient.","Neuromorphic disciplines view the brain as a coevolved system, simultaneously optimizing the hardware and the algorithms running on it.","There are clear efficiency gains when bringing the computations into a physical substrate, but we presently lack theories to guide efficient implementations.","Here, we present a principled computational model for neuromorphic systems in terms of spatio-temporal receptive fields, based on affine Gaussian kernels over space and leaky-integrator and leaky integrate-and-fire models over time.","Our theory is provably covariant to spatial affine and temporal scaling transformations, and with close similarities to the visual processing in mammalian brains.","We use these spatio-temporal receptive fields as a prior in an event-based vision task, and show that this improves the training of spiking networks, which otherwise is known as problematic for event-based vision.","This work combines efforts within scale-space theory and computational neuroscience to identify theoretically well-founded ways to process spatio-temporal signals in neuromorphic systems.","Our contributions are immediately relevant for signal processing and event-based vision, and can be extended to other processing tasks over space and time, such as memory and control."],"url":"http://arxiv.org/abs/2405.00318v1","category":"cs.NE"}
{"created":"2024-05-01 04:28:44","title":"Three-layer deep learning network random trees for fault diagnosis in chemical production process","abstract":"With the development of technology, the chemical production process is becoming increasingly complex and large-scale, making fault diagnosis particularly important. However, current diagnostic methods struggle to address the complexities of large-scale production processes. In this paper, we integrate the strengths of deep learning and machine learning technologies, combining the advantages of bidirectional long and short-term memory neural networks, fully connected neural networks, and the extra trees algorithm to propose a novel fault diagnostic model named three-layer deep learning network random trees (TDLN-trees). First, the deep learning component extracts temporal features from industrial data, combining and transforming them into a higher-level data representation. Second, the machine learning component processes and classifies the features extracted in the first step. An experimental analysis based on the Tennessee Eastman process verifies the superiority of the proposed method.","sentences":["With the development of technology, the chemical production process is becoming increasingly complex and large-scale, making fault diagnosis particularly important.","However, current diagnostic methods struggle to address the complexities of large-scale production processes.","In this paper, we integrate the strengths of deep learning and machine learning technologies, combining the advantages of bidirectional long and short-term memory neural networks, fully connected neural networks, and the extra trees algorithm to propose a novel fault diagnostic model named three-layer deep learning network random trees (TDLN-trees).","First, the deep learning component extracts temporal features from industrial data, combining and transforming them into a higher-level data representation.","Second, the machine learning component processes and classifies the features extracted in the first step.","An experimental analysis based on the Tennessee Eastman process verifies the superiority of the proposed method."],"url":"http://arxiv.org/abs/2405.00311v1","category":"cs.LG"}
{"created":"2024-05-01 04:00:09","title":"QUACK: Quantum Aligned Centroid Kernel","abstract":"Quantum computing (QC) seems to show potential for application in machine learning (ML). In particular quantum kernel methods (QKM) exhibit promising properties for use in supervised ML tasks. However, a major disadvantage of kernel methods is their unfavorable quadratic scaling with the number of training samples. Together with the limits imposed by currently available quantum hardware (NISQ devices) with their low qubit coherence times, small number of qubits, and high error rates, the use of QC in ML at an industrially relevant scale is currently impossible. As a small step in improving the potential applications of QKMs, we introduce QUACK, a quantum kernel algorithm whose time complexity scales linear with the number of samples during training, and independent of the number of training samples in the inference stage. In the training process, only the kernel entries for the samples and the centers of the classes are calculated, i.e. the maximum shape of the kernel for n samples and c classes is (n, c). During training, the parameters of the quantum kernel and the positions of the centroids are optimized iteratively. In the inference stage, for every new sample the circuit is only evaluated for every centroid, i.e. c times. We show that the QUACK algorithm nevertheless provides satisfactory results and can perform at a similar level as classical kernel methods with quadratic scaling during training. In addition, our (simulated) algorithm is able to handle high-dimensional datasets such as MNIST with 784 features without any dimensionality reduction.","sentences":["Quantum computing (QC) seems to show potential for application in machine learning (ML).","In particular quantum kernel methods (QKM) exhibit promising properties for use in supervised ML tasks.","However, a major disadvantage of kernel methods is their unfavorable quadratic scaling with the number of training samples.","Together with the limits imposed by currently available quantum hardware (NISQ devices) with their low qubit coherence times, small number of qubits, and high error rates, the use of QC in ML at an industrially relevant scale is currently impossible.","As a small step in improving the potential applications of QKMs, we introduce QUACK, a quantum kernel algorithm whose time complexity scales linear with the number of samples during training, and independent of the number of training samples in the inference stage.","In the training process, only the kernel entries for the samples and the centers of the classes are calculated, i.e. the maximum shape of the kernel for n samples and c classes is (n, c).","During training, the parameters of the quantum kernel and the positions of the centroids are optimized iteratively.","In the inference stage, for every new sample the circuit is only evaluated for every centroid, i.e. c times.","We show that the QUACK algorithm nevertheless provides satisfactory results and can perform at a similar level as classical kernel methods with quadratic scaling during training.","In addition, our (simulated) algorithm is able to handle high-dimensional datasets such as MNIST with 784 features without any dimensionality reduction."],"url":"http://arxiv.org/abs/2405.00304v1","category":"quant-ph"}
{"created":"2024-05-01 03:48:22","title":"The Reversing Machine: Reconstructing Memory Assumptions","abstract":"Existing anti-malware software and reverse engineering toolkits struggle with stealthy sub-OS rootkits due to limitations of run-time kernel-level monitoring. A malicious kernel-level driver can bypass OS-level anti-virus mechanisms easily. Although static analysis of such malware is possible, obfuscation and packing techniques complicate offline analysis. Moreover, current dynamic analyzers suffer from virtualization performance overhead and create detectable traces that allow modern malware to evade them.   To address these issues, we present \\textit{The Reversing Machine} (TRM), a new hypervisor-based memory introspection design for reverse engineering, reconstructing memory offsets, and fingerprinting evasive and obfuscated user-level and kernel-level malware. TRM proposes two novel techniques that enable efficient and transparent analysis of evasive malware: hooking a binary using suspended process creation for hypervisor-based memory introspection, and leveraging Mode-Based Execution Control (MBEC) to detect user/kernel mode transitions and memory access patterns. Unlike existing malware detection environments, TRM can extract full memory traces in user and kernel spaces and hook the entire target memory map to reconstruct arrays, structures within the operating system, and possible rootkits.   We perform TRM-assisted reverse engineering of kernel-level structures and show that it can speed up manual reverse engineering by 75\\% on average. We obfuscate known malware with the latest packing tools and successfully perform similarity detection. Furthermore, we demonstrate a real-world attack by deploying a modified rootkit onto a driver that bypasses state-of-the-art security auditing tools. We show that TRM can detect each threat and that, out of 24 state-of-the-art AV solutions, only TRM can detect the most advanced threats.","sentences":["Existing anti-malware software and reverse engineering toolkits struggle with stealthy sub-OS rootkits due to limitations of run-time kernel-level monitoring.","A malicious kernel-level driver can bypass OS-level anti-virus mechanisms easily.","Although static analysis of such malware is possible, obfuscation and packing techniques complicate offline analysis.","Moreover, current dynamic analyzers suffer from virtualization performance overhead and create detectable traces that allow modern malware to evade them.   ","To address these issues, we present \\textit{The Reversing Machine} (TRM), a new hypervisor-based memory introspection design for reverse engineering, reconstructing memory offsets, and fingerprinting evasive and obfuscated user-level and kernel-level malware.","TRM proposes two novel techniques that enable efficient and transparent analysis of evasive malware: hooking a binary using suspended process creation for hypervisor-based memory introspection, and leveraging Mode-Based Execution Control (MBEC) to detect user/kernel mode transitions and memory access patterns.","Unlike existing malware detection environments, TRM can extract full memory traces in user and kernel spaces and hook the entire target memory map to reconstruct arrays, structures within the operating system, and possible rootkits.   ","We perform TRM-assisted reverse engineering of kernel-level structures and show that it can speed up manual reverse engineering by 75\\% on average.","We obfuscate known malware with the latest packing tools and successfully perform similarity detection.","Furthermore, we demonstrate a real-world attack by deploying a modified rootkit onto a driver that bypasses state-of-the-art security auditing tools.","We show that TRM can detect each threat and that, out of 24 state-of-the-art AV solutions, only TRM can detect the most advanced threats."],"url":"http://arxiv.org/abs/2405.00298v1","category":"cs.CR"}
{"created":"2024-05-01 02:24:50","title":"Sub-terahertz optomechanics","abstract":"We demonstrate optomechanics in the sub-terahertz regime. An optical racetrack resonator, patterned from thin-film lithium niobate, is suspended to support mechanical structures oscillating at these extremely high frequencies, which are read out through cavity optomechanical coupling. Our hybrid platform paves the way for advancing mechanical systems in the quantum regime at elevated temperatures.","sentences":["We demonstrate optomechanics in the sub-terahertz regime.","An optical racetrack resonator, patterned from thin-film lithium niobate, is suspended to support mechanical structures oscillating at these extremely high frequencies, which are read out through cavity optomechanical coupling.","Our hybrid platform paves the way for advancing mechanical systems in the quantum regime at elevated temperatures."],"url":"http://arxiv.org/abs/2405.00284v1","category":"physics.optics"}
{"created":"2024-05-01 02:15:12","title":"Global News Synchrony and Diversity During the Start of the COVID-19 Pandemic","abstract":"News coverage profoundly affects how countries and individuals behave in international relations. Yet, we have little empirical evidence of how news coverage varies across countries. To enable studies of global news coverage, we develop an efficient computational methodology that comprises three components: (i) a transformer model to estimate multilingual news similarity; (ii) a global event identification system that clusters news based on a similarity network of news articles; and (iii) measures of news synchrony across countries and news diversity within a country, based on country-specific distributions of news coverage of the global events. Each component achieves state-of-the art performance, scaling seamlessly to massive datasets of millions of news articles. We apply the methodology to 60 million news articles published globally between January 1 and June 30, 2020, across 124 countries and 10 languages, detecting 4357 news events. We identify the factors explaining diversity and synchrony of news coverage across countries. Our study reveals that news media tend to cover a more diverse set of events in countries with larger Internet penetration, more official languages, larger religious diversity, higher economic inequality, and larger populations. Coverage of news events is more synchronized between countries that not only actively participate in commercial and political relations -- such as, pairs of countries with high bilateral trade volume, and countries that belong to the NATO military alliance or BRICS group of major emerging economies -- but also countries that share certain traits: an official language, high GDP, and high democracy indices.","sentences":["News coverage profoundly affects how countries and individuals behave in international relations.","Yet, we have little empirical evidence of how news coverage varies across countries.","To enable studies of global news coverage, we develop an efficient computational methodology that comprises three components: (i) a transformer model to estimate multilingual news similarity; (ii) a global event identification system that clusters news based on a similarity network of news articles; and (iii) measures of news synchrony across countries and news diversity within a country, based on country-specific distributions of news coverage of the global events.","Each component achieves state-of-the art performance, scaling seamlessly to massive datasets of millions of news articles.","We apply the methodology to 60 million news articles published globally between January 1 and June 30, 2020, across 124 countries and 10 languages, detecting 4357 news events.","We identify the factors explaining diversity and synchrony of news coverage across countries.","Our study reveals that news media tend to cover a more diverse set of events in countries with larger Internet penetration, more official languages, larger religious diversity, higher economic inequality, and larger populations.","Coverage of news events is more synchronized between countries that not only actively participate in commercial and political relations -- such as, pairs of countries with high bilateral trade volume, and countries that belong to the NATO military alliance or BRICS group of major emerging economies -- but also countries that share certain traits: an official language, high GDP, and high democracy indices."],"url":"http://arxiv.org/abs/2405.00280v1","category":"cs.SI"}
{"created":"2024-05-01 01:26:20","title":"Adaptive Integral Sliding Mode Control for Attitude Tracking of Underwater Robots With Large Range Pitch Variations in Confined Space","abstract":"Underwater robots play a crucial role in exploring aquatic environments. The ability to flexibly adjust their attitudes is essential for underwater robots to effectively accomplish tasks in confined space. However, the highly coupled six degrees of freedom dynamics resulting from attitude changes and the complex turbulence within limited spatial areas present significant challenges. To address the problem of attitude control of underwater robots, this letter investigates large-range pitch angle tracking during station holding as well as simultaneous roll and yaw angle control to enable versatile attitude adjustments. Based on dynamic modeling, this letter proposes an adaptive integral sliding mode controller (AISMC) that integrates an integral module into traditional sliding mode control (SMC) and adaptively adjusts the switching gain for improved tracking accuracy, reduced chattering, and enhanced robustness. The stability of the closed-loop control system is established through Lyapunov analysis. Extensive experiments and comparison studies are conducted using a commercial remotely operated vehicle (ROV), the results of which demonstrate that AISMC achieves satisfactory performance in attitude tracking control in confined space with unknown disturbances, significantly outperforming both PID and SMC.","sentences":["Underwater robots play a crucial role in exploring aquatic environments.","The ability to flexibly adjust their attitudes is essential for underwater robots to effectively accomplish tasks in confined space.","However, the highly coupled six degrees of freedom dynamics resulting from attitude changes and the complex turbulence within limited spatial areas present significant challenges.","To address the problem of attitude control of underwater robots, this letter investigates large-range pitch angle tracking during station holding as well as simultaneous roll and yaw angle control to enable versatile attitude adjustments.","Based on dynamic modeling, this letter proposes an adaptive integral sliding mode controller (AISMC) that integrates an integral module into traditional sliding mode control (SMC) and adaptively adjusts the switching gain for improved tracking accuracy, reduced chattering, and enhanced robustness.","The stability of the closed-loop control system is established through Lyapunov analysis.","Extensive experiments and comparison studies are conducted using a commercial remotely operated vehicle (ROV), the results of which demonstrate that AISMC achieves satisfactory performance in attitude tracking control in confined space with unknown disturbances, significantly outperforming both PID and SMC."],"url":"http://arxiv.org/abs/2405.00269v1","category":"cs.RO"}
{"created":"2024-05-01 00:36:39","title":"Informed Total-Error-Minimizing Priors: Interpretable cosmological parameter constraints despite complex nuisance effects","abstract":"While Bayesian inference techniques are standard in cosmological analyses, it is common to interpret resulting parameter constraints with a frequentist intuition. This intuition can fail, e.g. when marginalizing high-dimensional parameter spaces onto subsets of parameters, because of what has come to be known as projection effects or prior volume effects. We present the method of Informed Total-Error-Minimizing (ITEM) priors to address this. An ITEM prior is a prior distribution on a set of nuisance parameters, e.g. ones describing astrophysical or calibration systematics, intended to enforce the validity of a frequentist interpretation of the posterior constraints derived for a set of target parameters, e.g. cosmological parameters. Our method works as follows: For a set of plausible nuisance realizations, we generate target parameter posteriors using several different candidate priors for the nuisance parameters. We reject candidate priors that do not accomplish the minimum requirements of bias (of point estimates) and coverage (of confidence regions among a set of noisy realizations of the data) for the target parameters on one or more of the plausible nuisance realizations. Of the priors that survive this cut we select the ITEM prior as the one that minimizes the total error of the marginalized posteriors of the target parameters. As a proof of concept, we apply our method to the Density Split Statistics (DSS) measured in Dark Energy Survey Year 1 data. We demonstrate that the ITEM priors substantially reduce prior volume effects that otherwise arise and allow sharpened yet robust constraints on the parameters of interest.","sentences":["While Bayesian inference techniques are standard in cosmological analyses, it is common to interpret resulting parameter constraints with a frequentist intuition.","This intuition can fail, e.g. when marginalizing high-dimensional parameter spaces onto subsets of parameters, because of what has come to be known as projection effects or prior volume effects.","We present the method of Informed Total-Error-Minimizing (ITEM) priors to address this.","An ITEM prior is a prior distribution on a set of nuisance parameters, e.g. ones describing astrophysical or calibration systematics, intended to enforce the validity of a frequentist interpretation of the posterior constraints derived for a set of target parameters, e.g. cosmological parameters.","Our method works as follows:","For a set of plausible nuisance realizations, we generate target parameter posteriors using several different candidate priors for the nuisance parameters.","We reject candidate priors that do not accomplish the minimum requirements of bias (of point estimates) and coverage (of confidence regions among a set of noisy realizations of the data) for the target parameters on one or more of the plausible nuisance realizations.","Of the priors that survive this cut we select the ITEM prior as the one that minimizes the total error of the marginalized posteriors of the target parameters.","As a proof of concept, we apply our method to the Density Split Statistics (DSS) measured in Dark Energy Survey Year 1 data.","We demonstrate that the ITEM priors substantially reduce prior volume effects that otherwise arise and allow sharpened yet robust constraints on the parameters of interest."],"url":"http://arxiv.org/abs/2405.00261v1","category":"astro-ph.CO"}
{"created":"2024-05-01 00:24:23","title":"Optimization of Dark-Field CT for Lung Imaging","abstract":"Background: X-ray grating-based dark-field imaging can sense the small angle scattering caused by an object's micro-structure. This technique is sensitive to lung's porous alveoli and is able to detect lung disease at an early stage. Up to now, a human-scale dark-field CT has been built for lung imaging. Purpose: This study aimed to develop a more thorough optimization method for dark-field lung CT and summarize principles for system design. Methods: We proposed a metric in the form of contrast-to-noise ratio (CNR) for system parameter optimization, and designed a phantom with concentric circle shape to fit the task of lung disease detection. Finally, we developed the calculation method of the CNR metric, and analyzed the relation between CNR and system parameters. Results: We showed that with other parameters held constant, the CNR first increases and then decreases with the system auto-correlation length (ACL). The optimal ACL is nearly not influenced by system's visibility, and is only related to phantom's property, i.e., scattering material's size and phantom's absorption. For our phantom, the optimal ACL is about 0.21 {\\mu}m. As for system geometry, larger source-detector and isocenter-detector distance can increase the system's maximal ACL, helping the system meet the optimal ACL more easily. Conclusions: This study proposed a more reasonable metric and a task-based process for optimization, and demonstrated that the system optimal ACL is only related to the phantom's property.","sentences":["Background: X-ray grating-based dark-field imaging can sense the small angle scattering caused by an object's micro-structure.","This technique is sensitive to lung's porous alveoli and is able to detect lung disease at an early stage.","Up to now, a human-scale dark-field CT has been built for lung imaging.","Purpose:","This study aimed to develop a more thorough optimization method for dark-field lung CT and summarize principles for system design.","Methods: We proposed a metric in the form of contrast-to-noise ratio (CNR) for system parameter optimization, and designed a phantom with concentric circle shape to fit the task of lung disease detection.","Finally, we developed the calculation method of the CNR metric, and analyzed the relation between CNR and system parameters.","Results:","We showed that with other parameters held constant, the CNR first increases and then decreases with the system auto-correlation length (ACL).","The optimal ACL is nearly not influenced by system's visibility, and is only related to phantom's property, i.e., scattering material's size and phantom's absorption.","For our phantom, the optimal ACL is about 0.21 {\\mu}m.","As for system geometry, larger source-detector and isocenter-detector distance can increase the system's maximal ACL, helping the system meet the optimal ACL more easily.","Conclusions: This study proposed a more reasonable metric and a task-based process for optimization, and demonstrated that the system optimal ACL is only related to the phantom's property."],"url":"http://arxiv.org/abs/2405.00259v1","category":"physics.med-ph"}
{"created":"2024-04-30 23:41:44","title":"Bi-Lipschitz rigidity of discrete subgroups","abstract":"We obtain a bi-Lipschitz rigidity theorem for a Zariski dense discrete subgroup of a connected simple real algebraic group. As an application, we show that any Zariski dense discrete subgroup of a higher rank semisimple algebraic group $G$ cannot have a $C^1$-smooth antipodal limit set in $G/P$ for any non-maximal parabolic subgroup $P$.","sentences":["We obtain a bi-Lipschitz rigidity theorem for a Zariski dense discrete subgroup of a connected simple real algebraic group.","As an application, we show that any Zariski dense discrete subgroup of a higher rank semisimple algebraic group $G$ cannot have a $C^1$-smooth antipodal limit set in $G/P$ for any non-maximal parabolic subgroup $P$."],"url":"http://arxiv.org/abs/2405.00249v1","category":"math.GR"}
{"created":"2024-04-30 23:19:39","title":"A Meta-Game Evaluation Framework for Deep Multiagent Reinforcement Learning","abstract":"Evaluating deep multiagent reinforcement learning (MARL) algorithms is complicated by stochasticity in training and sensitivity of agent performance to the behavior of other agents. We propose a meta-game evaluation framework for deep MARL, by framing each MARL algorithm as a meta-strategy, and repeatedly sampling normal-form empirical games over combinations of meta-strategies resulting from different random seeds. Each empirical game captures both self-play and cross-play factors across seeds. These empirical games provide the basis for constructing a sampling distribution, using bootstrapping, over a variety of game analysis statistics. We use this approach to evaluate state-of-the-art deep MARL algorithms on a class of negotiation games. From statistics on individual payoffs, social welfare, and empirical best-response graphs, we uncover strategic relationships among self-play, population-based, model-free, and model-based MARL methods.We also investigate the effect of run-time search as a meta-strategy operator, and find via meta-game analysis that the search version of a meta-strategy generally leads to improved performance.","sentences":["Evaluating deep multiagent reinforcement learning (MARL) algorithms is complicated by stochasticity in training and sensitivity of agent performance to the behavior of other agents.","We propose a meta-game evaluation framework for deep MARL, by framing each MARL algorithm as a meta-strategy, and repeatedly sampling normal-form empirical games over combinations of meta-strategies resulting from different random seeds.","Each empirical game captures both self-play and cross-play factors across seeds.","These empirical games provide the basis for constructing a sampling distribution, using bootstrapping, over a variety of game analysis statistics.","We use this approach to evaluate state-of-the-art deep MARL algorithms on a class of negotiation games.","From statistics on individual payoffs, social welfare, and empirical best-response graphs, we uncover strategic relationships among self-play, population-based, model-free, and model-based MARL methods.","We also investigate the effect of run-time search as a meta-strategy operator, and find via meta-game analysis that the search version of a meta-strategy generally leads to improved performance."],"url":"http://arxiv.org/abs/2405.00243v1","category":"cs.MA"}
{"created":"2024-04-30 22:47:06","title":"The Monge-Ampere system in dimension two: a further regularity improvement","abstract":"We prove a convex integration result for the Monge-Amp\\`ere system, in case of dimension $d=2$ and arbitrary codimension $k\\geq 1$. Our prior result stated flexibility up to the H\\\"older regularity $\\mathcal{C}^{1,\\frac{1}{1+ 4/k}}$, whereas presently we achieve flexibility up to $\\mathcal{C}^{1,1}$ when $k\\geq 4$ and up to $\\mathcal{C}^{1,\\frac{2^k-1}{2^{k+1}-1}}$ for any $k$. This first result uses the approach of K\\\"allen, while the second result iterates on the approach of Cao-Hirsch-Inauen and agrees with it for $k=1$ at the H\\\"older regularity up to $\\mathcal{C}^{1,1/3}$.","sentences":["We prove a convex integration result for the Monge-Amp\\`ere system, in case of dimension $d=2$ and arbitrary codimension $k\\geq 1$.","Our prior result stated flexibility up to the H\\\"older regularity $\\mathcal{C}^{1,\\frac{1}{1+ 4/k}}$, whereas presently we achieve flexibility up to $\\mathcal{C}^{1,1}$ when $k\\geq 4$ and up to $\\mathcal{C}^{1,\\frac{2^k-1}{2^{k+1}-1}}$ for any $k$. This first result uses the approach of K\\\"allen, while the second result iterates on the approach of Cao-Hirsch-Inauen and agrees with it for $k=1$ at the H\\\"older regularity up to $\\mathcal{C}^{1,1/3}$."],"url":"http://arxiv.org/abs/2405.00231v1","category":"math.AP"}
{"created":"2024-04-30 22:34:22","title":"A decomposition-based approach for large-scale pickup and delivery problems","abstract":"With the advent of self-driving cars, experts envision autonomous mobility-on-demand services in the near future to cope with overloaded transportation systems in cities worldwide. Efficient operations are imperative to unlock such a system's maximum improvement potential. Existing approaches either consider a narrow planning horizon or ignore essential characteristics of the underlying problem. In this paper, we develop an algorithmic framework that allows the study of very large-scale pickup and delivery routing problems with more than 20 thousand requests, which arise in the context of integrated request pooling and vehicle-to-request dispatching. We conduct a computational study and present comparative results showing the characteristics of the developed approaches. Furthermore, we apply our algorithm to related benchmark instances from the literature to show the efficacy. Finally, we solve very large-scale instances and derive insights on upper-bound improvements regarding fleet sizing and customer delay acceptance from a practical perspective.","sentences":["With the advent of self-driving cars, experts envision autonomous mobility-on-demand services in the near future to cope with overloaded transportation systems in cities worldwide.","Efficient operations are imperative to unlock such a system's maximum improvement potential.","Existing approaches either consider a narrow planning horizon or ignore essential characteristics of the underlying problem.","In this paper, we develop an algorithmic framework that allows the study of very large-scale pickup and delivery routing problems with more than 20 thousand requests, which arise in the context of integrated request pooling and vehicle-to-request dispatching.","We conduct a computational study and present comparative results showing the characteristics of the developed approaches.","Furthermore, we apply our algorithm to related benchmark instances from the literature to show the efficacy.","Finally, we solve very large-scale instances and derive insights on upper-bound improvements regarding fleet sizing and customer delay acceptance from a practical perspective."],"url":"http://arxiv.org/abs/2405.00230v1","category":"math.OC"}
{"created":"2024-04-30 22:09:03","title":"Prescribed-Time Stability Properties of Interconnected Systems","abstract":"Achieving control objectives (e.g., stabilization or convergence of tracking error to zero, input-to-state stabilization) in \"prescribed time\" has attracted significant research interest in recent years. The key property of prescribed-time results unlike traditional \"asymptotic\" results is that the convergence or other control objectives are achieved within an arbitrary designer-specified time interval instead of asymptotically as time goes to infinity. In this paper, we consider cascade and feedback interconnections of prescribed-time input-to-state stable (ISS) systems and study conditions under which the overall states of such interconnected systems also converge to the origin in the prescribed time interval. We show that these conditions are intrinsically related to properties of the time-varying \"blow-up\" functions that are central to prescribed-time control designs. We also generalize the results to interconnections of an arbitrary number of systems. As an illustrative example, we consider an interconnection of two uncertain systems that are prescribed-time stabilized using two different control design methods and show that the two separate controllers can be put together to achieve prescribed-time stability of the interconnected system.","sentences":["Achieving control objectives (e.g., stabilization or convergence of tracking error to zero, input-to-state stabilization) in \"prescribed time\" has attracted significant research interest in recent years.","The key property of prescribed-time results unlike traditional \"asymptotic\" results is that the convergence or other control objectives are achieved within an arbitrary designer-specified time interval instead of asymptotically as time goes to infinity.","In this paper, we consider cascade and feedback interconnections of prescribed-time input-to-state stable (ISS) systems and study conditions under which the overall states of such interconnected systems also converge to the origin in the prescribed time interval.","We show that these conditions are intrinsically related to properties of the time-varying \"blow-up\" functions that are central to prescribed-time control designs.","We also generalize the results to interconnections of an arbitrary number of systems.","As an illustrative example, we consider an interconnection of two uncertain systems that are prescribed-time stabilized using two different control design methods and show that the two separate controllers can be put together to achieve prescribed-time stability of the interconnected system."],"url":"http://arxiv.org/abs/2405.00224v1","category":"math.OC"}
{"created":"2024-04-30 22:00:25","title":"Optimized Distribution of Entanglement Graph States in Quantum Networks","abstract":"Building large-scale quantum computers, essential to demonstrating quantum advantage, is a key challenge. Quantum Networks (QNs) can help address this challenge by enabling the construction of large, robust, and more capable quantum computing platforms by connecting smaller quantum computers. Moreover, unlike classical systems, QNs can enable fully secured long-distance communication. Thus, quantum networks lie at the heart of the success of future quantum information technologies. In quantum networks, multipartite entangled states distributed over the network help implement and support many quantum network applications for communications, sensing, and computing. Our work focuses on developing optimal techniques to generate and distribute multipartite entanglement states efficiently. Prior works on generating general multipartite entanglement states have focused on the objective of minimizing the number of maximally entangled pairs (EPs) while ignoring the heterogeneity of the network nodes and links as well as the stochastic nature of underlying processes. In this work, we develop a hypergraph based linear programming framework that delivers optimal (under certain assumptions) generation schemes for general multipartite entanglement represented by graph states, under the network resources, decoherence, and fidelity constraints, while considering the stochasticity of the underlying processes. We illustrate our technique by developing generation schemes for the special cases of path and tree graph states, and discuss optimized generation schemes for more general classes of graph states. Using extensive simulations over a quantum network simulator (NetSquid), we demonstrate the effectiveness of our developed techniques and show that they outperform prior known schemes by up to orders of magnitude.","sentences":["Building large-scale quantum computers, essential to demonstrating quantum advantage, is a key challenge.","Quantum Networks (QNs) can help address this challenge by enabling the construction of large, robust, and more capable quantum computing platforms by connecting smaller quantum computers.","Moreover, unlike classical systems, QNs can enable fully secured long-distance communication.","Thus, quantum networks lie at the heart of the success of future quantum information technologies.","In quantum networks, multipartite entangled states distributed over the network help implement and support many quantum network applications for communications, sensing, and computing.","Our work focuses on developing optimal techniques to generate and distribute multipartite entanglement states efficiently.","Prior works on generating general multipartite entanglement states have focused on the objective of minimizing the number of maximally entangled pairs (EPs) while ignoring the heterogeneity of the network nodes and links as well as the stochastic nature of underlying processes.","In this work, we develop a hypergraph based linear programming framework that delivers optimal (under certain assumptions) generation schemes for general multipartite entanglement represented by graph states, under the network resources, decoherence, and fidelity constraints, while considering the stochasticity of the underlying processes.","We illustrate our technique by developing generation schemes for the special cases of path and tree graph states, and discuss optimized generation schemes for more general classes of graph states.","Using extensive simulations over a quantum network simulator (NetSquid), we demonstrate the effectiveness of our developed techniques and show that they outperform prior known schemes by up to orders of magnitude."],"url":"http://arxiv.org/abs/2405.00222v1","category":"quant-ph"}
{"created":"2024-04-30 21:53:11","title":"Machine Learning-based Estimation of Respiratory Fluctuations in a Healthy Adult Population using BOLD fMRI and Head Motion Parameters","abstract":"Motivation: In many fMRI studies, respiratory signals are often missing or of poor quality. Therefore, it could be highly beneficial to have a tool to extract respiratory variation (RV) waveforms directly from fMRI data without the need for peripheral recording devices.   Goal(s): Investigate the hypothesis that head motion parameters contain valuable information regarding respiratory patter, which can help machine learning algorithms estimate the RV waveform.   Approach: This study proposes a CNN model for reconstruction of RV waveforms using head motion parameters and BOLD signals.   Results: This study showed that combining head motion parameters with BOLD signals enhances RV waveform estimation.   Impact: It is expected that application of the proposed method will lower the cost of fMRI studies, reduce complexity, and decrease the burden on participants as they will not be required to wear a respiratory bellows.","sentences":["Motivation: In many fMRI studies, respiratory signals are often missing or of poor quality.","Therefore, it could be highly beneficial to have a tool to extract respiratory variation (RV) waveforms directly from fMRI data without the need for peripheral recording devices.   ","Goal(s):","Investigate the hypothesis that head motion parameters contain valuable information regarding respiratory patter, which can help machine learning algorithms estimate the RV waveform.   ","Approach:","This study proposes a CNN model for reconstruction of RV waveforms using head motion parameters and BOLD signals.   ","Results:","This study showed that combining head motion parameters with BOLD signals enhances RV waveform estimation.   ","Impact: It is expected that application of the proposed method will lower the cost of fMRI studies, reduce complexity, and decrease the burden on participants as they will not be required to wear a respiratory bellows."],"url":"http://arxiv.org/abs/2405.00219v1","category":"cs.LG"}
{"created":"2024-04-30 21:39:07","title":"Analytical solutions of symmetric isotropic spin clusters","abstract":"Spin models like the Heisenberg Hamiltonian effectively describe the interactions of open-shell transition-metal ions on a lattice and can account for various properties of magnetic solids and molecules. Numerical methods are usually required to find exact or approximate eigenstates, but for small clusters with spatial symmetry, analytical solutions exist, and a few Heisenberg systems have been solved in closed form. This paper presents a simple, generally applicable approach to analytically solve isotropic spin clusters, based on adapting the basis to both total-spin and point-group symmetry to factor the Hamiltonian matrix into sufficiently small blocks. We demonstrate applications to small rings and polyhedra, some of which are straightforward to solve by successive spin-coupling for Heisenberg terms only; additional interactions, such as biquadratic exchange or multi-center terms necessitate symmetry adaptation.","sentences":["Spin models like the Heisenberg Hamiltonian effectively describe the interactions of open-shell transition-metal ions on a lattice and can account for various properties of magnetic solids and molecules.","Numerical methods are usually required to find exact or approximate eigenstates, but for small clusters with spatial symmetry, analytical solutions exist, and a few Heisenberg systems have been solved in closed form.","This paper presents a simple, generally applicable approach to analytically solve isotropic spin clusters, based on adapting the basis to both total-spin and point-group symmetry to factor the Hamiltonian matrix into sufficiently small blocks.","We demonstrate applications to small rings and polyhedra, some of which are straightforward to solve by successive spin-coupling for Heisenberg terms only; additional interactions, such as biquadratic exchange or multi-center terms necessitate symmetry adaptation."],"url":"http://arxiv.org/abs/2405.00214v1","category":"cond-mat.str-el"}
{"created":"2024-04-30 21:23:13","title":"Deposition of highly-crystalline AlScN thin films using synchronized HiPIMS -- from combinatorial screening to piezoelectric devices","abstract":"Fueled by the 5G revolution, the demand for advanced radio frequency micro-electromechanical systems (MEMS) based on AlScN is growing rapidly. However, synthesizing high-quality, textured AlScN thin films is challenging. Current approaches typically rely on high temperatures and expensive compound targets. In this study, we demonstrate the feasibility of ionized physical vapor deposition to deposit highly oriented AlScN films with minimal defects at lower temperatures. Using metal-ion synchronized high-power impulse magnetron co-sputtering (MIS-HiPIMS) we can selectively bombard the growing film with Al and/or Sc ions to enhance the adatom mobility while simultaneously providing the ability to tune stress and coat complex structures conformally. We find that the Sc solubility in wurtzite AlN is slightly reduced, whereas crystallinity and texture are markedly improved. Disoriented grains, a key challenge in growing AlScN films, are completely removed via substrate biasing, while the residual stress can be tailored by adjusting the same. The measured piezoelectric response of the films is in line with DFT predictions and on par with the current state of the art. Finally, we demonstrate conformal deposition of c-axis textured AlScN on structured Si wafers underlining the promise of ionized PVD for the fabrication of advanced RF filters and next-generation MEMS devices.","sentences":["Fueled by the 5G revolution, the demand for advanced radio frequency micro-electromechanical systems (MEMS) based on AlScN is growing rapidly.","However, synthesizing high-quality, textured AlScN thin films is challenging.","Current approaches typically rely on high temperatures and expensive compound targets.","In this study, we demonstrate the feasibility of ionized physical vapor deposition to deposit highly oriented AlScN films with minimal defects at lower temperatures.","Using metal-ion synchronized high-power impulse magnetron co-sputtering (MIS-HiPIMS) we can selectively bombard the growing film with Al and/or Sc ions to enhance the adatom mobility while simultaneously providing the ability to tune stress and coat complex structures conformally.","We find that the Sc solubility in wurtzite AlN is slightly reduced, whereas crystallinity and texture are markedly improved.","Disoriented grains, a key challenge in growing AlScN films, are completely removed via substrate biasing, while the residual stress can be tailored by adjusting the same.","The measured piezoelectric response of the films is in line with DFT predictions and on par with the current state of the art.","Finally, we demonstrate conformal deposition of c-axis textured AlScN on structured Si wafers underlining the promise of ionized PVD for the fabrication of advanced RF filters and next-generation MEMS devices."],"url":"http://arxiv.org/abs/2405.00210v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-30 21:05:31","title":"Field Report on a Wearable and Versatile Solution for Field Acquisition and Exploration","abstract":"This report presents a wearable plug-and-play platform for data acquisition in the field. The platform, extending a waterproof Pelican Case into a 20 kg backpack offers 5.5 hours of power autonomy, while recording data with two cameras, a lidar, an Inertial Measurement Unit (IMU), and a Global Navigation Satellite System (GNSS) receiver. The system only requires a single operator and is readily controlled with a built-in screen and buttons. Due to its small footprint, it offers greater flexibility than large vehicles typically deployed in off-trail environments. We describe the platform's design, detailing the mechanical parts, electrical components, and software stack. We explain the system's limitations, drawing from its extensive deployment spanning over 20 kilometers of trajectories across various seasons, environments, and weather conditions. We derive valuable lessons learned from these deployments and present several possible applications for the system. The possible use cases consider not only academic research but also insights from consultations with our industrial partners. The mechanical design including all CAD files, as well as the software stack, are publicly available at https://github.com/norlab-ulaval/backpack_workspace.","sentences":["This report presents a wearable plug-and-play platform for data acquisition in the field.","The platform, extending a waterproof Pelican Case into a 20 kg backpack offers 5.5 hours of power autonomy, while recording data with two cameras, a lidar, an Inertial Measurement Unit (IMU), and a Global Navigation Satellite System (GNSS) receiver.","The system only requires a single operator and is readily controlled with a built-in screen and buttons.","Due to its small footprint, it offers greater flexibility than large vehicles typically deployed in off-trail environments.","We describe the platform's design, detailing the mechanical parts, electrical components, and software stack.","We explain the system's limitations, drawing from its extensive deployment spanning over 20 kilometers of trajectories across various seasons, environments, and weather conditions.","We derive valuable lessons learned from these deployments and present several possible applications for the system.","The possible use cases consider not only academic research but also insights from consultations with our industrial partners.","The mechanical design including all CAD files, as well as the software stack, are publicly available at https://github.com/norlab-ulaval/backpack_workspace."],"url":"http://arxiv.org/abs/2405.00199v1","category":"cs.RO"}
{"created":"2024-04-30 21:02:46","title":"Data-driven identification of stable differential operators using constrained regression","abstract":"Identifying differential operators from data is essential for the mathematical modeling of complex physical and biological systems where massive datasets are available. These operators must be stable for accurate predictions for dynamics forecasting problems. In this article, we propose a novel methodology for learning sparse differential operators that are theoretically linearly stable by solving a constrained regression problem. These underlying constraints are obtained following linear stability for dynamical systems. We further extend this approach for learning nonlinear differential operators by determining linear stability constraints for linearized equations around an equilibrium point. The applicability of the proposed method is demonstrated for both linear and nonlinear partial differential equations such as 1-D scalar advection-diffusion equation, 1-D Burgers equation and 2-D advection equation. The results indicated that solutions to constrained regression problems with linear stability constraints provide accurate and linearly stable sparse differential operators.","sentences":["Identifying differential operators from data is essential for the mathematical modeling of complex physical and biological systems where massive datasets are available.","These operators must be stable for accurate predictions for dynamics forecasting problems.","In this article, we propose a novel methodology for learning sparse differential operators that are theoretically linearly stable by solving a constrained regression problem.","These underlying constraints are obtained following linear stability for dynamical systems.","We further extend this approach for learning nonlinear differential operators by determining linear stability constraints for linearized equations around an equilibrium point.","The applicability of the proposed method is demonstrated for both linear and nonlinear partial differential equations such as 1-D scalar advection-diffusion equation, 1-D Burgers equation and 2-D advection equation.","The results indicated that solutions to constrained regression problems with linear stability constraints provide accurate and linearly stable sparse differential operators."],"url":"http://arxiv.org/abs/2405.00198v1","category":"math.NA"}
{"created":"2024-04-30 20:58:42","title":"Room-temperature optomechanics with light-matter condensates","abstract":"In this work, we develop an optomechanical formalism for macroscopic quantum states in exciton-polariton systems with strong exciton-phonon interactions. We show that polariton optomechanical interactions induce dynamical backaction, resulting in dispersive and dissipative shifts in the complex vibrational response functions. Unlike conventional optomechanical systems, polariton optomechanics features high-dimensionality and phase-space confinement due to the dispersion relations of exciton-polaritons. Consequently, vibrational modes exhibit effective positive or negative mass depending on the detuning parameter, and are capable for the vibrational Bose condensation under the resonant conditions. We demonstrate the potential for vibrational control of polariton condensates at room temperature.","sentences":["In this work, we develop an optomechanical formalism for macroscopic quantum states in exciton-polariton systems with strong exciton-phonon interactions.","We show that polariton optomechanical interactions induce dynamical backaction, resulting in dispersive and dissipative shifts in the complex vibrational response functions.","Unlike conventional optomechanical systems, polariton optomechanics features high-dimensionality and phase-space confinement due to the dispersion relations of exciton-polaritons.","Consequently, vibrational modes exhibit effective positive or negative mass depending on the detuning parameter, and are capable for the vibrational Bose condensation under the resonant conditions.","We demonstrate the potential for vibrational control of polariton condensates at room temperature."],"url":"http://arxiv.org/abs/2405.00195v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-30 20:44:31","title":"Distribution of lowest eigenvalue in $k$-body bosonic random matrix ensembles","abstract":"We numerically study the distribution of the lowest eigenvalue of finite many-boson systems with $k$-body interactions modeled by Bosonic Embedded Gaussian Orthogonal [BEGOE($k$)] and Unitary [BEGUE($k$)] random matrix Ensembles. Following the recently established result that the $q$-normal describes the smooth form of the eigenvalue density of the $k$-body embedded ensembles, the first four moments of the distribution of lowest eigenvalues have been analyzed as a function of the $q$ parameter, with $q \\sim 1$ for $k = 1$ and $q = 0$ for $k = m$; $m$ being the number of bosons. Our results show the distribution exhibits a smooth transition from Gaussian like for $q$ close to 1 to a modified Gumbel like for intermediate values of $q$ to the well-known Tracy-Widom distribution for $q=0$.","sentences":["We numerically study the distribution of the lowest eigenvalue of finite many-boson systems with $k$-body interactions modeled by Bosonic Embedded Gaussian Orthogonal [BEGOE($k$)] and Unitary [BEGUE($k$)] random matrix Ensembles.","Following the recently established result that the $q$-normal describes the smooth form of the eigenvalue density of the $k$-body embedded ensembles, the first four moments of the distribution of lowest eigenvalues have been analyzed as a function of the $q$ parameter, with $q \\sim 1$ for $k = 1$ and $q = 0$ for $k = m$; $m$ being the number of bosons.","Our results show the distribution exhibits a smooth transition from Gaussian like for $q$ close to 1 to a modified Gumbel like for intermediate values of $q$ to the well-known Tracy-Widom distribution for $q=0$."],"url":"http://arxiv.org/abs/2405.00190v1","category":"quant-ph"}
{"created":"2024-04-30 20:36:38","title":"Comparing Motion Distortion Between Vehicle Field Deployments","abstract":"Recent advances in autonomous driving for uncrewed ground vehicles (UGVs) have spurred significant development, particularly in challenging terrains. This paper introduces a classification system assessing various UGV deployments reported in the literature. Our approach considers motion distortion features that include internal UGV features, such as mass and speed, and external features, such as terrain complexity, which all influence the efficiency of models and navigation systems. We present results that map UGV deployments relative to vehicle kinetic energy and terrain complexity, providing insights into the level of complexity and risk associated with different operational environments. Additionally, we propose a motion distortion metric to assess UGV navigation performance that does not require an explicit quantification of motion distortion features. Using this metric, we conduct a case study to illustrate the impact of motion distortion features on modeling accuracy. This research advocates for creating a comprehensive database containing many different motion distortion features, which would contribute to advancing the understanding of autonomous driving capabilities in rough conditions and provide a validation framework for future developments in UGV navigation systems.","sentences":["Recent advances in autonomous driving for uncrewed ground vehicles (UGVs) have spurred significant development, particularly in challenging terrains.","This paper introduces a classification system assessing various UGV deployments reported in the literature.","Our approach considers motion distortion features that include internal UGV features, such as mass and speed, and external features, such as terrain complexity, which all influence the efficiency of models and navigation systems.","We present results that map UGV deployments relative to vehicle kinetic energy and terrain complexity, providing insights into the level of complexity and risk associated with different operational environments.","Additionally, we propose a motion distortion metric to assess UGV navigation performance that does not require an explicit quantification of motion distortion features.","Using this metric, we conduct a case study to illustrate the impact of motion distortion features on modeling accuracy.","This research advocates for creating a comprehensive database containing many different motion distortion features, which would contribute to advancing the understanding of autonomous driving capabilities in rough conditions and provide a validation framework for future developments in UGV navigation systems."],"url":"http://arxiv.org/abs/2405.00189v1","category":"cs.RO"}
{"created":"2024-04-30 20:25:57","title":"Towards End-to-End Semi-Supervised Table Detection with Semantic Aligned Matching Transformer","abstract":"Table detection within document images is a crucial task in document processing, involving the identification and localization of tables. Recent strides in deep learning have substantially improved the accuracy of this task, but it still heavily relies on large labeled datasets for effective training. Several semi-supervised approaches have emerged to overcome this challenge, often employing CNN-based detectors with anchor proposals and post-processing techniques like non-maximal suppression (NMS). However, recent advancements in the field have shifted the focus towards transformer-based techniques, eliminating the need for NMS and emphasizing object queries and attention mechanisms. Previous research has focused on two key areas to improve transformer-based detectors: refining the quality of object queries and optimizing attention mechanisms. However, increasing object queries can introduce redundancy, while adjustments to the attention mechanism can increase complexity. To address these challenges, we introduce a semi-supervised approach employing SAM-DETR, a novel approach for precise alignment between object queries and target features. Our approach demonstrates remarkable reductions in false positives and substantial enhancements in table detection performance, particularly in complex documents characterized by diverse table structures. This work provides more efficient and accurate table detection in semi-supervised settings.","sentences":["Table detection within document images is a crucial task in document processing, involving the identification and localization of tables.","Recent strides in deep learning have substantially improved the accuracy of this task, but it still heavily relies on large labeled datasets for effective training.","Several semi-supervised approaches have emerged to overcome this challenge, often employing CNN-based detectors with anchor proposals and post-processing techniques like non-maximal suppression (NMS).","However, recent advancements in the field have shifted the focus towards transformer-based techniques, eliminating the need for NMS and emphasizing object queries and attention mechanisms.","Previous research has focused on two key areas to improve transformer-based detectors: refining the quality of object queries and optimizing attention mechanisms.","However, increasing object queries can introduce redundancy, while adjustments to the attention mechanism can increase complexity.","To address these challenges, we introduce a semi-supervised approach employing SAM-DETR, a novel approach for precise alignment between object queries and target features.","Our approach demonstrates remarkable reductions in false positives and substantial enhancements in table detection performance, particularly in complex documents characterized by diverse table structures.","This work provides more efficient and accurate table detection in semi-supervised settings."],"url":"http://arxiv.org/abs/2405.00187v1","category":"cs.CV"}
{"created":"2024-04-30 19:59:14","title":"Quantum Entanglement In Mixed-Spin Trimmer: Effects of A Magnetic Field And Heterogeneous g-Factors","abstract":"Mixed spin-(1/2,1/2,1) trimmer with two different Land\\'{e} g-factors and two different exchange couplings is considered. The main feature of the model is non-conserving magnetization. The Hamiltonian of the system is diagonalized analytically. We presented a detailed analysis of the ground state properties, revealing several possible ground state phase diagrams and magnetization profiles. The main focus is on how non-conserving magnetization affects quantum entanglement. We have found that non-conserving magnetization can bring to the continuous dependence of the entanglement quantifying parameter (negativity) on magnetic field within the same eigenstate, while for the case of uniform $g$-factors it is a constant. The main result is an essential enhancement of the entanglement in case of uniform couplings for one pair of spins caused by an arbitrary small difference in the values of $g$-factors. This enhancement is robust and brings to almost 7-fold increasing of the negativity. We have also found weakening of entanglement for other cases. Thus, non-conserving magnetization offers a broad opportunity to manipulate the entanglement by means of magnetic field.","sentences":["Mixed spin-(1/2,1/2,1) trimmer with two different Land\\'{e} g-factors and two different exchange couplings is considered.","The main feature of the model is non-conserving magnetization.","The Hamiltonian of the system is diagonalized analytically.","We presented a detailed analysis of the ground state properties, revealing several possible ground state phase diagrams and magnetization profiles.","The main focus is on how non-conserving magnetization affects quantum entanglement.","We have found that non-conserving magnetization can bring to the continuous dependence of the entanglement quantifying parameter (negativity) on magnetic field within the same eigenstate, while for the case of uniform $g$-factors it is a constant.","The main result is an essential enhancement of the entanglement in case of uniform couplings for one pair of spins caused by an arbitrary small difference in the values of $g$-factors.","This enhancement is robust and brings to almost 7-fold increasing of the negativity.","We have also found weakening of entanglement for other cases.","Thus, non-conserving magnetization offers a broad opportunity to manipulate the entanglement by means of magnetic field."],"url":"http://arxiv.org/abs/2405.00178v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-30 19:51:37","title":"Towards a Search Engine for Machines: Unified Ranking for Multiple Retrieval-Augmented Large Language Models","abstract":"This paper introduces uRAG--a framework with a unified retrieval engine that serves multiple downstream retrieval-augmented generation (RAG) systems. Each RAG system consumes the retrieval results for a unique purpose, such as open-domain question answering, fact verification, entity linking, and relation extraction. We introduce a generic training guideline that standardizes the communication between the search engine and the downstream RAG systems that engage in optimizing the retrieval model. This lays the groundwork for us to build a large-scale experimentation ecosystem consisting of 18 RAG systems that engage in training and 18 unknown RAG systems that use the uRAG as the new users of the search engine. Using this experimentation ecosystem, we answer a number of fundamental research questions that improve our understanding of promises and challenges in developing search engines for machines.","sentences":["This paper introduces uRAG--a framework with a unified retrieval engine that serves multiple downstream retrieval-augmented generation (RAG) systems.","Each RAG system consumes the retrieval results for a unique purpose, such as open-domain question answering, fact verification, entity linking, and relation extraction.","We introduce a generic training guideline that standardizes the communication between the search engine and the downstream RAG systems that engage in optimizing the retrieval model.","This lays the groundwork for us to build a large-scale experimentation ecosystem consisting of 18 RAG systems that engage in training and 18 unknown RAG systems that use the uRAG as the new users of the search engine.","Using this experimentation ecosystem, we answer a number of fundamental research questions that improve our understanding of promises and challenges in developing search engines for machines."],"url":"http://arxiv.org/abs/2405.00175v1","category":"cs.CL"}
{"created":"2024-04-30 19:48:31","title":"Negative curvature in locally reducible Artin groups","abstract":"In this paper, we define the 2-complete Artin complex and show that it is systolic for locally reducible Artin groups. The stabilizers of simplices in this complex are exactly the proper parabolic subgroups which are \"2-complete.\" We use this systolicity to show that parabolic subgroups, with 2-completions that are not the whole Artin group, are weakly malnormal. This allows us to conclude that many locally reducible Artin groups are acylindrically hyperbolic.","sentences":["In this paper, we define the 2-complete Artin complex and show that it is systolic for locally reducible Artin groups.","The stabilizers of simplices in this complex are exactly the proper parabolic subgroups which are \"2-complete.\"","We use this systolicity to show that parabolic subgroups, with 2-completions that are not the whole Artin group, are weakly malnormal.","This allows us to conclude that many locally reducible Artin groups are acylindrically hyperbolic."],"url":"http://arxiv.org/abs/2405.00173v1","category":"math.GR"}
{"created":"2024-04-30 19:39:54","title":"Planetary Nebula NGC 2818: Revealing its complex 3D morphology","abstract":"We carry out an advanced morpho-kinematic analysis of the Planetary Nebula (PN) NGC 2818, whose complex morphology is described by a basic bipolar component, filamentary structures and a knotty central region. We performed an upgrated 3D Morpho-kinematic (MK) model by employing the SHAPE software, combining for the first time in PNe optical 2D spatially resolved echelle spectra and Fabry-Perot data cubes. The best-fitting 3D model of NGC 2818 successfully reconstructs the main morphology, considering one bipolar component, radial filamentary structures, and an equatorial component as the geometrical locus of the group of cometary knots. The model shows that the equatorial component has the lower expansion velocity of the system at 70 $\\pm$ 20 km/s. The velocity of the bipolar component is 120 $\\pm$ 20 km/s, while all the filamentary structures were found to expand at higher velocities of 180 $\\pm$ 20 km/s. Moreover, Fabry-Perot data revealed for the first time a north-eastern filament expanding at a mean velocity of 80 $\\pm$ 20 km/s, while its equivalent counterpart in the southwestern region was confirmed by a new detected substructure in the echelle data. A new detected knotty structure at velocity -40 $\\pm$ 20 km/s is also reported, as expelled material from the fragmented eastern lobe of the nebula. We interpret the overall structure of NGC 2818 as the result of the evolution of a binary system that underwent the common envelope phase, in conjunction with the ejections of a magnetized jet, misaligned with respect to the symmetry axis of the bipolar/elliptical shell.","sentences":["We carry out an advanced morpho-kinematic analysis of the Planetary Nebula (PN) NGC 2818, whose complex morphology is described by a basic bipolar component, filamentary structures and a knotty central region.","We performed an upgrated 3D Morpho-kinematic (MK) model by employing the SHAPE software, combining for the first time in PNe optical 2D spatially resolved echelle spectra and Fabry-Perot data cubes.","The best-fitting 3D model of NGC 2818 successfully reconstructs the main morphology, considering one bipolar component, radial filamentary structures, and an equatorial component as the geometrical locus of the group of cometary knots.","The model shows that the equatorial component has the lower expansion velocity of the system at 70 $\\pm$ 20 km/s. The velocity of the bipolar component is 120 $\\pm$ 20 km/s, while all the filamentary structures were found to expand at higher velocities of 180 $\\pm$ 20 km/s. Moreover, Fabry-Perot data revealed for the first time a north-eastern filament expanding at a mean velocity of 80 $\\pm$ 20 km/s, while its equivalent counterpart in the southwestern region was confirmed by a new detected substructure in the echelle data.","A new detected knotty structure at velocity -40 $\\pm$ 20 km/s is also reported, as expelled material from the fragmented eastern lobe of the nebula.","We interpret the overall structure of NGC 2818 as the result of the evolution of a binary system that underwent the common envelope phase, in conjunction with the ejections of a magnetized jet, misaligned with respect to the symmetry axis of the bipolar/elliptical shell."],"url":"http://arxiv.org/abs/2405.00169v1","category":"astro-ph.SR"}
{"created":"2024-04-30 19:28:09","title":"Solvable Initial Value Problems Ruled by Discontinuous Ordinary Differential Equations","abstract":"We study initial value problems having dynamics ruled by discontinuous ordinary differential equations with the property of possessing a unique solution. We identify a precise class of such systems that we call \\emph{solvable intitial value problems} and we prove that for this class of problems the unique solution can always be obtained analytically via transfinite recursion. We present several examples including a nontrivial one whose solution yields, at an integer time, a real encoding of the halting set for Turing machines; therefore showcasing that the behavior of solvable systems is related to ordinal Turing computations.","sentences":["We study initial value problems having dynamics ruled by discontinuous ordinary differential equations with the property of possessing a unique solution.","We identify a precise class of such systems that we call \\emph{solvable intitial value problems} and we prove that for this class of problems the unique solution can always be obtained analytically via transfinite recursion.","We present several examples including a nontrivial one whose solution yields, at an integer time, a real encoding of the halting set for Turing machines; therefore showcasing that the behavior of solvable systems is related to ordinal Turing computations."],"url":"http://arxiv.org/abs/2405.00165v1","category":"cs.CC"}
{"created":"2024-04-30 19:26:54","title":"Logical analysis and contradiction detection in high-level requirements during the review process using sat-solver","abstract":"DO-178C stands out as a guiding standard for aviation system development processes. This standard not only mandates ensuring the consistency of requirements in the software verification process but also recognizes it as a mandatory element. The main objective of this study is to introduce a method for analyzing and identifying inconsistencies between high-level requirements using information obtained from a data dictionary. This method aims to transform high-level requirements into logical expressions and then thoroughly examine them using a SAT Solver to detect inconsistencies. While methods focused on identifying inconsistencies among requirements often appear in the literature, this study presents a novel approach to detect contradictions between non-natural language, systematically structured, and language-independent requirements. The goal of this approach is to significantly reduce the review time of high-level requirements in the software verification process. Evaluations indicate that the use of this method results in substantial time savings in the inconsistency detection process.","sentences":["DO-178C stands out as a guiding standard for aviation system development processes.","This standard not only mandates ensuring the consistency of requirements in the software verification process but also recognizes it as a mandatory element.","The main objective of this study is to introduce a method for analyzing and identifying inconsistencies between high-level requirements using information obtained from a data dictionary.","This method aims to transform high-level requirements into logical expressions and then thoroughly examine them using a SAT Solver to detect inconsistencies.","While methods focused on identifying inconsistencies among requirements often appear in the literature, this study presents a novel approach to detect contradictions between non-natural language, systematically structured, and language-independent requirements.","The goal of this approach is to significantly reduce the review time of high-level requirements in the software verification process.","Evaluations indicate that the use of this method results in substantial time savings in the inconsistency detection process."],"url":"http://arxiv.org/abs/2405.00163v1","category":"cs.SE"}
{"created":"2024-04-30 19:26:05","title":"Real Stability and Log Concavity are coNP-Complete","abstract":"Real-stable, Lorentzian, and log-concave polynomials are well-studied classes of polynomials, and have been powerful tools in resolving several conjectures. We show that the problems of deciding whether a polynomial of fixed degree is real stable or log concave are coNP-complete. On the other hand, while all homogeneous real-stable polynomials are Lorentzian and all Lorentzian polynomials are log concave on the positive orthant, the problem of deciding whether a polynomial of fixed degree is Lorentzian can be solved in polynomial time.","sentences":["Real-stable, Lorentzian, and log-concave polynomials are well-studied classes of polynomials, and have been powerful tools in resolving several conjectures.","We show that the problems of deciding whether a polynomial of fixed degree is real stable or log concave are coNP-complete.","On the other hand, while all homogeneous real-stable polynomials are Lorentzian and all Lorentzian polynomials are log concave on the positive orthant, the problem of deciding whether a polynomial of fixed degree is Lorentzian can be solved in polynomial time."],"url":"http://arxiv.org/abs/2405.00162v1","category":"math.OC"}
{"created":"2024-04-30 19:16:08","title":"Heterogeneity analysis provides evidence for a genetically homogeneous subtype of bipolar-disorder","abstract":"Bipolar disorder is a highly heritable brain disorder which affects an estimated 50 million people worldwide. Due to recent advances in genotyping technology and bioinformatics methodology, as well as the increase in the overall amount of available data, our understanding of the genetic underpinnings of BD has improved. A growing consensus is that BD is polygenic and heterogeneous, but the specifics of that heterogeneity are not yet well understood. Here we use a recently developed technique to investigate the genetic heterogeneity of bipolar disorder. We find strong statistical evidence for a `bicluster': a subset of bipolar subjects that exhibits a disease-specific genetic pattern. The structure illuminated by this bicluster replicates in several other data-sets and can be used to improve BD risk-prediction algorithms. We believe that this bicluster is likely to correspond to a genetically-distinct subtype of BD. More generally, we believe that our biclustering approach is a promising means of untangling the underlying heterogeneity of complex disease without the need for reliable subphenotypic data.","sentences":["Bipolar disorder is a highly heritable brain disorder which affects an estimated 50 million people worldwide.","Due to recent advances in genotyping technology and bioinformatics methodology, as well as the increase in the overall amount of available data, our understanding of the genetic underpinnings of BD has improved.","A growing consensus is that BD is polygenic and heterogeneous, but the specifics of that heterogeneity are not yet well understood.","Here we use a recently developed technique to investigate the genetic heterogeneity of bipolar disorder.","We find strong statistical evidence for a `bicluster': a subset of bipolar subjects that exhibits a disease-specific genetic pattern.","The structure illuminated by this bicluster replicates in several other data-sets and can be used to improve BD risk-prediction algorithms.","We believe that this bicluster is likely to correspond to a genetically-distinct subtype of BD.","More generally, we believe that our biclustering approach is a promising means of untangling the underlying heterogeneity of complex disease without the need for reliable subphenotypic data."],"url":"http://arxiv.org/abs/2405.00159v1","category":"q-bio.GN"}
{"created":"2024-04-30 19:13:04","title":"Information-Theoretic Opacity-Enforcement in Markov Decision Processes","abstract":"The paper studies information-theoretic opacity, an information-flow privacy property, in a setting involving two agents: A planning agent who controls a stochastic system and an observer who partially observes the system states. The goal of the observer is to infer some secret, represented by a random variable, from its partial observations, while the goal of the planning agent is to make the secret maximally opaque to the observer while achieving a satisfactory total return. Modeling the stochastic system using a Markov decision process, two classes of opacity properties are considered -- Last-state opacity is to ensure that the observer is uncertain if the last state is in a specific set and initial-state opacity is to ensure that the observer is unsure of the realization of the initial state. As the measure of opacity, we employ the Shannon conditional entropy capturing the information about the secret revealed by the observable. Then, we develop primal-dual policy gradient methods for opacity-enforcement planning subject to constraints on total returns. We propose novel algorithms to compute the policy gradient of entropy for each observation, leveraging message passing within the hidden Markov models. This gradient computation enables us to have stable and fast convergence. We demonstrate our solution of opacity-enforcement control through a grid world example.","sentences":["The paper studies information-theoretic opacity, an information-flow privacy property, in a setting involving two agents: A planning agent who controls a stochastic system and an observer who partially observes the system states.","The goal of the observer is to infer some secret, represented by a random variable, from its partial observations, while the goal of the planning agent is to make the secret maximally opaque to the observer while achieving a satisfactory total return.","Modeling the stochastic system using a Markov decision process, two classes of opacity properties are considered -- Last-state opacity is to ensure that the observer is uncertain if the last state is in a specific set and initial-state opacity is to ensure that the observer is unsure of the realization of the initial state.","As the measure of opacity, we employ the Shannon conditional entropy capturing the information about the secret revealed by the observable.","Then, we develop primal-dual policy gradient methods for opacity-enforcement planning subject to constraints on total returns.","We propose novel algorithms to compute the policy gradient of entropy for each observation, leveraging message passing within the hidden Markov models.","This gradient computation enables us to have stable and fast convergence.","We demonstrate our solution of opacity-enforcement control through a grid world example."],"url":"http://arxiv.org/abs/2405.00157v1","category":"eess.SY"}
{"created":"2024-04-30 18:43:16","title":"Averting multi-qubit burst errors in surface code magic state factories","abstract":"Fault-tolerant quantum computation relies on the assumption of time-invariant, sufficiently low physical error rates. However, current superconducting quantum computers suffer from frequent disruptive noise events, including cosmic ray impacts and shifting two-level system defects. Several methods have been proposed to mitigate these issues in software, but they add large overheads in terms of physical qubit count, as it is difficult to preserve logical information through burst error events. We focus on mitigating multi-qubit burst errors in magic state factories, which are expected to comprise up to 95% of the space cost of future quantum programs. Our key insight is that magic state factories do not need to preserve logical information over time; once we detect an increase in local physical error rates, we can simply turn off parts of the factory that are affected, re-map the factory to the new chip geometry, and continue operating. This is much more efficient than previous more general methods, and is resilient even under many simultaneous impact events. Using precise physical noise models, we show an efficient ray detection method and evaluate our strategy in different noise regimes. Compared to existing baselines, we find reductions in ray-induced overheads by several orders of magnitude, reducing total qubitcycle cost by geomean 6.5x to 13.9x depending on the noise model. This work reduces the burden on hardware by providing low-overhead software mitigation of these errors.","sentences":["Fault-tolerant quantum computation relies on the assumption of time-invariant, sufficiently low physical error rates.","However, current superconducting quantum computers suffer from frequent disruptive noise events, including cosmic ray impacts and shifting two-level system defects.","Several methods have been proposed to mitigate these issues in software, but they add large overheads in terms of physical qubit count, as it is difficult to preserve logical information through burst error events.","We focus on mitigating multi-qubit burst errors in magic state factories, which are expected to comprise up to 95% of the space cost of future quantum programs.","Our key insight is that magic state factories do not need to preserve logical information over time; once we detect an increase in local physical error rates, we can simply turn off parts of the factory that are affected, re-map the factory to the new chip geometry, and continue operating.","This is much more efficient than previous more general methods, and is resilient even under many simultaneous impact events.","Using precise physical noise models, we show an efficient ray detection method and evaluate our strategy in different noise regimes.","Compared to existing baselines, we find reductions in ray-induced overheads by several orders of magnitude, reducing total qubitcycle cost by geomean 6.5x to 13.9x depending on the noise model.","This work reduces the burden on hardware by providing low-overhead software mitigation of these errors."],"url":"http://arxiv.org/abs/2405.00146v1","category":"quant-ph"}
{"created":"2024-04-30 18:41:36","title":"Greater benefits of deep learning-based computer-aided detection systems for finding small signals in 3D volumetric medical images","abstract":"Purpose: Radiologists are tasked with visually scrutinizing large amounts of data produced by 3D volumetric imaging modalities. Small signals can go unnoticed during the 3d search because they are hard to detect in the visual periphery. Recent advances in machine learning and computer vision have led to effective computer-aided detection (CADe) support systems with the potential to mitigate perceptual errors.   Approach: Sixteen non-expert observers searched through digital breast tomosynthesis (DBT) phantoms and single cross-sectional slices of the DBT phantoms. The 3D/2D searches occurred with and without a convolutional neural network (CNN)-based CADe support system. The model provided observers with bounding boxes superimposed on the image stimuli while they looked for a small microcalcification signal and a large mass signal. Eye gaze positions were recorded and correlated with changes in the area under the ROC curve (AUC).   Results: The CNN-CADe improved the 3D search for the small microcalcification signal (delta AUC = 0.098, p = 0.0002) and the 2D search for the large mass signal (delta AUC = 0.076, p = 0.002). The CNN-CADe benefit in 3D for the small signal was markedly greater than in 2D (delta delta AUC = 0.066, p = 0.035). Analysis of individual differences suggests that those who explored the least with eye movements benefited the most from the CNN-CADe (r = -0.528, p = 0.036). However, for the large signal, the 2D benefit was not significantly greater than the 3D benefit (delta delta AUC = 0.033, p = 0.133).   Conclusion: The CNN-CADe brings unique performance benefits to the 3D (vs. 2D) search of small signals by reducing errors caused by the under-exploration of the volumetric data.","sentences":["Purpose: Radiologists are tasked with visually scrutinizing large amounts of data produced by 3D volumetric imaging modalities.","Small signals can go unnoticed during the 3d search because they are hard to detect in the visual periphery.","Recent advances in machine learning and computer vision have led to effective computer-aided detection (CADe) support systems with the potential to mitigate perceptual errors.   ","Approach: Sixteen non-expert observers searched through digital breast tomosynthesis (DBT) phantoms and single cross-sectional slices of the DBT phantoms.","The 3D/2D searches occurred with and without a convolutional neural network (CNN)-based CADe support system.","The model provided observers with bounding boxes superimposed on the image stimuli while they looked for a small microcalcification signal and a large mass signal.","Eye gaze positions were recorded and correlated with changes in the area under the ROC curve (AUC).   ","Results: The CNN-CADe improved the 3D search for the small microcalcification signal (delta AUC = 0.098, p = 0.0002) and the 2D search for the large mass signal (delta AUC = 0.076, p = 0.002).","The CNN-CADe benefit in 3D for the small signal was markedly greater than in 2D (delta delta AUC = 0.066, p = 0.035).","Analysis of individual differences suggests that those who explored the least with eye movements benefited the most from the CNN-CADe (r = -0.528, p = 0.036).","However, for the large signal, the 2D benefit was not significantly greater than the 3D benefit (delta delta AUC = 0.033, p = 0.133).   ","Conclusion: The CNN-CADe brings unique performance benefits to the 3D (vs. 2D) search of small signals by reducing errors caused by the under-exploration of the volumetric data."],"url":"http://arxiv.org/abs/2405.00144v1","category":"cs.HC"}
{"created":"2024-04-30 18:39:41","title":"Utilizing Machine Learning and 3D Neuroimaging to Predict Hearing Loss: A Comparative Analysis of Dimensionality Reduction and Regression Techniques","abstract":"In this project, we have explored machine learning approaches for predicting hearing loss thresholds on the brain's gray matter 3D images. We have solved the problem statement in two phases. In the first phase, we used a 3D CNN model to reduce high-dimensional input into latent space and decode it into an original image to represent the input in rich feature space. In the second phase, we utilized this model to reduce input into rich features and used these features to train standard machine learning models for predicting hearing thresholds. We have experimented with autoencoders and variational autoencoders in the first phase for dimensionality reduction and explored random forest, XGBoost and multi-layer perceptron for regressing the thresholds. We split the given data set into training and testing sets and achieved an 8.80 range and 22.57 range for PT500 and PT4000 on the test set, respectively. We got the lowest RMSE using multi-layer perceptron among the other models.   Our approach leverages the unique capabilities of VAEs to capture complex, non-linear relationships within high-dimensional neuroimaging data. We rigorously evaluated the models using various metrics, focusing on the root mean squared error (RMSE). The results highlight the efficacy of the multi-layer neural network model, which outperformed other techniques in terms of accuracy. This project advances the application of data mining in medical diagnostics and enhances our understanding of age-related hearing loss through innovative machine-learning frameworks.","sentences":["In this project, we have explored machine learning approaches for predicting hearing loss thresholds on the brain's gray matter 3D images.","We have solved the problem statement in two phases.","In the first phase, we used a 3D CNN model to reduce high-dimensional input into latent space and decode it into an original image to represent the input in rich feature space.","In the second phase, we utilized this model to reduce input into rich features and used these features to train standard machine learning models for predicting hearing thresholds.","We have experimented with autoencoders and variational autoencoders in the first phase for dimensionality reduction and explored random forest, XGBoost and multi-layer perceptron for regressing the thresholds.","We split the given data set into training and testing sets and achieved an 8.80 range and 22.57 range for PT500 and PT4000 on the test set, respectively.","We got the lowest RMSE using multi-layer perceptron among the other models.   ","Our approach leverages the unique capabilities of VAEs to capture complex, non-linear relationships within high-dimensional neuroimaging data.","We rigorously evaluated the models using various metrics, focusing on the root mean squared error (RMSE).","The results highlight the efficacy of the multi-layer neural network model, which outperformed other techniques in terms of accuracy.","This project advances the application of data mining in medical diagnostics and enhances our understanding of age-related hearing loss through innovative machine-learning frameworks."],"url":"http://arxiv.org/abs/2405.00142v1","category":"cs.LG"}
{"created":"2024-04-30 18:37:33","title":"Hydrodynamical simulations of merging galaxy clusters: giant dark matter particle colliders, powered by gravity","abstract":"Terrestrial particle accelerators collide charged particles, then watch the trajectory of outgoing debris - but they cannot manipulate dark matter. Fortunately, dark matter is the main component of galaxy clusters, which are continuously pulled together by gravity. We show that galaxy cluster mergers can be exploited as enormous, natural dark matter colliders. We analyse hydrodynamical simulations of a universe containing self-interacting dark matter (SIDM) in which all particles interact via gravity, and dark matter particles can also scatter off each other via a massive mediator. During cluster collisions, SIDM spreads out and lags behind cluster member galaxies. Individual systems can have quirky dynamics that makes them difficult to interpret. Statistically, however, we find that the mean or median of dark matter's spatial offset in many collisions can be robustly modelled, and is independent of our viewing angle and halo mass even in collisions between unequal-mass systems. If the SIDM cross-section were sigma/m = 0.1cm^2/g = 0.18 barn/GeV, the 'bulleticity' lag would be ~5 percent that of gas due to ram pressure, and could be detected at 95 percent confidence in weak lensing observations of ~100 well-chosen clusters.","sentences":["Terrestrial particle accelerators collide charged particles, then watch the trajectory of outgoing debris - but they cannot manipulate dark matter.","Fortunately, dark matter is the main component of galaxy clusters, which are continuously pulled together by gravity.","We show that galaxy cluster mergers can be exploited as enormous, natural dark matter colliders.","We analyse hydrodynamical simulations of a universe containing self-interacting dark matter (SIDM) in which all particles interact via gravity, and dark matter particles can also scatter off each other via a massive mediator.","During cluster collisions, SIDM spreads out and lags behind cluster member galaxies.","Individual systems can have quirky dynamics that makes them difficult to interpret.","Statistically, however, we find that the mean or median of dark matter's spatial offset in many collisions can be robustly modelled, and is independent of our viewing angle and halo mass even in collisions between unequal-mass systems.","If the SIDM cross-section were sigma/m = 0.1cm^2/g = 0.18 barn/GeV, the 'bulleticity' lag would be ~5 percent that of gas due to ram pressure, and could be detected at 95 percent confidence in weak lensing observations of ~100 well-chosen clusters."],"url":"http://arxiv.org/abs/2405.00140v1","category":"astro-ph.CO"}
{"created":"2024-05-01 17:47:44","title":"Connecting Infinity to Soft Factors","abstract":"In this note we study tree-level scattering amplitudes of gravitons under a natural deformation which in the large $z$ limit can be interpreted either as a $k$-hard-particle limit or as a $(n-k)$-soft-particle limit. When $k=2$ this becomes the standard BCFW deformation while for $k=3$ it leads to the Risager deformation. The hard- to soft-limit map we define motivates a way of computing the leading order behavior of amplitudes for large $z$ directly from soft limits. We check the proposal by applying the $k=3$ and $k=4$ versions to NMHV and N$^2$MHV gravity amplitudes respectively. The former reproduces in a few lines the result recently obtained by using CHY-like techniques in \\cite{BCL}. The N$^2$MHV formula is also remarkably simple and we give support for it using a CHY-like computation. In the $k=2$ case applied to any gravity amplitude, the multiple soft-limit analysis reproduces the correct ${\\cal O}(z^{-2})$ behavior while explicitly showing the source of the mysterious cancellation among Feynman diagrams that tames the behavior from the ${\\cal O}(z^{n-5})$ of individual Feynman diagrams down to the ${\\cal O}(z^{-2})$ of the amplitude.","sentences":["In this note we study tree-level scattering amplitudes of gravitons under a natural deformation which in the large $z$ limit can be interpreted either as a $k$-hard-particle limit or as a $(n-k)$-soft-particle limit.","When $k=2$ this becomes the standard BCFW deformation while for $k=3$ it leads to the Risager deformation.","The hard- to soft-limit map we define motivates a way of computing the leading order behavior of amplitudes for large $z$ directly from soft limits.","We check the proposal by applying the $k=3$ and $k=4$ versions to NMHV and N$^2$MHV gravity amplitudes respectively.","The former reproduces in a few lines the result recently obtained by using CHY-like techniques in \\cite{BCL}.","The N$^2$MHV formula is also remarkably simple and we give support for it using a CHY-like computation.","In the $k=2$ case applied to any gravity amplitude, the multiple soft-limit analysis reproduces the correct ${\\cal O}(z^{-2})$ behavior while explicitly showing the source of the mysterious cancellation among Feynman diagrams that tames the behavior from the ${\\cal O}(z^{n-5})$ of individual Feynman diagrams down to the ${\\cal O}(z^{-2})$ of the amplitude."],"url":"http://arxiv.org/abs/2405.00660v1","category":"hep-th"}
{"created":"2024-05-01 17:07:31","title":"Vacancy-mediated transport and segregation tendencies of solutes in FCC nickel under diffusional creep: A density functional theory study","abstract":"The Nabarro-Herring (N-H) diffusional creep theory postulates the vacancy-mediated transport of atoms under a stress gradient as the creep mechanism under low-stress and high-temperature conditions. In multicomponent alloys, we premise that this stress-assisted flow of vacancies to and from grain boundaries will produce elemental segregation. An observation of such segregation, validated with theoretical predictions, can provide the necessary experimental evidence for the occurrence of N-H creep. Theoretical calculations of the segregation tendencies via analyzing the dominant solute diffusion mechanisms and the difference in diffusivities of the elements are therefore essential. To this end, this study applies density functional theory calculations of migration barriers and solute-vacancy binding energies as input to the self-consistent mean field theory to assess the vacancy-mediated diffusion mechanisms, transport coefficients, and segregation tendencies of Co, Cr, Mo, Re, Ta, and W solutes in face-centered cubic Ni. We find Co, Re, and W to be slow diffusers at high temperatures and Cr, Mo, and Ta to be fast diffusers. Further analysis shows that the slow diffusers tend to always enrich at vacancy sinks over a wide range of temperatures. In contrast, the fast diffusers show a transition from depletion to enrichment as the temperature lowers. Furthermore, our analysis of the segregation tendencies under tensile hydrostatic strains shows that slow diffusers are largely unaffected by the strain and favor enrichment. On the other hand, the fast diffusers exhibit high sensitivity to strain and their segregation tendency can transition from depletion to enrichment at a given temperature. The transport coefficients calculated in this work are expected to serve as input to mesoscale microstructure models to provide a more rigorous assessment of solute segregation under N-H creep conditions.","sentences":["The Nabarro-Herring (N-H) diffusional creep theory postulates the vacancy-mediated transport of atoms under a stress gradient as the creep mechanism under low-stress and high-temperature conditions.","In multicomponent alloys, we premise that this stress-assisted flow of vacancies to and from grain boundaries will produce elemental segregation.","An observation of such segregation, validated with theoretical predictions, can provide the necessary experimental evidence for the occurrence of N-H creep.","Theoretical calculations of the segregation tendencies via analyzing the dominant solute diffusion mechanisms and the difference in diffusivities of the elements are therefore essential.","To this end, this study applies density functional theory calculations of migration barriers and solute-vacancy binding energies as input to the self-consistent mean field theory to assess the vacancy-mediated diffusion mechanisms, transport coefficients, and segregation tendencies of Co, Cr, Mo, Re, Ta, and W solutes in face-centered cubic Ni.","We find Co, Re, and W to be slow diffusers at high temperatures and Cr, Mo, and Ta to be fast diffusers.","Further analysis shows that the slow diffusers tend to always enrich at vacancy sinks over a wide range of temperatures.","In contrast, the fast diffusers show a transition from depletion to enrichment as the temperature lowers.","Furthermore, our analysis of the segregation tendencies under tensile hydrostatic strains shows that slow diffusers are largely unaffected by the strain and favor enrichment.","On the other hand, the fast diffusers exhibit high sensitivity to strain and their segregation tendency can transition from depletion to enrichment at a given temperature.","The transport coefficients calculated in this work are expected to serve as input to mesoscale microstructure models to provide a more rigorous assessment of solute segregation under N-H creep conditions."],"url":"http://arxiv.org/abs/2405.00639v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-01 15:52:35","title":"Pretzel monoids","abstract":"We introduce an interesting class of left adequate monoids which we call pretzel monoids. These, on the one hand, are monoids of birooted graphs with respect to a natural `glue-and-fold' operation, and on the other hand, are shown to be defined in the category of left adequate monoids by a natural class of presentations. They are also shown to be the free idempotent-pure expansions of right cancellative monoids, making them, in some sense, the left adequate analogues of Margolis-Meakin expansions for inverse monoids. The construction recovers the second author's geometric model of free left adequate monoids when the right cancellative monoid is free.","sentences":["We introduce an interesting class of left adequate monoids which we call pretzel monoids.","These, on the one hand, are monoids of birooted graphs with respect to a natural `glue-and-fold' operation, and on the other hand, are shown to be defined in the category of left adequate monoids by a natural class of presentations.","They are also shown to be the free idempotent-pure expansions of right cancellative monoids, making them, in some sense, the left adequate analogues of Margolis-Meakin expansions for inverse monoids.","The construction recovers the second author's geometric model of free left adequate monoids when the right cancellative monoid is free."],"url":"http://arxiv.org/abs/2405.00589v1","category":"math.RA"}
{"created":"2024-05-01 15:38:35","title":"Periodic nonlinear Schr\u00f6dinger equation with distributional potential and invariant measures","abstract":"In this paper, we continue some investigations on the periodic NLSE started by Lebowitz, Rose and Speer and by Bourgain with the addition of a distributional multiplicative potential. We prove that the equation is globally wellposed for a set of data of full normalized Gibbs measure, after suitable truncation in the focusing case. The set and the measure are invariant under the flow. The main ingredients used are Strichartz estimates on periodic NLS with distributional potential to obtain local well-posedness for low regularity initial data.","sentences":["In this paper, we continue some investigations on the periodic NLSE started by Lebowitz, Rose and Speer and by Bourgain with the addition of a distributional multiplicative potential.","We prove that the equation is globally wellposed for a set of data of full normalized Gibbs measure, after suitable truncation in the focusing case.","The set and the measure are invariant under the flow.","The main ingredients used are Strichartz estimates on periodic NLS with distributional potential to obtain local well-posedness for low regularity initial data."],"url":"http://arxiv.org/abs/2405.00583v1","category":"math.AP"}
{"created":"2024-05-01 15:17:40","title":"Remote Sensing Data Assimilation with a Chained Hydrologic-hydraulic Model for Flood Forecasting","abstract":"A chained hydrologic-hydraulic model is implemented using predicted runoff from a large-scale hydrologic model (namely ISBA-CTRIP) as inputs to local hydrodynamic models (TELEMAC-2D) to issue forecasts of water level and flood extent. The uncertainties in the hydrological forcing and in friction parameters are reduced by an Ensemble Kalman Filter that jointly assimilates in-situ water levels and flood extent maps derived from remote sensing observations. The data assimilation framework is cycled in a real-time forecasting configuration. A cycle consists of a reanalysis and a forecast phase. Over the analysis, observations up to the present are assimilated. An ensemble is then initialized from the last analyzed states and issued forecasts for next 36 hr. Three strategies of forcing data for this forecast are investigated: (i) using CTRIP runoff for reanalysis and forecast, (ii) using observed discharge for analysis, then CTRIP runoff for forecast and (iii) using observed discharge for reanalysis and keep a persistent discharge value for forecast. It was shown that the data assimilation strategy provides a reliable reanalysis in hindcast mode. The combination of observed discharge and CTRIP runoff provides the most accurate results. For all strategies, the quality of the forecast decreases as the lead time increases. When the errors in CTRIP forcing are non-stationary, the forecast capability may be reduced. This work demonstrates that the forcing provided by a hydrologic model, while imperfect, can be efficiently used as input to a hydraulic model to issue reanalysis and forecasts, thanks to the assimilation of in-situ and remote sensing observations.","sentences":["A chained hydrologic-hydraulic model is implemented using predicted runoff from a large-scale hydrologic model (namely ISBA-CTRIP) as inputs to local hydrodynamic models (TELEMAC-2D) to issue forecasts of water level and flood extent.","The uncertainties in the hydrological forcing and in friction parameters are reduced by an Ensemble Kalman Filter that jointly assimilates in-situ water levels and flood extent maps derived from remote sensing observations.","The data assimilation framework is cycled in a real-time forecasting configuration.","A cycle consists of a reanalysis and a forecast phase.","Over the analysis, observations up to the present are assimilated.","An ensemble is then initialized from the last analyzed states and issued forecasts for next 36 hr.","Three strategies of forcing data for this forecast are investigated: (i) using CTRIP runoff for reanalysis and forecast, (ii) using observed discharge for analysis, then CTRIP runoff for forecast and (iii) using observed discharge for reanalysis and keep a persistent discharge value for forecast.","It was shown that the data assimilation strategy provides a reliable reanalysis in hindcast mode.","The combination of observed discharge and CTRIP runoff provides the most accurate results.","For all strategies, the quality of the forecast decreases as the lead time increases.","When the errors in CTRIP forcing are non-stationary, the forecast capability may be reduced.","This work demonstrates that the forcing provided by a hydrologic model, while imperfect, can be efficiently used as input to a hydraulic model to issue reanalysis and forecasts, thanks to the assimilation of in-situ and remote sensing observations."],"url":"http://arxiv.org/abs/2405.00567v1","category":"eess.IV"}
{"created":"2024-05-01 15:07:47","title":"A Taste for Variety","abstract":"A decision maker repeatedly chooses one of a finite set of actions. In each period, the decision maker's payoff depends on fixed basic payoff of the chosen action and the frequency with which the action has been chosen in the past. We analyze optimal strategies associated with three types of evaluations of infinite payoffs: discounted present value, the limit inferior, and the limit superior of the partial averages. We show that when the first two are the evaluation schemes, a stationary strategy can always achieve the best possible outcome. However, for the latter evaluation scheme, a stationary strategy can achieve the best outcome only if all actions that are chosen with strictly positive frequency by an optimal stationary strategy have the same basic payoff.","sentences":["A decision maker repeatedly chooses one of a finite set of actions.","In each period, the decision maker's payoff depends on fixed basic payoff of the chosen action and the frequency with which the action has been chosen in the past.","We analyze optimal strategies associated with three types of evaluations of infinite payoffs: discounted present value, the limit inferior, and the limit superior of the partial averages.","We show that when the first two are the evaluation schemes, a stationary strategy can always achieve the best possible outcome.","However, for the latter evaluation scheme, a stationary strategy can achieve the best outcome only if all actions that are chosen with strictly positive frequency by an optimal stationary strategy have the same basic payoff."],"url":"http://arxiv.org/abs/2405.00561v1","category":"econ.TH"}
{"created":"2024-05-01 14:04:48","title":"Byzantine-Secure Relying Party for Resilient RPKI","abstract":"To protect against prefix hijacks, Resource Public Key Infrastructure (RPKI) has been standardized. To enjoy the security guarantees of RPKI validation, networks need to install a new component, the relying party validator, which fetches and validates RPKI objects and provides them to border routers. However, recent work shows that relying parties experience failures when retrieving RPKI objects and are vulnerable to attacks, all of which can disable RPKI validation. Therefore even the few adopters are not necessarily secure.   We make the first proposal that significantly improves the resilience and security of RPKI. We develop BRP, a Byzantine-Secure relying party implementation. In BRP the relying party nodes redundantly validate RPKI objects and reach a global consensus through voting. BRP provides an RPKI equivalent of public DNS, removing the need for networks to install, operate, and upgrade their own relying party instances while avoiding the need to trust operators of BRP nodes.   We show through simulations and experiments that BRP, as an intermediate RPKI service, results in less load on RPKI publication points and a robust output despite RPKI repository failures, jitter, and attacks. We engineer BRP to be fully backward compatible and readily deployable - it does not require any changes to the border routers and the RPKI repositories.   We demonstrate that BRP can protect many networks transparently, with either a decentralized or centralized deployment. BRP can be set up as a network of decentralized volunteer deployments, similarly to NTP and TOR, where different operators participate in the peering process with their node, and provide resilient and secure relying party validation to the Internet. BRP can also be hosted by a single operator as a centralized service, e.g., on one cloud or CDN, and provides RPKI validation benefits even when hosted on a single network.","sentences":["To protect against prefix hijacks, Resource Public Key Infrastructure (RPKI) has been standardized.","To enjoy the security guarantees of RPKI validation, networks need to install a new component, the relying party validator, which fetches and validates RPKI objects and provides them to border routers.","However, recent work shows that relying parties experience failures when retrieving RPKI objects and are vulnerable to attacks, all of which can disable RPKI validation.","Therefore even the few adopters are not necessarily secure.   ","We make the first proposal that significantly improves the resilience and security of RPKI.","We develop BRP, a Byzantine-Secure relying party implementation.","In BRP the relying party nodes redundantly validate RPKI objects and reach a global consensus through voting.","BRP provides an RPKI equivalent of public DNS, removing the need for networks to install, operate, and upgrade their own relying party instances while avoiding the need to trust operators of BRP nodes.   ","We show through simulations and experiments that BRP, as an intermediate RPKI service, results in less load on RPKI publication points and a robust output despite RPKI repository failures, jitter, and attacks.","We engineer BRP to be fully backward compatible and readily deployable - it does not require any changes to the border routers and the RPKI repositories.   ","We demonstrate that BRP can protect many networks transparently, with either a decentralized or centralized deployment.","BRP can be set up as a network of decentralized volunteer deployments, similarly to NTP and TOR, where different operators participate in the peering process with their node, and provide resilient and secure relying party validation to the Internet.","BRP can also be hosted by a single operator as a centralized service, e.g., on one cloud or CDN, and provides RPKI validation benefits even when hosted on a single network."],"url":"http://arxiv.org/abs/2405.00531v1","category":"cs.CR"}
{"created":"2024-05-01 14:01:48","title":"High-Order Block Toeplitz Inner-Bordering method for solving the Gelfand-Levitan-Marchenko equation","abstract":"We propose a high precision algorithm for solving the Gelfand-Levitan-Marchenko equation. The algorithm is based on the block version of the Toeplitz Inner-Bordering algorithm of Levinson's type. To approximate integrals, we use the high-precision one-sided and two-sided Gregory quadrature formulas. Also we use the Woodbury formula to construct a computational algorithm. This makes it possible to use the almost Toeplitz structure of the matrices for the fast calculations.","sentences":["We propose a high precision algorithm for solving the Gelfand-Levitan-Marchenko equation.","The algorithm is based on the block version of the Toeplitz Inner-Bordering algorithm of Levinson's type.","To approximate integrals, we use the high-precision one-sided and two-sided Gregory quadrature formulas.","Also we use the Woodbury formula to construct a computational algorithm.","This makes it possible to use the almost Toeplitz structure of the matrices for the fast calculations."],"url":"http://arxiv.org/abs/2405.00529v1","category":"math.NA"}
{"created":"2024-05-01 14:00:46","title":"Isospin violation effect and three-body decays of the $T_{cc}^{+}$ state","abstract":"In this work, we make a study of $T_{cc}^+$ state observed by the LHCb collaboration in 2021. In obtaining the effective potentials using the One-Boson-Exchange Potential Model we use an exponential form factor, and find that in the short and medium range, the contributions of the $\\pi$, $\\rho$ and $\\omega$ exchanges are comparable while in the long range the pion-exchange contribution is dominant. Based on the assumption that $T_{cc}^+$ is a loosely bound state of $D^*D$, we focus on its three-body decay using the meson-exchange method. Considering that the difference between the thresholds of $D^{*+}D^0$ and $D^{*0}D^+$ is even larger than the binding energy of $T_{cc}^+$, the isospin-breaking effect is amplified by the small binding energy of $T_{cc}^+$. Explicitly including such an isospin-breaking effect we obtain, by solving the Schr\\\"{o}dinger equation, that the probability of the isoscalar component is about $91\\%$ while that of the isovector component is around $9\\%$ for $T_{cc}^+$. Using the experimental value of the mass of $T_{cc}^+$ as an input, we obtain the wave function of $T_{cc}^+$ and further obtain its width via the three-body hadronic as well as the radiative decays. The total width we obtain is in agreement with the experimental value of the LHCb measurement with a unitarised Breit-Wigner profile. Conversely, the current results support the conclusion that $T_{cc}^+$ is a hadronic molecule of $D^*D$.","sentences":["In this work, we make a study of $T_{cc}^+$ state observed by the LHCb collaboration in 2021.","In obtaining the effective potentials using the One-Boson-Exchange Potential Model we use an exponential form factor, and find that in the short and medium range, the contributions of the $\\pi$, $\\rho$ and $\\omega$ exchanges are comparable while in the long range the pion-exchange contribution is dominant.","Based on the assumption that $T_{cc}^+$ is a loosely bound state of $D^*D$, we focus on its three-body decay using the meson-exchange method.","Considering that the difference between the thresholds of $D^{*+}D^0$ and $D^{*0}D^+$ is even larger than the binding energy of $T_{cc}^+$, the isospin-breaking effect is amplified by the small binding energy of $T_{cc}^+$. Explicitly including such an isospin-breaking effect we obtain, by solving the Schr\\\"{o}dinger equation, that the probability of the isoscalar component is about $91\\%$ while that of the isovector component is around $9\\%$ for $T_{cc}^+$. Using the experimental value of the mass of $T_{cc}^+$ as an input, we obtain the wave function of $T_{cc}^+$ and further obtain its width via the three-body hadronic as well as the radiative decays.","The total width we obtain is in agreement with the experimental value of the LHCb measurement with a unitarised Breit-Wigner profile.","Conversely, the current results support the conclusion that $T_{cc}^+$ is a hadronic molecule of $D^*D$."],"url":"http://arxiv.org/abs/2405.00525v1","category":"hep-ph"}
{"created":"2024-05-01 13:43:17","title":"Lorentzian polynomials and the independence sequences of graphs","abstract":"We study the multivariate independence polynomials of graphs and the log-concavity of the coefficients of their univariate restrictions. Let $R_{W_4}$ be the operator defined on simple and undirected graphs which replaces each edge with a caterpillar of size $4$. We prove that all graphs in the image of $R_{W_4}$ are what we call pre-Lorentzian, that is, their multivariate independence polynomial becomes Lorentzian after appropriate manipulations. In particular, as pre-Lorentzian graphs have log-concave (and therefore unimodal) independence sequences, our result makes progress on a conjecture of Alavi, Malde, Schwenk and Erd\\H{o}s which asks if the independence sequence of trees or forests is unimodal.","sentences":["We study the multivariate independence polynomials of graphs and the log-concavity of the coefficients of their univariate restrictions.","Let $R_{W_4}$ be the operator defined on simple and undirected graphs which replaces each edge with a caterpillar of size $4$. We prove that all graphs in the image of $R_{W_4}$ are what we call pre-Lorentzian, that is, their multivariate independence polynomial becomes Lorentzian after appropriate manipulations.","In particular, as pre-Lorentzian graphs have log-concave (and therefore unimodal) independence sequences, our result makes progress on a conjecture of Alavi, Malde, Schwenk and Erd\\H{o}s which asks if the independence sequence of trees or forests is unimodal."],"url":"http://arxiv.org/abs/2405.00511v1","category":"math.CO"}
{"created":"2024-05-01 13:41:38","title":"Noiseless Loss Suppression for Entanglement Distribution","abstract":"Recent work by Mi\\v{c}uda et al. (arXiv:1206.2852v1) suggests that pairing noiseless amplification with noiseless attenuation can conditionally suppress loss terms in the direct transmission of quantum states. Here we extend this work to entangled states: first, we explore bipartite states, specifically the two-mode squeezed vacuum (TMSV) and NOON states; and second, we examine M-partite states, concentrating on W and Greenberger-Horne-Zeilinger (GHZ) states. In analogy with the original proposal, our results demonstrate that in each case under consideration, a correct combination of attenuation and amplification techniques before and after transmission through a pure loss channel can restore the initial quantum state. However, we find that for both W and NOON states, the noiseless attenuation is redundant and not required to achieve loss term suppression. This work clarifies the role of noiseless attenuation when paired with noiseless amplification for entanglement distribution and provides an operational example of how GHZ and W state entanglement differs.","sentences":["Recent work by Mi\\v{c}uda et al. (arXiv:1206.2852v1) suggests that pairing noiseless amplification with noiseless attenuation can conditionally suppress loss terms in the direct transmission of quantum states.","Here we extend this work to entangled states: first, we explore bipartite states, specifically the two-mode squeezed vacuum (TMSV) and NOON states; and second, we examine M-partite states, concentrating on W and Greenberger-Horne-Zeilinger (GHZ) states.","In analogy with the original proposal, our results demonstrate that in each case under consideration, a correct combination of attenuation and amplification techniques before and after transmission through a pure loss channel can restore the initial quantum state.","However, we find that for both W and NOON states, the noiseless attenuation is redundant and not required to achieve loss term suppression.","This work clarifies the role of noiseless attenuation when paired with noiseless amplification for entanglement distribution and provides an operational example of how GHZ and W state entanglement differs."],"url":"http://arxiv.org/abs/2405.00510v1","category":"quant-ph"}
{"created":"2024-05-01 12:23:16","title":"A Comprehensive Survey of Dynamic Graph Neural Networks: Models, Frameworks, Benchmarks, Experiments and Challenges","abstract":"Dynamic Graph Neural Networks (GNNs) combine temporal information with GNNs to capture structural, temporal, and contextual relationships in dynamic graphs simultaneously, leading to enhanced performance in various applications. As the demand for dynamic GNNs continues to grow, numerous models and frameworks have emerged to cater to different application needs. There is a pressing need for a comprehensive survey that evaluates the performance, strengths, and limitations of various approaches in this domain. This paper aims to fill this gap by offering a thorough comparative analysis and experimental evaluation of dynamic GNNs. It covers 81 dynamic GNN models with a novel taxonomy, 12 dynamic GNN training frameworks, and commonly used benchmarks. We also conduct experimental results from testing representative nine dynamic GNN models and three frameworks on six standard graph datasets. Evaluation metrics focus on convergence accuracy, training efficiency, and GPU memory usage, enabling a thorough comparison of performance across various models and frameworks. From the analysis and evaluation results, we identify key challenges and offer principles for future research to enhance the design of models and frameworks in the dynamic GNNs field.","sentences":["Dynamic Graph Neural Networks (GNNs) combine temporal information with GNNs to capture structural, temporal, and contextual relationships in dynamic graphs simultaneously, leading to enhanced performance in various applications.","As the demand for dynamic GNNs continues to grow, numerous models and frameworks have emerged to cater to different application needs.","There is a pressing need for a comprehensive survey that evaluates the performance, strengths, and limitations of various approaches in this domain.","This paper aims to fill this gap by offering a thorough comparative analysis and experimental evaluation of dynamic GNNs.","It covers 81 dynamic GNN models with a novel taxonomy, 12 dynamic GNN training frameworks, and commonly used benchmarks.","We also conduct experimental results from testing representative nine dynamic GNN models and three frameworks on six standard graph datasets.","Evaluation metrics focus on convergence accuracy, training efficiency, and GPU memory usage, enabling a thorough comparison of performance across various models and frameworks.","From the analysis and evaluation results, we identify key challenges and offer principles for future research to enhance the design of models and frameworks in the dynamic GNNs field."],"url":"http://arxiv.org/abs/2405.00476v1","category":"cs.LG"}
{"created":"2024-05-01 12:15:48","title":"Floquet geometric entangling gates in ground-state manifolds of Rydberg atoms","abstract":"We propose an extension of the Floquet theory for constructing quantum entangling gates in ground-state manifolds of Rydberg atoms. By dynamically controlling periodically modulating the Rabi frequencies of transitions between ground and Rydberg states of atoms, error-resilient two-qubit entangling gates can be implemented in the regime of Rydberg blockade. According to different degrees of Floquet theory utilization, the fidelity of the resulting controlled gates surpasses that of the original reference. Our method only uses encoding in the ground states, and compared to the original scheme using Rydberg state for encoding, it is less susceptible to environmental interference, making it more practical to implement. Therefore, our approach may have broader applications or potential for further expansion of geometric quantum computation with neutral atoms.","sentences":["We propose an extension of the Floquet theory for constructing quantum entangling gates in ground-state manifolds of Rydberg atoms.","By dynamically controlling periodically modulating the Rabi frequencies of transitions between ground and Rydberg states of atoms, error-resilient two-qubit entangling gates can be implemented in the regime of Rydberg blockade.","According to different degrees of Floquet theory utilization, the fidelity of the resulting controlled gates surpasses that of the original reference.","Our method only uses encoding in the ground states, and compared to the original scheme using Rydberg state for encoding, it is less susceptible to environmental interference, making it more practical to implement.","Therefore, our approach may have broader applications or potential for further expansion of geometric quantum computation with neutral atoms."],"url":"http://arxiv.org/abs/2405.00471v1","category":"quant-ph"}
{"created":"2024-05-01 11:16:02","title":"Robust Semi-supervised Learning via $f$-Divergence and $\u03b1$-R\u00e9nyi Divergence","abstract":"This paper investigates a range of empirical risk functions and regularization methods suitable for self-training methods in semi-supervised learning. These approaches draw inspiration from various divergence measures, such as $f$-divergences and $\\alpha$-R\\'enyi divergences. Inspired by the theoretical foundations rooted in divergences, i.e., $f$-divergences and $\\alpha$-R\\'enyi divergence, we also provide valuable insights to enhance the understanding of our empirical risk functions and regularization techniques. In the pseudo-labeling and entropy minimization techniques as self-training methods for effective semi-supervised learning, the self-training process has some inherent mismatch between the true label and pseudo-label (noisy pseudo-labels) and some of our empirical risk functions are robust, concerning noisy pseudo-labels. Under some conditions, our empirical risk functions demonstrate better performance when compared to traditional self-training methods.","sentences":["This paper investigates a range of empirical risk functions and regularization methods suitable for self-training methods in semi-supervised learning.","These approaches draw inspiration from various divergence measures, such as $f$-divergences and $\\alpha$-R\\'enyi divergences.","Inspired by the theoretical foundations rooted in divergences, i.e., $f$-divergences and $\\alpha$-R\\'enyi divergence, we also provide valuable insights to enhance the understanding of our empirical risk functions and regularization techniques.","In the pseudo-labeling and entropy minimization techniques as self-training methods for effective semi-supervised learning, the self-training process has some inherent mismatch between the true label and pseudo-label (noisy pseudo-labels) and some of our empirical risk functions are robust, concerning noisy pseudo-labels.","Under some conditions, our empirical risk functions demonstrate better performance when compared to traditional self-training methods."],"url":"http://arxiv.org/abs/2405.00454v1","category":"cs.LG"}
{"created":"2024-05-01 11:12:08","title":"Predictive Accuracy-Based Active Learning for Medical Image Segmentation","abstract":"Active learning is considered a viable solution to alleviate the contradiction between the high dependency of deep learning-based segmentation methods on annotated data and the expensive pixel-level annotation cost of medical images. However, most existing methods suffer from unreliable uncertainty assessment and the struggle to balance diversity and informativeness, leading to poor performance in segmentation tasks. In response, we propose an efficient Predictive Accuracy-based Active Learning (PAAL) method for medical image segmentation, first introducing predictive accuracy to define uncertainty. Specifically, PAAL mainly consists of an Accuracy Predictor (AP) and a Weighted Polling Strategy (WPS). The former is an attached learnable module that can accurately predict the segmentation accuracy of unlabeled samples relative to the target model with the predicted posterior probability. The latter provides an efficient hybrid querying scheme by combining predicted accuracy and feature representation, aiming to ensure the uncertainty and diversity of the acquired samples. Extensive experiment results on multiple datasets demonstrate the superiority of PAAL. PAAL achieves comparable accuracy to fully annotated data while reducing annotation costs by approximately 50% to 80%, showcasing significant potential in clinical applications. The code is available at https://github.com/shijun18/PAAL-MedSeg.","sentences":["Active learning is considered a viable solution to alleviate the contradiction between the high dependency of deep learning-based segmentation methods on annotated data and the expensive pixel-level annotation cost of medical images.","However, most existing methods suffer from unreliable uncertainty assessment and the struggle to balance diversity and informativeness, leading to poor performance in segmentation tasks.","In response, we propose an efficient Predictive Accuracy-based Active Learning (PAAL) method for medical image segmentation, first introducing predictive accuracy to define uncertainty.","Specifically, PAAL mainly consists of an Accuracy Predictor (AP) and a Weighted Polling Strategy (WPS).","The former is an attached learnable module that can accurately predict the segmentation accuracy of unlabeled samples relative to the target model with the predicted posterior probability.","The latter provides an efficient hybrid querying scheme by combining predicted accuracy and feature representation, aiming to ensure the uncertainty and diversity of the acquired samples.","Extensive experiment results on multiple datasets demonstrate the superiority of PAAL.","PAAL achieves comparable accuracy to fully annotated data while reducing annotation costs by approximately 50% to 80%, showcasing significant potential in clinical applications.","The code is available at https://github.com/shijun18/PAAL-MedSeg."],"url":"http://arxiv.org/abs/2405.00452v1","category":"cs.CV"}
{"created":"2024-05-01 10:26:08","title":"Continuous sPatial-Temporal Deformable Image Registration (CPT-DIR) for motion modelling in radiotherapy: beyond classic voxel-based methods","abstract":"Background and purpose: Deformable image registration (DIR) is a crucial tool in radiotherapy for extracting and modelling organ motion. However, when significant changes and sliding boundaries are present, it faces compromised accuracy and uncertainty, determining the subsequential contour propagation and dose accumulation procedures. Materials and methods: We propose an implicit neural representation (INR)-based approach modelling motion continuously in both space and time, named Continues-sPatial-Temporal DIR (CPT-DIR). This method uses a multilayer perception (MLP) network to map 3D coordinate (x,y,z) to its corresponding velocity vector (vx,vy,vz). The displacement vectors (dx,dy,dz) are then calculated by integrating velocity vectors over time. The MLP's parameters can rapidly adapt to new cases without pre-training, enhancing optimisation. The DIR's performance was tested on the DIR-Lab dataset of 10 lung 4DCT cases, using metrics of landmark accuracy (TRE), contour conformity (Dice) and image similarity (MAE). Results: The proposed CPT-DIR can reduce landmark TRE from 2.79mm to 0.99mm, outperforming B-splines' results for all cases. The MAE of the whole-body region improves from 35.46HU to 28.99HU. Furthermore, CPT-DIR surpasses B-splines for accuracy in the sliding boundary region, lowering MAE and increasing Dice coefficients for the ribcage from 65.65HU and 90.41% to 42.04HU and 90.56%, versus 75.40HU and 89.30% without registration. Meanwhile, CPT-DIR offers significant speed advantages, completing in under 15 seconds compared to a few minutes with the conventional B-splines method. Conclusion: Leveraging the continuous representations, the CPT-DIR method significantly enhances registration accuracy, automation and speed, outperforming traditional B-splines in landmark and contour precision, particularly in the challenging areas.","sentences":["Background and purpose: Deformable image registration (DIR) is a crucial tool in radiotherapy for extracting and modelling organ motion.","However, when significant changes and sliding boundaries are present, it faces compromised accuracy and uncertainty, determining the subsequential contour propagation and dose accumulation procedures.","Materials and methods: We propose an implicit neural representation (INR)-based approach modelling motion continuously in both space and time, named Continues-sPatial-Temporal DIR (CPT-DIR).","This method uses a multilayer perception (MLP) network to map 3D coordinate (x,y,z) to its corresponding velocity vector (vx,vy,vz).","The displacement vectors (dx,dy,dz) are then calculated by integrating velocity vectors over time.","The MLP's parameters can rapidly adapt to new cases without pre-training, enhancing optimisation.","The DIR's performance was tested on the DIR-Lab dataset of 10 lung 4DCT cases, using metrics of landmark accuracy (TRE), contour conformity (Dice) and image similarity (MAE).","Results:","The proposed CPT-DIR can reduce landmark TRE from 2.79mm to 0.99mm, outperforming B-splines' results for all cases.","The MAE of the whole-body region improves from 35.46HU to 28.99HU.","Furthermore, CPT-DIR surpasses B-splines for accuracy in the sliding boundary region, lowering MAE and increasing Dice coefficients for the ribcage from 65.65HU and 90.41% to 42.04HU and 90.56%, versus 75.40HU and 89.30% without registration.","Meanwhile, CPT-DIR offers significant speed advantages, completing in under 15 seconds compared to a few minutes with the conventional B-splines method.","Conclusion: Leveraging the continuous representations, the CPT-DIR method significantly enhances registration accuracy, automation and speed, outperforming traditional B-splines in landmark and contour precision, particularly in the challenging areas."],"url":"http://arxiv.org/abs/2405.00430v1","category":"physics.med-ph"}
{"created":"2024-05-01 10:05:19","title":"Optimal Bias-Correction and Valid Inference in High-Dimensional Ridge Regression: A Closed-Form Solution","abstract":"Ridge regression is an indispensable tool in big data econometrics but suffers from bias issues affecting both statistical efficiency and scalability. We introduce an iterative strategy to correct the bias effectively when the dimension $p$ is less than the sample size $n$. For $p>n$, our method optimally reduces the bias to a level unachievable through linear transformations of the response. We employ a Ridge-Screening (RS) method to handle the remaining bias when $p>n$, creating a reduced model suitable for bias-correction. Under certain conditions, the selected model nests the true one, making RS a novel variable selection approach. We establish the asymptotic properties and valid inferences of our de-biased ridge estimators for both $p< n$ and $p>n$, where $p$ and $n$ may grow towards infinity, along with the number of iterations. Our method is validated using simulated and real-world data examples, providing a closed-form solution to bias challenges in ridge regression inferences.","sentences":["Ridge regression is an indispensable tool in big data econometrics but suffers from bias issues affecting both statistical efficiency and scalability.","We introduce an iterative strategy to correct the bias effectively when the dimension $p$ is less than the sample size $n$. For $p>n$, our method optimally reduces the bias to a level unachievable through linear transformations of the response.","We employ a Ridge-Screening (RS) method to handle the remaining bias when $p>n$, creating a reduced model suitable for bias-correction.","Under certain conditions, the selected model nests the true one, making RS a novel variable selection approach.","We establish the asymptotic properties and valid inferences of our de-biased ridge estimators for both $p< n$ and $p>n$, where $p$ and $n$ may grow towards infinity, along with the number of iterations.","Our method is validated using simulated and real-world data examples, providing a closed-form solution to bias challenges in ridge regression inferences."],"url":"http://arxiv.org/abs/2405.00424v1","category":"econ.EM"}
{"created":"2024-05-01 09:59:40","title":"On the Incompressible Limit of Current-Vortex Sheets with or without Surface Tension","abstract":"This is the second part of the two-paper sequence, which aims to present a comprehensive study for compressible current-vortex sheets with or without surface tension in ideal compressible magnetohydrodynamics (MHD). The results of this paper are two-fold: First, we establish the zero-surface-tension limit of compressible current-vortex sheets under certain stability conditions on the free interface; Second, when the two-phase flows are isentropic and the density functions converge to the same constant as Mach number goes to zero, we can drop the boundedness assumption (with respect to Mach number) on high-order time derivatives by combining the paradifferential approach applied to the evolution equation of the free interface, the structure of wave equations for the total pressure and the anisotropic Sobolev spaces with suitable weights of Mach number. To our knowledge, this is the first result that rigorously justifies the incompressible limit for both compressible vortex sheets and free-surface ideal MHD flows.","sentences":["This is the second part of the two-paper sequence, which aims to present a comprehensive study for compressible current-vortex sheets with or without surface tension in ideal compressible magnetohydrodynamics (MHD).","The results of this paper are two-fold: First, we establish the zero-surface-tension limit of compressible current-vortex sheets under certain stability conditions on the free interface; Second, when the two-phase flows are isentropic and the density functions converge to the same constant as Mach number goes to zero, we can drop the boundedness assumption (with respect to Mach number) on high-order time derivatives by combining the paradifferential approach applied to the evolution equation of the free interface, the structure of wave equations for the total pressure and the anisotropic Sobolev spaces with suitable weights of Mach number.","To our knowledge, this is the first result that rigorously justifies the incompressible limit for both compressible vortex sheets and free-surface ideal MHD flows."],"url":"http://arxiv.org/abs/2405.00421v1","category":"math.AP"}
{"created":"2024-05-01 09:34:42","title":"UCB-driven Utility Function Search for Multi-objective Reinforcement Learning","abstract":"In Multi-objective Reinforcement Learning (MORL) agents are tasked with optimising decision-making behaviours that trade-off between multiple, possibly conflicting, objectives. MORL based on decomposition is a family of solution methods that employ a number of utility functions to decompose the multi-objective problem into individual single-objective problems solved simultaneously in order to approximate a Pareto front of policies. We focus on the case of linear utility functions parameterised by weight vectors w. We introduce a method based on Upper Confidence Bound to efficiently search for the most promising weight vectors during different stages of the learning process, with the aim of maximising the hypervolume of the resulting Pareto front. The proposed method is shown to outperform various MORL baselines on Mujoco benchmark problems across different random seeds. The code is online at: https://github.com/SYCAMORE-1/ucb-MOPPO.","sentences":["In Multi-objective Reinforcement Learning (MORL) agents are tasked with optimising decision-making behaviours that trade-off between multiple, possibly conflicting, objectives.","MORL based on decomposition is a family of solution methods that employ a number of utility functions to decompose the multi-objective problem into individual single-objective problems solved simultaneously in order to approximate a Pareto front of policies.","We focus on the case of linear utility functions parameterised by weight vectors w.","We introduce a method based on Upper Confidence Bound to efficiently search for the most promising weight vectors during different stages of the learning process, with the aim of maximising the hypervolume of the resulting Pareto front.","The proposed method is shown to outperform various MORL baselines on Mujoco benchmark problems across different random seeds.","The code is online at: https://github.com/SYCAMORE-1/ucb-MOPPO."],"url":"http://arxiv.org/abs/2405.00410v1","category":"cs.LG"}
{"created":"2024-05-01 07:50:29","title":"Directed Redundancy in Time Series","abstract":"We quantify the average amount of redundant information that is transferred from a subset of relevant random source processes to a target process. To identify the relevant source processes, we consider those that are connected to the target process and in addition share a certain proportion of the total information causally provided to the target. Even if the relevant processes have no directed information exchange between them, they can still causally provide redundant information to the target. This makes it difficult to identify the relevant processes. To solve this issue, we propose the existence of a hidden redundancy process that governs the shared information among the relevant processes. We bound the redundancy by the minimal average directed redundancy from the relevant processes to the target, from the hidden redundancy process to the target, and from the hidden redundancy process to the relevant processes.","sentences":["We quantify the average amount of redundant information that is transferred from a subset of relevant random source processes to a target process.","To identify the relevant source processes, we consider those that are connected to the target process and in addition share a certain proportion of the total information causally provided to the target.","Even if the relevant processes have no directed information exchange between them, they can still causally provide redundant information to the target.","This makes it difficult to identify the relevant processes.","To solve this issue, we propose the existence of a hidden redundancy process that governs the shared information among the relevant processes.","We bound the redundancy by the minimal average directed redundancy from the relevant processes to the target, from the hidden redundancy process to the target, and from the hidden redundancy process to the relevant processes."],"url":"http://arxiv.org/abs/2405.00368v1","category":"cs.IT"}
{"created":"2024-05-01 07:41:46","title":"Object detection under the linear subspace model with application to cryo-EM images","abstract":"Detecting multiple unknown objects in noisy data is a key problem in many scientific fields, such as electron microscopy imaging. A common model for the unknown objects is the linear subspace model, which assumes that the objects can be expanded in some known basis (such as the Fourier basis). In this paper, we develop an object detection algorithm that under the linear subspace model is asymptotically guaranteed to detect all objects, while controlling the family wise error rate or the false discovery rate. Numerical simulations show that the algorithm also controls the error rate with high power in the non-asymptotic regime, even in highly challenging regimes. We apply the proposed algorithm to experimental electron microscopy data set, and show that it outperforms existing standard software.","sentences":["Detecting multiple unknown objects in noisy data is a key problem in many scientific fields, such as electron microscopy imaging.","A common model for the unknown objects is the linear subspace model, which assumes that the objects can be expanded in some known basis (such as the Fourier basis).","In this paper, we develop an object detection algorithm that under the linear subspace model is asymptotically guaranteed to detect all objects, while controlling the family wise error rate or the false discovery rate.","Numerical simulations show that the algorithm also controls the error rate with high power in the non-asymptotic regime, even in highly challenging regimes.","We apply the proposed algorithm to experimental electron microscopy data set, and show that it outperforms existing standard software."],"url":"http://arxiv.org/abs/2405.00364v1","category":"math.ST"}
{"created":"2024-05-01 07:24:02","title":"Optimal nonparametric estimation of the expected shortfall risk","abstract":"We address the problem of estimating the expected shortfall risk of a financial loss using a finite number of i.i.d. data. It is well known that the classical plug-in estimator suffers from poor statistical performance when faced with (heavy-tailed) distributions that are commonly used in financial contexts. Further, it lacks robustness, as the modification of even a single data point can cause a significant distortion. We propose a novel procedure for the estimation of the expected shortfall and prove that it recovers the best possible statistical properties (dictated by the central limit theorem) under minimal assumptions and for all finite numbers of data. Further, this estimator is adversarially robust: even if a (small) proportion of the data is maliciously modified, the procedure continuous to optimally estimate the true expected shortfall risk. We demonstrate that our estimator outperforms the classical plug-in estimator through a variety of numerical experiments across a range of standard loss distributions.","sentences":["We address the problem of estimating the expected shortfall risk of a financial loss using a finite number of i.i.d. data.","It is well known that the classical plug-in estimator suffers from poor statistical performance when faced with (heavy-tailed) distributions that are commonly used in financial contexts.","Further, it lacks robustness, as the modification of even a single data point can cause a significant distortion.","We propose a novel procedure for the estimation of the expected shortfall and prove that it recovers the best possible statistical properties (dictated by the central limit theorem) under minimal assumptions and for all finite numbers of data.","Further, this estimator is adversarially robust: even if a (small) proportion of the data is maliciously modified, the procedure continuous to optimally estimate the true expected shortfall risk.","We demonstrate that our estimator outperforms the classical plug-in estimator through a variety of numerical experiments across a range of standard loss distributions."],"url":"http://arxiv.org/abs/2405.00357v1","category":"q-fin.RM"}
{"created":"2024-05-01 07:13:31","title":"Dual-Role AoI-based Incentive Mechanism for HD map Crowdsourcing","abstract":"A high-quality fresh high-definition (HD) map is vital in enhancing transportation efficiency and safety in autonomous driving. Vehicle-based crowdsourcing offers a promising approach for updating HD maps. However, recruiting crowdsourcing vehicles involves making the challenging tradeoff between the HD map freshness and recruitment costs. Existing studies on HD map crowdsourcing often (1) prioritize maximizing spatial coverage and (2) overlook the dual role of crowdsourcing vehicles in HD maps, as vehicles serve both as contributors and customers of HD maps. This motivates us to propose the Dual-Role Age of Information (AoI) based Incentive Mechanism (DRAIM) to address these issues. % Specifically, we propose the trajectory age of information, incorporating the expected AoI of the HD map and the trajectory, to quantify a vehicle's HD map usage utility, which is freshness- and trajectory-dependent. DRAIM aims to achieve the company's tradeoff between freshness and recruitment costs.","sentences":["A high-quality fresh high-definition (HD) map is vital in enhancing transportation efficiency and safety in autonomous driving.","Vehicle-based crowdsourcing offers a promising approach for updating HD maps.","However, recruiting crowdsourcing vehicles involves making the challenging tradeoff between the HD map freshness and recruitment costs.","Existing studies on HD map crowdsourcing often (1) prioritize maximizing spatial coverage and (2) overlook the dual role of crowdsourcing vehicles in HD maps, as vehicles serve both as contributors and customers of HD maps.","This motivates us to propose the Dual-Role Age of Information (AoI) based Incentive Mechanism (DRAIM) to address these issues.","% Specifically, we propose the trajectory age of information, incorporating the expected AoI of the HD map and the trajectory, to quantify a vehicle's HD map usage utility, which is freshness- and trajectory-dependent.","DRAIM aims to achieve the company's tradeoff between freshness and recruitment costs."],"url":"http://arxiv.org/abs/2405.00353v1","category":"cs.GT"}
{"created":"2024-05-01 06:23:54","title":"Distillation Matters: Empowering Sequential Recommenders to Match the Performance of Large Language Model","abstract":"Owing to their powerful semantic reasoning capabilities, Large Language Models (LLMs) have been effectively utilized as recommenders, achieving impressive performance. However, the high inference latency of LLMs significantly restricts their practical deployment. To address this issue, this work investigates knowledge distillation from cumbersome LLM-based recommendation models to lightweight conventional sequential models. It encounters three challenges: 1) the teacher's knowledge may not always be reliable; 2) the capacity gap between the teacher and student makes it difficult for the student to assimilate the teacher's knowledge; 3) divergence in semantic space poses a challenge to distill the knowledge from embeddings. To tackle these challenges, this work proposes a novel distillation strategy, DLLM2Rec, specifically tailored for knowledge distillation from LLM-based recommendation models to conventional sequential models. DLLM2Rec comprises: 1) Importance-aware ranking distillation, which filters reliable and student-friendly knowledge by weighting instances according to teacher confidence and student-teacher consistency; 2) Collaborative embedding distillation integrates knowledge from teacher embeddings with collaborative signals mined from the data. Extensive experiments demonstrate the effectiveness of the proposed DLLM2Rec, boosting three typical sequential models with an average improvement of 47.97%, even enabling them to surpass LLM-based recommenders in some cases.","sentences":["Owing to their powerful semantic reasoning capabilities, Large Language Models (LLMs) have been effectively utilized as recommenders, achieving impressive performance.","However, the high inference latency of LLMs significantly restricts their practical deployment.","To address this issue, this work investigates knowledge distillation from cumbersome LLM-based recommendation models to lightweight conventional sequential models.","It encounters three challenges: 1) the teacher's knowledge may not always be reliable; 2) the capacity gap between the teacher and student makes it difficult for the student to assimilate the teacher's knowledge; 3) divergence in semantic space poses a challenge to distill the knowledge from embeddings.","To tackle these challenges, this work proposes a novel distillation strategy, DLLM2Rec, specifically tailored for knowledge distillation from LLM-based recommendation models to conventional sequential models.","DLLM2Rec comprises: 1) Importance-aware ranking distillation, which filters reliable and student-friendly knowledge by weighting instances according to teacher confidence and student-teacher consistency; 2) Collaborative embedding distillation integrates knowledge from teacher embeddings with collaborative signals mined from the data.","Extensive experiments demonstrate the effectiveness of the proposed DLLM2Rec, boosting three typical sequential models with an average improvement of 47.97%, even enabling them to surpass LLM-based recommenders in some cases."],"url":"http://arxiv.org/abs/2405.00338v1","category":"cs.IR"}
{"created":"2024-05-01 05:17:55","title":"Self-similar singularities for electron MHD","abstract":"We study several types of self-similar solutions for the electron magnetohydrodynamics (MHD) without resistivity, including locally self-similar solutions and pseudo-self-similar solutions. We show that under certain conditions, these types of self-similar blowup solutions can be excluded.","sentences":["We study several types of self-similar solutions for the electron magnetohydrodynamics (MHD) without resistivity, including locally self-similar solutions and pseudo-self-similar solutions.","We show that under certain conditions, these types of self-similar blowup solutions can be excluded."],"url":"http://arxiv.org/abs/2405.00324v1","category":"math.AP"}
{"created":"2024-05-01 03:49:50","title":"On a new class of BDF and IMEX schemes for parabolic type equations","abstract":"When applying the classical multistep schemes for solving differential equations, one often faces the dilemma that smaller time steps are needed with higher-order schemes, making it impractical to use high-order schemes for stiff problems. We construct in this paper a new class of BDF and implicit-explicit (IMEX) schemes for parabolic type equations based on the Taylor expansions at time $t^{n+\\beta}$ with $\\beta > 1$ being a tunable parameter. These new schemes, with a suitable $\\beta$, allow larger time steps at higher-order for stiff problems than that is allowed with a usual higher-order scheme. For parabolic type equations, we identify an explicit uniform multiplier for the new second- to fourth-order schemes, and conduct rigorously stability and error analysis by using the energy argument. We also present ample numerical examples to validate our findings.","sentences":["When applying the classical multistep schemes for solving differential equations, one often faces the dilemma that smaller time steps are needed with higher-order schemes, making it impractical to use high-order schemes for stiff problems.","We construct in this paper a new class of BDF and implicit-explicit (IMEX) schemes for parabolic type equations based on the Taylor expansions at time $t^{n+\\beta}$ with $\\beta > 1$ being a tunable parameter.","These new schemes, with a suitable $\\beta$, allow larger time steps at higher-order for stiff problems than that is allowed with a usual higher-order scheme.","For parabolic type equations, we identify an explicit uniform multiplier for the new second- to fourth-order schemes, and conduct rigorously stability and error analysis by using the energy argument.","We also present ample numerical examples to validate our findings."],"url":"http://arxiv.org/abs/2405.00300v1","category":"math.NA"}
{"created":"2024-05-01 03:48:26","title":"Thermal stability and phase transformation of $\u03b1$-, $\u03ba(\u03b5)$-, and $\u03b3$-Ga$_2$O$_3$ thin films to $\u03b2$-Ga$_2$O$_3$ under various ambient conditions","abstract":"Phase transitions in metastable $\\alpha$-, $\\kappa(\\epsilon)$-, and $\\gamma$-Ga$_2$O$_3$ films to thermodynamically stable $\\beta$-Ga$_2$O$_3$ during annealing in air, N$_2$, and vacuum have been systematically investigated via in-situ high-temperature X-ray diffraction and scanning electron microscopy. These respective polymorphs exhibited thermal stability to around 471-525$^\\circ$C, 773-825$^\\circ$C, and 490-575$^\\circ$C before transforming into $\\beta$-Ga$_2$O$_3$, across all tested ambient conditions. Particular crystallographic orientation relationships were observed before and after the phase transitions, i.e., (0006) $\\alpha$-Ga$_2$O$_3$ $\\parallel$ $(\\overline{4}02)$ $\\beta$-Ga$_2$O$_3$, (004) $\\kappa(\\epsilon)$-Ga$_2$O$_3$ $\\parallel$ (310) and $(\\overline{4}02)$ $\\beta$-Ga$_2$O$_3$, and (400) $\\gamma$-Ga$_2$O$_3$ $\\parallel$ (400) $\\beta$-Ga$_2$O$_3$. The phase transition of $\\alpha$-Ga$_2$O$_3$ to $\\beta$-Ga$_2$O$_3$ resulted in catastrophic damage to the film and upheaval of the surface. The respective primary and possibly secondary causes of this damage are the +8.6% volume expansion and the dual displacive and reconstructive transformations that occur during this transition. The $\\kappa(\\epsilon)$- and $\\gamma$-Ga$_2$O$_3$ films converted to $\\beta$-Ga$_2$O$_3$ via singular reconstructive transformations with small changes in volume and unchanged surface microstructures.","sentences":["Phase transitions in metastable $\\alpha$-, $\\kappa(\\epsilon)$-, and $\\gamma$-Ga$_2$O$_3$ films to thermodynamically stable $\\beta$-Ga$_2$O$_3$ during annealing in air, N$_2$, and vacuum have been systematically investigated via in-situ high-temperature X-ray diffraction and scanning electron microscopy.","These respective polymorphs exhibited thermal stability to around 471-525$^\\circ$C, 773-825$^\\circ$C, and 490-575$^\\circ$C before transforming into $\\beta$-Ga$_2$O$_3$, across all tested ambient conditions.","Particular crystallographic orientation relationships were observed before and after the phase transitions, i.e., (0006) $\\alpha$-Ga$_2$O$_3$ $\\parallel$ $(\\overline{4}02)$ $\\beta$-Ga$_2$O$_3$, (004) $\\kappa(\\epsilon)$-Ga$_2$O$_3$ $\\parallel$ (310) and $(\\overline{4}02)$ $\\beta$-Ga$_2$O$_3$, and (400) $\\gamma$-Ga$_2$O$_3$ $\\parallel$ (400) $\\beta$-Ga$_2$O$_3$. The phase transition of $\\alpha$-Ga$_2$O$_3$ to $\\beta$-Ga$_2$O$_3$ resulted in catastrophic damage to the film and upheaval of the surface.","The respective primary and possibly secondary causes of this damage are the +8.6% volume expansion and the dual displacive and reconstructive transformations that occur during this transition.","The $\\kappa(\\epsilon)$- and $\\gamma$-Ga$_2$O$_3$ films converted to $\\beta$-Ga$_2$O$_3$ via singular reconstructive transformations with small changes in volume and unchanged surface microstructures."],"url":"http://arxiv.org/abs/2405.00299v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-01 03:15:28","title":"MoPEFT: A Mixture-of-PEFTs for the Segment Anything Model","abstract":"The emergence of foundation models, such as the Segment Anything Model (SAM), has sparked interest in Parameter-Efficient Fine-Tuning (PEFT) methods that tailor these large models to application domains outside their training data. However, different PEFT techniques modify the representation of a model differently, making it a non-trivial task to select the most appropriate method for the domain of interest. We propose a new framework, Mixture-of-PEFTs methods (MoPEFT), that is inspired by traditional Mixture-of-Experts (MoE) methodologies and is utilized for fine-tuning SAM. Our MoPEFT framework incorporates three different PEFT techniques as submodules and dynamically learns to activate the ones that are best suited for a given data-task setup. We test our method on the Segment Anything Model and show that MoPEFT consistently outperforms other fine-tuning methods on the MESS benchmark.","sentences":["The emergence of foundation models, such as the Segment Anything Model (SAM), has sparked interest in Parameter-Efficient Fine-Tuning (PEFT) methods that tailor these large models to application domains outside their training data.","However, different PEFT techniques modify the representation of a model differently, making it a non-trivial task to select the most appropriate method for the domain of interest.","We propose a new framework, Mixture-of-PEFTs methods (MoPEFT), that is inspired by traditional Mixture-of-Experts (MoE) methodologies and is utilized for fine-tuning SAM.","Our MoPEFT framework incorporates three different PEFT techniques as submodules and dynamically learns to activate the ones that are best suited for a given data-task setup.","We test our method on the Segment Anything Model and show that MoPEFT consistently outperforms other fine-tuning methods on the MESS benchmark."],"url":"http://arxiv.org/abs/2405.00293v1","category":"cs.CV"}
{"created":"2024-05-01 00:40:41","title":"Improved Massively Parallel Triangle Counting in $O(1)$ Rounds","abstract":"In this short note, we give a novel algorithm for $O(1)$ round triangle counting in bounded arboricity graphs. Counting triangles in $O(1)$ rounds (exactly) is listed as one of the interesting remaining open problems in the recent survey of Im et al. [IKLMV23]. The previous paper of Biswas et al. [BELMR20], which achieved the best bounds under this setting, used $O(\\log \\log n)$ rounds in sublinear space per machine and $O(m\\alpha)$ total space where $\\alpha$ is the arboricity of the graph and $n$ and $m$ are the number of vertices and edges in the graph, respectively. Our new algorithm is very simple, achieves the optimal $O(1)$ rounds without increasing the space per machine and the total space, and has the potential of being easily implementable in practice.","sentences":["In this short note, we give a novel algorithm for $O(1)$ round triangle counting in bounded arboricity graphs.","Counting triangles in $O(1)$ rounds (exactly) is listed as one of the interesting remaining open problems in the recent survey of Im et al.","[IKLMV23].","The previous paper of Biswas et al.","[BELMR20], which achieved the best bounds under this setting, used $O(\\log \\log n)$ rounds in sublinear space per machine and $O(m\\alpha)$ total space where $\\alpha$ is the arboricity of the graph and $n$ and $m$ are the number of vertices and edges in the graph, respectively.","Our new algorithm is very simple, achieves the optimal $O(1)$ rounds without increasing the space per machine and the total space, and has the potential of being easily implementable in practice."],"url":"http://arxiv.org/abs/2405.00262v1","category":"cs.DS"}
{"created":"2024-04-30 23:37:24","title":"A Framework for Approximation Schemes on Knapsack and Packing Problems of Hyperspheres and Fat Objects","abstract":"Geometric packing problems have been investigated for centuries in mathematics. In contrast, works on sphere packing in the field of approximation algorithms are scarce. Most results are for squares and rectangles, and their d-dimensional counterparts. To help fill this gap, we present a framework that yields approximation schemes for the geometric knapsack problem as well as other packing problems and some generalizations, and that supports not only hyperspheres but also a wide range of shapes for the items and the bins. Our first result is a PTAS for the hypersphere multiple knapsack problem. In fact, we can deal with a more generalized version of the problem that contains additional constraints on the items. These constraints, under some conditions, can encompass very common and pertinent constraints such as conflict constraints, multiple-choice constraints, and capacity constraints. Our second result is a resource augmentation scheme for the multiple knapsack problem for a wide range of convex fat objects, which are not restricted to polygons and polytopes. Examples are ellipsoids, rhombi, hypercubes, hyperspheres under the Lp-norm, etc. Also, for the generalized version of the multiple knapsack problem, our technique still yields a PTAS under resource augmentation for these objects. Thirdly, we improve the resource augmentation schemes of fat objects to allow rotation on the objects by any angle. This result, in particular, brings something extra to our framework, since most results comprising such general objects are limited to translations. At last, our framework is able to contemplate other problems such as the cutting stock problem, the minimum-size bin packing problem and the multiple strip packing problem.","sentences":["Geometric packing problems have been investigated for centuries in mathematics.","In contrast, works on sphere packing in the field of approximation algorithms are scarce.","Most results are for squares and rectangles, and their d-dimensional counterparts.","To help fill this gap, we present a framework that yields approximation schemes for the geometric knapsack problem as well as other packing problems and some generalizations, and that supports not only hyperspheres but also a wide range of shapes for the items and the bins.","Our first result is a PTAS for the hypersphere multiple knapsack problem.","In fact, we can deal with a more generalized version of the problem that contains additional constraints on the items.","These constraints, under some conditions, can encompass very common and pertinent constraints such as conflict constraints, multiple-choice constraints, and capacity constraints.","Our second result is a resource augmentation scheme for the multiple knapsack problem for a wide range of convex fat objects, which are not restricted to polygons and polytopes.","Examples are ellipsoids, rhombi, hypercubes, hyperspheres under the Lp-norm, etc.","Also, for the generalized version of the multiple knapsack problem, our technique still yields a PTAS under resource augmentation for these objects.","Thirdly, we improve the resource augmentation schemes of fat objects to allow rotation on the objects by any angle.","This result, in particular, brings something extra to our framework, since most results comprising such general objects are limited to translations.","At last, our framework is able to contemplate other problems such as the cutting stock problem, the minimum-size bin packing problem and the multiple strip packing problem."],"url":"http://arxiv.org/abs/2405.00246v1","category":"cs.CG"}
{"created":"2024-04-30 23:09:23","title":"Noise reduction by bias cooling in gated Si/SixGe1-x qunatum dots","abstract":"Silicon-Germanium heterostructures are a promising quantum circuit platform, but crucial aspects as the long-term charge dynamics and cooldown-to-cooldown variations are still widely unexplored quantitatively. In this letter we present the results of an extensive bias cooling study performed on gated silicon-germanium quantum dots with an Al2O3-dielectric. Over 80 cooldowns were performed in the course of our investigations. The performance of the devices is assessed by low-frequency charge noise measurements in the band of 200 micro Hertz to 10 milli Hertz. We measure the total noise power as a function of the applied voltage during cooldown in four different devices and find a minimum in noise at 0.7V bias cooling voltage for all observed samples. We manage to decrease the total noise power median by a factor of 6 and compute a reduced tunneling current density using Schr\\\"odinger-Poisson simulations. Furthermore, we show the variation in noise from the same device in the course of eleven different cooldowns performed under the nominally same conditions.","sentences":["Silicon-Germanium heterostructures are a promising quantum circuit platform, but crucial aspects as the long-term charge dynamics and cooldown-to-cooldown variations are still widely unexplored quantitatively.","In this letter we present the results of an extensive bias cooling study performed on gated silicon-germanium quantum dots with an Al2O3-dielectric.","Over 80 cooldowns were performed in the course of our investigations.","The performance of the devices is assessed by low-frequency charge noise measurements in the band of 200 micro Hertz to 10 milli Hertz.","We measure the total noise power as a function of the applied voltage during cooldown in four different devices and find a minimum in noise at 0.7V bias cooling voltage for all observed samples.","We manage to decrease the total noise power median by a factor of 6 and compute a reduced tunneling current density using Schr\\\"odinger-Poisson simulations.","Furthermore, we show the variation in noise from the same device in the course of eleven different cooldowns performed under the nominally same conditions."],"url":"http://arxiv.org/abs/2405.00238v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-30 23:03:00","title":"Blockchain Price vs. Quantity Controls","abstract":"This paper studies the optimal transaction fee mechanisms for blockchains, focusing on the distinction between price-based ($\\mathcal{P}$) and quantity-based ($\\mathcal{Q}$) controls. By analyzing factors such as demand uncertainty, validator costs, cryptocurrency price fluctuations, price elasticity of demand, and levels of decentralization, we establish criteria that determine the selection of transaction fee mechanisms. We present a model framed around a Nash bargaining game, exploring how blockchain designers and validators negotiate fee structures to balance network welfare with profitability. Our findings suggest that the choice between $\\mathcal{P}$ and $\\mathcal{Q}$ mechanisms depends critically on the blockchain's specific technical and economic features. The study concludes that no single mechanism suits all contexts and highlights the potential for hybrid approaches that adaptively combine features of both $\\mathcal{P}$ and $\\mathcal{Q}$ to meet varying demands and market conditions.","sentences":["This paper studies the optimal transaction fee mechanisms for blockchains, focusing on the distinction between price-based ($\\mathcal{P}$) and quantity-based ($\\mathcal{Q}$) controls.","By analyzing factors such as demand uncertainty, validator costs, cryptocurrency price fluctuations, price elasticity of demand, and levels of decentralization, we establish criteria that determine the selection of transaction fee mechanisms.","We present a model framed around a Nash bargaining game, exploring how blockchain designers and validators negotiate fee structures to balance network welfare with profitability.","Our findings suggest that the choice between $\\mathcal{P}$ and $\\mathcal{Q}$ mechanisms depends critically on the blockchain's specific technical and economic features.","The study concludes that no single mechanism suits all contexts and highlights the potential for hybrid approaches that adaptively combine features of both $\\mathcal{P}$ and $\\mathcal{Q}$ to meet varying demands and market conditions."],"url":"http://arxiv.org/abs/2405.00235v1","category":"econ.GN"}
{"created":"2024-04-30 22:32:02","title":"Synthetic Face Datasets Generation via Latent Space Exploration from Brownian Identity Diffusion","abstract":"Face Recognition (FR) models are trained on large-scale datasets, which have privacy and ethical concerns. Lately, the use of synthetic data to complement or replace genuine data for the training of FR models has been proposed. While promising results have been obtained, it still remains unclear if generative models can yield diverse enough data for such tasks. In this work, we introduce a new method, inspired by the physical motion of soft particles subjected to stochastic Brownian forces, allowing us to sample identities distributions in a latent space under various constraints. With this in hands, we generate several face datasets and benchmark them by training FR models, showing that data generated with our method exceeds the performance of previously GAN-based datasets and achieves competitive performance with state-of-the-art diffusion-based synthetic datasets. We also show that this method can be used to mitigate leakage from the generator's training set and explore the ability of generative models to generate data beyond it.","sentences":["Face Recognition (FR) models are trained on large-scale datasets, which have privacy and ethical concerns.","Lately, the use of synthetic data to complement or replace genuine data for the training of FR models has been proposed.","While promising results have been obtained, it still remains unclear if generative models can yield diverse enough data for such tasks.","In this work, we introduce a new method, inspired by the physical motion of soft particles subjected to stochastic Brownian forces, allowing us to sample identities distributions in a latent space under various constraints.","With this in hands, we generate several face datasets and benchmark them by training FR models, showing that data generated with our method exceeds the performance of previously GAN-based datasets and achieves competitive performance with state-of-the-art diffusion-based synthetic datasets.","We also show that this method can be used to mitigate leakage from the generator's training set and explore the ability of generative models to generate data beyond it."],"url":"http://arxiv.org/abs/2405.00228v1","category":"cs.CV"}
{"created":"2024-04-30 22:14:25","title":"Optimized Non-Primary Channel Access Design in IEEE 802.11bn","abstract":"The IEEE 802.11 standards, culminating in IEEE 802.11be (Wi-Fi 7), have significantly expanded bandwidth capacities from 20 MHz to 320 MHz, marking a crucial evolution in wireless access technology. Despite these advancements, the full potential of these capacities remains largely untapped due to inefficiencies in channel management, in particular, the underutilization of secondary (non-primary) channels when the primary channel is occupied. This paper delves into the Non-Primary Channel Access (NPCA) protocol, initially proposed by the IEEE 802.11 Ultra-High Reliability (UHR) group, aimed at addressing these inefficiencies. Our research not only proposes an analytical model to assess the throughput of NPCA in terms of average throughput but also crucially identifies that the overhead associated with the NPCA protocol is significant and cannot be ignored. This overhead often undermines the effectiveness of the NPCA, challenging the assumption that it is invariably superior to traditional models. Based on these findings, we have developed and simulated a new hybrid model that dynamically integrates the strengths of both legacy and NPCA models. This model overall outperforms the existing models under all channel occupancy conditions, offering a robust solution to enhance throughput efficiency.","sentences":["The IEEE 802.11 standards, culminating in IEEE 802.11be (Wi-Fi 7), have significantly expanded bandwidth capacities from 20 MHz to 320 MHz, marking a crucial evolution in wireless access technology.","Despite these advancements, the full potential of these capacities remains largely untapped due to inefficiencies in channel management, in particular, the underutilization of secondary (non-primary) channels when the primary channel is occupied.","This paper delves into the Non-Primary Channel Access (NPCA) protocol, initially proposed by the IEEE 802.11 Ultra-High Reliability (UHR) group, aimed at addressing these inefficiencies.","Our research not only proposes an analytical model to assess the throughput of NPCA in terms of average throughput but also crucially identifies that the overhead associated with the NPCA protocol is significant and cannot be ignored.","This overhead often undermines the effectiveness of the NPCA, challenging the assumption that it is invariably superior to traditional models.","Based on these findings, we have developed and simulated a new hybrid model that dynamically integrates the strengths of both legacy and NPCA models.","This model overall outperforms the existing models under all channel occupancy conditions, offering a robust solution to enhance throughput efficiency."],"url":"http://arxiv.org/abs/2405.00227v1","category":"cs.NI"}
{"created":"2024-04-30 21:52:15","title":"GMC-PINNs: A new general Monte Carlo PINNs method for solving fractional partial differential equations on irregular domains","abstract":"Physics-Informed Neural Networks (PINNs) have been widely used for solving partial differential equations (PDEs) of different types, including fractional PDEs (fPDES) [29]. Herein, we propose a new general (quasi) Monte Carlo PINN for solving fPDEs on irregular domains. Specifically, instead of approximating fractional derivatives by Monte Carlo approximations of integrals as was done previously in [31], we use a more general Monte Carlo approximation method to solve different fPDEs, which is valid for fractional differentiation under any definition. Moreover, based on the ensemble probability density function, the generated nodes are all located in denser regions near the target point where we perform the differentiation. This has an unexpected connection with known finite difference methods on non-equidistant or nested grids, and hence our method inherits their advantages. At the same time, the generated nodes exhibit a block-like dense distribution, leading to a good computational efficiency of this approach. We present the framework for using this algorithm and apply it to several examples. Our results demonstrate the effectiveness of GMC-PINNs in dealing with irregular domain problems and show a higher computational efficiency compared to the original fPINN method. We also include comparisons with the Monte Carlo fPINN [31]. Finally, we use examples to demonstrate the effectiveness of the method in dealing with fuzzy boundary location problems, and then use the method to solve the coupled 3D fractional Bloch-Torrey equation defined in the ventricular domain of the human brain, and compare the results with classical numerical methods.","sentences":["Physics-Informed Neural Networks (PINNs) have been widely used for solving partial differential equations (PDEs) of different types, including fractional PDEs (fPDES)","[29].","Herein, we propose a new general (quasi) Monte Carlo PINN for solving fPDEs on irregular domains.","Specifically, instead of approximating fractional derivatives by Monte Carlo approximations of integrals as was done previously in [31], we use a more general Monte Carlo approximation method to solve different fPDEs, which is valid for fractional differentiation under any definition.","Moreover, based on the ensemble probability density function, the generated nodes are all located in denser regions near the target point where we perform the differentiation.","This has an unexpected connection with known finite difference methods on non-equidistant or nested grids, and hence our method inherits their advantages.","At the same time, the generated nodes exhibit a block-like dense distribution, leading to a good computational efficiency of this approach.","We present the framework for using this algorithm and apply it to several examples.","Our results demonstrate the effectiveness of GMC-PINNs in dealing with irregular domain problems and show a higher computational efficiency compared to the original fPINN method.","We also include comparisons with the Monte Carlo fPINN","[31].","Finally, we use examples to demonstrate the effectiveness of the method in dealing with fuzzy boundary location problems, and then use the method to solve the coupled 3D fractional Bloch-Torrey equation defined in the ventricular domain of the human brain, and compare the results with classical numerical methods."],"url":"http://arxiv.org/abs/2405.00217v1","category":"cs.LG"}
{"created":"2024-04-30 21:10:51","title":"Leveraging Active Subspaces to Capture Epistemic Model Uncertainty in Deep Generative Models for Molecular Design","abstract":"Deep generative models have been accelerating the inverse design process in material and drug design. Unlike their counterpart property predictors in typical molecular design frameworks, generative molecular design models have seen fewer efforts on uncertainty quantification (UQ) due to computational challenges in Bayesian inference posed by their large number of parameters. In this work, we focus on the junction-tree variational autoencoder (JT-VAE), a popular model for generative molecular design, and address this issue by leveraging the low dimensional active subspace to capture the uncertainty in the model parameters. Specifically, we approximate the posterior distribution over the active subspace parameters to estimate the epistemic model uncertainty in an extremely high dimensional parameter space. The proposed UQ scheme does not require alteration of the model architecture, making it readily applicable to any pre-trained model. Our experiments demonstrate the efficacy of the AS-based UQ and its potential impact on molecular optimization by exploring the model diversity under epistemic uncertainty.","sentences":["Deep generative models have been accelerating the inverse design process in material and drug design.","Unlike their counterpart property predictors in typical molecular design frameworks, generative molecular design models have seen fewer efforts on uncertainty quantification (UQ) due to computational challenges in Bayesian inference posed by their large number of parameters.","In this work, we focus on the junction-tree variational autoencoder (JT-VAE), a popular model for generative molecular design, and address this issue by leveraging the low dimensional active subspace to capture the uncertainty in the model parameters.","Specifically, we approximate the posterior distribution over the active subspace parameters to estimate the epistemic model uncertainty in an extremely high dimensional parameter space.","The proposed UQ scheme does not require alteration of the model architecture, making it readily applicable to any pre-trained model.","Our experiments demonstrate the efficacy of the AS-based UQ and its potential impact on molecular optimization by exploring the model diversity under epistemic uncertainty."],"url":"http://arxiv.org/abs/2405.00202v1","category":"cs.LG"}
{"created":"2024-04-30 20:31:39","title":"A Revisit of the Optimal Excess-of-Loss Contract","abstract":"It is well-known that Excess-of-Loss reinsurance has more marketability than Stop-Loss reinsurance, though Stop-Loss reinsurance is the most prominent setting discussed in the optimal (re)insurance design literature. We point out that optimal reinsurance policy under Stop-Loss leads to a zero insolvency probability, which motivates our paper. We provide a remedy to this peculiar property of the optimal Stop-Loss reinsurance contract by investigating the optimal Excess-of-Loss reinsurance contract instead. We also provide estimators for the optimal Excess-of-Loss and Stop-Loss contracts and investigate their statistical properties under many premium principle assumptions and various risk preferences, which according to our knowledge, have never been investigated in the literature. Simulated data and real-life data are used to illustrate our main theoretical findings.","sentences":["It is well-known that Excess-of-Loss reinsurance has more marketability than Stop-Loss reinsurance, though Stop-Loss reinsurance is the most prominent setting discussed in the optimal (re)insurance design literature.","We point out that optimal reinsurance policy under Stop-Loss leads to a zero insolvency probability, which motivates our paper.","We provide a remedy to this peculiar property of the optimal Stop-Loss reinsurance contract by investigating the optimal Excess-of-Loss reinsurance contract instead.","We also provide estimators for the optimal Excess-of-Loss and Stop-Loss contracts and investigate their statistical properties under many premium principle assumptions and various risk preferences, which according to our knowledge, have never been investigated in the literature.","Simulated data and real-life data are used to illustrate our main theoretical findings."],"url":"http://arxiv.org/abs/2405.00188v1","category":"stat.AP"}
{"created":"2024-04-30 20:18:11","title":"Finite-sample adjustments for comparing clustered adaptive interventions using data from a clustered SMART","abstract":"Adaptive interventions, aka dynamic treatment regimens, are sequences of pre-specified decision rules that guide the provision of treatment for an individual given information about their baseline and evolving needs, including in response to prior intervention. Clustered adaptive interventions (cAIs) extend this idea by guiding the provision of intervention at the level of clusters (e.g., clinics), but with the goal of improving outcomes at the level of individuals within the cluster (e.g., clinicians or patients within clinics). A clustered, sequential multiple-assignment randomized trials (cSMARTs) is a multistage, multilevel randomized trial design used to construct high-quality cAIs. In a cSMART, clusters are randomized at multiple intervention decision points; at each decision point, the randomization probability can depend on response to prior data. A challenge in cluster-randomized trials, including cSMARTs, is the deleterious effect of small samples of clusters on statistical inference, particularly via estimation of standard errors. \\par This manuscript develops finite-sample adjustment (FSA) methods for making improved statistical inference about the causal effects of cAIs in a cSMART. The paper develops FSA methods that (i) scale variance estimators using a degree-of-freedom adjustment, (ii) reference a t distribution (instead of a normal), and (iii) employ a ``bias corrected\" variance estimator. Method (iii) requires extensions that are unique to the analysis of cSMARTs. Extensive simulation experiments are used to test the performance of the methods. The methods are illustrated using the Adaptive School-based Implementation of CBT (ASIC) study, a cSMART designed to construct a cAI for improving the delivery of cognitive behavioral therapy (CBT) by school mental health professionals within high schools in Michigan.","sentences":["Adaptive interventions, aka dynamic treatment regimens, are sequences of pre-specified decision rules that guide the provision of treatment for an individual given information about their baseline and evolving needs, including in response to prior intervention.","Clustered adaptive interventions (cAIs) extend this idea by guiding the provision of intervention at the level of clusters (e.g., clinics), but with the goal of improving outcomes at the level of individuals within the cluster (e.g., clinicians or patients within clinics).","A clustered, sequential multiple-assignment randomized trials (cSMARTs) is a multistage, multilevel randomized trial design used to construct high-quality cAIs.","In a cSMART, clusters are randomized at multiple intervention decision points; at each decision point, the randomization probability can depend on response to prior data.","A challenge in cluster-randomized trials, including cSMARTs, is the deleterious effect of small samples of clusters on statistical inference, particularly via estimation of standard errors.","\\par","This manuscript develops finite-sample adjustment (FSA) methods for making improved statistical inference about the causal effects of cAIs in a cSMART.","The paper develops FSA methods that (i) scale variance estimators using a degree-of-freedom adjustment, (ii) reference a t distribution (instead of a normal), and (iii) employ a ``bias corrected\" variance estimator.","Method (iii) requires extensions that are unique to the analysis of cSMARTs.","Extensive simulation experiments are used to test the performance of the methods.","The methods are illustrated using the Adaptive School-based Implementation of CBT (ASIC) study, a cSMART designed to construct a cAI for improving the delivery of cognitive behavioral therapy (CBT) by school mental health professionals within high schools in Michigan."],"url":"http://arxiv.org/abs/2405.00185v1","category":"stat.ME"}
{"created":"2024-04-30 20:16:40","title":"Semi-Supervised Hierarchical Multi-Label Classifier Based on Local Information","abstract":"Scarcity of labeled data is a common problem in supervised classification, since hand-labeling can be time consuming, expensive or hard to label; on the other hand, large amounts of unlabeled information can be found. The problem of scarcity of labeled data is even more notorious in hierarchical classification, because the data of a node is split among its children, which results in few instances associated to the deepest nodes of the hierarchy. In this work it is proposed the semi-supervised hierarchical multi-label classifier based on local information (SSHMC-BLI) which can be trained with labeled and unlabeled data to perform hierarchical classification tasks. The method can be applied to any type of hierarchical problem, here we focus on the most difficult case: hierarchies of DAG type, where the instances can be associated to multiple paths of labels which can finish in an internal node. SSHMC-BLI builds pseudo-labels for each unlabeled instance from the paths of labels of its labeled neighbors, while it considers whether the unlabeled instance is similar to its neighbors. Experiments on 12 challenging datasets from functional genomics show that making use of unlabeled along with labeled data can help to improve the performance of a supervised hierarchical classifier trained only on labeled data, even with statistical significance.","sentences":["Scarcity of labeled data is a common problem in supervised classification, since hand-labeling can be time consuming, expensive or hard to label; on the other hand, large amounts of unlabeled information can be found.","The problem of scarcity of labeled data is even more notorious in hierarchical classification, because the data of a node is split among its children, which results in few instances associated to the deepest nodes of the hierarchy.","In this work it is proposed the semi-supervised hierarchical multi-label classifier based on local information (SSHMC-BLI) which can be trained with labeled and unlabeled data to perform hierarchical classification tasks.","The method can be applied to any type of hierarchical problem, here we focus on the most difficult case: hierarchies of DAG type, where the instances can be associated to multiple paths of labels which can finish in an internal node.","SSHMC-BLI builds pseudo-labels for each unlabeled instance from the paths of labels of its labeled neighbors, while it considers whether the unlabeled instance is similar to its neighbors.","Experiments on 12 challenging datasets from functional genomics show that making use of unlabeled along with labeled data can help to improve the performance of a supervised hierarchical classifier trained only on labeled data, even with statistical significance."],"url":"http://arxiv.org/abs/2405.00184v1","category":"cs.LG"}
{"created":"2024-04-30 20:05:52","title":"Heart Rate and Body Temperature Relationship in Children Admitted to PICU -- A Machine Learning Approach","abstract":"Vital signs have been essential clinical measures. Among these, body temperature (BT) and heart rate (HR) are particularly significant, and numerous studies explored their association in hospitalized adults and children. However, a lack of in-depth research persists in children admitted to the pediatric intensive care unit (PICU) despite their critical condition requiring particular attention. Objective: In this study, we explore the relationship between HR and BT in children from 0 to 18 years old admitted to the PICU of CHU Sainte-Justine Hospital. Methods: We applied Machine learning (ML) techniques to unravel subtle patterns and dependencies within our dataset to achieve this objective. Each algorithm undergoes meticulous hyperparameter tuning to optimize the model performance. Results: Our findings align with prior research, revealing a consistent trend of decreasing HR with increasing patient age, confirming the observed inverse correlation. Furthermore, a thorough analysis identifies Gradient Boosting Machines (GBM) implemented with Quantile regression (QR), as the most fitting model, effectively capturing the non-linear relationship between HR, BT, and age. Through testing the HR prediction model based on age and BT, the predictive model between the 5th and 95th percentiles accurately demonstrates the declining trend of HR with age, while HR increase with BT. Based on that, we have developed a user-friendly interface tailored to generate HR predictions at different percentiles based on three key input parameters : current HR, current BT, and patient's age. The resulting output enables caregivers to quickly determine whether a patient's HR falls within or outside the normal range, facilitating informed clinical decision-making. Thus, our results challenge previous studies' presumed direct linear association between HR and BT.","sentences":["Vital signs have been essential clinical measures.","Among these, body temperature (BT) and heart rate (HR) are particularly significant, and numerous studies explored their association in hospitalized adults and children.","However, a lack of in-depth research persists in children admitted to the pediatric intensive care unit (PICU) despite their critical condition requiring particular attention.","Objective:","In this study, we explore the relationship between HR and BT in children from 0 to 18 years old admitted to the PICU of CHU Sainte-Justine Hospital.","Methods: We applied Machine learning (ML) techniques to unravel subtle patterns and dependencies within our dataset to achieve this objective.","Each algorithm undergoes meticulous hyperparameter tuning to optimize the model performance.","Results: Our findings align with prior research, revealing a consistent trend of decreasing HR with increasing patient age, confirming the observed inverse correlation.","Furthermore, a thorough analysis identifies Gradient Boosting Machines (GBM) implemented with Quantile regression (QR), as the most fitting model, effectively capturing the non-linear relationship between HR, BT, and age.","Through testing the HR prediction model based on age and BT, the predictive model between the 5th and 95th percentiles accurately demonstrates the declining trend of HR with age, while HR increase with BT.","Based on that, we have developed a user-friendly interface tailored to generate HR predictions at different percentiles based on three key input parameters : current HR, current BT, and patient's age.","The resulting output enables caregivers to quickly determine whether a patient's HR falls within or outside the normal range, facilitating informed clinical decision-making.","Thus, our results challenge previous studies' presumed direct linear association between HR and BT."],"url":"http://arxiv.org/abs/2405.00180v1","category":"eess.SP"}
{"created":"2024-04-30 19:53:39","title":"Rockafellian Relaxation for PDE-Constrained Optimization with Distributional Uncertainty","abstract":"Stochastic optimization problems are generally known to be ill-conditioned to the form of the underlying uncertainty. A framework is introduced for optimal control problems with partial differential equations as constraints that is robust to inaccuracies in the precise form of the problem uncertainty. The framework is based on problem relaxation and involves optimizing a bivariate, \"Rockafellian\" objective functional that features both a standard control variable and an additional perturbation variable that handles the distributional ambiguity. In the presence of distributional corruption, the Rockafellian objective functionals are shown in the appropriate settings to $\\Gamma$-converge to uncorrupted objective functionals in the limit of vanishing corruption. Numerical examples illustrate the framework's utility for outlier detection and removal and for variance reduction.","sentences":["Stochastic optimization problems are generally known to be ill-conditioned to the form of the underlying uncertainty.","A framework is introduced for optimal control problems with partial differential equations as constraints that is robust to inaccuracies in the precise form of the problem uncertainty.","The framework is based on problem relaxation and involves optimizing a bivariate, \"Rockafellian\" objective functional that features both a standard control variable and an additional perturbation variable that handles the distributional ambiguity.","In the presence of distributional corruption, the Rockafellian objective functionals are shown in the appropriate settings to $\\Gamma$-converge to uncorrupted objective functionals in the limit of vanishing corruption.","Numerical examples illustrate the framework's utility for outlier detection and removal and for variance reduction."],"url":"http://arxiv.org/abs/2405.00176v1","category":"math.OC"}
{"created":"2024-04-30 19:43:01","title":"Re-visiting Skip-Gram Negative Sampling: Dimension Regularization for More Efficient Dissimilarity Preservation in Graph Embeddings","abstract":"A wide range of graph embedding objectives decompose into two components: one that attracts the embeddings of nodes that are perceived as similar, and another that repels embeddings of nodes that are perceived as dissimilar. Because real-world graphs are sparse and the number of dissimilar pairs grows quadratically with the number of nodes, Skip-Gram Negative Sampling (SGNS) has emerged as a popular and efficient repulsion approach. SGNS repels each node from a sample of dissimilar nodes, as opposed to all dissimilar nodes. In this work, we show that node-wise repulsion is, in aggregate, an approximate re-centering of the node embedding dimensions. Such dimension operations are much more scalable than node operations. The dimension approach, in addition to being more efficient, yields a simpler geometric interpretation of the repulsion. Our result extends findings from the self-supervised learning literature to the skip-gram model, establishing a connection between skip-gram node contrast and dimension regularization. We show that in the limit of large graphs, under mild regularity conditions, the original node repulsion objective converges to optimization with dimension regularization. We use this observation to propose an algorithm augmentation framework that speeds up any existing algorithm, supervised or unsupervised, using SGNS. The framework prioritizes node attraction and replaces SGNS with dimension regularization. We instantiate this generic framework for LINE and node2vec and show that the augmented algorithms preserve downstream performance while dramatically increasing efficiency.","sentences":["A wide range of graph embedding objectives decompose into two components: one that attracts the embeddings of nodes that are perceived as similar, and another that repels embeddings of nodes that are perceived as dissimilar.","Because real-world graphs are sparse and the number of dissimilar pairs grows quadratically with the number of nodes, Skip-Gram Negative Sampling (SGNS) has emerged as a popular and efficient repulsion approach.","SGNS repels each node from a sample of dissimilar nodes, as opposed to all dissimilar nodes.","In this work, we show that node-wise repulsion is, in aggregate, an approximate re-centering of the node embedding dimensions.","Such dimension operations are much more scalable than node operations.","The dimension approach, in addition to being more efficient, yields a simpler geometric interpretation of the repulsion.","Our result extends findings from the self-supervised learning literature to the skip-gram model, establishing a connection between skip-gram node contrast and dimension regularization.","We show that in the limit of large graphs, under mild regularity conditions, the original node repulsion objective converges to optimization with dimension regularization.","We use this observation to propose an algorithm augmentation framework that speeds up any existing algorithm, supervised or unsupervised, using SGNS.","The framework prioritizes node attraction and replaces SGNS with dimension regularization.","We instantiate this generic framework for LINE and node2vec and show that the augmented algorithms preserve downstream performance while dramatically increasing efficiency."],"url":"http://arxiv.org/abs/2405.00172v1","category":"cs.LG"}
{"created":"2024-04-30 19:18:01","title":"Planetesimal drift in eccentric disks: possible outward migration","abstract":"Radial drift of solid particles in the protoplanetary disk is often invoked as a threat to planet formation, as it removes solid material from the disk before it can be assembled into planets. However, it may also concentrate solids at particular locations in the disk, thus accelerating the coagulation process. Planetesimals are thought to drift much faster in an eccentric disk, due to their higher velocities with respect to the gas, but their drift rate has only been calculated using approximate means. In this work, we show that in some cases, previous estimates of the drift rate, based on a modification of the results for an axisymmetric disk, are highly inaccurate. In particular, we find that under some easily realized circumstances, planetesimals may drift outwards, rather than inwards. This results in the existence of radii in the disk that act as stable attractors of planetesimals. We show that this can lead to a local enhancement of more than an order of magnitude in the surface density of planetesimals, even when a wide dispersion of planetesimal size is considered.","sentences":["Radial drift of solid particles in the protoplanetary disk is often invoked as a threat to planet formation, as it removes solid material from the disk before it can be assembled into planets.","However, it may also concentrate solids at particular locations in the disk, thus accelerating the coagulation process.","Planetesimals are thought to drift much faster in an eccentric disk, due to their higher velocities with respect to the gas, but their drift rate has only been calculated using approximate means.","In this work, we show that in some cases, previous estimates of the drift rate, based on a modification of the results for an axisymmetric disk, are highly inaccurate.","In particular, we find that under some easily realized circumstances, planetesimals may drift outwards, rather than inwards.","This results in the existence of radii in the disk that act as stable attractors of planetesimals.","We show that this can lead to a local enhancement of more than an order of magnitude in the surface density of planetesimals, even when a wide dispersion of planetesimal size is considered."],"url":"http://arxiv.org/abs/2405.00160v1","category":"astro-ph.EP"}
{"created":"2024-04-30 18:40:23","title":"Organ Dose Equivalents of Albedo Protons and Neutrons Under Exposure to Large Solar Particle Events during Lunar Human Landing Missions","abstract":"Astronauts participating in lunar landing missions will encounter exposure to albedo particles emitted from the lunar surface as well as primary high-energy particles in the spectra of galactic cosmic rays (GCRs) and solar particle events (SPEs). While existing studies have examined particle energy spectra and absorbed doses in limited radiation exposure scenarios on and near the Moon, comprehensive research encompassing various shielding amounts and large SPEs on the lunar surface remains lacking. Additionally, detailed organ dose equivalents of albedo particles in a human model on the lunar surface have yet to be investigated. This work assesses the organ dose equivalents of albedo neutrons and protons during historically large SPEs in August 1972 and September 1989 utilizing realistic computational anthropomorphic human phantom for the first time. Dosimetric quantities within human organs have been evaluated based on the PHITS Monte Carlo simulation results and quality factors of the state-of-the-art NASA Space Cancer Risk (NSCR) model, as well as ICRP publications. The results with the NSCR model indicate that the albedo contribution to organ dose equivalent is less than 3% for 1 g/cm2 aluminum shielding, while it increases to more than 20% in some organs for 20 g/cm2 aluminum shielding during exposure to low-energy-proton-rich SPEs.","sentences":["Astronauts participating in lunar landing missions will encounter exposure to albedo particles emitted from the lunar surface as well as primary high-energy particles in the spectra of galactic cosmic rays (GCRs) and solar particle events (SPEs).","While existing studies have examined particle energy spectra and absorbed doses in limited radiation exposure scenarios on and near the Moon, comprehensive research encompassing various shielding amounts and large SPEs on the lunar surface remains lacking.","Additionally, detailed organ dose equivalents of albedo particles in a human model on the lunar surface have yet to be investigated.","This work assesses the organ dose equivalents of albedo neutrons and protons during historically large SPEs in August 1972 and September 1989 utilizing realistic computational anthropomorphic human phantom for the first time.","Dosimetric quantities within human organs have been evaluated based on the PHITS Monte Carlo simulation results and quality factors of the state-of-the-art NASA Space Cancer Risk (NSCR) model, as well as ICRP publications.","The results with the NSCR model indicate that the albedo contribution to organ dose equivalent is less than 3% for 1 g/cm2 aluminum shielding, while it increases to more than 20% in some organs for 20 g/cm2 aluminum shielding during exposure to low-energy-proton-rich SPEs."],"url":"http://arxiv.org/abs/2405.00143v1","category":"physics.space-ph"}
{"created":"2024-04-30 18:34:32","title":"Rolling in the Shadows: Analyzing the Extraction of MEV Across Layer-2 Rollups","abstract":"The emergence of decentralized finance has transformed asset trading on the blockchain, making traditional financial instruments more accessible while also introducing a series of exploitative economic practices known as Maximal Extractable Value (MEV). Concurrently, decentralized finance has embraced rollup-based Layer-2 solutions to facilitate asset trading at reduced transaction costs compared to Layer-1 solutions such as Ethereum. However, rollups lack a public mempool like Ethereum, making the extraction of MEV more challenging. In this paper, we investigate the prevalence and impact of MEV on Ethereum and prominent rollups such as Arbitrum, Optimism, and zkSync over a nearly three-year period. Our analysis encompasses various metrics including volume, profits, costs, competition, and response time to MEV opportunities. We discover that MEV is widespread on rollups, with trading volume comparable to Ethereum. We also find that, although MEV costs are lower on rollups, profits are also significantly lower compared to Ethereum. Additionally, we examine the prevalence of sandwich attacks on rollups. While our findings did not detect any sandwiching activity on popular rollups, we did identify the potential for cross-layer sandwich attacks facilitated by transactions that are sent across rollups and Ethereum. Consequently, we propose and evaluate the feasibility of three novel attacks that exploit cross-layer transactions, revealing that attackers could have already earned approximately 2 million USD through cross-layer sandwich attacks.","sentences":["The emergence of decentralized finance has transformed asset trading on the blockchain, making traditional financial instruments more accessible while also introducing a series of exploitative economic practices known as Maximal Extractable Value (MEV).","Concurrently, decentralized finance has embraced rollup-based Layer-2 solutions to facilitate asset trading at reduced transaction costs compared to Layer-1 solutions such as Ethereum.","However, rollups lack a public mempool like Ethereum, making the extraction of MEV more challenging.","In this paper, we investigate the prevalence and impact of MEV on Ethereum and prominent rollups such as Arbitrum, Optimism, and zkSync over a nearly three-year period.","Our analysis encompasses various metrics including volume, profits, costs, competition, and response time to MEV opportunities.","We discover that MEV is widespread on rollups, with trading volume comparable to Ethereum.","We also find that, although MEV costs are lower on rollups, profits are also significantly lower compared to Ethereum.","Additionally, we examine the prevalence of sandwich attacks on rollups.","While our findings did not detect any sandwiching activity on popular rollups, we did identify the potential for cross-layer sandwich attacks facilitated by transactions that are sent across rollups and Ethereum.","Consequently, we propose and evaluate the feasibility of three novel attacks that exploit cross-layer transactions, revealing that attackers could have already earned approximately 2 million USD through cross-layer sandwich attacks."],"url":"http://arxiv.org/abs/2405.00138v1","category":"cs.CR"}
{"created":"2024-04-30 18:28:45","title":"Engineering superpositions of N00N states using an asymmetric non-linear Mach-Zehnder interferometer","abstract":"We revisit a method for mapping arbitrary single-mode pure states into superpositions of N00N states using an asymmetric non-linear Mach-Zehnder interferometer (ANLMZI). This method would allow for one to tailor-make superpositions of N00N states where each axis of the two-mode joint-photon number distribution is weighted by the statistics of any single-mode pure state. The non-linearity of the ANLMZI comes in the form of a $\\chi^{\\left(3\\right)}$ self-Kerr interaction occurring on one of the intermediary modes of the interferometer. Motivated by the non-classical interference effects that occur at a beam splitter, we introduce inverse-engineering techniques aimed towards extrapolating optimal transformations for generating N00N state superpositions. These techniques are general enough so as to be employed to probe the means of generating states of any desired quantum properties.","sentences":["We revisit a method for mapping arbitrary single-mode pure states into superpositions of N00N states using an asymmetric non-linear Mach-Zehnder interferometer (ANLMZI).","This method would allow for one to tailor-make superpositions of N00N states where each axis of the two-mode joint-photon number distribution is weighted by the statistics of any single-mode pure state.","The non-linearity of the ANLMZI comes in the form of a $\\chi^{\\left(3\\right)}$ self-Kerr interaction occurring on one of the intermediary modes of the interferometer.","Motivated by the non-classical interference effects that occur at a beam splitter, we introduce inverse-engineering techniques aimed towards extrapolating optimal transformations for generating N00N state superpositions.","These techniques are general enough so as to be employed to probe the means of generating states of any desired quantum properties."],"url":"http://arxiv.org/abs/2405.00132v1","category":"quant-ph"}
{"created":"2024-04-30 18:28:36","title":"Finding Diverse Strings and Longest Common Subsequences in a Graph","abstract":"In this paper, we study for the first time the Diverse Longest Common Subsequences (LCSs) problem under Hamming distance. Given a set of a constant number of input strings, the problem asks to decide if there exists some subset $\\mathcal X$ of $K$ longest common subsequences whose diversity is no less than a specified threshold $\\Delta$, where we consider two types of diversities of a set $\\mathcal X$ of strings of equal length: the Sum diversity and the Min diversity defined as the sum and the minimum of the pairwise Hamming distance between any two strings in $\\mathcal X$, respectively. We analyze the computational complexity of the respective problems with Sum- and Min-diversity measures, called the Max-Sum and Max-Min Diverse LCSs, respectively, considering both approximation algorithms and parameterized complexity. Our results are summarized as follows. When $K$ is bounded, both problems are polynomial time solvable. In contrast, when $K$ is unbounded, both problems become NP-hard, while Max-Sum Diverse LCSs problem admits a PTAS. Furthermore, we analyze the parameterized complexity of both problems with combinations of parameters $K$ and $r$, where $r$ is the length of the candidate strings to be selected. Importantly, all positive results above are proven in a more general setting, where an input is an edge-labeled directed acyclic graph (DAG) that succinctly represents a set of strings of the same length. Negative results are proven in the setting where an input is explicitly given as a set of strings. The latter results are equipped with an encoding such a set as the longest common subsequences of a specific input string set.","sentences":["In this paper, we study for the first time the Diverse Longest Common Subsequences (LCSs) problem under Hamming distance.","Given a set of a constant number of input strings, the problem asks to decide if there exists some subset $\\mathcal X$ of $K$ longest common subsequences whose diversity is no less than a specified threshold $\\Delta$, where we consider two types of diversities of a set $\\mathcal X$ of strings of equal length: the Sum diversity and the Min diversity defined as the sum and the minimum of the pairwise Hamming distance between any two strings in $\\mathcal X$, respectively.","We analyze the computational complexity of the respective problems with Sum- and Min-diversity measures, called the Max-Sum and Max-Min Diverse LCSs, respectively, considering both approximation algorithms and parameterized complexity.","Our results are summarized as follows.","When $K$ is bounded, both problems are polynomial time solvable.","In contrast, when $K$ is unbounded, both problems become NP-hard, while Max-Sum Diverse LCSs problem admits a PTAS.","Furthermore, we analyze the parameterized complexity of both problems with combinations of parameters $K$ and $r$, where $r$ is the length of the candidate strings to be selected.","Importantly, all positive results above are proven in a more general setting, where an input is an edge-labeled directed acyclic graph (DAG) that succinctly represents a set of strings of the same length.","Negative results are proven in the setting where an input is explicitly given as a set of strings.","The latter results are equipped with an encoding such a set as the longest common subsequences of a specific input string set."],"url":"http://arxiv.org/abs/2405.00131v1","category":"cs.DS"}
{"created":"2024-04-30 18:15:53","title":"An enhanced POSTA based on Nelder-Mead simplex search and quadratic interpolation","abstract":"State transition algorithm (STA) is a metaheuristic method for global optimization. Recently, a modified STA named parameter optimal state transition algorithm (POSTA) is proposed. In POSTA, the performance of expansion operator, rotation operator and axesion operator is optimized through a parameter selection mechanism. But due to the insufficient utilization of historical information, POSTA still suffers from slow convergence speed and low solution accuracy on specific problems. To make better use of the historical information, Nelder-Mead (NM) simplex search and quadratic interpolation (QI) are integrated into POSTA. The enhanced POSTA is tested against 14 benchmark functions with 20-D, 30-D and 50-D space. An experimental comparison with several competitive metaheuristic methods demonstrates the effectiveness of the proposed method.","sentences":["State transition algorithm (STA) is a metaheuristic method for global optimization.","Recently, a modified STA named parameter optimal state transition algorithm (POSTA) is proposed.","In POSTA, the performance of expansion operator, rotation operator and axesion operator is optimized through a parameter selection mechanism.","But due to the insufficient utilization of historical information, POSTA still suffers from slow convergence speed and low solution accuracy on specific problems.","To make better use of the historical information, Nelder-Mead (NM) simplex search and quadratic interpolation (QI) are integrated into POSTA.","The enhanced POSTA is tested against 14 benchmark functions with 20-D, 30-D and 50-D space.","An experimental comparison with several competitive metaheuristic methods demonstrates the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2405.00122v1","category":"math.OC"}
{"created":"2024-04-30 18:08:29","title":"Causal Inference with High-dimensional Discrete Covariates","abstract":"When estimating causal effects from observational studies, researchers often need to adjust for many covariates to deconfound the non-causal relationship between exposure and outcome, among which many covariates are discrete. The behavior of commonly used estimators in the presence of many discrete covariates is not well understood since their properties are often analyzed under structural assumptions including sparsity and smoothness, which do not apply in discrete settings. In this work, we study the estimation of causal effects in a model where the covariates required for confounding adjustment are discrete but high-dimensional, meaning the number of categories $d$ is comparable with or even larger than sample size $n$. Specifically, we show the mean squared error of commonly used regression, weighting and doubly robust estimators is bounded by $\\frac{d^2}{n^2}+\\frac{1}{n}$. We then prove the minimax lower bound for the average treatment effect is of order $\\frac{d^2}{n^2 \\log^2 n}+\\frac{1}{n}$, which characterizes the fundamental difficulty of causal effect estimation in the high-dimensional discrete setting, and shows the estimators mentioned above are rate-optimal up to log-factors. We further consider additional structures that can be exploited, namely effect homogeneity and prior knowledge of the covariate distribution, and propose new estimators that enjoy faster convergence rates of order $\\frac{d}{n^2} + \\frac{1}{n}$, which achieve consistency in a broader regime. The results are illustrated empirically via simulation studies.","sentences":["When estimating causal effects from observational studies, researchers often need to adjust for many covariates to deconfound the non-causal relationship between exposure and outcome, among which many covariates are discrete.","The behavior of commonly used estimators in the presence of many discrete covariates is not well understood since their properties are often analyzed under structural assumptions including sparsity and smoothness, which do not apply in discrete settings.","In this work, we study the estimation of causal effects in a model where the covariates required for confounding adjustment are discrete but high-dimensional, meaning the number of categories $d$ is comparable with or even larger than sample size $n$. Specifically, we show the mean squared error of commonly used regression, weighting and doubly robust estimators is bounded by $\\frac{d^2}{n^2}+\\frac{1}{n}$. We then prove the minimax lower bound for the average treatment effect is of order $\\frac{d^2}{n^2 \\log^2 n}+\\frac{1}{n}$, which characterizes the fundamental difficulty of causal effect estimation in the high-dimensional discrete setting, and shows the estimators mentioned above are rate-optimal up to log-factors.","We further consider additional structures that can be exploited, namely effect homogeneity and prior knowledge of the covariate distribution, and propose new estimators that enjoy faster convergence rates of order $\\frac{d}{n^2} + \\frac{1}{n}$, which achieve consistency in a broader regime.","The results are illustrated empirically via simulation studies."],"url":"http://arxiv.org/abs/2405.00118v1","category":"math.ST"}
{"created":"2024-04-30 18:00:51","title":"Successive Interference Cancellation for ISAC in a Large Full-Duplex Cellular Network","abstract":"To reuse the scarce spectrum efficiently, a large full-duplex cellular network with integrated sensing and communication (ISAC) is studied. Monostatic detection at the base station (BS) is considered. At the BS, we receive two signals: the communication-mode uplink signal to be decoded and the radar-mode signal to be detected. After self-interference cancellation (SIC), inspired by NOMA, successive interference cancellation (SuIC) is a natural strategy at the BS to retrieve both signals. However, the ordering of SuIC, usually based on some measure of channel strength, is not clear as the radar-mode target is unknown. The detection signal suffers a double path-loss making it vulnerable, but the uplink signal to be decoded originates at a user which has much lower power than the BS making it weak as well. Further, the intercell interference from a large network reduces the channel disparity between the two signals. We investigate the impact of both SuIC orders at the BS, i.e., decoding $1^{st}$ or detecting $1^{st}$ and highlight the importance of careful order selection. We find the existence of a threshold target distance before which detecting $1^{st}$ is superior and decoding $2^{nd}$ does not suffer much. After this distance, both decoding $1^{st}$ and detecting $2^{nd}$ is superior. Similarly, a threshold UE power exists after which the optimum SuIC order changes. We consider imperfections in SIC; this helps highlight the vulnerability of the decoding and detection in the setup.","sentences":["To reuse the scarce spectrum efficiently, a large full-duplex cellular network with integrated sensing and communication (ISAC) is studied.","Monostatic detection at the base station (BS) is considered.","At the BS, we receive two signals: the communication-mode uplink signal to be decoded and the radar-mode signal to be detected.","After self-interference cancellation (SIC), inspired by NOMA, successive interference cancellation (SuIC) is a natural strategy at the BS to retrieve both signals.","However, the ordering of SuIC, usually based on some measure of channel strength, is not clear as the radar-mode target is unknown.","The detection signal suffers a double path-loss making it vulnerable, but the uplink signal to be decoded originates at a user which has much lower power than the BS making it weak as well.","Further, the intercell interference from a large network reduces the channel disparity between the two signals.","We investigate the impact of both SuIC orders at the BS, i.e., decoding $1^{st}$ or detecting $1^{st}$ and highlight the importance of careful order selection.","We find the existence of a threshold target distance before which detecting $1^{st}$ is superior and decoding $2^{nd}$ does not suffer much.","After this distance, both decoding $1^{st}$ and detecting $2^{nd}$ is superior.","Similarly, a threshold UE power exists after which the optimum SuIC order changes.","We consider imperfections in SIC; this helps highlight the vulnerability of the decoding and detection in the setup."],"url":"http://arxiv.org/abs/2405.00109v1","category":"cs.IT"}
{"created":"2024-04-30 18:00:11","title":"Gas, not dust: Migration of TESS/Gaia hot Jupiters possibly halted by the magnetospheres of protoplanetary disks","abstract":"(Abridged) The presence of short-period (< 10 days) planets around main sequence (MS) stars has been associated either with the dust-destruction region or with the magnetospheric gas-truncation radius in the protoplanetary disks that surround them during the pre-MS phase. However, previous analyses have only considered low-mass FGK stars, making it difficult to disentangle the two scenarios. This exploratory study is aimed at testing whether it is the inner dust or gas disk driving the location of short-period, giant planets. By combining TESS and Gaia DR3 data, we identified a sample of 47 intermediate-mass (1.5-3 M$_{\\odot}$) MS stars hosting confirmed and firm candidate hot Jupiters. We compared their orbits with the rough position of the inner dust and gas disks, which are well separated around their Herbig stars precursors. We also made a comparison with the orbits of confirmed hot Jupiters around a similarly extracted TESS/Gaia sample of low-mass sources (0.5-1.5 M$_{\\odot}$). Our results suggest that the inner gas (and not the dust) disk limits the innermost orbits of hot Jupiters around intermediate-mass stars. These findings also provide tentative support to previous works that have claimed this is indeed the case for low-mass sources. We propose that hot Jupiters could be explained via a combination of the core-accretion paradigm and migration up to the gas-truncation radius, which may be responsible for halting inward migration regardless of the stellar mass regime. Larger samples of intermediate-mass stars with hot Jupiters are necessary to confirm our hypothesis, which implies that massive Herbig stars without magnetospheres (> 3-4 M$_{\\odot}$) may be the most efficient in swallowing their newborn planets.","sentences":["(Abridged)","The presence of short-period (< 10 days) planets around main sequence (MS) stars has been associated either with the dust-destruction region or with the magnetospheric gas-truncation radius in the protoplanetary disks that surround them during the pre-MS phase.","However, previous analyses have only considered low-mass FGK stars, making it difficult to disentangle the two scenarios.","This exploratory study is aimed at testing whether it is the inner dust or gas disk driving the location of short-period, giant planets.","By combining TESS and Gaia DR3 data, we identified a sample of 47 intermediate-mass (1.5-3 M$_{\\odot}$) MS stars hosting confirmed and firm candidate hot Jupiters.","We compared their orbits with the rough position of the inner dust and gas disks, which are well separated around their Herbig stars precursors.","We also made a comparison with the orbits of confirmed hot Jupiters around a similarly extracted TESS/Gaia sample of low-mass sources (0.5-1.5 M$_{\\odot}$).","Our results suggest that the inner gas (and not the dust) disk limits the innermost orbits of hot Jupiters around intermediate-mass stars.","These findings also provide tentative support to previous works that have claimed this is indeed the case for low-mass sources.","We propose that hot Jupiters could be explained via a combination of the core-accretion paradigm and migration up to the gas-truncation radius, which may be responsible for halting inward migration regardless of the stellar mass regime.","Larger samples of intermediate-mass stars with hot Jupiters are necessary to confirm our hypothesis, which implies that massive Herbig stars without magnetospheres (> 3-4 M$_{\\odot}$) may be the most efficient in swallowing their newborn planets."],"url":"http://arxiv.org/abs/2405.00106v1","category":"astro-ph.EP"}
{"created":"2024-04-30 18:00:02","title":"Applying machine learning to Galactic Archaeology: how well can we recover the origin of stars in Milky Way-like galaxies?","abstract":"We present several machine learning (ML) models developed to efficiently separate stars formed in-situ in Milky Way-type galaxies from those that were formed externally and later accreted. These models, which include examples from artificial neural networks, decision trees and dimensionality reduction techniques, are trained on a sample of disc-like, Milky Way-mass galaxies drawn from the ARTEMIS cosmological hydrodynamical zoom-in simulations. We find that the input parameters which provide an optimal performance for these models consist of a combination of stellar positions, kinematics, chemical abundances ([Fe/H] and [$\\alpha$/Fe]) and photometric properties. Models from all categories perform similarly well, with area under the precision-recall curve (PR-AUC) scores of $\\simeq 0.6$. Beyond a galactocentric radius of $5$~kpc, models retrieve $>90\\%$ of accreted stars, with a sample purity close to $60\\%$, however the purity can be increased by adjusting the classification threshold. For one model, we also include host galaxy-specific properties in the training, to account for the variability of accretion histories of the hosts, however this does not lead to an improvement in performance. The ML models can identify accreted stars even in regions heavily dominated by the in-situ component (e.g., in the disc), and perform well on an unseen suite of simulations (the Auriga simulations). The general applicability bodes well for application of such methods on observational data to identify accreted substructures in the Milky Way without the need to resort to selection cuts for minimising the contamination from in-situ stars.","sentences":["We present several machine learning (ML) models developed to efficiently separate stars formed in-situ in Milky Way-type galaxies from those that were formed externally and later accreted.","These models, which include examples from artificial neural networks, decision trees and dimensionality reduction techniques, are trained on a sample of disc-like, Milky Way-mass galaxies drawn from the ARTEMIS cosmological hydrodynamical zoom-in simulations.","We find that the input parameters which provide an optimal performance for these models consist of a combination of stellar positions, kinematics, chemical abundances ([Fe/H] and [$\\alpha$/Fe]) and photometric properties.","Models from all categories perform similarly well, with area under the precision-recall curve (PR-AUC) scores of $\\simeq 0.6$. Beyond a galactocentric radius of $5$~kpc, models retrieve $>90\\%$ of accreted stars, with a sample purity close to $60\\%$, however the purity can be increased by adjusting the classification threshold.","For one model, we also include host galaxy-specific properties in the training, to account for the variability of accretion histories of the hosts, however this does not lead to an improvement in performance.","The ML models can identify accreted stars even in regions heavily dominated by the in-situ component (e.g., in the disc), and perform well on an unseen suite of simulations (the Auriga simulations).","The general applicability bodes well for application of such methods on observational data to identify accreted substructures in the Milky Way without the need to resort to selection cuts for minimising the contamination from in-situ stars."],"url":"http://arxiv.org/abs/2405.00102v1","category":"astro-ph.GA"}
{"created":"2024-04-30 18:00:01","title":"Type II t-J model in charge transfer regime in bilayer La$_3$Ni$_2$O$_7$ and trilayer La$_4$Ni$_3$O$_{10}$","abstract":"Recent observations of an 80 K superconductor in La$_3$Ni$_2$O$_7$ under high pressure have attracted significant attention. Recent experiments indicate that La$_3$Ni$_2$O$_7$ may be in the charge transfer regime, challenging the previous models based purely on the Ni $d_{x^2-y^2}$ and $d_{z^2}$ orbitals. In this study, we propose a low energy model that incorporates doped holes in the oxygen $p$ orbitals. Given that the parent nickel state is in the $3d^{8}$ configuration with a spin-one moment, doped hole only screens it down to spin-half, in contrast to the Zhang-Rice singlet in cuprate. We dub the single hole state as Zhang-Rice spin-half and build an effective model which includes three spin-one states ($d^8$) and two Zhang-Rice spin-half states ($d^8 L$). At moderate pressure around $20$ GPa, the dominated oxygen orbital is an in-plane Wannier orbital with the same lattice symmetry as the $d_{x^2-y^2}$ orbital. The resulting model reduces to the bilayer type II t-J model previously proposed in the Mott-Hubbard regime. Notably, the hopping between the in-plane $p$ orbitals of the two layers is still suppressed. Density matrix renormalization group (DMRG) simulation reveals a pairing dome with the optimal hole doping level at $x=0.4\\sim0.5$, distinct from the hole doped cuprate where optimal doping occurs around $x=0.19$. Further increasing pressure initially raises the critical temperature ($T_c$) until reaching an optimal pressure beyond which the $p_z$ orbital of oxygen becomes favorable and superconductivity is diminished. This shift from in-plane $p$ orbital to $p_z$ orbital may elucidate the experimentally observed superconducting dome with varying pressure. As an extension, we also suggest a trilayer version of the type II t-J model as the minimal model for pressured La$_4$Ni$_3$O$_{10}$, which is distinct from the models in the Mott-Hubbard regime.","sentences":["Recent observations of an 80 K superconductor in La$_3$Ni$_2$O$_7$ under high pressure have attracted significant attention.","Recent experiments indicate that La$_3$Ni$_2$O$_7$ may be in the charge transfer regime, challenging the previous models based purely on the Ni $d_{x^2-y^2}$ and $d_{z^2}$ orbitals.","In this study, we propose a low energy model that incorporates doped holes in the oxygen $p$ orbitals.","Given that the parent nickel state is in the $3d^{8}$ configuration with a spin-one moment, doped hole only screens it down to spin-half, in contrast to the Zhang-Rice singlet in cuprate.","We dub the single hole state as Zhang-Rice spin-half and build an effective model which includes three spin-one states ($d^8$) and two Zhang-Rice spin-half states ($d^8 L$).","At moderate pressure around $20$ GPa, the dominated oxygen orbital is an in-plane Wannier orbital with the same lattice symmetry as the $d_{x^2-y^2}$ orbital.","The resulting model reduces to the bilayer type II t-J model previously proposed in the Mott-Hubbard regime.","Notably, the hopping between the in-plane $p$ orbitals of the two layers is still suppressed.","Density matrix renormalization group (DMRG) simulation reveals a pairing dome with the optimal hole doping level at $x=0.4\\sim0.5$, distinct from the hole doped cuprate where optimal doping occurs around $x=0.19$. Further increasing pressure initially raises the critical temperature ($T_c$) until reaching an optimal pressure beyond which the $p_z$ orbital of oxygen becomes favorable and superconductivity is diminished.","This shift from in-plane $p$ orbital to $p_z$ orbital may elucidate the experimentally observed superconducting dome with varying pressure.","As an extension, we also suggest a trilayer version of the type II t-J model as the minimal model for pressured La$_4$Ni$_3$O$_{10}$, which is distinct from the models in the Mott-Hubbard regime."],"url":"http://arxiv.org/abs/2405.00092v1","category":"cond-mat.str-el"}
{"created":"2024-04-30 18:00:00","title":"Structure learning of Hamiltonians from real-time evolution","abstract":"We initiate the study of Hamiltonian structure learning from real-time evolution: given the ability to apply $e^{-\\mathrm{i} Ht}$ for an unknown local Hamiltonian $H = \\sum_{a = 1}^m \\lambda_a E_a$ on $n$ qubits, the goal is to recover $H$. This problem is already well-studied under the assumption that the interaction terms, $E_a$, are given, and only the interaction strengths, $\\lambda_a$, are unknown. But is it possible to learn a local Hamiltonian without prior knowledge of its interaction structure?   We present a new, general approach to Hamiltonian learning that not only solves the challenging structure learning variant, but also resolves other open questions in the area, all while achieving the gold standard of Heisenberg-limited scaling. In particular, our algorithm recovers the Hamiltonian to $\\varepsilon$ error with an evolution time scaling with $1/\\varepsilon$, and has the following appealing properties: (1) it does not need to know the Hamiltonian terms; (2) it works beyond the short-range setting, extending to any Hamiltonian $H$ where the sum of terms interacting with a qubit has bounded norm; (3) it evolves according to $H$ in constant time $t$ increments, thus achieving constant time resolution. To our knowledge, no prior algorithm with Heisenberg-limited scaling existed with even one of these properties. As an application, we can also learn Hamiltonians exhibiting power-law decay up to accuracy $\\varepsilon$ with total evolution time beating the standard limit of $1/\\varepsilon^2$.","sentences":["We initiate the study of Hamiltonian structure learning from real-time evolution: given the ability to apply $e^{-\\mathrm{i} Ht}$ for an unknown local Hamiltonian $H = \\sum_{a = 1}^m \\lambda_a E_a$ on $n$ qubits, the goal is to recover $H$.","This problem is already well-studied under the assumption that the interaction terms, $E_a$, are given, and only the interaction strengths, $\\lambda_a$, are unknown.","But is it possible to learn a local Hamiltonian without prior knowledge of its interaction structure?   ","We present a new, general approach to Hamiltonian learning that not only solves the challenging structure learning variant, but also resolves other open questions in the area, all while achieving the gold standard of Heisenberg-limited scaling.","In particular, our algorithm recovers the Hamiltonian to $\\varepsilon$ error with an evolution time scaling with $1/\\varepsilon$, and has the following appealing properties: (1) it does not need to know the Hamiltonian terms; (2) it works beyond the short-range setting, extending to any Hamiltonian $H$ where the sum of terms interacting with a qubit has bounded norm; (3) it evolves according to $H$ in constant time $t$ increments, thus achieving constant time resolution.","To our knowledge, no prior algorithm with Heisenberg-limited scaling existed with even one of these properties.","As an application, we can also learn Hamiltonians exhibiting power-law decay up to accuracy $\\varepsilon$ with total evolution time beating the standard limit of $1/\\varepsilon^2$."],"url":"http://arxiv.org/abs/2405.00082v1","category":"quant-ph"}
{"created":"2024-04-30 18:00:00","title":"X-Shooting ULLYSES: Massive Stars at Low Metallicity","abstract":"The Hubble Space Telescope has devoted 500 orbits to observing 250 massive stars with low metallicity in the ultraviolet (UV) range within the framework of the ULLYSES program. The X-Shooting ULLYSES (XShootU) project enhances the legacy value of this UV dataset by providing high-quality optical and near-infrared spectra, which are acquired using the wide-wavelength-coverage X-shooter spectrograph at ESO's Very Large Telescope. XShootU emphasises the importance of combining UV with optical spectra for the consistent determination of key stellar parameters such as effective temperature, surface gravity, luminosity, abundances, and wind characteristics including mass-loss rates as a function of metallicity. Since uncertainties in these parameters have implications across various branches of astrophysics, the data and modelling generated by the XShootU project are poised to significantly advance our understanding of massive stars at low metallicity. This is particularly crucial for confidently interpreting JWST data of the earliest stellar generations, making XShootU a unique resource for comprehending individual spectra of low-metallicity stars.","sentences":["The Hubble Space Telescope has devoted 500 orbits to observing 250 massive stars with low metallicity in the ultraviolet (UV) range within the framework of the ULLYSES program.","The X-Shooting ULLYSES (XShootU) project enhances the legacy value of this UV dataset by providing high-quality optical and near-infrared spectra, which are acquired using the wide-wavelength-coverage X-shooter spectrograph at ESO's Very Large Telescope.","XShootU emphasises the importance of combining UV with optical spectra for the consistent determination of key stellar parameters such as effective temperature, surface gravity, luminosity, abundances, and wind characteristics including mass-loss rates as a function of metallicity.","Since uncertainties in these parameters have implications across various branches of astrophysics, the data and modelling generated by the XShootU project are poised to significantly advance our understanding of massive stars at low metallicity.","This is particularly crucial for confidently interpreting JWST data of the earliest stellar generations, making XShootU a unique resource for comprehending individual spectra of low-metallicity stars."],"url":"http://arxiv.org/abs/2405.00085v1","category":"astro-ph.SR"}
{"created":"2024-04-30 18:00:00","title":"Cosmic Ray-Boosted Dark Matter at IceCube","abstract":"Cosmic ray (CR) upscattering of dark matter is considered as one of the most straightforward mechanisms to accelerate ambient dark matter, making it detectable at high threshold, large volume experiments. In this work, we revisit CR upscattered dark matter signals at the IceCube detector, focusing on lower energy data than was considered before. We consider both scattering with electrons and nuclei. In the latter, we include both elastic and deep-inelastic scattering computations. As concrete examples, we consider two benchmark models; Fermion dark matter with vector and scalar mediators. We compare our model projections with the most current constraints and show that the IceCube detector can detect CR-boosted dark matter especially with masses below $\\sim$ 100 keV when scattering with electrons and $\\sim$ MeV in the nucleon scattering case.","sentences":["Cosmic ray (CR) upscattering of dark matter is considered as one of the most straightforward mechanisms to accelerate ambient dark matter, making it detectable at high threshold, large volume experiments.","In this work, we revisit CR upscattered dark matter signals at the IceCube detector, focusing on lower energy data than was considered before.","We consider both scattering with electrons and nuclei.","In the latter, we include both elastic and deep-inelastic scattering computations.","As concrete examples, we consider two benchmark models; Fermion dark matter with vector and scalar mediators.","We compare our model projections with the most current constraints and show that the IceCube detector can detect CR-boosted dark matter especially with masses below $\\sim$ 100 keV when scattering with electrons and $\\sim$ MeV in the nucleon scattering case."],"url":"http://arxiv.org/abs/2405.00086v1","category":"hep-ph"}
{"created":"2024-04-30 14:55:14","title":"A global evidence map of human well-being and biodiversity co-benefits and trade-offs of natural climate solutions","abstract":"Natural climate solutions (NCS) are critical for mitigating climate change through ecosystem-based carbon removal and emissions reductions. NCS implementation can also generate biodiversity and human well-being co-benefits and trade-offs (\"NCS co-impacts\"), but the volume of evidence on NCS co-impacts has grown rapidly across disciplines, is poorly understood, and remains to be systematically collated and synthesized. A global evidence map of NCS co-impacts would overcome key barriers to NCS implementation by providing relevant information on co-benefits and trade-offs where carbon mitigation potential alone does not justify NCS projects. We employ large language models to assess over two million articles, finding 257,266 relevant articles on NCS co-impacts. We analyze this large and dispersed body of literature using innovative machine learning methods to extract relevant data (e.g., study location, species, and other key variables), and create a global evidence map on NCS co-impacts. Evidence on NCS co-impacts has grown approximately ten-fold in three decades, although some of the most abundant evidence is associated with pathways that have less mitigation potential. We find that studies often examine multiple NCS pathways, indicating natural NCS pathway complements, and each NCS is often associated with two or more coimpacts. Finally, NCS co-impacts evidence and priority areas for NCS are often mismatched--some countries with high mitigation potential from NCS have few published studies on the broader co-impacts of NCS implementation. Our work advances and makes available novel methods and systematic and representative data of NCS co-impacts studies, thus providing timely insights to inform NCS research and action globally.","sentences":["Natural climate solutions (NCS) are critical for mitigating climate change through ecosystem-based carbon removal and emissions reductions.","NCS implementation can also generate biodiversity and human well-being co-benefits and trade-offs (\"NCS co-impacts\"), but the volume of evidence on NCS co-impacts has grown rapidly across disciplines, is poorly understood, and remains to be systematically collated and synthesized.","A global evidence map of NCS co-impacts would overcome key barriers to NCS implementation by providing relevant information on co-benefits and trade-offs where carbon mitigation potential alone does not justify NCS projects.","We employ large language models to assess over two million articles, finding 257,266 relevant articles on NCS co-impacts.","We analyze this large and dispersed body of literature using innovative machine learning methods to extract relevant data (e.g., study location, species, and other key variables), and create a global evidence map on NCS co-impacts.","Evidence on NCS co-impacts has grown approximately ten-fold in three decades, although some of the most abundant evidence is associated with pathways that have less mitigation potential.","We find that studies often examine multiple NCS pathways, indicating natural NCS pathway complements, and each NCS is often associated with two or more coimpacts.","Finally, NCS co-impacts evidence and priority areas for NCS are often mismatched--some countries with high mitigation potential from NCS have few published studies on the broader co-impacts of NCS implementation.","Our work advances and makes available novel methods and systematic and representative data of NCS co-impacts studies, thus providing timely insights to inform NCS research and action globally."],"url":"http://arxiv.org/abs/2405.00079v1","category":"physics.soc-ph"}
{"created":"2024-05-01 16:29:33","title":"Capillary-Assisted Printing of Droplets at a Solid-Like Liquid-Liquid Interface","abstract":"Capillary forces guide the motion of biomolecular condensates, water-borne insects, and breakfast cereal. These surface-mediated interactions can be harnessed to build units into materials with exotic properties deriving from mesoscale structure. Droplets are promising building blocks for these materials, finding applications in tissue engineering, adaptive optics, and structural colour. However, the instability of water droplets at many liquid-liquid interfaces hampers the use of capillarity for the assembly of droplet-based materials. Here, we use nanoparticle surfactants to form solid-like oil-water interfaces at which aqueous droplets sit for extended periods. We find that microlitre-sized droplets at these interfaces attract each other over millimetric scales. We rationalize this interaction with a modified theory of capillarity. Applying printing methods allows us to finely control initial droplet positions, from which they self-assemble into cellular materials. Finally, by functionalising the interface with gold nanoparticles, we use plasmon-assisted optofluidics to manipulate these droplet-based materials with temperature gradients.","sentences":["Capillary forces guide the motion of biomolecular condensates, water-borne insects, and breakfast cereal.","These surface-mediated interactions can be harnessed to build units into materials with exotic properties deriving from mesoscale structure.","Droplets are promising building blocks for these materials, finding applications in tissue engineering, adaptive optics, and structural colour.","However, the instability of water droplets at many liquid-liquid interfaces hampers the use of capillarity for the assembly of droplet-based materials.","Here, we use nanoparticle surfactants to form solid-like oil-water interfaces at which aqueous droplets sit for extended periods.","We find that microlitre-sized droplets at these interfaces attract each other over millimetric scales.","We rationalize this interaction with a modified theory of capillarity.","Applying printing methods allows us to finely control initial droplet positions, from which they self-assemble into cellular materials.","Finally, by functionalising the interface with gold nanoparticles, we use plasmon-assisted optofluidics to manipulate these droplet-based materials with temperature gradients."],"url":"http://arxiv.org/abs/2405.00609v1","category":"cond-mat.soft"}
{"created":"2024-05-01 15:58:55","title":"The origin of wall-shear stress fluctuations in wall-bounded turbulence","abstract":"The origin of wall shear-stress fluctuations in wall turbulence was studied through energy dissipation at the wall. While confirming the universality in wall dissipation at small inner scales, the dissipation at larger scales is a consequence of near-wall scale interactions. In particular, the energy transport from the universal small to larger scale strengthens with Reynolds number due to the growing number of intermediate scales associated with the log layer. We anticipate that these insights broadly apply to all canonical wall-bounded turbulence for sufficiently high Reynolds numbers.","sentences":["The origin of wall shear-stress fluctuations in wall turbulence was studied through energy dissipation at the wall.","While confirming the universality in wall dissipation at small inner scales, the dissipation at larger scales is a consequence of near-wall scale interactions.","In particular, the energy transport from the universal small to larger scale strengthens with Reynolds number due to the growing number of intermediate scales associated with the log layer.","We anticipate that these insights broadly apply to all canonical wall-bounded turbulence for sufficiently high Reynolds numbers."],"url":"http://arxiv.org/abs/2405.00591v1","category":"physics.flu-dyn"}
{"created":"2024-05-01 15:18:37","title":"A novel central compact finite-difference scheme for third derivatives with high spectral resolution","abstract":"In this paper, we introduce a novel category of central compact schemes inspired by existing cell-node and cell-centered compact finite difference schemes, that offer a superior spectral resolution for solving the dispersive wave equation. In our approach, we leverage both the function values at the cell nodes and cell centers to calculate third-order spatial derivatives at the cell nodes. To compute spatial derivatives at the cell centers, we employ a technique that involves half-shifting the indices within the formula initially designed for the cell-nodes. In contrast to the conventional compact interpolation scheme, our proposed method effectively sidesteps the introduction of transfer errors. We employ the Taylor-series expansion-based method to calculate the finite difference coefficients. By conducting systematic Fourier analysis and numerical tests, we note that the methods exhibit exceptional characteristics such as high order, superior resolution, and low dissipation. Computational findings further illustrate the effectiveness of high-order compact schemes, particularly in addressing problems with a third derivative term.","sentences":["In this paper, we introduce a novel category of central compact schemes inspired by existing cell-node and cell-centered compact finite difference schemes, that offer a superior spectral resolution for solving the dispersive wave equation.","In our approach, we leverage both the function values at the cell nodes and cell centers to calculate third-order spatial derivatives at the cell nodes.","To compute spatial derivatives at the cell centers, we employ a technique that involves half-shifting the indices within the formula initially designed for the cell-nodes.","In contrast to the conventional compact interpolation scheme, our proposed method effectively sidesteps the introduction of transfer errors.","We employ the Taylor-series expansion-based method to calculate the finite difference coefficients.","By conducting systematic Fourier analysis and numerical tests, we note that the methods exhibit exceptional characteristics such as high order, superior resolution, and low dissipation.","Computational findings further illustrate the effectiveness of high-order compact schemes, particularly in addressing problems with a third derivative term."],"url":"http://arxiv.org/abs/2405.00569v1","category":"math.NA"}
{"created":"2024-05-01 11:29:22","title":"The Nucleus of a Compact Lie Group, and Support of Singularity Categories","abstract":"In this paper we adapt the notion of the nucleus defined by Benson, Carlson, and Robinson to compact Lie groups in non-modular characteristic. We show that it describes the singularities of the projective scheme of the cohomology of its classifying space. A notion of support for singularity categories of ring spectra (in the sense of Greenlees and Stevenson) is established, and is shown to be precisely the nucleus in this case, consistent with a conjecture of Benson and Greenlees for finite groups.","sentences":["In this paper we adapt the notion of the nucleus defined by Benson, Carlson, and Robinson to compact Lie groups in non-modular characteristic.","We show that it describes the singularities of the projective scheme of the cohomology of its classifying space.","A notion of support for singularity categories of ring spectra (in the sense of Greenlees and Stevenson) is established, and is shown to be precisely the nucleus in this case, consistent with a conjecture of Benson and Greenlees for finite groups."],"url":"http://arxiv.org/abs/2405.00457v1","category":"math.AT"}
{"created":"2024-05-01 06:26:35","title":"NC-SDF: Enhancing Indoor Scene Reconstruction Using Neural SDFs with View-Dependent Normal Compensation","abstract":"State-of-the-art neural implicit surface representations have achieved impressive results in indoor scene reconstruction by incorporating monocular geometric priors as additional supervision. However, we have observed that multi-view inconsistency between such priors poses a challenge for high-quality reconstructions. In response, we present NC-SDF, a neural signed distance field (SDF) 3D reconstruction framework with view-dependent normal compensation (NC). Specifically, we integrate view-dependent biases in monocular normal priors into the neural implicit representation of the scene. By adaptively learning and correcting the biases, our NC-SDF effectively mitigates the adverse impact of inconsistent supervision, enhancing both the global consistency and local details in the reconstructions. To further refine the details, we introduce an informative pixel sampling strategy to pay more attention to intricate geometry with higher information content. Additionally, we design a hybrid geometry modeling approach to improve the neural implicit representation. Experiments on synthetic and real-world datasets demonstrate that NC-SDF outperforms existing approaches in terms of reconstruction quality.","sentences":["State-of-the-art neural implicit surface representations have achieved impressive results in indoor scene reconstruction by incorporating monocular geometric priors as additional supervision.","However, we have observed that multi-view inconsistency between such priors poses a challenge for high-quality reconstructions.","In response, we present NC-SDF, a neural signed distance field (SDF) 3D reconstruction framework with view-dependent normal compensation (NC).","Specifically, we integrate view-dependent biases in monocular normal priors into the neural implicit representation of the scene.","By adaptively learning and correcting the biases, our NC-SDF effectively mitigates the adverse impact of inconsistent supervision, enhancing both the global consistency and local details in the reconstructions.","To further refine the details, we introduce an informative pixel sampling strategy to pay more attention to intricate geometry with higher information content.","Additionally, we design a hybrid geometry modeling approach to improve the neural implicit representation.","Experiments on synthetic and real-world datasets demonstrate that NC-SDF outperforms existing approaches in terms of reconstruction quality."],"url":"http://arxiv.org/abs/2405.00340v1","category":"cs.CV"}
{"created":"2024-05-01 00:30:13","title":"CREPE: Coordinate-Aware End-to-End Document Parser","abstract":"In this study, we formulate an OCR-free sequence generation model for visual document understanding (VDU). Our model not only parses text from document images but also extracts the spatial coordinates of the text based on the multi-head architecture. Named as Coordinate-aware End-to-end Document Parser (CREPE), our method uniquely integrates these capabilities by introducing a special token for OCR text, and token-triggered coordinate decoding. We also proposed a weakly-supervised framework for cost-efficient training, requiring only parsing annotations without high-cost coordinate annotations. Our experimental evaluations demonstrate CREPE's state-of-the-art performances on document parsing tasks. Beyond that, CREPE's adaptability is further highlighted by its successful usage in other document understanding tasks such as layout analysis, document visual question answering, and so one. CREPE's abilities including OCR and semantic parsing not only mitigate error propagation issues in existing OCR-dependent methods, it also significantly enhance the functionality of sequence generation models, ushering in a new era for document understanding studies.","sentences":["In this study, we formulate an OCR-free sequence generation model for visual document understanding (VDU).","Our model not only parses text from document images but also extracts the spatial coordinates of the text based on the multi-head architecture.","Named as Coordinate-aware End-to-end Document Parser (CREPE), our method uniquely integrates these capabilities by introducing a special token for OCR text, and token-triggered coordinate decoding.","We also proposed a weakly-supervised framework for cost-efficient training, requiring only parsing annotations without high-cost coordinate annotations.","Our experimental evaluations demonstrate CREPE's state-of-the-art performances on document parsing tasks.","Beyond that, CREPE's adaptability is further highlighted by its successful usage in other document understanding tasks such as layout analysis, document visual question answering, and so one.","CREPE's abilities including OCR and semantic parsing not only mitigate error propagation issues in existing OCR-dependent methods, it also significantly enhance the functionality of sequence generation models, ushering in a new era for document understanding studies."],"url":"http://arxiv.org/abs/2405.00260v1","category":"cs.CV"}
{"created":"2024-05-01 00:13:05","title":"ASAM: Boosting Segment Anything Model with Adversarial Tuning","abstract":"In the evolving landscape of computer vision, foundation models have emerged as pivotal tools, exhibiting exceptional adaptability to a myriad of tasks. Among these, the Segment Anything Model (SAM) by Meta AI has distinguished itself in image segmentation. However, SAM, like its counterparts, encounters limitations in specific niche applications, prompting a quest for enhancement strategies that do not compromise its inherent capabilities. This paper introduces ASAM, a novel methodology that amplifies SAM's performance through adversarial tuning. We harness the potential of natural adversarial examples, inspired by their successful implementation in natural language processing. By utilizing a stable diffusion model, we augment a subset (1%) of the SA-1B dataset, generating adversarial instances that are more representative of natural variations rather than conventional imperceptible perturbations. Our approach maintains the photorealism of adversarial examples and ensures alignment with original mask annotations, thereby preserving the integrity of the segmentation task. The fine-tuned ASAM demonstrates significant improvements across a diverse range of segmentation tasks without necessitating additional data or architectural modifications. The results of our extensive evaluations confirm that ASAM establishes new benchmarks in segmentation tasks, thereby contributing to the advancement of foundational models in computer vision. Our project page is in https://asam2024.github.io/.","sentences":["In the evolving landscape of computer vision, foundation models have emerged as pivotal tools, exhibiting exceptional adaptability to a myriad of tasks.","Among these, the Segment Anything Model (SAM) by Meta AI has distinguished itself in image segmentation.","However, SAM, like its counterparts, encounters limitations in specific niche applications, prompting a quest for enhancement strategies that do not compromise its inherent capabilities.","This paper introduces ASAM, a novel methodology that amplifies SAM's performance through adversarial tuning.","We harness the potential of natural adversarial examples, inspired by their successful implementation in natural language processing.","By utilizing a stable diffusion model, we augment a subset (1%) of the SA-1B dataset, generating adversarial instances that are more representative of natural variations rather than conventional imperceptible perturbations.","Our approach maintains the photorealism of adversarial examples and ensures alignment with original mask annotations, thereby preserving the integrity of the segmentation task.","The fine-tuned ASAM demonstrates significant improvements across a diverse range of segmentation tasks without necessitating additional data or architectural modifications.","The results of our extensive evaluations confirm that ASAM establishes new benchmarks in segmentation tasks, thereby contributing to the advancement of foundational models in computer vision.","Our project page is in https://asam2024.github.io/."],"url":"http://arxiv.org/abs/2405.00256v1","category":"cs.CV"}
{"created":"2024-04-30 23:29:26","title":"Towards Real-World HDR Video Reconstruction: A Large-Scale Benchmark Dataset and A Two-Stage Alignment Network","abstract":"As an important and practical way to obtain high dynamic range (HDR) video, HDR video reconstruction from sequences with alternating exposures is still less explored, mainly due to the lack of large-scale real-world datasets. Existing methods are mostly trained on synthetic datasets, which perform poorly in real scenes. In this work, to facilitate the development of real-world HDR video reconstruction, we present Real-HDRV, a large-scale real-world benchmark dataset for HDR video reconstruction, featuring various scenes, diverse motion patterns, and high-quality labels. Specifically, our dataset contains 500 LDRs-HDRs video pairs, comprising about 28,000 LDR frames and 4,000 HDR labels, covering daytime, nighttime, indoor, and outdoor scenes. To our best knowledge, our dataset is the largest real-world HDR video reconstruction dataset. Correspondingly, we propose an end-to-end network for HDR video reconstruction, where a novel two-stage strategy is designed to perform alignment sequentially. Specifically, the first stage performs global alignment with the adaptively estimated global offsets, reducing the difficulty of subsequent alignment. The second stage implicitly performs local alignment in a coarse-to-fine manner at the feature level using the adaptive separable convolution. Extensive experiments demonstrate that: (1) models trained on our dataset can achieve better performance on real scenes than those trained on synthetic datasets; (2) our method outperforms previous state-of-the-art methods. Our dataset is available at https://github.com/yungsyu99/Real-HDRV.","sentences":["As an important and practical way to obtain high dynamic range (HDR) video, HDR video reconstruction from sequences with alternating exposures is still less explored, mainly due to the lack of large-scale real-world datasets.","Existing methods are mostly trained on synthetic datasets, which perform poorly in real scenes.","In this work, to facilitate the development of real-world HDR video reconstruction, we present Real-HDRV, a large-scale real-world benchmark dataset for HDR video reconstruction, featuring various scenes, diverse motion patterns, and high-quality labels.","Specifically, our dataset contains 500 LDRs-HDRs video pairs, comprising about 28,000 LDR frames and 4,000 HDR labels, covering daytime, nighttime, indoor, and outdoor scenes.","To our best knowledge, our dataset is the largest real-world HDR video reconstruction dataset.","Correspondingly, we propose an end-to-end network for HDR video reconstruction, where a novel two-stage strategy is designed to perform alignment sequentially.","Specifically, the first stage performs global alignment with the adaptively estimated global offsets, reducing the difficulty of subsequent alignment.","The second stage implicitly performs local alignment in a coarse-to-fine manner at the feature level using the adaptive separable convolution.","Extensive experiments demonstrate that: (1) models trained on our dataset can achieve better performance on real scenes than those trained on synthetic datasets; (2) our method outperforms previous state-of-the-art methods.","Our dataset is available at https://github.com/yungsyu99/Real-HDRV."],"url":"http://arxiv.org/abs/2405.00244v1","category":"cs.CV"}
{"created":"2024-04-30 19:49:20","title":"Using sunRunner3D to interpret the global structure of the heliosphere from in situ measurements","abstract":"Understanding the large-scale three-dimensional structure of the inner heliosphere, while important in its own right, is crucial for space weather applications, such as forecasting the time of arrival and propagation of coronal mass ejections (CMEs). This study uses sunRunner3D (3D), a 3-D magnetohydrodynamic (MHD) model, to simulate solar wind (SW) streams and generate background states. SR3D employs the boundary conditions generated by CORona-HELiosphere (CORHEL) and the PLUTO code to compute the plasma properties of the SW with the MHD approximation up to 1.1 AU in the inner heliosphere. We demonstrate that SR3D reproduces global features of Corotating Interaction Regions (CIRs) observed by Earth-based spacecraft (OMNI) and the Solar TErrestial RElations Observatory (STEREO)-A for a set of Carrington rotations (CRs) that cover a period that lays in the late declining phase of solar cycle 24. Additionally, we demonstrate that the model solutions are valid in the corotating and inertial frames of references. Moreover, a comparison between SR3D simulations and in-situ measurements shows reasonable agreement with the observations, and our results are comparable to those achieved by Predictive Science Inc.'s Magnetohydrodynamic Algorithm outside a Sphere (MAS) code. We have also undertaken a comparative analysis with the Space Weather Adaptive Simulation Framework for Solar Wind (SWASTi-SW), a PLUTO physics-based model, to evaluate the precision of various initial boundary conditions. Finally, we discuss the disparities in the solutions derived from inertial and rotating frames.","sentences":["Understanding the large-scale three-dimensional structure of the inner heliosphere, while important in its own right, is crucial for space weather applications, such as forecasting the time of arrival and propagation of coronal mass ejections (CMEs).","This study uses sunRunner3D (3D), a 3-D magnetohydrodynamic (MHD) model, to simulate solar wind (SW) streams and generate background states.","SR3D employs the boundary conditions generated by CORona-HELiosphere (CORHEL) and the PLUTO code to compute the plasma properties of the SW with the MHD approximation up to 1.1 AU in the inner heliosphere.","We demonstrate that SR3D reproduces global features of Corotating Interaction Regions (CIRs) observed by Earth-based spacecraft (OMNI) and the Solar TErrestial RElations Observatory (STEREO)-A for a set of Carrington rotations (CRs) that cover a period that lays in the late declining phase of solar cycle 24.","Additionally, we demonstrate that the model solutions are valid in the corotating and inertial frames of references.","Moreover, a comparison between SR3D simulations and in-situ measurements shows reasonable agreement with the observations, and our results are comparable to those achieved by Predictive Science Inc.'s Magnetohydrodynamic Algorithm outside a Sphere (MAS) code.","We have also undertaken a comparative analysis with the Space Weather Adaptive Simulation Framework for Solar Wind (SWASTi-SW), a PLUTO physics-based model, to evaluate the precision of various initial boundary conditions.","Finally, we discuss the disparities in the solutions derived from inertial and rotating frames."],"url":"http://arxiv.org/abs/2405.00174v1","category":"astro-ph.SR"}
{"created":"2024-04-30 19:05:22","title":"HistNERo: Historical Named Entity Recognition for the Romanian Language","abstract":"This work introduces HistNERo, the first Romanian corpus for Named Entity Recognition (NER) in historical newspapers. The dataset contains 323k tokens of text, covering more than half of the 19th century (i.e., 1817) until the late part of the 20th century (i.e., 1990). Eight native Romanian speakers annotated the dataset with five named entities. The samples belong to one of the following four historical regions of Romania, namely Bessarabia, Moldavia, Transylvania, and Wallachia. We employed this proposed dataset to perform several experiments for NER using Romanian pre-trained language models. Our results show that the best model achieved a strict F1-score of 55.69%. Also, by reducing the discrepancies between regions through a novel domain adaption technique, we improved the performance on this corpus to a strict F1-score of 66.80%, representing an absolute gain of more than 10%.","sentences":["This work introduces HistNERo, the first Romanian corpus for Named Entity Recognition (NER) in historical newspapers.","The dataset contains 323k tokens of text, covering more than half of the 19th century (i.e., 1817) until the late part of the 20th century (i.e., 1990).","Eight native Romanian speakers annotated the dataset with five named entities.","The samples belong to one of the following four historical regions of Romania, namely Bessarabia, Moldavia, Transylvania, and Wallachia.","We employed this proposed dataset to perform several experiments for NER using Romanian pre-trained language models.","Our results show that the best model achieved a strict F1-score of 55.69%.","Also, by reducing the discrepancies between regions through a novel domain adaption technique, we improved the performance on this corpus to a strict F1-score of 66.80%, representing an absolute gain of more than 10%."],"url":"http://arxiv.org/abs/2405.00155v1","category":"cs.CL"}
{"created":"2024-04-30 18:29:25","title":"Unveiling the Physics of Neutron Stars: A 3D expedition into MAgneto-Thermal evolution in Isolated Neutron Stars with MATINS","abstract":"This doctoral thesis investigates the long-term evolution of the strong magnetic fields within isolated neutron stars (NSs), the most potent magnetic objects in the universe. Their magnetic influence extends beyond their surface to encompass the magnetised plasma in their vicinity. The overarching magnetic configuration significantly impacts the observable characteristics of the highly magnetised NSs, i.e., magnetars. Conversely, the internal magnetic field undergoes prolonged evolution spanning thousands to millions of years, intricately linked to thermal evolution. The diverse observable phenomena associated with NSs underscore the complex 3D nature of their magnetic structure, thereby requiring sophisticated numerical simulations. A central focus of this thesis involves a thorough exploration of state-of-the-art 3D coupled magneto-thermal evolution models. This marks a pioneering achievement as we conduct, for the first time, the most realistic 3D simulations to date, spanning the first million years of a NS's life using the newly developed code MATINS, which adeptly accounts for both Ohmic dissipation and Hall drift within the NS's crust. Our simulations incorporate highly accurate temperature-dependent microphysical calculations and adopt the star's structure based on a realistic equation of state. To address axial singularities in 3D simulations, we employ the cubed-sphere coordinates. We also account for corresponding relativistic factors in the evolution equations and use the latest envelope model from existing literature, in addition to an initial magnetic field structure derived from proton-NS dynamo simulations. Within this framework, we quantitatively simulate the thermal luminosity, timing properties, and magnetic field evolution, pushing the boundaries of numerical modeling capabilities and enabling the performance of several astrophysical studies within this thesis.","sentences":["This doctoral thesis investigates the long-term evolution of the strong magnetic fields within isolated neutron stars (NSs), the most potent magnetic objects in the universe.","Their magnetic influence extends beyond their surface to encompass the magnetised plasma in their vicinity.","The overarching magnetic configuration significantly impacts the observable characteristics of the highly magnetised NSs, i.e., magnetars.","Conversely, the internal magnetic field undergoes prolonged evolution spanning thousands to millions of years, intricately linked to thermal evolution.","The diverse observable phenomena associated with NSs underscore the complex 3D nature of their magnetic structure, thereby requiring sophisticated numerical simulations.","A central focus of this thesis involves a thorough exploration of state-of-the-art 3D coupled magneto-thermal evolution models.","This marks a pioneering achievement as we conduct, for the first time, the most realistic 3D simulations to date, spanning the first million years of a NS's life using the newly developed code MATINS, which adeptly accounts for both Ohmic dissipation and Hall drift within the NS's crust.","Our simulations incorporate highly accurate temperature-dependent microphysical calculations and adopt the star's structure based on a realistic equation of state.","To address axial singularities in 3D simulations, we employ the cubed-sphere coordinates.","We also account for corresponding relativistic factors in the evolution equations and use the latest envelope model from existing literature, in addition to an initial magnetic field structure derived from proton-NS dynamo simulations.","Within this framework, we quantitatively simulate the thermal luminosity, timing properties, and magnetic field evolution, pushing the boundaries of numerical modeling capabilities and enabling the performance of several astrophysical studies within this thesis."],"url":"http://arxiv.org/abs/2405.00133v1","category":"astro-ph.HE"}
{"created":"2024-04-29 17:59:16","title":"Stylus: Automatic Adapter Selection for Diffusion Models","abstract":"Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs. As such, adapters have been widely adopted by open-source communities, accumulating a database of over 100K adapters-most of which are highly customized with insufficient descriptions. This paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlight the performance gains of composing adapters. We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt's keywords. Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts' keywords by checking how well they fit the prompt. To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves greater CLIP-FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model. See stylus-diffusion.github.io for more.","sentences":["Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs.","As such, adapters have been widely adopted by open-source communities, accumulating a database of over 100K adapters-most of which are highly customized with insufficient descriptions.","This paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlight the performance gains of composing adapters.","We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt's keywords.","Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts' keywords by checking how well they fit the prompt.","To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings.","In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves greater CLIP-FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model.","See stylus-diffusion.github.io for more."],"url":"http://arxiv.org/abs/2404.18928v1","category":"cs.CV"}
{"created":"2024-04-29 17:53:54","title":"Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting","abstract":"Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models while maintaining a consistent sampling distribution. However, the conventional approach of training a separate draft model to achieve a satisfactory token acceptance rate can be costly. Drawing inspiration from early exiting, we propose a novel self-speculative decoding framework \\emph{Kangaroo}, which uses a fixed shallow sub-network as a self-draft model, with the remaining layers serving as the larger target model. We train a lightweight and efficient adapter module on top of the sub-network to bridge the gap between the sub-network and the full model's representation ability. It is noteworthy that the inference latency of the self-draft model may no longer be negligible compared to the large model, necessitating strategies to increase the token acceptance rate while minimizing the drafting steps of the small model. To address this challenge, we introduce an additional early exiting mechanism for generating draft tokens. Specifically, we halt the small model's subsequent prediction during the drafting phase once the confidence level for the current token falls below a certain threshold. Extensive experiments on the Spec-Bench demonstrate the effectiveness of Kangaroo. Under single-sequence verification, Kangaroo achieves speedups up to $1.68\\times$ on Spec-Bench, outperforming Medusa-1 with 88.7\\% fewer additional parameters (67M compared to 591M). The code for Kangaroo is available at https://github.com/Equationliu/Kangaroo.","sentences":["Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models while maintaining a consistent sampling distribution.","However, the conventional approach of training a separate draft model to achieve a satisfactory token acceptance rate can be costly.","Drawing inspiration from early exiting, we propose a novel self-speculative decoding framework \\emph{Kangaroo}, which uses a fixed shallow sub-network as a self-draft model, with the remaining layers serving as the larger target model.","We train a lightweight and efficient adapter module on top of the sub-network to bridge the gap between the sub-network and the full model's representation ability.","It is noteworthy that the inference latency of the self-draft model may no longer be negligible compared to the large model, necessitating strategies to increase the token acceptance rate while minimizing the drafting steps of the small model.","To address this challenge, we introduce an additional early exiting mechanism for generating draft tokens.","Specifically, we halt the small model's subsequent prediction during the drafting phase once the confidence level for the current token falls below a certain threshold.","Extensive experiments on the Spec-Bench demonstrate the effectiveness of Kangaroo.","Under single-sequence verification, Kangaroo achieves speedups up to $1.68\\times$ on Spec-Bench, outperforming Medusa-1 with 88.7\\% fewer additional parameters (67M compared to 591M).","The code for Kangaroo is available at https://github.com/Equationliu/Kangaroo."],"url":"http://arxiv.org/abs/2404.18911v1","category":"cs.CL"}
{"created":"2024-04-29 17:51:47","title":"Sample-Efficient Robust Multi-Agent Reinforcement Learning in the Face of Environmental Uncertainty","abstract":"To overcome the sim-to-real gap in reinforcement learning (RL), learned policies must maintain robustness against environmental uncertainties. While robust RL has been widely studied in single-agent regimes, in multi-agent environments, the problem remains understudied -- despite the fact that the problems posed by environmental uncertainties are often exacerbated by strategic interactions. This work focuses on learning in distributionally robust Markov games (RMGs), a robust variant of standard Markov games, wherein each agent aims to learn a policy that maximizes its own worst-case performance when the deployed environment deviates within its own prescribed uncertainty set. This results in a set of robust equilibrium strategies for all agents that align with classic notions of game-theoretic equilibria. Assuming a non-adaptive sampling mechanism from a generative model, we propose a sample-efficient model-based algorithm (DRNVI) with finite-sample complexity guarantees for learning robust variants of various notions of game-theoretic equilibria. We also establish an information-theoretic lower bound for solving RMGs, which confirms the near-optimal sample complexity of DRNVI with respect to problem-dependent factors such as the size of the state space, the target accuracy, and the horizon length.","sentences":["To overcome the sim-to-real gap in reinforcement learning (RL), learned policies must maintain robustness against environmental uncertainties.","While robust RL has been widely studied in single-agent regimes, in multi-agent environments, the problem remains understudied -- despite the fact that the problems posed by environmental uncertainties are often exacerbated by strategic interactions.","This work focuses on learning in distributionally robust Markov games (RMGs), a robust variant of standard Markov games, wherein each agent aims to learn a policy that maximizes its own worst-case performance when the deployed environment deviates within its own prescribed uncertainty set.","This results in a set of robust equilibrium strategies for all agents that align with classic notions of game-theoretic equilibria.","Assuming a non-adaptive sampling mechanism from a generative model, we propose a sample-efficient model-based algorithm (DRNVI) with finite-sample complexity guarantees for learning robust variants of various notions of game-theoretic equilibria.","We also establish an information-theoretic lower bound for solving RMGs, which confirms the near-optimal sample complexity of DRNVI with respect to problem-dependent factors such as the size of the state space, the target accuracy, and the horizon length."],"url":"http://arxiv.org/abs/2404.18909v1","category":"cs.LG"}
{"created":"2024-04-29 17:36:19","title":"Finite Element Approximation of the Fractional Porous Medium Equation","abstract":"We construct a finite element method for the numerical solution of a fractional porous medium equation on a bounded open Lipschitz polytopal domain $\\Omega \\subset \\mathbb{R}^{d}$, where $d = 2$ or $3$. The pressure in the model is defined as the solution of a fractional Poisson equation, involving the fractional Neumann Laplacian in terms of its spectral definition. We perform a rigorous passage to the limit as the spatial and temporal discretization parameters tend to zero and show that a subsequence of the sequence of finite element approximations defined by the proposed numerical method converges to a bounded and nonnegative weak solution of the initial-boundary-value problem under consideration. This result can be therefore viewed as a constructive proof of the existence of a nonnegative, energy-dissipative, weak solution to the initial-boundary-value problem for the fractional porous medium equation under consideration, based on the Neumann Laplacian. The convergence proof relies on results concerning the finite element approximation of the spectral fractional Laplacian and compactness techniques for nonlinear partial differential equations, together with properties of the equation, which are shown to be inherited by the numerical method. We also prove that the total energy associated with the problem under consideration exhibits exponential decay in time.","sentences":["We construct a finite element method for the numerical solution of a fractional porous medium equation on a bounded open Lipschitz polytopal domain $\\Omega \\subset \\mathbb{R}^{d}$, where $d = 2$ or $3$. The pressure in the model is defined as the solution of a fractional Poisson equation, involving the fractional Neumann Laplacian in terms of its spectral definition.","We perform a rigorous passage to the limit as the spatial and temporal discretization parameters tend to zero and show that a subsequence of the sequence of finite element approximations defined by the proposed numerical method converges to a bounded and nonnegative weak solution of the initial-boundary-value problem under consideration.","This result can be therefore viewed as a constructive proof of the existence of a nonnegative, energy-dissipative, weak solution to the initial-boundary-value problem for the fractional porous medium equation under consideration, based on the Neumann Laplacian.","The convergence proof relies on results concerning the finite element approximation of the spectral fractional Laplacian and compactness techniques for nonlinear partial differential equations, together with properties of the equation, which are shown to be inherited by the numerical method.","We also prove that the total energy associated with the problem under consideration exhibits exponential decay in time."],"url":"http://arxiv.org/abs/2404.18901v1","category":"math.NA"}
{"created":"2024-04-29 17:33:52","title":"Overcoming Knowledge Barriers: Online Imitation Learning from Observation with Pretrained World Models","abstract":"Incorporating the successful paradigm of pretraining and finetuning from Computer Vision and Natural Language Processing into decision-making has become increasingly popular in recent years. In this paper, we study Imitation Learning from Observation with pretrained models and find existing approaches such as BCO and AIME face knowledge barriers, specifically the Embodiment Knowledge Barrier (EKB) and the Demonstration Knowledge Barrier (DKB), greatly limiting their performance. The EKB arises when pretrained models lack knowledge about unseen observations, leading to errors in action inference. The DKB results from policies trained on limited demonstrations, hindering adaptability to diverse scenarios. We thoroughly analyse the underlying mechanism of these barriers and propose AIME-v2 upon AIME as a solution. AIME-v2 uses online interactions with data-driven regulariser to alleviate the EKB and mitigates the DKB by introducing a surrogate reward function to enhance policy training. Experimental results on tasks from the DeepMind Control Suite and Meta-World benchmarks demonstrate the effectiveness of these modifications in improving both sample-efficiency and converged performance. The study contributes valuable insights into resolving knowledge barriers for enhanced decision-making in pretraining-based approaches. Code will be available at https://github.com/argmax-ai/aime-v2.","sentences":["Incorporating the successful paradigm of pretraining and finetuning from Computer Vision and Natural Language Processing into decision-making has become increasingly popular in recent years.","In this paper, we study Imitation Learning from Observation with pretrained models and find existing approaches such as BCO and AIME face knowledge barriers, specifically the Embodiment Knowledge Barrier (EKB) and the Demonstration Knowledge Barrier (DKB), greatly limiting their performance.","The EKB arises when pretrained models lack knowledge about unseen observations, leading to errors in action inference.","The DKB results from policies trained on limited demonstrations, hindering adaptability to diverse scenarios.","We thoroughly analyse the underlying mechanism of these barriers and propose AIME-v2 upon AIME as a solution.","AIME-v2 uses online interactions with data-driven regulariser to alleviate the EKB and mitigates the DKB by introducing a surrogate reward function to enhance policy training.","Experimental results on tasks from the DeepMind Control Suite and Meta-World benchmarks demonstrate the effectiveness of these modifications in improving both sample-efficiency and converged performance.","The study contributes valuable insights into resolving knowledge barriers for enhanced decision-making in pretraining-based approaches.","Code will be available at https://github.com/argmax-ai/aime-v2."],"url":"http://arxiv.org/abs/2404.18896v1","category":"cs.LG"}
{"created":"2024-04-29 17:30:41","title":"Odd viscosity suppresses intermittency in direct turbulent cascades","abstract":"Intermittency refers to the broken self-similarity of turbulent flows caused by anomalous spatio-temporal fluctuations. In this Letter, we ask how intermittency is affected by a non-dissipative viscosity, known as odd viscosity, which appears in parity-breaking fluids such as magnetized polyatomic gases, electron fluids under magnetic field and spinning colloids or grains. Using a combination of Navier-Stokes simulations and theory, we show that intermittency is suppressed by odd viscosity at small scales. This effect is caused by parity-breaking waves, induced by odd viscosity, that break the multiple scale invariances of the Navier-Stokes equations. Building on this insight, we construct a two-channel helical shell model that reproduces the basic phenomenology of turbulent odd-viscous fluids including the suppression of anomalous scaling. Our findings illustrate how a fully developed direct cascade that is entirely self-similar can emerge below a tunable length scale, paving the way for designing turbulent flows with adjustable levels of intermittency.","sentences":["Intermittency refers to the broken self-similarity of turbulent flows caused by anomalous spatio-temporal fluctuations.","In this Letter, we ask how intermittency is affected by a non-dissipative viscosity, known as odd viscosity, which appears in parity-breaking fluids such as magnetized polyatomic gases, electron fluids under magnetic field and spinning colloids or grains.","Using a combination of Navier-Stokes simulations and theory, we show that intermittency is suppressed by odd viscosity at small scales.","This effect is caused by parity-breaking waves, induced by odd viscosity, that break the multiple scale invariances of the Navier-Stokes equations.","Building on this insight, we construct a two-channel helical shell model that reproduces the basic phenomenology of turbulent odd-viscous fluids including the suppression of anomalous scaling.","Our findings illustrate how a fully developed direct cascade that is entirely self-similar can emerge below a tunable length scale, paving the way for designing turbulent flows with adjustable levels of intermittency."],"url":"http://arxiv.org/abs/2404.18894v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 17:26:44","title":"An optimal lower bound for smooth convex functions","abstract":"First order methods endowed with global convergence guarantees operate using global lower bounds on the objective. The tightening of the bounds has been shown to increase both the theoretical guarantees and the practical performance. In this work, we define a global lower bound for smooth differentiable objectives that is optimal with respect to the collected oracle information. The bound can be readily employed by the Gradient Method with Memory to improve its performance. Further using the machinery underlying the optimal bounds, we introduce a modified version of the estimate sequence that we use to construct an Optimized Gradient Method with Memory possessing the best known convergence guarantees for its class of algorithms, even in terms of the proportionality constant. We additionally equip the method with an adaptive convergence guarantee adjustment procedure that is an effective replacement for line-search. Simulation results on synthetic but otherwise difficult smooth problems validate the theoretical properties of the bound and proposed methods.","sentences":["First order methods endowed with global convergence guarantees operate using global lower bounds on the objective.","The tightening of the bounds has been shown to increase both the theoretical guarantees and the practical performance.","In this work, we define a global lower bound for smooth differentiable objectives that is optimal with respect to the collected oracle information.","The bound can be readily employed by the Gradient Method with Memory to improve its performance.","Further using the machinery underlying the optimal bounds, we introduce a modified version of the estimate sequence that we use to construct an Optimized Gradient Method with Memory possessing the best known convergence guarantees for its class of algorithms, even in terms of the proportionality constant.","We additionally equip the method with an adaptive convergence guarantee adjustment procedure that is an effective replacement for line-search.","Simulation results on synthetic but otherwise difficult smooth problems validate the theoretical properties of the bound and proposed methods."],"url":"http://arxiv.org/abs/2404.18889v1","category":"math.OC"}
{"created":"2024-04-29 17:16:22","title":"Spivavtor: An Instruction Tuned Ukrainian Text Editing Model","abstract":"We introduce Spivavtor, a dataset, and instruction-tuned models for text editing focused on the Ukrainian language. Spivavtor is the Ukrainian-focused adaptation of the English-only CoEdIT model. Similar to CoEdIT, Spivavtor performs text editing tasks by following instructions in Ukrainian. This paper describes the details of the Spivavtor-Instruct dataset and Spivavtor models. We evaluate Spivavtor on a variety of text editing tasks in Ukrainian, such as Grammatical Error Correction (GEC), Text Simplification, Coherence, and Paraphrasing, and demonstrate its superior performance on all of them. We publicly release our best-performing models and data as resources to the community to advance further research in this space.","sentences":["We introduce Spivavtor, a dataset, and instruction-tuned models for text editing focused on the Ukrainian language.","Spivavtor is the Ukrainian-focused adaptation of the English-only CoEdIT model.","Similar to CoEdIT, Spivavtor performs text editing tasks by following instructions in Ukrainian.","This paper describes the details of the Spivavtor-Instruct dataset and Spivavtor models.","We evaluate Spivavtor on a variety of text editing tasks in Ukrainian, such as Grammatical Error Correction (GEC), Text Simplification, Coherence, and Paraphrasing, and demonstrate its superior performance on all of them.","We publicly release our best-performing models and data as resources to the community to advance further research in this space."],"url":"http://arxiv.org/abs/2404.18880v1","category":"cs.CL"}
{"created":"2024-04-29 17:14:21","title":"Spin coupling is all you need: Encoding strong electron correlation on quantum computers","abstract":"The performance of quantum algorithms for eigenvalue problems, such as computing Hamiltonian spectra, depends strongly on the overlap of the initial wavefunction and the target eigenvector. In a basis of Slater determinants, the representation of energy eigenstates of systems with $N$ strongly correlated electrons requires a number of determinants that scales exponentially with $N$. On classical processors, this restricts simulations to systems where $N$ is small. Here, we show that quantum computers can efficiently simulate strongly correlated molecular systems by directly encoding the dominant entanglement structure in the form of spin-coupled initial states. This avoids resorting to expensive classical or quantum state preparation heuristics and instead exploits symmetries in the wavefunction. We provide quantum circuits for deterministic preparation of a family of spin eigenfunctions with ${N \\choose N/2}$ Slater determinants with depth $\\mathcal{O}(N)$ and $\\mathcal{O}(N^2)$ local gates. Their use as highly entangled initial states in quantum algorithms reduces the total runtime of quantum phase estimation and related fault-tolerant methods by orders of magnitude. Furthermore, we assess the application of spin-coupled wavefunctions as initial states for a range of heuristic quantum algorithms, namely the variational quantum eigensolver, adiabatic state preparation, and different versions of quantum subspace diagonalization (QSD) including QSD based on real-time-evolved states. We also propose a novel QSD algorithm that exploits states obtained through adaptive quantum eigensolvers. For all algorithms, we demonstrate that using spin-coupled initial states drastically reduces the quantum resources required to simulate strongly correlated ground and excited states. Our work paves the way towards scalable quantum simulation of electronic structure for classically challenging systems.","sentences":["The performance of quantum algorithms for eigenvalue problems, such as computing Hamiltonian spectra, depends strongly on the overlap of the initial wavefunction and the target eigenvector.","In a basis of Slater determinants, the representation of energy eigenstates of systems with $N$ strongly correlated electrons requires a number of determinants that scales exponentially with $N$. On classical processors, this restricts simulations to systems where $N$ is small.","Here, we show that quantum computers can efficiently simulate strongly correlated molecular systems by directly encoding the dominant entanglement structure in the form of spin-coupled initial states.","This avoids resorting to expensive classical or quantum state preparation heuristics and instead exploits symmetries in the wavefunction.","We provide quantum circuits for deterministic preparation of a family of spin eigenfunctions with ${N \\choose N/2}$ Slater determinants with depth $\\mathcal{O}(N)$ and $\\mathcal{O}(N^2)$ local gates.","Their use as highly entangled initial states in quantum algorithms reduces the total runtime of quantum phase estimation and related fault-tolerant methods by orders of magnitude.","Furthermore, we assess the application of spin-coupled wavefunctions as initial states for a range of heuristic quantum algorithms, namely the variational quantum eigensolver, adiabatic state preparation, and different versions of quantum subspace diagonalization (QSD) including QSD based on real-time-evolved states.","We also propose a novel QSD algorithm that exploits states obtained through adaptive quantum eigensolvers.","For all algorithms, we demonstrate that using spin-coupled initial states drastically reduces the quantum resources required to simulate strongly correlated ground and excited states.","Our work paves the way towards scalable quantum simulation of electronic structure for classically challenging systems."],"url":"http://arxiv.org/abs/2404.18878v1","category":"quant-ph"}
{"created":"2024-04-29 16:42:58","title":"MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection","abstract":"In this paper, we present a different way to use two modalities, in which either one modality or the other is seen by a single model. This can be useful when adapting an unimodal model to leverage more information while respecting a limited computational budget. This would mean having a single model that is able to deal with any modalities. To describe this, we coined the term anymodal learning. An example of this, is a use case where, surveillance in a room when the lights are off would be much more valuable using an infrared modality while a visible one would provide more discriminative information when lights are on. This work investigates how to efficiently leverage visible and infrared/thermal modalities for transformer-based object detection backbone to create an anymodal architecture. Our work does not create any inference overhead during the testing while exploring an effective way to exploit the two modalities during the training. To accomplish such a task, we introduce the novel anymodal training technique: Mixed Patches (MiPa), in conjunction with a patch-wise domain agnostic module, which is responsible of learning the best way to find a common representation of both modalities. This approach proves to be able to balance modalities by reaching competitive results on individual modality benchmarks with the alternative of using an unimodal architecture on three different visible-infrared object detection datasets. Finally, our proposed method, when used as a regularization for the strongest modality, can beat the performance of multimodal fusion methods while only requiring a single modality during inference. Notably, MiPa became the state-of-the-art on the LLVIP visible/infrared benchmark. Code: https://github.com/heitorrapela/MiPa","sentences":["In this paper, we present a different way to use two modalities, in which either one modality or the other is seen by a single model.","This can be useful when adapting an unimodal model to leverage more information while respecting a limited computational budget.","This would mean having a single model that is able to deal with any modalities.","To describe this, we coined the term anymodal learning.","An example of this, is a use case where, surveillance in a room when the lights are off would be much more valuable using an infrared modality while a visible one would provide more discriminative information when lights are on.","This work investigates how to efficiently leverage visible and infrared/thermal modalities for transformer-based object detection backbone to create an anymodal architecture.","Our work does not create any inference overhead during the testing while exploring an effective way to exploit the two modalities during the training.","To accomplish such a task, we introduce the novel anymodal training technique: Mixed Patches (MiPa), in conjunction with a patch-wise domain agnostic module, which is responsible of learning the best way to find a common representation of both modalities.","This approach proves to be able to balance modalities by reaching competitive results on individual modality benchmarks with the alternative of using an unimodal architecture on three different visible-infrared object detection datasets.","Finally, our proposed method, when used as a regularization for the strongest modality, can beat the performance of multimodal fusion methods while only requiring a single modality during inference.","Notably, MiPa became the state-of-the-art on the LLVIP visible/infrared benchmark.","Code: https://github.com/heitorrapela/MiPa"],"url":"http://arxiv.org/abs/2404.18849v1","category":"cs.CV"}
{"created":"2024-04-29 16:42:26","title":"FeDeRA:Efficient Fine-tuning of Language Models in Federated Learning Leveraging Weight Decomposition","abstract":"Pre-trained Language Models (PLMs) have shown excellent performance on various downstream tasks after fine-tuning. Nevertheless, the escalating concerns surrounding user privacy have posed significant challenges to centralized training reliant on extensive data collection. Federated learning, which only requires training on the clients and aggregates weights on the server without sharing data, has emerged as a solution. However, the substantial parameter size of PLMs places a significant burden on the computational resources of client devices, while also leading to costly communication expenses. Introducing Parameter-Efficient Fine-Tuning(PEFT) into federated learning can effectively address this problem. However, we observe that the non-IID data in federated learning leads to a gap in performance between the PEFT method and full parameter fine-tuning(FFT). To overcome this, we propose FeDeRA, an improvement over the Low-Rank Adaption(LoRA) method in federated learning. FeDeRA uses the same adapter module as LoRA. However, the difference lies in FeDeRA's initialization of the adapter module by performing Singular Value Decomposition (SVD) on the pre-trained matrix and selecting its principal components. We conducted extensive experiments, using RoBERTa and DeBERTaV3, on six datasets, comparing the methods including FFT and the other three different PEFT methods. FeDeRA outperforms all other PEFT methods and is comparable to or even surpasses the performance of FFT method. We also deployed federated learning on Jetson AGX Orin and compared the time required by different methods to achieve the target accuracy on specific tasks. Compared to FFT, FeDeRA reduces the training time by 95.9\\%, 97.9\\%, 96.9\\% and 97.3\\%, 96.5\\%, 96.5\\% respectively on three tasks using RoBERTa and DeBERTaV3. The overall experiments indicate that FeDeRA achieves good performance while also maintaining efficiency.","sentences":["Pre-trained Language Models (PLMs) have shown excellent performance on various downstream tasks after fine-tuning.","Nevertheless, the escalating concerns surrounding user privacy have posed significant challenges to centralized training reliant on extensive data collection.","Federated learning, which only requires training on the clients and aggregates weights on the server without sharing data, has emerged as a solution.","However, the substantial parameter size of PLMs places a significant burden on the computational resources of client devices, while also leading to costly communication expenses.","Introducing Parameter-Efficient Fine-Tuning(PEFT) into federated learning can effectively address this problem.","However, we observe that the non-IID data in federated learning leads to a gap in performance between the PEFT method and full parameter fine-tuning(FFT).","To overcome this, we propose FeDeRA, an improvement over the Low-Rank Adaption(LoRA) method in federated learning.","FeDeRA uses the same adapter module as LoRA.","However, the difference lies in FeDeRA's initialization of the adapter module by performing Singular Value Decomposition (SVD) on the pre-trained matrix and selecting its principal components.","We conducted extensive experiments, using RoBERTa and DeBERTaV3, on six datasets, comparing the methods including FFT and the other three different PEFT methods.","FeDeRA outperforms all other PEFT methods and is comparable to or even surpasses the performance of FFT method.","We also deployed federated learning on Jetson AGX Orin and compared the time required by different methods to achieve the target accuracy on specific tasks.","Compared to FFT, FeDeRA reduces the training time by 95.9\\%, 97.9\\%, 96.9\\% and 97.3\\%, 96.5\\%, 96.5\\% respectively on three tasks using RoBERTa and DeBERTaV3.","The overall experiments indicate that FeDeRA achieves good performance while also maintaining efficiency."],"url":"http://arxiv.org/abs/2404.18848v2","category":"cs.LG"}
{"created":"2024-04-29 16:35:06","title":"The Role of Normal and Non-Normal Contributions to Enstrophy Production in the Near-Wall Region of a Turbulent Channel Flow","abstract":"The turbulent boundary-layer is a region where both preferential dissipation of energy and the production of significant vorticity arises as a consequence of the strong velocity gradients. Previous work has shown that, following a Reynolds decomposition of the enstrophy production, the purely fluctuating contribution is the dominant term and that near the wall this varies in a complex manner with height. In this study we additionally decompose the strain rate and vorticity terms into normal and non-normal components using a Schur decomposition and are able to explain all these features in terms of contributions at different heights from constituents involving different combinations of normal and non-normal quantities. What is surprising about our results is that while the mean shear and the action of larger scale structures should mean that non-normal effects are of over-riding importance, the most important individual term involves the fluctuating, normal straining in the transverse direction. Furthermore, the reason that the term that involves only non-normal contributions is smaller on average than that involving normal straining coupled to non-normal vorticity is that in the former case there are individual constituents that are negative in the mean. Hence, we not only explain the nature of near-wall enstrophy production in greater detail, but highlight how local straining that is orthogonal to the direction of the dominant mean and fluctuating shear plays a crucial role in amplifying vorticity that is yet to have developed sufficiently to gain a solid body rotational component.","sentences":["The turbulent boundary-layer is a region where both preferential dissipation of energy and the production of significant vorticity arises as a consequence of the strong velocity gradients.","Previous work has shown that, following a Reynolds decomposition of the enstrophy production, the purely fluctuating contribution is the dominant term and that near the wall this varies in a complex manner with height.","In this study we additionally decompose the strain rate and vorticity terms into normal and non-normal components using a Schur decomposition and are able to explain all these features in terms of contributions at different heights from constituents involving different combinations of normal and non-normal quantities.","What is surprising about our results is that while the mean shear and the action of larger scale structures should mean that non-normal effects are of over-riding importance, the most important individual term involves the fluctuating, normal straining in the transverse direction.","Furthermore, the reason that the term that involves only non-normal contributions is smaller on average than that involving normal straining coupled to non-normal vorticity is that in the former case there are individual constituents that are negative in the mean.","Hence, we not only explain the nature of near-wall enstrophy production in greater detail, but highlight how local straining that is orthogonal to the direction of the dominant mean and fluctuating shear plays a crucial role in amplifying vorticity that is yet to have developed sufficiently to gain a solid body rotational component."],"url":"http://arxiv.org/abs/2404.18844v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 16:28:38","title":"Deep orthogonal decomposition: a continuously adaptive data-driven approach to model order reduction","abstract":"We develop a novel deep learning technique, termed Deep Orthogonal Decomposition (DOD), for dimensionality reduction and reduced order modeling of parameter dependent partial differential equations. The approach consists in the construction of a deep neural network model that approximates the solution manifold through a continuously adaptive local basis. In contrast to global methods, such as Principal Orthogonal Decomposition (POD), the adaptivity allows the DOD to overcome the Kolmogorov barrier, making the approach applicable to a wide spectrum of parametric problems. Furthermore, due to its hybrid linear-nonlinear nature, the DOD can accommodate both intrusive and nonintrusive techniques, providing highly interpretable latent representations and tighter control on error propagation. For this reason, the proposed approach stands out as a valuable alternative to other nonlinear techniques, such as deep autoencoders. The methodology is discussed both theoretically and practically, evaluating its performances on problems featuring nonlinear PDEs, singularities, and parametrized geometries.","sentences":["We develop a novel deep learning technique, termed Deep Orthogonal Decomposition (DOD), for dimensionality reduction and reduced order modeling of parameter dependent partial differential equations.","The approach consists in the construction of a deep neural network model that approximates the solution manifold through a continuously adaptive local basis.","In contrast to global methods, such as Principal Orthogonal Decomposition (POD), the adaptivity allows the DOD to overcome the Kolmogorov barrier, making the approach applicable to a wide spectrum of parametric problems.","Furthermore, due to its hybrid linear-nonlinear nature, the DOD can accommodate both intrusive and nonintrusive techniques, providing highly interpretable latent representations and tighter control on error propagation.","For this reason, the proposed approach stands out as a valuable alternative to other nonlinear techniques, such as deep autoencoders.","The methodology is discussed both theoretically and practically, evaluating its performances on problems featuring nonlinear PDEs, singularities, and parametrized geometries."],"url":"http://arxiv.org/abs/2404.18841v1","category":"math.NA"}
{"created":"2024-04-29 16:26:27","title":"Accurate adaptive deep learning method for solving elliptic problems","abstract":"Deep learning method is of great importance in solving partial differential equations. In this paper, inspired by the failure-informed idea proposed by Gao et.al. (SIAM Journal on Scientific Computing 45(4)(2023)) and as an improvement, a new accurate adaptive deep learning method is proposed for solving elliptic problems, including the interface problems and the convection-dominated problems. Based on the failure probability framework, the piece-wise uniform distribution is used to approximate the optimal proposal distribution and an kernel-based method is proposed for efficient sampling. Together with the improved Levenberg-Marquardt optimization method, the proposed adaptive deep learning method shows great potential in improving solution accuracy. Numerical tests on the elliptic problems without interface conditions, on the elliptic interface problem, and on the convection-dominated problems demonstrate the effectiveness of the proposed method, as it reduces the relative errors by a factor varying from $10^2$ to $10^4$ for different cases.","sentences":["Deep learning method is of great importance in solving partial differential equations.","In this paper, inspired by the failure-informed idea proposed by Gao et.al.","(SIAM Journal on Scientific Computing 45(4)(2023))","and as an improvement, a new accurate adaptive deep learning method is proposed for solving elliptic problems, including the interface problems and the convection-dominated problems.","Based on the failure probability framework, the piece-wise uniform distribution is used to approximate the optimal proposal distribution and an kernel-based method is proposed for efficient sampling.","Together with the improved Levenberg-Marquardt optimization method, the proposed adaptive deep learning method shows great potential in improving solution accuracy.","Numerical tests on the elliptic problems without interface conditions, on the elliptic interface problem, and on the convection-dominated problems demonstrate the effectiveness of the proposed method, as it reduces the relative errors by a factor varying from $10^2$ to $10^4$ for different cases."],"url":"http://arxiv.org/abs/2404.18838v1","category":"math.NA"}
{"created":"2024-04-29 16:15:01","title":"Demonstration of system-bath physics on gate-based quantum computer","abstract":"We demonstrate algorithmic cooling on IBM-Q devices. We utilize inherent qubit noise to simulate the equilibration of an interacting spin system towards its ground state, when coupled to a dissipative auxiliary-spin bath. The steady-state correlations in the system are defined by the system Hamiltonian and are stable as long as the algorithm can be executed. In particular, we demonstrate the relaxation of system spins to ferromagnetic and antiferromagnetic ordering, controlled by the definition of the Hamiltonian. We are able to perform simulated cooling for global systems of up to three system spins and four auxiliary spins.","sentences":["We demonstrate algorithmic cooling on IBM-Q devices.","We utilize inherent qubit noise to simulate the equilibration of an interacting spin system towards its ground state, when coupled to a dissipative auxiliary-spin bath.","The steady-state correlations in the system are defined by the system Hamiltonian and are stable as long as the algorithm can be executed.","In particular, we demonstrate the relaxation of system spins to ferromagnetic and antiferromagnetic ordering, controlled by the definition of the Hamiltonian.","We are able to perform simulated cooling for global systems of up to three system spins and four auxiliary spins."],"url":"http://arxiv.org/abs/2404.18828v2","category":"quant-ph"}
{"created":"2024-04-29 15:40:40","title":"A Partial Replication of MaskFormer in TensorFlow on TPUs for the TensorFlow Model Garden","abstract":"This paper undertakes the task of replicating the MaskFormer model a universal image segmentation model originally developed using the PyTorch framework, within the TensorFlow ecosystem, specifically optimized for execution on Tensor Processing Units (TPUs). Our implementation exploits the modular constructs available within the TensorFlow Model Garden (TFMG), encompassing elements such as the data loader, training orchestrator, and various architectural components, tailored and adapted to meet the specifications of the MaskFormer model. We address key challenges encountered during the replication, non-convergence issues, slow training, adaptation of loss functions, and the integration of TPU-specific functionalities. We verify our reproduced implementation and present qualitative results on the COCO dataset. Although our implementation meets some of the objectives for end-to-end reproducibility, we encountered challenges in replicating the PyTorch version of MaskFormer in TensorFlow. This replication process is not straightforward and requires substantial engineering efforts. Specifically, it necessitates the customization of various components within the TFMG, alongside thorough verification and hyper-parameter tuning. The replication is available at: https://github.com/PurdueDualityLab/tf-maskformer/tree/main/official/projects/maskformer","sentences":["This paper undertakes the task of replicating the MaskFormer model a universal image segmentation model originally developed using the PyTorch framework, within the TensorFlow ecosystem, specifically optimized for execution on Tensor Processing Units (TPUs).","Our implementation exploits the modular constructs available within the TensorFlow Model Garden (TFMG), encompassing elements such as the data loader, training orchestrator, and various architectural components, tailored and adapted to meet the specifications of the MaskFormer model.","We address key challenges encountered during the replication, non-convergence issues, slow training, adaptation of loss functions, and the integration of TPU-specific functionalities.","We verify our reproduced implementation and present qualitative results on the COCO dataset.","Although our implementation meets some of the objectives for end-to-end reproducibility, we encountered challenges in replicating the PyTorch version of MaskFormer in TensorFlow.","This replication process is not straightforward and requires substantial engineering efforts.","Specifically, it necessitates the customization of various components within the TFMG, alongside thorough verification and hyper-parameter tuning.","The replication is available at: https://github.com/PurdueDualityLab/tf-maskformer/tree/main/official/projects/maskformer"],"url":"http://arxiv.org/abs/2404.18801v1","category":"cs.CV"}
{"created":"2024-04-29 15:38:14","title":"Extending h adaptivity with refinement patterns","abstract":"This contribution introduces the idea of refinement patterns for the generation of optimal meshes in the context of the Finite Element Method. The main idea is to generate a library of possible patterns on which elements can be refined and use this library to inform an h adaptive code on how to handle complex refinements in regions of interest. There are no restrictions on the type of elements that can be refined, and the patterns can be generated for any element type. The main advantage of this approach is that it allows for the generation of optimal meshes in a systematic way where, even if a certain pattern is not available, it can easily be included through a simple text file with nodes and sub-elements. The contribution presents a detailed methodology for incorporating refinement patterns into h adaptive Finite Element Method codes and demonstrates the effectiveness of the approach through mesh refinement of problems with complex geometries.","sentences":["This contribution introduces the idea of refinement patterns for the generation of optimal meshes in the context of the Finite Element Method.","The main idea is to generate a library of possible patterns on which elements can be refined and use this library to inform an h adaptive code on how to handle complex refinements in regions of interest.","There are no restrictions on the type of elements that can be refined, and the patterns can be generated for any element type.","The main advantage of this approach is that it allows for the generation of optimal meshes in a systematic way where, even if a certain pattern is not available, it can easily be included through a simple text file with nodes and sub-elements.","The contribution presents a detailed methodology for incorporating refinement patterns into h adaptive Finite Element Method codes and demonstrates the effectiveness of the approach through mesh refinement of problems with complex geometries."],"url":"http://arxiv.org/abs/2404.18800v2","category":"math.NA"}
{"created":"2024-04-29 15:36:13","title":"Location-Based Load Balancing for Energy-Efficient Cell-Free Networks","abstract":"Cell-Free Massive MIMO (CF mMIMO) has emerged as a potential enabler for future networks. It has been shown that these networks are much more energy-efficient than classical cellular systems when they are serving users at peak capacity. However, these CF mMIMO networks are designed for peak traffic loads, and when this is not the case, they are significantly over-dimensioned and not at all energy efficient. To this end, Adaptive Access Point (AP) ON/OFF Switching (ASO) strategies have been developed to save energy when the network is not at peak traffic loads by putting unnecessary APs to sleep. Unfortunately, the existing strategies rely on measuring channel state information between every user and every access point, resulting in significant measurement energy consumption overheads. Furthermore, the current state-of-art approach has a computational complexity that scales exponentially with the number of APs. In this work, we present a novel convex feasibility testing method that allows checking per-user Quality-of-Service (QoS) requirements without necessarily considering all possible access point activations. We then propose an iterative algorithm for activating access points until all users' requirements are fulfilled. We show that our method has comparable performance to the optimal solution whilst avoiding solving costly mixed-integer problems and measuring channel state information on only a limited subset of APs.","sentences":["Cell-Free Massive MIMO (CF mMIMO) has emerged as a potential enabler for future networks.","It has been shown that these networks are much more energy-efficient than classical cellular systems when they are serving users at peak capacity.","However, these CF mMIMO networks are designed for peak traffic loads, and when this is not the case, they are significantly over-dimensioned and not at all energy efficient.","To this end, Adaptive Access Point (AP) ON/OFF Switching (ASO) strategies have been developed to save energy when the network is not at peak traffic loads by putting unnecessary APs to sleep.","Unfortunately, the existing strategies rely on measuring channel state information between every user and every access point, resulting in significant measurement energy consumption overheads.","Furthermore, the current state-of-art approach has a computational complexity that scales exponentially with the number of APs.","In this work, we present a novel convex feasibility testing method that allows checking per-user Quality-of-Service (QoS) requirements without necessarily considering all possible access point activations.","We then propose an iterative algorithm for activating access points until all users' requirements are fulfilled.","We show that our method has comparable performance to the optimal solution whilst avoiding solving costly mixed-integer problems and measuring channel state information on only a limited subset of APs."],"url":"http://arxiv.org/abs/2404.18799v1","category":"eess.SP"}
{"created":"2024-04-29 15:34:32","title":"Multi-Agent Synchronization Tasks","abstract":"In multi-agent reinforcement learning (MARL), coordination plays a crucial role in enhancing agents' performance beyond what they could achieve through cooperation alone. The interdependence of agents' actions, coupled with the need for communication, leads to a domain where effective coordination is crucial. In this paper, we introduce and define $\\textit{Multi-Agent Synchronization Tasks}$ (MSTs), a novel subset of multi-agent tasks. We describe one MST, that we call $\\textit{Synchronized Predator-Prey}$, offering a detailed description that will serve as the basis for evaluating a selection of recent state-of-the-art (SOTA) MARL algorithms explicitly designed to address coordination challenges through the use of communication strategies. Furthermore, we present empirical evidence that reveals the limitations of the algorithms assessed to solve MSTs, demonstrating their inability to scale effectively beyond 2-agent coordination tasks in scenarios where communication is a requisite component. Finally, the results raise questions about the applicability of recent SOTA approaches for complex coordination tasks (i.e. MSTs) and prompt further exploration into the underlying causes of their limitations in this context.","sentences":["In multi-agent reinforcement learning (MARL), coordination plays a crucial role in enhancing agents' performance beyond what they could achieve through cooperation alone.","The interdependence of agents' actions, coupled with the need for communication, leads to a domain where effective coordination is crucial.","In this paper, we introduce and define $\\textit{Multi-Agent Synchronization Tasks}$ (MSTs), a novel subset of multi-agent tasks.","We describe one MST, that we call $\\textit{Synchronized Predator-Prey}$, offering a detailed description that will serve as the basis for evaluating a selection of recent state-of-the-art (SOTA) MARL algorithms explicitly designed to address coordination challenges through the use of communication strategies.","Furthermore, we present empirical evidence that reveals the limitations of the algorithms assessed to solve MSTs, demonstrating their inability to scale effectively beyond 2-agent coordination tasks in scenarios where communication is a requisite component.","Finally, the results raise questions about the applicability of recent SOTA approaches for complex coordination tasks (i.e. MSTs) and prompt further exploration into the underlying causes of their limitations in this context."],"url":"http://arxiv.org/abs/2404.18798v1","category":"cs.MA"}
{"created":"2024-04-29 15:18:07","title":"Improved bounds for group testing in arbitrary hypergraphs","abstract":"Recent papers initiated the study of a generalization of group testing where the potentially contaminated sets are the members of a given hypergraph F=(V,E). This generalization finds application in contexts where contaminations can be conditioned by some kinds of social and geographical clusterings. The paper focuses on few-stage group testing algorithms, i.e., slightly adaptive algorithms where tests are performed in stages and all tests performed in the same stage should be decided at the very beginning of the stage. In particular, the paper presents the first two-stage algorithm that uses o(dlog|E|) tests for general hypergraphs with hyperedges of size at most d, and a three-stage algorithm that improves by a d^{1/6} factor on the number of tests of the best known three-stage algorithm. These algorithms are special cases of an s-stage algorithm designed for an arbitrary positive integer s<= d. The design of this algorithm resort to a new non-adaptive algorithm (one-stage algorithm), i.e., an algorithm where all tests must be decided beforehand. Further, we derive a lower bound for non-adaptive group testing. For E sufficiently large, the lower bound is very close to the upper bound on the number of tests of the best non-adaptive group testing algorithm known in the literature, and it is the first lower bound that improves on the information theoretic lower bound Omega(log |E|).","sentences":["Recent papers initiated the study of a generalization of group testing where the potentially contaminated sets are the members of a given hypergraph F=(V,E).","This generalization finds application in contexts where contaminations can be conditioned by some kinds of social and geographical clusterings.","The paper focuses on few-stage group testing algorithms, i.e., slightly adaptive algorithms where tests are performed in stages and all tests performed in the same stage should be decided at the very beginning of the stage.","In particular, the paper presents the first two-stage algorithm that uses o(dlog|E|) tests for general hypergraphs with hyperedges of size at most d, and a three-stage algorithm that improves by a d^{1/6} factor on the number of tests of the best known three-stage algorithm.","These algorithms are special cases of an s-stage algorithm designed for an arbitrary positive integer s<= d.","The design of this algorithm resort to a new non-adaptive algorithm (one-stage algorithm), i.e., an algorithm where all tests must be decided beforehand.","Further, we derive a lower bound for non-adaptive group testing.","For E sufficiently large, the lower bound is very close to the upper bound on the number of tests of the best non-adaptive group testing algorithm known in the literature, and it is the first lower bound that improves on the information theoretic lower bound Omega(log |E|)."],"url":"http://arxiv.org/abs/2404.18783v2","category":"cs.DS"}
{"created":"2024-04-29 15:17:59","title":"Whale Optimization Algorithm-based Fractional Order Fuzzy Type-II PI Control for Modular Multilevel Converters","abstract":"Designing a robust controller for Modular Multilevel Converters (MMCs) is crucial to ensure stability and optimal dynamic performance under various operating conditions, including faulty and disturbed scenarios. The primary objective of controlling grid-connected MMCs (GC-MMCs) is to accurately track real and reactive power references while maintaining excellent harmonic performance in the output response. This paper proposes a novel model-free control strategy for GC-MMCs, employing a Fractional Order Proportional-Integral (FOPI) controller and a Fractional Order Fuzzy type-II Proportional-Integral (FOFPI) controller. The FOFPI controller utilizes a type-II Fuzzy Inference System (FIS) to adaptively adjust the proportional and derivative gains during the control process, enabling effective control of the MMC under diverse operating conditions. The type-II FIS, which leverages type-II fuzzy sets, can mitigate uncertainty and nonlinearity in the system. Furthermore, the incorporation of fractional-order mathematics enhances the flexibility of the proposed controllers. To optimize the initial parameters of the proposed controllers, the Whale Optimization Algorithm (WOA), a meta-heuristic algorithm, is employed. The results demonstrate that the proposed controllers exhibit superior performance under voltage disturbance conditions, varying input voltage, and can ensure the stability of the MMC.","sentences":["Designing a robust controller for Modular Multilevel Converters (MMCs) is crucial to ensure stability and optimal dynamic performance under various operating conditions, including faulty and disturbed scenarios.","The primary objective of controlling grid-connected MMCs (GC-MMCs) is to accurately track real and reactive power references while maintaining excellent harmonic performance in the output response.","This paper proposes a novel model-free control strategy for GC-MMCs, employing a Fractional Order Proportional-Integral (FOPI) controller and a Fractional Order Fuzzy type-II Proportional-Integral (FOFPI) controller.","The FOFPI controller utilizes a type-II Fuzzy Inference System (FIS) to adaptively adjust the proportional and derivative gains during the control process, enabling effective control of the MMC under diverse operating conditions.","The type-II FIS, which leverages type-II fuzzy sets, can mitigate uncertainty and nonlinearity in the system.","Furthermore, the incorporation of fractional-order mathematics enhances the flexibility of the proposed controllers.","To optimize the initial parameters of the proposed controllers, the Whale Optimization Algorithm (WOA), a meta-heuristic algorithm, is employed.","The results demonstrate that the proposed controllers exhibit superior performance under voltage disturbance conditions, varying input voltage, and can ensure the stability of the MMC."],"url":"http://arxiv.org/abs/2404.18782v1","category":"eess.SY"}
{"created":"2024-04-29 15:17:30","title":"Wavelet-based tools to analyze, filter, and reconstruct transient gravitational-wave signals","abstract":"The analysis of gravitational-wave (GW) signals is one of the most challenging application areas of signal processing. Wavelet transforms are specially helpful in detecting and analyzing GW transients and several analysis pipelines are based on these transforms, both continuous and discrete. While discrete wavelet transforms have distinct advantages in terms of computing efficiency, continuous wavelet transforms (CWT) produce smooth and visually stunning time-frequency maps. In addition to wavelets the Q-transform is also used, which is a Morlet wavelet-like transform where the width of the Gaussian envelope is parameterized by a parameter denoted by Q. To date, the use of CWTs in GW data analysis has been limited by the higher computational load when compared with discrete wavelets, and also by the lack of an inversion formula for wavelet families that do not satisfy the admissibility condition. In this paper we consider Morlet wavelets parameterized in the same way as the Q-transform (hence the name wavelet Q-transform) which have all the advantages of the Morlet wavelets and where the wavelet transform can be inverted with a computationally efficient specialization of the non-standard inversion formula of Lebedeva and Postnikov [Lebedeva and Postnikov, Royal Society Open Science, 1 (2014) 140124]. We also introduce a two-parameter extension (the wavelet Qp-transform) which is well-adapted to chirping signals like those originating from compact binary coalescences (CBC), and show that it is also invertible just like the wavelet Q-transform. The inversion formulas of both transforms allow for effective noise filtering and produce very clean reconstructions of GW signals. Our preliminary results indicate that the method could be well suited to perform accurate tests of General Relativity by comparing modeled and unmodeled reconstructions of CBC GW signals.","sentences":["The analysis of gravitational-wave (GW) signals is one of the most challenging application areas of signal processing.","Wavelet transforms are specially helpful in detecting and analyzing GW transients and several analysis pipelines are based on these transforms, both continuous and discrete.","While discrete wavelet transforms have distinct advantages in terms of computing efficiency, continuous wavelet transforms (CWT) produce smooth and visually stunning time-frequency maps.","In addition to wavelets the Q-transform is also used, which is a Morlet wavelet-like transform where the width of the Gaussian envelope is parameterized by a parameter denoted by Q. To date, the use of CWTs in GW data analysis has been limited by the higher computational load when compared with discrete wavelets, and also by the lack of an inversion formula for wavelet families that do not satisfy the admissibility condition.","In this paper we consider Morlet wavelets parameterized in the same way as the Q-transform (hence the name wavelet Q-transform) which have all the advantages of the Morlet wavelets and where the wavelet transform can be inverted with a computationally efficient specialization of the non-standard inversion formula of Lebedeva and Postnikov [Lebedeva and Postnikov, Royal Society Open Science, 1 (2014) 140124].","We also introduce a two-parameter extension (the wavelet Qp-transform) which is well-adapted to chirping signals like those originating from compact binary coalescences (CBC), and show that it is also invertible just like the wavelet Q-transform.","The inversion formulas of both transforms allow for effective noise filtering and produce very clean reconstructions of GW signals.","Our preliminary results indicate that the method could be well suited to perform accurate tests of General Relativity by comparing modeled and unmodeled reconstructions of CBC GW signals."],"url":"http://arxiv.org/abs/2404.18781v1","category":"gr-qc"}
{"created":"2024-04-29 14:56:11","title":"Transitive Vision-Language Prompt Learning for Domain Generalization","abstract":"The vision-language pre-training has enabled deep models to make a huge step forward in generalizing across unseen domains. The recent learning method based on the vision-language pre-training model is a great tool for domain generalization and can solve this problem to a large extent. However, there are still some issues that an advancement still suffers from trading-off between domain invariance and class separability, which are crucial in current DG problems. However, there are still some issues that an advancement still suffers from trading-off between domain invariance and class separability, which are crucial in current DG problems. In this paper, we introduce a novel prompt learning strategy that leverages deep vision prompts to address domain invariance while utilizing language prompts to ensure class separability, coupled with adaptive weighting mechanisms to balance domain invariance and class separability. Extensive experiments demonstrate that deep vision prompts effectively extract domain-invariant features, significantly improving the generalization ability of deep models and achieving state-of-the-art performance on three datasets.","sentences":["The vision-language pre-training has enabled deep models to make a huge step forward in generalizing across unseen domains.","The recent learning method based on the vision-language pre-training model is a great tool for domain generalization and can solve this problem to a large extent.","However, there are still some issues that an advancement still suffers from trading-off between domain invariance and class separability, which are crucial in current DG problems.","However, there are still some issues that an advancement still suffers from trading-off between domain invariance and class separability, which are crucial in current DG problems.","In this paper, we introduce a novel prompt learning strategy that leverages deep vision prompts to address domain invariance while utilizing language prompts to ensure class separability, coupled with adaptive weighting mechanisms to balance domain invariance and class separability.","Extensive experiments demonstrate that deep vision prompts effectively extract domain-invariant features, significantly improving the generalization ability of deep models and achieving state-of-the-art performance on three datasets."],"url":"http://arxiv.org/abs/2404.18758v1","category":"cs.CV"}
{"created":"2024-04-29 14:47:32","title":"Evaluating the Effectiveness of Video Anomaly Detection in the Wild: Online Learning and Inference for Real-world Deployment","abstract":"Video Anomaly Detection (VAD) identifies unusual activities in video streams, a key technology with broad applications ranging from surveillance to healthcare. Tackling VAD in real-life settings poses significant challenges due to the dynamic nature of human actions, environmental variations, and domain shifts. Many research initiatives neglect these complexities, often concentrating on traditional testing methods that fail to account for performance on unseen datasets, creating a gap between theoretical models and their real-world utility. Online learning is a potential strategy to mitigate this issue by allowing models to adapt to new information continuously. This paper assesses how well current VAD algorithms can adjust to real-life conditions through an online learning framework, particularly those based on pose analysis, for their efficiency and privacy advantages. Our proposed framework enables continuous model updates with streaming data from novel environments, thus mirroring actual world challenges and evaluating the models' ability to adapt in real-time while maintaining accuracy. We investigate three state-of-the-art models in this setting, focusing on their adaptability across different domains. Our findings indicate that, even under the most challenging conditions, our online learning approach allows a model to preserve 89.39% of its original effectiveness compared to its offline-trained counterpart in a specific target domain.","sentences":["Video Anomaly Detection (VAD) identifies unusual activities in video streams, a key technology with broad applications ranging from surveillance to healthcare.","Tackling VAD in real-life settings poses significant challenges due to the dynamic nature of human actions, environmental variations, and domain shifts.","Many research initiatives neglect these complexities, often concentrating on traditional testing methods that fail to account for performance on unseen datasets, creating a gap between theoretical models and their real-world utility.","Online learning is a potential strategy to mitigate this issue by allowing models to adapt to new information continuously.","This paper assesses how well current VAD algorithms can adjust to real-life conditions through an online learning framework, particularly those based on pose analysis, for their efficiency and privacy advantages.","Our proposed framework enables continuous model updates with streaming data from novel environments, thus mirroring actual world challenges and evaluating the models' ability to adapt in real-time while maintaining accuracy.","We investigate three state-of-the-art models in this setting, focusing on their adaptability across different domains.","Our findings indicate that, even under the most challenging conditions, our online learning approach allows a model to preserve 89.39% of its original effectiveness compared to its offline-trained counterpart in a specific target domain."],"url":"http://arxiv.org/abs/2404.18747v1","category":"cs.CV"}
{"created":"2024-04-29 14:46:35","title":"Enhancing Interactive Image Retrieval With Query Rewriting Using Large Language Models and Vision Language Models","abstract":"Image search stands as a pivotal task in multimedia and computer vision, finding applications across diverse domains, ranging from internet search to medical diagnostics. Conventional image search systems operate by accepting textual or visual queries, retrieving the top-relevant candidate results from the database. However, prevalent methods often rely on single-turn procedures, introducing potential inaccuracies and limited recall. These methods also face the challenges, such as vocabulary mismatch and the semantic gap, constraining their overall effectiveness. To address these issues, we propose an interactive image retrieval system capable of refining queries based on user relevance feedback in a multi-turn setting. This system incorporates a vision language model (VLM) based image captioner to enhance the quality of text-based queries, resulting in more informative queries with each iteration. Moreover, we introduce a large language model (LLM) based denoiser to refine text-based query expansions, mitigating inaccuracies in image descriptions generated by captioning models. To evaluate our system, we curate a new dataset by adapting the MSR-VTT video retrieval dataset to the image retrieval task, offering multiple relevant ground truth images for each query. Through comprehensive experiments, we validate the effectiveness of our proposed system against baseline methods, achieving state-of-the-art performance with a notable 10\\% improvement in terms of recall. Our contributions encompass the development of an innovative interactive image retrieval system, the integration of an LLM-based denoiser, the curation of a meticulously designed evaluation dataset, and thorough experimental validation.","sentences":["Image search stands as a pivotal task in multimedia and computer vision, finding applications across diverse domains, ranging from internet search to medical diagnostics.","Conventional image search systems operate by accepting textual or visual queries, retrieving the top-relevant candidate results from the database.","However, prevalent methods often rely on single-turn procedures, introducing potential inaccuracies and limited recall.","These methods also face the challenges, such as vocabulary mismatch and the semantic gap, constraining their overall effectiveness.","To address these issues, we propose an interactive image retrieval system capable of refining queries based on user relevance feedback in a multi-turn setting.","This system incorporates a vision language model (VLM) based image captioner to enhance the quality of text-based queries, resulting in more informative queries with each iteration.","Moreover, we introduce a large language model (LLM) based denoiser to refine text-based query expansions, mitigating inaccuracies in image descriptions generated by captioning models.","To evaluate our system, we curate a new dataset by adapting the MSR-VTT video retrieval dataset to the image retrieval task, offering multiple relevant ground truth images for each query.","Through comprehensive experiments, we validate the effectiveness of our proposed system against baseline methods, achieving state-of-the-art performance with a notable 10\\% improvement in terms of recall.","Our contributions encompass the development of an innovative interactive image retrieval system, the integration of an LLM-based denoiser, the curation of a meticulously designed evaluation dataset, and thorough experimental validation."],"url":"http://arxiv.org/abs/2404.18746v1","category":"cs.MM"}
{"created":"2024-04-29 14:42:32","title":"Torsion-induced axions in string theory, quantum gravity and the cosmological tensions","abstract":"We discuss the role of torsion in string theory on inducing pseudoscalar degrees of freedom (axions), which in turn couple to (gravitational) Chern-Simons (CS) anomalous terms. Such interactions can induce inflation, of running vacuum type, not requiring external inflaton fields, through condensation of the anomalous terms as a consequence of primordial chiral gravitational-wave (GW) tensor perturbations in a weak-quantum gravity setting. The presence of an UV cutoff for the GW quantum graviton modes opens up the system, leading to a dissipative behaviour realised via the presence of non trivial imaginary parts of the gravitational CS terms. The naive estimate of the life time of inflation based on such imaginary parts, which afflict the pertinent GW Hamiltonian, is quite consistent with the estimates of the duration of inflation based on an analysis of the condensate-induced linear-axion-potential by means of dynamical systems. Such quantum-gravity effects can also contribute positively to the alleviation of cosmological tensions if they survive today. In the talk we discuss the conditions under which such a result may be achieved. We also discuss the potential role of other axions in string theory, coming from compactification, in inducing enhanced densities of primordial black holes during RVM inflation, thereby contributing to significantly increased percentages of these black holes that can play the role of dark matter components. Moreover, under certain circumstances, that we shall discuss in some detail, it is also possible that the initially massless torsion-induced axions can acquire a non-trivial mass during the radiation era, thereby providing additional dark matter components in the Universe. With regards to this aspect, we also emphasise the role of massive right-handed neutrinos, provided that such excitations exist in the relevant spectra.","sentences":["We discuss the role of torsion in string theory on inducing pseudoscalar degrees of freedom (axions), which in turn couple to (gravitational) Chern-Simons (CS) anomalous terms.","Such interactions can induce inflation, of running vacuum type, not requiring external inflaton fields, through condensation of the anomalous terms as a consequence of primordial chiral gravitational-wave (GW) tensor perturbations in a weak-quantum gravity setting.","The presence of an UV cutoff for the GW quantum graviton modes opens up the system, leading to a dissipative behaviour realised via the presence of non trivial imaginary parts of the gravitational CS terms.","The naive estimate of the life time of inflation based on such imaginary parts, which afflict the pertinent GW Hamiltonian, is quite consistent with the estimates of the duration of inflation based on an analysis of the condensate-induced linear-axion-potential by means of dynamical systems.","Such quantum-gravity effects can also contribute positively to the alleviation of cosmological tensions if they survive today.","In the talk we discuss the conditions under which such a result may be achieved.","We also discuss the potential role of other axions in string theory, coming from compactification, in inducing enhanced densities of primordial black holes during RVM inflation, thereby contributing to significantly increased percentages of these black holes that can play the role of dark matter components.","Moreover, under certain circumstances, that we shall discuss in some detail, it is also possible that the initially massless torsion-induced axions can acquire a non-trivial mass during the radiation era, thereby providing additional dark matter components in the Universe.","With regards to this aspect, we also emphasise the role of massive right-handed neutrinos, provided that such excitations exist in the relevant spectra."],"url":"http://arxiv.org/abs/2404.18741v1","category":"gr-qc"}
{"created":"2024-04-29 14:10:08","title":"Innovative Integration of Visual Foundation Model with a Robotic Arm on a Mobile Platform","abstract":"In the rapidly advancing field of robotics, the fusion of state-of-the-art visual technologies with mobile robotic arms has emerged as a critical integration. This paper introduces a novel system that combines the Segment Anything model (SAM) -- a transformer-based visual foundation model -- with a robotic arm on a mobile platform. The design of integrating a depth camera on the robotic arm's end-effector ensures continuous object tracking, significantly mitigating environmental uncertainties. By deploying on a mobile platform, our grasping system has an enhanced mobility, playing a key role in dynamic environments where adaptability are critical. This synthesis enables dynamic object segmentation, tracking, and grasping. It also elevates user interaction, allowing the robot to intuitively respond to various modalities such as clicks, drawings, or voice commands, beyond traditional robotic systems. Empirical assessments in both simulated and real-world demonstrate the system's capabilities. This configuration opens avenues for wide-ranging applications, from industrial settings, agriculture, and household tasks, to specialized assignments and beyond.","sentences":["In the rapidly advancing field of robotics, the fusion of state-of-the-art visual technologies with mobile robotic arms has emerged as a critical integration.","This paper introduces a novel system that combines the Segment Anything model (SAM) -- a transformer-based visual foundation model -- with a robotic arm on a mobile platform.","The design of integrating a depth camera on the robotic arm's end-effector ensures continuous object tracking, significantly mitigating environmental uncertainties.","By deploying on a mobile platform, our grasping system has an enhanced mobility, playing a key role in dynamic environments where adaptability are critical.","This synthesis enables dynamic object segmentation, tracking, and grasping.","It also elevates user interaction, allowing the robot to intuitively respond to various modalities such as clicks, drawings, or voice commands, beyond traditional robotic systems.","Empirical assessments in both simulated and real-world demonstrate the system's capabilities.","This configuration opens avenues for wide-ranging applications, from industrial settings, agriculture, and household tasks, to specialized assignments and beyond."],"url":"http://arxiv.org/abs/2404.18720v1","category":"cs.RO"}
{"created":"2024-04-29 14:02:02","title":"Adaptive Reinforcement Learning for Robot Control","abstract":"Deep reinforcement learning (DRL) has shown remarkable success in simulation domains, yet its application in designing robot controllers remains limited, due to its single-task orientation and insufficient adaptability to environmental changes. To overcome these limitations, we present a novel adaptive agent that leverages transfer learning techniques to dynamically adapt policy in response to different tasks and environmental conditions. The approach is validated through the blimp control challenge, where multitasking capabilities and environmental adaptability are essential. The agent is trained using a custom, highly parallelized simulator built on IsaacGym. We perform zero-shot transfer to fly the blimp in the real world to solve various tasks. We share our code at \\url{https://github.com/robot-perception-group/adaptive\\_agent/}.","sentences":["Deep reinforcement learning (DRL) has shown remarkable success in simulation domains, yet its application in designing robot controllers remains limited, due to its single-task orientation and insufficient adaptability to environmental changes.","To overcome these limitations, we present a novel adaptive agent that leverages transfer learning techniques to dynamically adapt policy in response to different tasks and environmental conditions.","The approach is validated through the blimp control challenge, where multitasking capabilities and environmental adaptability are essential.","The agent is trained using a custom, highly parallelized simulator built on IsaacGym.","We perform zero-shot transfer to fly the blimp in the real world to solve various tasks.","We share our code at \\url{https://github.com/robot-perception-group/adaptive\\_agent/}."],"url":"http://arxiv.org/abs/2404.18713v1","category":"cs.RO"}
{"created":"2024-04-29 13:49:12","title":"Real-fluid Transport Property Computations Based on the Boltzmann-weighted Full-dimensional Potential Model","abstract":"The intermolecular potential plays crucial roles in real-fluid interactions away from the ideal-gas equilibrium, such as supercritical fluid, high-enthalpy fluid, plasma interactions, etc. We propose a Boltzmann-weighted Full-dimensional (BWF) potential model for real-fluid computations. It includes diverse intermolecular interactions so as to determine the potential well, molecular diameter, dipole moment, polarizability of species without introducing bath gases, allowing more accurate descriptions of potential surfaces with more potential parameters. The anisotropy and temperature dependence of potential parameters are also considered by applying the Boltzmann weighting on all orientations. Through the high-level Symmetry-Adapted Perturbation Theory calculations, full-dimensional potential energy surface datasets are obtained in 432 orientations for each species. Subsequently, the Boltzmann-weighted Full-dimensional potential parameters are derived by training the dataset exceeding 5*106 data, including nonpolar and polar molecules, radicals, long-chain molecules, and ions. These BWF transport properties calculated by the BWF potential have been compared against the Lennard-Jones transport properties as well as experimental viscosity, mass diffusivity, and thermal conductivity coefficients. It shows discrepancies of viscosity coefficients within 1% and 5% for nonpolar and polar molecules, respectively. Furthermore, this potential model is applied to study radicals, long-chain molecules, and ions, for which the experimental data is rarely accessed in high accuracy. It indicates significant prediction improvements of complex interactions between various particles. The new transport properties are also embedded to predict the laminar flame speeds and the flame extinction limits of methane, dimethyl ether, and n-heptane at elevated pressures, confirming its predictivity and effectiveness.","sentences":["The intermolecular potential plays crucial roles in real-fluid interactions away from the ideal-gas equilibrium, such as supercritical fluid, high-enthalpy fluid, plasma interactions, etc.","We propose a Boltzmann-weighted Full-dimensional (BWF) potential model for real-fluid computations.","It includes diverse intermolecular interactions so as to determine the potential well, molecular diameter, dipole moment, polarizability of species without introducing bath gases, allowing more accurate descriptions of potential surfaces with more potential parameters.","The anisotropy and temperature dependence of potential parameters are also considered by applying the Boltzmann weighting on all orientations.","Through the high-level Symmetry-Adapted Perturbation Theory calculations, full-dimensional potential energy surface datasets are obtained in 432 orientations for each species.","Subsequently, the Boltzmann-weighted Full-dimensional potential parameters are derived by training the dataset exceeding 5*106 data, including nonpolar and polar molecules, radicals, long-chain molecules, and ions.","These BWF transport properties calculated by the BWF potential have been compared against the Lennard-Jones transport properties as well as experimental viscosity, mass diffusivity, and thermal conductivity coefficients.","It shows discrepancies of viscosity coefficients within 1% and 5% for nonpolar and polar molecules, respectively.","Furthermore, this potential model is applied to study radicals, long-chain molecules, and ions, for which the experimental data is rarely accessed in high accuracy.","It indicates significant prediction improvements of complex interactions between various particles.","The new transport properties are also embedded to predict the laminar flame speeds and the flame extinction limits of methane, dimethyl ether, and n-heptane at elevated pressures, confirming its predictivity and effectiveness."],"url":"http://arxiv.org/abs/2404.18700v1","category":"physics.app-ph"}
{"created":"2024-04-29 13:43:49","title":"Dual-Modal Prompting for Sketch-Based Image Retrieval","abstract":"Sketch-based image retrieval (SBIR) associates hand-drawn sketches with their corresponding realistic images. In this study, we aim to tackle two major challenges of this task simultaneously: i) zero-shot, dealing with unseen categories, and ii) fine-grained, referring to intra-category instance-level retrieval. Our key innovation lies in the realization that solely addressing this cross-category and fine-grained recognition task from the generalization perspective may be inadequate since the knowledge accumulated from limited seen categories might not be fully valuable or transferable to unseen target categories. Inspired by this, in this work, we propose a dual-modal prompting CLIP (DP-CLIP) network, in which an adaptive prompting strategy is designed. Specifically, to facilitate the adaptation of our DP-CLIP toward unpredictable target categories, we employ a set of images within the target category and the textual category label to respectively construct a set of category-adaptive prompt tokens and channel scales. By integrating the generated guidance, DP-CLIP could gain valuable category-centric insights, efficiently adapting to novel categories and capturing unique discriminative clues for effective retrieval within each target category. With these designs, our DP-CLIP outperforms the state-of-the-art fine-grained zero-shot SBIR method by 7.3% in Acc.@1 on the Sketchy dataset. Meanwhile, in the other two category-level zero-shot SBIR benchmarks, our method also achieves promising performance.","sentences":["Sketch-based image retrieval (SBIR) associates hand-drawn sketches with their corresponding realistic images.","In this study, we aim to tackle two major challenges of this task simultaneously: i) zero-shot, dealing with unseen categories, and ii) fine-grained, referring to intra-category instance-level retrieval.","Our key innovation lies in the realization that solely addressing this cross-category and fine-grained recognition task from the generalization perspective may be inadequate since the knowledge accumulated from limited seen categories might not be fully valuable or transferable to unseen target categories.","Inspired by this, in this work, we propose a dual-modal prompting CLIP (DP-CLIP) network, in which an adaptive prompting strategy is designed.","Specifically, to facilitate the adaptation of our DP-CLIP toward unpredictable target categories, we employ a set of images within the target category and the textual category label to respectively construct a set of category-adaptive prompt tokens and channel scales.","By integrating the generated guidance, DP-CLIP could gain valuable category-centric insights, efficiently adapting to novel categories and capturing unique discriminative clues for effective retrieval within each target category.","With these designs, our DP-CLIP outperforms the state-of-the-art fine-grained zero-shot SBIR method by 7.3% in Acc.@1 on the Sketchy dataset.","Meanwhile, in the other two category-level zero-shot SBIR benchmarks, our method also achieves promising performance."],"url":"http://arxiv.org/abs/2404.18695v1","category":"cs.CV"}
{"created":"2024-04-29 13:34:19","title":"Socially Adaptive Path Planning Based on Generative Adversarial Network","abstract":"The natural interaction between robots and pedestrians in the process of autonomous navigation is crucial for the intelligent development of mobile robots, which requires robots to fully consider social rules and guarantee the psychological comfort of pedestrians. Among the research results in the field of robotic path planning, the learning-based socially adaptive algorithms have performed well in some specific human-robot interaction environments. However, human-robot interaction scenarios are diverse and constantly changing in daily life, and the generalization of robot socially adaptive path planning remains to be further investigated. In order to address this issue, this work proposes a new socially adaptive path planning algorithm by combining the generative adversarial network (GAN) with the Optimal Rapidly-exploring Random Tree (RRT*) navigation algorithm. Firstly, a GAN model with strong generalization performance is proposed to adapt the navigation algorithm to more scenarios. Secondly, a GAN model based Optimal Rapidly-exploring Random Tree navigation algorithm (GAN-RRT*) is proposed to generate paths in human-robot interaction environments. Finally, we propose a socially adaptive path planning framework named GAN-RTIRL, which combines the GAN model with Rapidly-exploring random Trees Inverse Reinforcement Learning (RTIRL) to improve the homotopy rate between planned and demonstration paths. In the GAN-RTIRL framework, the GAN-RRT* path planner can update the GAN model from the demonstration path. In this way, the robot can generate more anthropomorphic paths in human-robot interaction environments and has stronger generalization in more complex environments. Experimental results reveal that our proposed method can effectively improve the anthropomorphic degree of robot motion planning and the homotopy rate between planned and demonstration paths.","sentences":["The natural interaction between robots and pedestrians in the process of autonomous navigation is crucial for the intelligent development of mobile robots, which requires robots to fully consider social rules and guarantee the psychological comfort of pedestrians.","Among the research results in the field of robotic path planning, the learning-based socially adaptive algorithms have performed well in some specific human-robot interaction environments.","However, human-robot interaction scenarios are diverse and constantly changing in daily life, and the generalization of robot socially adaptive path planning remains to be further investigated.","In order to address this issue, this work proposes a new socially adaptive path planning algorithm by combining the generative adversarial network (GAN) with the Optimal Rapidly-exploring Random Tree (RRT*) navigation algorithm.","Firstly, a GAN model with strong generalization performance is proposed to adapt the navigation algorithm to more scenarios.","Secondly, a GAN model based Optimal Rapidly-exploring Random Tree navigation algorithm (GAN-RRT*) is proposed to generate paths in human-robot interaction environments.","Finally, we propose a socially adaptive path planning framework named GAN-RTIRL, which combines the GAN model with Rapidly-exploring random Trees Inverse Reinforcement Learning (RTIRL) to improve the homotopy rate between planned and demonstration paths.","In the GAN-RTIRL framework, the GAN-RRT* path planner can update the GAN model from the demonstration path.","In this way, the robot can generate more anthropomorphic paths in human-robot interaction environments and has stronger generalization in more complex environments.","Experimental results reveal that our proposed method can effectively improve the anthropomorphic degree of robot motion planning and the homotopy rate between planned and demonstration paths."],"url":"http://arxiv.org/abs/2404.18687v1","category":"cs.RO"}
{"created":"2024-04-29 13:23:38","title":"How Deep Is Your Gaze? Leveraging Distance in Image-Based Gaze Analysis","abstract":"Image thumbnails are a valuable data source for fixation filtering, scanpath classification, and visualization of eye tracking data. They are typically extracted with a constant size, approximating the foveated area. As a consequence, the focused area of interest in the scene becomes less prominent in the thumbnail with increasing distance, affecting image-based analysis techniques. In this work, we propose depth-adaptive thumbnails, a method for varying image size according to the eye-to-object distance. Adjusting the visual angle relative to the distance leads to a zoom effect on the focused area. We evaluate our approach on recordings in augmented reality, investigating the similarity of thumbnails and scanpaths. Our quantitative findings suggest that considering the eye-to-object distance improves the quality of data analysis and visualization. We demonstrate the utility of depth-adaptive thumbnails for applications in scanpath comparison and visualization.","sentences":["Image thumbnails are a valuable data source for fixation filtering, scanpath classification, and visualization of eye tracking data.","They are typically extracted with a constant size, approximating the foveated area.","As a consequence, the focused area of interest in the scene becomes less prominent in the thumbnail with increasing distance, affecting image-based analysis techniques.","In this work, we propose depth-adaptive thumbnails, a method for varying image size according to the eye-to-object distance.","Adjusting the visual angle relative to the distance leads to a zoom effect on the focused area.","We evaluate our approach on recordings in augmented reality, investigating the similarity of thumbnails and scanpaths.","Our quantitative findings suggest that considering the eye-to-object distance improves the quality of data analysis and visualization.","We demonstrate the utility of depth-adaptive thumbnails for applications in scanpath comparison and visualization."],"url":"http://arxiv.org/abs/2404.18680v1","category":"cs.HC"}
{"created":"2024-04-29 13:15:22","title":"Exact symmetry conservation and automatic mesh refinement in discrete initial boundary value problems","abstract":"We present a novel solution procedure for initial boundary value problems. The procedure is based on an action principle, in which coordinate maps are included as dynamical degrees of freedom. This reparametrization invariant action is formulated in an abstract parameter space and an energy density scale associated with the space-time coordinates separates the dynamics of the coordinate maps and of the propagating fields. Treating coordinates as dependent, i.e. dynamical quantities, offers the opportunity to discretize the action while retaining all space-time symmetries and also provides the basis for automatic adaptive mesh refinement (AMR). The presence of unbroken space-time symmetries after discretization also ensures that the associated continuum Noether charges remain exactly conserved. The presence of coordinate maps in addition provides new freedom in the choice of boundary conditions. An explicit numerical example for wave propagation in $1+1$ dimensions is provided, using recently developed regularized summation-by-parts finite difference operators.","sentences":["We present a novel solution procedure for initial boundary value problems.","The procedure is based on an action principle, in which coordinate maps are included as dynamical degrees of freedom.","This reparametrization invariant action is formulated in an abstract parameter space and an energy density scale associated with the space-time coordinates separates the dynamics of the coordinate maps and of the propagating fields.","Treating coordinates as dependent, i.e. dynamical quantities, offers the opportunity to discretize the action while retaining all space-time symmetries and also provides the basis for automatic adaptive mesh refinement (AMR).","The presence of unbroken space-time symmetries after discretization also ensures that the associated continuum Noether charges remain exactly conserved.","The presence of coordinate maps in addition provides new freedom in the choice of boundary conditions.","An explicit numerical example for wave propagation in $1+1$ dimensions is provided, using recently developed regularized summation-by-parts finite difference operators."],"url":"http://arxiv.org/abs/2404.18676v1","category":"math.NA"}
{"created":"2024-04-29 13:13:10","title":"Open-Source Drift Detection Tools in Action: Insights from Two Use Cases","abstract":"Data drifts pose a critical challenge in the lifecycle of machine learning (ML) models, affecting their performance and reliability. In response to this challenge, we present a microbenchmark study, called D3Bench, which evaluates the efficacy of open-source drift detection tools. D3Bench examines the capabilities of Evidently AI, NannyML, and Alibi-Detect, leveraging real-world data from two smart building use cases.We prioritize assessing the functional suitability of these tools to identify and analyze data drifts. Furthermore, we consider a comprehensive set of non-functional criteria, such as the integrability with ML pipelines, the adaptability to diverse data types, user-friendliness, computational efficiency, and resource demands. Our findings reveal that Evidently AI stands out for its general data drift detection, whereas NannyML excels at pinpointing the precise timing of shifts and evaluating their consequent effects on predictive accuracy.","sentences":["Data drifts pose a critical challenge in the lifecycle of machine learning (ML) models, affecting their performance and reliability.","In response to this challenge, we present a microbenchmark study, called D3Bench, which evaluates the efficacy of open-source drift detection tools.","D3Bench examines the capabilities of Evidently AI, NannyML, and Alibi-Detect, leveraging real-world data from two smart building use cases.","We prioritize assessing the functional suitability of these tools to identify and analyze data drifts.","Furthermore, we consider a comprehensive set of non-functional criteria, such as the integrability with ML pipelines, the adaptability to diverse data types, user-friendliness, computational efficiency, and resource demands.","Our findings reveal that Evidently AI stands out for its general data drift detection, whereas NannyML excels at pinpointing the precise timing of shifts and evaluating their consequent effects on predictive accuracy."],"url":"http://arxiv.org/abs/2404.18673v1","category":"cs.DB"}
{"created":"2024-04-29 12:48:42","title":"Terrain characterisation for online adaptability of automated sonar processing: Lessons learnt from operationally applying ATR to sidescan sonar in MCM applications","abstract":"The performance of Automated Recognition (ATR) algorithms on side-scan sonar imagery has shown to degrade rapidly when deployed on non benign environments. Complex seafloors and acoustic artefacts constitute distractors in the form of strong textural patterns, creating false detections or preventing detections of true objects. This paper presents two online seafloor characterisation techniques to improve explainability during Autonomous Underwater Vehicles (AUVs) missions. Importantly and as opposed to previous work in the domain, these techniques are not based on a model and require limited input from human operators, making it suitable for real-time onboard processing. Both techniques rely on an unsupervised machine learning approach to extract terrain features which relate to the human understanding of terrain complexity. The first technnique provides a quantitative, application-driven terrain characterisation metric based on the performance of an ATR algorithm. The second method provides a way to incorporate subject matter expertise and enables contextualisation and explainability in support for scenario-dependent subjective terrain characterisation. The terrain complexity matches the expectation of seasoned users making this tool desirable and trustworthy in comparison to traditional unsupervised approaches. We finally detail an application of these techniques to repair a Mine Countermeasures (MCM) mission carried with SeeByte autonomy framework Neptune.","sentences":["The performance of Automated Recognition (ATR) algorithms on side-scan sonar imagery has shown to degrade rapidly when deployed on non benign environments.","Complex seafloors and acoustic artefacts constitute distractors in the form of strong textural patterns, creating false detections or preventing detections of true objects.","This paper presents two online seafloor characterisation techniques to improve explainability during Autonomous Underwater Vehicles (AUVs) missions.","Importantly and as opposed to previous work in the domain, these techniques are not based on a model and require limited input from human operators, making it suitable for real-time onboard processing.","Both techniques rely on an unsupervised machine learning approach to extract terrain features which relate to the human understanding of terrain complexity.","The first technnique provides a quantitative, application-driven terrain characterisation metric based on the performance of an ATR algorithm.","The second method provides a way to incorporate subject matter expertise and enables contextualisation and explainability in support for scenario-dependent subjective terrain characterisation.","The terrain complexity matches the expectation of seasoned users making this tool desirable and trustworthy in comparison to traditional unsupervised approaches.","We finally detail an application of these techniques to repair a Mine Countermeasures (MCM) mission carried with SeeByte autonomy framework Neptune."],"url":"http://arxiv.org/abs/2404.18663v1","category":"cs.CV"}
{"created":"2024-04-29 12:28:52","title":"Dynamical Photon Condensation into Wannier-Stark States","abstract":"Strongly coupled light-matter systems can exhibit nonequilibrium collective phenomena due to loss and gain processes on the one hand and effective photon-photon interactions on the other hand. Here we study a photonic lattice system composed of a linear array of driven-dissipative coupled cavities (or cavity modes) with linearly increasing resonance frequencies across the lattice. The model amounts to a driven-dissipative Bose-Hubbard model in a tilted potential without the particle-conservation constraint. We predict a diverse range of stationary and non-stationary states resulted from the interplay of the tilt, tunneling, on-site interactions, and the loss and gain processes. Our key finding is that, under weak on-site interactions, photons mostly Bose condense into a selected, single-particle Wannier-Stark state, instead of exhibiting expected Bloch oscillations. As the strength of the photon-photon interactions increase, a non-stationary regime emerges which is marked surprisingly by periodic Bloch-type oscillations. These intriguing, nontrivial effects are a direct consequence of the driven-dissipative nature of the system.","sentences":["Strongly coupled light-matter systems can exhibit nonequilibrium collective phenomena due to loss and gain processes on the one hand and effective photon-photon interactions on the other hand.","Here we study a photonic lattice system composed of a linear array of driven-dissipative coupled cavities (or cavity modes) with linearly increasing resonance frequencies across the lattice.","The model amounts to a driven-dissipative Bose-Hubbard model in a tilted potential without the particle-conservation constraint.","We predict a diverse range of stationary and non-stationary states resulted from the interplay of the tilt, tunneling, on-site interactions, and the loss and gain processes.","Our key finding is that, under weak on-site interactions, photons mostly Bose condense into a selected, single-particle Wannier-Stark state, instead of exhibiting expected Bloch oscillations.","As the strength of the photon-photon interactions increase, a non-stationary regime emerges which is marked surprisingly by periodic Bloch-type oscillations.","These intriguing, nontrivial effects are a direct consequence of the driven-dissipative nature of the system."],"url":"http://arxiv.org/abs/2404.18647v1","category":"quant-ph"}
{"created":"2024-04-29 12:24:43","title":"Low-Overhead Defect-Adaptive Surface Code with Bandage-Like Super-Stabilizers","abstract":"To make practical quantum algorithms work, large-scale quantum processors protected by error-correcting codes are required to resist noise and ensure reliable computational outcomes. However, a major challenge arises from defects in processor fabrication, as well as occasional losses or cosmic rays during the computing process, all of which can lead to qubit malfunctions and disrupt error-correcting codes' normal operations. In this context, we introduce an automatic adapter to implement the surface code on defective lattices. Unlike previous approaches, this adapter leverages newly proposed bandage-like super-stabilizers to save more qubits when defects are clustered, thus enhancing the code distance and reducing super-stabilizer weight. For instance, in comparison with earlier methods, with a code size of 27 and a random defect rate of 2\\%, the disabled qubits decrease by $1/3$, and the average preserved code distance increases by 63\\%. This demonstrates a significant reduction in overhead when handling defects using our approach, and this advantage amplifies with increasing processor size and defect rates. Our work presents a low-overhead, automated solution to the challenge of adapting the surface code to defects, an essential step towards scaling up the construction of large-scale quantum computers for practical applications.","sentences":["To make practical quantum algorithms work, large-scale quantum processors protected by error-correcting codes are required to resist noise and ensure reliable computational outcomes.","However, a major challenge arises from defects in processor fabrication, as well as occasional losses or cosmic rays during the computing process, all of which can lead to qubit malfunctions and disrupt error-correcting codes' normal operations.","In this context, we introduce an automatic adapter to implement the surface code on defective lattices.","Unlike previous approaches, this adapter leverages newly proposed bandage-like super-stabilizers to save more qubits when defects are clustered, thus enhancing the code distance and reducing super-stabilizer weight.","For instance, in comparison with earlier methods, with a code size of 27 and a random defect rate of 2\\%, the disabled qubits decrease by $1/3$, and the average preserved code distance increases by 63\\%.","This demonstrates a significant reduction in overhead when handling defects using our approach, and this advantage amplifies with increasing processor size and defect rates.","Our work presents a low-overhead, automated solution to the challenge of adapting the surface code to defects, an essential step towards scaling up the construction of large-scale quantum computers for practical applications."],"url":"http://arxiv.org/abs/2404.18644v1","category":"quant-ph"}
{"created":"2024-04-29 11:41:34","title":"FlexiFilm: Long Video Generation with Flexible Conditions","abstract":"Generating long and consistent videos has emerged as a significant yet challenging problem. While most existing diffusion-based video generation models, derived from image generation models, demonstrate promising performance in generating short videos, their simple conditioning mechanism and sampling strategy-originally designed for image generation-cause severe performance degradation when adapted to long video generation. This results in prominent temporal inconsistency and overexposure. Thus, in this work, we introduce FlexiFilm, a new diffusion model tailored for long video generation. Our framework incorporates a temporal conditioner to establish a more consistent relationship between generation and multi-modal conditions, and a resampling strategy to tackle overexposure. Empirical results demonstrate FlexiFilm generates long and consistent videos, each over 30 seconds in length, outperforming competitors in qualitative and quantitative analyses. Project page: https://y-ichen.github.io/FlexiFilm-Page/","sentences":["Generating long and consistent videos has emerged as a significant yet challenging problem.","While most existing diffusion-based video generation models, derived from image generation models, demonstrate promising performance in generating short videos, their simple conditioning mechanism and sampling strategy-originally designed for image generation-cause severe performance degradation when adapted to long video generation.","This results in prominent temporal inconsistency and overexposure.","Thus, in this work, we introduce FlexiFilm, a new diffusion model tailored for long video generation.","Our framework incorporates a temporal conditioner to establish a more consistent relationship between generation and multi-modal conditions, and a resampling strategy to tackle overexposure.","Empirical results demonstrate FlexiFilm generates long and consistent videos, each over 30 seconds in length, outperforming competitors in qualitative and quantitative analyses.","Project page: https://y-ichen.github.io/FlexiFilm-Page/"],"url":"http://arxiv.org/abs/2404.18620v1","category":"cs.CV"}
{"created":"2024-04-29 11:39:59","title":"Nonlinear Superconducting Magnetoelectric Effect","abstract":"A supercurrent flow can induce a nonvanishing spin magnetization in noncentrosymmetric superconductors with spin-orbit interaction. Often known as the non-dissipative magnetoelectric effect, these are most commonly found at linear order in supercurrent flow. Here, we argue that a nonlinear superconducting magnetoelectric effect (NSM) can naturally manifest in altermagnet/superconductor (ALM/SC) heterostructures: NSM manifests as a spin polarization generated as a second-order response to a driving supercurrent. Strikingly, we find NSM is the leading order magnetization response in ALM/SC heterostructures and survives even in the presence of centrosymmetry; $C_4 \\mathcal{T}$ symmetry in altermagnets zeroes both the equilibrium magnetization as well as out-of-plane linear magnetoelectric response. This renders NSM a powerful electric and non-dissipative means of controlling magnetization in ALM/SC heterostructures, a promising platform for superconducting spintronics.","sentences":["A supercurrent flow can induce a nonvanishing spin magnetization in noncentrosymmetric superconductors with spin-orbit interaction.","Often known as the non-dissipative magnetoelectric effect, these are most commonly found at linear order in supercurrent flow.","Here, we argue that a nonlinear superconducting magnetoelectric effect (NSM) can naturally manifest in altermagnet/superconductor (ALM/SC) heterostructures: NSM manifests as a spin polarization generated as a second-order response to a driving supercurrent.","Strikingly, we find NSM is the leading order magnetization response in ALM/SC heterostructures and survives even in the presence of centrosymmetry; $C_4 \\mathcal{T}$ symmetry in altermagnets zeroes both the equilibrium magnetization as well as out-of-plane linear magnetoelectric response.","This renders NSM a powerful electric and non-dissipative means of controlling magnetization in ALM/SC heterostructures, a promising platform for superconducting spintronics."],"url":"http://arxiv.org/abs/2404.18616v1","category":"cond-mat.supr-con"}
{"created":"2024-04-29 11:30:50","title":"Enhancing Prosthetic Safety and Environmental Adaptability: A Visual-Inertial Prosthesis Motion Estimation Approach on Uneven Terrains","abstract":"Environment awareness is crucial for enhancing walking safety and stability of amputee wearing powered prosthesis when crossing uneven terrains such as stairs and obstacles. However, existing environmental perception systems for prosthesis only provide terrain types and corresponding parameters, which fails to prevent potential collisions when crossing uneven terrains and may lead to falls and other severe consequences. In this paper, a visual-inertial motion estimation approach is proposed for prosthesis to perceive its movement and the changes of spatial relationship between the prosthesis and uneven terrain when traversing them. To achieve this, we estimate the knee motion by utilizing a depth camera to perceive the environment and align feature points extracted from stairs and obstacles. Subsequently, an error-state Kalman filter is incorporated to fuse the inertial data into visual estimations to reduce the feature extraction error and obtain a more robust estimation. The motion of prosthetic joint and toe are derived using the prosthesis model parameters. Experiment conducted on our collected dataset and stair walking trials with a powered prosthesis shows that the proposed method can accurately tracking the motion of the human leg and prosthesis with an average root-mean-square error of toe trajectory less than 5 cm. The proposed method is expected to enable the environmental adaptive control for prosthesis, thereby enhancing amputee's safety and mobility in uneven terrains.","sentences":["Environment awareness is crucial for enhancing walking safety and stability of amputee wearing powered prosthesis when crossing uneven terrains such as stairs and obstacles.","However, existing environmental perception systems for prosthesis only provide terrain types and corresponding parameters, which fails to prevent potential collisions when crossing uneven terrains and may lead to falls and other severe consequences.","In this paper, a visual-inertial motion estimation approach is proposed for prosthesis to perceive its movement and the changes of spatial relationship between the prosthesis and uneven terrain when traversing them.","To achieve this, we estimate the knee motion by utilizing a depth camera to perceive the environment and align feature points extracted from stairs and obstacles.","Subsequently, an error-state Kalman filter is incorporated to fuse the inertial data into visual estimations to reduce the feature extraction error and obtain a more robust estimation.","The motion of prosthetic joint and toe are derived using the prosthesis model parameters.","Experiment conducted on our collected dataset and stair walking trials with a powered prosthesis shows that the proposed method can accurately tracking the motion of the human leg and prosthesis with an average root-mean-square error of toe trajectory less than 5 cm.","The proposed method is expected to enable the environmental adaptive control for prosthesis, thereby enhancing amputee's safety and mobility in uneven terrains."],"url":"http://arxiv.org/abs/2404.18612v1","category":"cs.RO"}
{"created":"2024-04-29 11:04:43","title":"A hybrid prognosis approach for robust lifetime control of commercial wind turbines","abstract":"Dynamic fluctuations in the wind field to which a wind turbine (WT) is exposed to are responsible for fatigue loads on its components. To reduce structural loads in WTs, advanced control schemes have been proposed. In recent years, prognosis-based lifetime control of WTs has become increasingly important. In this approach, the prognostic controller gains are adapted based on the stateof-health (SOH) of the WT component to achieve the desired lifetime. However, stochastic wind dynamics complicates estimation of the SOH of a WT. More recently, robust controllers have been combined with real-time damage evaluation models to meet prognosis objectives. Most rely on model-based online load cycle counting algorithms to determine fatigue damage, with analytical models providing the degradation estimate. However, most use load measurements that are either unreliable or unavailable in commercial WTs, limiting their practicality. In this contribution, a hybrid prognosis scheme combining data-driven load prediction and model-based damage estimation models for robust lifetime control of commercial WTs is proposed. A data-driven support vector machine (SVM) regression model is trained using loading data obtained from dynamic simulations using a {\\mu}-synthesis robust disturbance accommodating controller (RDAC). The regression model uses available WT measurements to predict tower load. Based on this prediction, an online rain-flow counting (RFC) damage evaluation model estimates the damage level and lifetime of the tower. The RDAC controller gains are dynamically adapted to achieve a predefined damage limit and lifetime. The proposed approach is evaluated on a 5 MW reference WT and its performance is compared with a model-based prognosis scheme using ideal WT tower measurement. Results demonstrate the efficacy of the proposed approach to control the fatigue lifetime in WT components.","sentences":["Dynamic fluctuations in the wind field to which a wind turbine (WT) is exposed to are responsible for fatigue loads on its components.","To reduce structural loads in WTs, advanced control schemes have been proposed.","In recent years, prognosis-based lifetime control of WTs has become increasingly important.","In this approach, the prognostic controller gains are adapted based on the stateof-health (SOH) of the WT component to achieve the desired lifetime.","However, stochastic wind dynamics complicates estimation of the SOH of a WT.","More recently, robust controllers have been combined with real-time damage evaluation models to meet prognosis objectives.","Most rely on model-based online load cycle counting algorithms to determine fatigue damage, with analytical models providing the degradation estimate.","However, most use load measurements that are either unreliable or unavailable in commercial WTs, limiting their practicality.","In this contribution, a hybrid prognosis scheme combining data-driven load prediction and model-based damage estimation models for robust lifetime control of commercial WTs is proposed.","A data-driven support vector machine (SVM) regression model is trained using loading data obtained from dynamic simulations using a {\\mu}-synthesis robust disturbance accommodating controller (RDAC).","The regression model uses available WT measurements to predict tower load.","Based on this prediction, an online rain-flow counting (RFC) damage evaluation model estimates the damage level and lifetime of the tower.","The RDAC controller gains are dynamically adapted to achieve a predefined damage limit and lifetime.","The proposed approach is evaluated on a 5 MW reference WT and its performance is compared with a model-based prognosis scheme using ideal WT tower measurement.","Results demonstrate the efficacy of the proposed approach to control the fatigue lifetime in WT components."],"url":"http://arxiv.org/abs/2404.18593v1","category":"eess.SY"}
{"created":"2024-04-29 10:59:07","title":"The link between hyperuniformity, Coulomb energy, and Wasserstein distance to Lebesgue for two-dimensional point processes","abstract":"We investigate the interplay between three possible properties of stationary point processes: i) Finite Coulomb energy with short-scale regularization, ii) Finite $2$-Wasserstein transportation distance to the Lebesgue measure and iii) Hyperuniformity. In dimension $2$, we prove that i) implies ii), which is known to imply iii), and we provide simple counter-examples to both converse implications. However, we prove that ii) implies i) for processes with a uniformly bounded density of points, and that i) - finiteness of the regularized Coulomb energy - is equivalent to a certain property of quantitative hyperuniformity that is just slightly stronger than hyperuniformity itself.   Our proof relies on the classical link between $H^{-1}$-norm and $2$-Wasserstein distance between measures, on the screening construction for Coulomb gases (of which we present an adaptation to $2$-Wasserstein space which might be of independent interest), and on recent necessary and sufficient conditions for the existence of stationary \"electric\" fields compatible with a given stationary point process.","sentences":["We investigate the interplay between three possible properties of stationary point processes: i) Finite Coulomb energy with short-scale regularization, ii) Finite $2$-Wasserstein transportation distance to the Lebesgue measure and iii) Hyperuniformity.","In dimension $2$, we prove that i) implies ii), which is known to imply iii), and we provide simple counter-examples to both converse implications.","However, we prove that ii) implies i) for processes with a uniformly bounded density of points, and that i) - finiteness of the regularized Coulomb energy - is equivalent to a certain property of quantitative hyperuniformity that is just slightly stronger than hyperuniformity itself.   ","Our proof relies on the classical link between $H^{-1}$-norm and $2$-Wasserstein distance between measures, on the screening construction for Coulomb gases (of which we present an adaptation to $2$-Wasserstein space which might be of independent interest), and on recent necessary and sufficient conditions for the existence of stationary \"electric\" fields compatible with a given stationary point process."],"url":"http://arxiv.org/abs/2404.18588v1","category":"math.PR"}
{"created":"2024-04-29 10:13:49","title":"Pre-relaxation in quantum, classical, and quantum-classical two-impurity models","abstract":"We numerically study the relaxation dynamics of impurity-host systems, focusing on the presence of long-lived metastable states in the non-equilibrium dynamics after an initial excitation of the impurities. In generic systems, an excited impurity coupled to a large bath at zero temperature is expected to relax and approach its ground state over time. However, certain exceptional cases exhibit metastability, where the system remains in an excited state on timescales largely exceeding the typical relaxation time. We study this phenomenon for three prototypical impurity models: a tight-binding quantum model of independent spinless fermions on a lattice with two stub impurities, a classical-spin Heisenberg model with two weakly coupled classical impurity spins, and a tight-binding quantum model of independent electrons with two classical impurity spins. Through numerical integration of the fundamental equations of motion, we find that all three models exhibit similar qualitative behavior: complete relaxation for nearest-neighbor impurities and incomplete or strongly delayed relaxation for next-nearest-neighbor impurities. The underlying mechanisms leading to this behavior differ between models and include impurity-induced bound states, emergent approximately conserved local observables, and exact cancellation of local and nonlocal dissipation effects.","sentences":["We numerically study the relaxation dynamics of impurity-host systems, focusing on the presence of long-lived metastable states in the non-equilibrium dynamics after an initial excitation of the impurities.","In generic systems, an excited impurity coupled to a large bath at zero temperature is expected to relax and approach its ground state over time.","However, certain exceptional cases exhibit metastability, where the system remains in an excited state on timescales largely exceeding the typical relaxation time.","We study this phenomenon for three prototypical impurity models: a tight-binding quantum model of independent spinless fermions on a lattice with two stub impurities, a classical-spin Heisenberg model with two weakly coupled classical impurity spins, and a tight-binding quantum model of independent electrons with two classical impurity spins.","Through numerical integration of the fundamental equations of motion, we find that all three models exhibit similar qualitative behavior: complete relaxation for nearest-neighbor impurities and incomplete or strongly delayed relaxation for next-nearest-neighbor impurities.","The underlying mechanisms leading to this behavior differ between models and include impurity-induced bound states, emergent approximately conserved local observables, and exact cancellation of local and nonlocal dissipation effects."],"url":"http://arxiv.org/abs/2404.18566v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-29 09:56:32","title":"Doubly Adaptive Importance Sampling","abstract":"We propose an adaptive importance sampling scheme for Gaussian approximations of intractable posteriors. Optimization-based approximations like variational inference can be too inaccurate while existing Monte Carlo methods can be too slow. Therefore, we propose a hybrid where, at each iteration, the Monte Carlo effective sample size can be guaranteed at a fixed computational cost by interpolating between natural-gradient variational inference and importance sampling. The amount of damping in the updates adapts to the posterior and guarantees the effective sample size. Gaussianity enables the use of Stein's lemma to obtain gradient-based optimization in the highly damped variational inference regime and a reduction of Monte Carlo error for undamped adaptive importance sampling. The result is a generic, embarrassingly parallel and adaptive posterior approximation method. Numerical studies on simulated and real data show its competitiveness with other, less general methods.","sentences":["We propose an adaptive importance sampling scheme for Gaussian approximations of intractable posteriors.","Optimization-based approximations like variational inference can be too inaccurate while existing Monte Carlo methods can be too slow.","Therefore, we propose a hybrid where, at each iteration, the Monte Carlo effective sample size can be guaranteed at a fixed computational cost by interpolating between natural-gradient variational inference and importance sampling.","The amount of damping in the updates adapts to the posterior and guarantees the effective sample size.","Gaussianity enables the use of Stein's lemma to obtain gradient-based optimization in the highly damped variational inference regime and a reduction of Monte Carlo error for undamped adaptive importance sampling.","The result is a generic, embarrassingly parallel and adaptive posterior approximation method.","Numerical studies on simulated and real data show its competitiveness with other, less general methods."],"url":"http://arxiv.org/abs/2404.18556v1","category":"stat.CO"}
{"created":"2024-04-29 09:45:46","title":"IncidentResponseGPT: Generating Traffic Incident Response Plans with Generative Artificial Intelligence","abstract":"Traffic congestion due to road incidents poses a significant challenge in urban environments, leading to increased pollution, economic losses, and traffic congestion. Efficiently managing these incidents is imperative for mitigating their adverse effects; however, the complexity of urban traffic systems and the variety of potential incidents represent a considerable obstacle. This paper introduces IncidentResponseGPT, an innovative solution designed to assist traffic management authorities by providing rapid, informed, and adaptable traffic incident response plans. By integrating a Generative AI platform with real-time traffic incident reports and operational guidelines, our system aims to streamline the decision-making process in responding to traffic incidents. The research addresses the critical challenges involved in deploying AI in traffic management, including overcoming the complexity of urban traffic networks, ensuring real-time decision-making capabilities, aligning with local laws and regulations, and securing public acceptance for AI-driven systems. Through a combination of text analysis of accident reports, validation of AI recommendations through traffic simulation, and implementation of transparent and validated AI systems, IncidentResponseGPT offers a promising approach to optimizing traffic flow and reducing congestion in the face of traffic incidents. The relevance of this work extends to traffic management authorities, emergency response teams, and municipal bodies, all integral stakeholders in urban traffic control and incident management. By proposing a novel solution to the identified challenges, this research aims to develop a framework that not only facilitates faster resolution of traffic incidents but also minimizes their overall impact on urban traffic systems.","sentences":["Traffic congestion due to road incidents poses a significant challenge in urban environments, leading to increased pollution, economic losses, and traffic congestion.","Efficiently managing these incidents is imperative for mitigating their adverse effects; however, the complexity of urban traffic systems and the variety of potential incidents represent a considerable obstacle.","This paper introduces IncidentResponseGPT, an innovative solution designed to assist traffic management authorities by providing rapid, informed, and adaptable traffic incident response plans.","By integrating a Generative AI platform with real-time traffic incident reports and operational guidelines, our system aims to streamline the decision-making process in responding to traffic incidents.","The research addresses the critical challenges involved in deploying AI in traffic management, including overcoming the complexity of urban traffic networks, ensuring real-time decision-making capabilities, aligning with local laws and regulations, and securing public acceptance for AI-driven systems.","Through a combination of text analysis of accident reports, validation of AI recommendations through traffic simulation, and implementation of transparent and validated AI systems, IncidentResponseGPT offers a promising approach to optimizing traffic flow and reducing congestion in the face of traffic incidents.","The relevance of this work extends to traffic management authorities, emergency response teams, and municipal bodies, all integral stakeholders in urban traffic control and incident management.","By proposing a novel solution to the identified challenges, this research aims to develop a framework that not only facilitates faster resolution of traffic incidents but also minimizes their overall impact on urban traffic systems."],"url":"http://arxiv.org/abs/2404.18550v1","category":"cs.LG"}
{"created":"2024-04-29 09:34:25","title":"Time Machine GPT","abstract":"Large language models (LLMs) are often trained on extensive, temporally indiscriminate text corpora, reflecting the lack of datasets with temporal metadata. This approach is not aligned with the evolving nature of language. Conventional methods for creating temporally adapted language models often depend on further pre-training static models on time-specific data. This paper presents a new approach: a series of point-in-time LLMs called Time Machine GPT (TiMaGPT), specifically designed to be nonprognosticative. This ensures they remain uninformed about future factual information and linguistic changes. This strategy is beneficial for understanding language evolution and is of critical importance when applying models in dynamic contexts, such as time-series forecasting, where foresight of future information can prove problematic. We provide access to both the models and training datasets.","sentences":["Large language models (LLMs) are often trained on extensive, temporally indiscriminate text corpora, reflecting the lack of datasets with temporal metadata.","This approach is not aligned with the evolving nature of language.","Conventional methods for creating temporally adapted language models often depend on further pre-training static models on time-specific data.","This paper presents a new approach: a series of point-in-time LLMs called Time Machine GPT (TiMaGPT), specifically designed to be nonprognosticative.","This ensures they remain uninformed about future factual information and linguistic changes.","This strategy is beneficial for understanding language evolution and is of critical importance when applying models in dynamic contexts, such as time-series forecasting, where foresight of future information can prove problematic.","We provide access to both the models and training datasets."],"url":"http://arxiv.org/abs/2404.18543v1","category":"cs.CL"}
{"created":"2024-04-29 09:33:53","title":"OAEI Machine Learning Dataset for Online Model Generation","abstract":"Ontology and knowledge graph matching systems are evaluated annually by the Ontology Alignment Evaluation Initiative (OAEI). More and more systems use machine learning-based approaches, including large language models. The training and validation datasets are usually determined by the system developer and often a subset of the reference alignments are used. This sampling is against the OAEI rules and makes a fair comparison impossible. Furthermore, those models are trained offline (a trained and optimized model is packaged into the matcher) and therefore the systems are specifically trained for those tasks. In this paper, we introduce a dataset that contains training, validation, and test sets for most of the OAEI tracks. Thus, online model learning (the systems must adapt to the given input alignment without human intervention) is made possible to enable a fair comparison for ML-based systems. We showcase the usefulness of the dataset by fine-tuning the confidence thresholds of popular systems.","sentences":["Ontology and knowledge graph matching systems are evaluated annually by the Ontology Alignment Evaluation Initiative (OAEI).","More and more systems use machine learning-based approaches, including large language models.","The training and validation datasets are usually determined by the system developer and often a subset of the reference alignments are used.","This sampling is against the OAEI rules and makes a fair comparison impossible.","Furthermore, those models are trained offline (a trained and optimized model is packaged into the matcher) and therefore the systems are specifically trained for those tasks.","In this paper, we introduce a dataset that contains training, validation, and test sets for most of the OAEI tracks.","Thus, online model learning (the systems must adapt to the given input alignment without human intervention) is made possible to enable a fair comparison for ML-based systems.","We showcase the usefulness of the dataset by fine-tuning the confidence thresholds of popular systems."],"url":"http://arxiv.org/abs/2404.18542v1","category":"cs.IR"}
{"created":"2024-04-29 09:25:18","title":"Adaptive (re)operations facilitate environmental flow maintenance downstream of multi-purpose reservoirs","abstract":"Multi-purpose reservoirs support socioeconomic development by providing irrigation, domestic water supply, hydropower, and other services. However, impoundment of water impacts instream aquatic ecosystems. Thus, the concept of minimum environmental flows (MEFs) was established to restore the benefits of naturally flowing rivers by specifying minimum flow rates to be maintained downstream of dams.But varying legislative contexts under which multi-purpose reservoirs operate may not always necessitate MEF releases. To what extent the release of MEF affects other sectoral benefits remains an open-ended and possibly a site-specific inquiry. A related issue is - how does the order in which releases are prioritized influences sectoral performances? We analyse these issues for the Nagarjuna Sagar reservoir, one of the largest multipurpose reservoirs in southern India. We formulate two versions of a multi-objective decision problem. PF_MEF formulation prioritizes MEF releases over releases for water demand satisfaction, followed by hydropower releases. PF_nMEF formulation follows the regional legislative rule releasing first for demand satisfaction, followed by hydropower and MEF releases. Results thus indicate that prioritizing MEF releases improves can meet MEF requirements without significant compromises in other objectives. We hypothesize that similar investigations may reveal how simple modification of release order may improve ability of other reservoirs to meet environmental goals.","sentences":["Multi-purpose reservoirs support socioeconomic development by providing irrigation, domestic water supply, hydropower, and other services.","However, impoundment of water impacts instream aquatic ecosystems.","Thus, the concept of minimum environmental flows (MEFs) was established to restore the benefits of naturally flowing rivers by specifying minimum flow rates to be maintained downstream of dams.","But varying legislative contexts under which multi-purpose reservoirs operate may not always necessitate MEF releases.","To what extent the release of MEF affects other sectoral benefits remains an open-ended and possibly a site-specific inquiry.","A related issue is - how does the order in which releases are prioritized influences sectoral performances?","We analyse these issues for the Nagarjuna Sagar reservoir, one of the largest multipurpose reservoirs in southern India.","We formulate two versions of a multi-objective decision problem.","PF_MEF formulation prioritizes MEF releases over releases for water demand satisfaction, followed by hydropower releases.","PF_nMEF formulation follows the regional legislative rule releasing first for demand satisfaction, followed by hydropower and MEF releases.","Results thus indicate that prioritizing MEF releases improves can meet MEF requirements without significant compromises in other objectives.","We hypothesize that similar investigations may reveal how simple modification of release order may improve ability of other reservoirs to meet environmental goals."],"url":"http://arxiv.org/abs/2404.18535v1","category":"math.OC"}
{"created":"2024-04-29 09:19:05","title":"MileBench: Benchmarking MLLMs in Long Context","abstract":"Despite the advancements and impressive performance of Multimodal Large Language Models (MLLMs) on benchmarks, their effectiveness in real-world, long-context, and multi-image tasks is unclear due to the benchmarks' limited scope. Existing benchmarks often focus on single-image and short-text samples, and when assessing multi-image tasks, they either limit the image count or focus on specific task (e.g time-series captioning), potentially obscuring the performance challenges of MLLMs. To address these limitations, we introduce MileBench, a pioneering benchmark designed to test the MultImodal Long-contExt capabilities of MLLMs. This benchmark comprises not only multimodal long contexts, but also multiple tasks requiring both comprehension and generation. We establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMs' long-context adaptation capacity and their ability to complete tasks in long-context scenarios. Our experimental results, obtained from testing 20 models, revealed that while the closed-source GPT-4(Vision) and Gemini 1.5 outperform others, most open-source MLLMs struggle in long-context situations. Interestingly, the performance gap tends to widen with an increase in the number of images. We strongly encourage an intensification of research efforts towards enhancing MLLMs' long-context capabilities, especially in scenarios involving multiple images.","sentences":["Despite the advancements and impressive performance of Multimodal Large Language Models (MLLMs) on benchmarks, their effectiveness in real-world, long-context, and multi-image tasks is unclear due to the benchmarks' limited scope.","Existing benchmarks often focus on single-image and short-text samples, and when assessing multi-image tasks, they either limit the image count or focus on specific task (e.g time-series captioning), potentially obscuring the performance challenges of MLLMs.","To address these limitations, we introduce MileBench, a pioneering benchmark designed to test the MultImodal Long-contExt capabilities of MLLMs.","This benchmark comprises not only multimodal long contexts, but also multiple tasks requiring both comprehension and generation.","We establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMs' long-context adaptation capacity and their ability to complete tasks in long-context scenarios.","Our experimental results, obtained from testing 20 models, revealed that while the closed-source GPT-4(Vision) and Gemini 1.5 outperform others, most open-source MLLMs struggle in long-context situations.","Interestingly, the performance gap tends to widen with an increase in the number of images.","We strongly encourage an intensification of research efforts towards enhancing MLLMs' long-context capabilities, especially in scenarios involving multiple images."],"url":"http://arxiv.org/abs/2404.18532v1","category":"cs.CL"}
{"created":"2024-04-29 08:50:17","title":"Towards Image Synthesis with Photon Counting Stellar Intensity Interferometry","abstract":"Stellar intensity interferometry (SII) is based on the correlation of the light intensity fluctuations of a star detected at two or more telescopes, with no need to combine the collected photons directly. A measurement of the correlation in full \"photon-counting mode\" was experimented with fast photon counters in Italy (2016-2020) and is currently being adapted to the ASTRI Mini-Array. Performing image synthesis with \"photon-counting\" SII requires a series of preparatory activities that involve the optimization of the pipelines for the treatment of time series acquired at extremely high photon rates, the development of efficient and innovative algorithms for the cross-correlation of the arrival times in large time series and the development of a preliminary version of a dedicated pipeline for the synthesis of images starting from interferometric data. Here we present the project and the present status of the activities.","sentences":["Stellar intensity interferometry (SII) is based on the correlation of the light intensity fluctuations of a star detected at two or more telescopes, with no need to combine the collected photons directly.","A measurement of the correlation in full \"photon-counting mode\" was experimented with fast photon counters in Italy (2016-2020) and is currently being adapted to the ASTRI Mini-Array.","Performing image synthesis with \"photon-counting\" SII requires a series of preparatory activities that involve the optimization of the pipelines for the treatment of time series acquired at extremely high photon rates, the development of efficient and innovative algorithms for the cross-correlation of the arrival times in large time series and the development of a preliminary version of a dedicated pipeline for the synthesis of images starting from interferometric data.","Here we present the project and the present status of the activities."],"url":"http://arxiv.org/abs/2404.18507v1","category":"astro-ph.IM"}
{"created":"2024-05-01 17:59:45","title":"Spectrally Pruned Gaussian Fields with Neural Compensation","abstract":"Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered attention for its fast rendering speed and high rendering quality. However, this comes with high memory consumption, e.g., a well-trained Gaussian field may utilize three million Gaussian primitives and over 700 MB of memory. We credit this high memory footprint to the lack of consideration for the relationship between primitives. In this paper, we propose a memory-efficient Gaussian field named SUNDAE with spectral pruning and neural compensation. On one hand, we construct a graph on the set of Gaussian primitives to model their relationship and design a spectral down-sampling module to prune out primitives while preserving desired signals. On the other hand, to compensate for the quality loss of pruning Gaussians, we exploit a lightweight neural network head to mix splatted features, which effectively compensates for quality losses while capturing the relationship between primitives in its weights. We demonstrate the performance of SUNDAE with extensive results. For example, SUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla Gaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB memory, on the Mip-NeRF360 dataset. Codes are publicly available at https://runyiyang.github.io/projects/SUNDAE/.","sentences":["Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered attention for its fast rendering speed and high rendering quality.","However, this comes with high memory consumption, e.g., a well-trained Gaussian field may utilize three million Gaussian primitives and over 700 MB of memory.","We credit this high memory footprint to the lack of consideration for the relationship between primitives.","In this paper, we propose a memory-efficient Gaussian field named SUNDAE with spectral pruning and neural compensation.","On one hand, we construct a graph on the set of Gaussian primitives to model their relationship and design a spectral down-sampling module to prune out primitives while preserving desired signals.","On the other hand, to compensate for the quality loss of pruning Gaussians, we exploit a lightweight neural network head to mix splatted features, which effectively compensates for quality losses while capturing the relationship between primitives in its weights.","We demonstrate the performance of SUNDAE with extensive results.","For example, SUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla Gaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB memory, on the Mip-NeRF360 dataset.","Codes are publicly available at https://runyiyang.github.io/projects/SUNDAE/."],"url":"http://arxiv.org/abs/2405.00676v1","category":"cs.CV"}
{"created":"2024-05-01 17:54:31","title":"Clique packings in random graphs","abstract":"We consider the question of how many edge-disjoint near-maximal cliques may be found in the dense Erd\\H{o}s-R\\'enyi random graph $G(n,p)$. Recently Acan and Kahn showed that the largest such family contains only $O(n^2/(\\log{n})^3)$ cliques, with high probability, which disproved a conjecture of Alon and Spencer. We prove the corresponding lower bound, $\\Omega(n^2/(\\log{n})^3)$, by considering a random graph process which sequentially selects and deletes near-maximal cliques. To analyse this process we use the Differential Equation Method. We also give a new proof of the upper bound $O(n^2/(\\log{n})^3)$ and discuss the problem of the precise size of the largest such clique packing.","sentences":["We consider the question of how many edge-disjoint near-maximal cliques may be found in the dense Erd\\H{o}s-R\\'enyi random graph $G(n,p)$. Recently Acan and Kahn showed that the largest such family contains only $O(n^2/(\\log{n})^3)$ cliques, with high probability, which disproved a conjecture of Alon and Spencer.","We prove the corresponding lower bound, $\\Omega(n^2/(\\log{n})^3)$, by considering a random graph process which sequentially selects and deletes near-maximal cliques.","To analyse this process we use the Differential Equation Method.","We also give a new proof of the upper bound $O(n^2/(\\log{n})^3)$ and discuss the problem of the precise size of the largest such clique packing."],"url":"http://arxiv.org/abs/2405.00667v1","category":"math.CO"}
{"created":"2024-05-01 17:28:27","title":"The Geometry of Loop Spaces II: Corrections","abstract":"This paper contains corrections to Madea, Rosenberg, Torres-Ardila, \"The Geometry of Loop Spaces II: Characteristic Classes,\" Advances in Math. (287), 2016, 485-518. The main change is that results about $\\pi_1({\\rm Diff}(M))$ are replaced by results about $\\pi_1({\\rm Isom}(M))$, where Diff$(M)$, Isom$(M)$ refer to the diffeomorphism and isometry group of the manifold $M$.","sentences":["This paper contains corrections to Madea, Rosenberg, Torres-Ardila, \"The Geometry of Loop Spaces II: Characteristic Classes,\" Advances in Math.","(287), 2016, 485-518.","The main change is that results about $\\pi_1({\\rm Diff}(M))$ are replaced by results about $\\pi_1({\\rm Isom}(M))$, where Diff$(M)$, Isom$(M)$ refer to the diffeomorphism and isometry group of the manifold $M$."],"url":"http://arxiv.org/abs/2405.00651v1","category":"math.DG"}
{"created":"2024-05-01 17:18:46","title":"Gradient-based Automatic Per-Weight Mixed Precision Quantization for Neural Networks On-Chip","abstract":"Model size and inference speed at deployment time, are major challenges in many deep learning applications. A promising strategy to overcome these challenges is quantization. However, a straightforward uniform quantization to very low precision can result in significant accuracy loss. Mixed-precision quantization, based on the idea that certain parts of the network can accommodate lower precision without compromising performance compared to other parts, offers a potential solution. In this work, we present High Granularity Quantization (HGQ), an innovative quantization-aware training method designed to fine-tune the per-weight and per-activation precision in an automatic way for ultra-low latency and low power neural networks which are to be deployed on FPGAs. We demonstrate that HGQ can outperform existing methods by a substantial margin, achieving resource reduction by up to a factor of 20 and latency improvement by a factor of 5 while preserving accuracy.","sentences":["Model size and inference speed at deployment time, are major challenges in many deep learning applications.","A promising strategy to overcome these challenges is quantization.","However, a straightforward uniform quantization to very low precision can result in significant accuracy loss.","Mixed-precision quantization, based on the idea that certain parts of the network can accommodate lower precision without compromising performance compared to other parts, offers a potential solution.","In this work, we present High Granularity Quantization (HGQ), an innovative quantization-aware training method designed to fine-tune the per-weight and per-activation precision in an automatic way for ultra-low latency and low power neural networks which are to be deployed on FPGAs.","We demonstrate that HGQ can outperform existing methods by a substantial margin, achieving resource reduction by up to a factor of 20 and latency improvement by a factor of 5 while preserving accuracy."],"url":"http://arxiv.org/abs/2405.00645v1","category":"cs.LG"}
{"created":"2024-05-01 17:08:02","title":"Stochastic fluids with transport noise: Approximating diffusion from data using SVD and ensemble forecast back-propagation","abstract":"We introduce and test methods for the calibration of the diffusion term in Stochastic Partial Differential Equations (SPDEs) describing fluids. We take two approaches, one uses ideas from the singular value decomposition and the Biot-Savart law. The other backpropagates through an ensemble forecast, with respect to diffusion parameters, to minimise a probabilistic ensemble forecasting metric. We describe the approaches in the specific context of solutions to SPDEs describing the evolution of fluid particles, sometimes called inviscid vortex methods. The methods are tested in an idealised setting in which the reference data is a known realisation of the parameterised SPDE, and also using a forecast verification metric known as the Continuous Rank Probability Score (CRPS).","sentences":["We introduce and test methods for the calibration of the diffusion term in Stochastic Partial Differential Equations (SPDEs) describing fluids.","We take two approaches, one uses ideas from the singular value decomposition and the Biot-Savart law.","The other backpropagates through an ensemble forecast, with respect to diffusion parameters, to minimise a probabilistic ensemble forecasting metric.","We describe the approaches in the specific context of solutions to SPDEs describing the evolution of fluid particles, sometimes called inviscid vortex methods.","The methods are tested in an idealised setting in which the reference data is a known realisation of the parameterised SPDE, and also using a forecast verification metric known as the Continuous Rank Probability Score (CRPS)."],"url":"http://arxiv.org/abs/2405.00640v1","category":"physics.flu-dyn"}
{"created":"2024-05-01 15:29:55","title":"Discovering robust biomarkers of neurological disorders from functional MRI using graph neural networks: A Review","abstract":"Graph neural networks (GNN) have emerged as a popular tool for modelling functional magnetic resonance imaging (fMRI) datasets. Many recent studies have reported significant improvements in disorder classification performance via more sophisticated GNN designs and highlighted salient features that could be potential biomarkers of the disorder. In this review, we provide an overview of how GNN and model explainability techniques have been applied on fMRI datasets for disorder prediction tasks, with a particular emphasis on the robustness of biomarkers produced for neurodegenerative diseases and neuropsychiatric disorders. We found that while most studies have performant models, salient features highlighted in these studies vary greatly across studies on the same disorder and little has been done to evaluate their robustness. To address these issues, we suggest establishing new standards that are based on objective evaluation metrics to determine the robustness of these potential biomarkers. We further highlight gaps in the existing literature and put together a prediction-attribution-evaluation framework that could set the foundations for future research on improving the robustness of potential biomarkers discovered via GNNs.","sentences":["Graph neural networks (GNN) have emerged as a popular tool for modelling functional magnetic resonance imaging (fMRI) datasets.","Many recent studies have reported significant improvements in disorder classification performance via more sophisticated GNN designs and highlighted salient features that could be potential biomarkers of the disorder.","In this review, we provide an overview of how GNN and model explainability techniques have been applied on fMRI datasets for disorder prediction tasks, with a particular emphasis on the robustness of biomarkers produced for neurodegenerative diseases and neuropsychiatric disorders.","We found that while most studies have performant models, salient features highlighted in these studies vary greatly across studies on the same disorder and little has been done to evaluate their robustness.","To address these issues, we suggest establishing new standards that are based on objective evaluation metrics to determine the robustness of these potential biomarkers.","We further highlight gaps in the existing literature and put together a prediction-attribution-evaluation framework that could set the foundations for future research on improving the robustness of potential biomarkers discovered via GNNs."],"url":"http://arxiv.org/abs/2405.00577v1","category":"cs.LG"}
{"created":"2024-05-01 14:01:37","title":"Einstein-Hilbert gravity, higher derivatives and a scalar matter field","abstract":"The present paper extends two previous one's on pure gravity dealing with Einstein-Hilbert and higher derivatives by including a massless scalar field as representative of matter. We study the renormalization to all orders of perturbation theory, provide the Slavnov-Taylor identity, symmetric partial differential equations and derive finiteness properties in the Landau gauge. It is shown that beginning with one-loop negative norm states originating from higher derivatives disappear.","sentences":["The present paper extends two previous one's on pure gravity dealing with Einstein-Hilbert and higher derivatives by including a massless scalar field as representative of matter.","We study the renormalization to all orders of perturbation theory, provide the Slavnov-Taylor identity, symmetric partial differential equations and derive finiteness properties in the Landau gauge.","It is shown that beginning with one-loop negative norm states originating from higher derivatives disappear."],"url":"http://arxiv.org/abs/2405.00528v1","category":"hep-th"}
{"created":"2024-05-01 13:38:03","title":"NeRF-Guided Unsupervised Learning of RGB-D Registration","abstract":"This paper focuses on training a robust RGB-D registration model without ground-truth pose supervision. Existing methods usually adopt a pairwise training strategy based on differentiable rendering, which enforces the photometric and the geometric consistency between the two registered frames as supervision. However, this frame-to-frame framework suffers from poor multi-view consistency due to factors such as lighting changes, geometry occlusion and reflective materials. In this paper, we present NeRF-UR, a novel frame-to-model optimization framework for unsupervised RGB-D registration. Instead of frame-to-frame consistency, we leverage the neural radiance field (NeRF) as a global model of the scene and use the consistency between the input and the NeRF-rerendered frames for pose optimization. This design can significantly improve the robustness in scenarios with poor multi-view consistency and provides better learning signal for the registration model. Furthermore, to bootstrap the NeRF optimization, we create a synthetic dataset, Sim-RGBD, through a photo-realistic simulator to warm up the registration model. By first training the registration model on Sim-RGBD and later unsupervisedly fine-tuning on real data, our framework enables distilling the capability of feature extraction and registration from simulation to reality. Our method outperforms the state-of-the-art counterparts on two popular indoor RGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper reproduction.","sentences":["This paper focuses on training a robust RGB-D registration model without ground-truth pose supervision.","Existing methods usually adopt a pairwise training strategy based on differentiable rendering, which enforces the photometric and the geometric consistency between the two registered frames as supervision.","However, this frame-to-frame framework suffers from poor multi-view consistency due to factors such as lighting changes, geometry occlusion and reflective materials.","In this paper, we present NeRF-UR, a novel frame-to-model optimization framework for unsupervised RGB-D registration.","Instead of frame-to-frame consistency, we leverage the neural radiance field (NeRF) as a global model of the scene and use the consistency between the input and the NeRF-rerendered frames for pose optimization.","This design can significantly improve the robustness in scenarios with poor multi-view consistency and provides better learning signal for the registration model.","Furthermore, to bootstrap the NeRF optimization, we create a synthetic dataset, Sim-RGBD, through a photo-realistic simulator to warm up the registration model.","By first training the registration model on Sim-RGBD and later unsupervisedly fine-tuning on real data, our framework enables distilling the capability of feature extraction and registration from simulation to reality.","Our method outperforms the state-of-the-art counterparts on two popular indoor RGB-D datasets, ScanNet and 3DMatch.","Code and models will be released for paper reproduction."],"url":"http://arxiv.org/abs/2405.00507v1","category":"cs.CV"}
{"created":"2024-05-01 13:20:21","title":"Pseudo-Riemannian symmetric spaces of signature (2,2)","abstract":"We study all four-dimensional simply-connected indecomposable non-semisimple pseudo-Riemannian symmetric spaces whose metric has signature (2,2). We present models and compute their isometry groups. We solve the problem of the existence or non-existence of compact quotients by properly acting discrete subgroups of the isometry group. This continues and completes earlier work by Maeta.","sentences":["We study all four-dimensional simply-connected indecomposable non-semisimple pseudo-Riemannian symmetric spaces whose metric has signature (2,2).","We present models and compute their isometry groups.","We solve the problem of the existence or non-existence of compact quotients by properly acting discrete subgroups of the isometry group.","This continues and completes earlier work by Maeta."],"url":"http://arxiv.org/abs/2405.00501v1","category":"math.DG"}
{"created":"2024-05-01 11:17:58","title":"The Maxwell evolution equation of electromagnetic resonators: a mathematical proof with explicit derivation","abstract":"In a recent publication [1], the authors employ electromagnetic quasinormal-mode theory to establish an \"exact\" Maxwell evolution (EME) equation governing resonator dynamics. This resulting equation bears resemblance to the classical evolution equation proposed heuristically in the temporal coupled-mode theory (CMT), yet it also presents differences. One significant difference is that the driving force in the EME equation is proportional to the derivative of the incident field, whereas in the CMT evolution equation, it is proportional to the incident field itself. This unexpected finding was substantiated by a numerical test and a mathematical demonstration in [1]. The test remains undisputed in this study. However, the demonstration unfortunately relies on unjustified shortcuts. We promptly highlight these shortcuts while proposing an alternative demonstration that is direct and indisputable. The new demonstration provides a firm mathematical foundation for the EME equation and reinforces its credibility.","sentences":["In a recent publication [1], the authors employ electromagnetic quasinormal-mode theory to establish an \"exact\" Maxwell evolution (EME) equation governing resonator dynamics.","This resulting equation bears resemblance to the classical evolution equation proposed heuristically in the temporal coupled-mode theory (CMT), yet it also presents differences.","One significant difference is that the driving force in the EME equation is proportional to the derivative of the incident field, whereas in the CMT evolution equation, it is proportional to the incident field itself.","This unexpected finding was substantiated by a numerical test and a mathematical demonstration in [1].","The test remains undisputed in this study.","However, the demonstration unfortunately relies on unjustified shortcuts.","We promptly highlight these shortcuts while proposing an alternative demonstration that is direct and indisputable.","The new demonstration provides a firm mathematical foundation for the EME equation and reinforces its credibility."],"url":"http://arxiv.org/abs/2405.00455v1","category":"physics.optics"}
{"created":"2024-05-01 09:58:00","title":"The Serre spectral sequence of a Lie subalgebroid","abstract":"We study a spectral sequence approximating Lie algebroid cohomology associated to a Lie subalgebroid. This is a simultaneous generalisation of several classical constructions in differential geometry, including the Leray-Serre spectral sequence for de Rham cohomology associated to a fibration, the Hochschild-Serre spectral sequence for Lie algebras, and the Mackenzie spectral sequence for Lie algebroid extensions. We show that, for wide Lie subalgebroids, the spectral sequence converges to the Lie algebroid cohomology, and that, for Lie subalgebroids over proper submanifolds, the spectral sequence converges to the formal Lie algebroid cohomology. We discuss applications and recover several constructions in Poisson geometry in which this spectral sequence has appeared naturally in the literature.","sentences":["We study a spectral sequence approximating Lie algebroid cohomology associated to a Lie subalgebroid.","This is a simultaneous generalisation of several classical constructions in differential geometry, including the Leray-Serre spectral sequence for de Rham cohomology associated to a fibration, the Hochschild-Serre spectral sequence for Lie algebras, and the Mackenzie spectral sequence for Lie algebroid extensions.","We show that, for wide Lie subalgebroids, the spectral sequence converges to the Lie algebroid cohomology, and that, for Lie subalgebroids over proper submanifolds, the spectral sequence converges to the formal Lie algebroid cohomology.","We discuss applications and recover several constructions in Poisson geometry in which this spectral sequence has appeared naturally in the literature."],"url":"http://arxiv.org/abs/2405.00419v1","category":"math.DG"}
{"created":"2024-05-01 09:50:02","title":"Ergodicity for 2D Navier-Stokes equations with a degenerate pure jump noise","abstract":"In this paper, we establish the ergodicity for stochastic 2D Navier-Stokes equations driven by a highly degenerate pure jump L\\'evy noise. The noise could appear in as few as four directions. This gives an affirmative anwser to a longstanding problem. The case of Gaussian noise was treated in Hairer and Mattingly [\\emph{Ann. of Math.}, 164(3):993--1032, 2006]. To obtain the uniqueness of invariant measure, we use Malliavin calculus and anticipating stochastic calculus to establish the equi-continuity of the semigroup, the so-called {\\em e-property}, and prove some weak irreducibility of the solution process.","sentences":["In this paper, we establish the ergodicity for stochastic 2D Navier-Stokes equations driven by a highly degenerate pure jump L\\'evy noise.","The noise could appear in as few as four directions.","This gives an affirmative anwser to a longstanding problem.","The case of Gaussian noise was treated in Hairer and Mattingly [\\emph{Ann. of Math.}, 164(3):993--1032, 2006].","To obtain the uniqueness of invariant measure, we use Malliavin calculus and anticipating stochastic calculus to establish the equi-continuity of the semigroup, the so-called {\\em e-property}, and prove some weak irreducibility of the solution process."],"url":"http://arxiv.org/abs/2405.00414v1","category":"math.PR"}
{"created":"2024-05-01 09:06:30","title":"Optimized Drug Design using Multi-Objective Evolutionary Algorithms with SELFIES","abstract":"Computer aided drug design is a promising approach to reduce the tremendous costs, i.e. time and resources, for developing new medicinal drugs. It finds application in aiding the traversal of the vast chemical space of potentially useful compounds. In this paper, we deploy multi-objective evolutionary algorithms, namely NSGA-II, NSGA-III, and MOEA/D, for this purpose. At the same time, we used the SELFIES string representation method. In addition to the QED and SA score, we optimize compounds using the GuacaMol benchmark multi-objective task sets. Our results indicate that all three algorithms show converging behavior and successfully optimize the defined criteria whilst differing mainly in the number of potential solutions found. We observe that novel and promising candidates for synthesis are discovered among obtained compounds in the Pareto-sets.","sentences":["Computer aided drug design is a promising approach to reduce the tremendous costs, i.e. time and resources, for developing new medicinal drugs.","It finds application in aiding the traversal of the vast chemical space of potentially useful compounds.","In this paper, we deploy multi-objective evolutionary algorithms, namely NSGA-II, NSGA-III, and MOEA/D, for this purpose.","At the same time, we used the SELFIES string representation method.","In addition to the QED and SA score, we optimize compounds using the GuacaMol benchmark multi-objective task sets.","Our results indicate that all three algorithms show converging behavior and successfully optimize the defined criteria whilst differing mainly in the number of potential solutions found.","We observe that novel and promising candidates for synthesis are discovered among obtained compounds in the Pareto-sets."],"url":"http://arxiv.org/abs/2405.00401v1","category":"cs.NE"}
{"created":"2024-05-01 08:25:32","title":"Modified least squares method and a review of its applications in machine learning and fractional differential/integral equations","abstract":"The least squares method provides the best-fit curve by minimizing the total squares error. In this work, we provide the modified least squares method based on the fractional orthogonal polynomials that belong to the space $M_{n}^{\\lambda} := \\text{span}\\{1,x^{\\lambda},x^{2\\lambda},\\ldots,x^{n\\lambda}\\},~\\lambda \\in (0,2]$. Numerical experiments demonstrate how to solve different problems using the modified least squares method. Moreover, the results show the advantage of the modified least squares method compared to the classical least squares method. Furthermore, we discuss the various applications of the modified least squares method in the fields like fractional differential/integral equations and machine learning.","sentences":["The least squares method provides the best-fit curve by minimizing the total squares error.","In this work, we provide the modified least squares method based on the fractional orthogonal polynomials that belong to the space $M_{n}^{\\lambda} := \\text{span}\\{1,x^{\\lambda},x^{2\\lambda},\\ldots,x^{n\\lambda}\\},~\\lambda \\in (0,2]$. Numerical experiments demonstrate how to solve different problems using the modified least squares method.","Moreover, the results show the advantage of the modified least squares method compared to the classical least squares method.","Furthermore, we discuss the various applications of the modified least squares method in the fields like fractional differential/integral equations and machine learning."],"url":"http://arxiv.org/abs/2405.00382v1","category":"math.NA"}
{"created":"2024-05-01 07:51:11","title":"Singular velocity of the Stokes and Navier-Stokes equations near boundary in the half space","abstract":"Local behaviors near boundary are analyzed for solutions of the Stokes and Navier-Stoke equations in the half space with localized non-smooth boundary data. We construct solutions of Stokes equations whose velocity field is not bounded near boundary away from the support of boundary data, although velocity and gradient velocity of solutions are locally square integrable. This is an improvement compared to known results in the sense that velocity field is unbounded itself, since previously constructed solutions were bounded near boundary, although their normal derivatives are singular. We also establish singular solutions and their derivatives that do not belong to $L^q_{\\rm{loc}}$ near boundary with $q> 1$. For such examples, there corresponding pressures turn out not to be locally integrable. Similar construction via a perturbation argument is available to the Navier-Stokes equations near boundary as well.","sentences":["Local behaviors near boundary are analyzed for solutions of the Stokes and Navier-Stoke equations in the half space with localized non-smooth boundary data.","We construct solutions of Stokes equations whose velocity field is not bounded near boundary away from the support of boundary data, although velocity and gradient velocity of solutions are locally square integrable.","This is an improvement compared to known results in the sense that velocity field is unbounded itself, since previously constructed solutions were bounded near boundary, although their normal derivatives are singular.","We also establish singular solutions and their derivatives that do not belong to $L^q_{\\rm{loc}}$ near boundary with $q> 1$. For such examples, there corresponding pressures turn out not to be locally integrable.","Similar construction via a perturbation argument is available to the Navier-Stokes equations near boundary as well."],"url":"http://arxiv.org/abs/2405.00369v1","category":"math.AP"}
{"created":"2024-05-01 06:55:58","title":"Spectrum of Hybrid Charmonium, Bottomonium and $B_c$ Mesons by Power Series Method","abstract":"Power series method (PSM) is revisited to find the masses of S, P, and D states of conventional charmonium ($c\\overline{c}$), bottomonium ($b\\overline{b}$), and $B_c$ ($\\overline{b}c$) mesons by assuming the solution of N-dimensional radial Schrodinger equation in series form. An extension in the potential model is proposed by fitting it with lattice data to study the hybrid mesons. The proposed potential model is used to find the masses of hybrid $c\\overline{c}$, $b\\overline{b}$, and $\\overline{b}c$ mesons by applying the power series method. Calculated results are compared with theoretical findings and available experimental data. Our results can be helpful for the investigation of newly experimentally discovered charmonium, bottomonium, and $B_c$ states.","sentences":["Power series method (PSM) is revisited to find the masses of S, P, and D states of conventional charmonium ($c\\overline{c}$), bottomonium ($b\\overline{b}$), and $B_c$ ($\\overline{b}c$) mesons by assuming the solution of N-dimensional radial Schrodinger equation in series form.","An extension in the potential model is proposed by fitting it with lattice data to study the hybrid mesons.","The proposed potential model is used to find the masses of hybrid $c\\overline{c}$, $b\\overline{b}$, and $\\overline{b}c$ mesons by applying the power series method.","Calculated results are compared with theoretical findings and available experimental data.","Our results can be helpful for the investigation of newly experimentally discovered charmonium, bottomonium, and $B_c$ states."],"url":"http://arxiv.org/abs/2405.00350v1","category":"hep-ph"}
{"created":"2024-05-01 06:37:55","title":"Multi-task Learning-based Joint CSI Prediction and Predictive Transmitter Selection for Security","abstract":"In mobile communication scenarios, the acquired channel state information (CSI) rapidly becomes outdated due to fast-changing channels. Opportunistic transmitter selection based on current CSI for secrecy improvement may be outdated during actual transmission, negating the diversity benefit of transmitter selection. Motivated by this problem, we propose a joint CSI prediction and predictive selection of the optimal transmitter strategy based on historical CSI by exploiting the temporal correlation among CSIs. The proposed solution utilizes the multi-task learning (MTL) framework by employing a single Long Short-Term Memory (LSTM) network architecture that simultaneously learns two tasks of predicting the CSI and selecting the optimal transmitter in parallel instead of learning these tasks sequentially. The proposed LSTM architecture outperforms convolutional neural network (CNN) based architecture due to its superior ability to capture temporal features in the data. Compared to the sequential task learning models, the MTL architecture provides superior predicted secrecy performance for a large variation in the number of transmitters and the speed of mobile nodes. It also offers significant computational and memory efficiency, leading to a substantial saving in computational time by around 40 percent.","sentences":["In mobile communication scenarios, the acquired channel state information (CSI) rapidly becomes outdated due to fast-changing channels.","Opportunistic transmitter selection based on current CSI for secrecy improvement may be outdated during actual transmission, negating the diversity benefit of transmitter selection.","Motivated by this problem, we propose a joint CSI prediction and predictive selection of the optimal transmitter strategy based on historical CSI by exploiting the temporal correlation among CSIs.","The proposed solution utilizes the multi-task learning (MTL) framework by employing a single Long Short-Term Memory (LSTM) network architecture that simultaneously learns two tasks of predicting the CSI and selecting the optimal transmitter in parallel instead of learning these tasks sequentially.","The proposed LSTM architecture outperforms convolutional neural network (CNN) based architecture due to its superior ability to capture temporal features in the data.","Compared to the sequential task learning models, the MTL architecture provides superior predicted secrecy performance for a large variation in the number of transmitters and the speed of mobile nodes.","It also offers significant computational and memory efficiency, leading to a substantial saving in computational time by around 40 percent."],"url":"http://arxiv.org/abs/2405.00345v1","category":"eess.SP"}
{"created":"2024-05-01 06:02:28","title":"Comparison of Ion-Proton Differential Speed between ICMEs and Solar Wind near 1 au","abstract":"The elemental abundance of ICMEs and solar wind near 1 au is often adopted to represent the abundance in the corresponding coronal sources. However, the absolute abundance of heavy ions (relative to hydrogen) near 1 au might be different from the coronal abundance due to the ion-proton differential speed ($V_{ip}$). To illustrate the $V_{ip}$ characteristics and explore whether it influences the absolute abundance analysis for ICMEs and solar wind, we perform a statistical study on the $V_{ip}$ for He$^{2+}$, C$^{5+}$, O$^{6+}$, and Fe$^{10+}$ in both ICMEs and solar wind based on measurements of Advanced Composition Explorer. The results show that the $V_{ip}$ is negligible within ICMEs and slow solar wind ($<$ 400 km s$^{-1}$), while obvious in the intermediate (400 -- 600 km s$^{-1}$) and fast wind ($>$ 600 km s$^{-1}$). Previous studies showed that the $V_{ip}$ in ICMEs keeps negligible during propagation from 0.3 to 5 au, but in solar wind it increases with the decreasing heliocentric distance. Therefore, it might be questionable to infer the absolute abundance of coronal sources through in-situ abundance near 1 au for solar wind. Fortunately, the ion-oxygen (O$^{6+}$) differential speed ($V_{io}$) is negligible for He$^{2+}$, C$^{5+}$, and Fe$^{10+}$ within both ICMEs and solar wind, and previous studies suggested that the $V_{io}$ does not vary significantly with the heliocentric distance. This indicates that various heavy ions always flow at the same bulk speed and their relative abundance (relative to oxygen) near 1 au can represent the coronal abundance for both ICMEs and solar wind.","sentences":["The elemental abundance of ICMEs and solar wind near 1 au is often adopted to represent the abundance in the corresponding coronal sources.","However, the absolute abundance of heavy ions (relative to hydrogen) near 1 au might be different from the coronal abundance due to the ion-proton differential speed ($V_{ip}$).","To illustrate the $V_{ip}$ characteristics and explore whether it influences the absolute abundance analysis for ICMEs and solar wind, we perform a statistical study on the $V_{ip}$ for He$^{2+}$, C$^{5+}$, O$^{6+}$, and Fe$^{10+}$ in both ICMEs and solar wind based on measurements of Advanced Composition Explorer.","The results show that the $V_{ip}$ is negligible within ICMEs and slow solar wind ($<$ 400 km s$^{-1}$), while obvious in the intermediate (400 -- 600 km s$^{-1}$) and fast wind ($>$ 600 km s$^{-1}$).","Previous studies showed that the $V_{ip}$ in ICMEs keeps negligible during propagation from 0.3 to 5 au, but in solar wind it increases with the decreasing heliocentric distance.","Therefore, it might be questionable to infer the absolute abundance of coronal sources through in-situ abundance near 1 au for solar wind.","Fortunately, the ion-oxygen (O$^{6+}$) differential speed ($V_{io}$) is negligible for He$^{2+}$, C$^{5+}$, and Fe$^{10+}$ within both ICMEs and solar wind, and previous studies suggested that the $V_{io}$ does not vary significantly with the heliocentric distance.","This indicates that various heavy ions always flow at the same bulk speed and their relative abundance (relative to oxygen) near 1 au can represent the coronal abundance for both ICMEs and solar wind."],"url":"http://arxiv.org/abs/2405.00336v1","category":"astro-ph.SR"}
{"created":"2024-05-01 04:28:25","title":"Study of proton-proton Scattering using Phase Function Method","abstract":"Background: The study of np and pp scattering, central to understanding nuclear force, remains an optional topic in many undergraduate nuclear physics curriculum. Purpose: The main thrust of this paper is to study pp scattering using the phase function method to obtain the observed S-wave phase shifts and cross-sections at various energies. Methods: The pp interaction has been modeled by choosing the Malfliet-Tjon potential for the nuclear part along with the screened Coulomb potential. The phase equation has been solved to obtain scattering phase shifts using the fourth-order RK method (RK-4). Results: The interaction potential obtained from optimized parameters matches well with the realistic Argonne V18 potential for 1S0 state of pp scattering and the scattering phase shifts as well as the cross-section for energies ranging from 1-350 MeV are in good agreement with expected data. Conclusion: Introducing the phase function method for S-wave (l=0) could bring this interesting study of nucleon-nucleon scattering to the undergraduate classroom.","sentences":["Background: The study of np and pp scattering, central to understanding nuclear force, remains an optional topic in many undergraduate nuclear physics curriculum.","Purpose: The main thrust of this paper is to study pp scattering using the phase function method to obtain the observed S-wave phase shifts and cross-sections at various energies.","Methods: The pp interaction has been modeled by choosing the Malfliet-Tjon potential for the nuclear part along with the screened Coulomb potential.","The phase equation has been solved to obtain scattering phase shifts using the fourth-order RK method (RK-4).","Results:","The interaction potential obtained from optimized parameters matches well with the realistic Argonne V18 potential for 1S0 state of pp scattering and the scattering phase shifts as well as the cross-section for energies ranging from 1-350 MeV are in good agreement with expected data.","Conclusion: Introducing the phase function method for S-wave (l=0) could bring this interesting study of nucleon-nucleon scattering to the undergraduate classroom."],"url":"http://arxiv.org/abs/2405.00310v1","category":"nucl-th"}
{"created":"2024-05-01 02:55:15","title":"Message-Passing Interatomic Potentials Learn Non-Local Electrostatic Interactions","abstract":"This work demonstrates that E(3)-equivariant graph neural network interatomic potentials (GNN-IPs) effectively learn non-local electrostatic interactions and charge transfers. Using a toy model with point charges, it is confirmed that GNN-IPs adeptly interpolate and extrapolate electrostatic interactions, independent of geometric coupling. It is also found that the electrostatic energy between an atom pair is distributed among neighboring atoms, rather than localized solely within the two atoms involved. Furthermore, GNN-IPs have proven successful in capturing non-local interactions through charge transfer on density functional theory reference data, as evidenced by comparative analyses of the potential energy surfaces for Au clusters on both MgO and Al-doped MgO substrates.","sentences":["This work demonstrates that E(3)-equivariant graph neural network interatomic potentials (GNN-IPs) effectively learn non-local electrostatic interactions and charge transfers.","Using a toy model with point charges, it is confirmed that GNN-IPs adeptly interpolate and extrapolate electrostatic interactions, independent of geometric coupling.","It is also found that the electrostatic energy between an atom pair is distributed among neighboring atoms, rather than localized solely within the two atoms involved.","Furthermore, GNN-IPs have proven successful in capturing non-local interactions through charge transfer on density functional theory reference data, as evidenced by comparative analyses of the potential energy surfaces for Au clusters on both MgO and Al-doped MgO substrates."],"url":"http://arxiv.org/abs/2405.00290v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-01 01:28:37","title":"Base change and Iwasawa Main Conjectures for ${\\rm GL}_2$","abstract":"Let $E/\\mathbb{Q}$ be an elliptic curve of conductor $N$, $p$ an odd prime of good ordinary reduction such that $E[p]$ is an irreducible $G_{\\mathbb{Q}}$-module, and $K$ an imaginary quadratic field with all primes dividing $Np$ split. We prove Iwasawa Main Conjectures for the $\\mathbb{Z}_p$-cyclotomic and $\\mathbb{Z}_p$-anticyclotomic deformation of $E$ over $\\mathbb{Q}$ and $K$ respectively, dispensing with any of the ramification hypotheses on $E[p]$ in previous works. Using base change, the proofs are based on Wan's divisibility towards a three-variable main conjecture for $E$ over a quartic CM field containing $K$.   As an application, we prove cases of the two-variable main conjecture for $E$ over $K$. The one-variable main conjectures imply the $p$-part of the conjectural Birch and Swinnerton-Dyer formula if ${\\rm ord}_{s=1}L(E,s)\\leq 1$. They are also an ingredient in the proof of Kolyvagin's conjecture and its cyclotomic variant in our joint work with Grossi \\cite{BCGS}.","sentences":["Let $E/\\mathbb{Q}$ be an elliptic curve of conductor $N$, $p$ an odd prime of good ordinary reduction such that $E[p]$ is an irreducible $G_{\\mathbb{Q}}$-module, and $K$ an imaginary quadratic field with all primes dividing $Np$ split.","We prove Iwasawa Main Conjectures for the $\\mathbb{Z}_p$-cyclotomic and $\\mathbb{Z}_p$-anticyclotomic deformation of $E$ over $\\mathbb{Q}$ and $K$ respectively, dispensing with any of the ramification hypotheses on $E[p]$ in previous works.","Using base change, the proofs are based on Wan's divisibility towards a three-variable main conjecture for $E$ over a quartic CM field containing $K$.   As an application, we prove cases of the two-variable main conjecture for $E$ over $K$. The one-variable main conjectures imply the $p$-part of the conjectural Birch and Swinnerton-Dyer formula if ${\\rm ord}_{s=1}L(E,s)\\leq 1$.","They are also an ingredient in the proof of Kolyvagin's conjecture and its cyclotomic variant in our joint work with Grossi \\cite{BCGS}."],"url":"http://arxiv.org/abs/2405.00270v1","category":"math.NT"}
{"created":"2024-04-30 21:22:54","title":"Superluminal matter waves","abstract":"The Dirac equation has resided among the greatest successes of modern physics since its emergence as the first quantum mechanical theory fully compatible with special relativity. This compatibility ensures that the expectation value of the velocity is less than the vacuum speed of light. Here, we show that the Dirac equation admits free-particle solutions where the peak amplitude of the wavefunction can travel at any velocity, including those exceeding the vacuum speed of light, despite having a subluminal velocity expectation value. The solutions are constructed by superposing basis functions with correlations in momentum space. These arbitrary velocity wavefunctions feature a near-constant profile and may impact quantum mechanical processes that are sensitive to the local value of the probability density as opposed to expectation values.","sentences":["The Dirac equation has resided among the greatest successes of modern physics since its emergence as the first quantum mechanical theory fully compatible with special relativity.","This compatibility ensures that the expectation value of the velocity is less than the vacuum speed of light.","Here, we show that the Dirac equation admits free-particle solutions where the peak amplitude of the wavefunction can travel at any velocity, including those exceeding the vacuum speed of light, despite having a subluminal velocity expectation value.","The solutions are constructed by superposing basis functions with correlations in momentum space.","These arbitrary velocity wavefunctions feature a near-constant profile and may impact quantum mechanical processes that are sensitive to the local value of the probability density as opposed to expectation values."],"url":"http://arxiv.org/abs/2405.00209v1","category":"quant-ph"}
{"created":"2024-04-30 21:11:50","title":"A simple truth hidden in plain sight: All molecules are entangled according to chemical common sense","abstract":"An equation that determines the numbers of electrons for molecules is proposed based on chemical common sense. It shows that all molecules are entangled in number of electrons and results in the fundamental assumption of molecular energy convexity that underpins molecular quantum mechanics. It also leads to the concept of fractional numbers of electrons for molecules in a statistical sense. The energy of a molecule is piecewise linear with respect to its continuous number of electrons. Wavefunction interpretation of this equation of nature shows that an individual molecule with noninteger number of electrons is locally physical, whereas a molecule with integer number of electrons is locally real. The complete theoretical proof of the equation is still to be had.","sentences":["An equation that determines the numbers of electrons for molecules is proposed based on chemical common sense.","It shows that all molecules are entangled in number of electrons and results in the fundamental assumption of molecular energy convexity that underpins molecular quantum mechanics.","It also leads to the concept of fractional numbers of electrons for molecules in a statistical sense.","The energy of a molecule is piecewise linear with respect to its continuous number of electrons.","Wavefunction interpretation of this equation of nature shows that an individual molecule with noninteger number of electrons is locally physical, whereas a molecule with integer number of electrons is locally real.","The complete theoretical proof of the equation is still to be had."],"url":"http://arxiv.org/abs/2405.00203v1","category":"physics.chem-ph"}
{"created":"2024-04-30 19:01:29","title":"Homological dimensions, the Gorenstein property, and special cases of some conjectures","abstract":"Our purpose in this work is multifold. First, we provide general criteria for the finiteness of the projective and injective dimensions of a finite module $M$ over a (commutative) Noetherian ring $R$. Second, in the other direction, we investigate the impact of the finiteness of certain homological dimensions of $M$ if $R$ is local, mainly when $R$ is Cohen-Macaulay and with a partial focus on duals. Along the way, we produce various freeness criteria for modules. Finally, we give applications, including characterizations of when $R$ is Gorenstein (and other ring-theoretic properties as well, sometimes in the prime characteristic setting), particularly by means of its anticanonical module, and in addition we address special cases of some long-standing conjectures; for instance, we confirm the 1985 conjecture of Vasconcelos on normal modules in case the module of differentials is almost Cohen-Macaulay.","sentences":["Our purpose in this work is multifold.","First, we provide general criteria for the finiteness of the projective and injective dimensions of a finite module $M$ over a (commutative) Noetherian ring $R$. Second, in the other direction, we investigate the impact of the finiteness of certain homological dimensions of $M$ if $R$ is local, mainly when $R$ is Cohen-Macaulay and with a partial focus on duals.","Along the way, we produce various freeness criteria for modules.","Finally, we give applications, including characterizations of when $R$ is Gorenstein (and other ring-theoretic properties as well, sometimes in the prime characteristic setting), particularly by means of its anticanonical module, and in addition we address special cases of some long-standing conjectures; for instance, we confirm the 1985 conjecture of Vasconcelos on normal modules in case the module of differentials is almost Cohen-Macaulay."],"url":"http://arxiv.org/abs/2405.00152v1","category":"math.AC"}
{"created":"2024-04-30 18:54:03","title":"Gravitational Stress Tensor and Current at Null Infinity in Three Dimensions","abstract":"We develop the framework that reveals the intrinsic conserved stress tensor and current associated with the null infinity of a three-dimensional ($3d$) asymptotically flat spacetime. These are, respectively, canonical conjugates of degenerate metric and Ehresmann connection of the boundary Carrollian geometry. Their conservation reproduces the Bondi-mass and angular momentum conservation equations if the asymptotic boundary is endowed with a torsional affine connection that we specify. Our analysis and results shed further light on the $3d$ flat holography; the stress tensor and current give rise to an asymptotically flat fluid/gravity correspondence. The requirement of a well-defined $3d$ action principle yields Schwarzian action at null infinity governing the dynamics induced by reparametrizations over the celestial circle, in accord with the codimension $2$ holography of $3d$ flat spacetimes.","sentences":["We develop the framework that reveals the intrinsic conserved stress tensor and current associated with the null infinity of a three-dimensional ($3d$) asymptotically flat spacetime.","These are, respectively, canonical conjugates of degenerate metric and Ehresmann connection of the boundary Carrollian geometry.","Their conservation reproduces the Bondi-mass and angular momentum conservation equations if the asymptotic boundary is endowed with a torsional affine connection that we specify.","Our analysis and results shed further light on the $3d$ flat holography; the stress tensor and current give rise to an asymptotically flat fluid/gravity correspondence.","The requirement of a well-defined $3d$ action principle yields Schwarzian action at null infinity governing the dynamics induced by reparametrizations over the celestial circle, in accord with the codimension $2$ holography of $3d$ flat spacetimes."],"url":"http://arxiv.org/abs/2405.00149v1","category":"hep-th"}
{"created":"2024-04-30 18:28:09","title":"A Flexible 2.5D Medical Image Segmentation Approach with In-Slice and Cross-Slice Attention","abstract":"Deep learning has become the de facto method for medical image segmentation, with 3D segmentation models excelling in capturing complex 3D structures and 2D models offering high computational efficiency. However, segmenting 2.5D images, which have high in-plane but low through-plane resolution, is a relatively unexplored challenge. While applying 2D models to individual slices of a 2.5D image is feasible, it fails to capture the spatial relationships between slices. On the other hand, 3D models face challenges such as resolution inconsistencies in 2.5D images, along with computational complexity and susceptibility to overfitting when trained with limited data. In this context, 2.5D models, which capture inter-slice correlations using only 2D neural networks, emerge as a promising solution due to their reduced computational demand and simplicity in implementation. In this paper, we introduce CSA-Net, a flexible 2.5D segmentation model capable of processing 2.5D images with an arbitrary number of slices through an innovative Cross-Slice Attention (CSA) module. This module uses the cross-slice attention mechanism to effectively capture 3D spatial information by learning long-range dependencies between the center slice (for segmentation) and its neighboring slices. Moreover, CSA-Net utilizes the self-attention mechanism to understand correlations among pixels within the center slice. We evaluated CSA-Net on three 2.5D segmentation tasks: (1) multi-class brain MRI segmentation, (2) binary prostate MRI segmentation, and (3) multi-class prostate MRI segmentation. CSA-Net outperformed leading 2D and 2.5D segmentation methods across all three tasks, demonstrating its efficacy and superiority. Our code is publicly available at https://github.com/mirthAI/CSA-Net.","sentences":["Deep learning has become the de facto method for medical image segmentation, with 3D segmentation models excelling in capturing complex 3D structures and 2D models offering high computational efficiency.","However, segmenting 2.5D images, which have high in-plane but low through-plane resolution, is a relatively unexplored challenge.","While applying 2D models to individual slices of a 2.5D image is feasible, it fails to capture the spatial relationships between slices.","On the other hand, 3D models face challenges such as resolution inconsistencies in 2.5D images, along with computational complexity and susceptibility to overfitting when trained with limited data.","In this context, 2.5D models, which capture inter-slice correlations using only 2D neural networks, emerge as a promising solution due to their reduced computational demand and simplicity in implementation.","In this paper, we introduce CSA-Net, a flexible 2.5D segmentation model capable of processing 2.5D images with an arbitrary number of slices through an innovative Cross-Slice Attention (CSA) module.","This module uses the cross-slice attention mechanism to effectively capture 3D spatial information by learning long-range dependencies between the center slice (for segmentation) and its neighboring slices.","Moreover, CSA-Net utilizes the self-attention mechanism to understand correlations among pixels within the center slice.","We evaluated CSA-Net on three 2.5D segmentation tasks: (1) multi-class brain MRI segmentation, (2) binary prostate MRI segmentation, and (3) multi-class prostate MRI segmentation.","CSA-Net outperformed leading 2D and 2.5D segmentation methods across all three tasks, demonstrating its efficacy and superiority.","Our code is publicly available at https://github.com/mirthAI/CSA-Net."],"url":"http://arxiv.org/abs/2405.00130v1","category":"eess.IV"}
{"created":"2024-04-30 18:19:44","title":"Neural network based emulation of galaxy power spectrum covariances -- A reanalysis of BOSS DR12 data","abstract":"We train neural networks to quickly generate redshift-space galaxy power spectrum covariances from a given parameter set (cosmology and galaxy bias). This covariance emulator utilizes a combination of traditional fully-connected network layers and transformer architecture to accurately predict covariance matrices for the high redshift, north galactic cap sample of the BOSS DR12 galaxy catalog. We run simulated likelihood analyses with emulated and brute-force computed covariances, and we quantify the network's performance via two different metrics: 1) difference in $\\chi^2$ and 2) likelihood contours for simulated BOSS DR 12 analyses. We find that the emulator returns excellent results over a large parameter range. We then use our emulator to perform a re-analysis of the BOSS HighZ NGC galaxy power spectrum, and find that varying covariance with cosmology along with the model vector produces $\\Omega_m = 0.276^{+0.013}_{-0.015}$, $H_0 = 70.2\\pm 1.9$ km/s/Mpc, and $\\sigma_8 = 0.674^{+0.058}_{-0.077}$. These constraints represent an average $0.46\\sigma$ shift in best-fit values and a $5\\%$ increase in constraining power compared to fixing the covariance matrix ($\\Omega_m = 0.293\\pm 0.017$, $H_0 = 70.3\\pm 2.0$ km/s/Mpc, $\\sigma_8 = 0.702^{+0.063}_{-0.075}$). This work demonstrates that emulators for more complex cosmological quantities than second-order statistics can be trained over a wide parameter range at sufficiently high accuracy to be implemented in realistic likelihood analyses.","sentences":["We train neural networks to quickly generate redshift-space galaxy power spectrum covariances from a given parameter set (cosmology and galaxy bias).","This covariance emulator utilizes a combination of traditional fully-connected network layers and transformer architecture to accurately predict covariance matrices for the high redshift, north galactic cap sample of the BOSS DR12 galaxy catalog.","We run simulated likelihood analyses with emulated and brute-force computed covariances, and we quantify the network's performance via two different metrics: 1) difference in $\\chi^2$ and 2) likelihood contours for simulated BOSS DR 12 analyses.","We find that the emulator returns excellent results over a large parameter range.","We then use our emulator to perform a re-analysis of the BOSS HighZ NGC galaxy power spectrum, and find that varying covariance with cosmology along with the model vector produces $\\Omega_m = 0.276^{+0.013}_{-0.015}$, $H_0 = 70.2\\pm 1.9$ km/s/Mpc, and $\\sigma_8 = 0.674^{+0.058}_{-0.077}$.","These constraints represent an average $0.46\\sigma$ shift in best-fit values and a $5\\%$ increase in constraining power compared to fixing the covariance matrix ($\\Omega_m = 0.293\\pm 0.017$, $H_0 = 70.3\\pm 2.0$ km/s/Mpc, $\\sigma_8 = 0.702^{+0.063}_{-0.075}$).","This work demonstrates that emulators for more complex cosmological quantities than second-order statistics can be trained over a wide parameter range at sufficiently high accuracy to be implemented in realistic likelihood analyses."],"url":"http://arxiv.org/abs/2405.00125v1","category":"astro-ph.CO"}
{"created":"2024-04-30 18:17:44","title":"Graph Neural Network Approach to Semantic Type Detection in Tables","abstract":"This study addresses the challenge of detecting semantic column types in relational tables, a key task in many real-world applications. While language models like BERT have improved prediction accuracy, their token input constraints limit the simultaneous processing of intra-table and inter-table information. We propose a novel approach using Graph Neural Networks (GNNs) to model intra-table dependencies, allowing language models to focus on inter-table information. Our proposed method not only outperforms existing state-of-the-art algorithms but also offers novel insights into the utility and functionality of various GNN types for semantic type detection. The code is available at https://github.com/hoseinzadeehsan/GAIT","sentences":["This study addresses the challenge of detecting semantic column types in relational tables, a key task in many real-world applications.","While language models like BERT have improved prediction accuracy, their token input constraints limit the simultaneous processing of intra-table and inter-table information.","We propose a novel approach using Graph Neural Networks (GNNs) to model intra-table dependencies, allowing language models to focus on inter-table information.","Our proposed method not only outperforms existing state-of-the-art algorithms but also offers novel insights into the utility and functionality of various GNN types for semantic type detection.","The code is available at https://github.com/hoseinzadeehsan/GAIT"],"url":"http://arxiv.org/abs/2405.00123v1","category":"cs.LG"}
{"created":"2024-04-30 18:04:20","title":"Alternate Computation of Gravitational Effects from a Single Loop of Inflationary Scalars","abstract":"We present a new computation of the renormalized graviton self-energy induced by a loop of massless, minimally coupled scalars on de Sitter background. Our result takes account of the need to include a finite renormalization of the cosmological constant, which was not included in the first analysis. We also avoid preconceptions concerning structure functions and instead express the result as a linear combination of 21 tensor differential operators. By using our result to quantum-correct the linearized effective field equation we derive logarithmic corrections to both the electric components of the Weyl tensor for gravitational radiation and to the two potentials which quantify the gravitational response to a static point mass.","sentences":["We present a new computation of the renormalized graviton self-energy induced by a loop of massless, minimally coupled scalars on de Sitter background.","Our result takes account of the need to include a finite renormalization of the cosmological constant, which was not included in the first analysis.","We also avoid preconceptions concerning structure functions and instead express the result as a linear combination of 21 tensor differential operators.","By using our result to quantum-correct the linearized effective field equation we derive logarithmic corrections to both the electric components of the Weyl tensor for gravitational radiation and to the two potentials which quantify the gravitational response to a static point mass."],"url":"http://arxiv.org/abs/2405.00116v1","category":"gr-qc"}
{"created":"2024-04-30 18:00:05","title":"Astronomy's climate emissions: Global travel to scientific meetings in 2019","abstract":"Travel to academic conferences -- where international flights are the norm -- is responsible for a sizeable fraction of the greenhouse gas (GHG) emissions associated with academic work. In order to provide a benchmark for comparison with other fields, as well as for future reduction strategies and assessments, we estimate the CO2-equivalent emissions for conference travel in the field of astronomy for the prepandemic year 2019. The GHG emission of the international astronomical community's 362 conferences and schools in 2019 amounted to 42,500 tCO2e, assuming a radiative-forcing index factor of 1.95 for air travel. This equates to an average of 1.0 $\\pm$ 0.6 tCO2e per participant per meeting. The total travel distance adds up to roughly 1.5 Astronomical Units, that is, 1.5 times the distance between the Earth and the Sun. We present scenarios for the reduction of this value, for instance with virtual conferencing or hub models, while still prioritizing the benefits conferences bring to the scientific community.","sentences":["Travel to academic conferences -- where international flights are the norm -- is responsible for a sizeable fraction of the greenhouse gas (GHG) emissions associated with academic work.","In order to provide a benchmark for comparison with other fields, as well as for future reduction strategies and assessments, we estimate the CO2-equivalent emissions for conference travel in the field of astronomy for the prepandemic year 2019.","The GHG emission of the international astronomical community's 362 conferences and schools in 2019 amounted to 42,500 tCO2e, assuming a radiative-forcing index factor of 1.95 for air travel.","This equates to an average of 1.0 $\\pm$ 0.6 tCO2e per participant per meeting.","The total travel distance adds up to roughly 1.5 Astronomical Units, that is, 1.5 times the distance between the Earth and the Sun.","We present scenarios for the reduction of this value, for instance with virtual conferencing or hub models, while still prioritizing the benefits conferences bring to the scientific community."],"url":"http://arxiv.org/abs/2405.00104v1","category":"physics.soc-ph"}
{"created":"2024-04-30 10:53:30","title":"BrainODE: Dynamic Brain Signal Analysis via Graph-Aided Neural Ordinary Differential Equations","abstract":"Brain network analysis is vital for understanding the neural interactions regarding brain structures and functions, and identifying potential biomarkers for clinical phenotypes. However, widely used brain signals such as Blood Oxygen Level Dependent (BOLD) time series generated from functional Magnetic Resonance Imaging (fMRI) often manifest three challenges: (1) missing values, (2) irregular samples, and (3) sampling misalignment, due to instrumental limitations, impacting downstream brain network analysis and clinical outcome predictions. In this work, we propose a novel model called BrainODE to achieve continuous modeling of dynamic brain signals using Ordinary Differential Equations (ODE). By learning latent initial values and neural ODE functions from irregular time series, BrainODE effectively reconstructs brain signals at any time point, mitigating the aforementioned three data challenges of brain signals altogether. Comprehensive experimental results on real-world neuroimaging datasets demonstrate the superior performance of BrainODE and its capability of addressing the three data challenges.","sentences":["Brain network analysis is vital for understanding the neural interactions regarding brain structures and functions, and identifying potential biomarkers for clinical phenotypes.","However, widely used brain signals such as Blood Oxygen Level Dependent (BOLD) time series generated from functional Magnetic Resonance Imaging (fMRI) often manifest three challenges: (1) missing values, (2) irregular samples, and (3) sampling misalignment, due to instrumental limitations, impacting downstream brain network analysis and clinical outcome predictions.","In this work, we propose a novel model called BrainODE to achieve continuous modeling of dynamic brain signals using Ordinary Differential Equations (ODE).","By learning latent initial values and neural ODE functions from irregular time series, BrainODE effectively reconstructs brain signals at any time point, mitigating the aforementioned three data challenges of brain signals altogether.","Comprehensive experimental results on real-world neuroimaging datasets demonstrate the superior performance of BrainODE and its capability of addressing the three data challenges."],"url":"http://arxiv.org/abs/2405.00077v1","category":"cs.LG"}
{"created":"2024-04-30 07:24:41","title":"PAODING: A High-fidelity Data-free Pruning Toolkit for Debloating Pre-trained Neural Networks","abstract":"We present PAODING, a toolkit to debloat pretrained neural network models through the lens of data-free pruning. To preserve the model fidelity, PAODING adopts an iterative process, which dynamically measures the effect of deleting a neuron to identify candidates that have the least impact to the output layer. Our evaluation shows that PAODING can significantly reduce the model size, generalize on different datasets and models, and meanwhile preserve the model fidelity in terms of test accuracy and adversarial robustness. PAODING is publicly available on PyPI via https://pypi.org/project/paoding-dl.","sentences":["We present PAODING, a toolkit to debloat pretrained neural network models through the lens of data-free pruning.","To preserve the model fidelity, PAODING adopts an iterative process, which dynamically measures the effect of deleting a neuron to identify candidates that have the least impact to the output layer.","Our evaluation shows that PAODING can significantly reduce the model size, generalize on different datasets and models, and meanwhile preserve the model fidelity in terms of test accuracy and adversarial robustness.","PAODING is publicly available on PyPI via https://pypi.org/project/paoding-dl."],"url":"http://arxiv.org/abs/2405.00074v1","category":"cs.LG"}
{"created":"2024-05-01 17:04:20","title":"Robustness of graph embedding methods for community detection","abstract":"This study investigates the robustness of graph embedding methods for community detection in the face of network perturbations, specifically edge deletions. Graph embedding techniques, which represent nodes as low-dimensional vectors, are widely used for various graph machine learning tasks due to their ability to capture structural properties of networks effectively. However, the impact of perturbations on the performance of these methods remains relatively understudied. The research considers state-of-the-art graph embedding methods from two families: matrix factorization (e.g., LE, LLE, HOPE, M-NMF) and random walk-based (e.g., DeepWalk, LINE, node2vec). Through experiments conducted on both synthetic and real-world networks, the study reveals varying degrees of robustness within each family of graph embedding methods. The robustness is found to be influenced by factors such as network size, initial community partition strength, and the type of perturbation. Notably, node2vec and LLE consistently demonstrate higher robustness for community detection across different scenarios, including networks with degree and community size heterogeneity. These findings highlight the importance of selecting an appropriate graph embedding method based on the specific characteristics of the network and the task at hand, particularly in scenarios where robustness to perturbations is crucial.","sentences":["This study investigates the robustness of graph embedding methods for community detection in the face of network perturbations, specifically edge deletions.","Graph embedding techniques, which represent nodes as low-dimensional vectors, are widely used for various graph machine learning tasks due to their ability to capture structural properties of networks effectively.","However, the impact of perturbations on the performance of these methods remains relatively understudied.","The research considers state-of-the-art graph embedding methods from two families: matrix factorization (e.g., LE, LLE, HOPE, M-NMF) and random walk-based (e.g., DeepWalk, LINE, node2vec).","Through experiments conducted on both synthetic and real-world networks, the study reveals varying degrees of robustness within each family of graph embedding methods.","The robustness is found to be influenced by factors such as network size, initial community partition strength, and the type of perturbation.","Notably, node2vec and LLE consistently demonstrate higher robustness for community detection across different scenarios, including networks with degree and community size heterogeneity.","These findings highlight the importance of selecting an appropriate graph embedding method based on the specific characteristics of the network and the task at hand, particularly in scenarios where robustness to perturbations is crucial."],"url":"http://arxiv.org/abs/2405.00636v1","category":"physics.soc-ph"}
{"created":"2024-05-01 15:32:22","title":"LEAP: Optimization Hierarchical Federated Learning on Non-IID Data with Coalition Formation Game","abstract":"Although Hierarchical Federated Learning (HFL) utilizes edge servers (ESs) to alleviate communication burdens, its model performance will be degraded by non-IID data and limited communication resources. Current works often assume that data is uniformly distributed, which however contradicts the heterogeneity of IoT. Solutions of additional model training to check the data distribution inevitably increases computational costs and the risk of privacy leakage. The challenges in solving these issues are how to reduce the impact of non-IID data without involving raw data and how to rationalize the communication resource allocation for addressing straggler problem. To tackle these challenges, we propose a novel optimization method based on coaLition formation gamE and grAdient Projection, called LEAP. Specifically, we combine edge data distribution with coalition formation game innovatively to adjust the correlations between clients and ESs dynamically, which ensures optimal correlations. We further capture the client heterogeneity to achieve the rational bandwidth allocation from coalition perception and determine the optimal transmission power within specified delay constraints at client level. Experimental results on four real datasets show that LEAP is able to achieve 20.62% improvement in model accuracy compared to the state-of-the-art baselines. Moreover, LEAP effectively reduce transmission energy consumption by at least about 2.24 times.","sentences":["Although Hierarchical Federated Learning (HFL) utilizes edge servers (ESs) to alleviate communication burdens, its model performance will be degraded by non-IID data and limited communication resources.","Current works often assume that data is uniformly distributed, which however contradicts the heterogeneity of IoT. Solutions of additional model training to check the data distribution inevitably increases computational costs and the risk of privacy leakage.","The challenges in solving these issues are how to reduce the impact of non-IID data without involving raw data and how to rationalize the communication resource allocation for addressing straggler problem.","To tackle these challenges, we propose a novel optimization method based on coaLition formation gamE and grAdient Projection, called LEAP.","Specifically, we combine edge data distribution with coalition formation game innovatively to adjust the correlations between clients and ESs dynamically, which ensures optimal correlations.","We further capture the client heterogeneity to achieve the rational bandwidth allocation from coalition perception and determine the optimal transmission power within specified delay constraints at client level.","Experimental results on four real datasets show that LEAP is able to achieve 20.62% improvement in model accuracy compared to the state-of-the-art baselines.","Moreover, LEAP effectively reduce transmission energy consumption by at least about 2.24 times."],"url":"http://arxiv.org/abs/2405.00579v1","category":"cs.GT"}
{"created":"2024-05-01 13:21:29","title":"The M33 Synoptic Stellar Survey. III. Miras and LPVs in griJHKs","abstract":"We present the results of a search for Miras and long-period variables (LPVs) in M33 using griJHKs archival observations from the Canada-France-Hawai'i Telescope. We use multiband information and machine learning techniques to identify and characterize these variables. We recover ~1,300 previously-discovered Mira candidates and identify ~13,000 new Miras and LPVs. We detect for the first time a clear first-overtone pulsation sequence among Mira candidates in this galaxy. We use O-rich, fundamental-mode Miras in the LMC and M33 to derive a distance modulus for the latter of 24.629 +/- 0.046 mag.","sentences":["We present the results of a search for Miras and long-period variables (LPVs) in M33 using griJHKs archival observations from the Canada-France-Hawai'i Telescope.","We use multiband information and machine learning techniques to identify and characterize these variables.","We recover ~1,300 previously-discovered Mira candidates and identify ~13,000 new Miras and LPVs.","We detect for the first time a clear first-overtone pulsation sequence among Mira candidates in this galaxy.","We use O-rich, fundamental-mode Miras in the LMC and M33 to derive a distance modulus for the latter of 24.629 +/- 0.046 mag."],"url":"http://arxiv.org/abs/2405.00503v1","category":"astro-ph.GA"}
{"created":"2024-05-01 12:04:28","title":"Harnessing the Power of Multiple Minds: Lessons Learned from LLM Routing","abstract":"With the rapid development of LLMs, it is natural to ask how to harness their capabilities efficiently. In this paper, we explore whether it is feasible to direct each input query to a single most suitable LLM. To this end, we propose LLM routing for challenging reasoning tasks. Our extensive experiments suggest that such routing shows promise but is not feasible in all scenarios, so more robust approaches should be investigated to fill this gap.","sentences":["With the rapid development of LLMs, it is natural to ask how to harness their capabilities efficiently.","In this paper, we explore whether it is feasible to direct each input query to a single most suitable LLM.","To this end, we propose LLM routing for challenging reasoning tasks.","Our extensive experiments suggest that such routing shows promise but is not feasible in all scenarios, so more robust approaches should be investigated to fill this gap."],"url":"http://arxiv.org/abs/2405.00467v1","category":"cs.CL"}
{"created":"2024-05-01 08:36:13","title":"Variational Bayesian Methods for a Tree-Structured Stick-Breaking Process Mixture of Gaussians","abstract":"The Bayes coding algorithm for context tree source is a successful example of Bayesian tree estimation in text compression in information theory. This algorithm provides an efficient parametric representation of the posterior tree distribution and exact updating of its parameters. We apply this algorithm to a clustering task in machine learning. More specifically, we apply it to Bayesian estimation of the tree-structured stick-breaking process (TS-SBP) mixture models. For TS-SBP mixture models, only Markov chain Monte Carlo methods have been proposed so far, but any variational Bayesian methods have not been proposed yet. In this paper, we propose a variational Bayesian method that has a subroutine similar to the Bayes coding algorithm for context tree sources. We confirm its behavior by a numerical experiment on a toy example.","sentences":["The Bayes coding algorithm for context tree source is a successful example of Bayesian tree estimation in text compression in information theory.","This algorithm provides an efficient parametric representation of the posterior tree distribution and exact updating of its parameters.","We apply this algorithm to a clustering task in machine learning.","More specifically, we apply it to Bayesian estimation of the tree-structured stick-breaking process (TS-SBP) mixture models.","For TS-SBP mixture models, only Markov chain Monte Carlo methods have been proposed so far, but any variational Bayesian methods have not been proposed yet.","In this paper, we propose a variational Bayesian method that has a subroutine similar to the Bayes coding algorithm for context tree sources.","We confirm its behavior by a numerical experiment on a toy example."],"url":"http://arxiv.org/abs/2405.00385v1","category":"stat.ML"}
{"created":"2024-05-01 03:59:06","title":"Joint Optimization of Piecewise Linear Ensembles","abstract":"Tree ensembles achieve state-of-the-art performance despite being greedily optimized. Global refinement (GR) reduces greediness by jointly and globally optimizing all constant leaves. We propose Joint Optimization of Piecewise Linear ENsembles (JOPLEN), a piecewise-linear extension of GR. Compared to GR, JOPLEN improves model flexibility and can apply common penalties, including sparsity-promoting matrix norms and subspace-norms, to nonlinear prediction. We evaluate the Frobenius norm, $\\ell_{2,1}$ norm, and Laplacian regularization for 146 regression and classification datasets; JOPLEN, combined with GB trees and RF, achieves superior performance in both settings. Additionally, JOPLEN with a nuclear norm penalty empirically learns smooth and subspace-aligned functions. Finally, we perform multitask feature selection by extending the Dirty LASSO. JOPLEN Dirty LASSO achieves a superior feature sparsity/performance tradeoff to linear and gradient boosted approaches. We anticipate that JOPLEN will improve regression, classification, and feature selection across many fields.","sentences":["Tree ensembles achieve state-of-the-art performance despite being greedily optimized.","Global refinement (GR) reduces greediness by jointly and globally optimizing all constant leaves.","We propose Joint Optimization of Piecewise Linear ENsembles (JOPLEN), a piecewise-linear extension of GR.","Compared to GR, JOPLEN improves model flexibility and can apply common penalties, including sparsity-promoting matrix norms and subspace-norms, to nonlinear prediction.","We evaluate the Frobenius norm, $\\ell_{2,1}$ norm, and Laplacian regularization for 146 regression and classification datasets; JOPLEN, combined with GB trees and RF, achieves superior performance in both settings.","Additionally, JOPLEN with a nuclear norm penalty empirically learns smooth and subspace-aligned functions.","Finally, we perform multitask feature selection by extending the Dirty LASSO.","JOPLEN Dirty LASSO achieves a superior feature sparsity/performance tradeoff to linear and gradient boosted approaches.","We anticipate that JOPLEN will improve regression, classification, and feature selection across many fields."],"url":"http://arxiv.org/abs/2405.00303v1","category":"cs.LG"}
{"created":"2024-04-30 23:49:26","title":"Semantically Consistent Video Inpainting with Conditional Diffusion Models","abstract":"Current state-of-the-art methods for video inpainting typically rely on optical flow or attention-based approaches to inpaint masked regions by propagating visual information across frames. While such approaches have led to significant progress on standard benchmarks, they struggle with tasks that require the synthesis of novel content that is not present in other frames. In this paper we reframe video inpainting as a conditional generative modeling problem and present a framework for solving such problems with conditional video diffusion models. We highlight the advantages of using a generative approach for this task, showing that our method is capable of generating diverse, high-quality inpaintings and synthesizing new content that is spatially, temporally, and semantically consistent with the provided context.","sentences":["Current state-of-the-art methods for video inpainting typically rely on optical flow or attention-based approaches to inpaint masked regions by propagating visual information across frames.","While such approaches have led to significant progress on standard benchmarks, they struggle with tasks that require the synthesis of novel content that is not present in other frames.","In this paper we reframe video inpainting as a conditional generative modeling problem and present a framework for solving such problems with conditional video diffusion models.","We highlight the advantages of using a generative approach for this task, showing that our method is capable of generating diverse, high-quality inpaintings and synthesizing new content that is spatially, temporally, and semantically consistent with the provided context."],"url":"http://arxiv.org/abs/2405.00251v1","category":"cs.CV"}
{"created":"2024-04-30 23:17:43","title":"Fast MRI Reconstruction Using Deep Learning-based Compressed Sensing: A Systematic Review","abstract":"Magnetic resonance imaging (MRI) has revolutionized medical imaging, providing a non-invasive and highly detailed look into the human body. However, the long acquisition times of MRI present challenges, causing patient discomfort, motion artifacts, and limiting real-time applications. To address these challenges, researchers are exploring various techniques to reduce acquisition time and improve the overall efficiency of MRI. One such technique is compressed sensing (CS), which reduces data acquisition by leveraging image sparsity in transformed spaces. In recent years, deep learning (DL) has been integrated with CS-MRI, leading to a new framework that has seen remarkable growth. DL-based CS-MRI approaches are proving to be highly effective in accelerating MR imaging without compromising image quality. This review comprehensively examines DL-based CS-MRI techniques, focusing on their role in increasing MR imaging speed. We provide a detailed analysis of each category of DL-based CS-MRI including end-to-end, unroll optimization, self-supervised, and federated learning. Our systematic review highlights significant contributions and underscores the exciting potential of DL in CS-MRI. Additionally, our systematic review efficiently summarizes key results and trends in DL-based CS-MRI including quantitative metrics, the dataset used, acceleration factors, and the progress of and research interest in DL techniques over time. Finally, we discuss potential future directions and the importance of DL-based CS-MRI in the advancement of medical imaging. To facilitate further research in this area, we provide a GitHub repository that includes up-to-date DL-based CS-MRI publications and publicly available datasets - https://github.com/mosaf/Awesome-DL-based-CS-MRI.","sentences":["Magnetic resonance imaging (MRI) has revolutionized medical imaging, providing a non-invasive and highly detailed look into the human body.","However, the long acquisition times of MRI present challenges, causing patient discomfort, motion artifacts, and limiting real-time applications.","To address these challenges, researchers are exploring various techniques to reduce acquisition time and improve the overall efficiency of MRI.","One such technique is compressed sensing (CS), which reduces data acquisition by leveraging image sparsity in transformed spaces.","In recent years, deep learning (DL) has been integrated with CS-MRI, leading to a new framework that has seen remarkable growth.","DL-based CS-MRI approaches are proving to be highly effective in accelerating MR imaging without compromising image quality.","This review comprehensively examines DL-based CS-MRI techniques, focusing on their role in increasing MR imaging speed.","We provide a detailed analysis of each category of DL-based CS-MRI including end-to-end, unroll optimization, self-supervised, and federated learning.","Our systematic review highlights significant contributions and underscores the exciting potential of DL in CS-MRI.","Additionally, our systematic review efficiently summarizes key results and trends in DL-based CS-MRI including quantitative metrics, the dataset used, acceleration factors, and the progress of and research interest in DL techniques over time.","Finally, we discuss potential future directions and the importance of DL-based CS-MRI in the advancement of medical imaging.","To facilitate further research in this area, we provide a GitHub repository that includes up-to-date DL-based CS-MRI publications and publicly available datasets - https://github.com/mosaf/Awesome-DL-based-CS-MRI."],"url":"http://arxiv.org/abs/2405.00241v1","category":"physics.med-ph"}
{"created":"2024-04-30 23:09:54","title":"IgCONDA-PET: Implicitly-Guided Counterfactual Diffusion for Detecting Anomalies in PET Images","abstract":"Minimizing the need for pixel-level annotated data for training PET anomaly segmentation networks is crucial, particularly due to time and cost constraints related to expert annotations. Current un-/weakly-supervised anomaly detection methods rely on autoencoder or generative adversarial networks trained only on healthy data, although these are more challenging to train. In this work, we present a weakly supervised and Implicitly guided COuNterfactual diffusion model for Detecting Anomalies in PET images, branded as IgCONDA-PET. The training is conditioned on image class labels (healthy vs. unhealthy) along with implicit guidance to generate counterfactuals for an unhealthy image with anomalies. The counterfactual generation process synthesizes the healthy counterpart for a given unhealthy image, and the difference between the two facilitates the identification of anomaly locations. The code is available at: https://github.com/igcondapet/IgCONDA-PET.git","sentences":["Minimizing the need for pixel-level annotated data for training PET anomaly segmentation networks is crucial, particularly due to time and cost constraints related to expert annotations.","Current un-/weakly-supervised anomaly detection methods rely on autoencoder or generative adversarial networks trained only on healthy data, although these are more challenging to train.","In this work, we present a weakly supervised and Implicitly guided COuNterfactual diffusion model for Detecting Anomalies in PET images, branded as IgCONDA-PET.","The training is conditioned on image class labels (healthy vs. unhealthy) along with implicit guidance to generate counterfactuals for an unhealthy image with anomalies.","The counterfactual generation process synthesizes the healthy counterpart for a given unhealthy image, and the difference between the two facilitates the identification of anomaly locations.","The code is available at: https://github.com/igcondapet/IgCONDA-PET.git"],"url":"http://arxiv.org/abs/2405.00239v1","category":"eess.IV"}
{"created":"2024-04-30 23:00:30","title":"Conceiving Naturally After IVF: the effect of assisted reproduction on obstetric interventions and child health at birth","abstract":"A growing share of the world's population is being born via assisted reproductive technology (ART), including in-vitro fertilisation (IVF). However, two concerns persist. First, ART pregnancies correlate with predictors of poor outcomes at birth--and it is unclear whether this relationship is causal. Second, the emotional and financial costs associated with ART-use might exacerbate defensive medical behaviour, where physicians intervene more than necessary to reduce the risk of adverse medical outcomes and litigation. We address the challenge of identifying the pure effect of ART-use on both maternal and infant outcomes at birth by leveraging exogenous variation in the success of ART cycles. We compare the obstetric outcomes for ART-conceived births with those of spontaneously-conceived births after a failed ART treatment. Moreover, we flexibly adjust for key confounders using double machine learning. We do this using clinical registry ART data and administrative maternal and infant data from New South Wales (NSW) between 2009-2017. We find that ART slightly decreases the risk of obstetric interventions, lowering the risk of a caesarean section and increasing the rate of spontaneous labour (+3.5 p.p.). Moreover, we find that ART has a statistically and clinically insignificant effect on infant health outcomes.   Keywords: Fertility, Assisted reproduction, IVF, Caesarean Section, Obstetric, Infertility. JEL classification: I10, I12, I19.","sentences":["A growing share of the world's population is being born via assisted reproductive technology (ART), including in-vitro fertilisation (IVF).","However, two concerns persist.","First, ART pregnancies correlate with predictors of poor outcomes at birth--and it is unclear whether this relationship is causal.","Second, the emotional and financial costs associated with ART-use might exacerbate defensive medical behaviour, where physicians intervene more than necessary to reduce the risk of adverse medical outcomes and litigation.","We address the challenge of identifying the pure effect of ART-use on both maternal and infant outcomes at birth by leveraging exogenous variation in the success of ART cycles.","We compare the obstetric outcomes for ART-conceived births with those of spontaneously-conceived births after a failed ART treatment.","Moreover, we flexibly adjust for key confounders using double machine learning.","We do this using clinical registry ART data and administrative maternal and infant data from New South Wales (NSW) between 2009-2017.","We find that ART slightly decreases the risk of obstetric interventions, lowering the risk of a caesarean section and increasing the rate of spontaneous labour (+3.5 p.p.).","Moreover, we find that ART has a statistically and clinically insignificant effect on infant health outcomes.   ","Keywords: Fertility, Assisted reproduction, IVF, Caesarean Section, Obstetric, Infertility.","JEL classification: I10, I12, I19."],"url":"http://arxiv.org/abs/2405.00234v1","category":"econ.GN"}
{"created":"2024-04-30 22:47:07","title":"Constraining the giant radio galaxy population with machine learning and Bayesian inference","abstract":"Large-scale sky surveys at low frequencies, like the LOFAR Two-metre Sky Survey (LoTSS), allow for the detection and characterisation of unprecedented numbers of giant radio galaxies (GRGs, or 'giants'). In this work, by automating the creation of radio--optical catalogues, we aim to significantly expand the census of known giants. We then combine this sample with a forward model to constrain GRG properties of cosmological interest. In particular, we automate radio source component association through machine learning and optical host identification for resolved radio sources. We create a radio--optical catalogue for the full LoTSS Data Release 2 (DR2) and select all possible giants. We combine our candidates with an existing catalogue of LoTSS DR2 crowd-sourced GRG candidates and visually confirm or reject them. To infer intrinsic GRG properties from GRG observations, we develop further a population-based forward model that takes into account selection effects and constrain its parameters using Bayesian inference. We confirm 5,647 previously unknown giants from the crowd-sourced catalogue and 2,597 previously unknown giants from the ML-driven catalogue. Our confirmations and discoveries bring the total number of known giants to at least 11,585. We predict a comoving GRG number density $n_\\mathrm{GRG} = 13 \\pm 10\\ (100\\ \\mathrm{Mpc})^{-3}$, close to a recent estimate of the number density of luminous non-giant radio galaxies. We derive a current-day GRG lobe volume-filling fraction $V_\\mathrm{GRG-CW}(z = 0) = 1.4 \\pm 1.1 \\cdot 10^{-5}$ in clusters and filaments of the Cosmic Web. Our analysis suggests that giants are more common than previously thought. Moreover, tentative results imply that it is possible that magnetic fields once contained in giants pervade a significant ($\\gtrsim 10\\%$) fraction of today's Cosmic Web.","sentences":["Large-scale sky surveys at low frequencies, like the LOFAR Two-metre Sky Survey (LoTSS), allow for the detection and characterisation of unprecedented numbers of giant radio galaxies (GRGs, or 'giants').","In this work, by automating the creation of radio--optical catalogues, we aim to significantly expand the census of known giants.","We then combine this sample with a forward model to constrain GRG properties of cosmological interest.","In particular, we automate radio source component association through machine learning and optical host identification for resolved radio sources.","We create a radio--optical catalogue for the full LoTSS Data Release 2 (DR2) and select all possible giants.","We combine our candidates with an existing catalogue of LoTSS DR2 crowd-sourced GRG candidates and visually confirm or reject them.","To infer intrinsic GRG properties from GRG observations, we develop further a population-based forward model that takes into account selection effects and constrain its parameters using Bayesian inference.","We confirm 5,647 previously unknown giants from the crowd-sourced catalogue and 2,597 previously unknown giants from the ML-driven catalogue.","Our confirmations and discoveries bring the total number of known giants to at least 11,585.","We predict a comoving GRG number density $n_\\mathrm{GRG} = 13 \\pm 10\\ (100\\ \\mathrm{Mpc})^{-3}$, close to a recent estimate of the number density of luminous non-giant radio galaxies.","We derive a current-day GRG lobe volume-filling fraction $V_\\mathrm{GRG-CW}(z = 0) = 1.4 \\pm 1.1 \\cdot 10^{-5}$ in clusters and filaments of the Cosmic Web.","Our analysis suggests that giants are more common than previously thought.","Moreover, tentative results imply that it is possible that magnetic fields once contained in giants pervade a significant ($\\gtrsim 10\\%$) fraction of today's Cosmic Web."],"url":"http://arxiv.org/abs/2405.00232v1","category":"astro-ph.GA"}
{"created":"2024-04-30 21:06:52","title":"In-Context Learning with Long-Context Models: An In-Depth Exploration","abstract":"As model context lengths continue to increase, the number of demonstrations that can be provided in-context approaches the size of entire training datasets. We study the behavior of in-context learning (ICL) at this extreme scale on multiple datasets and models. We show that, for many datasets with large label spaces, performance continues to increase with hundreds or thousands of demonstrations. We contrast this with example retrieval and finetuning: example retrieval shows excellent performance at low context lengths but has diminished gains with more demonstrations; finetuning is more data hungry than ICL but can sometimes exceed long-context ICL performance with additional data. We use this ICL setting as a testbed to study several properties of both in-context learning and long-context models. We show that long-context ICL is less sensitive to random input shuffling than short-context ICL, that grouping of same-label examples can negatively impact performance, and that the performance boosts we see do not arise from cumulative gain from encoding many examples together. We conclude that although long-context ICL can be surprisingly effective, most of this gain comes from attending back to similar examples rather than task learning.","sentences":["As model context lengths continue to increase, the number of demonstrations that can be provided in-context approaches the size of entire training datasets.","We study the behavior of in-context learning (ICL) at this extreme scale on multiple datasets and models.","We show that, for many datasets with large label spaces, performance continues to increase with hundreds or thousands of demonstrations.","We contrast this with example retrieval and finetuning: example retrieval shows excellent performance at low context lengths but has diminished gains with more demonstrations; finetuning is more data hungry than ICL but can sometimes exceed long-context ICL performance with additional data.","We use this ICL setting as a testbed to study several properties of both in-context learning and long-context models.","We show that long-context ICL is less sensitive to random input shuffling than short-context ICL, that grouping of same-label examples can negatively impact performance, and that the performance boosts we see do not arise from cumulative gain from encoding many examples together.","We conclude that although long-context ICL can be surprisingly effective, most of this gain comes from attending back to similar examples rather than task learning."],"url":"http://arxiv.org/abs/2405.00200v1","category":"cs.CL"}
{"created":"2024-04-30 20:53:05","title":"Thawed Gaussian wavepacket dynamics with $\u0394$-machine learned potentials","abstract":"A method for performing variable-width (thawed) Gaussian wavepacket (GWP) variational dynamics on machine-learned potentials is presented. Instead of fitting the potential energy surface (PES), the anharmonic correction to the global harmonic approximation (GHA) is fitted using kernel ridge regression -- this is a $\\Delta$-machine learning approach. The training set consists of energy differences between ab initio electronic energies and values given by the GHA. The learned potential is subsequently used to propagate a single thawed GWP using the time-dependent variational principle to compute the autocorrelation function, which provides direct access to vibronic spectra via its Fourier transform. We applied the developed method to simulate the photoelectron spectrum of ammonia and found excellent agreement between theoretical and experimental spectra. We show that fitting the anharmonic corrections requires a smaller training set as compared to fitting total electronic energies. We also demonstrate that our approach allows to reduce the dimensionality of the nuclear space used to scan the PES when constructing the training set. Thus, only the degrees of freedom associated with large amplitude motions need to be treated with $\\Delta$-machine learning, which paves a way for reliable simulations of vibronic spectra of large floppy molecules.","sentences":["A method for performing variable-width (thawed) Gaussian wavepacket (GWP) variational dynamics on machine-learned potentials is presented.","Instead of fitting the potential energy surface (PES), the anharmonic correction to the global harmonic approximation (GHA) is fitted using kernel ridge regression -- this is a $\\Delta$-machine learning approach.","The training set consists of energy differences between ab initio electronic energies and values given by the GHA.","The learned potential is subsequently used to propagate a single thawed GWP using the time-dependent variational principle to compute the autocorrelation function, which provides direct access to vibronic spectra via its Fourier transform.","We applied the developed method to simulate the photoelectron spectrum of ammonia and found excellent agreement between theoretical and experimental spectra.","We show that fitting the anharmonic corrections requires a smaller training set as compared to fitting total electronic energies.","We also demonstrate that our approach allows to reduce the dimensionality of the nuclear space used to scan the PES when constructing the training set.","Thus, only the degrees of freedom associated with large amplitude motions need to be treated with $\\Delta$-machine learning, which paves a way for reliable simulations of vibronic spectra of large floppy molecules."],"url":"http://arxiv.org/abs/2405.00193v1","category":"physics.chem-ph"}
{"created":"2024-04-30 19:15:33","title":"BayesBlend: Easy Model Blending using Pseudo-Bayesian Model Averaging, Stacking and Hierarchical Stacking in Python","abstract":"Averaging predictions from multiple competing inferential models frequently outperforms predictions from any single model, providing that models are optimally weighted to maximize predictive performance. This is particularly the case in so-called $\\mathcal{M}$-open settings where the true model is not in the set of candidate models, and may be neither mathematically reifiable nor known precisely. This practice of model averaging has a rich history in statistics and machine learning, and there are currently a number of methods to estimate the weights for constructing model-averaged predictive distributions. Nonetheless, there are few existing software packages that can estimate model weights from the full variety of methods available, and none that blend model predictions into a coherent predictive distribution according to the estimated weights. In this paper, we introduce the BayesBlend Python package, which provides a user-friendly programming interface to estimate weights and blend multiple (Bayesian) models' predictive distributions. BayesBlend implements pseudo-Bayesian model averaging, stacking and, uniquely, hierarchical Bayesian stacking to estimate model weights. We demonstrate the usage of BayesBlend with examples of insurance loss modeling.","sentences":["Averaging predictions from multiple competing inferential models frequently outperforms predictions from any single model, providing that models are optimally weighted to maximize predictive performance.","This is particularly the case in so-called $\\mathcal{M}$-open settings where the true model is not in the set of candidate models, and may be neither mathematically reifiable nor known precisely.","This practice of model averaging has a rich history in statistics and machine learning, and there are currently a number of methods to estimate the weights for constructing model-averaged predictive distributions.","Nonetheless, there are few existing software packages that can estimate model weights from the full variety of methods available, and none that blend model predictions into a coherent predictive distribution according to the estimated weights.","In this paper, we introduce the BayesBlend Python package, which provides a user-friendly programming interface to estimate weights and blend multiple (Bayesian) models' predictive distributions.","BayesBlend implements pseudo-Bayesian model averaging, stacking and, uniquely, hierarchical Bayesian stacking to estimate model weights.","We demonstrate the usage of BayesBlend with examples of insurance loss modeling."],"url":"http://arxiv.org/abs/2405.00158v1","category":"stat.ME"}
{"created":"2024-04-30 18:25:59","title":"Complex contagions can outperform simple contagions for network reconstruction with dense networks or saturated dynamics","abstract":"Network scientists often use complex dynamic processes to describe network contagions, but tools for fitting contagion models typically assume simple dynamics. Here, we address this gap by developing a nonparametric method to reconstruct a network and dynamics from a series of node states, using a model that breaks the dichotomy between simple pairwise and complex neighborhood-based contagions. We then show that a network is more easily reconstructed when observed through the lens of complex contagions if it is dense or the dynamic saturates, and that simple contagions are better otherwise.","sentences":["Network scientists often use complex dynamic processes to describe network contagions, but tools for fitting contagion models typically assume simple dynamics.","Here, we address this gap by developing a nonparametric method to reconstruct a network and dynamics from a series of node states, using a model that breaks the dichotomy between simple pairwise and complex neighborhood-based contagions.","We then show that a network is more easily reconstructed when observed through the lens of complex contagions if it is dense or the dynamic saturates, and that simple contagions are better otherwise."],"url":"http://arxiv.org/abs/2405.00129v1","category":"cs.SI"}
{"created":"2024-04-30 16:35:08","title":"Recommenadation aided Caching using Combinatorial Multi-armed Bandits","abstract":"We study content caching with recommendations in a wireless network where the users are connected through a base station equipped with a finite-capacity cache. We assume a fixed set of contents with unknown user preferences and content popularities. We can recommend a subset of the contents to the users which encourages the users to request these contents. Recommendation can thus be used to increase cache hits. We formulate the cache hit optimization problem as a combinatorial multi-armed bandit (CMAB). We propose a UCB-based algorithm to decide which contents to cache and recommend. We provide an upper bound on the regret of our algorithm. We numerically demonstrate the performance of our algorithm and compare it to state-of-the-art algorithms.","sentences":["We study content caching with recommendations in a wireless network where the users are connected through a base station equipped with a finite-capacity cache.","We assume a fixed set of contents with unknown user preferences and content popularities.","We can recommend a subset of the contents to the users which encourages the users to request these contents.","Recommendation can thus be used to increase cache hits.","We formulate the cache hit optimization problem as a combinatorial multi-armed bandit (CMAB).","We propose a UCB-based algorithm to decide which contents to cache and recommend.","We provide an upper bound on the regret of our algorithm.","We numerically demonstrate the performance of our algorithm and compare it to state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2405.00080v1","category":"cs.LG"}
{"created":"2024-05-01 17:35:54","title":"Shape optimization of slip-driven axisymmetric microswimmers","abstract":"In this work, we develop a computational framework that aims at simultaneously optimizing the shape and the slip velocity of an axisymmetric microswimmer suspended in a viscous fluid. We consider shapes of a given reduced volume that maximize the swimming efficiency, i.e., the (size-independent) ratio of the power loss arising from towing the rigid body of the same shape and size at the same translation velocity to the actual power loss incurred by swimming via the slip velocity. The optimal slip and efficiency (with shape fixed) are here given in terms of two Stokes flow solutions, and we then establish shape sensitivity formulas of adjoint-solution that provide objective function derivatives with respect to any set of shape parameters on the sole basis of the above two flow solutions. Our computational treatment relies on a fast and accurate boundary integral solver for solving all Stokes flow problems. We validate our analytic shape derivative formulas via comparisons against finite-difference gradient evaluations, and present several shape optimization examples.","sentences":["In this work, we develop a computational framework that aims at simultaneously optimizing the shape and the slip velocity of an axisymmetric microswimmer suspended in a viscous fluid.","We consider shapes of a given reduced volume that maximize the swimming efficiency, i.e., the (size-independent) ratio of the power loss arising from towing the rigid body of the same shape and size at the same translation velocity to the actual power loss incurred by swimming via the slip velocity.","The optimal slip and efficiency (with shape fixed) are here given in terms of two Stokes flow solutions, and we then establish shape sensitivity formulas of adjoint-solution that provide objective function derivatives with respect to any set of shape parameters on the sole basis of the above two flow solutions.","Our computational treatment relies on a fast and accurate boundary integral solver for solving all Stokes flow problems.","We validate our analytic shape derivative formulas via comparisons against finite-difference gradient evaluations, and present several shape optimization examples."],"url":"http://arxiv.org/abs/2405.00656v1","category":"math.OC"}
{"created":"2024-05-01 17:26:12","title":"Electronic Coherences in Molecules: The Projected Nuclear Quantum Momentum as a Hidden Agent","abstract":"Electronic coherences are key to understanding and controlling photo-induced molecular transformations. We identify a crucial quantum-mechanical feature of electron-nuclear correlation, the projected nuclear quantum momenta, essential to capture the correct coherence behavior. In simulations, we show that, unlike traditional trajectory-based schemes, exact-factorization-based methods approximate these correlation terms, and correctly capture electronic coherences in a range of situations, including their spatial dependence, an important aspect that influences subsequent electron dynamics and that is becoming accessible in more experiments.","sentences":["Electronic coherences are key to understanding and controlling photo-induced molecular transformations.","We identify a crucial quantum-mechanical feature of electron-nuclear correlation, the projected nuclear quantum momenta, essential to capture the correct coherence behavior.","In simulations, we show that, unlike traditional trajectory-based schemes, exact-factorization-based methods approximate these correlation terms, and correctly capture electronic coherences in a range of situations, including their spatial dependence, an important aspect that influences subsequent electron dynamics and that is becoming accessible in more experiments."],"url":"http://arxiv.org/abs/2405.00649v1","category":"physics.chem-ph"}
{"created":"2024-05-01 17:16:46","title":"Electronic and Optical Excitations in van der Waals Materials from a Non-Empirical Wannier-Localized Optimally-Tuned Screened Range-Separated Hybrid Functional","abstract":"Accurate prediction of electronic and optical excitations in van der Waals (vdW) materials is a long-standing challenge for density functional theory. The recently proposed Wannier-localized optimally-tuned screened range-separated hybrid (WOT-SRSH) functional has proven successful in non-empirical determination of electronic band gaps and optical absorption spectra for various covalent and ionic crystals. However, for vdW materials the tuning of the material- and structure-dependent functional parameters has, until now, only been attained semi-empirically. Here, we present a non-empirical WOT-SRSH approach applicable to vdW materials, with the optimal functional parameters transferable between monolayer and bulk. We apply this methodology to prototypical vdW materials: black phosphorus, molybdenum disulfide, and hexagonal boron nitride (in the latter case including zero-point renormalization). We show that the WOT-SRSH approach consistently achieves accuracy levels comparable to experiments and ab initio many-body perturbation theory (MBPT) calculations for band structures and optical absorption spectra, both on its own and as an optimal starting point for MBPT calculations.","sentences":["Accurate prediction of electronic and optical excitations in van der Waals (vdW) materials is a long-standing challenge for density functional theory.","The recently proposed Wannier-localized optimally-tuned screened range-separated hybrid (WOT-SRSH) functional has proven successful in non-empirical determination of electronic band gaps and optical absorption spectra for various covalent and ionic crystals.","However, for vdW materials the tuning of the material- and structure-dependent functional parameters has, until now, only been attained semi-empirically.","Here, we present a non-empirical WOT-SRSH approach applicable to vdW materials, with the optimal functional parameters transferable between monolayer and bulk.","We apply this methodology to prototypical vdW materials: black phosphorus, molybdenum disulfide, and hexagonal boron nitride (in the latter case including zero-point renormalization).","We show that the WOT-SRSH approach consistently achieves accuracy levels comparable to experiments and ab initio many-body perturbation theory (MBPT) calculations for band structures and optical absorption spectra, both on its own and as an optimal starting point for MBPT calculations."],"url":"http://arxiv.org/abs/2405.00643v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-01 16:35:44","title":"An Expectation-Maximization Relaxed Method for Privacy Funnel","abstract":"The privacy funnel (PF) gives a framework of privacy-preserving data release, where the goal is to release useful data while also limiting the exposure of associated sensitive information. This framework has garnered significant interest due to its broad applications in characterization of the privacy-utility tradeoff. Hence, there is a strong motivation to develop numerical methods with high precision and theoretical convergence guarantees. In this paper, we propose a novel relaxation variant based on Jensen's inequality of the objective function for the computation of the PF problem. This model is proved to be equivalent to the original in terms of optimal solutions and optimal values. Based on our proposed model, we develop an accurate algorithm which only involves closed-form iterations. The convergence of our algorithm is theoretically guaranteed through descent estimation and Pinsker's inequality. Numerical results demonstrate the effectiveness of our proposed algorithm.","sentences":["The privacy funnel (PF) gives a framework of privacy-preserving data release, where the goal is to release useful data while also limiting the exposure of associated sensitive information.","This framework has garnered significant interest due to its broad applications in characterization of the privacy-utility tradeoff.","Hence, there is a strong motivation to develop numerical methods with high precision and theoretical convergence guarantees.","In this paper, we propose a novel relaxation variant based on Jensen's inequality of the objective function for the computation of the PF problem.","This model is proved to be equivalent to the original in terms of optimal solutions and optimal values.","Based on our proposed model, we develop an accurate algorithm which only involves closed-form iterations.","The convergence of our algorithm is theoretically guaranteed through descent estimation and Pinsker's inequality.","Numerical results demonstrate the effectiveness of our proposed algorithm."],"url":"http://arxiv.org/abs/2405.00616v1","category":"cs.IT"}
{"created":"2024-05-01 16:17:39","title":"A Preprocessing and Evaluation Toolbox for Trajectory Prediction Research on the Drone Datasets","abstract":"The availability of high-quality datasets is crucial for the development of behavior prediction algorithms in autonomous vehicles. This paper highlights the need for standardizing the use of certain datasets for motion forecasting research to simplify comparative analysis and proposes a set of tools and practices to achieve this. Drawing on extensive experience and a comprehensive review of current literature, we summarize our proposals for preprocessing, visualizing, and evaluation in the form of an open-sourced toolbox designed for researchers working on trajectory prediction problems. The clear specification of necessary preprocessing steps and evaluation metrics is intended to alleviate development efforts and facilitate the comparison of results across different studies. The toolbox is available at: https://github.com/westny/dronalize.","sentences":["The availability of high-quality datasets is crucial for the development of behavior prediction algorithms in autonomous vehicles.","This paper highlights the need for standardizing the use of certain datasets for motion forecasting research to simplify comparative analysis and proposes a set of tools and practices to achieve this.","Drawing on extensive experience and a comprehensive review of current literature, we summarize our proposals for preprocessing, visualizing, and evaluation in the form of an open-sourced toolbox designed for researchers working on trajectory prediction problems.","The clear specification of necessary preprocessing steps and evaluation metrics is intended to alleviate development efforts and facilitate the comparison of results across different studies.","The toolbox is available at: https://github.com/westny/dronalize."],"url":"http://arxiv.org/abs/2405.00604v1","category":"cs.RO"}
{"created":"2024-05-01 16:13:31","title":"How founder motivations, goals, and actions influence early trajectories of online communities","abstract":"Online communities offer their members various benefits, such as information access, social and emotional support, and entertainment. Despite the important role that founders play in shaping communities, prior research has focused primarily on what drives users to participate and contribute; the motivations and goals of founders remain underexplored. To uncover how and why online communities get started, we present findings from a survey of 951 recent founders of Reddit communities. We find that topical interest is the most common motivation for community creation, followed by motivations to exchange information, connect with others, and self-promote. Founders have heterogeneous goals for their nascent communities, but they tend to privilege community quality and engagement over sheer growth. These differences in founders' early attitudes towards their communities help predict not only the community-building actions that they pursue, but also the ability of their communities to attract visitors, contributors, and subscribers over the first 28 days. We end with a discussion of the implications for researchers, designers, and founders of online communities.","sentences":["Online communities offer their members various benefits, such as information access, social and emotional support, and entertainment.","Despite the important role that founders play in shaping communities, prior research has focused primarily on what drives users to participate and contribute; the motivations and goals of founders remain underexplored.","To uncover how and why online communities get started, we present findings from a survey of 951 recent founders of Reddit communities.","We find that topical interest is the most common motivation for community creation, followed by motivations to exchange information, connect with others, and self-promote.","Founders have heterogeneous goals for their nascent communities, but they tend to privilege community quality and engagement over sheer growth.","These differences in founders' early attitudes towards their communities help predict not only the community-building actions that they pursue, but also the ability of their communities to attract visitors, contributors, and subscribers over the first 28 days.","We end with a discussion of the implications for researchers, designers, and founders of online communities."],"url":"http://arxiv.org/abs/2405.00601v1","category":"cs.HC"}
{"created":"2024-05-01 15:40:38","title":"Construction of extremal Type II $\\mathbb{Z}_{8}$-codes via doubling method","abstract":"Extremal Type II $\\mathbb{Z}_{8}$-codes are a class of self-dual $\\mathbb{Z}_{8}$-codes with Euclidean weights divisible by $16$ and the largest possible minimum Euclidean weight for a given length. We introduce a doubling method for constructing a Type II $\\mathbb{Z}_{2k}$-code of length $n$ from a known Type II $\\mathbb{Z}_{2k}$-code of length $n$. Based on this method, we develop an algorithm to construct new extremal Type II $\\mathbb{Z}_8$-codes starting from an extremal Type II $\\mathbb{Z}_8$-code of type $(\\frac{n}{2},0,0)$ with an extremal $\\mathbb{Z}_4$-residue code and length $24, 32$ or $40$.   We construct at least ten new extremal Type II $\\mathbb{Z}_8$-codes of length $32$ and type $(15,1,1)$. Extremal Type II $\\mathbb{Z}_8$-codes of length $32$ of this type were not known before. Moreover, the binary residue codes of the constructed extremal $\\mathbb{Z}_8$-codes are optimal $[32,15]$ binary codes.","sentences":["Extremal Type II $\\mathbb{Z}_{8}$-codes are a class of self-dual $\\mathbb{Z}_{8}$-codes with Euclidean weights divisible by $16$ and the largest possible minimum Euclidean weight for a given length.","We introduce a doubling method for constructing a Type II $\\mathbb{Z}_{2k}$-code of length $n$ from a known Type II $\\mathbb{Z}_{2k}$-code of length $n$. Based on this method, we develop an algorithm to construct new extremal Type II $\\mathbb{Z}_8$-codes starting from an extremal Type II $\\mathbb{Z}_8$-code of type $(\\frac{n}{2},0,0)$ with an extremal $\\mathbb{Z}_4$-residue code and length $24, 32$ or $40$.   We construct at least ten new extremal Type II $\\mathbb{Z}_8$-codes of length $32$ and type $(15,1,1)$. Extremal Type II $\\mathbb{Z}_8$-codes of length $32$ of this type were not known before.","Moreover, the binary residue codes of the constructed extremal $\\mathbb{Z}_8$-codes are optimal $","[32,15]$ binary codes."],"url":"http://arxiv.org/abs/2405.00584v1","category":"cs.IT"}
{"created":"2024-05-01 15:12:53","title":"Physics-Informed Acoustic Liner Optimization: Balancing Drag and Noise","abstract":"We present pore-resolved Direct Numerical Simulations (DNS) of turbulent flows grazing over acoustic liners with aerodynamically and/or acoustically optimized orifice configurations. Our DNS explore a large parameter space, studying various families of orifice geometries, including the influence of orifice shape, orientation, and the number of orifices. All flow cases show an increase in drag compared to the smooth wall. However, the added drag can be reduced by as much as $\\sim$55\\% as compared to conventional acoustic liners by simply altering the shape of the orifice or its orientation, in the case of a non-circular orifice. Complementary acoustic simulations demonstrate that this reduced drag may be achieved while maintaining the same noise reduction properties over a wide range of frequencies.","sentences":["We present pore-resolved Direct Numerical Simulations (DNS) of turbulent flows grazing over acoustic liners with aerodynamically and/or acoustically optimized orifice configurations.","Our DNS explore a large parameter space, studying various families of orifice geometries, including the influence of orifice shape, orientation, and the number of orifices.","All flow cases show an increase in drag compared to the smooth wall.","However, the added drag can be reduced by as much as $\\sim$55\\% as compared to conventional acoustic liners by simply altering the shape of the orifice or its orientation, in the case of a non-circular orifice.","Complementary acoustic simulations demonstrate that this reduced drag may be achieved while maintaining the same noise reduction properties over a wide range of frequencies."],"url":"http://arxiv.org/abs/2405.00563v1","category":"physics.flu-dyn"}
{"created":"2024-05-01 12:21:29","title":"On Convergence of Discrete Schemes for Computing the Rate-Distortion Function of Continuous Source","abstract":"Computing the rate-distortion function for continuous sources is commonly regarded as a standard continuous optimization problem. When numerically addressing this problem, a typical approach involves discretizing the source space and subsequently solving the associated discrete problem. However, existing literature has predominantly concentrated on the convergence analysis of solving discrete problems, usually neglecting the convergence relationship between the original continuous optimization and its associated discrete counterpart. This neglect is not rigorous, since the solution of a discrete problem does not necessarily imply convergence to the solution of the original continuous problem, especially for non-linear problems. To address this gap, our study employs rigorous mathematical analysis, which constructs a series of finite-dimensional spaces approximating the infinite-dimensional space of the probability measure, establishing that solutions from discrete schemes converge to those from the continuous problems.","sentences":["Computing the rate-distortion function for continuous sources is commonly regarded as a standard continuous optimization problem.","When numerically addressing this problem, a typical approach involves discretizing the source space and subsequently solving the associated discrete problem.","However, existing literature has predominantly concentrated on the convergence analysis of solving discrete problems, usually neglecting the convergence relationship between the original continuous optimization and its associated discrete counterpart.","This neglect is not rigorous, since the solution of a discrete problem does not necessarily imply convergence to the solution of the original continuous problem, especially for non-linear problems.","To address this gap, our study employs rigorous mathematical analysis, which constructs a series of finite-dimensional spaces approximating the infinite-dimensional space of the probability measure, establishing that solutions from discrete schemes converge to those from the continuous problems."],"url":"http://arxiv.org/abs/2405.00474v1","category":"cs.IT"}
{"created":"2024-05-01 11:58:48","title":"On the best constants of Schur multipliers of second order divided difference functions","abstract":"We give a new proof of the boundedness of bilinear Schur multipliers of second order divided difference functions, as obtained earlier by Potapov, Skripka and Sukochev in their proof of Koplienko's conjecture on the existence of higher order spectral shift functions. Our proof is based on recent methods involving bilinear transference and the H\\\"ormander-Mikhlin-Schur multiplier theorem. Our approach provides a significant sharpening of the known asymptotic bounds of bilinear Schur multipliers of second order divided difference functions. Furthermore, we give a new lower bound of these bilinear Schur multipliers, giving again a fundamental improvement on the best known bounds obtained by Coine, Le Merdy, Potapov, Sukochev and Tomskova.   More precisely, we prove that for $f \\in C^2(\\mathbb{R})$ and $1 < p, p_1, p_2 < \\infty$ with $\\frac{1}{p} = \\frac{1}{p_1} + \\frac{1}{p_2}$ we have \\[ \\Vert M_{f^{[2]}}: S_{p_1} \\times S_{p_2} \\rightarrow S_p \\Vert \\lesssim \\Vert f'' \\Vert_\\infty D(p, p_1, p_2), \\] where the constant $D(p, p_1, p_2)$ is specified in Theorem 7.1 and $D(p, 2p, 2p) \\approx p^4 p^\\ast$ with $p^\\ast$ the H\\\"older conjugate of $p$. We further show that for $f(\\lambda) = \\lambda \\vert \\lambda \\vert$, $\\lambda \\in \\mathbb{R}$, for every $1 < p < \\infty$ we have \\[ p^2 p^\\ast \\lesssim \\Vert M_{f^{[2]}}: S_{2p} \\times S_{2p} \\rightarrow S_p \\Vert. \\] Here $f^{[2]}$ is the second order divided difference function of $f$ with $M_{f^{[2]}}$ the associated Schur multiplier. In particular it follows that our estimate $D(p, 2p, 2p)$ is optimal for $p \\searrow 1$.","sentences":["We give a new proof of the boundedness of bilinear Schur multipliers of second order divided difference functions, as obtained earlier by Potapov, Skripka and Sukochev in their proof of Koplienko's conjecture on the existence of higher order spectral shift functions.","Our proof is based on recent methods involving bilinear transference and the H\\\"ormander-Mikhlin-Schur multiplier theorem.","Our approach provides a significant sharpening of the known asymptotic bounds of bilinear Schur multipliers of second order divided difference functions.","Furthermore, we give a new lower bound of these bilinear Schur multipliers, giving again a fundamental improvement on the best known bounds obtained by Coine, Le Merdy, Potapov, Sukochev and Tomskova.   ","More precisely, we prove that for $f \\in C^2(\\mathbb{R})$ and $1 < p, p_1, p_2 < \\infty$ with $\\frac{1}{p} = \\frac{1}{p_1} + \\frac{1}{p_2}$ we have \\[ \\Vert M_{f^{[2]}}: S_{p_1} \\times S_{p_2} \\rightarrow S_p \\Vert \\lesssim \\Vert f'' \\Vert_\\infty D(p, p_1, p_2), \\] where the constant $D(p, p_1, p_2)$ is specified in Theorem 7.1 and $D(p, 2p, 2p)","\\approx p^4 p^\\ast$ with $p^\\ast$ the H\\\"older conjugate of $p$. We further show that for $f(\\lambda) = \\lambda \\vert \\lambda \\vert$, $\\lambda \\in \\mathbb{R}$, for every $1 < p < \\infty$ we have \\[ p^2 p^\\ast \\lesssim \\Vert M_{f^{[2]}}: S_{2p} \\times S_{2p} \\rightarrow S_p \\Vert.","\\]","Here $f^{[2]}$ is the second order divided difference function of $f$ with $M_{f^{[2]}}$ the associated Schur multiplier.","In particular it follows that our estimate $D(p, 2p, 2p)$ is optimal for $p \\searrow 1$."],"url":"http://arxiv.org/abs/2405.00464v1","category":"math.CA"}
{"created":"2024-05-01 08:30:58","title":"Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol","abstract":"This paper presents a baseline approach and an experimental protocol for a specific content verification problem: detecting discrepancies between the audio and video modalities in multimedia content. We first design and optimize an audio-visual scene classifier, to compare with existing classification baselines that use both modalities. Then, by applying this classifier separately to the audio and the visual modality, we can detect scene-class inconsistencies between them. To facilitate further research and provide a common evaluation platform, we introduce an experimental protocol and a benchmark dataset simulating such inconsistencies. Our approach achieves state-of-the-art results in scene classification and promising outcomes in audio-visual discrepancies detection, highlighting its potential in content verification applications.","sentences":["This paper presents a baseline approach and an experimental protocol for a specific content verification problem: detecting discrepancies between the audio and video modalities in multimedia content.","We first design and optimize an audio-visual scene classifier, to compare with existing classification baselines that use both modalities.","Then, by applying this classifier separately to the audio and the visual modality, we can detect scene-class inconsistencies between them.","To facilitate further research and provide a common evaluation platform, we introduce an experimental protocol and a benchmark dataset simulating such inconsistencies.","Our approach achieves state-of-the-art results in scene classification and promising outcomes in audio-visual discrepancies detection, highlighting its potential in content verification applications."],"url":"http://arxiv.org/abs/2405.00384v1","category":"cs.CV"}
{"created":"2024-05-01 07:20:16","title":"Nuclear mass predictions with anisotropic kernel ridge regression","abstract":"The anisotropic kernel ridge regression (AKRR) approach in nuclear mass predictions is developed by introducing the anisotropic kernel function into the kernel ridge regression (KRR) approach, without introducing new weight parameter or input in the training. A combination of double two-dimensional Gaussian kernel function is adopted, and the corresponding hyperparameters are optimized carefully by cross-validations. The anisotropic kernel shows cross-shape pattern, which highlights the correlations among the isotopes with the same proton number, and that among the isotones with the same neutron number. Significant improvements are achieved by the AKRR approach in both the interpolation and the extrapolation predictions of nuclear masses comparing with the original KRR approach.","sentences":["The anisotropic kernel ridge regression (AKRR) approach in nuclear mass predictions is developed by introducing the anisotropic kernel function into the kernel ridge regression (KRR) approach, without introducing new weight parameter or input in the training.","A combination of double two-dimensional Gaussian kernel function is adopted, and the corresponding hyperparameters are optimized carefully by cross-validations.","The anisotropic kernel shows cross-shape pattern, which highlights the correlations among the isotopes with the same proton number, and that among the isotones with the same neutron number.","Significant improvements are achieved by the AKRR approach in both the interpolation and the extrapolation predictions of nuclear masses comparing with the original KRR approach."],"url":"http://arxiv.org/abs/2405.00356v1","category":"nucl-th"}
{"created":"2024-05-01 02:05:27","title":"Beyond a Richardson-Gaudin mean-field: Slater-Condon rules and perturbation theory","abstract":"Richardson-Gaudin states provide a basis of the Hilbert space for strongly correlated electrons. In this study, optimal expressions for the transition density matrix elements between Richardson-Gaudin states are obtained with a cost comparable with the corresponding reduced density matrix elements. Analogues of the Slater-Condon rules are identified based on the number of near-zero singular values of the RG state overlap matrix. Finally, a perturbative approach is shown to be close in quality to a configuration interaction of Richardson-Gaudin states while being feasible to compute.","sentences":["Richardson-Gaudin states provide a basis of the Hilbert space for strongly correlated electrons.","In this study, optimal expressions for the transition density matrix elements between Richardson-Gaudin states are obtained with a cost comparable with the corresponding reduced density matrix elements.","Analogues of the Slater-Condon rules are identified based on the number of near-zero singular values of the RG state overlap matrix.","Finally, a perturbative approach is shown to be close in quality to a configuration interaction of Richardson-Gaudin states while being feasible to compute."],"url":"http://arxiv.org/abs/2405.00279v1","category":"physics.chem-ph"}
{"created":"2024-04-30 23:35:31","title":"Flexible multi-bunch-length operation for continuous-wave x-ray free-electron lasers","abstract":"The X-ray free-electron lasers (XFELs) are cutting-edge instruments pivotal in a broad range of fields, providing high-power X-ray pulses with durations spanning from femtoseconds to attoseconds. One of the critical challenges in XFEL facilities is the simultaneous accommodation of diverse requirements for XFEL operation modes and photon properties across different undulator lines. This paper proposes a dipole-kicker combination in the bunch compressors to vary the electron bunch length for the continuous-wave XFEL facilities driven by a superconducting linac. This method enables optimization of the electron bunch length on a per-bunch basis, tailored to each specific needs of each undulator. Through start-to-end simulations based on the parameters of the Shanghai high-repetition-rate XFEL and extreme light facility, we demonstrate the feasibility of this technique. The results show its effectiveness in enabling simultaneous operations of self-amplified spontaneous emission and externally seeded FEL across different undulator lines, ensuring optimal electron bunch compression for each undulator line.","sentences":["The X-ray free-electron lasers (XFELs) are cutting-edge instruments pivotal in a broad range of fields, providing high-power X-ray pulses with durations spanning from femtoseconds to attoseconds.","One of the critical challenges in XFEL facilities is the simultaneous accommodation of diverse requirements for XFEL operation modes and photon properties across different undulator lines.","This paper proposes a dipole-kicker combination in the bunch compressors to vary the electron bunch length for the continuous-wave XFEL facilities driven by a superconducting linac.","This method enables optimization of the electron bunch length on a per-bunch basis, tailored to each specific needs of each undulator.","Through start-to-end simulations based on the parameters of the Shanghai high-repetition-rate XFEL and extreme light facility, we demonstrate the feasibility of this technique.","The results show its effectiveness in enabling simultaneous operations of self-amplified spontaneous emission and externally seeded FEL across different undulator lines, ensuring optimal electron bunch compression for each undulator line."],"url":"http://arxiv.org/abs/2405.00245v1","category":"physics.acc-ph"}
{"created":"2024-04-30 20:47:35","title":"Resource-compact time-optimal quantum computation","abstract":"Fault-tolerant quantum computation enables reliable quantum computation but incurs a significant overhead from both time and resource perspectives. To reduce computation time, Austin G. Fowler proposed time-optimal quantum computation by constructing a quantum circuit for a fault-tolerant $T$ gate without probabilistic $S$ gate correction. In this work, we introduce a resource-compact quantum circuit that significantly reduces resource requirements by more than 60% for a fault-tolerant $T$ gate without probabilistic $S$ gate correction. Consequently, we present a quantum circuit that minimizes resource utilization for time-optimal quantum computation, demonstrating efficient time-optimal quantum computation. Additionally, we describe an efficient form involving initialization, CNOTs, and measurements, laying the foundation for the development of an efficient compiler for fault-tolerant quantum computation.","sentences":["Fault-tolerant quantum computation enables reliable quantum computation but incurs a significant overhead from both time and resource perspectives.","To reduce computation time, Austin G. Fowler proposed time-optimal quantum computation by constructing a quantum circuit for a fault-tolerant $T$ gate without probabilistic $S$ gate correction.","In this work, we introduce a resource-compact quantum circuit that significantly reduces resource requirements by more than 60% for a fault-tolerant $T$ gate without probabilistic $S$ gate correction.","Consequently, we present a quantum circuit that minimizes resource utilization for time-optimal quantum computation, demonstrating efficient time-optimal quantum computation.","Additionally, we describe an efficient form involving initialization, CNOTs, and measurements, laying the foundation for the development of an efficient compiler for fault-tolerant quantum computation."],"url":"http://arxiv.org/abs/2405.00191v1","category":"quant-ph"}
{"created":"2024-04-30 19:04:53","title":"EEvA: Fast Expert-Based Algorithms for Buffer Page Replacement","abstract":"Optimal page replacement is an important problem in efficient buffer management. The range of replacement strategies known in the literature varies from simple but efficient FIFO-based algorithms to more accurate but potentially costly methods tailored to specific data access patterns. The principal issue in adopting a pattern-specific replacement logic in a DB buffer manager is to guarantee non-degradation in general high-load regimes. In this paper, we propose a new family of page replacement algorithms for DB buffer manager which demonstrate a superior performance wrt competitors on custom data access patterns and imply a low computational overhead on TPC-C. We provide theoretical foundations and an extensive experimental study on the proposed algorithms which covers synthetic benchmarks and an implementation in an open-source DB kernel evaluated on TPC-C.","sentences":["Optimal page replacement is an important problem in efficient buffer management.","The range of replacement strategies known in the literature varies from simple but efficient FIFO-based algorithms to more accurate but potentially costly methods tailored to specific data access patterns.","The principal issue in adopting a pattern-specific replacement logic in a DB buffer manager is to guarantee non-degradation in general high-load regimes.","In this paper, we propose a new family of page replacement algorithms for DB buffer manager which demonstrate a superior performance wrt competitors on custom data access patterns and imply a low computational overhead on TPC-C. We provide theoretical foundations and an extensive experimental study on the proposed algorithms which covers synthetic benchmarks and an implementation in an open-source DB kernel evaluated on TPC-C."],"url":"http://arxiv.org/abs/2405.00154v1","category":"cs.DB"}
{"created":"2024-04-30 18:58:46","title":"An analytical formula for signal optimization in stimulated photon-photon scattering setup with three laser pulses","abstract":"We consider a setup to detect stimulated photon-photon scattering using high-power lasers. Signal photons are emitted from an overlap of the incoming intense laser pulses focused in vacuum from three sides. We derive and justify a general approximate analytical formula for the angular distribution and total yield of such signal photons in terms of the parameters of the incoming pulses, including their intensity, carrier frequencies, durations, focusing, polarizations, mutual orientation and overlap. Using the obtained formula a parametric study of the signal is carried out and optimization is performed.","sentences":["We consider a setup to detect stimulated photon-photon scattering using high-power lasers.","Signal photons are emitted from an overlap of the incoming intense laser pulses focused in vacuum from three sides.","We derive and justify a general approximate analytical formula for the angular distribution and total yield of such signal photons in terms of the parameters of the incoming pulses, including their intensity, carrier frequencies, durations, focusing, polarizations, mutual orientation and overlap.","Using the obtained formula a parametric study of the signal is carried out and optimization is performed."],"url":"http://arxiv.org/abs/2405.00151v1","category":"hep-ph"}
{"created":"2024-04-30 18:52:09","title":"A Robust Optimization Approach to Network Control Using Local Information Exchange","abstract":"Designing policies for a network of agents is typically done by formulating an optimization problem where each agent has access to state measurements of all the other agents in the network. Such policy designs with centralized information exchange result in optimization problems that are typically hard to solve, require establishing substantial communication links, and do not promote privacy since all information is shared among the agents. Designing policies based on arbitrary communication structures can lead to non-convex optimization problems which are typically NP-hard. In this work, we propose an optimization framework for decentralized policy designs. In contrast to the centralized information exchange, our approach requires only local communication exchange among the neighboring agents matching the physical coupling of the network. Thus, each agent only requires information from its direct neighbors, minimizing the need for excessive communication and promoting privacy amongst the agents. Using robust optimization techniques, we formulate a convex optimization problem with a loosely coupled structure that can be solved efficiently. We numerically demonstrate the efficacy of the proposed approach in energy management and supply chain applications. We show that the proposed approach leads to solutions that closely approximate those obtained by the centralized formulation only at a fraction of the computational effort.","sentences":["Designing policies for a network of agents is typically done by formulating an optimization problem where each agent has access to state measurements of all the other agents in the network.","Such policy designs with centralized information exchange result in optimization problems that are typically hard to solve, require establishing substantial communication links, and do not promote privacy since all information is shared among the agents.","Designing policies based on arbitrary communication structures can lead to non-convex optimization problems which are typically NP-hard.","In this work, we propose an optimization framework for decentralized policy designs.","In contrast to the centralized information exchange, our approach requires only local communication exchange among the neighboring agents matching the physical coupling of the network.","Thus, each agent only requires information from its direct neighbors, minimizing the need for excessive communication and promoting privacy amongst the agents.","Using robust optimization techniques, we formulate a convex optimization problem with a loosely coupled structure that can be solved efficiently.","We numerically demonstrate the efficacy of the proposed approach in energy management and supply chain applications.","We show that the proposed approach leads to solutions that closely approximate those obtained by the centralized formulation only at a fraction of the computational effort."],"url":"http://arxiv.org/abs/2405.00148v1","category":"math.OC"}
{"created":"2024-04-30 18:21:42","title":"A variational approach to sampling in diffusion processes","abstract":"We revisit the work of Mitter and Newton on an information-theoretic interpretation of Bayes' formula through the Gibbs variational principle. This formulation allowed them to pose nonlinear estimation for diffusion processes as a problem in stochastic optimal control, so that the posterior density of the signal given the observation path could be sampled by adding a drift to the signal process. We show that this control-theoretic approach to sampling provides a common mechanism underlying several distinct problems involving diffusion processes, specifically importance sampling using Feynman-Kac averages, time reversal, and Schr\\\"odinger bridges.","sentences":["We revisit the work of Mitter and Newton on an information-theoretic interpretation of Bayes' formula through the Gibbs variational principle.","This formulation allowed them to pose nonlinear estimation for diffusion processes as a problem in stochastic optimal control, so that the posterior density of the signal given the observation path could be sampled by adding a drift to the signal process.","We show that this control-theoretic approach to sampling provides a common mechanism underlying several distinct problems involving diffusion processes, specifically importance sampling using Feynman-Kac averages, time reversal, and Schr\\\"odinger bridges."],"url":"http://arxiv.org/abs/2405.00126v1","category":"math.OC"}
{"created":"2024-04-30 18:10:11","title":"Riesz Energy with a Radial External Field: When is the Equilibrium Support a Sphere?","abstract":"We consider Riesz energy problems with radial external fields. We study the question of whether or not the equilibrium is the uniform distribution on a sphere. We develop general necessary as well as general sufficient conditions on the external field that apply to powers of the Euclidean norm as well as certain Lennard--Jones type fields. Additionally, in the former case, we completely characterize the values of the power for which dimension reduction occurs in the sense that the support of the equilibrium measure becomes a sphere. We also briefly discuss the relation between these problems and certain constrained optimization problems. Our approach involves the Frostman characterization, the Funk--Hecke formula, and the calculus of hypergeometric functions.","sentences":["We consider Riesz energy problems with radial external fields.","We study the question of whether or not the equilibrium is the uniform distribution on a sphere.","We develop general necessary as well as general sufficient conditions on the external field that apply to powers of the Euclidean norm as well as certain Lennard--Jones type fields.","Additionally, in the former case, we completely characterize the values of the power for which dimension reduction occurs in the sense that the support of the equilibrium measure becomes a sphere.","We also briefly discuss the relation between these problems and certain constrained optimization problems.","Our approach involves the Frostman characterization, the Funk--Hecke formula, and the calculus of hypergeometric functions."],"url":"http://arxiv.org/abs/2405.00120v1","category":"math.CA"}
{"created":"2024-05-01 17:58:18","title":"Universal Bounds on CFT Distance Conjecture","abstract":"For any unitary conformal field theory in two dimensions with the central charge $c$, we prove that, if there is a nontrivial primary operator whose conformal dimension $\\Delta$ vanishes in some limit on the conformal manifold, the Zamolodchikov distance $t$ to the limit is infinite, the approach to this limit is exponential $\\Delta = \\exp(- \\alpha t +O(1) )$, and the decay rate obeys the universal bounds $c^{-1/2} \\leq \\alpha \\leq 1$. In the limit, we also find that an infinite tower of primary operators emerges without a gap above the vacuum and that the conformal field theory becomes locally a tensor product of a sigma-model in the large radius limit and a compact theory. As a corollary, we establish a part of the Distance Conjecture about gravitational theories in three-dimensional anti-de Sitter space. In particular, our bounds on $\\alpha$ indicate that the emergence of exponentially light particles is inevitable as the moduli field corresponding to $t$ rolls beyond the Planck scale along the steepest path and that this phenomenon can begin already at the curvature scale of the bulk geometry.","sentences":["For any unitary conformal field theory in two dimensions with the central charge $c$, we prove that, if there is a nontrivial primary operator whose conformal dimension $\\Delta$ vanishes in some limit on the conformal manifold, the Zamolodchikov distance $t$ to the limit is infinite, the approach to this limit is exponential $\\Delta = \\exp(- \\alpha t +O(1) )$, and the decay rate obeys the universal bounds $c^{-1/2} \\leq \\alpha \\leq 1$.","In the limit, we also find that an infinite tower of primary operators emerges without a gap above the vacuum and that the conformal field theory becomes locally a tensor product of a sigma-model in the large radius limit and a compact theory.","As a corollary, we establish a part of the Distance Conjecture about gravitational theories in three-dimensional anti-de Sitter space.","In particular, our bounds on $\\alpha$ indicate that the emergence of exponentially light particles is inevitable as the moduli field corresponding to $t$ rolls beyond the Planck scale along the steepest path and that this phenomenon can begin already at the curvature scale of the bulk geometry."],"url":"http://arxiv.org/abs/2405.00674v1","category":"hep-th"}
{"created":"2024-05-01 17:57:03","title":"Euclid preparation. LensMC, weak lensing cosmic shear measurement with forward modelling and Markov Chain Monte Carlo sampling","abstract":"LensMC is a weak lensing shear measurement method developed for Euclid and Stage-IV surveys. It is based on forward modelling to deal with convolution by a point spread function with comparable size to many galaxies; sampling the posterior distribution of galaxy parameters via Markov Chain Monte Carlo; and marginalisation over nuisance parameters for each of the 1.5 billion galaxies observed by Euclid. The scientific performance is quantified through high-fidelity images based on the Euclid Flagship simulations and emulation of the Euclid VIS images; realistic clustering with a mean surface number density of 250 arcmin$^{-2}$ ($I_{\\rm E}<29.5$) for galaxies, and 6 arcmin$^{-2}$ ($I_{\\rm E}<26$) for stars; and a diffraction-limited chromatic point spread function with a full width at half maximum of $0.^{\\!\\prime\\prime}2$ and spatial variation across the field of view. Objects are measured with a density of 90 arcmin$^{-2}$ ($I_{\\rm E}<26.5$) in 4500 deg$^2$. The total shear bias is broken down into measurement (our main focus here) and selection effects (which will be addressed elsewhere). We find: measurement multiplicative and additive biases of $m_1=(-3.6\\pm0.2)\\times10^{-3}$, $m_2=(-4.3\\pm0.2)\\times10^{-3}$, $c_1=(-1.78\\pm0.03)\\times10^{-4}$, $c_2=(0.09\\pm0.03)\\times10^{-4}$; a large detection bias with a multiplicative component of $1.2\\times10^{-2}$ and an additive component of $-3\\times10^{-4}$; and a measurement PSF leakage of $\\alpha_1=(-9\\pm3)\\times10^{-4}$ and $\\alpha_2=(2\\pm3)\\times10^{-4}$. When model bias is suppressed, the obtained measurement biases are close to Euclid requirement and largely dominated by undetected faint galaxies ($-5\\times10^{-3}$). Although significant, model bias will be straightforward to calibrate given the weak sensitivity.","sentences":["LensMC is a weak lensing shear measurement method developed for Euclid and Stage-IV surveys.","It is based on forward modelling to deal with convolution by a point spread function with comparable size to many galaxies; sampling the posterior distribution of galaxy parameters via Markov Chain Monte Carlo; and marginalisation over nuisance parameters for each of the 1.5 billion galaxies observed by Euclid.","The scientific performance is quantified through high-fidelity images based on the Euclid Flagship simulations and emulation of the Euclid VIS images; realistic clustering with a mean surface number density of 250 arcmin$^{-2}$ ($I_{\\rm E}<29.5$) for galaxies, and 6 arcmin$^{-2}$ ($I_{\\rm E}<26$) for stars; and a diffraction-limited chromatic point spread function with a full width at half maximum of $0.^{\\!\\prime\\prime}2$ and spatial variation across the field of view.","Objects are measured with a density of 90 arcmin$^{-2}$ ($I_{\\rm E}<26.5$) in 4500 deg$^2$.","The total shear bias is broken down into measurement (our main focus here) and selection effects (which will be addressed elsewhere).","We find: measurement multiplicative and additive biases of $m_1=(-3.6\\pm0.2)\\times10^{-3}$, $m_2=(-4.3\\pm0.2)\\times10^{-3}$, $c_1=(-1.78\\pm0.03)\\times10^{-4}$, $c_2=(0.09\\pm0.03)\\times10^{-4}$; a large detection bias with a multiplicative component of $1.2\\times10^{-2}$ and an additive component of $-3\\times10^{-4}$; and a measurement PSF leakage of $\\alpha_1=(-9\\pm3)\\times10^{-4}$ and $\\alpha_2=(2\\pm3)\\times10^{-4}$. When model bias is suppressed, the obtained measurement biases are close to Euclid requirement and largely dominated by undetected faint galaxies ($-5\\times10^{-3}$).","Although significant, model bias will be straightforward to calibrate given the weak sensitivity."],"url":"http://arxiv.org/abs/2405.00669v1","category":"astro-ph.CO"}
{"created":"2024-05-01 17:32:09","title":"Interplay between domain walls and magnetization curling induced by chemical modulations in cylindrical nanowires","abstract":"Cylindrical magnetic nanowires have been proposed as a means of storing and processing information in a 3D medium, based on the motion of domain walls~(DWs). Introducing short chemical modulations in such wires would allow for reliable digital control of DWs. Here, we outline the intricate physics of the interaction of domain walls with modulations to control their motion, combining micromagnetic simulations and experimental evidence. This interaction combines a long-range moderate magnetostatic repulsion with a local energy well. The latter depends on the respective circulation sense of magnetization in the domain wall and modulation. We also show that a modulation has the ability to switch the internal circulation of a DW upon its propagation, thereby acting as a polarizing component and opening the possibility to exploit not only the position of walls, but also their internal structure.","sentences":["Cylindrical magnetic nanowires have been proposed as a means of storing and processing information in a 3D medium, based on the motion of domain walls~(DWs).","Introducing short chemical modulations in such wires would allow for reliable digital control of DWs.","Here, we outline the intricate physics of the interaction of domain walls with modulations to control their motion, combining micromagnetic simulations and experimental evidence.","This interaction combines a long-range moderate magnetostatic repulsion with a local energy well.","The latter depends on the respective circulation sense of magnetization in the domain wall and modulation.","We also show that a modulation has the ability to switch the internal circulation of a DW upon its propagation, thereby acting as a polarizing component and opening the possibility to exploit not only the position of walls, but also their internal structure."],"url":"http://arxiv.org/abs/2405.00652v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-01 17:10:18","title":"Tomography of flavoured leptogenesis with primordial blue gravitational waves","abstract":"We explore a scenario where an early epoch of matter domination is driven by the mass scale $M_N$ of the right-handed neutrinos, which also characterizes the different flavour regimes of leptogenesis. Such a matter-domination epoch gives rise to peculiar spectral imprints on primordial Gravitational Waves (GWs) produced during inflation. We point out that the characteristic spectral features are detectable in multiple frequency bands with current and future GW experiments in case of Blue GWs (BGWs) described by a power-law with a positive spectral index $(n_T >0)$ and an amplitude compatible with Cosmic Microwave Background (CMB) measurements. We find that the three-flavour leptogenesis regime with $M_N \\lesssim 10^9~{\\rm GeV}$ imprints BGWs more prominently than the two-flavour and one-flavour regimes characterized by a higher right-neutrino mass scale. In particular, a two-flavour (three-flavour) leptogenesis regime is expected to leave distinct imprints in the mHz ($\\mu$Hz) band. Moreover, we translate the current Big Bang Nucleosynthesis (BBN) and LIGO limits on the GW energy density into constraints on the flavour leptogenesis parameter space for different GW spectral index $n_T$. Interestingly, a three-flavour leptogenesis regime can offer a unique signal testable in the next LIGO run with a correlated signature in the PTA frequency band with an amplitude comparable to the one expected from supermassive black holes.","sentences":["We explore a scenario where an early epoch of matter domination is driven by the mass scale $M_N$ of the right-handed neutrinos, which also characterizes the different flavour regimes of leptogenesis.","Such a matter-domination epoch gives rise to peculiar spectral imprints on primordial Gravitational Waves (GWs) produced during inflation.","We point out that the characteristic spectral features are detectable in multiple frequency bands with current and future GW experiments in case of Blue GWs (BGWs) described by a power-law with a positive spectral index $(n_T >0)$ and an amplitude compatible with Cosmic Microwave Background (CMB) measurements.","We find that the three-flavour leptogenesis regime with $M_N \\lesssim 10^9~{\\rm","GeV}$ imprints BGWs more prominently than the two-flavour and one-flavour regimes characterized by a higher right-neutrino mass scale.","In particular, a two-flavour (three-flavour) leptogenesis regime is expected to leave distinct imprints in the mHz ($\\mu$Hz) band.","Moreover, we translate the current Big Bang Nucleosynthesis (BBN) and LIGO limits on the GW energy density into constraints on the flavour leptogenesis parameter space for different GW spectral index $n_T$. Interestingly, a three-flavour leptogenesis regime can offer a unique signal testable in the next LIGO run with a correlated signature in the PTA frequency band with an amplitude comparable to the one expected from supermassive black holes."],"url":"http://arxiv.org/abs/2405.00641v1","category":"hep-ph"}
{"created":"2024-05-01 16:34:39","title":"X-ray spectral properties of dust-obscured galaxies in the XMM-SERVS coverage of the XMM-LSS field","abstract":"With an aim to unveil the population of obscured AGN hosted in high-$z$ dust-obscured galaxies (DOGs), we performed X-ray spectral study of $34$ DOGs ($0.59$ $\\leq$ $z$ $\\leq$ $4.65$) lying within $5.3$ deg$^{2}$ of the XMM-SERVS coverage in the XMM-LSS field. To improve the spectral quality of individual sources, we combined all the existing $\\textit{XMM-Newton}$ data and also included $\\textit{Chandra}$/ACIS data, whenever available. We find that the X-ray spectra of our DOGs can be fitted with a simple absorbed power law or with a physically-motivated BORUS02 model. The line-of-sight column densities ($N_{\\textrm{H}}$) in our sources span across a wide range ($1.02$ $\\times$ $10^{22}$ cm$^{-2}$ $\\leq$ $N_{\\textrm{H}}$ $\\leq$ $1.21$ $\\times$ $10^{24}$ cm$^{-2}$), with a substantial fraction ($\\sim$ $17.6$ per cent) of them being heavily obscured ($N_{\\textrm{H}}$ $\\geq$ $10^{23}$ cm$^{-2}$). We also identified one new CT-AGN candidate, yielding the CT-AGN fraction in our sample to be only $3$ per cent. The absorption-corrected $2.0-10$ keV X-ray luminosities of our sources ($2.00~\\times~10^{43}$ erg s$^{-1}$ $\\leq$ $L_{\\textrm{2-10 keV}}^{\\textrm{int}}$ $\\leq$ $6.17~\\times~10^{45}$ erg s$^{-1}$) suggest them to be luminous quasars. The $N_{\\textrm{H}}$ versus Eddington ratio diagnostic plot infers that our sample consists of a heterogeneous population that includes a small fraction ($\\sim$ $12$ per cent) of DOGs belonging to an early phase (Hot DOGs) during which accretion and obscuration peaks, while the remaining DOGs belong to an intermediate or late phase during which radiative feedback from the dominant AGN blows away surrounding obscuring material.","sentences":["With an aim to unveil the population of obscured AGN hosted in high-$z$ dust-obscured galaxies (DOGs), we performed X-ray spectral study of $34$ DOGs ($0.59$ $\\leq$ $z$ $\\leq$ $4.65$) lying within $5.3$ deg$^{2}$ of the XMM-SERVS coverage in the XMM-LSS field.","To improve the spectral quality of individual sources, we combined all the existing $\\textit{XMM-Newton}$ data and also included $\\textit{Chandra}$/ACIS data, whenever available.","We find that the X-ray spectra of our DOGs can be fitted with a simple absorbed power law or with a physically-motivated BORUS02 model.","The line-of-sight column densities ($N_{\\textrm{H}}$) in our sources span across a wide range ($1.02$ $\\times$ $10^{22}$ cm$^{-2}$ $\\leq$ $N_{\\textrm{H}}$ $\\leq$ $1.21$ $\\times$ $10^{24}$ cm$^{-2}$), with a substantial fraction ($\\sim$ $17.6$ per cent) of them being heavily obscured ($N_{\\textrm{H}}$ $\\geq$ $10^{23}$ cm$^{-2}$).","We also identified one new CT-AGN candidate, yielding the CT-AGN fraction in our sample to be only $3$ per cent.","The absorption-corrected $2.0-10$ keV X-ray luminosities of our sources ($2.00~\\times~10^{43}$ erg s$^{-1}$ $\\leq$ $L_{\\textrm{2-10 keV}}^{\\textrm{int}}$ $\\leq$ $6.17~\\times~10^{45}$ erg s$^{-1}$) suggest them to be luminous quasars.","The $N_{\\textrm{H}}$ versus Eddington ratio diagnostic plot infers that our sample consists of a heterogeneous population that includes a small fraction ($\\sim$ $12$ per cent) of DOGs belonging to an early phase (Hot DOGs) during which accretion and obscuration peaks, while the remaining DOGs belong to an intermediate or late phase during which radiative feedback from the dominant AGN blows away surrounding obscuring material."],"url":"http://arxiv.org/abs/2405.00613v1","category":"astro-ph.GA"}
{"created":"2024-05-01 14:50:51","title":"Supermassive gauginos in supergravity inflation with high-scale SUSY breaking","abstract":"A model of supergravity inflation we recently proposed can produce slow roll inflation and a realistic spectrum of particles even without F-term supersymmetry breaking. Supersymmetry is broken only by a D-term induced by a recently discovered new type of Fayet-Iliopoulos (FI) term. Almost all supersymmetric partners of the standard model fields can get masses as high as the inflationary Hubble scale. The exception is gauginos, for which the vanishing of F-terms implies an exact cancellation that keeps their masses exactly zero. To cure this problem without spoiling the simplicity of our model we introduce a new term that further enlarges the space of supergravity effective actions. It is an F-term that, similarly to the new FI term, becomes singular in the supersymmetric limit. We show that this term can produce large gaugino masses without altering the spectrum of other states and without lowering the cutoff of the effecive theory.","sentences":["A model of supergravity inflation we recently proposed can produce slow roll inflation and a realistic spectrum of particles even without F-term supersymmetry breaking.","Supersymmetry is broken only by a D-term induced by a recently discovered new type of Fayet-Iliopoulos (FI) term.","Almost all supersymmetric partners of the standard model fields can get masses as high as the inflationary Hubble scale.","The exception is gauginos, for which the vanishing of F-terms implies an exact cancellation that keeps their masses exactly zero.","To cure this problem without spoiling the simplicity of our model we introduce a new term that further enlarges the space of supergravity effective actions.","It is an F-term that, similarly to the new FI term, becomes singular in the supersymmetric limit.","We show that this term can produce large gaugino masses without altering the spectrum of other states and without lowering the cutoff of the effecive theory."],"url":"http://arxiv.org/abs/2405.00551v1","category":"hep-th"}
{"created":"2024-05-01 14:48:18","title":"Spin-spin correlators on the $\u03b2$/$\u03b2^{\\star}$ boundaries in 2D Ising-like models: non-universality in the scaling region","abstract":"In this work, we investigate quantitative properties of correlation functions on the boundaries between two 2D Ising-like models with dual parameters $\\beta$ and $\\beta^{\\star}$. Spin-spin correlators in such constructions without reflection symmetry with respect to transnational-invariant directions are usually represented as $2\\times 2$ block Toeplitz determinants which are normally significantly harder than the scalar ($1\\times 1$-blocked) versions. Nevertheless, we show that for the specific $\\beta/\\beta^{\\star}$ boundaries considered in this work, the symbol matrices allow explicit commutative Wiener-Hopf factorizations. However, the Wiener-Hopf factors at different $z$ do not commute. We will show that due to this non-commutativity, \"logarithmic divergences\" and non-universal short distance contributions in the Wiener-Hopf factors fail to factorize out completely in the re-scaled correlators. This leads to non-universality of the leading large $r$ asymptotics at the order $\\frac {e^{-r}}{r^{\\frac{3}{2}}}$, even when the constant terms are re-scaled to be the same.","sentences":["In this work, we investigate quantitative properties of correlation functions on the boundaries between two 2D Ising-like models with dual parameters $\\beta$ and $\\beta^{\\star}$. Spin-spin correlators in such constructions without reflection symmetry with respect to transnational-invariant directions are usually represented as $2\\times 2$ block Toeplitz determinants which are normally significantly harder than the scalar ($1\\times 1$-blocked) versions.","Nevertheless, we show that for the specific $\\beta/\\beta^{\\star}$ boundaries considered in this work, the symbol matrices allow explicit commutative Wiener-Hopf factorizations.","However, the Wiener-Hopf factors at different $z$ do not commute.","We will show that due to this non-commutativity, \"logarithmic divergences\" and non-universal short distance contributions in the Wiener-Hopf factors fail to factorize out completely in the re-scaled correlators.","This leads to non-universality of the leading large $r$ asymptotics at the order $\\frac {e^{-r}}{r^{\\frac{3}{2}}}$, even when the constant terms are re-scaled to be the same."],"url":"http://arxiv.org/abs/2405.00550v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-01 14:11:52","title":"Bayesian Varying-Effects Vector Autoregressive Models for Inference of Brain Connectivity Networks and Covariate Effects in Pediatric Traumatic Brain Injury","abstract":"In this paper, we develop an analytical approach for estimating brain connectivity networks that accounts for subject heterogeneity. More specifically, we consider a novel extension of a multi-subject Bayesian vector autoregressive model that estimates group-specific directed brain connectivity networks and accounts for the effects of covariates on the network edges. We adopt a flexible approach, allowing for (possibly) non-linear effects of the covariates on edge strength via a novel Bayesian nonparametric prior that employs a weighted mixture of Gaussian processes. For posterior inference, we achieve computational scalability by implementing a variational Bayes scheme. Our approach enables simultaneous estimation of group-specific networks and selection of relevant covariate effects. We show improved performance over competing two-stage approaches on simulated data. We apply our method on resting-state fMRI data from children with a history of traumatic brain injury and healthy controls to estimate the effects of age and sex on the group-level connectivities. Our results highlight differences in the distribution of parent nodes. They also suggest alteration in the relation of age, with peak edge strength in children with traumatic brain injury (TBI), and differences in effective connectivity strength between males and females.","sentences":["In this paper, we develop an analytical approach for estimating brain connectivity networks that accounts for subject heterogeneity.","More specifically, we consider a novel extension of a multi-subject Bayesian vector autoregressive model that estimates group-specific directed brain connectivity networks and accounts for the effects of covariates on the network edges.","We adopt a flexible approach, allowing for (possibly) non-linear effects of the covariates on edge strength via a novel Bayesian nonparametric prior that employs a weighted mixture of Gaussian processes.","For posterior inference, we achieve computational scalability by implementing a variational Bayes scheme.","Our approach enables simultaneous estimation of group-specific networks and selection of relevant covariate effects.","We show improved performance over competing two-stage approaches on simulated data.","We apply our method on resting-state fMRI data from children with a history of traumatic brain injury and healthy controls to estimate the effects of age and sex on the group-level connectivities.","Our results highlight differences in the distribution of parent nodes.","They also suggest alteration in the relation of age, with peak edge strength in children with traumatic brain injury (TBI), and differences in effective connectivity strength between males and females."],"url":"http://arxiv.org/abs/2405.00535v1","category":"stat.ME"}
{"created":"2024-05-01 13:57:41","title":"Temperature dependence of Coherent versus spontaneous Raman Scattering","abstract":"Due to their sub picosecond temporal resolution, coherent Raman spectroscopies have been proposed as a viable extension of Spontaneous Raman (SR) thermometry, to determine dynamics of mode specific vibrational energy content during out of equilibrium molecular processes. Here we show that the presence of multiple laser fields stimulating the vibrational coherences introduces additional quantum pathways, resulting in destructive interference. This ultimately reduces the thermal sensitivity of single spectral lines, nullifying it for harmonic vibrations and temperature independent polarizability. We demonstrate how harnessing anharmonic signatures such as vibrational hot bands enables coherent Raman thermometry.","sentences":["Due to their sub picosecond temporal resolution, coherent Raman spectroscopies have been proposed as a viable extension of Spontaneous Raman (SR) thermometry, to determine dynamics of mode specific vibrational energy content during out of equilibrium molecular processes.","Here we show that the presence of multiple laser fields stimulating the vibrational coherences introduces additional quantum pathways, resulting in destructive interference.","This ultimately reduces the thermal sensitivity of single spectral lines, nullifying it for harmonic vibrations and temperature independent polarizability.","We demonstrate how harnessing anharmonic signatures such as vibrational hot bands enables coherent Raman thermometry."],"url":"http://arxiv.org/abs/2405.00521v1","category":"physics.optics"}
{"created":"2024-05-01 13:41:02","title":"Polarization Perspectives on Hercules X-1: Further Constraining the Geometry","abstract":"We conduct a comprehensive analysis of the accreting X-ray pulsar, Hercules X-1, utilizing data from IXPE and NuSTAR. IXPE performed five observations of Her X-1, consisting of three in the Main-on state and two in the Short-on state. Our time-resolved analysis uncovers the linear correlations between the flux and polarization degree as well as the pulse fraction and polarization degree. Geometry parameters are rigorously constrained by fitting the phase-resolved modulations of Cyclotron Resonance Scattering Feature and polarization angle with a simple dipole model and Rotating Vector Model respectively, yielding roughly consistent results. The changes of $\\chi_{\\rm p}$ (the position angle of the pulsar's spin axis on the plane of the sky) between different Main-on observations suggest the possible forced precession of the neutron star crust. Furthermore, a linear association between the energy of Cyclotron Resonance Scattering Feature and polarization angle implies the prevalence of a dominant dipole magnetic field, and their phase-resolved modulations likely arise from viewing angle effects.","sentences":["We conduct a comprehensive analysis of the accreting X-ray pulsar, Hercules X-1, utilizing data from IXPE and NuSTAR.","IXPE performed five observations of Her X-1, consisting of three in the Main-on state and two in the Short-on state.","Our time-resolved analysis uncovers the linear correlations between the flux and polarization degree as well as the pulse fraction and polarization degree.","Geometry parameters are rigorously constrained by fitting the phase-resolved modulations of Cyclotron Resonance Scattering Feature and polarization angle with a simple dipole model and Rotating Vector Model respectively, yielding roughly consistent results.","The changes of $\\chi_{\\rm p}$ (the position angle of the pulsar's spin axis on the plane of the sky) between different Main-on observations suggest the possible forced precession of the neutron star crust.","Furthermore, a linear association between the energy of Cyclotron Resonance Scattering Feature and polarization angle implies the prevalence of a dominant dipole magnetic field, and their phase-resolved modulations likely arise from viewing angle effects."],"url":"http://arxiv.org/abs/2405.00509v1","category":"astro-ph.HE"}
{"created":"2024-05-01 13:30:21","title":"JWST meets Chandra: a large population of Compton thick, feedback-free, and X-ray weak AGN, with a sprinkle of SNe","abstract":"We investigate the X-ray properties of a large sample of 71 broad line and narrow line AGN at 2<z<11 discovered by JWST in the GOODS fields, which have the deepest Chandra observations ever obtained. Despite the widespread presence of AGN signatures in their rest-optical and -UV spectra, the vast majority of them is X-ray undetected. The stacked X-ray data of the non-detected sources also results in a non-detection. The upper limit on the X-ray emission for many of these AGN is one or even two orders of magnitude lower than expected from a standard AGN SED. Heavy X-ray absorption by clouds with large (Compton thick) column density and low dust content, such as the Broad Line Region (BLR) clouds, can explain the X-ray weakness. In this scenario the BLR covering factor should be much larger than in low-z AGN or luminous quasar; this is supported by the larger equivalent width of the broad component of Halpha in JWST-selected AGN. We also find that the JWST-discovered AGN lack the prominent, fast outflows characterizing low-z AGN and luminous quasars, suggesting that, in JWST-selected AGN, dense gas lingers in the nuclear region, resulting in large covering factors. We also note that a large fraction of JWST-selected AGN match the definition of NLSy1, typically characterized by a steep X-ray spectrum, and this can further contribute to their observed weakness at high-z. Finally, we discuss that the broad Balmer lines used to identify type 1 AGN cannot be ascribed to Very Massive Stars, Tidal Disruption Events, or Supernovae, although we show that a minority of the faintest broad lines could potentially be associated with the echo of superluminous SNe or TDE. Scenarios in which the broad lines are ascribed to galactic outflows are also untenable. We emphasize that confirming any of the scenarios discussed above will require X-ray missions more sensitive than Chandra. (abridged)","sentences":["We investigate the X-ray properties of a large sample of 71 broad line and narrow line AGN at 2<z<11 discovered by JWST in the GOODS fields, which have the deepest Chandra observations ever obtained.","Despite the widespread presence of AGN signatures in their rest-optical and -UV spectra, the vast majority of them is X-ray undetected.","The stacked X-ray data of the non-detected sources also results in a non-detection.","The upper limit on the X-ray emission for many of these AGN is one or even two orders of magnitude lower than expected from a standard AGN SED.","Heavy X-ray absorption by clouds with large (Compton thick) column density and low dust content, such as the Broad Line Region (BLR) clouds, can explain the X-ray weakness.","In this scenario the BLR covering factor should be much larger than in low-z AGN or luminous quasar; this is supported by the larger equivalent width of the broad component of Halpha in JWST-selected AGN.","We also find that the JWST-discovered AGN lack the prominent, fast outflows characterizing low-z AGN and luminous quasars, suggesting that, in JWST-selected AGN, dense gas lingers in the nuclear region, resulting in large covering factors.","We also note that a large fraction of JWST-selected AGN match the definition of NLSy1, typically characterized by a steep X-ray spectrum, and this can further contribute to their observed weakness at high-z.","Finally, we discuss that the broad Balmer lines used to identify type 1 AGN cannot be ascribed to Very Massive Stars, Tidal Disruption Events, or Supernovae, although we show that a minority of the faintest broad lines could potentially be associated with the echo of superluminous SNe or TDE.","Scenarios in which the broad lines are ascribed to galactic outflows are also untenable.","We emphasize that confirming any of the scenarios discussed above will require X-ray missions more sensitive than Chandra.","(abridged)"],"url":"http://arxiv.org/abs/2405.00504v1","category":"astro-ph.GA"}
{"created":"2024-05-01 13:11:08","title":"Stochastic Averaging of Radiative Transfer Coefficients for Relativistic Electrons","abstract":"Synchrotron emissivities, absorptivities, and Faraday rotation and conversion coefficients are needed in modeling a variety of astrophysical sources, including Event Horizon Telescope (EHT) sources. We develop a method for estimating transfer coefficients that exploits their linear dependence on the electron distribution function, decomposing the distribution function into a sum of parts each of whose emissivity can be calculated easily. We refer to this procedure as stochastic averaging and apply it in two contexts. First, we use it to estimate the emissivity of an isotropic $\\kappa$ distribution function with a high energy cutoff. The resulting coefficients can be evaluated efficiently enough to be used directly in ray-tracing calculations, and we provide an example calculation. Second, we use stochastic averaging to assess the effect of subgrid turbulence on the volume-averaged emissivity and along the way provide a prescription for a turbulent emissivity. We find that for parameters appropriate to EHT sources turbulence reduces the emissivity slightly. In the infrared turbulence can dramatically increase the emissivity.","sentences":["Synchrotron emissivities, absorptivities, and Faraday rotation and conversion coefficients are needed in modeling a variety of astrophysical sources, including Event Horizon Telescope (EHT) sources.","We develop a method for estimating transfer coefficients that exploits their linear dependence on the electron distribution function, decomposing the distribution function into a sum of parts each of whose emissivity can be calculated easily.","We refer to this procedure as stochastic averaging and apply it in two contexts.","First, we use it to estimate the emissivity of an isotropic $\\kappa$ distribution function with a high energy cutoff.","The resulting coefficients can be evaluated efficiently enough to be used directly in ray-tracing calculations, and we provide an example calculation.","Second, we use stochastic averaging to assess the effect of subgrid turbulence on the volume-averaged emissivity and along the way provide a prescription for a turbulent emissivity.","We find that for parameters appropriate to EHT sources turbulence reduces the emissivity slightly.","In the infrared turbulence can dramatically increase the emissivity."],"url":"http://arxiv.org/abs/2405.00499v1","category":"astro-ph.HE"}
{"created":"2024-05-01 13:06:07","title":"Study of $B_{(s)}$ meson decays to $D_{0}^{\\ast}(2300) ,D_{s0}^{\\ast}(2317) , D_{s1}(2460)$ and $D_{s1}(2536)$ within the covariant light-front approach","abstract":"In this work, we investigate the form factors of the transitions $B_{(s)} \\to D_{0}^{\\ast}(2300),D_{s0}^{\\ast}(2317),$ $ D_{s1}(2460) $ and $ D_{s1}(2536)$ in the covariant light-front quark model (CLFQM), where these final states are considered as P-wave excited charmed mesons. In order to obtain the form factors for the physical transition processes, we need to extend these form factors from the space-like region to the time-like region. The $q^{2}$-dependence for each transition form factor is also plotted. Then, combined with those form factors, the branching ratios of the two-body nonleptonic decays $B_{(s)}\\to D^*_{(s)0}(2300,2317)M, D_{s1}(2460,2536)M$ with $M$ being a light pseudoscalar (vector) meson or a charmed meson are calculated by considering the QCD radiative corrections to the hadronic matrix elements with the QCD factorization approach. Most of our predictions are comparable to the results given by other theoretical approaches and the present available data.","sentences":["In this work, we investigate the form factors of the transitions $B_{(s)} \\to D_{0}^{\\ast}(2300),D_{s0}^{\\ast}(2317),$ $ D_{s1}(2460) $ and $ D_{s1}(2536)$ in the covariant light-front quark model (CLFQM), where these final states are considered as P-wave excited charmed mesons.","In order to obtain the form factors for the physical transition processes, we need to extend these form factors from the space-like region to the time-like region.","The $q^{2}$-dependence for each transition form factor is also plotted.","Then, combined with those form factors, the branching ratios of the two-body nonleptonic decays $B_{(s)}\\to D^*_{(s)0}(2300,2317)M, D_{s1}(2460,2536)M$ with $M$ being a light pseudoscalar (vector) meson or a charmed meson are calculated by considering the QCD radiative corrections to the hadronic matrix elements with the QCD factorization approach.","Most of our predictions are comparable to the results given by other theoretical approaches and the present available data."],"url":"http://arxiv.org/abs/2405.00496v1","category":"hep-ph"}
{"created":"2024-05-01 11:02:00","title":"Quark spin-orbit correlations in spin-0 and spin-1 mesons using the light-front quark model","abstract":"We have investigated the spin-orbital angular momentum correlations for the active quark inside the light and heavy mesons for both the spin-0 and spin-1 cases. These correlations can be derived from the generalised transverse momentum dependent distributions (GTMDs) as well as the generalised parton distributions (GPDs). We employ the overlap representation of light-front wave functions in the light-front quark model (LFQM) to calculate our analytical results. The dependence of spin-orbit correlations (SOCs) on the longitudinal momentum fraction $x$ as well as the transverse momentum dependence $\\mathbf{k}_{\\perp}$ has been graphically presented. Even though the SOCs have already been studied for the spin-0 pions and kaons in other approaches, no calculations for the other light and heavy spin-0 mesons have been reported in literature. Further, the correlations for any of the light and heavy spin-1 mesons have been studied for the first time in the present work.","sentences":["We have investigated the spin-orbital angular momentum correlations for the active quark inside the light and heavy mesons for both the spin-0 and spin-1 cases.","These correlations can be derived from the generalised transverse momentum dependent distributions (GTMDs) as well as the generalised parton distributions (GPDs).","We employ the overlap representation of light-front wave functions in the light-front quark model (LFQM) to calculate our analytical results.","The dependence of spin-orbit correlations (SOCs) on the longitudinal momentum fraction $x$ as well as the transverse momentum dependence $\\mathbf{k}_{\\perp}$ has been graphically presented.","Even though the SOCs have already been studied for the spin-0 pions and kaons in other approaches, no calculations for the other light and heavy spin-0 mesons have been reported in literature.","Further, the correlations for any of the light and heavy spin-1 mesons have been studied for the first time in the present work."],"url":"http://arxiv.org/abs/2405.00446v1","category":"hep-ph"}
{"created":"2024-05-01 11:01:39","title":"Study of vector and axial-vector form factors and the decay parameters for the semileptonic hyperon decays","abstract":"Using the standard parametrization of the dipole form, we have studied the vulnerability of $Q^2$ on the vector form factors ($f_i^{B_iB_f}(Q^2)$) and axial-vector form factors ($g_i^{B_iB_f}(Q^2)$), $i=1,2,3$ computed for the semileptonic $B_i \\rightarrow B_f l \\bar{\\nu}$ decays for hyperons in the framework of chiral constituent quark model ($\\chi$CQM). Both, strangeness changing as well as strangeness conserving decays have been examined. We also present the dependence of the ratio of hyperon semileptonic decay constants $g_1(Q^2)/f_1(Q^2)$ for these decays. Further, we calculate the CKM matrix elements $V_{ud}$ from strangeness conserving and $V_{us}$ from strangeness changing hyperon decays.","sentences":["Using the standard parametrization of the dipole form, we have studied the vulnerability of $Q^2$ on the vector form factors ($f_i^{B_iB_f}(Q^2)$) and axial-vector form factors ($g_i^{B_iB_f}(Q^2)$), $i=1,2,3$ computed for the semileptonic $B_i \\rightarrow B_f l \\bar{\\nu}$ decays for hyperons in the framework of chiral constituent quark model ($\\chi$CQM).","Both, strangeness changing as well as strangeness conserving decays have been examined.","We also present the dependence of the ratio of hyperon semileptonic decay constants $g_1(Q^2)/f_1(Q^2)$ for these decays.","Further, we calculate the CKM matrix elements $V_{ud}$ from strangeness conserving and $V_{us}$ from strangeness changing hyperon decays."],"url":"http://arxiv.org/abs/2405.00444v1","category":"hep-ph"}
{"created":"2024-05-01 10:59:48","title":"$1/f^\u03b1$ noise in the Robin Hood model","abstract":"We consider the Robin Hood dynamics, a one-dimensional extremal self-organized critical model that describes the evolution of low-temperature creep. One of the key quantities is the time evolution of the state variable (force noise). To understand the temporal correlations, we compute the power spectra of the local force fluctuations and apply finite-size scaling to get scaling functions and critical exponents. We find a signature of the $1/f^{\\alpha}$ noise for the local force with a nontrivial value of the spectral exponent $0< \\alpha < 2$. We also examine temporal fluctuations in the position of the extremal site and a local activity signal. We present results for different local interaction rules of the model.","sentences":["We consider the Robin Hood dynamics, a one-dimensional extremal self-organized critical model that describes the evolution of low-temperature creep.","One of the key quantities is the time evolution of the state variable (force noise).","To understand the temporal correlations, we compute the power spectra of the local force fluctuations and apply finite-size scaling to get scaling functions and critical exponents.","We find a signature of the $1/f^{\\alpha}$ noise for the local force with a nontrivial value of the spectral exponent $0< \\alpha < 2$.","We also examine temporal fluctuations in the position of the extremal site and a local activity signal.","We present results for different local interaction rules of the model."],"url":"http://arxiv.org/abs/2405.00443v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-01 10:34:58","title":"Quantum algorithms for N-1 security in power grids","abstract":"In recent years, the supply and demand of electricity has significantly increased. As a result, the interconnecting grid infrastructure has required (and will continue to require) further expansion, while allowing for rapid resolution of unforeseen failures. Energy grid operators strive for networks that satisfy different levels of security requirements. In the case of N-1 security for medium voltage networks, the goal is to ensure the continued provision of electricity in the event of a single-link failure. However, the process of determining if networks are N-1 secure is known to scale polynomially in the network size. This poses restrictions if we increase our demand of the network. In that case, more computationally hard cases will occur in practice and the computation time also increases significantly. In this work, we explore the potential of quantum computers to provide a more scalable solution. In particular, we consider gate-based quantum computing, quantum annealing, and photonic quantum computing.","sentences":["In recent years, the supply and demand of electricity has significantly increased.","As a result, the interconnecting grid infrastructure has required (and will continue to require) further expansion, while allowing for rapid resolution of unforeseen failures.","Energy grid operators strive for networks that satisfy different levels of security requirements.","In the case of N-1 security for medium voltage networks, the goal is to ensure the continued provision of electricity in the event of a single-link failure.","However, the process of determining if networks are N-1 secure is known to scale polynomially in the network size.","This poses restrictions if we increase our demand of the network.","In that case, more computationally hard cases will occur in practice and the computation time also increases significantly.","In this work, we explore the potential of quantum computers to provide a more scalable solution.","In particular, we consider gate-based quantum computing, quantum annealing, and photonic quantum computing."],"url":"http://arxiv.org/abs/2405.00434v1","category":"quant-ph"}
{"created":"2024-05-01 09:49:23","title":"Four loop renormalization in six dimensions using Forcer","abstract":"We employ the Forcer algorithm to renormalize a variety of six dimensional field theories to four loops. In order to achieve this we construct the Forcer master integrals in six dimensions from their four dimensional counterparts by using the Tarasov method. The $\\epsilon$ expansion of the six dimensional masters are determined up to weight $9$ where $d$ $=$ $6$ $-$ $2\\epsilon$. By applying the Forcer routine the four loop MSbar renormalization of $\\phi^3$ theory is reproduced before gauge theories are considered. The renormalization of these theories is also determined in the MOMt scheme. For instance the absence of $\\zeta_4$ and $\\zeta_6$ is confirmed to five loops in the MOMt renormalization of $\\phi^3$ theory. We also evaluate the three loop $\\beta$-function of the gauge coupling in six dimensional QCD.","sentences":["We employ the Forcer algorithm to renormalize a variety of six dimensional field theories to four loops.","In order to achieve this we construct the Forcer master integrals in six dimensions from their four dimensional counterparts by using the Tarasov method.","The $\\epsilon$ expansion of the six dimensional masters are determined up to weight $9$ where $d$ $=$ $6$ $-$ $2\\epsilon$. By applying the Forcer routine the four loop MSbar renormalization of $\\phi^3$ theory is reproduced before gauge theories are considered.","The renormalization of these theories is also determined in the MOMt scheme.","For instance the absence of $\\zeta_4$ and $\\zeta_6$ is confirmed to five loops in the MOMt renormalization of $\\phi^3$ theory.","We also evaluate the three loop $\\beta$-function of the gauge coupling in six dimensional QCD."],"url":"http://arxiv.org/abs/2405.00413v1","category":"hep-th"}
{"created":"2024-05-01 09:33:18","title":"Unconventional pairing in Ising superconductors: Application to monolayer NbSe$_2$","abstract":"The presence of a non-centrosymmetric crystal structure and in-plane mirror symmetry allows an Ising spin-orbit coupling to form in some two-dimensional materials. Examples include transition metal dichalcogenide superconductors like monolayer NbSe$_2$, MoS$_2$, TaS$_2$, and PbTe$_2$, where a nontrivial nature of the superconducting state is currently being explored. In this study, we develop a microscopic formalism for Ising superconductors that captures the superconducting instability arising from a momentum-dependent spin- and charge-fluctuation-mediated pairing interaction. We apply our pairing model to the electronic structure of monolayer NbSe$_2$, where first-principles calculations reveal the presence of strong paramagnetic fluctuations. Our calculations provide a quantitative measure of the mixing between the even- and odd-parity superconducting states and its variation with Coulomb interaction. Further, numerical analysis in the presence of an external Zeeman field reveals the role of Ising spin-orbit coupling and mixing of odd-parity superconducting state in influencing the low-temperature enhancement of the critical magnetic field.","sentences":["The presence of a non-centrosymmetric crystal structure and in-plane mirror symmetry allows an Ising spin-orbit coupling to form in some two-dimensional materials.","Examples include transition metal dichalcogenide superconductors like monolayer NbSe$_2$, MoS$_2$, TaS$_2$, and PbTe$_2$, where a nontrivial nature of the superconducting state is currently being explored.","In this study, we develop a microscopic formalism for Ising superconductors that captures the superconducting instability arising from a momentum-dependent spin- and charge-fluctuation-mediated pairing interaction.","We apply our pairing model to the electronic structure of monolayer NbSe$_2$, where first-principles calculations reveal the presence of strong paramagnetic fluctuations.","Our calculations provide a quantitative measure of the mixing between the even- and odd-parity superconducting states and its variation with Coulomb interaction.","Further, numerical analysis in the presence of an external Zeeman field reveals the role of Ising spin-orbit coupling and mixing of odd-parity superconducting state in influencing the low-temperature enhancement of the critical magnetic field."],"url":"http://arxiv.org/abs/2405.00409v1","category":"cond-mat.supr-con"}
{"created":"2024-05-01 08:52:42","title":"Particle production from gluon-nucleon interactions in relativistic heavy ion collisions","abstract":"We propose a particle production mechanism analogous to the particle photoproduction processes, arising from the gluon-nucleon interactions in relativistic heavy ion collisions. The comparison is made on the effect of the gluon-nucleon interactions on the photon production in Au+Au collisions at $\\sqrt{s_{NN}}=$200 GeV and Pb+Pb collisions at $\\sqrt{s_{NN}}=$2.76 TeV. The numerical results indicate that as the collision energy increases, the contribution of gluon-nucleon interactions becomes more prominent.","sentences":["We propose a particle production mechanism analogous to the particle photoproduction processes, arising from the gluon-nucleon interactions in relativistic heavy ion collisions.","The comparison is made on the effect of the gluon-nucleon interactions on the photon production in Au+Au collisions at $\\sqrt{s_{NN}}=$200 GeV and Pb+Pb collisions at $\\sqrt{s_{NN}}=$2.76 TeV.","The numerical results indicate that as the collision energy increases, the contribution of gluon-nucleon interactions becomes more prominent."],"url":"http://arxiv.org/abs/2405.00396v1","category":"nucl-th"}
{"created":"2024-05-01 06:18:04","title":"DEVILS/MIGHTEE/GAMA/DINGO: The Impact of SFR Timescales on the SFR-Radio Luminosity Correlation","abstract":"The tight relationship between infrared luminosity (L$_\\mathrm{TIR}$) and 1.4 GHz radio continuum luminosity (L$_\\mathrm{1.4GHz}$) has proven useful for understanding star formation free from dust obscuration. Infrared emission in star-forming galaxies typically arises from recently formed, dust-enshrouded stars, whereas radio synchrotron emission is expected from subsequent supernovae. By leveraging the wealth of ancillary far-ultraviolet - far-infrared photometry from the Deep Extragalactic VIsible Legacy Survey (DEVILS) and Galaxy and Mass Assembly (GAMA) surveys, combined with 1.4 GHz observations from the MeerKAT International GHz Tiered Extragalactic Exploration (MIGHTEE) survey and Deep Investigation of Neutral Gas Origins (DINGO) projects, we investigate the impact of timescale differences between far-ultraviolet - far-infrared and radio-derived star formation rate (SFR) tracers. We examine how the SED-derived star formation histories (SFH) of galaxies can be used to explain discrepancies in these SFR tracers, which are sensitive to different timescales. Galaxies exhibiting an increasing SFH have systematically higher L$_\\mathrm{TIR}$ and SED-derived SFRs than predicted from their 1.4 GHz radio luminosity. This indicates that insufficient time has passed for subsequent supernovae-driven radio emission to accumulate. We show that backtracking the SFR(t) of galaxies along their SED-derived SFHs to a time several hundred megayears prior to their observed epoch will both linearise the SFR-L$_\\mathrm{1.4GHz}$ relation and reduce the overall scatter. The minimum scatter in the SFR(t)-L$_\\mathrm{1.4GHz}$ is reached at 200 - 300 Myr prior, consistent with theoretical predictions for the timescales required to disperse the cosmic ray electrons responsible for the synchrotron emission.","sentences":["The tight relationship between infrared luminosity (L$_\\mathrm{TIR}$) and 1.4 GHz radio continuum luminosity (L$_\\mathrm{1.4GHz}$) has proven useful for understanding star formation free from dust obscuration.","Infrared emission in star-forming galaxies typically arises from recently formed, dust-enshrouded stars, whereas radio synchrotron emission is expected from subsequent supernovae.","By leveraging the wealth of ancillary far-ultraviolet - far-infrared photometry from the Deep Extragalactic VIsible Legacy Survey (DEVILS) and Galaxy and Mass Assembly (GAMA) surveys, combined with 1.4 GHz observations from the MeerKAT International GHz Tiered Extragalactic Exploration (MIGHTEE) survey and Deep Investigation of Neutral Gas Origins (DINGO) projects, we investigate the impact of timescale differences between far-ultraviolet - far-infrared and radio-derived star formation rate (SFR) tracers.","We examine how the SED-derived star formation histories (SFH) of galaxies can be used to explain discrepancies in these SFR tracers, which are sensitive to different timescales.","Galaxies exhibiting an increasing SFH have systematically higher L$_\\mathrm{TIR}$ and SED-derived SFRs than predicted from their 1.4 GHz radio luminosity.","This indicates that insufficient time has passed for subsequent supernovae-driven radio emission to accumulate.","We show that backtracking the SFR(t) of galaxies along their SED-derived SFHs to a time several hundred megayears prior to their observed epoch will both linearise the SFR-L$_\\mathrm{1.4GHz}$ relation and reduce the overall scatter.","The minimum scatter in the SFR(t)-L$_\\mathrm{1.4GHz}$ is reached at 200 - 300 Myr prior, consistent with theoretical predictions for the timescales required to disperse the cosmic ray electrons responsible for the synchrotron emission."],"url":"http://arxiv.org/abs/2405.00337v1","category":"astro-ph.GA"}
{"created":"2024-05-01 03:28:05","title":"Migration of Accreting Planets and Black Holes in Disks","abstract":"Nascent planets are thought to lose angular momentum (AM) to the gaseous protoplanetary disk (PPD) via gravitational interactions, leading to inward migration. A similar migration process also applies to stellar-mass black holes (BHs) embedded in AGN disks. However, AM exchange via accretion onto the planet/BH may strongly influence the migration torque. In this study, we perform 2D global hydrodynamic simulations of an accreting planet/BH embedded in a disk, where AM exchange between the planet/BH and disk via gravity, accretion, pressure and viscosity are considered. When accretion is turned off, we recover the linear estimate for Type I migration torque. However, in all of our accreting simulations, we find outward migration due to the positive AM deposited onto the accreting body by the disk gas. Our simulations achieve the global steady state for the transport of mass and AM: The mass and AM fluxes are constant across the disk except for jumps ($\\Delta\\dot M$ and $\\Delta\\dot J$) at the planet's location, and the jumps match the accretion rate and torque on the planet. Our findings suggest that extra caution is needed when applying the standard results of disk migration to accreting planets and BHs.","sentences":["Nascent planets are thought to lose angular momentum (AM) to the gaseous protoplanetary disk (PPD) via gravitational interactions, leading to inward migration.","A similar migration process also applies to stellar-mass black holes (BHs) embedded in AGN disks.","However, AM exchange via accretion onto the planet/BH may strongly influence the migration torque.","In this study, we perform 2D global hydrodynamic simulations of an accreting planet/BH embedded in a disk, where AM exchange between the planet/BH and disk via gravity, accretion, pressure and viscosity are considered.","When accretion is turned off, we recover the linear estimate for Type I migration torque.","However, in all of our accreting simulations, we find outward migration due to the positive AM deposited onto the accreting body by the disk gas.","Our simulations achieve the global steady state for the transport of mass and AM: The mass and AM fluxes are constant across the disk except for jumps ($\\Delta\\dot M$ and $\\Delta\\dot J$) at the planet's location, and the jumps match the accretion rate and torque on the planet.","Our findings suggest that extra caution is needed when applying the standard results of disk migration to accreting planets and BHs."],"url":"http://arxiv.org/abs/2405.00296v1","category":"astro-ph.EP"}
{"created":"2024-05-01 02:16:19","title":"Doubly-Charm and Doubly-Bottom Pentaquark molecular States via the QCD sum rules","abstract":"In the present work, the doubly-charm and doubly-bottom pentaquark molecular states $D^{(*)}\\Sigma_c^{(*)}$ and $B^{(*)}\\Sigma_b^{(*)}$ are studied via the QCD sum rules. Sixteen color singlet-singlet type currents with the definite isospin-spin-parity $IJ^P$ are constructed to interpolate the corresponding hadronic states with the same quantum numbers. The masses and pole residues of those doubly-heavy pentaquark molecular states are calculated, the results show that their masses are all below the corresponding meson-baryon thresholds, which means that they are possible bound states, not resonant states, moreover, the possible decay channels for the doubly-charm molecular states are given.","sentences":["In the present work, the doubly-charm and doubly-bottom pentaquark molecular states $D^{(*)}\\Sigma_c^{(*)}$ and $B^{(*)}\\Sigma_b^{(*)}$ are studied via the QCD sum rules.","Sixteen color singlet-singlet type currents with the definite isospin-spin-parity $IJ^P$ are constructed to interpolate the corresponding hadronic states with the same quantum numbers.","The masses and pole residues of those doubly-heavy pentaquark molecular states are calculated, the results show that their masses are all below the corresponding meson-baryon thresholds, which means that they are possible bound states, not resonant states, moreover, the possible decay channels for the doubly-charm molecular states are given."],"url":"http://arxiv.org/abs/2405.00281v1","category":"hep-ph"}
{"created":"2024-05-01 01:58:22","title":"Structure of Dubrovin-Zhang free energy functions and universal identities","abstract":"We prove a structural theorem relating the higher genera free energy functions of the Dubrovin-Zhang hierarchies to those of the trivial theory, that is, the Witten-Kontsevich free energy functions. As an important application, for any given genus $g\\geq 1$, we construct a set of universal identities valid for the free energy functions of any Dubrovin-Zhang hierarchy.","sentences":["We prove a structural theorem relating the higher genera free energy functions of the Dubrovin-Zhang hierarchies to those of the trivial theory, that is, the Witten-Kontsevich free energy functions.","As an important application, for any given genus $g\\geq 1$, we construct a set of universal identities valid for the free energy functions of any Dubrovin-Zhang hierarchy."],"url":"http://arxiv.org/abs/2405.00276v1","category":"math-ph"}
{"created":"2024-05-01 00:19:23","title":"The relationship between the turnover frequency and photo-ionisation in radio sources","abstract":"We investigate the connection between the turnover frequency in the radio spectrum, $\\nu_{\\rm TO}$, and the rate of ionising ultra-violet photons, $Q_{\\rm HI}$, in extragalactic sources. From a large, optically selected, sample we find $\\nu_{\\rm TO}$ to be correlated with $Q_{\\rm HI}$ in sources which exhibit a turnover. The significance of the correlation decreases when we include power-law radio sources as limits, by assuming that the turnover frequency occurs below the lowest value observed. However, the power-law fit sources are less well sampled across the band and so these may just be contributing noise to the data. Given that the observed $\\nu_{\\rm TO}$--$Q_{\\rm HI}$ correlation is purely empirical, we use the ionising photon rate to obtain the electron density in a free-free absorption model. For each of the constant, exponential, constant plus exponential (Milky Way) and spherical models of the gas distribution, there is also an increase in the turnover frequency with ionising photon rate. Furthermore, for a given gas mass, we find that the turnover frequency is anti-correlated with the scale-factor of the gas density. While other mechanisms, such as ageing electrons or synchrotron self-absorption, may be required to reproduce the spectral indices, for an exponential scale-factor similar to the linear size, this simple free-free absorption model reproduces the turnover-size correlation seen in radio sources.","sentences":["We investigate the connection between the turnover frequency in the radio spectrum, $\\nu_{\\rm TO}$, and the rate of ionising ultra-violet photons, $Q_{\\rm HI}$, in extragalactic sources.","From a large, optically selected, sample we find $\\nu_{\\rm TO}$ to be correlated with $Q_{\\rm HI}$ in sources which exhibit a turnover.","The significance of the correlation decreases when we include power-law radio sources as limits, by assuming that the turnover frequency occurs below the lowest value observed.","However, the power-law fit sources are less well sampled across the band and so these may just be contributing noise to the data.","Given that the observed $\\nu_{\\rm TO}$--$Q_{\\rm HI}$ correlation is purely empirical, we use the ionising photon rate to obtain the electron density in a free-free absorption model.","For each of the constant, exponential, constant plus exponential (Milky Way) and spherical models of the gas distribution, there is also an increase in the turnover frequency with ionising photon rate.","Furthermore, for a given gas mass, we find that the turnover frequency is anti-correlated with the scale-factor of the gas density.","While other mechanisms, such as ageing electrons or synchrotron self-absorption, may be required to reproduce the spectral indices, for an exponential scale-factor similar to the linear size, this simple free-free absorption model reproduces the turnover-size correlation seen in radio sources."],"url":"http://arxiv.org/abs/2405.00257v1","category":"astro-ph.GA"}
{"created":"2024-04-30 23:56:38","title":"CodeHalu: Code Hallucinations in LLMs Driven by Execution-based Verification","abstract":"Large Language Models (LLMs) have made significant advancements in the field of code generation, offering unprecedented support for automated programming and assisting developers. However, LLMs sometimes generate code that appears plausible but fails to meet the expected requirements or executes incorrectly. This phenomenon of hallucinations in the coding field has not been explored. To advance the community's understanding and research on code hallucinations in LLMs, we propose a definition method for these hallucinations based on execution verification and introduce the concept of code hallucinations for the first time. We categorize code hallucinations into four main types: mapping, naming, resource, and logic hallucinations, each further divided into different subcategories to better understand and address the unique challenges faced by LLMs during code generation. To systematically evaluate code hallucinations, we propose a dynamic detection algorithm for code hallucinations and construct the CodeHalu benchmark, which includes 8,883 samples from 699 tasks, to actively detect hallucination phenomena in LLMs during programming. We tested 16 popular LLMs on this benchmark to evaluate the frequency and nature of their hallucinations during code generation. The findings reveal significant variations in the accuracy and reliability of LLMs in generating code, highlighting the urgent need to improve models and training methods to ensure the functional correctness and safety of automatically generated code. This study not only classifies and quantifies code hallucinations but also provides insights for future improvements in LLM-based code generation research. The CodeHalu benchmark and code are publicly available at https://github.com/yuchen814/CodeHalu.","sentences":["Large Language Models (LLMs) have made significant advancements in the field of code generation, offering unprecedented support for automated programming and assisting developers.","However, LLMs sometimes generate code that appears plausible but fails to meet the expected requirements or executes incorrectly.","This phenomenon of hallucinations in the coding field has not been explored.","To advance the community's understanding and research on code hallucinations in LLMs, we propose a definition method for these hallucinations based on execution verification and introduce the concept of code hallucinations for the first time.","We categorize code hallucinations into four main types: mapping, naming, resource, and logic hallucinations, each further divided into different subcategories to better understand and address the unique challenges faced by LLMs during code generation.","To systematically evaluate code hallucinations, we propose a dynamic detection algorithm for code hallucinations and construct the CodeHalu benchmark, which includes 8,883 samples from 699 tasks, to actively detect hallucination phenomena in LLMs during programming.","We tested 16 popular LLMs on this benchmark to evaluate the frequency and nature of their hallucinations during code generation.","The findings reveal significant variations in the accuracy and reliability of LLMs in generating code, highlighting the urgent need to improve models and training methods to ensure the functional correctness and safety of automatically generated code.","This study not only classifies and quantifies code hallucinations but also provides insights for future improvements in LLM-based code generation research.","The CodeHalu benchmark and code are publicly available at https://github.com/yuchen814/CodeHalu."],"url":"http://arxiv.org/abs/2405.00253v1","category":"cs.CL"}
{"created":"2024-04-30 23:14:41","title":"Intersection Theorem for DG-modules","abstract":"Let A be a commutative noetherian local DG-ring with bounded cohomology. The Intersection Theorem for DG-modules is examined and some of its applications are provided. The first is to prove the DG-setting of the amplitude inequality, New Intersection Theorem and Krull's principle ideal theorem. The second is to solve completely the Minamoto's conjecture in [Israel J. Math. 242 (2021) 1-36]. The third is to show the DG-version of the Bass conjecture about Cohen-Macaulay rings and the Vasconcelos conjecture about Gorenstein rings.","sentences":["Let A be a commutative noetherian local DG-ring with bounded cohomology.","The Intersection Theorem for DG-modules is examined and some of its applications are provided.","The first is to prove the DG-setting of the amplitude inequality, New Intersection Theorem and Krull's principle ideal theorem.","The second is to solve completely the Minamoto's conjecture in [Israel J. Math. 242 (2021) 1-36].","The third is to show the DG-version of the Bass conjecture about Cohen-Macaulay rings and the Vasconcelos conjecture about Gorenstein rings."],"url":"http://arxiv.org/abs/2405.00240v1","category":"math.AC"}
{"created":"2024-04-30 23:06:14","title":"A Categorical Approach to Coalgebraic Fixpoint Logic","abstract":"We define a framework for incorporating alternation-free fixpoint logics into the dual-adjunction setup for coalgebraic modal logics. We achieve this by using order-enriched categories. We give a least-solution semantics as well as an initial algebra semantics, and prove they are equivalent. We also show how to place the alternation-free coalgebraic $\\mu$-calculus in this framework, as well as PDL and a logic with a probabilistic dynamic modality.","sentences":["We define a framework for incorporating alternation-free fixpoint logics into the dual-adjunction setup for coalgebraic modal logics.","We achieve this by using order-enriched categories.","We give a least-solution semantics as well as an initial algebra semantics, and prove they are equivalent.","We also show how to place the alternation-free coalgebraic $\\mu$-calculus in this framework, as well as PDL and a logic with a probabilistic dynamic modality."],"url":"http://arxiv.org/abs/2405.00237v1","category":"cs.LO"}
{"created":"2024-04-30 22:12:29","title":"Photon propagator for inflation in the general covariant gauge","abstract":"Photon propagator for power-law inflation is considered in the general covariant gauges within the canonical quantization formalism. Photon mode functions in covariant gauges are considerably more complicated than their scalar counterparts, except for the special choice of the gauge-fixing parameter we call the simple covariant gauge. We explicitly construct the position space photon propagator in the simple covariant gauge, and find the result considerably more complicated than its scalar counterpart. This is because of the need for explicitly inverting the Laplace operator acting on the scalar propagator, which results in Appell's fourth function. Our propagator correctly reproduces the de Sitter and flat space limits. We use this propagator to compute two simple observables: the off-coincident field strength-field strength correlator and the energy-momentum tensor, both of which yield consistent results. As a spinoff of our computation we also give the exact expression for the Coulomb gauge propagator in power-law inflation in arbitrary dimensions.","sentences":["Photon propagator for power-law inflation is considered in the general covariant gauges within the canonical quantization formalism.","Photon mode functions in covariant gauges are considerably more complicated than their scalar counterparts, except for the special choice of the gauge-fixing parameter we call the simple covariant gauge.","We explicitly construct the position space photon propagator in the simple covariant gauge, and find the result considerably more complicated than its scalar counterpart.","This is because of the need for explicitly inverting the Laplace operator acting on the scalar propagator, which results in Appell's fourth function.","Our propagator correctly reproduces the de Sitter and flat space limits.","We use this propagator to compute two simple observables: the off-coincident field strength-field strength correlator and the energy-momentum tensor, both of which yield consistent results.","As a spinoff of our computation we also give the exact expression for the Coulomb gauge propagator in power-law inflation in arbitrary dimensions."],"url":"http://arxiv.org/abs/2405.00226v1","category":"hep-th"}
{"created":"2024-04-30 21:20:17","title":"A Primer on the Inner Workings of Transformer-based Language Models","abstract":"The rapid progress of research aimed at interpreting the inner workings of advanced language models has highlighted a need for contextualizing the insights gained from years of work in this area. This primer provides a concise technical introduction to the current techniques used to interpret the inner workings of Transformer-based language models, focusing on the generative decoder-only architecture. We conclude by presenting a comprehensive overview of the known internal mechanisms implemented by these models, uncovering connections across popular approaches and active research directions in this area.","sentences":["The rapid progress of research aimed at interpreting the inner workings of advanced language models has highlighted a need for contextualizing the insights gained from years of work in this area.","This primer provides a concise technical introduction to the current techniques used to interpret the inner workings of Transformer-based language models, focusing on the generative decoder-only architecture.","We conclude by presenting a comprehensive overview of the known internal mechanisms implemented by these models, uncovering connections across popular approaches and active research directions in this area."],"url":"http://arxiv.org/abs/2405.00208v1","category":"cs.CL"}
{"created":"2024-04-30 20:49:13","title":"A momentum-space theory for topological magnons in 2D ferromagnetic skyrmion lattices","abstract":"Magnon dynamics in skyrmion lattices have garnered significant interest due to their potential applications in topological magnonics. Existing theories often follow a single-momentum approach, assuming significant Dzyaloshinskii-Moriya Interaction (DMI) to minimize the skyrmion's dimensions, which can lead to oversimplification in describing magnon behavior. This study introduces a multi-momentum operator theory for magnons in large 2D skyrmions, where each skyrmion encompasses several thousand spins. The proposed theory fully transforms the magnon Hamiltonian into momentum space, incorporating off-diagonal terms to capture umklapp scattering caused by the skyrmion wave vectors. Our results reveal deviations from single-momentum theories, demonstrating that flat bands are not universal features of the skyrmionic magnon spectrum. Additionally, we find that manipulating the skyrmion size with an external magnetic field induces multiple topological phase transitions. At high magnetic fields, the low-energy magnon spectrum becomes densely packed and entirely topological, resembling a topological band continuum.","sentences":["Magnon dynamics in skyrmion lattices have garnered significant interest due to their potential applications in topological magnonics.","Existing theories often follow a single-momentum approach, assuming significant Dzyaloshinskii-Moriya Interaction (DMI) to minimize the skyrmion's dimensions, which can lead to oversimplification in describing magnon behavior.","This study introduces a multi-momentum operator theory for magnons in large 2D skyrmions, where each skyrmion encompasses several thousand spins.","The proposed theory fully transforms the magnon Hamiltonian into momentum space, incorporating off-diagonal terms to capture umklapp scattering caused by the skyrmion wave vectors.","Our results reveal deviations from single-momentum theories, demonstrating that flat bands are not universal features of the skyrmionic magnon spectrum.","Additionally, we find that manipulating the skyrmion size with an external magnetic field induces multiple topological phase transitions.","At high magnetic fields, the low-energy magnon spectrum becomes densely packed and entirely topological, resembling a topological band continuum."],"url":"http://arxiv.org/abs/2405.00192v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-30 19:28:07","title":"Divergences in the effective loop interaction of the Chern-Simons bosons with leptons. The unitary gauge case","abstract":"In this paper, we consider the extension of the Standard Model with Chern-Simons type interaction. There is a new vector massive boson (Chern-Simons bosons) in this extension. Using only three-particle dimension-4 interaction of the Chern-Simons bosons with vector bosons of the SM, we consider effective loop interaction of a new vector boson with leptons. We consider the renormalizability of this loop interaction and conclude that for the case of computation of loop diagrams in the unitary gauge, we can not get rid of the divergences in the effective interaction of the Chern-Simons bosons with leptons.","sentences":["In this paper, we consider the extension of the Standard Model with Chern-Simons type interaction.","There is a new vector massive boson (Chern-Simons bosons) in this extension.","Using only three-particle dimension-4 interaction of the Chern-Simons bosons with vector bosons of the SM, we consider effective loop interaction of a new vector boson with leptons.","We consider the renormalizability of this loop interaction and conclude that for the case of computation of loop diagrams in the unitary gauge, we can not get rid of the divergences in the effective interaction of the Chern-Simons bosons with leptons."],"url":"http://arxiv.org/abs/2405.00164v1","category":"hep-ph"}
{"created":"2024-04-30 19:24:56","title":"Estimating Heterogeneous Treatment Effects with Item-Level Outcome Data: Insights from Item Response Theory","abstract":"Analyses of heterogeneous treatment effects (HTE) are common in applied causal inference research. However, when outcomes are latent variables assessed via psychometric instruments such as educational tests, standard methods ignore the potential HTE that may exist among the individual items of the outcome measure. Failing to account for \"item-level\" HTE (IL-HTE) can lead to both estimated standard errors that are too small and identification challenges in the estimation of treatment-by-covariate interaction effects. We demonstrate how Item Response Theory (IRT) models that estimate a treatment effect for each assessment item can both address these challenges and provide new insights into HTE generally. This study articulates the theoretical rationale for the IL-HTE model and demonstrates its practical value using data from 20 randomized controlled trials in economics, education, and health. Our results show that the IL-HTE model reveals item-level variation masked by average treatment effects, provides more accurate statistical inference, allows for estimates of the generalizability of causal effects, resolves identification problems in the estimation of interaction effects, and provides estimates of standardized treatment effect sizes corrected for attenuation due to measurement error.","sentences":["Analyses of heterogeneous treatment effects (HTE) are common in applied causal inference research.","However, when outcomes are latent variables assessed via psychometric instruments such as educational tests, standard methods ignore the potential HTE that may exist among the individual items of the outcome measure.","Failing to account for \"item-level\" HTE (IL-HTE) can lead to both estimated standard errors that are too small and identification challenges in the estimation of treatment-by-covariate interaction effects.","We demonstrate how Item Response Theory (IRT) models that estimate a treatment effect for each assessment item can both address these challenges and provide new insights into HTE generally.","This study articulates the theoretical rationale for the IL-HTE model and demonstrates its practical value using data from 20 randomized controlled trials in economics, education, and health.","Our results show that the IL-HTE model reveals item-level variation masked by average treatment effects, provides more accurate statistical inference, allows for estimates of the generalizability of causal effects, resolves identification problems in the estimation of interaction effects, and provides estimates of standardized treatment effect sizes corrected for attenuation due to measurement error."],"url":"http://arxiv.org/abs/2405.00161v1","category":"econ.EM"}
{"created":"2024-04-30 19:02:15","title":"Supernova limits on 'QCD axion-like particles'","abstract":"In this paper, we explore the phenomenology of massive Axion-Like Particles (ALPs) coupled to quarks and gluons, dubbed 'QCD ALPs', with an emphasis on the associated low-energy observables. ALPs coupled to gluons and quarks not only induce nuclear interactions at scales below the QCD-scale, relevant for ALP production in supernovae (SNe), but naturally also couple to photons similarly to the QCD-axion. We discuss the link between the high-energy formulation of ALP theories and their effective couplings with nucleons and photons. The induced photon coupling allows ALPs with masses $m_a\\gtrsim1$ MeV to efficiently decay into photons, and astrophysical observables severely constrain the ALP parameter space. We show that a combination of arguments related to SN events rule out ALP-nucleon couplings down to $g_{aN}\\gtrsim 10^{-11}- 10^{-10}$ for $m_a\\gtrsim1$ MeV - a region of the parameter space that was hitherto unconstrained.","sentences":["In this paper, we explore the phenomenology of massive Axion-Like Particles (ALPs) coupled to quarks and gluons, dubbed 'QCD ALPs', with an emphasis on the associated low-energy observables.","ALPs coupled to gluons and quarks not only induce nuclear interactions at scales below the QCD-scale, relevant for ALP production in supernovae (SNe), but naturally also couple to photons similarly to the QCD-axion.","We discuss the link between the high-energy formulation of ALP theories and their effective couplings with nucleons and photons.","The induced photon coupling allows ALPs with masses $m_a\\gtrsim1$ MeV to efficiently decay into photons, and astrophysical observables severely constrain the ALP parameter space.","We show that a combination of arguments related to SN events rule out ALP-nucleon couplings down to $g_{aN}\\gtrsim 10^{-11}- 10^{-10}$ for $m_a\\gtrsim1$ MeV - a region of the parameter space that was hitherto unconstrained."],"url":"http://arxiv.org/abs/2405.00153v1","category":"hep-ph"}
{"created":"2024-04-30 18:50:35","title":"A deconstruction of methods to derive one-point lensing statistics","abstract":"Gravitational lensing is a crucial tool for exploring cosmic phenomena, providing insights into galaxy clustering, dark matter, and dark energy. Given the substantial computational demands of $N$-body simulations, approximate methods like $\\texttt{PINOCCHIO}$ and $\\texttt{turboGL}$ offer viable alternatives for simulating lensing probability density functions (PDFs). This paper evaluates these methods in contexts where baryonic effects are negligible, focusing on dark matter-dominated models and assessing their effectiveness across both weak and strong lensing regimes. Our comparative analysis reveals that these methods are particularly effective for applications involving electromagnetic and gravitational wave point sources, where strong lensing events are infrequent. Both $\\texttt{PINOCCHIO}$ and $\\texttt{turboGL}$ perform well in modeling the weak-lensing region influenced by mildly nonlinear structures. However, they lose accuracy in capturing small-scale nonlinear matter fields, owing to oversimplified assumptions about internal halo structures and reliance on perturbation theory. The analysis shows that $\\texttt{PINOCCHIO}$ achieves an 8-15% agreement with $N$-body simulations for the second-to-fourth moments of lensing PDFs. These findings aim to inform future studies on gravitational lensing of point sources, which are increasingly relevant with upcoming supernova and gravitational wave datasets.","sentences":["Gravitational lensing is a crucial tool for exploring cosmic phenomena, providing insights into galaxy clustering, dark matter, and dark energy.","Given the substantial computational demands of $N$-body simulations, approximate methods like $\\texttt{PINOCCHIO}$ and $\\texttt{turboGL}$ offer viable alternatives for simulating lensing probability density functions (PDFs).","This paper evaluates these methods in contexts where baryonic effects are negligible, focusing on dark matter-dominated models and assessing their effectiveness across both weak and strong lensing regimes.","Our comparative analysis reveals that these methods are particularly effective for applications involving electromagnetic and gravitational wave point sources, where strong lensing events are infrequent.","Both $\\texttt{PINOCCHIO}$ and $\\texttt{turboGL}$ perform well in modeling the weak-lensing region influenced by mildly nonlinear structures.","However, they lose accuracy in capturing small-scale nonlinear matter fields, owing to oversimplified assumptions about internal halo structures and reliance on perturbation theory.","The analysis shows that $\\texttt{PINOCCHIO}$ achieves an 8-15% agreement with $N$-body simulations for the second-to-fourth moments of lensing PDFs.","These findings aim to inform future studies on gravitational lensing of point sources, which are increasingly relevant with upcoming supernova and gravitational wave datasets."],"url":"http://arxiv.org/abs/2405.00147v1","category":"astro-ph.CO"}
{"created":"2024-04-30 18:31:37","title":"Improving Channel Resilience for Task-Oriented Semantic Communications: A Unified Information Bottleneck Approach","abstract":"Task-oriented semantic communications (TSC) enhance radio resource efficiency by transmitting task-relevant semantic information. However, current research often overlooks the inherent semantic distinctions among encoded features. Due to unavoidable channel variations from time and frequency-selective fading, semantically sensitive feature units could be more susceptible to erroneous inference if corrupted by dynamic channels. Therefore, this letter introduces a unified channel-resilient TSC framework via information bottleneck. This framework complements existing TSC approaches by controlling information flow to capture fine-grained feature-level semantic robustness. Experiments on a case study for real-time subchannel allocation validate the framework's effectiveness.","sentences":["Task-oriented semantic communications (TSC) enhance radio resource efficiency by transmitting task-relevant semantic information.","However, current research often overlooks the inherent semantic distinctions among encoded features.","Due to unavoidable channel variations from time and frequency-selective fading, semantically sensitive feature units could be more susceptible to erroneous inference if corrupted by dynamic channels.","Therefore, this letter introduces a unified channel-resilient TSC framework via information bottleneck.","This framework complements existing TSC approaches by controlling information flow to capture fine-grained feature-level semantic robustness.","Experiments on a case study for real-time subchannel allocation validate the framework's effectiveness."],"url":"http://arxiv.org/abs/2405.00135v1","category":"cs.IT"}
{"created":"2024-04-30 18:24:56","title":"Target-Specific De Novo Peptide Binder Design with DiffPepBuilder","abstract":"Despite the exciting progress in target-specific de novo protein binder design, peptide binder design remains challenging due to the flexibility of peptide structures and the scarcity of protein-peptide complex structure data. In this study, we curated a large synthetic dataset, referred to as PepPC-F, from the abundant protein-protein interface data and developed DiffPepBuilder, a de novo target-specific peptide binder generation method that utilizes an SE(3)-equivariant diffusion model trained on PepPC-F to co-design peptide sequences and structures. DiffPepBuilder also introduces disulfide bonds to stabilize the generated peptide structures. We tested DiffPepBuilder on 30 experimentally verified strong peptide binders with available protein-peptide complex structures. DiffPepBuilder was able to effectively recall the native structures and sequences of the peptide ligands and to generate novel peptide binders with improved binding free energy. We subsequently conducted de novo generation case studies on three targets. In both the regeneration test and case studies, DiffPepBuilder outperformed AfDesign and RFdiffusion coupled with ProteinMPNN, in terms of sequence and structure recall, interface quality, and structural diversity. Molecular dynamics simulations confirmed that the introduction of disulfide bonds enhanced the structural rigidity and binding performance of the generated peptides. As a general peptide binder de novo design tool, DiffPepBuilder can be used to design peptide binders for given protein targets with three dimensional and binding site information.","sentences":["Despite the exciting progress in target-specific de novo protein binder design, peptide binder design remains challenging due to the flexibility of peptide structures and the scarcity of protein-peptide complex structure data.","In this study, we curated a large synthetic dataset, referred to as PepPC-F, from the abundant protein-protein interface data and developed DiffPepBuilder, a de novo target-specific peptide binder generation method that utilizes an SE(3)-equivariant diffusion model trained on PepPC-F to co-design peptide sequences and structures.","DiffPepBuilder also introduces disulfide bonds to stabilize the generated peptide structures.","We tested DiffPepBuilder on 30 experimentally verified strong peptide binders with available protein-peptide complex structures.","DiffPepBuilder was able to effectively recall the native structures and sequences of the peptide ligands and to generate novel peptide binders with improved binding free energy.","We subsequently conducted de novo generation case studies on three targets.","In both the regeneration test and case studies, DiffPepBuilder outperformed AfDesign and RFdiffusion coupled with ProteinMPNN, in terms of sequence and structure recall, interface quality, and structural diversity.","Molecular dynamics simulations confirmed that the introduction of disulfide bonds enhanced the structural rigidity and binding performance of the generated peptides.","As a general peptide binder de novo design tool, DiffPepBuilder can be used to design peptide binders for given protein targets with three dimensional and binding site information."],"url":"http://arxiv.org/abs/2405.00128v1","category":"q-bio.BM"}
{"created":"2024-04-30 18:17:47","title":"Primordial black hole probes of heavy neutral leptons","abstract":"Primordial black holes (PBH), while still constituting a viable dark matter component, are expected to evaporate through Hawking radiation. Assuming the semi-classical approximation holds up to near the Planck scale, PBHs are expected to evaporate by the present time, emitting a significant flux of particles in their final moments, if produced in the early Universe with an initial mass of $\\sim 10^{15}$ g. These ''exploding'' black holes will release a burst of Standard Model particles alongside any additional degrees of freedom, should they exist. We explore the possibility that heavy neutral leptons (HNL), mixing with active neutrinos, are emitted in the final evaporation stages. We calculate the expected number of active neutrinos from such an event, including contributions due to the HNL decay for different assumptions on the mixings. We infer sensitivities on the active-sterile neutrino mixing and on the sterile neutrino mass, finding that, for instance, for the scenario where $U_{\\tau 4}\\neq 0$, IceCube could improve current constraints by $\\sim 2$ orders of magnitude, for HNLs masses between 0.1 - 1 GeV, for a PBH at a distance of $\\sim 10^{-4}$ pc from Earth.","sentences":["Primordial black holes (PBH), while still constituting a viable dark matter component, are expected to evaporate through Hawking radiation.","Assuming the semi-classical approximation holds up to near the Planck scale, PBHs are expected to evaporate by the present time, emitting a significant flux of particles in their final moments, if produced in the early Universe with an initial mass of $\\sim 10^{15}$ g.","These ''exploding'' black holes will release a burst of Standard Model particles alongside any additional degrees of freedom, should they exist.","We explore the possibility that heavy neutral leptons (HNL), mixing with active neutrinos, are emitted in the final evaporation stages.","We calculate the expected number of active neutrinos from such an event, including contributions due to the HNL decay for different assumptions on the mixings.","We infer sensitivities on the active-sterile neutrino mixing and on the sterile neutrino mass, finding that, for instance, for the scenario where $U_{\\tau 4}\\neq 0$, IceCube could improve current constraints by $\\sim 2$ orders of magnitude, for HNLs masses between 0.1 - 1 GeV, for a PBH at a distance of $\\sim 10^{-4}$ pc from Earth."],"url":"http://arxiv.org/abs/2405.00124v1","category":"hep-ph"}
{"created":"2024-04-30 18:08:49","title":"Sterile neutrino dark matter within the $\u03bd$SMEFT","abstract":"Sterile neutrinos with masses at the $\\mathrm{keV}$ scale and mixing to the active neutrinos offer an elegant explanation of the observed dark matter (DM) density. However, the very same mixing inevitably leads to radiative photon emission and the non-observation of such peaked $X$-ray lines rules out this minimal sterile neutrino DM hypothesis. We show that in the context of the Standard Model effective field theory with sterile neutrinos ($\\nu$SMEFT), higher dimensional operators can produce sterile neutrino DM in a broad range of parameter space. In particular, $\\nu$SMEFT interactions can open the large mixing parameter space due to their destructive interference, through operator mixing or matching, in the $X$-ray emission. We also find that, even in the zero mixing limit, the DM density can always be explained by $\\nu$SMEFT operators. The testability of the studied $\\nu$SMEFT operators in searches for electric dipole moments, neutrinoless double beta decay, and pion decay measurements is discussed.","sentences":["Sterile neutrinos with masses at the $\\mathrm{keV}$ scale and mixing to the active neutrinos offer an elegant explanation of the observed dark matter (DM) density.","However, the very same mixing inevitably leads to radiative photon emission and the non-observation of such peaked $X$-ray lines rules out this minimal sterile neutrino DM hypothesis.","We show that in the context of the Standard Model effective field theory with sterile neutrinos ($\\nu$SMEFT), higher dimensional operators can produce sterile neutrino DM in a broad range of parameter space.","In particular, $\\nu$SMEFT interactions can open the large mixing parameter space due to their destructive interference, through operator mixing or matching, in the $X$-ray emission.","We also find that, even in the zero mixing limit, the DM density can always be explained by $\\nu$SMEFT operators.","The testability of the studied $\\nu$SMEFT operators in searches for electric dipole moments, neutrinoless double beta decay, and pion decay measurements is discussed."],"url":"http://arxiv.org/abs/2405.00119v1","category":"hep-ph"}
{"created":"2024-04-30 18:02:01","title":"The Extremely Metal-Poor SN 2023ufx: A Local Analog to High-Redshift Type II Supernovae","abstract":"We present extensive observations of the Type II supernova (SN II) 2023ufx which is likely the most metal-poor SN II observed to-date. It exploded in the outskirts of a low-metallicity ($Z_{\\rm host} \\sim 0.1~Z_\\odot$) dwarf ($M_g = -13.23\\pm0.15$~mag; $r_e\\sim 1$~kpc) galaxy. The explosion is luminous, peaking at $M_g\\approx -18.5~$mag, and shows rapid evolution. The $r$-band (pseudo-bolometric) light curve has a shock-cooling phase lasting 20 (17) days followed by a 19 (23)-day plateau. The entire optically-thick phase lasts only $\\approx 55~$days following explosion, indicating that the red supergiant progenitor had a thinned H envelope prior to explosion. The early spectra obtained during the shock-cooling phase show no evidence for narrow emission features and limit the pre-explosion mass-loss rate to $\\dot{M} \\lesssim 10^{-3}~\\rm M_\\odot$/yr. The photospheric-phase spectra are devoid of prominent metal absorption features, indicating a progenitor metallicity of $\\lesssim 0.1~Z_\\odot$. The semi-nebular ($\\sim 60-130~$d) spectra reveal weak Fe II, but other metal species typically observed at these phases (Ti II, Sc II, Ba II) are conspicuously absent. The late-phase optical and near-infrared spectra also reveal broad ($\\approx 10^4~\\rm{km}~\\rm s^{-1}$) double-peaked H$\\alpha$, P$\\beta$, and P$\\gamma$ emission profiles suggestive of a fast outflow launched during the explosion. Outflows are typically attributed to rapidly-rotating progenitors which also prefer metal-poor environments. This is only the second SN II with $\\lesssim 0.1~Z_\\odot$ and both exhibit peculiar evolution, suggesting a sizable fraction of metal-poor SNe II have distinct properties compared to nearby metal-enriched SNe II. These observations lay the groundwork for modeling the metal-poor SNe II expected in the early Universe.","sentences":["We present extensive observations of the Type II supernova (SN II) 2023ufx which is likely the most metal-poor SN II observed to-date.","It exploded in the outskirts of a low-metallicity ($Z_{\\rm host} \\sim 0.1~Z_\\odot$) dwarf ($M_g = -13.23\\pm0.15$~mag; $r_e\\sim 1$~kpc) galaxy.","The explosion is luminous, peaking at $M_g\\approx -18.5~$mag, and shows rapid evolution.","The $r$-band (pseudo-bolometric) light curve has a shock-cooling phase lasting 20 (17) days followed by a 19 (23)-day plateau.","The entire optically-thick phase lasts only $\\approx 55~$days following explosion, indicating that the red supergiant progenitor had a thinned H envelope prior to explosion.","The early spectra obtained during the shock-cooling phase show no evidence for narrow emission features and limit the pre-explosion mass-loss rate to $\\dot{M} \\lesssim 10^{-3}~\\rm M_\\odot$/yr.","The photospheric-phase spectra are devoid of prominent metal absorption features, indicating a progenitor metallicity of $\\lesssim 0.1~Z_\\odot$. The semi-nebular ($\\sim 60-130~$d) spectra reveal weak Fe II, but other metal species typically observed at these phases (Ti II, Sc II, Ba II) are conspicuously absent.","The late-phase optical and near-infrared spectra also reveal broad ($\\approx 10^4~\\rm{km}~\\rm s^{-1}$) double-peaked H$\\alpha$, P$\\beta$, and P$\\gamma$ emission profiles suggestive of a fast outflow launched during the explosion.","Outflows are typically attributed to rapidly-rotating progenitors which also prefer metal-poor environments.","This is only the second SN II with $\\lesssim 0.1~Z_\\odot$ and both exhibit peculiar evolution, suggesting a sizable fraction of metal-poor SNe II have distinct properties compared to nearby metal-enriched SNe II.","These observations lay the groundwork for modeling the metal-poor SNe II expected in the early Universe."],"url":"http://arxiv.org/abs/2405.00113v1","category":"astro-ph.SR"}
{"created":"2024-04-30 18:01:40","title":"Transmon Qubit Constraints on Dark Matter-Nucleon Scattering","abstract":"We recently pointed out that power measurements of single quasiparticle devices can be used to detect dark matter. These devices have the lowest known energy thresholds, far surpassing standard direct detection experiments, requiring energy deposition above only about an meV. We calculate dark matter induced quasiparticle densities in transmon qubits, and use the latest transmon qubit measurements that provide one of the strongest existing lab-based bounds on dark matter-nucleon scattering below about 100 MeV. We strongly constrain sub-component dark matter, using both a dark matter population thermalized in the Earth as well as the dark matter wind from the Galactic halo. We demonstrate future potential sensitivities using devices with low quasiparticle densities.","sentences":["We recently pointed out that power measurements of single quasiparticle devices can be used to detect dark matter.","These devices have the lowest known energy thresholds, far surpassing standard direct detection experiments, requiring energy deposition above only about an meV.","We calculate dark matter induced quasiparticle densities in transmon qubits, and use the latest transmon qubit measurements that provide one of the strongest existing lab-based bounds on dark matter-nucleon scattering below about 100 MeV.","We strongly constrain sub-component dark matter, using both a dark matter population thermalized in the Earth as well as the dark matter wind from the Galactic halo.","We demonstrate future potential sensitivities using devices with low quasiparticle densities."],"url":"http://arxiv.org/abs/2405.00112v1","category":"hep-ph"}
{"created":"2024-04-30 18:01:24","title":"Vorticity Suppression by Particle Lag Effects in Shock-Driven Multiphase Instability","abstract":"Shock-driven multiphase mixing occurs in many physical systems such as explosive dispersal of chemical or biological agents, in the evolution of supernova remnants, and in supersonic and detonative combustion engines. This mixing process is driven by the Shock Driven Multiphase Instability (SDMI), a derivative of the canonical Richtmyer-Meshkov Instability (RMI). The SDMI deviates from the RMI as particle lag effects become significant, where a higher momentum deficit leads to longer equilibration times and a reduction in hydrodynamic mixing. In this work, the effect of particle lag (rate of momentum transfer) on the SDMI evolution was isolated and investigated utilizing solid nondeforming and nonevaporating particles of differing sizes while holding the effective density ratio (mass of particles in the interface) constant. Three particle sizes were selected with increasing velocity relaxation times. Experiments were conducted by accelerating a cylindrical interface comprised of air and seeded particles surrounded by clean (particle-free) air with a Mach 1.35 shock wave. The development of the multiphase interface was measured using particle imaging velocimetry (PIV). Circulation measurements showed a decrease in mixing with increasing particle size. Finally, a new model, derived from theory, is proposed to predict circulation deposition, mixing energy, in the SDMI based on shock strength, effective density ratio, and particle response times.","sentences":["Shock-driven multiphase mixing occurs in many physical systems such as explosive dispersal of chemical or biological agents, in the evolution of supernova remnants, and in supersonic and detonative combustion engines.","This mixing process is driven by the Shock Driven Multiphase Instability (SDMI), a derivative of the canonical Richtmyer-Meshkov Instability (RMI).","The SDMI deviates from the RMI as particle lag effects become significant, where a higher momentum deficit leads to longer equilibration times and a reduction in hydrodynamic mixing.","In this work, the effect of particle lag (rate of momentum transfer) on the SDMI evolution was isolated and investigated utilizing solid nondeforming and nonevaporating particles of differing sizes while holding the effective density ratio (mass of particles in the interface) constant.","Three particle sizes were selected with increasing velocity relaxation times.","Experiments were conducted by accelerating a cylindrical interface comprised of air and seeded particles surrounded by clean (particle-free) air with a Mach 1.35 shock wave.","The development of the multiphase interface was measured using particle imaging velocimetry (PIV).","Circulation measurements showed a decrease in mixing with increasing particle size.","Finally, a new model, derived from theory, is proposed to predict circulation deposition, mixing energy, in the SDMI based on shock strength, effective density ratio, and particle response times."],"url":"http://arxiv.org/abs/2405.00111v1","category":"physics.flu-dyn"}
{"created":"2024-04-30 18:00:19","title":"Flavor Phenomenology of Light Dark Vectors","abstract":"Light dark matter with flavor-violating couplings to fermions may be copiously produced in the laboratory as missing energy from decays of SM particles. Here we study the effective Lagrangian of a light dark vector with generic dipole or vector couplings. We calculate the resulting two-body decay rates of mesons, baryons and leptons as a function of the dark vector mass and show that existing experimental limits probe UV scales as large as $10^{12} \\,\\mathrm{GeV}$. We also derive the general RGEs in order to constrain the flavor-universal UV scenario, where all flavor violation arises radiatively proportional to the CKM matrix.","sentences":["Light dark matter with flavor-violating couplings to fermions may be copiously produced in the laboratory as missing energy from decays of SM particles.","Here we study the effective Lagrangian of a light dark vector with generic dipole or vector couplings.","We calculate the resulting two-body decay rates of mesons, baryons and leptons as a function of the dark vector mass and show that existing experimental limits probe UV scales as large as $10^{12} \\,\\mathrm{GeV}$.","We also derive the general RGEs in order to constrain the flavor-universal UV scenario, where all flavor violation arises radiatively proportional to the CKM matrix."],"url":"http://arxiv.org/abs/2405.00108v1","category":"hep-ph"}
{"created":"2024-04-30 18:00:02","title":"Cosmic Reionization on Computers: The Evolution of Ionizing Background and Mean Free Path","abstract":"Observations of the end stages of reionization indicate that at $z\\approx 5-6$, the ionizing background is not uniform and the mean free path (MFP) changes drastically. As MFP is closely related to the distribution of Lyman Limit Systems and Damped Lyman-alpha Systems (LLSs and DLAs, or ionizing photon \"sinks\"), it is important to understand them. In this study, we utilize the CROC simulations, which have both sufficient spatial resolution to resolve galaxy formation and LLSs alongside a fully coupled radiative transfer to simulate the reionization processes. In our analysis, we connect the evolution of the ionizing background and the MFP. We analyze two CROC boxes with distinct reionization histories and find that the distribution of ionizing background in both simulations display significant skewness that deviate from log-normal. Further, the ionizing background in late reionization box still displays significant fluctuations ($\\sim 40\\%$) at $z\\approx5$. We also measure the MFP along sightlines that start 0.15 pMpc away from the center of potential quasar hosting halos. The evolution of the MFP measured from these sightlines exhibits a break that coincides with when all the neutral islands disappear in the reionization history of each box (the `ankle' of the reionization history of the box). In the absence of LLSs, the MFP will be biased high by $\\approx 20\\%$ at $z\\approx 5$. We also compare the MFP measured in random sightlines. We find that at $z\\approx 5$ the MFP measured in sightlines that start from massive halos are systematically smaller by $\\approx 10\\%$ compared with the MFP measured in random sightlines. We attribute this difference to the concentration of dense structures within 1 pMpc from massive halos. Our findings highlight the importance of high fidelity models in the interpretation of observational measurements.","sentences":["Observations of the end stages of reionization indicate that at $z\\approx 5-6$, the ionizing background is not uniform and the mean free path (MFP) changes drastically.","As MFP is closely related to the distribution of Lyman Limit Systems and Damped Lyman-alpha Systems (LLSs and DLAs, or ionizing photon \"sinks\"), it is important to understand them.","In this study, we utilize the CROC simulations, which have both sufficient spatial resolution to resolve galaxy formation and LLSs alongside a fully coupled radiative transfer to simulate the reionization processes.","In our analysis, we connect the evolution of the ionizing background and the MFP.","We analyze two CROC boxes with distinct reionization histories and find that the distribution of ionizing background in both simulations display significant skewness that deviate from log-normal.","Further, the ionizing background in late reionization box still displays significant fluctuations ($\\sim 40\\%$) at $z\\approx5$. We also measure the MFP along sightlines that start 0.15 pMpc away from the center of potential quasar hosting halos.","The evolution of the MFP measured from these sightlines exhibits a break that coincides with when all the neutral islands disappear in the reionization history of each box (the `ankle' of the reionization history of the box).","In the absence of LLSs, the MFP will be biased high by $\\approx 20\\%$ at $z\\approx 5$.","We also compare the MFP measured in random sightlines.","We find that at $z\\approx 5$ the MFP measured in sightlines that start from massive halos are systematically smaller by $\\approx 10\\%$ compared with the MFP measured in random sightlines.","We attribute this difference to the concentration of dense structures within 1 pMpc from massive halos.","Our findings highlight the importance of high fidelity models in the interpretation of observational measurements."],"url":"http://arxiv.org/abs/2405.00100v1","category":"astro-ph.CO"}
{"created":"2024-04-30 18:00:02","title":"$\\mathcal{N} = 2$ Orbi-S-Folds","abstract":"We introduce a new class of non-compact backgrounds of Type IIB string theory preserving eight supercharges by combining S-folds and non-perturbative 7-branes wrapping orbifolds, and study the four-dimensional superconformal field theories arising at low energy on $D3$-branes probing them. We draw a precise correspondence between this setup and the torus compactification of six-dimensional orbi-instanton theories with a Stiefel-Whitney twist, and use it to determine the main features of such strongly-coupled systems, like central charges, spectra of Coulomb-branch operators, networks of Higgs-branch flows. Finally, with the aim to improve our understanding of the landscape of $\\mathcal{N}=2$ superconformal field theories, and possibly to extend their classification beyond rank two, we provide a detailed catalogue of all the rank-three theories that our framework gives access to.","sentences":["We introduce a new class of non-compact backgrounds of Type IIB string theory preserving eight supercharges by combining S-folds and non-perturbative 7-branes wrapping orbifolds, and study the four-dimensional superconformal field theories arising at low energy on $D3$-branes probing them.","We draw a precise correspondence between this setup and the torus compactification of six-dimensional orbi-instanton theories with a Stiefel-Whitney twist, and use it to determine the main features of such strongly-coupled systems, like central charges, spectra of Coulomb-branch operators, networks of Higgs-branch flows.","Finally, with the aim to improve our understanding of the landscape of $\\mathcal{N}=2$ superconformal field theories, and possibly to extend their classification beyond rank two, we provide a detailed catalogue of all the rank-three theories that our framework gives access to."],"url":"http://arxiv.org/abs/2405.00101v1","category":"hep-th"}
{"created":"2024-04-30 18:00:01","title":"How to rule out $(g-2)_\u03bc$ in $U(1)_{L_\u03bc-L_\u03c4}$ with White Dwarf Cooling","abstract":"In recent years, the gauge group $U(1)_{L_\\mu-L_\\tau}$ has received a lot of attention since it can, in principle, account for the observed excess in the anomalous muon magnetic moment $(g-2)_\\mu$, as well as the Hubble tension. Due to unavoidable, loop-induced kinetic mixing with the SM photon and $Z$, the $U(1)_{L_\\mu-L_\\tau}$ gauge boson $A'$ can contribute to stellar cooling via decays into neutrinos. In this work, we perform for the first time an \\textit{ab initio} computation of the neutrino emissivities of white dwarf stars due to plasmon decay in a model of gauged $U(1)_{L_\\mu-L_\\tau}$. Our central finding is that an observation of the early-stage white dwarf neutrino luminosity at the 30% level could exclude (or partially exclude) the remaining allowed parameter space for explaining $(g-2)_\\mu$. In this work, we present the relevant white dwarf sensitivities over the entire $A'$ mass range. In particular, we have performed a rigorous computation of the luminosities in the resonant regime, where the $A'$ mass is comparable to the white dwarf plasma frequencies.","sentences":["In recent years, the gauge group $U(1)_{L_\\mu-L_\\tau}$ has received a lot of attention since it can, in principle, account for the observed excess in the anomalous muon magnetic moment $(g-2)_\\mu$, as well as the Hubble tension.","Due to unavoidable, loop-induced kinetic mixing with the SM photon and $Z$, the $U(1)_{L_\\mu-L_\\tau}$ gauge boson $A'$ can contribute to stellar cooling via decays into neutrinos.","In this work, we perform for the first time an \\textit{ab initio} computation of the neutrino emissivities of white dwarf stars due to plasmon decay in a model of gauged $U(1)_{L_\\mu-L_\\tau}$. Our central finding is that an observation of the early-stage white dwarf neutrino luminosity at the 30% level could exclude (or partially exclude) the remaining allowed parameter space for explaining $(g-2)_\\mu$. In this work, we present the relevant white dwarf sensitivities over the entire $A'$ mass range.","In particular, we have performed a rigorous computation of the luminosities in the resonant regime, where the $A'$ mass is comparable to the white dwarf plasma frequencies."],"url":"http://arxiv.org/abs/2405.00094v1","category":"hep-ph"}
{"created":"2024-04-30 18:00:01","title":"Evidence for a toroidal magnetic field in the core of 3C 84","abstract":"The spatial scales of relativistic radio jets, probed by relativistic magneto-hydrodynamic jet launching simulations (RMHDs) and by most very-long-baseline interferometry (VLBI) observations differ by an order of magnitude. Bridging the gap between these RMHD simulations and VLBI observations requires selecting nearby active galactic nuclei (AGN), the parsec-scale region of which can be resolved. 3C 84 is a nearby bright AGN fulfilling the necessary requirements: it is launching a powerful, relativistic jet powered by a central supermassive black hole, while also being very bright. Using 22 GHz global VLBI measurements of 3C 84 we aim to study its sub-parsec region in both total intensity and linear polarisation, to explore the properties of this jet, with a linear resolution of $\\sim0.1$ parsec. We test different simulation setups by altering the bulk Lorentz factor $\\Gamma$ of the jet, as well as the magnetic field configuration (toroidal, poloidal, helical). We confirm the persistence of a limb brightened structure, which reaches deep into the sub-parsec region. The corresponding electric vector position angles (EVPAs) follow the bulk jet flow inside but tend to be orthogonal to it near the edges. Our state-of-the-art RMHD simulations show that this geometry is consistent with a spine-sheath model, associated with a mildly relativistic flow and a toroidal magnetic field configuration.","sentences":["The spatial scales of relativistic radio jets, probed by relativistic magneto-hydrodynamic jet launching simulations (RMHDs) and by most very-long-baseline interferometry (VLBI) observations differ by an order of magnitude.","Bridging the gap between these RMHD simulations and VLBI observations requires selecting nearby active galactic nuclei (AGN), the parsec-scale region of which can be resolved.","3C 84 is a nearby bright AGN fulfilling the necessary requirements: it is launching a powerful, relativistic jet powered by a central supermassive black hole, while also being very bright.","Using 22 GHz global VLBI measurements of 3C 84 we aim to study its sub-parsec region in both total intensity and linear polarisation, to explore the properties of this jet, with a linear resolution of $\\sim0.1$ parsec.","We test different simulation setups by altering the bulk Lorentz factor $\\Gamma$ of the jet, as well as the magnetic field configuration (toroidal, poloidal, helical).","We confirm the persistence of a limb brightened structure, which reaches deep into the sub-parsec region.","The corresponding electric vector position angles (EVPAs) follow the bulk jet flow inside but tend to be orthogonal to it near the edges.","Our state-of-the-art RMHD simulations show that this geometry is consistent with a spine-sheath model, associated with a mildly relativistic flow and a toroidal magnetic field configuration."],"url":"http://arxiv.org/abs/2405.00097v1","category":"astro-ph.HE"}
{"created":"2024-04-30 18:00:00","title":"Density of States, Black Holes and the Emergent String Conjecture","abstract":"We study universal features of the density of one-particle states $\\rho(E)$ in weakly coupled theories of gravity at energies above the quantum gravity cutoff $\\Lambda$, defined as the scale suppressing higher-derivative corrections to the Einstein--Hilbert action. Using thermodynamic properties of black holes, we show that in asymptotically flat spacetimes, certain features of $\\rho(E)$ above the black hole threshold $M_{\\rm min}$ are an indicator for the existence of large extra dimensions, and cannot be reproduced by any lower-dimensional field theory with finitely many fields satisfying the weak energy condition. Based on the properties of gravitational scattering amplitudes, we argue that there needs to exist a (possibly higher-dimensional) effective description of gravity valid up to the cutoff $\\Lambda$. Combining this with thermodynamic arguments we demonstrate that $\\rho(E)$ has to grow exponentially for energies $\\Lambda \\ll E \\ll M_{\\rm min}$. Furthermore we show that the tension of any weakly coupled $p$-brane with $p\\geq 1$ is bounded from below by $\\Lambda^{p-1}$. We use this to argue that any tower of weakly coupled states with mass below $\\Lambda$ has to be a Kaluza--Klein (KK) tower. Altogether these results indicate that in gravitational weak-coupling limits the lightest tower of states is either a KK tower, or has an exponentially growing degeneracy thereby resembling a string tower. This provides evidence for the Emergent String Conjecture without explicitly relying on string theory or supersymmetry.","sentences":["We study universal features of the density of one-particle states $\\rho(E)$ in weakly coupled theories of gravity at energies above the quantum gravity cutoff $\\Lambda$, defined as the scale suppressing higher-derivative corrections to the Einstein--Hilbert action.","Using thermodynamic properties of black holes, we show that in asymptotically flat spacetimes, certain features of $\\rho(E)$ above the black hole threshold $M_{\\rm min}$ are an indicator for the existence of large extra dimensions, and cannot be reproduced by any lower-dimensional field theory with finitely many fields satisfying the weak energy condition.","Based on the properties of gravitational scattering amplitudes, we argue that there needs to exist a (possibly higher-dimensional) effective description of gravity valid up to the cutoff $\\Lambda$. Combining this with thermodynamic arguments we demonstrate that $\\rho(E)$ has to grow exponentially for energies $\\Lambda","\\ll E \\ll M_{\\rm","min}$.","Furthermore we show that the tension of any weakly coupled $p$-brane with $p\\geq 1$ is bounded from below by $\\Lambda^{p-1}$. We use this to argue that any tower of weakly coupled states with mass below $\\Lambda$ has to be a Kaluza--Klein (KK) tower.","Altogether these results indicate that in gravitational weak-coupling limits the lightest tower of states is either a KK tower, or has an exponentially growing degeneracy thereby resembling a string tower.","This provides evidence for the Emergent String Conjecture without explicitly relying on string theory or supersymmetry."],"url":"http://arxiv.org/abs/2405.00083v1","category":"hep-th"}
{"created":"2024-04-30 18:00:00","title":"Quasi-stars as a Means of Rapid Black Hole Growth in the Early Universe","abstract":"JWST observations demonstrate that supermassive black holes (SMBHs) exist by redshifts $z \\gtrsim 10$, providing further evidence for \"direct collapse\" black hole (BH) formation, whereby massive ($\\sim 10^{3-5} M_{\\odot}$) SMBH seeds are generated within a few Myr as a byproduct of the rapid inflow of gas into the centers of protogalaxies. Here we analyze the intermediate \"quasi-star\" phase that accompanies some direct collapse models, during which a natal BH accretes mass from and energetically sustains (through accretion) an overlying gaseous envelope. We argue that previous estimates of the maximum BH mass that can be reached during this stage, $\\sim 1\\%$ of the total quasi-star mass, are unphysical, and arise from underestimating the efficiency with which energy can be transported outward from regions close to the BH. We construct new quasi-star models that consist of an inner, \"saturated-convection\" region (which conforms to a convection-dominated accretion flow near the BH) matched to an outer, adiabatic envelope. These solutions exist up to a BH mass of $\\sim 60\\%$ the total quasi-star mass, at which point the adiabatic envelope contains only 2\\% of the mass (with the remaining $\\sim 38\\%$ in the saturated-convection region), and this upper limit is reached within a time of $20-40$ Myr. We conclude that quasi-stars remain a viable route for producing SMBHs at large redshifts, consistent with recent JWST observations.","sentences":["JWST observations demonstrate that supermassive black holes (SMBHs) exist by redshifts $z \\gtrsim 10$, providing further evidence for \"direct collapse\" black hole (BH) formation, whereby massive ($\\sim 10^{3-5} M_{\\odot}$) SMBH seeds are generated within a few Myr as a byproduct of the rapid inflow of gas into the centers of protogalaxies.","Here we analyze the intermediate \"quasi-star\" phase that accompanies some direct collapse models, during which a natal BH accretes mass from and energetically sustains (through accretion) an overlying gaseous envelope.","We argue that previous estimates of the maximum BH mass that can be reached during this stage, $\\sim 1\\%$ of the total quasi-star mass, are unphysical, and arise from underestimating the efficiency with which energy can be transported outward from regions close to the BH.","We construct new quasi-star models that consist of an inner, \"saturated-convection\" region (which conforms to a convection-dominated accretion flow near the BH) matched to an outer, adiabatic envelope.","These solutions exist up to a BH mass of $\\sim 60\\%$ the total quasi-star mass, at which point the adiabatic envelope contains only 2\\% of the mass (with the remaining $\\sim 38\\%$ in the saturated-convection region), and this upper limit is reached within a time of $20-40$ Myr.","We conclude that quasi-stars remain a viable route for producing SMBHs at large redshifts, consistent with recent JWST observations."],"url":"http://arxiv.org/abs/2405.00084v1","category":"astro-ph.GA"}
{"created":"2024-04-30 18:00:00","title":"NICER Discovery of the Accreting Millisecond X-ray Pulsar SRGA J144459.2-604207","abstract":"We present the discovery, with the Neutron Star Interior Composition Explorer (NICER), of the 447.9 Hz accreting millisecond X-ray pulsar (AMXP) SRGA J144459.2-604207, which underwent a four-week long outburst starting on 2024 February 15. The AMXP resides in a 5.22 hr binary, orbiting a low-mass companion donor with $M_d>0.1M_\\odot$. We report on the temporal and spectral properties from NICER observations during the early days of the outburst, from 2024 February 21 through 2024 February 23, during which NICER also detected a type-I X-ray burst that exhibited a plateau lasting ~6 s. The spectra of the persistent emission were well described by an absorbed thermal blackbody and power-law model, with blackbody temperature $kT\\approx0.9{\\rm\\,keV}$ and power-law photon index $\\Gamma\\approx1.9$. Time-resolved burst spectroscopy confirmed the thermonuclear nature of the burst, where an additional blackbody component reached a maximum temperature of nearly $kT\\approx3{\\rm\\,keV}$ at the peak of the burst. We discuss the nature of the companion as well as the type-I X-ray burst.","sentences":["We present the discovery, with the Neutron Star Interior Composition Explorer (NICER), of the 447.9 Hz accreting millisecond X-ray pulsar (AMXP) SRGA J144459.2-604207, which underwent a four-week long outburst starting on 2024 February 15.","The AMXP resides in a 5.22 hr binary, orbiting a low-mass companion donor with $M_d>0.1M_\\odot$. We report on the temporal and spectral properties from NICER observations during the early days of the outburst, from 2024 February 21 through 2024 February 23, during which NICER also detected a type-I X-ray burst that exhibited a plateau lasting ~6 s.","The spectra of the persistent emission were well described by an absorbed thermal blackbody and power-law model, with blackbody temperature $kT\\approx0.9{\\rm\\,keV}$ and power-law photon index $\\Gamma\\approx1.9$. Time-resolved burst spectroscopy confirmed the thermonuclear nature of the burst, where an additional blackbody component reached a maximum temperature of nearly $kT\\approx3{\\rm\\,keV}$ at the peak of the burst.","We discuss the nature of the companion as well as the type-I X-ray burst."],"url":"http://arxiv.org/abs/2405.00087v1","category":"astro-ph.HE"}
{"created":"2024-04-30 18:00:00","title":"Mass from Nothing","abstract":"We study the Abelian Higgs model with multiple scalar fields, but without mass terms. Solving the model non-perturbatively order-by-order in the number of scalar fields, we find that radiative corrections generate masses for the scalar and gauge boson, without spontaneous symmetry breaking. The mass scales are set by the $\\Lambda$-parameter of the electroweak running coupling, thereby naturally avoiding the hierarchy problem. No part of our calculation employs a weak-coupling expansion, and we find that the perturbative vacuum is metastable, and hence must decay to the stable non-perturbative vacuum of the theory, which we identify. Although the field content of our Lagrangian is standard, our results predict the existence of two heavy scalar resonances in addition to the Higgs. We believe that these predicted resonances will ultimately allow experimentalists to discriminate between our method and standard solutions of the Higgs model.","sentences":["We study the Abelian Higgs model with multiple scalar fields, but without mass terms.","Solving the model non-perturbatively order-by-order in the number of scalar fields, we find that radiative corrections generate masses for the scalar and gauge boson, without spontaneous symmetry breaking.","The mass scales are set by the $\\Lambda$-parameter of the electroweak running coupling, thereby naturally avoiding the hierarchy problem.","No part of our calculation employs a weak-coupling expansion, and we find that the perturbative vacuum is metastable, and hence must decay to the stable non-perturbative vacuum of the theory, which we identify.","Although the field content of our Lagrangian is standard, our results predict the existence of two heavy scalar resonances in addition to the Higgs.","We believe that these predicted resonances will ultimately allow experimentalists to discriminate between our method and standard solutions of the Higgs model."],"url":"http://arxiv.org/abs/2405.00088v1","category":"hep-ph"}
{"created":"2024-04-30 18:00:00","title":"Can the QCD axion feed a dark energy component?","abstract":"A pseudo Nambu-Goldstone boson (PNGB) coupled to a confining gauge group via an anomalous term is characterised, during the confining phase transition, by a temperature dependent mass $m^2(T) \\propto T^{-n}$. For $n>2$, a non-relativistic population of such particles dominating the cosmological energy density would act as dark energy (DE), accelerating the expansion. We study the possibility that a PNGB $\\varphi_b$ coupled to a hidden gauge group that is presently undergoing confinement could realise this scenario. To obtain the observed amount of DE, the number density of $\\varphi_b$ must be boosted by some mechanism. Assuming that the QCD axion $\\varphi_a$ constitutes the dark matter (DM), a non-adiabatic level crossing between $\\varphi_a$ and $\\varphi_b$ shortly before matter-DE equality can convert a small fraction of DM into DE, providing such mechanism and explaining the coincidence puzzle.","sentences":["A pseudo Nambu-Goldstone boson (PNGB) coupled to a confining gauge group via an anomalous term is characterised, during the confining phase transition, by a temperature dependent mass $m^2(T)","\\propto T^{-n}$.","For $n>2$, a non-relativistic population of such particles dominating the cosmological energy density would act as dark energy (DE), accelerating the expansion.","We study the possibility that a PNGB $\\varphi_b$ coupled to a hidden gauge group that is presently undergoing confinement could realise this scenario.","To obtain the observed amount of DE, the number density of $\\varphi_b$ must be boosted by some mechanism.","Assuming that the QCD axion $\\varphi_a$ constitutes the dark matter (DM), a non-adiabatic level crossing between $\\varphi_a$ and $\\varphi_b$ shortly before matter-DE equality can convert a small fraction of DM into DE, providing such mechanism and explaining the coincidence puzzle."],"url":"http://arxiv.org/abs/2405.00090v1","category":"hep-ph"}
