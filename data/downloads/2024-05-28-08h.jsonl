{"created":"2024-05-27 17:59:56","title":"Matryoshka Multimodal Models","abstract":"Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in visual-linguistic reasoning. These models first embed images into a fixed large number of visual tokens and then feed them into a Large Language Model (LLM). However, this design causes an excessive number of tokens for dense visual scenarios such as high-resolution images and videos, leading to great inefficiency. While token pruning/merging methods do exist, they produce a single length output for each image and do not afford flexibility in trading off information density v.s. efficiency. Inspired by the concept of Matryoshka Dolls, we propose M3: Matryoshka Multimodal Models, which learns to represent visual content as nested sets of visual tokens that capture information across multiple coarse-to-fine granularities. Our approach offers several unique benefits for LMMs: (1) One can explicitly control the visual granularity per test instance during inference, e.g. , adjusting the number of tokens used to represent an image based on the anticipated complexity or simplicity of the content; (2) M3 provides a framework for analyzing the granularity needed for existing datasets, where we find that COCO-style benchmarks only need around ~9 visual tokens to obtain accuracy similar to that of using all 576 tokens; (3) Our approach provides a foundation to explore the best trade-off between performance and visual token length at sample level, where our investigation reveals that a large gap exists between the oracle upper bound and current fixed-scale representations.","sentences":["Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in visual-linguistic reasoning.","These models first embed images into a fixed large number of visual tokens and then feed them into a Large Language Model (LLM).","However, this design causes an excessive number of tokens for dense visual scenarios such as high-resolution images and videos, leading to great inefficiency.","While token pruning/merging methods do exist, they produce a single length output for each image and do not afford flexibility in trading off information density v.s. efficiency.","Inspired by the concept of Matryoshka Dolls, we propose M3:","Matryoshka Multimodal Models, which learns to represent visual content as nested sets of visual tokens that capture information across multiple coarse-to-fine granularities.","Our approach offers several unique benefits for LMMs: (1) One can explicitly control the visual granularity per test instance during inference, e.g. , adjusting the number of tokens used to represent an image based on the anticipated complexity or simplicity of the content; (2) M3 provides a framework for analyzing the granularity needed for existing datasets, where we find that COCO-style benchmarks only need around ~9 visual tokens to obtain accuracy similar to that of using all 576 tokens; (3) Our approach provides a foundation to explore the best trade-off between performance and visual token length at sample level, where our investigation reveals that a large gap exists between the oracle upper bound and current fixed-scale representations."],"url":"http://arxiv.org/abs/2405.17430v1","category":"cs.CV"}
{"created":"2024-05-27 17:59:51","title":"GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction","abstract":"3D semantic occupancy prediction aims to obtain 3D fine-grained geometry and semantics of the surrounding scene and is an important task for the robustness of vision-centric autonomous driving. Most existing methods employ dense grids such as voxels as scene representations, which ignore the sparsity of occupancy and the diversity of object scales and thus lead to unbalanced allocation of resources. To address this, we propose an object-centric representation to describe 3D scenes with sparse 3D semantic Gaussians where each Gaussian represents a flexible region of interest and its semantic features. We aggregate information from images through the attention mechanism and iteratively refine the properties of 3D Gaussians including position, covariance, and semantics. We then propose an efficient Gaussian-to-voxel splatting method to generate 3D occupancy predictions, which only aggregates the neighboring Gaussians for a certain position. We conduct extensive experiments on the widely adopted nuScenes and KITTI-360 datasets. Experimental results demonstrate that GaussianFormer achieves comparable performance with state-of-the-art methods with only 17.8% - 24.8% of their memory consumption. Code is available at: https://github.com/huang-yh/GaussianFormer.","sentences":["3D semantic occupancy prediction aims to obtain 3D fine-grained geometry and semantics of the surrounding scene and is an important task for the robustness of vision-centric autonomous driving.","Most existing methods employ dense grids such as voxels as scene representations, which ignore the sparsity of occupancy and the diversity of object scales and thus lead to unbalanced allocation of resources.","To address this, we propose an object-centric representation to describe 3D scenes with sparse 3D semantic Gaussians where each Gaussian represents a flexible region of interest and its semantic features.","We aggregate information from images through the attention mechanism and iteratively refine the properties of 3D Gaussians including position, covariance, and semantics.","We then propose an efficient Gaussian-to-voxel splatting method to generate 3D occupancy predictions, which only aggregates the neighboring Gaussians for a certain position.","We conduct extensive experiments on the widely adopted nuScenes and KITTI-360 datasets.","Experimental results demonstrate that GaussianFormer achieves comparable performance with state-of-the-art methods with only 17.8% - 24.8% of their memory consumption.","Code is available at: https://github.com/huang-yh/GaussianFormer."],"url":"http://arxiv.org/abs/2405.17429v1","category":"cs.CV"}
{"created":"2024-05-27 17:59:45","title":"NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models","abstract":"Decoder-only large language model (LLM)-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce the NV-Embed model with a variety of architectural designs and training procedures to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility. For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last <EOS> token embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For model training, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval datasets into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. Combining these techniques, our NV-Embed model, using only publicly available data, has achieved a record-high score of 69.32, ranking No. 1 on the Massive Text Embedding Benchmark (MTEB) (as of May 24, 2024), with 56 tasks, encompassing retrieval, reranking, classification, clustering, and semantic textual similarity tasks. Notably, our model also attains the highest score of 59.36 on 15 retrieval tasks in the MTEB benchmark (also known as BEIR). We will open-source the model at: https://huggingface.co/nvidia/NV-Embed-v1.","sentences":["Decoder-only large language model (LLM)-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval.","In this work, we introduce the NV-Embed model with a variety of architectural designs and training procedures to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility.","For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last <EOS> token embedding from LLMs.","To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training.","For model training, we introduce a two-stage contrastive instruction-tuning method.","It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples.","At stage-2, it blends various non-retrieval datasets into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance.","Combining these techniques, our NV-Embed model, using only publicly available data, has achieved a record-high score of 69.32, ranking No. 1 on the Massive Text Embedding Benchmark (MTEB) (as of May 24, 2024), with 56 tasks, encompassing retrieval, reranking, classification, clustering, and semantic textual similarity tasks.","Notably, our model also attains the highest score of 59.36 on 15 retrieval tasks in the MTEB benchmark (also known as BEIR).","We will open-source the model at: https://huggingface.co/nvidia/NV-Embed-v1."],"url":"http://arxiv.org/abs/2405.17428v1","category":"cs.CL"}
{"created":"2024-05-27 17:59:41","title":"Reason3D: Searching and Reasoning 3D Segmentation via Large Language Model","abstract":"Recent advancements in multimodal large language models (LLMs) have shown their potential in various domains, especially concept reasoning. Despite these developments, applications in understanding 3D environments remain limited. This paper introduces Reason3D, a novel LLM designed for comprehensive 3D understanding. Reason3D takes point cloud data and text prompts as input to produce textual responses and segmentation masks, facilitating advanced tasks like 3D reasoning segmentation, hierarchical searching, express referring, and question answering with detailed mask outputs. Specifically, we propose a hierarchical mask decoder to locate small objects within expansive scenes. This decoder initially generates a coarse location estimate covering the object's general area. This foundational estimation facilitates a detailed, coarse-to-fine segmentation strategy that significantly enhances the precision of object identification and segmentation. Experiments validate that Reason3D achieves remarkable results on large-scale ScanNet and Matterport3D datasets for 3D express referring, 3D question answering, and 3D reasoning segmentation tasks. Code and models are available at: https://github.com/KuanchihHuang/Reason3D.","sentences":["Recent advancements in multimodal large language models (LLMs) have shown their potential in various domains, especially concept reasoning.","Despite these developments, applications in understanding 3D environments remain limited.","This paper introduces Reason3D, a novel LLM designed for comprehensive 3D understanding.","Reason3D takes point cloud data and text prompts as input to produce textual responses and segmentation masks, facilitating advanced tasks like 3D reasoning segmentation, hierarchical searching, express referring, and question answering with detailed mask outputs.","Specifically, we propose a hierarchical mask decoder to locate small objects within expansive scenes.","This decoder initially generates a coarse location estimate covering the object's general area.","This foundational estimation facilitates a detailed, coarse-to-fine segmentation strategy that significantly enhances the precision of object identification and segmentation.","Experiments validate that Reason3D achieves remarkable results on large-scale ScanNet and Matterport3D datasets for 3D express referring, 3D question answering, and 3D reasoning segmentation tasks.","Code and models are available at: https://github.com/KuanchihHuang/Reason3D."],"url":"http://arxiv.org/abs/2405.17427v1","category":"cs.CV"}
{"created":"2024-05-27 17:59:32","title":"LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence","abstract":"Due to the need to interact with the real world, embodied agents are required to possess comprehensive prior knowledge, long-horizon planning capability, and a swift response speed. Despite recent large language model (LLM) based agents achieving promising performance, they still exhibit several limitations. For instance, the output of LLMs is a descriptive sentence, which is ambiguous when determining specific actions. To address these limitations, we introduce the large auto-regressive model (LARM). LARM leverages both text and multi-view images as input and predicts subsequent actions in an auto-regressive manner. To train LARM, we develop a novel data format named auto-regressive node transmission structure and assemble a corresponding dataset. Adopting a two-phase training regimen, LARM successfully harvests enchanted equipment in Minecraft, which demands significantly more complex decision-making chains than the highest achievements of prior best methods. Besides, the speed of LARM is 6.8x faster.","sentences":["Due to the need to interact with the real world, embodied agents are required to possess comprehensive prior knowledge, long-horizon planning capability, and a swift response speed.","Despite recent large language model (LLM) based agents achieving promising performance, they still exhibit several limitations.","For instance, the output of LLMs is a descriptive sentence, which is ambiguous when determining specific actions.","To address these limitations, we introduce the large auto-regressive model (LARM).","LARM leverages both text and multi-view images as input and predicts subsequent actions in an auto-regressive manner.","To train LARM, we develop a novel data format named auto-regressive node transmission structure and assemble a corresponding dataset.","Adopting a two-phase training regimen, LARM successfully harvests enchanted equipment in Minecraft, which demands significantly more complex decision-making chains than the highest achievements of prior best methods.","Besides, the speed of LARM is 6.8x faster."],"url":"http://arxiv.org/abs/2405.17424v1","category":"cs.CV"}
{"created":"2024-05-27 17:59:25","title":"Privacy-Aware Visual Language Models","abstract":"This paper aims to advance our understanding of how Visual Language Models (VLMs) handle privacy-sensitive information, a crucial concern as these technologies become integral to everyday life. To this end, we introduce a new benchmark PrivBench, which contains images from 8 sensitive categories such as passports, or fingerprints. We evaluate 10 state-of-the-art VLMs on this benchmark and observe a generally limited understanding of privacy, highlighting a significant area for model improvement. Based on this we introduce PrivTune, a new instruction-tuning dataset aimed at equipping VLMs with knowledge about visual privacy. By tuning two pretrained VLMs, TinyLLaVa and MiniGPT-v2, on this small dataset, we achieve strong gains in their ability to recognize sensitive content, outperforming even GPT4-V. At the same time, we show that privacy-tuning only minimally affects the VLMs performance on standard benchmarks such as VQA. Overall, this paper lays out a crucial challenge for making VLMs effective in handling real-world data safely and provides a simple recipe that takes the first step towards building privacy-aware VLMs.","sentences":["This paper aims to advance our understanding of how Visual Language Models (VLMs) handle privacy-sensitive information, a crucial concern as these technologies become integral to everyday life.","To this end, we introduce a new benchmark PrivBench, which contains images from 8 sensitive categories such as passports, or fingerprints.","We evaluate 10 state-of-the-art VLMs on this benchmark and observe a generally limited understanding of privacy, highlighting a significant area for model improvement.","Based on this we introduce PrivTune, a new instruction-tuning dataset aimed at equipping VLMs with knowledge about visual privacy.","By tuning two pretrained VLMs, TinyLLaVa and MiniGPT-v2, on this small dataset, we achieve strong gains in their ability to recognize sensitive content, outperforming even GPT4-V.","At the same time, we show that privacy-tuning only minimally affects the VLMs performance on standard benchmarks such as VQA.","Overall, this paper lays out a crucial challenge for making VLMs effective in handling real-world data safely and provides a simple recipe that takes the first step towards building privacy-aware VLMs."],"url":"http://arxiv.org/abs/2405.17423v1","category":"cs.CV"}
{"created":"2024-05-27 17:59:23","title":"Hardness-Aware Scene Synthesis for Semi-Supervised 3D Object Detection","abstract":"3D object detection aims to recover the 3D information of concerning objects and serves as the fundamental task of autonomous driving perception. Its performance greatly depends on the scale of labeled training data, yet it is costly to obtain high-quality annotations for point cloud data. While conventional methods focus on generating pseudo-labels for unlabeled samples as supplements for training, the structural nature of 3D point cloud data facilitates the composition of objects and backgrounds to synthesize realistic scenes. Motivated by this, we propose a hardness-aware scene synthesis (HASS) method to generate adaptive synthetic scenes to improve the generalization of the detection models. We obtain pseudo-labels for unlabeled objects and generate diverse scenes with different compositions of objects and backgrounds. As the scene synthesis is sensitive to the quality of pseudo-labels, we further propose a hardness-aware strategy to reduce the effect of low-quality pseudo-labels and maintain a dynamic pseudo-database to ensure the diversity and quality of synthetic scenes. Extensive experimental results on the widely used KITTI and Waymo datasets demonstrate the superiority of the proposed HASS method, which outperforms existing semi-supervised learning methods on 3D object detection. Code: https://github.com/wzzheng/HASS.","sentences":["3D object detection aims to recover the 3D information of concerning objects and serves as the fundamental task of autonomous driving perception.","Its performance greatly depends on the scale of labeled training data, yet it is costly to obtain high-quality annotations for point cloud data.","While conventional methods focus on generating pseudo-labels for unlabeled samples as supplements for training, the structural nature of 3D point cloud data facilitates the composition of objects and backgrounds to synthesize realistic scenes.","Motivated by this, we propose a hardness-aware scene synthesis (HASS) method to generate adaptive synthetic scenes to improve the generalization of the detection models.","We obtain pseudo-labels for unlabeled objects and generate diverse scenes with different compositions of objects and backgrounds.","As the scene synthesis is sensitive to the quality of pseudo-labels, we further propose a hardness-aware strategy to reduce the effect of low-quality pseudo-labels and maintain a dynamic pseudo-database to ensure the diversity and quality of synthetic scenes.","Extensive experimental results on the widely used KITTI and Waymo datasets demonstrate the superiority of the proposed HASS method, which outperforms existing semi-supervised learning methods on 3D object detection.","Code: https://github.com/wzzheng/HASS."],"url":"http://arxiv.org/abs/2405.17422v1","category":"cs.CV"}
{"created":"2024-05-27 17:59:02","title":"MultiOOD: Scaling Out-of-Distribution Detection for Multiple Modalities","abstract":"Detecting out-of-distribution (OOD) samples is important for deploying machine learning models in safety-critical applications such as autonomous driving and robot-assisted surgery. Existing research has mainly focused on unimodal scenarios on image data. However, real-world applications are inherently multimodal, which makes it essential to leverage information from multiple modalities to enhance the efficacy of OOD detection. To establish a foundation for more realistic Multimodal OOD Detection, we introduce the first-of-its-kind benchmark, MultiOOD, characterized by diverse dataset sizes and varying modality combinations. We first evaluate existing unimodal OOD detection algorithms on MultiOOD, observing that the mere inclusion of additional modalities yields substantial improvements. This underscores the importance of utilizing multiple modalities for OOD detection. Based on the observation of Modality Prediction Discrepancy between in-distribution (ID) and OOD data, and its strong correlation with OOD performance, we propose the Agree-to-Disagree (A2D) algorithm to encourage such discrepancy during training. Moreover, we introduce a novel outlier synthesis method, NP-Mix, which explores broader feature spaces by leveraging the information from nearest neighbor classes and complements A2D to strengthen OOD detection performance. Extensive experiments on MultiOOD demonstrate that training with A2D and NP-Mix improves existing OOD detection algorithms by a large margin. Our source code and MultiOOD benchmark are available at https://github.com/donghao51/MultiOOD.","sentences":["Detecting out-of-distribution (OOD) samples is important for deploying machine learning models in safety-critical applications such as autonomous driving and robot-assisted surgery.","Existing research has mainly focused on unimodal scenarios on image data.","However, real-world applications are inherently multimodal, which makes it essential to leverage information from multiple modalities to enhance the efficacy of OOD detection.","To establish a foundation for more realistic Multimodal OOD Detection, we introduce the first-of-its-kind benchmark, MultiOOD, characterized by diverse dataset sizes and varying modality combinations.","We first evaluate existing unimodal OOD detection algorithms on MultiOOD, observing that the mere inclusion of additional modalities yields substantial improvements.","This underscores the importance of utilizing multiple modalities for OOD detection.","Based on the observation of Modality Prediction Discrepancy between in-distribution (ID) and OOD data, and its strong correlation with OOD performance, we propose the Agree-to-Disagree (A2D) algorithm to encourage such discrepancy during training.","Moreover, we introduce a novel outlier synthesis method, NP-Mix, which explores broader feature spaces by leveraging the information from nearest neighbor classes and complements A2D to strengthen OOD detection performance.","Extensive experiments on MultiOOD demonstrate that training with A2D and NP-Mix improves existing OOD detection algorithms by a large margin.","Our source code and MultiOOD benchmark are available at https://github.com/donghao51/MultiOOD."],"url":"http://arxiv.org/abs/2405.17419v1","category":"cs.CV"}
{"created":"2024-05-27 17:58:48","title":"Self-Corrected Multimodal Large Language Model for End-to-End Robot Manipulation","abstract":"Robot manipulation policies have shown unsatisfactory action performance when confronted with novel task or object instances. Hence, the capability to automatically detect and self-correct failure action is essential for a practical robotic system. Recently, Multimodal Large Language Models (MLLMs) have shown promise in visual instruction following and demonstrated strong reasoning abilities in various tasks. To unleash general MLLMs as an end-to-end robotic agent, we introduce a Self-Corrected (SC)-MLLM, equipping our model not only to predict end-effector poses but also to autonomously recognize and correct failure actions. Specifically, we first conduct parameter-efficient fine-tuning to empower MLLM with pose prediction ability, which is reframed as a language modeling problem. When facing execution failures, our model learns to identify low-level action error causes (i.e., position and rotation errors) and adaptively seeks prompt feedback from experts. Based on the feedback, SC-MLLM rethinks the current failure scene and generates the corrected actions. Furthermore, we design a continuous policy learning method for successfully corrected samples, enhancing the model's adaptability to the current scene configuration and reducing the frequency of expert intervention. To evaluate our SC-MLLM, we conduct extensive experiments in both simulation and real-world settings. SC-MLLM agent significantly improve manipulation accuracy compared to previous state-of-the-art robotic MLLM (ManipLLM), increasing from 57\\% to 79\\% on seen object categories and from 47\\% to 69\\% on unseen novel categories.","sentences":["Robot manipulation policies have shown unsatisfactory action performance when confronted with novel task or object instances.","Hence, the capability to automatically detect and self-correct failure action is essential for a practical robotic system.","Recently, Multimodal Large Language Models (MLLMs) have shown promise in visual instruction following and demonstrated strong reasoning abilities in various tasks.","To unleash general MLLMs as an end-to-end robotic agent, we introduce a Self-Corrected (SC)-MLLM, equipping our model not only to predict end-effector poses but also to autonomously recognize and correct failure actions.","Specifically, we first conduct parameter-efficient fine-tuning to empower MLLM with pose prediction ability, which is reframed as a language modeling problem.","When facing execution failures, our model learns to identify low-level action error causes (i.e., position and rotation errors) and adaptively seeks prompt feedback from experts.","Based on the feedback, SC-MLLM rethinks the current failure scene and generates the corrected actions.","Furthermore, we design a continuous policy learning method for successfully corrected samples, enhancing the model's adaptability to the current scene configuration and reducing the frequency of expert intervention.","To evaluate our SC-MLLM, we conduct extensive experiments in both simulation and real-world settings.","SC-MLLM agent significantly improve manipulation accuracy compared to previous state-of-the-art robotic MLLM (ManipLLM), increasing from 57\\% to 79\\% on seen object categories and from 47\\% to 69\\% on unseen novel categories."],"url":"http://arxiv.org/abs/2405.17418v1","category":"cs.CV"}
{"created":"2024-05-27 17:58:23","title":"A Recipe for Unbounded Data Augmentation in Visual Reinforcement Learning","abstract":"$Q$-learning algorithms are appealing for real-world applications due to their data-efficiency, but they are very prone to overfitting and training instabilities when trained from visual observations. Prior work, namely SVEA, finds that selective application of data augmentation can improve the visual generalization of RL agents without destabilizing training. We revisit its recipe for data augmentation, and find an assumption that limits its effectiveness to augmentations of a photometric nature. Addressing these limitations, we propose a generalized recipe, SADA, that works with wider varieties of augmentations. We benchmark its effectiveness on DMC-GB2 -- our proposed extension of the popular DMControl Generalization Benchmark -- as well as tasks from Meta-World and the Distracting Control Suite, and find that our method, SADA, greatly improves training stability and generalization of RL agents across a diverse set of augmentations. Visualizations, code, and benchmark: see https://aalmuzairee.github.io/SADA/","sentences":["$Q$-learning algorithms are appealing for real-world applications due to their data-efficiency, but they are very prone to overfitting and training instabilities when trained from visual observations.","Prior work, namely SVEA, finds that selective application of data augmentation can improve the visual generalization of RL agents without destabilizing training.","We revisit its recipe for data augmentation, and find an assumption that limits its effectiveness to augmentations of a photometric nature.","Addressing these limitations, we propose a generalized recipe, SADA, that works with wider varieties of augmentations.","We benchmark its effectiveness on DMC-GB2 -- our proposed extension of the popular DMControl Generalization Benchmark -- as well as tasks from Meta-World and the Distracting Control Suite, and find that our method, SADA, greatly improves training stability and generalization of RL agents across a diverse set of augmentations.","Visualizations, code, and benchmark: see https://aalmuzairee.github.io/SADA/"],"url":"http://arxiv.org/abs/2405.17416v1","category":"cs.LG"}
{"created":"2024-05-27 17:58:01","title":"Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control","abstract":"Research on video generation has recently made tremendous progress, enabling high-quality videos to be generated from text prompts or images. Adding control to the video generation process is an important goal moving forward and recent approaches that condition video generation models on camera trajectories make strides towards it. Yet, it remains challenging to generate a video of the same scene from multiple different camera trajectories. Solutions to this multi-video generation problem could enable large-scale 3D scene generation with editable camera trajectories, among other applications. We introduce collaborative video diffusion (CVD) as an important step towards this vision. The CVD framework includes a novel cross-video synchronization module that promotes consistency between corresponding frames of the same video rendered from different camera poses using an epipolar attention mechanism. Trained on top of a state-of-the-art camera-control module for video generation, CVD generates multiple videos rendered from different camera trajectories with significantly better consistency than baselines, as shown in extensive experiments. Project page: https://collaborativevideodiffusion.github.io/.","sentences":["Research on video generation has recently made tremendous progress, enabling high-quality videos to be generated from text prompts or images.","Adding control to the video generation process is an important goal moving forward and recent approaches that condition video generation models on camera trajectories make strides towards it.","Yet, it remains challenging to generate a video of the same scene from multiple different camera trajectories.","Solutions to this multi-video generation problem could enable large-scale 3D scene generation with editable camera trajectories, among other applications.","We introduce collaborative video diffusion (CVD) as an important step towards this vision.","The CVD framework includes a novel cross-video synchronization module that promotes consistency between corresponding frames of the same video rendered from different camera poses using an epipolar attention mechanism.","Trained on top of a state-of-the-art camera-control module for video generation, CVD generates multiple videos rendered from different camera trajectories with significantly better consistency than baselines, as shown in extensive experiments.","Project page: https://collaborativevideodiffusion.github.io/."],"url":"http://arxiv.org/abs/2405.17414v1","category":"cs.CV"}
{"created":"2024-05-27 17:57:20","title":"Enhancing Music Genre Classification through Multi-Algorithm Analysis and User-Friendly Visualization","abstract":"The aim of this study is to teach an algorithm how to recognize different types of music. Users will submit songs for analysis. Since the algorithm hasn't heard these songs before, it needs to figure out what makes each song unique. It does this by breaking down the songs into different parts and studying things like rhythm, melody, and tone via supervised learning because the program learns from examples that are already labelled. One important thing to consider when classifying music is its genre, which can be quite complex. To ensure accuracy, we use five different algorithms, each working independently, to analyze the songs. This helps us get a more complete understanding of each song's characteristics. Therefore, our goal is to correctly identify the genre of each submitted song. Once the analysis is done, the results are presented using a graphing tool, making it easy for users to understand and provide feedback.","sentences":["The aim of this study is to teach an algorithm how to recognize different types of music.","Users will submit songs for analysis.","Since the algorithm hasn't heard these songs before, it needs to figure out what makes each song unique.","It does this by breaking down the songs into different parts and studying things like rhythm, melody, and tone via supervised learning because the program learns from examples that are already labelled.","One important thing to consider when classifying music is its genre, which can be quite complex.","To ensure accuracy, we use five different algorithms, each working independently, to analyze the songs.","This helps us get a more complete understanding of each song's characteristics.","Therefore, our goal is to correctly identify the genre of each submitted song.","Once the analysis is done, the results are presented using a graphing tool, making it easy for users to understand and provide feedback."],"url":"http://arxiv.org/abs/2405.17413v1","category":"cs.SD"}
{"created":"2024-05-27 17:57:12","title":"Towards One Model for Classical Dimensionality Reduction: A Probabilistic Perspective on UMAP and t-SNE","abstract":"This paper shows that the dimensionality reduction methods, UMAP and t-SNE, can be approximately recast as MAP inference methods corresponding to a generalized Wishart-based model introduced in ProbDR. This interpretation offers deeper theoretical insights into these algorithms, while introducing tools with which similar dimensionality reduction methods can be studied.","sentences":["This paper shows that the dimensionality reduction methods, UMAP and t-SNE, can be approximately recast as MAP inference methods corresponding to a generalized Wishart-based model introduced in ProbDR.","This interpretation offers deeper theoretical insights into these algorithms, while introducing tools with which similar dimensionality reduction methods can be studied."],"url":"http://arxiv.org/abs/2405.17412v1","category":"stat.ML"}
{"created":"2024-05-27 17:57:08","title":"A universal constraint on axion non-Abelian dynamics during inflation","abstract":"Inflationary models equipped with Chern-Simons coupling between their axion and gauge sectors exhibit an array of interesting signals including a testable chiral gravitational wave spectrum. The energy injection in the gauge sector triggered by the rolling axion leads to a well-studied enhancement of gauge field fluctuations. These may in turn affect observables such as the scalar and tensor spectra and also account for non-linear corrections to field propagators. In this work, we focus on non-Abelian gauge sectors. We show that gauge field self-interactions and axion-gauge field non-linear couplings significantly renormalize the gauge field mode function. Operating within the regime of validity of the perturbative treatment places strong constraints on the accessible parameter space of this class of models. We calculate corrections to the gauge field propagator that are universally present in these scenarios. Enforcing perturbativity on such propagators leads to bounds that are competitive with those stemming from analytical estimates on the onset of the strong backreaction regime.","sentences":["Inflationary models equipped with Chern-Simons coupling between their axion and gauge sectors exhibit an array of interesting signals including a testable chiral gravitational wave spectrum.","The energy injection in the gauge sector triggered by the rolling axion leads to a well-studied enhancement of gauge field fluctuations.","These may in turn affect observables such as the scalar and tensor spectra and also account for non-linear corrections to field propagators.","In this work, we focus on non-Abelian gauge sectors.","We show that gauge field self-interactions and axion-gauge field non-linear couplings significantly renormalize the gauge field mode function.","Operating within the regime of validity of the perturbative treatment places strong constraints on the accessible parameter space of this class of models.","We calculate corrections to the gauge field propagator that are universally present in these scenarios.","Enforcing perturbativity on such propagators leads to bounds that are competitive with those stemming from analytical estimates on the onset of the strong backreaction regime."],"url":"http://arxiv.org/abs/2405.17411v1","category":"astro-ph.CO"}
{"created":"2024-05-27 17:56:51","title":"Ultraheavy Ultrahigh-Energy Cosmic Rays","abstract":"We investigate the propagation of ultraheavy (UH) nuclei as ultrahigh-energy cosmic rays (UHECRs). We show that their energy loss lengths at $\\lesssim300$~EeV are significantly longer than those of protons and intermediate nuclei, and that the highest-energy cosmic rays with energies beyond $\\sim100$~EeV, including the Amaterasu particle, may originate from such UH-UHECRs. We derive constraints on the contribution of UH-UHECR sources, and find that they are consistent with energy generation rate densities of UHECRs from collapsars and neutron star mergers.","sentences":["We investigate the propagation of ultraheavy (UH) nuclei as ultrahigh-energy cosmic rays (UHECRs).","We show that their energy loss lengths at $\\lesssim300$~EeV are significantly longer than those of protons and intermediate nuclei, and that the highest-energy cosmic rays with energies beyond $\\sim100$~EeV, including the Amaterasu particle, may originate from such UH-UHECRs.","We derive constraints on the contribution of UH-UHECR sources, and find that they are consistent with energy generation rate densities of UHECRs from collapsars and neutron star mergers."],"url":"http://arxiv.org/abs/2405.17409v1","category":"astro-ph.HE"}
{"created":"2024-05-27 17:56:05","title":"Revisiting flares in Sagittarius A* based on general relativistic magnetohydrodynamic numerical simulations of black hole accretion","abstract":"High-resolution observations with GRAVITY-VLTI instrument have provided abundant information about the flares in Sgr A*, the supermassive black hole in our Galactic center, including the time-dependent location of the centroid (a \"hot spot\"), the light curve, and polarization. Yuan et al. (2009) proposed a \"coronal mass ejection\" model to explain the flares and their association with the plasma ejection. The key idea is that magnetic reconnection in the accretion flow produces the flares and results in the formation and ejection of flux ropes. The dynamical process proposed in the model has been confirmed by three-dimensional GRMHD simulations in a later work. Based on this scenario, in our previous works the radiation of the flux rope has been calculated analytically and compared to the observations. In the present paper, we develop the model by directly using numerical simulation data to interpret observations. We first identify flux ropes formed due to reconnection from the data. By assuming that electrons are accelerated in the reconnection current sheet and flow into the flux rope and emit their radiation there, we have calculated the time-dependent energy distribution of electrons after phenomenologically considering their injection due to reconnection acceleration, radiative and adiabatic cooling. The radiation of these electrons is calculated using the ray-tracing approach. The trajectory of the hot spot, the radiation light curve during the flare, and the polarization are calculated. These results are compared with the GRAVITY observations and good consistencies are found.","sentences":["High-resolution observations with GRAVITY-VLTI instrument have provided abundant information about the flares in Sgr A*, the supermassive black hole in our Galactic center, including the time-dependent location of the centroid (a \"hot spot\"), the light curve, and polarization.","Yuan et al.","(2009) proposed a \"coronal mass ejection\" model to explain the flares and their association with the plasma ejection.","The key idea is that magnetic reconnection in the accretion flow produces the flares and results in the formation and ejection of flux ropes.","The dynamical process proposed in the model has been confirmed by three-dimensional GRMHD simulations in a later work.","Based on this scenario, in our previous works the radiation of the flux rope has been calculated analytically and compared to the observations.","In the present paper, we develop the model by directly using numerical simulation data to interpret observations.","We first identify flux ropes formed due to reconnection from the data.","By assuming that electrons are accelerated in the reconnection current sheet and flow into the flux rope and emit their radiation there, we have calculated the time-dependent energy distribution of electrons after phenomenologically considering their injection due to reconnection acceleration, radiative and adiabatic cooling.","The radiation of these electrons is calculated using the ray-tracing approach.","The trajectory of the hot spot, the radiation light curve during the flare, and the polarization are calculated.","These results are compared with the GRAVITY observations and good consistencies are found."],"url":"http://arxiv.org/abs/2405.17408v1","category":"astro-ph.HE"}
{"created":"2024-05-27 17:53:29","title":"Human4DiT: Free-view Human Video Generation with 4D Diffusion Transformer","abstract":"We present a novel approach for generating high-quality, spatio-temporally coherent human videos from a single image under arbitrary viewpoints. Our framework combines the strengths of U-Nets for accurate condition injection and diffusion transformers for capturing global correlations across viewpoints and time. The core is a cascaded 4D transformer architecture that factorizes attention across views, time, and spatial dimensions, enabling efficient modeling of the 4D space. Precise conditioning is achieved by injecting human identity, camera parameters, and temporal signals into the respective transformers. To train this model, we curate a multi-dimensional dataset spanning images, videos, multi-view data and 3D/4D scans, along with a multi-dimensional training strategy. Our approach overcomes the limitations of previous methods based on GAN or UNet-based diffusion models, which struggle with complex motions and viewpoint changes. Through extensive experiments, we demonstrate our method's ability to synthesize realistic, coherent and free-view human videos, paving the way for advanced multimedia applications in areas such as virtual reality and animation. Our project website is https://human4dit.github.io.","sentences":["We present a novel approach for generating high-quality, spatio-temporally coherent human videos from a single image under arbitrary viewpoints.","Our framework combines the strengths of U-Nets for accurate condition injection and diffusion transformers for capturing global correlations across viewpoints and time.","The core is a cascaded 4D transformer architecture that factorizes attention across views, time, and spatial dimensions, enabling efficient modeling of the 4D space.","Precise conditioning is achieved by injecting human identity, camera parameters, and temporal signals into the respective transformers.","To train this model, we curate a multi-dimensional dataset spanning images, videos, multi-view data and 3D/4D scans, along with a multi-dimensional training strategy.","Our approach overcomes the limitations of previous methods based on GAN or UNet-based diffusion models, which struggle with complex motions and viewpoint changes.","Through extensive experiments, we demonstrate our method's ability to synthesize realistic, coherent and free-view human videos, paving the way for advanced multimedia applications in areas such as virtual reality and animation.","Our project website is https://human4dit.github.io."],"url":"http://arxiv.org/abs/2405.17405v1","category":"cs.CV"}
{"created":"2024-05-27 17:52:12","title":"Spectral Greedy Coresets for Graph Neural Networks","abstract":"The ubiquity of large-scale graphs in node-classification tasks significantly hinders the real-world applications of Graph Neural Networks (GNNs). Node sampling, graph coarsening, and dataset condensation are effective strategies for enhancing data efficiency. However, owing to the interdependence of graph nodes, coreset selection, which selects subsets of the data examples, has not been successfully applied to speed up GNN training on large graphs, warranting special treatment. This paper studies graph coresets for GNNs and avoids the interdependence issue by selecting ego-graphs (i.e., neighborhood subgraphs around a node) based on their spectral embeddings. We decompose the coreset selection problem for GNNs into two phases: a coarse selection of widely spread ego graphs and a refined selection to diversify their topologies. We design a greedy algorithm that approximately optimizes both objectives. Our spectral greedy graph coreset (SGGC) scales to graphs with millions of nodes, obviates the need for model pre-training, and applies to low-homophily graphs. Extensive experiments on ten datasets demonstrate that SGGC outperforms other coreset methods by a wide margin, generalizes well across GNN architectures, and is much faster than graph condensation.","sentences":["The ubiquity of large-scale graphs in node-classification tasks significantly hinders the real-world applications of Graph Neural Networks (GNNs).","Node sampling, graph coarsening, and dataset condensation are effective strategies for enhancing data efficiency.","However, owing to the interdependence of graph nodes, coreset selection, which selects subsets of the data examples, has not been successfully applied to speed up GNN training on large graphs, warranting special treatment.","This paper studies graph coresets for GNNs and avoids the interdependence issue by selecting ego-graphs (i.e., neighborhood subgraphs around a node) based on their spectral embeddings.","We decompose the coreset selection problem for GNNs into two phases: a coarse selection of widely spread ego graphs and a refined selection to diversify their topologies.","We design a greedy algorithm that approximately optimizes both objectives.","Our spectral greedy graph coreset (SGGC) scales to graphs with millions of nodes, obviates the need for model pre-training, and applies to low-homophily graphs.","Extensive experiments on ten datasets demonstrate that SGGC outperforms other coreset methods by a wide margin, generalizes well across GNN architectures, and is much faster than graph condensation."],"url":"http://arxiv.org/abs/2405.17404v1","category":"cs.LG"}
{"created":"2024-05-27 17:51:36","title":"A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training","abstract":"Training diffusion models is always a computation-intensive task. In this paper, we introduce a novel speed-up method for diffusion model training, called, which is based on a closer look at time steps. Our key findings are: i) Time steps can be empirically divided into acceleration, deceleration, and convergence areas based on the process increment. ii) These time steps are imbalanced, with many concentrated in the convergence area. iii) The concentrated steps provide limited benefits for diffusion training. To address this, we design an asymmetric sampling strategy that reduces the frequency of steps from the convergence area while increasing the sampling probability for steps from other areas. Additionally, we propose a weighting strategy to emphasize the importance of time steps with rapid-change process increments. As a plug-and-play and architecture-agnostic approach, SpeeD consistently achieves 3-times acceleration across various diffusion architectures, datasets, and tasks. Notably, due to its simple design, our approach significantly reduces the cost of diffusion model training with minimal overhead. Our research enables more researchers to train diffusion models at a lower cost.","sentences":["Training diffusion models is always a computation-intensive task.","In this paper, we introduce a novel speed-up method for diffusion model training, called, which is based on a closer look at time steps.","Our key findings are: i) Time steps can be empirically divided into acceleration, deceleration, and convergence areas based on the process increment.","ii)","These time steps are imbalanced, with many concentrated in the convergence area.","iii)","The concentrated steps provide limited benefits for diffusion training.","To address this, we design an asymmetric sampling strategy that reduces the frequency of steps from the convergence area while increasing the sampling probability for steps from other areas.","Additionally, we propose a weighting strategy to emphasize the importance of time steps with rapid-change process increments.","As a plug-and-play and architecture-agnostic approach, SpeeD consistently achieves 3-times acceleration across various diffusion architectures, datasets, and tasks.","Notably, due to its simple design, our approach significantly reduces the cost of diffusion model training with minimal overhead.","Our research enables more researchers to train diffusion models at a lower cost."],"url":"http://arxiv.org/abs/2405.17403v1","category":"cs.LG"}
{"created":"2024-05-27 17:51:24","title":"THREAD: Thinking Deeper with Recursive Spawning","abstract":"Large language models (LLMs) have shown impressive capabilities across diverse settings, but still struggle as the length and complexity of the context increases. To address this challenge, we propose Thinking Recursively and Dynamically (ThReaD). THREAD frames model generation as a thread of execution that, based on the context, can run to completion or dynamically spawn new threads. By spawning, threads can offload work (e.g., thinking, retrieving information) to child threads, which only return tokens needed for the parent thread to do its work. In effect, this enables the model to adapt, as needed, the amount of intermediate work used to produce tokens. We apply THREAD in the settings of LLM task solving and question answering, where the dynamic threading allows the model to recursively decompose the given task or question into progressively simpler sub-problems that can be solved by separate child threads. We test THREAD, implemented using a few-shot learning approach, on diverse benchmarks for agent tasks and data-grounded question answering. THREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these benchmarks, including ALFWorld, TextCraft, and WebShop, along with two new benchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD outperforms existing frameworks by 10% to 50% absolute points with smaller models, including Llama-3-8b and CodeLlama-7b.","sentences":["Large language models (LLMs) have shown impressive capabilities across diverse settings, but still struggle as the length and complexity of the context increases.","To address this challenge, we propose Thinking Recursively and Dynamically (ThReaD).","THREAD frames model generation as a thread of execution that, based on the context, can run to completion or dynamically spawn new threads.","By spawning, threads can offload work (e.g., thinking, retrieving information) to child threads, which only return tokens needed for the parent thread to do its work.","In effect, this enables the model to adapt, as needed, the amount of intermediate work used to produce tokens.","We apply THREAD in the settings of LLM task solving and question answering, where the dynamic threading allows the model to recursively decompose the given task or question into progressively simpler sub-problems that can be solved by separate child threads.","We test THREAD, implemented using a few-shot learning approach, on diverse benchmarks for agent tasks and data-grounded question answering.","THREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these benchmarks, including ALFWorld, TextCraft, and WebShop, along with two new benchmarks, DataCommons QA and MIMIC-III ICU QA.","In addition, THREAD outperforms existing frameworks by 10% to 50% absolute points with smaller models, including Llama-3-8b and CodeLlama-7b."],"url":"http://arxiv.org/abs/2405.17402v1","category":"cs.CL"}
{"created":"2024-05-27 17:49:25","title":"A new search pipeline for gravitational waves with higher-order modes using mode-by-mode filtering","abstract":"Nearly all template-based gravitational wave (GW) searches only include the quasi-circular quadrupolar modes of the signals in their templates. Including additional degrees of freedom in the GW templates corresponding to higher-order harmonics, orbital precession, or eccentricity is challenging because: ($i$) the size of template banks and the matched-filtering cost increases significantly with the number of degrees of freedom, $(ii)$ if these additional degrees are not included properly, the search can lose sensitivity overall (due to an increase in the rate of background triggers). Here, we focus on including aligned-spin higher harmonics in GW search templates. We use a new mode-by-mode filtering approach, where we separately filter GW strain data with three harmonics [namely $(\\ell, |m|)=(2,2)$, $(3,3)$ and $(4,4)$]. This results in an increase in the matched-filtering cost by only a factor of $3$ compared to that of a $(2,2)$-only search. We develop computationally cheap trigger-ranking statistics to optimally combine the different signal-to-noise ratios (SNR) timeseries from different harmonics, which ensure only physically-allowed combinations of the different harmonics are triggered on. We use an empirical template-dependent background model in our ranking statistic to account for non-Gaussian transients. In addition, we develop a tool called band eraser which specifically excises narrow time-varying noisy bands in time-frequency space (without having to excise entire time chunks in the data). New GW candidate events that we detect using our search pipeline and the details of our template banks are discussed in accompanying papers: Wadekar et al. [1] and [2] respectively. Apart from higher harmonics, we expect our methodology to also be useful for cheap and optimal searches including orbital precession and eccentricity in GW waveforms.","sentences":["Nearly all template-based gravitational wave (GW) searches only include the quasi-circular quadrupolar modes of the signals in their templates.","Including additional degrees of freedom in the GW templates corresponding to higher-order harmonics, orbital precession, or eccentricity is challenging because: ($i$) the size of template banks and the matched-filtering cost increases significantly with the number of degrees of freedom, $(ii)$ if these additional degrees are not included properly, the search can lose sensitivity overall (due to an increase in the rate of background triggers).","Here, we focus on including aligned-spin higher harmonics in GW search templates.","We use a new mode-by-mode filtering approach, where we separately filter GW strain data with three harmonics [namely $(\\ell, |m|)=(2,2)$, $(3,3)$ and $(4,4)$].","This results in an increase in the matched-filtering cost by only a factor of $3$ compared to that of a $(2,2)$-only search.","We develop computationally cheap trigger-ranking statistics to optimally combine the different signal-to-noise ratios (SNR) timeseries from different harmonics, which ensure only physically-allowed combinations of the different harmonics are triggered on.","We use an empirical template-dependent background model in our ranking statistic to account for non-Gaussian transients.","In addition, we develop a tool called band eraser which specifically excises narrow time-varying noisy bands in time-frequency space (without having to excise entire time chunks in the data).","New GW candidate events that we detect using our search pipeline and the details of our template banks are discussed in accompanying papers:","Wadekar et al.","[1] and [2] respectively.","Apart from higher harmonics, we expect our methodology to also be useful for cheap and optimal searches including orbital precession and eccentricity in GW waveforms."],"url":"http://arxiv.org/abs/2405.17400v1","category":"gr-qc"}
{"created":"2024-05-27 17:49:18","title":"Transformers Can Do Arithmetic with the Right Embeddings","abstract":"The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further.   With positions resolved, we can study the logical extrapolation ability of transformers. Can they solve arithmetic problems that are larger and more complex than those in their training data? We find that training on only 20 digit numbers with a single GPU for one day, we can reach state-of-the-art performance, achieving up to 99% accuracy on 100 digit addition problems. Finally, we show that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication.","sentences":["The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits.","We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number.","In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further.   ","With positions resolved, we can study the logical extrapolation ability of transformers.","Can they solve arithmetic problems that are larger and more complex than those in their training data?","We find that training on only 20 digit numbers with a single GPU for one day, we can reach state-of-the-art performance, achieving up to 99% accuracy on 100 digit addition problems.","Finally, we show that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication."],"url":"http://arxiv.org/abs/2405.17399v1","category":"cs.LG"}
{"created":"2024-05-27 17:49:15","title":"Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability","abstract":"World models can foresee the outcomes of different actions, which is of paramount importance for autonomous driving. Nevertheless, existing driving world models still have limitations in generalization to unseen environments, prediction fidelity of critical details, and action controllability for flexible application. In this paper, we present Vista, a generalizable driving world model with high fidelity and versatile controllability. Based on a systematic diagnosis of existing methods, we introduce several key ingredients to address these limitations. To accurately predict real-world dynamics at high resolution, we propose two novel losses to promote the learning of moving instances and structural information. We also devise an effective latent replacement approach to inject historical frames as priors for coherent long-horizon rollouts. For action controllability, we incorporate a versatile set of controls from high-level intentions (command, goal point) to low-level maneuvers (trajectory, angle, and speed) through an efficient learning strategy. After large-scale training, the capabilities of Vista can seamlessly generalize to different scenarios. Extensive experiments on multiple datasets show that Vista outperforms the most advanced general-purpose video generator in over 70% of comparisons and surpasses the best-performing driving world model by 55% in FID and 27% in FVD. Moreover, for the first time, we utilize the capacity of Vista itself to establish a generalizable reward for real-world action evaluation without accessing the ground truth actions.","sentences":["World models can foresee the outcomes of different actions, which is of paramount importance for autonomous driving.","Nevertheless, existing driving world models still have limitations in generalization to unseen environments, prediction fidelity of critical details, and action controllability for flexible application.","In this paper, we present Vista, a generalizable driving world model with high fidelity and versatile controllability.","Based on a systematic diagnosis of existing methods, we introduce several key ingredients to address these limitations.","To accurately predict real-world dynamics at high resolution, we propose two novel losses to promote the learning of moving instances and structural information.","We also devise an effective latent replacement approach to inject historical frames as priors for coherent long-horizon rollouts.","For action controllability, we incorporate a versatile set of controls from high-level intentions (command, goal point) to low-level maneuvers (trajectory, angle, and speed) through an efficient learning strategy.","After large-scale training, the capabilities of Vista can seamlessly generalize to different scenarios.","Extensive experiments on multiple datasets show that Vista outperforms the most advanced general-purpose video generator in over 70% of comparisons and surpasses the best-performing driving world model by 55% in FID and 27% in FVD.","Moreover, for the first time, we utilize the capacity of Vista itself to establish a generalizable reward for real-world action evaluation without accessing the ground truth actions."],"url":"http://arxiv.org/abs/2405.17398v1","category":"cs.CV"}
{"created":"2024-05-27 17:48:44","title":"Cosmological constraints on curved quintessence","abstract":"Dynamical dark energy has gained renewed interest due to recent theoretical and observational developments. In the present paper, we focus on a string-motivated dark energy set-up, and perform a detailed cosmological analysis of exponential quintessence with potential $V=V_0 e^{-\\lambda\\phi}$, allowing for non-zero spatial curvature. We first gain some physical intuition into the full evolution of such a scenario by analysing the corresponding dynamical system. Then, we test the model using a combination of Planck CMB data, DESI BAO data, as well as recent supernovae datasets. For the model parameter $\\lambda$, we obtain a preference for nonzero values: $\\lambda = 0.48^{+0.28}_{-0.21},\\; 0.68^{+0.31}_{-0.20},\\; 0.77^{+0.18}_{-0.15}$ at 68% C.L. when combining CMB+DESI with Pantheon+, Union3 and DES-Y5 supernovae datasets respectively. We find no significant hint for spatial curvature. We discuss the implications of current cosmological results for the exponential quintessence model, and more generally for dark energy in string theory.","sentences":["Dynamical dark energy has gained renewed interest due to recent theoretical and observational developments.","In the present paper, we focus on a string-motivated dark energy set-up, and perform a detailed cosmological analysis of exponential quintessence with potential $V=V_0 e^{-\\lambda\\phi}$, allowing for non-zero spatial curvature.","We first gain some physical intuition into the full evolution of such a scenario by analysing the corresponding dynamical system.","Then, we test the model using a combination of Planck CMB data, DESI BAO data, as well as recent supernovae datasets.","For the model parameter $\\lambda$, we obtain a preference for nonzero values: $\\lambda = 0.48^{+0.28}_{-0.21},\\; 0.68^{+0.31}_{-0.20},\\; 0.77^{+0.18}_{-0.15}$ at 68% C.L. when combining CMB+DESI with Pantheon+, Union3 and DES-Y5 supernovae datasets respectively.","We find no significant hint for spatial curvature.","We discuss the implications of current cosmological results for the exponential quintessence model, and more generally for dark energy in string theory."],"url":"http://arxiv.org/abs/2405.17396v1","category":"astro-ph.CO"}
{"created":"2024-05-27 17:46:22","title":"EASI-Tex: Edge-Aware Mesh Texturing from Single Image","abstract":"We present a novel approach for single-image mesh texturing, which employs a diffusion model with judicious conditioning to seamlessly transfer an object's texture from a single RGB image to a given 3D mesh object. We do not assume that the two objects belong to the same category, and even if they do, there can be significant discrepancies in their geometry and part proportions. Our method aims to rectify the discrepancies by conditioning a pre-trained Stable Diffusion generator with edges describing the mesh through ControlNet, and features extracted from the input image using IP-Adapter to generate textures that respect the underlying geometry of the mesh and the input texture without any optimization or training. We also introduce Image Inversion, a novel technique to quickly personalize the diffusion model for a single concept using a single image, for cases where the pre-trained IP-Adapter falls short in capturing all the details from the input image faithfully. Experimental results demonstrate the efficiency and effectiveness of our edge-aware single-image mesh texturing approach, coined EASI-Tex, in preserving the details of the input texture on diverse 3D objects, while respecting their geometry.","sentences":["We present a novel approach for single-image mesh texturing, which employs a diffusion model with judicious conditioning to seamlessly transfer an object's texture from a single RGB image to a given 3D mesh object.","We do not assume that the two objects belong to the same category, and even if they do, there can be significant discrepancies in their geometry and part proportions.","Our method aims to rectify the discrepancies by conditioning a pre-trained Stable Diffusion generator with edges describing the mesh through ControlNet, and features extracted from the input image using IP-Adapter to generate textures that respect the underlying geometry of the mesh and the input texture without any optimization or training.","We also introduce Image Inversion, a novel technique to quickly personalize the diffusion model for a single concept using a single image, for cases where the pre-trained IP-Adapter falls short in capturing all the details from the input image faithfully.","Experimental results demonstrate the efficiency and effectiveness of our edge-aware single-image mesh texturing approach, coined EASI-Tex, in preserving the details of the input texture on diverse 3D objects, while respecting their geometry."],"url":"http://arxiv.org/abs/2405.17393v1","category":"cs.CV"}
{"created":"2024-05-27 17:44:33","title":"Dataset-learning duality and emergent criticality","abstract":"In artificial neural networks, the activation dynamics of non-trainable variables is strongly coupled to the learning dynamics of trainable variables. During the activation pass, the boundary neurons (e.g., input neurons) are mapped to the bulk neurons (e.g., hidden neurons), and during the learning pass, both bulk and boundary neurons are mapped to changes in trainable variables (e.g., weights and biases). For example, in feed-forward neural networks, forward propagation is the activation pass and backward propagation is the learning pass. We show that a composition of the two maps establishes a duality map between a subspace of non-trainable boundary variables (e.g., dataset) and a tangent subspace of trainable variables (i.e., learning). In general, the dataset-learning duality is a complex non-linear map between high-dimensional spaces, but in a learning equilibrium, the problem can be linearized and reduced to many weakly coupled one-dimensional problems. We use the duality to study the emergence of criticality, or the power-law distributions of fluctuations of the trainable variables. In particular, we show that criticality can emerge in the learning system even from the dataset in a non-critical state, and that the power-law distribution can be modified by changing either the activation function or the loss function.","sentences":["In artificial neural networks, the activation dynamics of non-trainable variables is strongly coupled to the learning dynamics of trainable variables.","During the activation pass, the boundary neurons (e.g., input neurons) are mapped to the bulk neurons (e.g., hidden neurons), and during the learning pass, both bulk and boundary neurons are mapped to changes in trainable variables (e.g., weights and biases).","For example, in feed-forward neural networks, forward propagation is the activation pass and backward propagation is the learning pass.","We show that a composition of the two maps establishes a duality map between a subspace of non-trainable boundary variables (e.g., dataset) and a tangent subspace of trainable variables (i.e., learning).","In general, the dataset-learning duality is a complex non-linear map between high-dimensional spaces, but in a learning equilibrium, the problem can be linearized and reduced to many weakly coupled one-dimensional problems.","We use the duality to study the emergence of criticality, or the power-law distributions of fluctuations of the trainable variables.","In particular, we show that criticality can emerge in the learning system even from the dataset in a non-critical state, and that the power-law distribution can be modified by changing either the activation function or the loss function."],"url":"http://arxiv.org/abs/2405.17391v1","category":"cs.LG"}
{"created":"2024-05-27 17:42:02","title":"Non-Unitary Quantum Machine Learning","abstract":"We introduce several novel probabilistic quantum algorithms that overcome the normal unitary restrictions in quantum machine learning by leveraging the Linear Combination of Unitaries (LCU) method. Among our contributions are quantum native implementations of Residual Networks (ResNet); demonstrating a path to avoiding barren plateaus while maintaining the complexity of models that are hard to simulate classically. Furthermore, by generalising to allow control of the strength of residual connections, we show that the lower bound of the LCU success probability can be set to any arbitrary desired value. We also implement a quantum analogue of average pooling layers from convolutional networks. Our empirical analysis demonstrates that the LCU success probability remains stable for the MNIST database, unlocking a potential quadratic advantage in terms of image size compared to classical techniques. Finally, we propose a general framework for irreducible subspace projections for quantum encoded data. Using this, we demonstrate a novel rotationally invariant encoding for point cloud data via Schur-Weyl duality. We also show how this framework can be used to parameterise and control the amount of symmetry in an encoding; demonstrating improved classification performance for partially permutation invariant encoded point cloud data when compared to non-invariant or fully permutation invariant encodings. These new general algorithmic frameworks are all constructed under the same LCU method, suggesting that even more novel algorithms could be achieved by utilising the LCU technique.","sentences":["We introduce several novel probabilistic quantum algorithms that overcome the normal unitary restrictions in quantum machine learning by leveraging the Linear Combination of Unitaries (LCU) method.","Among our contributions are quantum native implementations of Residual Networks (ResNet); demonstrating a path to avoiding barren plateaus while maintaining the complexity of models that are hard to simulate classically.","Furthermore, by generalising to allow control of the strength of residual connections, we show that the lower bound of the LCU success probability can be set to any arbitrary desired value.","We also implement a quantum analogue of average pooling layers from convolutional networks.","Our empirical analysis demonstrates that the LCU success probability remains stable for the MNIST database, unlocking a potential quadratic advantage in terms of image size compared to classical techniques.","Finally, we propose a general framework for irreducible subspace projections for quantum encoded data.","Using this, we demonstrate a novel rotationally invariant encoding for point cloud data via Schur-Weyl duality.","We also show how this framework can be used to parameterise and control the amount of symmetry in an encoding; demonstrating improved classification performance for partially permutation invariant encoded point cloud data when compared to non-invariant or fully permutation invariant encodings.","These new general algorithmic frameworks are all constructed under the same LCU method, suggesting that even more novel algorithms could be achieved by utilising the LCU technique."],"url":"http://arxiv.org/abs/2405.17388v1","category":"quant-ph"}
{"created":"2024-05-27 17:41:54","title":"MindMerger: Efficient Boosting LLM Reasoning in non-English Languages","abstract":"Reasoning capabilities are crucial for Large Language Models (LLMs), yet a notable gap exists between English and non-English languages. To bridge this disparity, some works fine-tune LLMs to relearn reasoning capabilities in non-English languages, while others replace non-English inputs with an external model's outputs such as English translation text to circumvent the challenge of LLM understanding non-English. Unfortunately, these methods often underutilize the built-in skilled reasoning and useful language understanding capabilities of LLMs. In order to better utilize the minds of reasoning and language understanding in LLMs, we propose a new method, namely MindMerger, which merges LLMs with the external language understanding capabilities from multilingual models to boost the multilingual reasoning performance. Furthermore, a two-step training scheme is introduced to first train to embeded the external capabilities into LLMs and then train the collaborative utilization of the external capabilities and the built-in capabilities in LLMs. Experiments on three multilingual reasoning datasets and a language understanding dataset demonstrate that MindMerger consistently outperforms all baselines, especially in low-resource languages. Without updating the parameters of LLMs, the average accuracy improved by 6.7% and 8.0% across all languages and low-resource languages on the MGSM dataset, respectively.","sentences":["Reasoning capabilities are crucial for Large Language Models (LLMs), yet a notable gap exists between English and non-English languages.","To bridge this disparity, some works fine-tune LLMs to relearn reasoning capabilities in non-English languages, while others replace non-English inputs with an external model's outputs such as English translation text to circumvent the challenge of LLM understanding non-English.","Unfortunately, these methods often underutilize the built-in skilled reasoning and useful language understanding capabilities of LLMs.","In order to better utilize the minds of reasoning and language understanding in LLMs, we propose a new method, namely MindMerger, which merges LLMs with the external language understanding capabilities from multilingual models to boost the multilingual reasoning performance.","Furthermore, a two-step training scheme is introduced to first train to embeded the external capabilities into LLMs and then train the collaborative utilization of the external capabilities and the built-in capabilities in LLMs.","Experiments on three multilingual reasoning datasets and a language understanding dataset demonstrate that MindMerger consistently outperforms all baselines, especially in low-resource languages.","Without updating the parameters of LLMs, the average accuracy improved by 6.7% and 8.0% across all languages and low-resource languages on the MGSM dataset, respectively."],"url":"http://arxiv.org/abs/2405.17386v1","category":"cs.CL"}
{"created":"2024-05-27 17:38:33","title":"ReMoDetect: Reward Models Recognize Aligned LLM's Generations","abstract":"The remarkable capabilities and easy accessibility of large language models (LLMs) have significantly increased societal risks (e.g., fake news generation), necessitating the development of LLM-generated text (LGT) detection methods for safe usage. However, detecting LGTs is challenging due to the vast number of LLMs, making it impractical to account for each LLM individually; hence, it is crucial to identify the common characteristics shared by these models. In this paper, we draw attention to a common feature of recent powerful LLMs, namely the alignment training, i.e., training LLMs to generate human-preferable texts. Our key finding is that as these aligned LLMs are trained to maximize the human preferences, they generate texts with higher estimated preferences even than human-written texts; thus, such texts are easily detected by using the reward model (i.e., an LLM trained to model human preference distribution). Based on this finding, we propose two training schemes to further improve the detection ability of the reward model, namely (i) continual preference fine-tuning to make the reward model prefer aligned LGTs even further and (ii) reward modeling of Human/LLM mixed texts (a rephrased texts from human-written texts using aligned LLMs), which serves as a median preference text corpus between LGTs and human-written texts to learn the decision boundary better. We provide an extensive evaluation by considering six text domains across twelve aligned LLMs, where our method demonstrates state-of-the-art results. Code is available at https://github.com/hyunseoklee-ai/reward_llm_detect.","sentences":["The remarkable capabilities and easy accessibility of large language models (LLMs) have significantly increased societal risks (e.g., fake news generation), necessitating the development of LLM-generated text (LGT) detection methods for safe usage.","However, detecting LGTs is challenging due to the vast number of LLMs, making it impractical to account for each LLM individually; hence, it is crucial to identify the common characteristics shared by these models.","In this paper, we draw attention to a common feature of recent powerful LLMs, namely the alignment training, i.e., training LLMs to generate human-preferable texts.","Our key finding is that as these aligned LLMs are trained to maximize the human preferences, they generate texts with higher estimated preferences even than human-written texts; thus, such texts are easily detected by using the reward model (i.e., an LLM trained to model human preference distribution).","Based on this finding, we propose two training schemes to further improve the detection ability of the reward model, namely (i) continual preference fine-tuning to make the reward model prefer aligned LGTs even further and (ii) reward modeling of Human/LLM mixed texts (a rephrased texts from human-written texts using aligned LLMs), which serves as a median preference text corpus between LGTs and human-written texts to learn the decision boundary better.","We provide an extensive evaluation by considering six text domains across twelve aligned LLMs, where our method demonstrates state-of-the-art results.","Code is available at https://github.com/hyunseoklee-ai/reward_llm_detect."],"url":"http://arxiv.org/abs/2405.17382v1","category":"cs.LG"}
{"created":"2024-05-27 17:36:56","title":"Rokhlin Dimension and Inductive Limit Actions on AF-algebras","abstract":"Given a separable, AF-algebra A and an inductive limit action on A of a finitely generated abelian group with finite Rokhlin dimension with commuting towers, we give a local description of the associated crossed product C*-algebra. In particular, when A is unital and $\\alpha \\in Aut(A)$ is approximately inner and has the Rokhlin property, we conclude that $A \\rtimes_{\\alpha} \\mathbb{Z}$ is an A$\\mathbb{T}$-algebra.","sentences":["Given a separable, AF-algebra A and an inductive limit action on A of a finitely generated abelian group with finite Rokhlin dimension with commuting towers, we give a local description of the associated crossed product C*-algebra.","In particular, when A is unital and $\\alpha \\in Aut(A)$ is approximately inner and has the Rokhlin property, we conclude that $A \\rtimes_{\\alpha} \\mathbb{Z}$ is an A$\\mathbb{T}$-algebra."],"url":"http://arxiv.org/abs/2405.17380v1","category":"math.OA"}
{"created":"2024-05-27 17:36:01","title":"RTL-Repo: A Benchmark for Evaluating LLMs on Large-Scale RTL Design Projects","abstract":"Large Language Models (LLMs) have demonstrated potential in assisting with Register Transfer Level (RTL) design tasks. Nevertheless, there remains to be a significant gap in benchmarks that accurately reflect the complexity of real-world RTL projects. To address this, this paper presents RTL-Repo, a benchmark specifically designed to evaluate LLMs on large-scale RTL design projects. RTL-Repo includes a comprehensive dataset of more than 4000 Verilog code samples extracted from public GitHub repositories, with each sample providing the full context of the corresponding repository. We evaluate several state-of-the-art models on the RTL-Repo benchmark, including GPT-4, GPT-3.5, Starcoder2, alongside Verilog-specific models like VeriGen and RTLCoder, and compare their performance in generating Verilog code for complex projects. The RTL-Repo benchmark provides a valuable resource for the hardware design community to assess and compare LLMs' performance in real-world RTL design scenarios and train LLMs specifically for Verilog code generation in complex, multi-file RTL projects. RTL-Repo is open-source and publicly available on Github.","sentences":["Large Language Models (LLMs) have demonstrated potential in assisting with Register Transfer Level (RTL) design tasks.","Nevertheless, there remains to be a significant gap in benchmarks that accurately reflect the complexity of real-world RTL projects.","To address this, this paper presents RTL-Repo, a benchmark specifically designed to evaluate LLMs on large-scale RTL design projects.","RTL-Repo includes a comprehensive dataset of more than 4000 Verilog code samples extracted from public GitHub repositories, with each sample providing the full context of the corresponding repository.","We evaluate several state-of-the-art models on the RTL-Repo benchmark, including GPT-4, GPT-3.5, Starcoder2, alongside Verilog-specific models like VeriGen and RTLCoder, and compare their performance in generating Verilog code for complex projects.","The RTL-Repo benchmark provides a valuable resource for the hardware design community to assess and compare LLMs' performance in real-world RTL design scenarios and train LLMs specifically for Verilog code generation in complex, multi-file RTL projects.","RTL-Repo is open-source and publicly available on Github."],"url":"http://arxiv.org/abs/2405.17378v1","category":"cs.LG"}
{"created":"2024-05-27 17:32:37","title":"Federating Dynamic Models using Early-Exit Architectures for Automatic Speech Recognition on Heterogeneous Clients","abstract":"Automatic speech recognition models require large amounts of speech recordings for training. However, the collection of such data often is cumbersome and leads to privacy concerns. Federated learning has been widely used as an effective decentralized technique that collaboratively learns a shared prediction model while keeping the data local on different clients. Unfortunately, client devices often feature limited computation and communication resources leading to practical difficulties for large models. In addition, the heterogeneity that characterizes edge devices makes it sub-optimal to generate a single model that fits all of them. Differently from the recent literature, where multiple models with different architectures are used, in this work, we propose using dynamical architectures which, employing early-exit solutions, can adapt their processing (i.e. traversed layers) depending on the input and on the operation conditions. This solution falls in the realm of partial training methods and brings two benefits: a single model is used on a variety of devices; federating the models after local training is straightforward. Experiments on public datasets show that our proposed approach is effective and can be combined with basic federated learning strategies.","sentences":["Automatic speech recognition models require large amounts of speech recordings for training.","However, the collection of such data often is cumbersome and leads to privacy concerns.","Federated learning has been widely used as an effective decentralized technique that collaboratively learns a shared prediction model while keeping the data local on different clients.","Unfortunately, client devices often feature limited computation and communication resources leading to practical difficulties for large models.","In addition, the heterogeneity that characterizes edge devices makes it sub-optimal to generate a single model that fits all of them.","Differently from the recent literature, where multiple models with different architectures are used, in this work, we propose using dynamical architectures which, employing early-exit solutions, can adapt their processing (i.e. traversed layers) depending on the input and on the operation conditions.","This solution falls in the realm of partial training methods and brings two benefits: a single model is used on a variety of devices; federating the models after local training is straightforward.","Experiments on public datasets show that our proposed approach is effective and can be combined with basic federated learning strategies."],"url":"http://arxiv.org/abs/2405.17376v1","category":"cs.CL"}
{"created":"2024-05-27 17:28:25","title":"BehaviorGPT: Smart Agent Simulation for Autonomous Driving with Next-Patch Prediction","abstract":"Simulating realistic interactions among traffic agents is crucial for efficiently validating the safety of autonomous driving systems. Existing leading simulators primarily use an encoder-decoder structure to encode the historical trajectories for future simulation. However, such a paradigm complicates the model architecture, and the manual separation of history and future trajectories leads to low data utilization. To address these challenges, we propose Behavior Generative Pre-trained Transformers (BehaviorGPT), a decoder-only, autoregressive architecture designed to simulate the sequential motion of multiple agents. Crucially, our approach discards the traditional separation between \"history\" and \"future,\" treating each time step as the \"current\" one, resulting in a simpler, more parameter- and data-efficient design that scales seamlessly with data and computation. Additionally, we introduce the Next-Patch Prediction Paradigm (NP3), which enables models to reason at the patch level of trajectories and capture long-range spatial-temporal interactions. BehaviorGPT ranks first across several metrics on the Waymo Sim Agents Benchmark, demonstrating its exceptional performance in multi-agent and agent-map interactions. We outperformed state-of-the-art models with a realism score of 0.741 and improved the minADE metric to 1.540, with an approximately 91.6% reduction in model parameters.","sentences":["Simulating realistic interactions among traffic agents is crucial for efficiently validating the safety of autonomous driving systems.","Existing leading simulators primarily use an encoder-decoder structure to encode the historical trajectories for future simulation.","However, such a paradigm complicates the model architecture, and the manual separation of history and future trajectories leads to low data utilization.","To address these challenges, we propose Behavior Generative Pre-trained Transformers (BehaviorGPT), a decoder-only, autoregressive architecture designed to simulate the sequential motion of multiple agents.","Crucially, our approach discards the traditional separation between \"history\" and \"future,\" treating each time step as the \"current\" one, resulting in a simpler, more parameter- and data-efficient design that scales seamlessly with data and computation.","Additionally, we introduce the Next-Patch Prediction Paradigm (NP3), which enables models to reason at the patch level of trajectories and capture long-range spatial-temporal interactions.","BehaviorGPT ranks first across several metrics on the Waymo Sim Agents Benchmark, demonstrating its exceptional performance in multi-agent and agent-map interactions.","We outperformed state-of-the-art models with a realism score of 0.741 and improved the minADE metric to 1.540, with an approximately 91.6% reduction in model parameters."],"url":"http://arxiv.org/abs/2405.17372v1","category":"cs.AI"}
{"created":"2024-05-27 17:24:11","title":"Predict joint angle of body parts based on sequence pattern recognition","abstract":"The way organs are positioned and moved in the workplace can cause pain and physical harm. Therefore, ergonomists use ergonomic risk assessments based on visual observation of the workplace, or review pictures and videos taken in the workplace. Sometimes the workers in the photos are not in perfect condition. Some parts of the workers' bodies may not be in the camera's field of view, could be obscured by objects, or by self-occlusion, this is the main problem in 2D human posture recognition. It is difficult to predict the position of body parts when they are not visible in the image, and geometric mathematical methods are not entirely suitable for this purpose. Therefore, we created a dataset with artificial images of a 3D human model, specifically for painful postures, and real human photos from different viewpoints. Each image we captured was based on a predefined joint angle for each 3D model or human model. We created various images, including images where some body parts are not visible. Nevertheless, the joint angle is estimated beforehand, so we could study the case by converting the input images into the sequence of joint connections between predefined body parts and extracting the desired joint angle with a convolutional neural network. In the end, we obtained root mean square error (RMSE) of 12.89 and mean absolute error (MAE) of 4.7 on the test dataset.","sentences":["The way organs are positioned and moved in the workplace can cause pain and physical harm.","Therefore, ergonomists use ergonomic risk assessments based on visual observation of the workplace, or review pictures and videos taken in the workplace.","Sometimes the workers in the photos are not in perfect condition.","Some parts of the workers' bodies may not be in the camera's field of view, could be obscured by objects, or by self-occlusion, this is the main problem in 2D human posture recognition.","It is difficult to predict the position of body parts when they are not visible in the image, and geometric mathematical methods are not entirely suitable for this purpose.","Therefore, we created a dataset with artificial images of a 3D human model, specifically for painful postures, and real human photos from different viewpoints.","Each image we captured was based on a predefined joint angle for each 3D model or human model.","We created various images, including images where some body parts are not visible.","Nevertheless, the joint angle is estimated beforehand, so we could study the case by converting the input images into the sequence of joint connections between predefined body parts and extracting the desired joint angle with a convolutional neural network.","In the end, we obtained root mean square error (RMSE) of 12.89 and mean absolute error (MAE) of 4.7 on the test dataset."],"url":"http://arxiv.org/abs/2405.17369v1","category":"cs.CV"}
{"created":"2024-05-27 17:19:02","title":"EM-GANSim: Real-time and Accurate EM Simulation Using Conditional GANs for 3D Indoor Scenes","abstract":"We present a novel machine-learning (ML) approach (EM-GANSim) for real-time electromagnetic (EM) propagation that is used for wireless communication simulation in 3D indoor environments. Our approach uses a modified conditional Generative Adversarial Network (GAN) that incorporates encoded geometry and transmitter location while adhering to the electromagnetic propagation theory. The overall physically-inspired learning is able to predict the power distribution in 3D scenes, which is represented using heatmaps. Our overall accuracy is comparable to ray tracing-based EM simulation, as evidenced by lower mean squared error values. Furthermore, our GAN-based method drastically reduces the computation time, achieving a 5X speedup on complex benchmarks. In practice, it can compute the signal strength in a few milliseconds on any location in 3D indoor environments. We also present a large dataset of 3D models and EM ray tracing-simulated heatmaps. To the best of our knowledge, EM-GANSim is the first real-time algorithm for EM simulation in complex 3D indoor environments. We plan to release the code and the dataset.","sentences":["We present a novel machine-learning (ML) approach (EM-GANSim) for real-time electromagnetic (EM) propagation that is used for wireless communication simulation in 3D indoor environments.","Our approach uses a modified conditional Generative Adversarial Network (GAN) that incorporates encoded geometry and transmitter location while adhering to the electromagnetic propagation theory.","The overall physically-inspired learning is able to predict the power distribution in 3D scenes, which is represented using heatmaps.","Our overall accuracy is comparable to ray tracing-based EM simulation, as evidenced by lower mean squared error values.","Furthermore, our GAN-based method drastically reduces the computation time, achieving a 5X speedup on complex benchmarks.","In practice, it can compute the signal strength in a few milliseconds on any location in 3D indoor environments.","We also present a large dataset of 3D models and EM ray tracing-simulated heatmaps.","To the best of our knowledge, EM-GANSim is the first real-time algorithm for EM simulation in complex 3D indoor environments.","We plan to release the code and the dataset."],"url":"http://arxiv.org/abs/2405.17366v1","category":"cs.LG"}
{"created":"2024-05-27 17:14:45","title":"Speech Loudness in Broadcasting and Streaming","abstract":"The introduction and regulation of loudness in broadcasting and streaming brought clear benefits to the audience, e.g., a level of uniformity across programs and channels. Yet, speech loudness is frequently reported as being too low in certain passages, which can hinder the full understanding and enjoyment of movies and TV programs. This paper proposes expanding the set of loudness-based measures typically used in the industry. We focus on speech loudness, and we show that, when clean speech is not available, Deep Neural Networks (DNNs) can be used to isolate the speech signal and so to accurately estimate speech loudness, providing a more precise estimate compared to speech-gated loudness. Moreover, we define critical passages, i.e., passages in which speech is likely to be hard to understand. Critical passages are defined based on the local Speech Loudness Deviation (SLD) and the local Speech-to-Background Loudness Difference (SBLD), as SLD and SBLD significantly contribute to intelligibility and listening effort. In contrast to other more comprehensive measures of intelligibility and listening effort, SLD and SBLD can be straightforwardly measured, are intuitive, and, most importantly, can be easily controlled by adjusting the speech level in the mix or by enabling personalization at the user's end. Finally, examples are provided that show how the detection of critical passages can support the evaluation and control of the speech signal during and after content production.","sentences":["The introduction and regulation of loudness in broadcasting and streaming brought clear benefits to the audience, e.g., a level of uniformity across programs and channels.","Yet, speech loudness is frequently reported as being too low in certain passages, which can hinder the full understanding and enjoyment of movies and TV programs.","This paper proposes expanding the set of loudness-based measures typically used in the industry.","We focus on speech loudness, and we show that, when clean speech is not available, Deep Neural Networks (DNNs) can be used to isolate the speech signal and so to accurately estimate speech loudness, providing a more precise estimate compared to speech-gated loudness.","Moreover, we define critical passages, i.e., passages in which speech is likely to be hard to understand.","Critical passages are defined based on the local Speech Loudness Deviation (SLD) and the local Speech-to-Background Loudness Difference (SBLD), as SLD and SBLD significantly contribute to intelligibility and listening effort.","In contrast to other more comprehensive measures of intelligibility and listening effort, SLD and SBLD can be straightforwardly measured, are intuitive, and, most importantly, can be easily controlled by adjusting the speech level in the mix or by enabling personalization at the user's end.","Finally, examples are provided that show how the detection of critical passages can support the evaluation and control of the speech signal during and after content production."],"url":"http://arxiv.org/abs/2405.17364v1","category":"eess.AS"}
{"created":"2024-05-27 17:02:35","title":"Rethinking Transformers in Solving POMDPs","abstract":"Sequential decision-making algorithms such as reinforcement learning (RL) in real-world scenarios inevitably face environments with partial observability. This paper scrutinizes the effectiveness of a popular architecture, namely Transformers, in Partially Observable Markov Decision Processes (POMDPs) and reveals its theoretical limitations. We establish that regular languages, which Transformers struggle to model, are reducible to POMDPs. This poses a significant challenge for Transformers in learning POMDP-specific inductive biases, due to their lack of inherent recurrence found in other models like RNNs. This paper casts doubt on the prevalent belief in Transformers as sequence models for RL and proposes to introduce a point-wise recurrent structure. The Deep Linear Recurrent Unit (LRU) emerges as a well-suited alternative for Partially Observable RL, with empirical results highlighting the sub-optimal performance of the Transformer and considerable strength of LRU.","sentences":["Sequential decision-making algorithms such as reinforcement learning (RL) in real-world scenarios inevitably face environments with partial observability.","This paper scrutinizes the effectiveness of a popular architecture, namely Transformers, in Partially Observable Markov Decision Processes (POMDPs) and reveals its theoretical limitations.","We establish that regular languages, which Transformers struggle to model, are reducible to POMDPs.","This poses a significant challenge for Transformers in learning POMDP-specific inductive biases, due to their lack of inherent recurrence found in other models like RNNs.","This paper casts doubt on the prevalent belief in Transformers as sequence models for RL and proposes to introduce a point-wise recurrent structure.","The Deep Linear Recurrent Unit (LRU) emerges as a well-suited alternative for Partially Observable RL, with empirical results highlighting the sub-optimal performance of the Transformer and considerable strength of LRU."],"url":"http://arxiv.org/abs/2405.17358v1","category":"cs.LG"}
{"created":"2024-05-27 16:54:49","title":"DOF-GS: Adjustable Depth-of-Field 3D Gaussian Splatting for Refocusing,Defocus Rendering and Blur Removal","abstract":"3D Gaussian Splatting-based techniques have recently advanced 3D scene reconstruction and novel view synthesis, achieving high-quality real-time rendering. However, these approaches are inherently limited by the underlying pinhole camera assumption in modeling the images and hence only work for All-in-Focus (AiF) sharp image inputs. This severely affects their applicability in real-world scenarios where images often exhibit defocus blur due to the limited depth-of-field (DOF) of imaging devices. Additionally, existing 3D Gaussian Splatting (3DGS) methods also do not support rendering of DOF effects.   To address these challenges, we introduce DOF-GS that allows for rendering adjustable DOF effects, removing defocus blur as well as refocusing of 3D scenes, all from multi-view images degraded by defocus blur. To this end, we re-imagine the traditional Gaussian Splatting pipeline by employing a finite aperture camera model coupled with explicit, differentiable defocus rendering guided by the Circle-of-Confusion (CoC). The proposed framework provides for dynamic adjustment of DOF effects by changing the aperture and focal distance of the underlying camera model on-demand. It also enables rendering varying DOF effects of 3D scenes post-optimization, and generating AiF images from defocused training images. Furthermore, we devise a joint optimization strategy to further enhance details in the reconstructed scenes by jointly optimizing rendered defocused and AiF images. Our experimental results indicate that DOF-GS produces high-quality sharp all-in-focus renderings conditioned on inputs compromised by defocus blur, with the training process incurring only a modest increase in GPU memory consumption. We further demonstrate the applications of the proposed method for adjustable defocus rendering and refocusing of the 3D scene from input images degraded by defocus blur.","sentences":["3D Gaussian Splatting-based techniques have recently advanced 3D scene reconstruction and novel view synthesis, achieving high-quality real-time rendering.","However, these approaches are inherently limited by the underlying pinhole camera assumption in modeling the images and hence only work for All-in-Focus (AiF) sharp image inputs.","This severely affects their applicability in real-world scenarios where images often exhibit defocus blur due to the limited depth-of-field (DOF) of imaging devices.","Additionally, existing 3D Gaussian Splatting (3DGS) methods also do not support rendering of DOF effects.   ","To address these challenges, we introduce DOF-GS that allows for rendering adjustable DOF effects, removing defocus blur as well as refocusing of 3D scenes, all from multi-view images degraded by defocus blur.","To this end, we re-imagine the traditional Gaussian Splatting pipeline by employing a finite aperture camera model coupled with explicit, differentiable defocus rendering guided by the Circle-of-Confusion (CoC).","The proposed framework provides for dynamic adjustment of DOF effects by changing the aperture and focal distance of the underlying camera model on-demand.","It also enables rendering varying DOF effects of 3D scenes post-optimization, and generating AiF images from defocused training images.","Furthermore, we devise a joint optimization strategy to further enhance details in the reconstructed scenes by jointly optimizing rendered defocused and AiF images.","Our experimental results indicate that DOF-GS produces high-quality sharp all-in-focus renderings conditioned on inputs compromised by defocus blur, with the training process incurring only a modest increase in GPU memory consumption.","We further demonstrate the applications of the proposed method for adjustable defocus rendering and refocusing of the 3D scene from input images degraded by defocus blur."],"url":"http://arxiv.org/abs/2405.17351v1","category":"cs.CV"}
{"created":"2024-05-27 16:54:43","title":"Non commutative classical and Quantum fractionary Cosmology: Anisotropic Bianchi Type I case","abstract":"In this work, we will explore the effects of non-commutativity in fractional classical and quantum schemes using the anisotropicc Bianchi Type I cosmological model coupled to a scalar field in the K-essence formalism. We introduce non-commutative variables considering that all minisuperspace variables $q^i_{nc}$ do not commute, so the symplectic structure was modified, resulting in some changes with respect to the traditional formalism. In the quantum regime, the probability density presents a new structure in the scalar field corresponding to the value of the non-commutative parameter.","sentences":["In this work, we will explore the effects of non-commutativity in fractional classical and quantum schemes using the anisotropicc Bianchi Type I cosmological model coupled to a scalar field in the K-essence formalism.","We introduce non-commutative variables considering that all minisuperspace variables $q^i_{nc}$ do not commute, so the symplectic structure was modified, resulting in some changes with respect to the traditional formalism.","In the quantum regime, the probability density presents a new structure in the scalar field corresponding to the value of the non-commutative parameter."],"url":"http://arxiv.org/abs/2405.17350v1","category":"gr-qc"}
{"created":"2024-05-27 16:54:15","title":"Metric structural human connectomes: localization and multifractality of eigenmodes","abstract":"In this study, we explore the fundamental principles behind the architecture of the human brain's structural connectome, from the perspective of spectral analysis of Laplacian and adjacency matrices. Building on the idea that the brain strikes a balance between efficient information processing and minimizing wiring costs, we aim to understand the impact of the metric properties of the connectome and how they relate to the existence of an inherent scale. We demonstrate that a simple generative model, combining nonlinear preferential attachment with an exponential penalty for spatial distance between nodes, can effectively reproduce several key characteristics of the human connectome, including spectral density, edge length distribution, eigenmode localization and local clustering properties. We also delve into the finer spectral properties of the human structural connectomes by evaluating the inverse participation ratios ($\\text{IPR}_q$) across various parts of the spectrum. Our analysis reveals that the level statistics in the soft cluster region of the Laplacian spectrum deviate from a purely Poisson distribution due to interactions between clusters. Additionally, we identified scar-like localized modes with large IPR values in the continuum spectrum. We identify multiple fractal eigenmodes distributed across different parts of the spectrum, evaluate their fractal dimensions and find a power-law relationship in the return probability, which is a hallmark of critical behavior. We discuss the conjectures that a brain operates in the Griffiths or multifractal phases.","sentences":["In this study, we explore the fundamental principles behind the architecture of the human brain's structural connectome, from the perspective of spectral analysis of Laplacian and adjacency matrices.","Building on the idea that the brain strikes a balance between efficient information processing and minimizing wiring costs, we aim to understand the impact of the metric properties of the connectome and how they relate to the existence of an inherent scale.","We demonstrate that a simple generative model, combining nonlinear preferential attachment with an exponential penalty for spatial distance between nodes, can effectively reproduce several key characteristics of the human connectome, including spectral density, edge length distribution, eigenmode localization and local clustering properties.","We also delve into the finer spectral properties of the human structural connectomes by evaluating the inverse participation ratios ($\\text{IPR}_q$) across various parts of the spectrum.","Our analysis reveals that the level statistics in the soft cluster region of the Laplacian spectrum deviate from a purely Poisson distribution due to interactions between clusters.","Additionally, we identified scar-like localized modes with large IPR values in the continuum spectrum.","We identify multiple fractal eigenmodes distributed across different parts of the spectrum, evaluate their fractal dimensions and find a power-law relationship in the return probability, which is a hallmark of critical behavior.","We discuss the conjectures that a brain operates in the Griffiths or multifractal phases."],"url":"http://arxiv.org/abs/2405.17349v1","category":"q-bio.NC"}
{"created":"2024-05-27 16:49:29","title":"Prompt Optimization with Human Feedback","abstract":"Large language models (LLMs) have demonstrated remarkable performances in various tasks. However, the performance of LLMs heavily depends on the input prompt, which has given rise to a number of recent works on prompt optimization. However, previous works often require the availability of a numeric score to assess the quality of every prompt. Unfortunately, when a human user interacts with a black-box LLM, attaining such a score is often infeasible and unreliable. Instead, it is usually significantly easier and more reliable to obtain preference feedback from a human user, i.e., showing the user the responses generated from a pair of prompts and asking the user which one is preferred. Therefore, in this paper, we study the problem of prompt optimization with human feedback (POHF), in which we aim to optimize the prompt for a black-box LLM using only human preference feedback. Drawing inspiration from dueling bandits, we design a theoretically principled strategy to select a pair of prompts to query for preference feedback in every iteration, and hence introduce our algorithm named automated POHF (APOHF). We apply our APOHF algorithm to various tasks, including optimizing user instructions, prompt optimization for text-to-image generative models, and response optimization with human feedback (i.e., further refining the response using a variant of our APOHF). The results demonstrate that our APOHF can efficiently find a good prompt using a small number of preference feedback instances. Our code can be found at \\url{https://github.com/xqlin98/APOHF}.","sentences":["Large language models (LLMs) have demonstrated remarkable performances in various tasks.","However, the performance of LLMs heavily depends on the input prompt, which has given rise to a number of recent works on prompt optimization.","However, previous works often require the availability of a numeric score to assess the quality of every prompt.","Unfortunately, when a human user interacts with a black-box LLM, attaining such a score is often infeasible and unreliable.","Instead, it is usually significantly easier and more reliable to obtain preference feedback from a human user, i.e., showing the user the responses generated from a pair of prompts and asking the user which one is preferred.","Therefore, in this paper, we study the problem of prompt optimization with human feedback (POHF), in which we aim to optimize the prompt for a black-box LLM using only human preference feedback.","Drawing inspiration from dueling bandits, we design a theoretically principled strategy to select a pair of prompts to query for preference feedback in every iteration, and hence introduce our algorithm named automated POHF (APOHF).","We apply our APOHF algorithm to various tasks, including optimizing user instructions, prompt optimization for text-to-image generative models, and response optimization with human feedback (i.e., further refining the response using a variant of our APOHF).","The results demonstrate that our APOHF can efficiently find a good prompt using a small number of preference feedback instances.","Our code can be found at \\url{https://github.com/xqlin98/APOHF}."],"url":"http://arxiv.org/abs/2405.17346v1","category":"cs.LG"}
{"created":"2024-05-27 16:49:22","title":"Exploring and steering the moral compass of Large Language Models","abstract":"Large Language Models (LLMs) have become central to advancing automation and decision-making across various sectors, raising significant ethical questions. This study proposes a comprehensive comparative analysis of the most advanced LLMs to assess their moral profiles. We subjected several state-of-the-art models to a selection of ethical dilemmas and found that all the proprietary ones are mostly utilitarian and all of the open-weights ones align mostly with values-based ethics. Furthermore, when using the Moral Foundations Questionnaire, all models we probed - except for Llama 2- displayed a strong liberal bias. Lastly, in order to causally intervene in one of the studied models, we propose a novel similarity-specific activation steering technique. Using this method, we were able to reliably steer the model's moral compass to different ethical schools. All of these results showcase that there is an ethical dimension in already deployed LLMs, an aspect that is generally overlooked.","sentences":["Large Language Models (LLMs) have become central to advancing automation and decision-making across various sectors, raising significant ethical questions.","This study proposes a comprehensive comparative analysis of the most advanced LLMs to assess their moral profiles.","We subjected several state-of-the-art models to a selection of ethical dilemmas and found that all the proprietary ones are mostly utilitarian and all of the open-weights ones align mostly with values-based ethics.","Furthermore, when using the Moral Foundations Questionnaire, all models we probed - except for Llama 2- displayed a strong liberal bias.","Lastly, in order to causally intervene in one of the studied models, we propose a novel similarity-specific activation steering technique.","Using this method, we were able to reliably steer the model's moral compass to different ethical schools.","All of these results showcase that there is an ethical dimension in already deployed LLMs, an aspect that is generally overlooked."],"url":"http://arxiv.org/abs/2405.17345v1","category":"cs.AI"}
{"created":"2024-05-27 16:48:56","title":"Bounded geometry for PCF-special subvarieties","abstract":"For each integer $d\\geq 2$, let $M_d$ denote the moduli space of maps $f: \\mathbb{P}^1\\to \\mathbb{P}^1$ of degree $d$. We study the geometric configurations of subsets of postcritically finite (or PCF) maps in $M_d$. A complex-algebraic subvariety $Y \\subset M_d$ is said to be PCF-special if it contains a Zariski-dense set of PCF maps. Here we prove that there are only finitely many positive-dimensional irreducible PCF-special subvarieties in $M_d$ with degree $\\leq D$. In addition, there exist constants $N = N(D,d)$ and $B = B(D,d)$ so that for any complex algebraic subvariety $X \\subset M_d$ of degree $\\leq D$, the Zariski closure $\\overline{X\\cap\\mathrm{PCF}}~$ has at most $N$ irreducible components, each with degree $\\leq B$. We also prove generalizations of these results for points with small critical height in $M_d(\\bar{\\mathbb{Q}})$.","sentences":["For each integer $d\\geq 2$, let $M_d$ denote the moduli space of maps $f: \\mathbb{P}^1\\to \\mathbb{P}^1$ of degree $d$.","We study the geometric configurations of subsets of postcritically finite (or PCF) maps in $M_d$. A complex-algebraic subvariety $Y \\subset M_d$ is said to be PCF-special if it contains a Zariski-dense set of PCF maps.","Here we prove that there are only finitely many positive-dimensional irreducible PCF-special subvarieties in $M_d$ with degree $\\leq D$.","In addition, there exist constants $N = N(D,d)$ and $B = B(D,d)$ so that for any complex algebraic subvariety $X \\subset M_d$ of degree $\\leq D$, the Zariski closure $\\overline{X\\cap\\mathrm{PCF}}~$ has at most $N$ irreducible components, each with degree $\\leq B$.","We also prove generalizations of these results for points with small critical height in $M_d(\\bar{\\mathbb{Q}})$."],"url":"http://arxiv.org/abs/2405.17343v1","category":"math.DS"}
{"created":"2024-05-27 16:48:42","title":"Measuring Exploration: Review and Systematic Evaluation of Modelling to Generate Alternatives Methods in Macro-Energy Systems Planning Models","abstract":"As decarbonization agendas mature, macro-energy systems modelling studies have increasingly focused on enhanced decision support methods that move beyond least-cost modelling to improve consideration of additional objectives and tradeoffs. One candidate is Modeling to Generate Alternatives (MGA), which systematically explores new objectives without explicit stakeholder elicitation. Previous literature lacks both a comprehensive review of MGA vector selection methods in large-scale energy system models and comparative testing of their relative efficacies in this setting. To fill this gap, this paper provides a comprehensive review of the MGA literature, identifying at least seven MGA vector selection methodologies and carrying out a systematic evaluation of four: Hop-Skip-Jump, Random Vector, Variable Min/Max, and Modelling All Alternatives. We examine each method's runtime, parallelizability, new solution discovery efficiency, and spatial exploration in lower dimensional (N <= 100) spaces, as well as spatial exploration in a three-zone, 8760-hour capacity expansion model case. Through these tests, we find Random Vector provides the broadest exploration of the near-optimal feasible region and Variable Min/Max provides the most extreme results, while the two tie on computational speed. We thus propose a new Hybrid vector selection approach combining the two methods to take advantage of the strengths of each. Additional analysis is provided on MGA variable selection, in which we demonstrate MGA problems formulated over generation variables fail to retain cost-optimal dispatch and are thus not reflective of real operations of equivalent hypothetical capacity choices. As such, we recommend future studies utilize a parallelized combined vector approach over the set of capacity variables for best results in computational speed and spatial exploration while retaining optimal dispatch.","sentences":["As decarbonization agendas mature, macro-energy systems modelling studies have increasingly focused on enhanced decision support methods that move beyond least-cost modelling to improve consideration of additional objectives and tradeoffs.","One candidate is Modeling to Generate Alternatives (MGA), which systematically explores new objectives without explicit stakeholder elicitation.","Previous literature lacks both a comprehensive review of MGA vector selection methods in large-scale energy system models and comparative testing of their relative efficacies in this setting.","To fill this gap, this paper provides a comprehensive review of the MGA literature, identifying at least seven MGA vector selection methodologies and carrying out a systematic evaluation of four: Hop-Skip-Jump, Random Vector, Variable Min/Max, and Modelling All Alternatives.","We examine each method's runtime, parallelizability, new solution discovery efficiency, and spatial exploration in lower dimensional (N <= 100) spaces, as well as spatial exploration in a three-zone, 8760-hour capacity expansion model case.","Through these tests, we find Random Vector provides the broadest exploration of the near-optimal feasible region and Variable Min/Max provides the most extreme results, while the two tie on computational speed.","We thus propose a new Hybrid vector selection approach combining the two methods to take advantage of the strengths of each.","Additional analysis is provided on MGA variable selection, in which we demonstrate MGA problems formulated over generation variables fail to retain cost-optimal dispatch and are thus not reflective of real operations of equivalent hypothetical capacity choices.","As such, we recommend future studies utilize a parallelized combined vector approach over the set of capacity variables for best results in computational speed and spatial exploration while retaining optimal dispatch."],"url":"http://arxiv.org/abs/2405.17342v1","category":"math.OC"}
{"created":"2024-05-27 16:47:50","title":"Searching for signatures of new physics in $B \\to K \\, \u03bd\\, \\overline\u03bd$ to distinguish between Dirac and Majorana neutrinos","abstract":"We conduct a model-independent analysis of the distinct signatures of various generic new physics possibilities in the decay $B \\to K \\, \\nu \\, \\overline{\\nu}$ by analyzing the branching ratio as well as the missing mass-square distribution. Considering the final neutrinos to be of the same flavor with non-zero mass, we discuss the new physics contributions for both Dirac and Majorana neutrino possibilities. In our study, we utilize the analytical relations among form factors in semi-leptonic $B \\to K$ transitions, which are consistent with current lattice QCD predictions to a very high numerical accuracy. We provide constraints on different new physics parameters, taking into account the recent measurement of $B^+ \\to K^+ \\, \\nu \\, \\overline{\\nu}$ branching ratio by the Belle-II collaboration. In future, if the missing mass-square distribution for $B^+ \\to K^+ \\, \\nu \\, \\overline{\\nu}$ decay gets reported by Belle-II with analysis of more events than their present data set, one can not only investigate possible new physics effects in these decays, but also probe the Dirac/Majorana nature of the neutrinos using quantum statistics, since a difference between the two cases is known to exist in the presence of non-standard neutrino interactions.","sentences":["We conduct a model-independent analysis of the distinct signatures of various generic new physics possibilities in the decay $B \\to K \\, \\nu \\, \\overline{\\nu}$ by analyzing the branching ratio as well as the missing mass-square distribution.","Considering the final neutrinos to be of the same flavor with non-zero mass, we discuss the new physics contributions for both Dirac and Majorana neutrino possibilities.","In our study, we utilize the analytical relations among form factors in semi-leptonic $B \\to K$ transitions, which are consistent with current lattice QCD predictions to a very high numerical accuracy.","We provide constraints on different new physics parameters, taking into account the recent measurement of $B^+ \\to K^+ \\, \\nu \\, \\overline{\\nu}$ branching ratio by the Belle-II collaboration.","In future, if the missing mass-square distribution for $B^+ \\to K^+ \\, \\nu \\, \\overline{\\nu}$ decay gets reported by Belle-II with analysis of more events than their present data set, one can not only investigate possible new physics effects in these decays, but also probe the Dirac/Majorana nature of the neutrinos using quantum statistics, since a difference between the two cases is known to exist in the presence of non-standard neutrino interactions."],"url":"http://arxiv.org/abs/2405.17341v1","category":"hep-ph"}
{"created":"2024-05-27 16:42:51","title":"Physics-Informed Real NVP for Satellite Power System Fault Detection","abstract":"The unique challenges posed by the space environment, characterized by extreme conditions and limited accessibility, raise the need for robust and reliable techniques to identify and prevent satellite faults. Fault detection methods in the space sector are required to ensure mission success and to protect valuable assets. In this context, this paper proposes an Artificial Intelligence (AI) based fault detection methodology and evaluates its performance on ADAPT (Advanced Diagnostics and Prognostics Testbed), an Electrical Power System (EPS) dataset, crafted in laboratory by NASA.   Our study focuses on the application of a physics-informed (PI) real-valued non-volume preserving (Real NVP) model for fault detection in space systems. The efficacy of this method is systematically compared against other AI approaches such as Gated Recurrent Unit (GRU) and Autoencoder-based techniques.   Results show that our physics-informed approach outperforms existing methods of fault detection, demonstrating its suitability for addressing the unique challenges of satellite EPS sub-system faults. Furthermore, we unveil the competitive advantage of physics-informed loss in AI models to address specific space needs, namely robustness, reliability, and power constraints, crucial for space exploration and satellite missions.","sentences":["The unique challenges posed by the space environment, characterized by extreme conditions and limited accessibility, raise the need for robust and reliable techniques to identify and prevent satellite faults.","Fault detection methods in the space sector are required to ensure mission success and to protect valuable assets.","In this context, this paper proposes an Artificial Intelligence (AI) based fault detection methodology and evaluates its performance on ADAPT (Advanced Diagnostics and Prognostics Testbed), an Electrical Power System (EPS) dataset, crafted in laboratory by NASA.   ","Our study focuses on the application of a physics-informed (PI) real-valued non-volume preserving (Real NVP) model for fault detection in space systems.","The efficacy of this method is systematically compared against other AI approaches such as Gated Recurrent Unit (GRU) and Autoencoder-based techniques.   ","Results show that our physics-informed approach outperforms existing methods of fault detection, demonstrating its suitability for addressing the unique challenges of satellite EPS sub-system faults.","Furthermore, we unveil the competitive advantage of physics-informed loss in AI models to address specific space needs, namely robustness, reliability, and power constraints, crucial for space exploration and satellite missions."],"url":"http://arxiv.org/abs/2405.17339v1","category":"cs.LG"}
{"created":"2024-05-27 16:37:34","title":"Cost-efficient Knowledge-based Question Answering with Large Language Models","abstract":"Knowledge-based question answering (KBQA) is widely used in many scenarios that necessitate domain knowledge. Large language models (LLMs) bring opportunities to KBQA, while their costs are significantly higher and absence of domain-specific knowledge during pre-training. We are motivated to combine LLMs and prior small models on knowledge graphs (KGMs) for both inferential accuracy and cost saving. However, it remains challenging since accuracy and cost are not readily combined in the optimization as two distinct metrics. It is also laborious for model selection since different models excel in diverse knowledge. To this end, we propose Coke, a novel cost-efficient strategy for KBQA with LLMs, modeled as a tailored multi-armed bandit problem to minimize calls to LLMs within limited budgets. We first formulate the accuracy expectation with a cluster-level Thompson Sampling for either KGMs or LLMs. A context-aware policy is optimized to further distinguish the expert model subject to the question semantics. The overall decision is bounded by the cost regret according to historical expenditure on failures. Extensive experiments showcase the superior performance of Coke, which moves the Pareto frontier with up to 20.89% saving of GPT-4 fees while achieving a 2.74% higher accuracy on the benchmark datasets.","sentences":["Knowledge-based question answering (KBQA) is widely used in many scenarios that necessitate domain knowledge.","Large language models (LLMs) bring opportunities to KBQA, while their costs are significantly higher and absence of domain-specific knowledge during pre-training.","We are motivated to combine LLMs and prior small models on knowledge graphs (KGMs) for both inferential accuracy and cost saving.","However, it remains challenging since accuracy and cost are not readily combined in the optimization as two distinct metrics.","It is also laborious for model selection since different models excel in diverse knowledge.","To this end, we propose Coke, a novel cost-efficient strategy for KBQA with LLMs, modeled as a tailored multi-armed bandit problem to minimize calls to LLMs within limited budgets.","We first formulate the accuracy expectation with a cluster-level Thompson Sampling for either KGMs or LLMs.","A context-aware policy is optimized to further distinguish the expert model subject to the question semantics.","The overall decision is bounded by the cost regret according to historical expenditure on failures.","Extensive experiments showcase the superior performance of Coke, which moves the Pareto frontier with up to 20.89% saving of GPT-4 fees while achieving a 2.74% higher accuracy on the benchmark datasets."],"url":"http://arxiv.org/abs/2405.17337v1","category":"cs.CL"}
{"created":"2024-05-27 16:35:00","title":"Serial Monopoly on Blockchains with Quasi-patient Users","abstract":"This paper introduces and investigates an extension of the price dynamics in serial monopoly blockchain described in Nisan [Nis23], tailored to accommodate quasi-patient users. Our model reflects users' diminishing interest in having their transactions added to the ledger over time, resulting in only a fraction $\\delta$ of the current demand persisting in the subsequent round. The framework presented by Lavi et al. [LSZ22], where users are impatient and derive utility only from immediate transaction inclusion in the next block, corresponds to $\\delta=0$. Fully patient users who wait forever as in [Nis23], correspond to $\\delta=1$ in our model. This work provides new bounds on the price dynamics for the more interesting case $\\delta\\in(0,1)$, showing somewhat unexpected effects on the dynamics itself. While the dynamics for the fully patient case is essentially \"oblivious\" of the structure of the daily demand curve, this is no longer true for finite $\\delta < 1$. Moreover, the dynamics undergoes a \"transition phase\" where for some $\\delta$ it behaves as in the fully patient setting ($\\delta=1$), and for some smaller values $\\delta'<\\delta$ it stops \"oscillating\" and stays at the highest (\"monopolist\") price. We provide quantitative bounds and analytical results that apply to different demand functions showing that the bounds for $\\delta=1$ are not tight in general, for $\\delta<1$. These provide guarantees on the minimum (\"admission\") price such that transaction willing to pay that price are eventually included (and those who do not want are never included).","sentences":["This paper introduces and investigates an extension of the price dynamics in serial monopoly blockchain described in Nisan","[Nis23], tailored to accommodate quasi-patient users.","Our model reflects users' diminishing interest in having their transactions added to the ledger over time, resulting in only a fraction $\\delta$ of the current demand persisting in the subsequent round.","The framework presented by Lavi et al.","[LSZ22], where users are impatient and derive utility only from immediate transaction inclusion in the next block, corresponds to $\\delta=0$. Fully patient users who wait forever as in [Nis23], correspond to $\\delta=1$ in our model.","This work provides new bounds on the price dynamics for the more interesting case $\\delta\\in(0,1)$, showing somewhat unexpected effects on the dynamics itself.","While the dynamics for the fully patient case is essentially \"oblivious\" of the structure of the daily demand curve, this is no longer true for finite $\\delta <","1$.","Moreover, the dynamics undergoes a \"transition phase\" where for some $\\delta$ it behaves as in the fully patient setting ($\\delta=1$), and for some smaller values $\\delta'<\\delta$ it stops \"oscillating\" and stays at the highest (\"monopolist\") price.","We provide quantitative bounds and analytical results that apply to different demand functions showing that the bounds for $\\delta=1$ are not tight in general, for $\\delta<1$. These provide guarantees on the minimum (\"admission\") price such that transaction willing to pay that price are eventually included (and those who do not want are never included)."],"url":"http://arxiv.org/abs/2405.17334v1","category":"cs.GT"}
{"created":"2024-05-27 16:34:18","title":"Conditioning on Time is All You Need for Synthetic Survival Data Generation","abstract":"Synthetic data generation holds considerable promise, offering avenues to enhance privacy, fairness, and data accessibility. Despite the availability of various methods for generating synthetic tabular data, challenges persist, particularly in specialized applications such as survival analysis. One significant obstacle in survival data generation is censoring, which manifests as not knowing the precise timing of observed (target) events for certain instances. Existing methods face difficulties in accurately reproducing the real distribution of event times for both observed (uncensored) events and censored events, i.e., the generated event-time distributions do not accurately match the underlying distributions of the real data. So motivated, we propose a simple paradigm to produce synthetic survival data by generating covariates conditioned on event times (and censoring indicators), thus allowing one to reuse existing conditional generative models for tabular data without significant computational overhead, and without making assumptions about the (usually unknown) generation mechanism underlying censoring. We evaluate this method via extensive experiments on real-world datasets. Our methodology outperforms multiple competitive baselines at generating survival data, while improving the performance of downstream survival models trained on it and tested on real data.","sentences":["Synthetic data generation holds considerable promise, offering avenues to enhance privacy, fairness, and data accessibility.","Despite the availability of various methods for generating synthetic tabular data, challenges persist, particularly in specialized applications such as survival analysis.","One significant obstacle in survival data generation is censoring, which manifests as not knowing the precise timing of observed (target) events for certain instances.","Existing methods face difficulties in accurately reproducing the real distribution of event times for both observed (uncensored) events and censored events, i.e., the generated event-time distributions do not accurately match the underlying distributions of the real data.","So motivated, we propose a simple paradigm to produce synthetic survival data by generating covariates conditioned on event times (and censoring indicators), thus allowing one to reuse existing conditional generative models for tabular data without significant computational overhead, and without making assumptions about the (usually unknown) generation mechanism underlying censoring.","We evaluate this method via extensive experiments on real-world datasets.","Our methodology outperforms multiple competitive baselines at generating survival data, while improving the performance of downstream survival models trained on it and tested on real data."],"url":"http://arxiv.org/abs/2405.17333v1","category":"stat.ML"}
{"created":"2024-05-27 16:28:37","title":"Joint MIMO Transceiver and Reflector Design for Reconfigurable Intelligent Surface-Assisted Communication","abstract":"In this paper, we consider a reconfigurable intelligent surface (RIS)-assisted multiple-input multiple-output communication system with multiple antennas at both the base station (BS) and the user. We plan to maximize the achievable rate through jointly optimizing the transmit precoding matrix, the receive combining matrix, and the RIS reflection matrix under the constraints of the transmit power at the BS and the unit-modulus reflection at the RIS. Regarding the non-trivial problem form, we initially reformulate it into an considerable problem to make it tractable by utilizing the relationship between the achievable rate and the weighted minimum mean squared error. Next, the transmit precoding matrix, the receive combining matrix, and the RIS reflection matrix are alternately optimized. In particular, the optimal transmit precoding matrix and receive combining matrix are obtained in closed forms. Furthermore, a pair of computationally efficient methods are proposed for the RIS reflection matrix, namely the semi-definite relaxation (SDR) method and the successive closed form (SCF) method. We theoretically prove that both methods are ensured to converge, and the SCF-based algorithm is able to converges to a Karush-Kuhn-Tucker point of the problem.","sentences":["In this paper, we consider a reconfigurable intelligent surface (RIS)-assisted multiple-input multiple-output communication system with multiple antennas at both the base station (BS) and the user.","We plan to maximize the achievable rate through jointly optimizing the transmit precoding matrix, the receive combining matrix, and the RIS reflection matrix under the constraints of the transmit power at the BS and the unit-modulus reflection at the RIS.","Regarding the non-trivial problem form, we initially reformulate it into an considerable problem to make it tractable by utilizing the relationship between the achievable rate and the weighted minimum mean squared error.","Next, the transmit precoding matrix, the receive combining matrix, and the RIS reflection matrix are alternately optimized.","In particular, the optimal transmit precoding matrix and receive combining matrix are obtained in closed forms.","Furthermore, a pair of computationally efficient methods are proposed for the RIS reflection matrix, namely the semi-definite relaxation (SDR) method and the successive closed form (SCF) method.","We theoretically prove that both methods are ensured to converge, and the SCF-based algorithm is able to converges to a Karush-Kuhn-Tucker point of the problem."],"url":"http://arxiv.org/abs/2405.17329v1","category":"cs.IT"}
{"created":"2024-05-27 16:26:19","title":"Exploring hidden priors when interpreting gravitational wave and electromagnetic probes of the nuclear equation of state","abstract":"The wide range of sub- and super-nuclear densities achieved in neutron stars makes them ideal probes of dense nuclear behavior in the form of the nuclear equation of state (EoS). Studying neutron stars both in isolation and in highly dynamic events, many recent observations, most famously the gravitational wave and electromagnetic signals associated with the BNS merger GW170817/AT2017gfo, have provided suggestive insight into these highest nuclear densities. Measurements of galactic neutron star masses and radii from NICER and other radio and X-ray measurements provide critical complementary perspectives, bounding other features of the EoS. Though nominally congruent, in this paper we highlight many underappreciated \"hidden\" priors embedded in joint analysis of these many messengers, and their systematic impact on joint EoS inference. In this work, we perform a careful step-by-step Bayesian inference using a simple low-dimensional parametric EoS model, incrementally adding information from galactic pulsars, gravitational wave sources, and more speculative constraints involving kilonovae and the neutron star maximum mass. At each stage, we carefully discuss the marginal likelihood, explaining how hidden priors impact conclusions. Conversely, we also quantify how much information these measurements have, arguing that many if not most have minimal impact relative to the explicit and hidden prior assumptions. Specifically, we find two features dominate our inference: on the one hand, the choice of EoS parameterization and particularly hyperprior, and on the other the astrophysical priors associated with interpreting events, particularly the multimessenger source GW170817. In an appendix, we outline a simple semianalytic projection suitable for assessing the measurability of the EoS with ensembles of present and future detections.","sentences":["The wide range of sub- and super-nuclear densities achieved in neutron stars makes them ideal probes of dense nuclear behavior in the form of the nuclear equation of state (EoS).","Studying neutron stars both in isolation and in highly dynamic events, many recent observations, most famously the gravitational wave and electromagnetic signals associated with the BNS merger GW170817/AT2017gfo, have provided suggestive insight into these highest nuclear densities.","Measurements of galactic neutron star masses and radii from NICER and other radio and X-ray measurements provide critical complementary perspectives, bounding other features of the EoS. Though nominally congruent, in this paper we highlight many underappreciated \"hidden\" priors embedded in joint analysis of these many messengers, and their systematic impact on joint EoS inference.","In this work, we perform a careful step-by-step Bayesian inference using a simple low-dimensional parametric EoS model, incrementally adding information from galactic pulsars, gravitational wave sources, and more speculative constraints involving kilonovae and the neutron star maximum mass.","At each stage, we carefully discuss the marginal likelihood, explaining how hidden priors impact conclusions.","Conversely, we also quantify how much information these measurements have, arguing that many if not most have minimal impact relative to the explicit and hidden prior assumptions.","Specifically, we find two features dominate our inference: on the one hand, the choice of EoS parameterization and particularly hyperprior, and on the other the astrophysical priors associated with interpreting events, particularly the multimessenger source GW170817.","In an appendix, we outline a simple semianalytic projection suitable for assessing the measurability of the EoS with ensembles of present and future detections."],"url":"http://arxiv.org/abs/2405.17326v1","category":"astro-ph.HE"}
{"created":"2024-05-27 16:23:50","title":"Novel Approaches for ML-Assisted Particle Track Reconstruction and Hit Clustering","abstract":"Track reconstruction is a vital aspect of High-Energy Physics (HEP) and plays a critical role in major experiments. In this study, we delve into unexplored avenues for particle track reconstruction and hit clustering. Firstly, we enhance the algorithmic design effort by utilising a simplified simulator (REDVID) to generate training data that is specifically composed for simplicity. We demonstrate the effectiveness of this data in guiding the development of optimal network architectures. Additionally, we investigate the application of image segmentation networks for this task, exploring their potential for accurate track reconstruction. Moreover, we approach the task from a different perspective by treating it as a hit sequence to track sequence translation problem. Specifically, we explore the utilisation of Transformer architectures for tracking purposes. Our preliminary findings are covered in detail. By considering this novel approach, we aim to uncover new insights and potential advancements in track reconstruction. This research sheds light on previously unexplored methods and provides valuable insights for the field of particle track reconstruction and hit clustering in HEP.","sentences":["Track reconstruction is a vital aspect of High-Energy Physics (HEP) and plays a critical role in major experiments.","In this study, we delve into unexplored avenues for particle track reconstruction and hit clustering.","Firstly, we enhance the algorithmic design effort by utilising a simplified simulator (REDVID) to generate training data that is specifically composed for simplicity.","We demonstrate the effectiveness of this data in guiding the development of optimal network architectures.","Additionally, we investigate the application of image segmentation networks for this task, exploring their potential for accurate track reconstruction.","Moreover, we approach the task from a different perspective by treating it as a hit sequence to track sequence translation problem.","Specifically, we explore the utilisation of Transformer architectures for tracking purposes.","Our preliminary findings are covered in detail.","By considering this novel approach, we aim to uncover new insights and potential advancements in track reconstruction.","This research sheds light on previously unexplored methods and provides valuable insights for the field of particle track reconstruction and hit clustering in HEP."],"url":"http://arxiv.org/abs/2405.17325v1","category":"hep-ex"}
{"created":"2024-05-27 16:23:34","title":"Leveraging Offline Data in Linear Latent Bandits","abstract":"Sequential decision-making domains such as recommender systems, healthcare and education often have unobserved heterogeneity in the population that can be modeled using latent bandits $-$ a framework where an unobserved latent state determines the model for a trajectory. While the latent bandit framework is compelling, the extent of its generality is unclear. We first address this by establishing a de Finetti theorem for decision processes, and show that $\\textit{every}$ exchangeable and coherent stateless decision process is a latent bandit. The latent bandit framework lends itself particularly well to online learning with offline datasets, a problem of growing interest in sequential decision-making. One can leverage offline latent bandit data to learn a complex model for each latent state, so that an agent can simply learn the latent state online to act optimally. We focus on a linear model for a latent bandit with $d_A$-dimensional actions, where the latent states lie in an unknown $d_K$-dimensional subspace for $d_K \\ll d_A$. We present SOLD, a novel principled method to learn this subspace from short offline trajectories with guarantees. We then provide two methods to leverage this subspace online: LOCAL-UCB and ProBALL-UCB. We demonstrate that LOCAL-UCB enjoys $\\tilde O(\\min(d_A\\sqrt{T}, d_K\\sqrt{T}(1+\\sqrt{d_AT/d_KN})))$ regret guarantees, where the effective dimension is lower when the size $N$ of the offline dataset is larger. ProBALL-UCB enjoys a slightly weaker guarantee, but is more practical and computationally efficient. Finally, we establish the efficacy of our methods using experiments on both synthetic data and real-life movie recommendation data from MovieLens.","sentences":["Sequential decision-making domains such as recommender systems, healthcare and education often have unobserved heterogeneity in the population that can be modeled using latent bandits $-$ a framework where an unobserved latent state determines the model for a trajectory.","While the latent bandit framework is compelling, the extent of its generality is unclear.","We first address this by establishing a de Finetti theorem for decision processes, and show that $\\textit{every}$ exchangeable and coherent stateless decision process is a latent bandit.","The latent bandit framework lends itself particularly well to online learning with offline datasets, a problem of growing interest in sequential decision-making.","One can leverage offline latent bandit data to learn a complex model for each latent state, so that an agent can simply learn the latent state online to act optimally.","We focus on a linear model for a latent bandit with $d_A$-dimensional actions, where the latent states lie in an unknown $d_K$-dimensional subspace for $d_K \\ll d_A$. We present SOLD, a novel principled method to learn this subspace from short offline trajectories with guarantees.","We then provide two methods to leverage this subspace online: LOCAL-UCB and ProBALL-UCB.","We demonstrate that LOCAL-UCB enjoys $\\tilde O(\\min(d_A\\sqrt{T}, d_K\\sqrt{T}(1+\\sqrt{d_AT/d_KN})))$ regret guarantees, where the effective dimension is lower when the size $N$ of the offline dataset is larger.","ProBALL-UCB enjoys a slightly weaker guarantee, but is more practical and computationally efficient.","Finally, we establish the efficacy of our methods using experiments on both synthetic data and real-life movie recommendation data from MovieLens."],"url":"http://arxiv.org/abs/2405.17324v1","category":"cs.LG"}
{"created":"2024-05-27 16:21:41","title":"Evaluation of computational and energy performance in matrix multiplication algorithms on CPU and GPU using MKL, cuBLAS and SYCL","abstract":"Matrix multiplication is fundamental in the backpropagation algorithm used to train deep neural network models. Libraries like Intel's MKL or NVIDIA's cuBLAS implemented new and optimized matrix multiplication techniques that increase performance and reduce computational costs. These techniques can also be implemented in CUDA and SYCL and functions with AVX2 and AVX512 instructions, which have lower performance but better precision. The study compares execution times and power consumption using PAPI and PERF and compares accuracy for different matrix sizes. Comparisons were made on architectures such as third and fourth-generation Intel CPUs and NVIDIA V100 and A100 GPUs. The MKL library showed the best performance with a slight loss of precision, while OpenMP and SYCL on the CPU implementation showed the best accuracy but a loss of performance. On the other hand, the results on GPU showed that cuBLAS with tensor cores had the best performance; however, it had a cost in accuracy. The cuBLAS library without these specialized cores shows minimal performance loss and much higher accuracy. The data obtained on different architectures showed that the CPU could achieve performance close to that obtained on the GPU with increased power consumption. These results are conditional on certain hardware specifications, such as the number of cores, clock frequency, processor generation for the CPU, and the speed and bandwidth of the PCI bus and device architecture (compute capability) for the GPU.","sentences":["Matrix multiplication is fundamental in the backpropagation algorithm used to train deep neural network models.","Libraries like Intel's MKL or NVIDIA's cuBLAS implemented new and optimized matrix multiplication techniques that increase performance and reduce computational costs.","These techniques can also be implemented in CUDA and SYCL and functions with AVX2 and AVX512 instructions, which have lower performance but better precision.","The study compares execution times and power consumption using PAPI and PERF and compares accuracy for different matrix sizes.","Comparisons were made on architectures such as third and fourth-generation Intel CPUs and NVIDIA V100 and A100 GPUs.","The MKL library showed the best performance with a slight loss of precision, while OpenMP and SYCL on the CPU implementation showed the best accuracy but a loss of performance.","On the other hand, the results on GPU showed that cuBLAS with tensor cores had the best performance; however, it had a cost in accuracy.","The cuBLAS library without these specialized cores shows minimal performance loss and much higher accuracy.","The data obtained on different architectures showed that the CPU could achieve performance close to that obtained on the GPU with increased power consumption.","These results are conditional on certain hardware specifications, such as the number of cores, clock frequency, processor generation for the CPU, and the speed and bandwidth of the PCI bus and device architecture (compute capability) for the GPU."],"url":"http://arxiv.org/abs/2405.17322v1","category":"cs.DC"}
{"created":"2024-05-27 16:16:41","title":"The interpolation problem: When can you pass a curve of a given type through N random points in space?","abstract":"The interpolation problem is a natural and fundamental question whose roots trace back to ancient Greece. The story is long and rich, with many chapters, and a complete solution has been obtained only recently. Exploring it leads us on a tour through a number of general themes in geometry. This concrete problem motivates fundamental concepts such as moduli spaces and their properties, deformation theory, normal bundles, and more. Questions about smooth objects lead us to consider singular (non-smooth) objects, and in fact these smooth objects are studied by instead focusing on somehow simpler \"non-smooth\" objects, and then deforming them.","sentences":["The interpolation problem is a natural and fundamental question whose roots trace back to ancient Greece.","The story is long and rich, with many chapters, and a complete solution has been obtained only recently.","Exploring it leads us on a tour through a number of general themes in geometry.","This concrete problem motivates fundamental concepts such as moduli spaces and their properties, deformation theory, normal bundles, and more.","Questions about smooth objects lead us to consider singular (non-smooth) objects, and in fact these smooth objects are studied by instead focusing on somehow simpler \"non-smooth\" objects, and then deforming them."],"url":"http://arxiv.org/abs/2405.17313v1","category":"math.AG"}
{"created":"2024-05-27 16:13:13","title":"Rigidity results for Serrin's overdetermined problems in Riemannian manifolds","abstract":"In this work, we are interested in studying Serrin's overdetermined problems in Riemannian manifolds. For manifolds endowed with a conformal vector field, we prove a Pohozoaev-type identity to show a Serrin's type rigidity result using the P-function approach introduced by Weinberger. We proceed with a conformal change to achieve this goal, starting from a geometric Pohozaev identity due to Schoen. Moreover, we obtain a symmetry result for the associated Dirichlet problem by using a generalized normalized wall shear stress bound.","sentences":["In this work, we are interested in studying Serrin's overdetermined problems in Riemannian manifolds.","For manifolds endowed with a conformal vector field, we prove a Pohozoaev-type identity to show a Serrin's type rigidity result using the P-function approach introduced by Weinberger.","We proceed with a conformal change to achieve this goal, starting from a geometric Pohozaev identity due to Schoen.","Moreover, we obtain a symmetry result for the associated Dirichlet problem by using a generalized normalized wall shear stress bound."],"url":"http://arxiv.org/abs/2405.17312v1","category":"math.DG"}
{"created":"2024-05-27 16:11:39","title":"Exact dynamics of quantum dissipative $XX$ models: Wannier-Stark localization in the fragmented operator space","abstract":"We address dissipative dynamics of the one-dimensional nearest-neighbour $XX$ spin-$1/2$ chain governed by the Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) equation. In the absence of dissipation the model is integrable. We identify a broad class of dissipative terms that generically destroy integrability but leave the operator space of the model fragmented into an extensive number of dynamically disjoint subspaces of varying dimensions. In sufficiently small subspaces the GKSL equation in the Heisenberg representation can be easily solved, sometimes in a closed analytical form. We provide an example of such an exact solution for a specific choice of dissipative terms. It is found that observables experience the Wannier-Stark localization in the corresponding operator subspace. As a result, the expectation values of the observables are linear combinations of essentially a few discrete decay modes, the long time dynamics being governed by the slowest mode. We examine the complex Liouvillian eigenvalue corresponding to this latter mode as a function of the dissipation strength. We find an exceptional point at a critical dissipation strength that separates oscillating and non-oscillating decay. We also describe a different type of dissipation that leads to a single decay mode in the whole operator subspace. Finally, we point out that our exact solutions of the GKSL equation entail exact solutions of the Schr\\\"odinger equation describing the quench dynamics in closed spin ladders dual to the dissipative spin chains.","sentences":["We address dissipative dynamics of the one-dimensional nearest-neighbour $XX$ spin-$1/2$ chain governed by the Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) equation.","In the absence of dissipation the model is integrable.","We identify a broad class of dissipative terms that generically destroy integrability but leave the operator space of the model fragmented into an extensive number of dynamically disjoint subspaces of varying dimensions.","In sufficiently small subspaces the GKSL equation in the Heisenberg representation can be easily solved, sometimes in a closed analytical form.","We provide an example of such an exact solution for a specific choice of dissipative terms.","It is found that observables experience the Wannier-Stark localization in the corresponding operator subspace.","As a result, the expectation values of the observables are linear combinations of essentially a few discrete decay modes, the long time dynamics being governed by the slowest mode.","We examine the complex Liouvillian eigenvalue corresponding to this latter mode as a function of the dissipation strength.","We find an exceptional point at a critical dissipation strength that separates oscillating and non-oscillating decay.","We also describe a different type of dissipation that leads to a single decay mode in the whole operator subspace.","Finally, we point out that our exact solutions of the GKSL equation entail exact solutions of the Schr\\\"odinger equation describing the quench dynamics in closed spin ladders dual to the dissipative spin chains."],"url":"http://arxiv.org/abs/2405.17310v1","category":"cond-mat.str-el"}
{"created":"2024-05-27 16:10:49","title":"Survey of Graph Neural Network for Internet of Things and NextG Networks","abstract":"The exponential increase in Internet of Things (IoT) devices coupled with 6G pushing towards higher data rates and connected devices has sparked a surge in data. Consequently, harnessing the full potential of data-driven machine learning has become one of the important thrusts. In addition to the advancement in wireless technology, it is important to efficiently use the resources available and meet the users' requirements. Graph Neural Networks (GNNs) have emerged as a promising paradigm for effectively modeling and extracting insights which inherently exhibit complex network structures due to its high performance and accuracy, scalability, adaptability, and resource efficiency. There is a lack of a comprehensive survey that focuses on the applications and advances GNN has made in the context of IoT and Next Generation (NextG) networks. To bridge that gap, this survey starts by providing a detailed description of GNN's terminologies, architecture, and the different types of GNNs. Then we provide a comprehensive survey of the advancements in applying GNNs for IoT from the perspective of data fusion and intrusion detection. Thereafter, we survey the impact GNN has made in improving spectrum awareness. Next, we provide a detailed account of how GNN has been leveraged for networking and tactical systems. Through this survey, we aim to provide a comprehensive resource for researchers to learn more about GNN in the context of wireless networks, and understand its state-of-the-art use cases while contrasting to other machine learning approaches. Finally, we also discussed the challenges and wide range of future research directions to further motivate the use of GNN for IoT and NextG Networks.","sentences":["The exponential increase in Internet of Things (IoT) devices coupled with 6G pushing towards higher data rates and connected devices has sparked a surge in data.","Consequently, harnessing the full potential of data-driven machine learning has become one of the important thrusts.","In addition to the advancement in wireless technology, it is important to efficiently use the resources available and meet the users' requirements.","Graph Neural Networks (GNNs) have emerged as a promising paradigm for effectively modeling and extracting insights which inherently exhibit complex network structures due to its high performance and accuracy, scalability, adaptability, and resource efficiency.","There is a lack of a comprehensive survey that focuses on the applications and advances GNN has made in the context of IoT and Next Generation (NextG) networks.","To bridge that gap, this survey starts by providing a detailed description of GNN's terminologies, architecture, and the different types of GNNs.","Then we provide a comprehensive survey of the advancements in applying GNNs for IoT from the perspective of data fusion and intrusion detection.","Thereafter, we survey the impact GNN has made in improving spectrum awareness.","Next, we provide a detailed account of how GNN has been leveraged for networking and tactical systems.","Through this survey, we aim to provide a comprehensive resource for researchers to learn more about GNN in the context of wireless networks, and understand its state-of-the-art use cases while contrasting to other machine learning approaches.","Finally, we also discussed the challenges and wide range of future research directions to further motivate the use of GNN for IoT and NextG Networks."],"url":"http://arxiv.org/abs/2405.17309v1","category":"cs.LG"}
{"created":"2024-05-27 16:08:00","title":"Controllable Longer Image Animation with Diffusion Models","abstract":"Generating realistic animated videos from static images is an important area of research in computer vision. Methods based on physical simulation and motion prediction have achieved notable advances, but they are often limited to specific object textures and motion trajectories, failing to exhibit highly complex environments and physical dynamics. In this paper, we introduce an open-domain controllable image animation method using motion priors with video diffusion models. Our method achieves precise control over the direction and speed of motion in the movable region by extracting the motion field information from videos and learning moving trajectories and strengths. Current pretrained video generation models are typically limited to producing very short videos, typically less than 30 frames. In contrast, we propose an efficient long-duration video generation method based on noise reschedule specifically tailored for image animation tasks, facilitating the creation of videos over 100 frames in length while maintaining consistency in content scenery and motion coordination. Specifically, we decompose the denoise process into two distinct phases: the shaping of scene contours and the refining of motion details. Then we reschedule the noise to control the generated frame sequences maintaining long-distance noise correlation. We conducted extensive experiments with 10 baselines, encompassing both commercial tools and academic methodologies, which demonstrate the superiority of our method. Our project page: \\url{https://wangqiang9.github.io/Controllable.github.io/}","sentences":["Generating realistic animated videos from static images is an important area of research in computer vision.","Methods based on physical simulation and motion prediction have achieved notable advances, but they are often limited to specific object textures and motion trajectories, failing to exhibit highly complex environments and physical dynamics.","In this paper, we introduce an open-domain controllable image animation method using motion priors with video diffusion models.","Our method achieves precise control over the direction and speed of motion in the movable region by extracting the motion field information from videos and learning moving trajectories and strengths.","Current pretrained video generation models are typically limited to producing very short videos, typically less than 30 frames.","In contrast, we propose an efficient long-duration video generation method based on noise reschedule specifically tailored for image animation tasks, facilitating the creation of videos over 100 frames in length while maintaining consistency in content scenery and motion coordination.","Specifically, we decompose the denoise process into two distinct phases: the shaping of scene contours and the refining of motion details.","Then we reschedule the noise to control the generated frame sequences maintaining long-distance noise correlation.","We conducted extensive experiments with 10 baselines, encompassing both commercial tools and academic methodologies, which demonstrate the superiority of our method.","Our project page: \\url{https://wangqiang9.github.io/Controllable.github.io/}"],"url":"http://arxiv.org/abs/2405.17306v1","category":"cs.CV"}
{"created":"2024-05-27 16:07:03","title":"Stochastic Omega-Regular Verification and Control with Supermartingales","abstract":"We present for the first time a supermartingale certificate for $\\omega$-regular specifications. We leverage the Robbins & Siegmund convergence theorem to characterize supermartingale certificates for the almost-sure acceptance of Streett conditions on general stochastic processes, which we call Streett supermartingales. This enables effective verification and control of discrete-time stochastic dynamical models with infinite state space under $\\omega$-regular and linear temporal logic specifications. Our result generalises reachability, safety, reach-avoid, persistence and recurrence specifications; our contribution applies to discrete-time stochastic dynamical models and probabilistic programs with discrete and continuous state spaces and distributions, and carries over to deterministic models and programs. We provide a synthesis algorithm for control policies and Streett supermartingales as proof certificates for $\\omega$-regular objectives, which is sound and complete for supermartingales and control policies with polynomial templates and any stochastic dynamical model whose post-expectation is expressible as a polynomial. We additionally provide an optimisation of our algorithm that reduces the problem to satisfiability modulo theories, under the assumption that templates and post-expectation are in piecewise linear form. We have built a prototype and have demonstrated the efficacy of our approach on several exemplar $\\omega$-regular verification and control synthesis problems.","sentences":["We present for the first time a supermartingale certificate for $\\omega$-regular specifications.","We leverage the Robbins & Siegmund convergence theorem to characterize supermartingale certificates for the almost-sure acceptance of Streett conditions on general stochastic processes, which we call Streett supermartingales.","This enables effective verification and control of discrete-time stochastic dynamical models with infinite state space under $\\omega$-regular and linear temporal logic specifications.","Our result generalises reachability, safety, reach-avoid, persistence and recurrence specifications; our contribution applies to discrete-time stochastic dynamical models and probabilistic programs with discrete and continuous state spaces and distributions, and carries over to deterministic models and programs.","We provide a synthesis algorithm for control policies and Streett supermartingales as proof certificates for $\\omega$-regular objectives, which is sound and complete for supermartingales and control policies with polynomial templates and any stochastic dynamical model whose post-expectation is expressible as a polynomial.","We additionally provide an optimisation of our algorithm that reduces the problem to satisfiability modulo theories, under the assumption that templates and post-expectation are in piecewise linear form.","We have built a prototype and have demonstrated the efficacy of our approach on several exemplar $\\omega$-regular verification and control synthesis problems."],"url":"http://arxiv.org/abs/2405.17304v1","category":"cs.LO"}
{"created":"2024-05-27 16:03:52","title":"A waveform model for the missing quadrupole mode from black hole coalescence: memory effect and ringdown of the $(\\ell=2,m=0)$ spherical harmonic","abstract":"In this paper we describe a model for the $(\\ell=2, m=0)$ spherical harmonic mode of the gravitational wave signal emitted by the coalescence of binary black holes, in particular, spin-aligned systems. This mode can be viewed as consisting of two components, gravitational wave memory and quasi-normal ringdown, which are both included in our model. Depending on the parameters of the binary and the sensitivity curve of the detector, but in particular for high masses, the ringdown part can contribute significantly to the signal-to-noise ratio. The model is constructed using the methods of the phenomenological waveforms program, and is calibrated to public numerical relativity data from the SXS waveforms catalog, with the analytical results derived from the BMS balance laws. The code has been implemented as an extension to the computationally efficient IMRPhenomTHM model, it can therefore be used for computationally expensive applications such as Bayesian parameter estimation. The region of validity of our model in the parameter space is given by: $q\\leq10$ and $\\chi_{1},\\chi_{2}\\in[-1,1]$, and no restrictions apply in terms of the length of the waveforms.","sentences":["In this paper we describe a model for the $(\\ell=2, m=0)$ spherical harmonic mode of the gravitational wave signal emitted by the coalescence of binary black holes, in particular, spin-aligned systems.","This mode can be viewed as consisting of two components, gravitational wave memory and quasi-normal ringdown, which are both included in our model.","Depending on the parameters of the binary and the sensitivity curve of the detector, but in particular for high masses, the ringdown part can contribute significantly to the signal-to-noise ratio.","The model is constructed using the methods of the phenomenological waveforms program, and is calibrated to public numerical relativity data from the SXS waveforms catalog, with the analytical results derived from the BMS balance laws.","The code has been implemented as an extension to the computationally efficient IMRPhenomTHM model, it can therefore be used for computationally expensive applications such as Bayesian parameter estimation.","The region of validity of our model in the parameter space is given by: $q\\leq10$ and $\\chi_{1},\\chi_{2}\\in[-1,1]$, and no restrictions apply in terms of the length of the waveforms."],"url":"http://arxiv.org/abs/2405.17302v1","category":"gr-qc"}
{"created":"2024-05-27 16:02:29","title":"Quantum violations of simultaneous reality","abstract":"With basis on (i) the physical principle of local causality and (ii) a certain notion of elements of reality, Einstein, Podolsky, and Rosen (EPR) put forward an argument showing that physical instances may exist in which two non-commuting observables can be simultaneous elements of the physical reality. Here, by introducing an operational criterion of simultaneous reality, we show the very opposite, that is, quantum mechanics actually prevents non-commuting observables to be simultaneous elements of reality in general. In addition, we introduce a measure to quantify the extent to which the criterion is violated and explore the implications of such a measure in connection with incompatibility and correlations. Our findings suggest new manners of intepreting quantum phenomena.","sentences":["With basis on (i) the physical principle of local causality and (ii) a certain notion of elements of reality, Einstein, Podolsky, and Rosen (EPR) put forward an argument showing that physical instances may exist in which two non-commuting observables can be simultaneous elements of the physical reality.","Here, by introducing an operational criterion of simultaneous reality, we show the very opposite, that is, quantum mechanics actually prevents non-commuting observables to be simultaneous elements of reality in general.","In addition, we introduce a measure to quantify the extent to which the criterion is violated and explore the implications of such a measure in connection with incompatibility and correlations.","Our findings suggest new manners of intepreting quantum phenomena."],"url":"http://arxiv.org/abs/2405.17300v1","category":"quant-ph"}
{"created":"2024-05-27 16:00:45","title":"Simplicity Bias of Two-Layer Networks beyond Linearly Separable Data","abstract":"Simplicity bias, the propensity of deep models to over-rely on simple features, has been identified as a potential reason for limited out-of-distribution generalization of neural networks (Shah et al., 2020). Despite the important implications, this phenomenon has been theoretically confirmed and characterized only under strong dataset assumptions, such as linear separability (Lyu et al., 2021). In this work, we characterize simplicity bias for general datasets in the context of two-layer neural networks initialized with small weights and trained with gradient flow. Specifically, we prove that in the early training phases, network features cluster around a few directions that do not depend on the size of the hidden layer. Furthermore, for datasets with an XOR-like pattern, we precisely identify the learned features and demonstrate that simplicity bias intensifies during later training stages. These results indicate that features learned in the middle stages of training may be more useful for OOD transfer. We support this hypothesis with experiments on image data.","sentences":["Simplicity bias, the propensity of deep models to over-rely on simple features, has been identified as a potential reason for limited out-of-distribution generalization of neural networks (Shah et al., 2020).","Despite the important implications, this phenomenon has been theoretically confirmed and characterized only under strong dataset assumptions, such as linear separability (Lyu et al., 2021).","In this work, we characterize simplicity bias for general datasets in the context of two-layer neural networks initialized with small weights and trained with gradient flow.","Specifically, we prove that in the early training phases, network features cluster around a few directions that do not depend on the size of the hidden layer.","Furthermore, for datasets with an XOR-like pattern, we precisely identify the learned features and demonstrate that simplicity bias intensifies during later training stages.","These results indicate that features learned in the middle stages of training may be more useful for OOD transfer.","We support this hypothesis with experiments on image data."],"url":"http://arxiv.org/abs/2405.17299v1","category":"stat.ML"}
{"created":"2024-05-27 16:00:00","title":"In-sensor Computing ANN Capacitive Sensors","abstract":"This letter proposes an in-sensor computing multiply-and-accumulate (MAC) circuit based on capacitance. The MAC circuits can constitute an artificial neural network(ANN) layer and be operated as ANN classifiers and autoencoders. The proposed circuit is a promising scheme for capacitive ANN image sensors, showing competitively high efficiency and lower power.","sentences":["This letter proposes an in-sensor computing multiply-and-accumulate (MAC) circuit based on capacitance.","The MAC circuits can constitute an artificial neural network(ANN) layer and be operated as ANN classifiers and autoencoders.","The proposed circuit is a promising scheme for capacitive ANN image sensors, showing competitively high efficiency and lower power."],"url":"http://arxiv.org/abs/2405.17295v1","category":"eess.SP"}
{"created":"2024-05-27 15:58:34","title":"Efficient Ensembles Improve Training Data Attribution","abstract":"Training data attribution (TDA) methods aim to quantify the influence of individual training data points on the model predictions, with broad applications in data-centric AI, such as mislabel detection, data selection, and copyright compensation. However, existing methods in this field, which can be categorized as retraining-based and gradient-based, have struggled with the trade-off between computational efficiency and attribution efficacy. Retraining-based methods can accurately attribute complex non-convex models but are computationally prohibitive, while gradient-based methods are efficient but often fail for non-convex models. Recent research has shown that augmenting gradient-based methods with ensembles of multiple independently trained models can achieve significantly better attribution efficacy. However, this approach remains impractical for very large-scale applications.   In this work, we discover that expensive, fully independent training is unnecessary for ensembling the gradient-based methods, and we propose two efficient ensemble strategies, DROPOUT ENSEMBLE and LORA ENSEMBLE, alternative to naive independent ensemble. These strategies significantly reduce training time (up to 80%), serving time (up to 60%), and space cost (up to 80%) while maintaining similar attribution efficacy to the naive independent ensemble. Our extensive experimental results demonstrate that the proposed strategies are effective across multiple TDA methods on diverse datasets and models, including generative settings, significantly advancing the Pareto frontier of TDA methods with better computational efficiency and attribution efficacy.","sentences":["Training data attribution (TDA) methods aim to quantify the influence of individual training data points on the model predictions, with broad applications in data-centric AI, such as mislabel detection, data selection, and copyright compensation.","However, existing methods in this field, which can be categorized as retraining-based and gradient-based, have struggled with the trade-off between computational efficiency and attribution efficacy.","Retraining-based methods can accurately attribute complex non-convex models but are computationally prohibitive, while gradient-based methods are efficient but often fail for non-convex models.","Recent research has shown that augmenting gradient-based methods with ensembles of multiple independently trained models can achieve significantly better attribution efficacy.","However, this approach remains impractical for very large-scale applications.   ","In this work, we discover that expensive, fully independent training is unnecessary for ensembling the gradient-based methods, and we propose two efficient ensemble strategies, DROPOUT ENSEMBLE and LORA ENSEMBLE, alternative to naive independent ensemble.","These strategies significantly reduce training time (up to 80%), serving time (up to 60%), and space cost (up to 80%) while maintaining similar attribution efficacy to the naive independent ensemble.","Our extensive experimental results demonstrate that the proposed strategies are effective across multiple TDA methods on diverse datasets and models, including generative settings, significantly advancing the Pareto frontier of TDA methods with better computational efficiency and attribution efficacy."],"url":"http://arxiv.org/abs/2405.17293v1","category":"cs.LG"}
{"created":"2024-05-27 15:56:50","title":"Tame deformations of map germs","abstract":"In deformations of map germs, the topological triviality at the boundary preserves, in some fixed neighbourhood, the general fibre along the family. We give conditions under which this conservation holds.","sentences":["In deformations of map germs, the topological triviality at the boundary preserves, in some fixed neighbourhood, the general fibre along the family.","We give conditions under which this conservation holds."],"url":"http://arxiv.org/abs/2405.17292v1","category":"math.AG"}
{"created":"2024-05-27 15:52:27","title":"Opinion-Guided Reinforcement Learning","abstract":"Human guidance is often desired in reinforcement learning to improve the performance of the learning agent. However, human insights are often mere opinions and educated guesses rather than well-formulated arguments. While opinions are subject to uncertainty, e.g., due to partial informedness or ignorance about a problem, they also emerge earlier than hard evidence could be produced. Thus, guiding reinforcement learning agents through opinions offers the potential for more performant learning processes, but comes with the challenge of modeling and managing opinions in a formal way. In this article, we present a method to guide reinforcement learning agents through opinions. To this end, we provide an end-to-end method to model and manage advisors' opinions. To assess the utility of the approach, we evaluate it with synthetic and human advisors, at different levels of uncertainty, and under multiple advise strategies. Our results indicate that opinions, even if uncertain, improve the performance of reinforcement learning agents, resulting in higher rewards, more efficient exploration, and a better reinforced policy. Although we demonstrate our approach in a simplified topological running example, our approach is applicable to complex problems with higher dimensions as well.","sentences":["Human guidance is often desired in reinforcement learning to improve the performance of the learning agent.","However, human insights are often mere opinions and educated guesses rather than well-formulated arguments.","While opinions are subject to uncertainty, e.g., due to partial informedness or ignorance about a problem, they also emerge earlier than hard evidence could be produced.","Thus, guiding reinforcement learning agents through opinions offers the potential for more performant learning processes, but comes with the challenge of modeling and managing opinions in a formal way.","In this article, we present a method to guide reinforcement learning agents through opinions.","To this end, we provide an end-to-end method to model and manage advisors' opinions.","To assess the utility of the approach, we evaluate it with synthetic and human advisors, at different levels of uncertainty, and under multiple advise strategies.","Our results indicate that opinions, even if uncertain, improve the performance of reinforcement learning agents, resulting in higher rewards, more efficient exploration, and a better reinforced policy.","Although we demonstrate our approach in a simplified topological running example, our approach is applicable to complex problems with higher dimensions as well."],"url":"http://arxiv.org/abs/2405.17287v1","category":"cs.LG"}
{"created":"2024-05-27 15:49:51","title":"Asymptotics of extensions of simple $\\mathbb Q$-algebras","abstract":"We answer various questions concerning the distribution of extensions of a given central simple algebra $K$ over a number field. Specifically, we give asymptotics for the count of inner Galois extensions $L|K$ of fixed degree and center with bounded discriminant. We also relate the distribution of outer extensions of $K$ to the distribution of field extensions of its center $Z(K)$. This paper generalizes the study of asymptotics of field extensions to the noncommutative case in an analogous manner to the program initiated by Deschamps and Legrand to extend inverse Galois theory to skew fields.","sentences":["We answer various questions concerning the distribution of extensions of a given central simple algebra $K$ over a number field.","Specifically, we give asymptotics for the count of inner Galois extensions $L|K$ of fixed degree and center with bounded discriminant.","We also relate the distribution of outer extensions of $K$ to the distribution of field extensions of its center $Z(K)$. This paper generalizes the study of asymptotics of field extensions to the noncommutative case in an analogous manner to the program initiated by Deschamps and Legrand to extend inverse Galois theory to skew fields."],"url":"http://arxiv.org/abs/2405.17286v1","category":"math.NT"}
{"created":"2024-05-27 15:47:46","title":"An NLP Crosswalk Between the Common Core State Standards and NAEP Item Specifications","abstract":"Natural language processing (NLP) is rapidly developing for applications in educational assessment. In this paper, I describe an NLP-based procedure that can be used to support subject matter experts in establishing a crosswalk between item specifications and content standards. This paper extends recent work by proposing and demonstrating the use of multivariate similarity based on embedding vectors for sentences or texts. In particular, a hybrid regression procedure is demonstrated for establishing the match of each content standard to multiple item specifications. The procedure is used to evaluate the match of the Common Core State Standards (CCSS) for mathematics at grade 4 to the corresponding item specifications for the 2026 National Assessment of Educational Progress (NAEP).","sentences":["Natural language processing (NLP) is rapidly developing for applications in educational assessment.","In this paper, I describe an NLP-based procedure that can be used to support subject matter experts in establishing a crosswalk between item specifications and content standards.","This paper extends recent work by proposing and demonstrating the use of multivariate similarity based on embedding vectors for sentences or texts.","In particular, a hybrid regression procedure is demonstrated for establishing the match of each content standard to multiple item specifications.","The procedure is used to evaluate the match of the Common Core State Standards (CCSS) for mathematics at grade 4 to the corresponding item specifications for the 2026 National Assessment of Educational Progress (NAEP)."],"url":"http://arxiv.org/abs/2405.17284v1","category":"cs.CL"}
{"created":"2024-05-27 15:44:31","title":"Anisotropic Third Harmonic Generation in Two-Dimensional Tin Sulfide","abstract":"The in-plane anisotropic properties of two-dimensional (2D) group IV monochalcogenides provide an additional degree of freedom which can be used in future optoelectronic devices. Here, it is shown that the third harmonic generation (THG) signal produced by ultrathin tin (II) sulfide (SnS) is in-plane anisotropic with respect to the incident linear polarization of the laser field. We fit the experimental polarization-resolved THG (P-THG) measurements with a nonlinear optics model, which accounts for the orthorhombic crystal structure of 2D SnS. We calculate the relative magnitudes of the \\{chi}^(3) tensor components by recording and simultaneously fitting both orthogonal components of the P-THG intensity. Furthermore, we introduce a THG anisotropy ratio, whose calculated values compare the total THG intensity when the excitation linear polarization is along the armchair crystallographic direction with the case when it is along the zigzag direction. Our results provide quantitative information on the anisotropic nature of the THG process in SnS, paving the way to a better understanding of anisotropic nonlinear light-matter interactions, and the development of polarization-sensitive nonlinear optical devices.","sentences":["The in-plane anisotropic properties of two-dimensional (2D) group IV monochalcogenides provide an additional degree of freedom which can be used in future optoelectronic devices.","Here, it is shown that the third harmonic generation (THG) signal produced by ultrathin tin (II) sulfide (SnS) is in-plane anisotropic with respect to the incident linear polarization of the laser field.","We fit the experimental polarization-resolved THG (P-THG) measurements with a nonlinear optics model, which accounts for the orthorhombic crystal structure of 2D SnS. We calculate the relative magnitudes of the \\{chi}^(3) tensor components by recording and simultaneously fitting both orthogonal components of the P-THG intensity.","Furthermore, we introduce a THG anisotropy ratio, whose calculated values compare the total THG intensity when the excitation linear polarization is along the armchair crystallographic direction with the case when it is along the zigzag direction.","Our results provide quantitative information on the anisotropic nature of the THG process in SnS, paving the way to a better understanding of anisotropic nonlinear light-matter interactions, and the development of polarization-sensitive nonlinear optical devices."],"url":"http://arxiv.org/abs/2405.17281v1","category":"physics.optics"}
{"created":"2024-05-27 15:44:06","title":"A Library for Automatic Natural Language Generation of Spanish Texts","abstract":"In this article we present a novel system for natural language generation (NLG) of Spanish sentences from a minimum set of meaningful words (such as nouns, verbs and adjectives) which, unlike other state-of-the-art solutions, performs the NLG task in a fully automatic way, exploiting both knowledge-based and statistical approaches. Relying on its linguistic knowledge of vocabulary and grammar, the system is able to generate complete, coherent and correctly spelled sentences from the main word sets presented by the user. The system, which was designed to be integrable, portable and efficient, can be easily adapted to other languages by design and can feasibly be integrated in a wide range of digital devices. During its development we also created a supplementary lexicon for Spanish, aLexiS, with wide coverage and high precision, as well as syntactic trees from a freely available definite-clause grammar. The resulting NLG library has been evaluated both automatically and manually (annotation). The system can potentially be used in different application domains such as augmentative communication and automatic generation of administrative reports or news.","sentences":["In this article we present a novel system for natural language generation (NLG) of Spanish sentences from a minimum set of meaningful words (such as nouns, verbs and adjectives) which, unlike other state-of-the-art solutions, performs the NLG task in a fully automatic way, exploiting both knowledge-based and statistical approaches.","Relying on its linguistic knowledge of vocabulary and grammar, the system is able to generate complete, coherent and correctly spelled sentences from the main word sets presented by the user.","The system, which was designed to be integrable, portable and efficient, can be easily adapted to other languages by design and can feasibly be integrated in a wide range of digital devices.","During its development we also created a supplementary lexicon for Spanish, aLexiS, with wide coverage and high precision, as well as syntactic trees from a freely available definite-clause grammar.","The resulting NLG library has been evaluated both automatically and manually (annotation).","The system can potentially be used in different application domains such as augmentative communication and automatic generation of administrative reports or news."],"url":"http://arxiv.org/abs/2405.17280v1","category":"cs.CL"}
{"created":"2024-05-27 15:40:34","title":"Socially-Aware Shared Control Navigation for Assistive Mobile Robots in the Built Environment","abstract":"As the number of Persons with Disabilities (PWD), particularly those with one or more physical impairments, increases, there is an increasing demand for assistive robotic technologies that can support independent mobility in the built environment and reduce the burden on caregivers. Current assistive mobility platforms (e.g., robotic wheelchairs) often fail to incorporate user preferences and control, leading to reduced trust and efficiency. Existing shared control algorithms do not allow the incorporation of the user control preferences inside the navigation framework or the path planning algorithm. In addition, existing dynamic local planner algorithms for robotic wheelchairs do not take into account the social spaces of people, potentially leading such platforms to infringe upon these areas and cause discomfort. To address these concerns, this work introduces a novel socially-aware shared autonomy-based navigation system for assistive mobile robotic platforms.   Our navigation framework comprises a Global Planner and a Local Planner. To implement the Global Planner, the proposed approach introduces a novel User Preference Field (UPF) theory within its global planning framework, explicitly acknowledging user preferences to adeptly navigate away from congested areas. For the Local Planner, we propose a Socially-aware Shared Control-based Model Predictive Control with Dynamic Control Barrier Function (SS-MPC-DCBF) to adjust movements in real-time, integrating user preferences for safer, more autonomous navigation. Evaluation results show that our Global Planner aligns closely with user preferences compared to baselines, and our Local Planner demonstrates enhanced safety and efficiency in dynamic and static scenarios. This integrated approach fosters trust and autonomy, crucial for the acceptance of assistive mobility technologies in the built environment.","sentences":["As the number of Persons with Disabilities (PWD), particularly those with one or more physical impairments, increases, there is an increasing demand for assistive robotic technologies that can support independent mobility in the built environment and reduce the burden on caregivers.","Current assistive mobility platforms (e.g., robotic wheelchairs) often fail to incorporate user preferences and control, leading to reduced trust and efficiency.","Existing shared control algorithms do not allow the incorporation of the user control preferences inside the navigation framework or the path planning algorithm.","In addition, existing dynamic local planner algorithms for robotic wheelchairs do not take into account the social spaces of people, potentially leading such platforms to infringe upon these areas and cause discomfort.","To address these concerns, this work introduces a novel socially-aware shared autonomy-based navigation system for assistive mobile robotic platforms.   ","Our navigation framework comprises a Global Planner and a Local Planner.","To implement the Global Planner, the proposed approach introduces a novel User Preference Field (UPF) theory within its global planning framework, explicitly acknowledging user preferences to adeptly navigate away from congested areas.","For the Local Planner, we propose a Socially-aware Shared Control-based Model Predictive Control with Dynamic Control Barrier Function (SS-MPC-DCBF) to adjust movements in real-time, integrating user preferences for safer, more autonomous navigation.","Evaluation results show that our Global Planner aligns closely with user preferences compared to baselines, and our Local Planner demonstrates enhanced safety and efficiency in dynamic and static scenarios.","This integrated approach fosters trust and autonomy, crucial for the acceptance of assistive mobility technologies in the built environment."],"url":"http://arxiv.org/abs/2405.17279v1","category":"cs.RO"}
{"created":"2024-05-27 15:35:39","title":"An extension of Krishnan's central limit theorem to the Brown-Thompson groups","abstract":"We extend a central limit theorem, recently established for the Thompson group $F=F_2$ by Krishnan, to the Brown-Thompson groups $F_p$, where $p$ is any integer greater than or equal to $2$. The non-commutative probability space considered is the group algebra $\\mathbb{C}[F_p]$, equipped with the canonical trace. The random variables in question are $a_n:= (x_n + x_n^{-1})/\\sqrt{2}$, where $\\{x_i\\}_{i\\geq 0}$ represents the standard family of infinite generators. Analogously to the case of $F=F_2$, it is established that the limit distribution of $s_n = (a_0 + \\ldots + a_{n-1})/\\sqrt{n}$ converges to the standard normal distribution.   Furthermore, it is demonstrated that for a state corresponding to Jones's oriented subgroup $\\vec{F}$, such a central limit theorem does not hold.","sentences":["We extend a central limit theorem, recently established for the Thompson group $F=F_2$ by Krishnan, to the Brown-Thompson groups $F_p$, where $p$ is any integer greater than or equal to $2$. The non-commutative probability space considered is the group algebra $\\mathbb{C}[F_p]$, equipped with the canonical trace.","The random variables in question are $a_n:= (x_n + x_n^{-1})/\\sqrt{2}$, where $\\{x_i\\}_{i\\geq 0}$ represents the standard family of infinite generators.","Analogously to the case of $F=F_2$, it is established that the limit distribution of $s_n = (a_0 + \\ldots + a_{n-1})/\\sqrt{n}$ converges to the standard normal distribution.   ","Furthermore, it is demonstrated that for a state corresponding to Jones's oriented subgroup $\\vec{F}$, such a central limit theorem does not hold."],"url":"http://arxiv.org/abs/2405.17275v1","category":"math.OA"}
{"created":"2024-05-27 15:33:16","title":"DPN: Decoupling Partition and Navigation for Neural Solvers of Min-max Vehicle Routing Problems","abstract":"The min-max vehicle routing problem (min-max VRP) traverses all given customers by assigning several routes and aims to minimize the length of the longest route. Recently, reinforcement learning (RL)-based sequential planning methods have exhibited advantages in solving efficiency and optimality. However, these methods fail to exploit the problem-specific properties in learning representations, resulting in less effective features for decoding optimal routes. This paper considers the sequential planning process of min-max VRPs as two coupled optimization tasks: customer partition for different routes and customer navigation in each route (i.e., partition and navigation). To effectively process min-max VRP instances, we present a novel attention-based Partition-and-Navigation encoder (P&N Encoder) that learns distinct embeddings for partition and navigation. Furthermore, we utilize an inherent symmetry in decoding routes and develop an effective agent-permutation-symmetric (APS) loss function. Experimental results demonstrate that the proposed Decoupling-Partition-Navigation (DPN) method significantly surpasses existing learning-based methods in both single-depot and multi-depot min-max VRPs. Our code is available at","sentences":["The min-max vehicle routing problem (min-max VRP) traverses all given customers by assigning several routes and aims to minimize the length of the longest route.","Recently, reinforcement learning (RL)-based sequential planning methods have exhibited advantages in solving efficiency and optimality.","However, these methods fail to exploit the problem-specific properties in learning representations, resulting in less effective features for decoding optimal routes.","This paper considers the sequential planning process of min-max VRPs as two coupled optimization tasks: customer partition for different routes and customer navigation in each route (i.e., partition and navigation).","To effectively process min-max VRP instances, we present a novel attention-based Partition-and-Navigation encoder (P&N Encoder) that learns distinct embeddings for partition and navigation.","Furthermore, we utilize an inherent symmetry in decoding routes and develop an effective agent-permutation-symmetric (APS) loss function.","Experimental results demonstrate that the proposed Decoupling-Partition-Navigation (DPN) method significantly surpasses existing learning-based methods in both single-depot and multi-depot min-max VRPs.","Our code is available at"],"url":"http://arxiv.org/abs/2405.17272v1","category":"cs.LG"}
{"created":"2024-05-27 15:25:32","title":"FedHPL: Efficient Heterogeneous Federated Learning with Prompt Tuning and Logit Distillation","abstract":"Federated learning (FL) is a popular privacy-preserving paradigm that enables distributed clients to collaboratively train models with a central server while keeping raw data locally. In practice, distinct model architectures, varying data distributions, and limited resources across local clients inevitably cause model performance degradation and a slowdown in convergence speed. However, existing FL methods can only solve some of the above heterogeneous challenges and have obvious performance limitations. Notably, a unified framework has not yet been explored to overcome these challenges. Accordingly, we propose FedHPL, a parameter-efficient unified $\\textbf{Fed}$erated learning framework for $\\textbf{H}$eterogeneous settings based on $\\textbf{P}$rompt tuning and $\\textbf{L}$ogit distillation. Specifically, we employ a local prompt tuning scheme that leverages a few learnable visual prompts to efficiently fine-tune the frozen pre-trained foundation model for downstream tasks, thereby accelerating training and improving model performance under limited local resources and data heterogeneity. Moreover, we design a global logit distillation scheme to handle the model heterogeneity and guide the local training. In detail, we leverage logits to implicitly capture local knowledge and design a weighted knowledge aggregation mechanism to generate global client-specific logits. We provide a theoretical guarantee on the generalization error bound for FedHPL. The experiments on various benchmark datasets under diverse settings of models and data demonstrate that our framework outperforms state-of-the-art FL approaches, with less computation overhead and training rounds.","sentences":["Federated learning (FL) is a popular privacy-preserving paradigm that enables distributed clients to collaboratively train models with a central server while keeping raw data locally.","In practice, distinct model architectures, varying data distributions, and limited resources across local clients inevitably cause model performance degradation and a slowdown in convergence speed.","However, existing FL methods can only solve some of the above heterogeneous challenges and have obvious performance limitations.","Notably, a unified framework has not yet been explored to overcome these challenges.","Accordingly, we propose FedHPL, a parameter-efficient unified $\\textbf{Fed}$erated learning framework for $\\textbf{H}$eterogeneous settings based on $\\textbf{P}$rompt tuning and $\\textbf{L}$ogit distillation.","Specifically, we employ a local prompt tuning scheme that leverages a few learnable visual prompts to efficiently fine-tune the frozen pre-trained foundation model for downstream tasks, thereby accelerating training and improving model performance under limited local resources and data heterogeneity.","Moreover, we design a global logit distillation scheme to handle the model heterogeneity and guide the local training.","In detail, we leverage logits to implicitly capture local knowledge and design a weighted knowledge aggregation mechanism to generate global client-specific logits.","We provide a theoretical guarantee on the generalization error bound for FedHPL.","The experiments on various benchmark datasets under diverse settings of models and data demonstrate that our framework outperforms state-of-the-art FL approaches, with less computation overhead and training rounds."],"url":"http://arxiv.org/abs/2405.17267v1","category":"cs.LG"}
{"created":"2024-05-27 15:23:54","title":"Assessing uncertainty in Gaussian mixtures-based entropy estimation","abstract":"Entropy estimation plays a crucial role in various fields, such as information theory, statistical data science, and machine learning. However, traditional entropy estimation methods often struggle with complex data distributions. Mixture-based estimation of entropy has been recently proposed and gained attention due to its ease of use and accuracy. This paper presents a novel approach to quantify the uncertainty associated with this mixture-based entropy estimation method using weighted likelihood bootstrap. Unlike standard methods, our approach leverages the underlying mixture structure by assigning random weights to observations in a weighted likelihood bootstrap procedure, leading to more accurate uncertainty estimation. The generation of weights is also investigated, leading to the proposal of using weights obtained from a Dirichlet distribution with parameter $\\alpha = 0.8137$ instead of the usual $\\alpha = 1$. Furthermore, the use of centered percentile intervals emerges as the preferred choice to ensure empirical coverage close to the nominal level. Extensive simulation studies comparing different resampling strategies are presented and results discussed. The proposed approach is illustrated by analyzing the log-returns of daily Gold prices at COMEX for the years 2014--2022, and the Net Rating scores, an advanced statistic used in basketball analytics, for NBA teams with reference to the 2022/23 regular season.","sentences":["Entropy estimation plays a crucial role in various fields, such as information theory, statistical data science, and machine learning.","However, traditional entropy estimation methods often struggle with complex data distributions.","Mixture-based estimation of entropy has been recently proposed and gained attention due to its ease of use and accuracy.","This paper presents a novel approach to quantify the uncertainty associated with this mixture-based entropy estimation method using weighted likelihood bootstrap.","Unlike standard methods, our approach leverages the underlying mixture structure by assigning random weights to observations in a weighted likelihood bootstrap procedure, leading to more accurate uncertainty estimation.","The generation of weights is also investigated, leading to the proposal of using weights obtained from a Dirichlet distribution with parameter $\\alpha = 0.8137$ instead of the usual $\\alpha = 1$.","Furthermore, the use of centered percentile intervals emerges as the preferred choice to ensure empirical coverage close to the nominal level.","Extensive simulation studies comparing different resampling strategies are presented and results discussed.","The proposed approach is illustrated by analyzing the log-returns of daily Gold prices at COMEX for the years 2014--2022, and the Net Rating scores, an advanced statistic used in basketball analytics, for NBA teams with reference to the 2022/23 regular season."],"url":"http://arxiv.org/abs/2405.17265v1","category":"stat.ME"}
{"created":"2024-05-27 15:22:58","title":"On the Noise Robustness of In-Context Learning for Text Generation","abstract":"Large language models (LLMs) have shown impressive performance on downstream tasks by in-context learning (ICL), which heavily relies on the quality of demonstrations selected from a large set of annotated examples. Recent works claim that in-context learning is robust to noisy demonstrations in text classification. In this work, we show that, on text generation tasks, noisy annotations significantly hurt the performance of in-context learning. To circumvent the issue, we propose a simple and effective approach called Local Perplexity Ranking (LPR), which replaces the \"noisy\" candidates with their nearest neighbors that are more likely to be clean. Our method is motivated by analyzing the perplexity deviation caused by noisy labels and decomposing perplexity into inherent perplexity and matching perplexity. Our key idea behind LPR is thus to decouple the matching perplexity by performing the ranking among the neighbors in semantic space. Our approach can prevent the selected demonstrations from including mismatched input-label pairs while preserving the effectiveness of the original selection methods. Extensive experiments demonstrate the effectiveness of LPR, improving the EM score by up to 18.75 on common benchmarks with noisy annotations.","sentences":["Large language models (LLMs) have shown impressive performance on downstream tasks by in-context learning (ICL), which heavily relies on the quality of demonstrations selected from a large set of annotated examples.","Recent works claim that in-context learning is robust to noisy demonstrations in text classification.","In this work, we show that, on text generation tasks, noisy annotations significantly hurt the performance of in-context learning.","To circumvent the issue, we propose a simple and effective approach called Local Perplexity Ranking (LPR), which replaces the \"noisy\" candidates with their nearest neighbors that are more likely to be clean.","Our method is motivated by analyzing the perplexity deviation caused by noisy labels and decomposing perplexity into inherent perplexity and matching perplexity.","Our key idea behind LPR is thus to decouple the matching perplexity by performing the ranking among the neighbors in semantic space.","Our approach can prevent the selected demonstrations from including mismatched input-label pairs while preserving the effectiveness of the original selection methods.","Extensive experiments demonstrate the effectiveness of LPR, improving the EM score by up to 18.75 on common benchmarks with noisy annotations."],"url":"http://arxiv.org/abs/2405.17264v1","category":"cs.CL"}
{"created":"2024-05-27 15:18:12","title":"Accelerating Simulation of Two-Phase Flows with Neural PDE Surrogates","abstract":"Simulation is a powerful tool to better understand physical systems, but generally requires computationally expensive numerical methods. Downstream applications of such simulations can become computationally infeasible if they require many forward solves, for example in the case of inverse design with many degrees of freedom. In this work, we investigate and extend neural PDE solvers as a tool to aid in scaling simulations for two-phase flow problems, and simulations of oil expulsion from a pore specifically. We extend existing numerical methods for this problem to a more complex setting involving varying geometries of the domain to generate a challenging dataset. Further, we investigate three prominent neural PDE solver methods, namely the UNet, DRN and U-FNO, and extend them for characteristics of the oil-expulsion problem: (1) spatial conditioning on the geometry; (2) periodicity in the boundary; (3) approximate mass conservation. We scale all methods and benchmark their speed-accuracy trade-off, evaluate qualitative properties, and perform an ablation study. We find that the investigated methods can accurately model the droplet dynamics with up to three orders of magnitude speed-up, that our extensions improve performance over the baselines, and that the introduced varying geometries constitute a significantly more challenging setting over the previously considered oil expulsion problem.","sentences":["Simulation is a powerful tool to better understand physical systems, but generally requires computationally expensive numerical methods.","Downstream applications of such simulations can become computationally infeasible if they require many forward solves, for example in the case of inverse design with many degrees of freedom.","In this work, we investigate and extend neural PDE solvers as a tool to aid in scaling simulations for two-phase flow problems, and simulations of oil expulsion from a pore specifically.","We extend existing numerical methods for this problem to a more complex setting involving varying geometries of the domain to generate a challenging dataset.","Further, we investigate three prominent neural PDE solver methods, namely the UNet, DRN and U-FNO, and extend them for characteristics of the oil-expulsion problem: (1) spatial conditioning on the geometry; (2) periodicity in the boundary; (3) approximate mass conservation.","We scale all methods and benchmark their speed-accuracy trade-off, evaluate qualitative properties, and perform an ablation study.","We find that the investigated methods can accurately model the droplet dynamics with up to three orders of magnitude speed-up, that our extensions improve performance over the baselines, and that the introduced varying geometries constitute a significantly more challenging setting over the previously considered oil expulsion problem."],"url":"http://arxiv.org/abs/2405.17260v1","category":"cs.LG"}
{"created":"2024-05-27 15:15:08","title":"$\\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning","abstract":"Low-rank adapters (LoRA) and their variants are popular parameter-efficient fine-tuning (PEFT) techniques that closely match full model fine-tune performance while requiring only a small number of additional parameters. These additional LoRA parameters are specific to the base model being adapted. When the base model needs to be deprecated and replaced with a new one, all the associated LoRA modules need to be re-trained. Such re-training requires access to the data used to train the LoRA for the original base model. This is especially problematic for commercial cloud applications where the LoRA modules and the base models are hosted by service providers who may not be allowed to host proprietary client task data. To address this challenge, we propose $\\textit{Trans-LoRA}$ -- a novel method for lossless, nearly data-free transfer of LoRAs across base models. Our approach relies on synthetic data to transfer LoRA modules. Using large language models, we design a synthetic data generator to approximate the data-generating process of the $\\textit{observed}$ task data subset. Training on the resulting synthetic dataset transfers LoRA modules to new models. We show the effectiveness of our approach using both LLama and Gemma model families. Our approach achieves lossless (mostly improved) LoRA transfer between models within and across different base model families, and even between different PEFT methods, on a wide variety of tasks.","sentences":["Low-rank adapters (LoRA) and their variants are popular parameter-efficient fine-tuning (PEFT) techniques that closely match full model fine-tune performance while requiring only a small number of additional parameters.","These additional LoRA parameters are specific to the base model being adapted.","When the base model needs to be deprecated and replaced with a new one, all the associated LoRA modules need to be re-trained.","Such re-training requires access to the data used to train the LoRA for the original base model.","This is especially problematic for commercial cloud applications where the LoRA modules and the base models are hosted by service providers who may not be allowed to host proprietary client task data.","To address this challenge, we propose $\\textit{Trans-LoRA}$ -- a novel method for lossless, nearly data-free transfer of LoRAs across base models.","Our approach relies on synthetic data to transfer LoRA modules.","Using large language models, we design a synthetic data generator to approximate the data-generating process of the $\\textit{observed}$ task data subset.","Training on the resulting synthetic dataset transfers LoRA modules to new models.","We show the effectiveness of our approach using both LLama and Gemma model families.","Our approach achieves lossless (mostly improved)","LoRA transfer between models within and across different base model families, and even between different PEFT methods, on a wide variety of tasks."],"url":"http://arxiv.org/abs/2405.17258v1","category":"cs.LG"}
{"created":"2024-05-27 15:10:26","title":"Classifying post-Minkowskian geometries for gravitational waves via loop-by-loop Baikov","abstract":"We use the loop-by-loop Baikov representation to investigate the geometries in Feynman integrals contributing to the classical dynamics of a black-hole two-body system in the post-Minkowskian expansion of general relativity. These geometries determine the spaces of functions to which the corresponding Feynman diagrams evaluate. As a proof of principle, we provide a full classification of the geometries appearing up to three loops, i.e. fourth post-Minkowskian order, for all diagrams relevant to the conservative as well as the dissipative dynamics, finding full agreement with the literature. Moreover, we show that the non-planar top topology at four loops, which is the most complicated sector with respect to integration-byparts identities, has an algebraic leading singularity and thus can only depend on non-trivial geometries through its subsectors.","sentences":["We use the loop-by-loop Baikov representation to investigate the geometries in Feynman integrals contributing to the classical dynamics of a black-hole two-body system in the post-Minkowskian expansion of general relativity.","These geometries determine the spaces of functions to which the corresponding Feynman diagrams evaluate.","As a proof of principle, we provide a full classification of the geometries appearing up to three loops, i.e. fourth post-Minkowskian order, for all diagrams relevant to the conservative as well as the dissipative dynamics, finding full agreement with the literature.","Moreover, we show that the non-planar top topology at four loops, which is the most complicated sector with respect to integration-byparts identities, has an algebraic leading singularity and thus can only depend on non-trivial geometries through its subsectors."],"url":"http://arxiv.org/abs/2405.17255v1","category":"hep-th"}
{"created":"2024-05-27 15:07:57","title":"Gaussian Embedding of Temporal Networks","abstract":"Representing the nodes of continuous-time temporal graphs in a low-dimensional latent space has wide-ranging applications, from prediction to visualization. Yet, analyzing continuous-time relational data with timestamped interactions introduces unique challenges due to its sparsity. Merely embedding nodes as trajectories in the latent space overlooks this sparsity, emphasizing the need to quantify uncertainty around the latent positions. In this paper, we propose TGNE (\\textbf{T}emporal \\textbf{G}aussian \\textbf{N}etwork \\textbf{E}mbedding), an innovative method that bridges two distinct strands of literature: the statistical analysis of networks via Latent Space Models (LSM)\\cite{Hoff2002} and temporal graph machine learning. TGNE embeds nodes as piece-wise linear trajectories of Gaussian distributions in the latent space, capturing both structural information and uncertainty around the trajectories. We evaluate TGNE's effectiveness in reconstructing the original graph and modelling uncertainty. The results demonstrate that TGNE generates competitive time-varying embedding locations compared to common baselines for reconstructing unobserved edge interactions based on observed edges. Furthermore, the uncertainty estimates align with the time-varying degree distribution in the network, providing valuable insights into the temporal dynamics of the graph. To facilitate reproducibility, we provide an open-source implementation of TGNE at \\url{https://github.com/aida-ugent/tgne}.","sentences":["Representing the nodes of continuous-time temporal graphs in a low-dimensional latent space has wide-ranging applications, from prediction to visualization.","Yet, analyzing continuous-time relational data with timestamped interactions introduces unique challenges due to its sparsity.","Merely embedding nodes as trajectories in the latent space overlooks this sparsity, emphasizing the need to quantify uncertainty around the latent positions.","In this paper, we propose TGNE (\\textbf{T}emporal \\textbf{G}aussian \\textbf{N}etwork \\textbf{E}mbedding), an innovative method that bridges two distinct strands of literature: the statistical analysis of networks via Latent Space Models (LSM)\\cite{Hoff2002} and temporal graph machine learning.","TGNE embeds nodes as piece-wise linear trajectories of Gaussian distributions in the latent space, capturing both structural information and uncertainty around the trajectories.","We evaluate TGNE's effectiveness in reconstructing the original graph and modelling uncertainty.","The results demonstrate that TGNE generates competitive time-varying embedding locations compared to common baselines for reconstructing unobserved edge interactions based on observed edges.","Furthermore, the uncertainty estimates align with the time-varying degree distribution in the network, providing valuable insights into the temporal dynamics of the graph.","To facilitate reproducibility, we provide an open-source implementation of TGNE at \\url{https://github.com/aida-ugent/tgne}."],"url":"http://arxiv.org/abs/2405.17253v1","category":"cs.LG"}
{"created":"2024-05-27 15:07:04","title":"GenWarp: Single Image to Novel Views with Semantic-Preserving Generative Warping","abstract":"Generating novel views from a single image remains a challenging task due to the complexity of 3D scenes and the limited diversity in the existing multi-view datasets to train a model on. Recent research combining large-scale text-to-image (T2I) models with monocular depth estimation (MDE) has shown promise in handling in-the-wild images. In these methods, an input view is geometrically warped to novel views with estimated depth maps, then the warped image is inpainted by T2I models. However, they struggle with noisy depth maps and loss of semantic details when warping an input view to novel viewpoints. In this paper, we propose a novel approach for single-shot novel view synthesis, a semantic-preserving generative warping framework that enables T2I generative models to learn where to warp and where to generate, through augmenting cross-view attention with self-attention. Our approach addresses the limitations of existing methods by conditioning the generative model on source view images and incorporating geometric warping signals. Qualitative and quantitative evaluations demonstrate that our model outperforms existing methods in both in-domain and out-of-domain scenarios. Project page is available at https://GenWarp-NVS.github.io/.","sentences":["Generating novel views from a single image remains a challenging task due to the complexity of 3D scenes and the limited diversity in the existing multi-view datasets to train a model on.","Recent research combining large-scale text-to-image (T2I) models with monocular depth estimation (MDE) has shown promise in handling in-the-wild images.","In these methods, an input view is geometrically warped to novel views with estimated depth maps, then the warped image is inpainted by T2I models.","However, they struggle with noisy depth maps and loss of semantic details when warping an input view to novel viewpoints.","In this paper, we propose a novel approach for single-shot novel view synthesis, a semantic-preserving generative warping framework that enables T2I generative models to learn where to warp and where to generate, through augmenting cross-view attention with self-attention.","Our approach addresses the limitations of existing methods by conditioning the generative model on source view images and incorporating geometric warping signals.","Qualitative and quantitative evaluations demonstrate that our model outperforms existing methods in both in-domain and out-of-domain scenarios.","Project page is available at https://GenWarp-NVS.github.io/."],"url":"http://arxiv.org/abs/2405.17251v1","category":"cs.CV"}
{"created":"2024-05-27 15:06:03","title":"\"Pass the butter\": A study on desktop-classic multitasking robotic arm based on advanced YOLOv7 and BERT","abstract":"In recent years, various intelligent autonomous robots have begun to appear in daily life and production. Desktop-level robots are characterized by their flexible deployment, rapid response, and suitability for light workload environments. In order to meet the current societal demand for service robot technology, this study proposes using a miniaturized desktop-level robot (by ROS) as a carrier, locally deploying a natural language model (NLP-BERT), and integrating visual recognition (CV-YOLO) and speech recognition technology (ASR-Whisper) as inputs to achieve autonomous decision-making and rational action by the desktop robot. Three comprehensive experiments were designed to validate the robotic arm, and the results demonstrate excellent performance using this approach across all three experiments. In Task 1, the execution rates for speech recognition and action performance were 92.6% and 84.3%, respectively. In Task 2, the highest execution rates under the given conditions reached 92.1% and 84.6%, while in Task 3, the highest execution rates were 95.2% and 80.8%, respectively. Therefore, it can be concluded that the proposed solution integrating ASR, NLP, and other technologies on edge devices is feasible and provides a technical and engineering foundation for realizing multimodal desktop-level robots.","sentences":["In recent years, various intelligent autonomous robots have begun to appear in daily life and production.","Desktop-level robots are characterized by their flexible deployment, rapid response, and suitability for light workload environments.","In order to meet the current societal demand for service robot technology, this study proposes using a miniaturized desktop-level robot (by ROS) as a carrier, locally deploying a natural language model (NLP-BERT), and integrating visual recognition (CV-YOLO) and speech recognition technology (ASR-Whisper) as inputs to achieve autonomous decision-making and rational action by the desktop robot.","Three comprehensive experiments were designed to validate the robotic arm, and the results demonstrate excellent performance using this approach across all three experiments.","In Task 1, the execution rates for speech recognition and action performance were 92.6% and 84.3%, respectively.","In Task 2, the highest execution rates under the given conditions reached 92.1% and 84.6%, while in Task 3, the highest execution rates were 95.2% and 80.8%, respectively.","Therefore, it can be concluded that the proposed solution integrating ASR, NLP, and other technologies on edge devices is feasible and provides a technical and engineering foundation for realizing multimodal desktop-level robots."],"url":"http://arxiv.org/abs/2405.17250v1","category":"cs.RO"}
{"created":"2024-05-27 15:04:50","title":"Assessing LLMs Suitability for Knowledge Graph Completion","abstract":"Recent work shown the capability of Large Language Models (LLMs) to solve tasks related to Knowledge Graphs, such as Knowledge Graph Completion, even in Zero- or Few-Shot paradigms. However, they are known to hallucinate answers, or output results in a non-deterministic manner, thus leading to wrongly reasoned responses, even if they satisfy the user's demands. To highlight opportunities and challenges in knowledge graphs-related tasks, we experiment with two distinguished LLMs, namely Mixtral-8x7B-Instruct-v0.1, and gpt-3.5-turbo-0125, on Knowledge Graph Completion for static knowledge graphs, using prompts constructed following the TELeR taxonomy, in Zero- and One-Shot contexts, on a Task-Oriented Dialogue system use case. When evaluated using both strict and flexible metrics measurement manners, our results show that LLMs could be fit for such a task if prompts encapsulate sufficient information and relevant examples.","sentences":["Recent work shown the capability of Large Language Models (LLMs) to solve tasks related to Knowledge Graphs, such as Knowledge Graph Completion, even in Zero- or Few-Shot paradigms.","However, they are known to hallucinate answers, or output results in a non-deterministic manner, thus leading to wrongly reasoned responses, even if they satisfy the user's demands.","To highlight opportunities and challenges in knowledge graphs-related tasks, we experiment with two distinguished LLMs, namely Mixtral-8x7B-Instruct-v0.1, and gpt-3.5-turbo-0125, on Knowledge Graph Completion for static knowledge graphs, using prompts constructed following the TELeR taxonomy, in Zero- and One-Shot contexts, on a Task-Oriented Dialogue system use case.","When evaluated using both strict and flexible metrics measurement manners, our results show that LLMs could be fit for such a task if prompts encapsulate sufficient information and relevant examples."],"url":"http://arxiv.org/abs/2405.17249v1","category":"cs.CL"}
{"created":"2024-05-27 15:01:23","title":"An Introduction to Vision-Language Modeling","abstract":"Following the recent popularity of Large Language Models (LLMs), several attempts have been made to extend them to the visual domain. From having a visual assistant that could guide us through unfamiliar environments to generative models that produce images using only a high-level text description, the vision-language model (VLM) applications will significantly impact our relationship with technology. However, there are many challenges that need to be addressed to improve the reliability of those models. While language is discrete, vision evolves in a much higher dimensional space in which concepts cannot always be easily discretized. To better understand the mechanics behind mapping vision to language, we present this introduction to VLMs which we hope will help anyone who would like to enter the field. First, we introduce what VLMs are, how they work, and how to train them. Then, we present and discuss approaches to evaluate VLMs. Although this work primarily focuses on mapping images to language, we also discuss extending VLMs to videos.","sentences":["Following the recent popularity of Large Language Models (LLMs), several attempts have been made to extend them to the visual domain.","From having a visual assistant that could guide us through unfamiliar environments to generative models that produce images using only a high-level text description, the vision-language model (VLM) applications will significantly impact our relationship with technology.","However, there are many challenges that need to be addressed to improve the reliability of those models.","While language is discrete, vision evolves in a much higher dimensional space in which concepts cannot always be easily discretized.","To better understand the mechanics behind mapping vision to language, we present this introduction to VLMs which we hope will help anyone who would like to enter the field.","First, we introduce what VLMs are, how they work, and how to train them.","Then, we present and discuss approaches to evaluate VLMs.","Although this work primarily focuses on mapping images to language, we also discuss extending VLMs to videos."],"url":"http://arxiv.org/abs/2405.17247v1","category":"cs.LG"}
{"created":"2024-05-27 15:01:04","title":"Galaxy: A Resource-Efficient Collaborative Edge AI System for In-situ Transformer Inference","abstract":"Transformer-based models have unlocked a plethora of powerful intelligent applications at the edge, such as voice assistant in smart home. Traditional deployment approaches offload the inference workloads to the remote cloud server, which would induce substantial pressure on the backbone network as well as raise users' privacy concerns. To address that, in-situ inference has been recently recognized for edge intelligence, but it still confronts significant challenges stemming from the conflict between intensive workloads and limited on-device computing resources. In this paper, we leverage our observation that many edge environments usually comprise a rich set of accompanying trusted edge devices with idle resources and propose Galaxy, a collaborative edge AI system that breaks the resource walls across heterogeneous edge devices for efficient Transformer inference acceleration. Galaxy introduces a novel hybrid model parallelism to orchestrate collaborative inference, along with a heterogeneity-aware parallelism planning for fully exploiting the resource potential. Furthermore, Galaxy devises a tile-based fine-grained overlapping of communication and computation to mitigate the impact of tensor synchronizations on inference latency under bandwidth-constrained edge environments. Extensive evaluation based on prototype implementation demonstrates that Galaxy remarkably outperforms state-of-the-art approaches under various edge environment setups, achieving up to 2.5x end-to-end latency reduction.","sentences":["Transformer-based models have unlocked a plethora of powerful intelligent applications at the edge, such as voice assistant in smart home.","Traditional deployment approaches offload the inference workloads to the remote cloud server, which would induce substantial pressure on the backbone network as well as raise users' privacy concerns.","To address that, in-situ inference has been recently recognized for edge intelligence, but it still confronts significant challenges stemming from the conflict between intensive workloads and limited on-device computing resources.","In this paper, we leverage our observation that many edge environments usually comprise a rich set of accompanying trusted edge devices with idle resources and propose Galaxy, a collaborative edge AI system that breaks the resource walls across heterogeneous edge devices for efficient Transformer inference acceleration.","Galaxy introduces a novel hybrid model parallelism to orchestrate collaborative inference, along with a heterogeneity-aware parallelism planning for fully exploiting the resource potential.","Furthermore, Galaxy devises a tile-based fine-grained overlapping of communication and computation to mitigate the impact of tensor synchronizations on inference latency under bandwidth-constrained edge environments.","Extensive evaluation based on prototype implementation demonstrates that Galaxy remarkably outperforms state-of-the-art approaches under various edge environment setups, achieving up to 2.5x end-to-end latency reduction."],"url":"http://arxiv.org/abs/2405.17245v1","category":"cs.DC"}
{"created":"2024-05-27 14:58:24","title":"Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement Learning","abstract":"Both entropy-minimizing and entropy-maximizing (curiosity) objectives for unsupervised reinforcement learning (RL) have been shown to be effective in different environments, depending on the environment's level of natural entropy. However, neither method alone results in an agent that will consistently learn intelligent behavior across environments. In an effort to find a single entropy-based method that will encourage emergent behaviors in any environment, we propose an agent that can adapt its objective online, depending on the entropy conditions by framing the choice as a multi-armed bandit problem. We devise a novel intrinsic feedback signal for the bandit, which captures the agent's ability to control the entropy in its environment. We demonstrate that such agents can learn to control entropy and exhibit emergent behaviors in both high- and low-entropy regimes and can learn skillful behaviors in benchmark tasks. Videos of the trained agents and summarized findings can be found on our project page https://sites.google.com/view/surprise-adaptive-agents","sentences":["Both entropy-minimizing and entropy-maximizing (curiosity) objectives for unsupervised reinforcement learning (RL) have been shown to be effective in different environments, depending on the environment's level of natural entropy.","However, neither method alone results in an agent that will consistently learn intelligent behavior across environments.","In an effort to find a single entropy-based method that will encourage emergent behaviors in any environment, we propose an agent that can adapt its objective online, depending on the entropy conditions by framing the choice as a multi-armed bandit problem.","We devise a novel intrinsic feedback signal for the bandit, which captures the agent's ability to control the entropy in its environment.","We demonstrate that such agents can learn to control entropy and exhibit emergent behaviors in both high- and low-entropy regimes and can learn skillful behaviors in benchmark tasks.","Videos of the trained agents and summarized findings can be found on our project page https://sites.google.com/view/surprise-adaptive-agents"],"url":"http://arxiv.org/abs/2405.17243v1","category":"cs.LG"}
{"created":"2024-05-27 14:57:40","title":"Content-Style Decoupling for Unsupervised Makeup Transfer without Generating Pseudo Ground Truth","abstract":"The absence of real targets to guide the model training is one of the main problems with the makeup transfer task. Most existing methods tackle this problem by synthesizing pseudo ground truths (PGTs). However, the generated PGTs are often sub-optimal and their imprecision will eventually lead to performance degradation. To alleviate this issue, in this paper, we propose a novel Content-Style Decoupled Makeup Transfer (CSD-MT) method, which works in a purely unsupervised manner and thus eliminates the negative effects of generating PGTs. Specifically, based on the frequency characteristics analysis, we assume that the low-frequency (LF) component of a face image is more associated with its makeup style information, while the high-frequency (HF) component is more related to its content details. This assumption allows CSD-MT to decouple the content and makeup style information in each face image through the frequency decomposition. After that, CSD-MT realizes makeup transfer by maximizing the consistency of these two types of information between the transferred result and input images, respectively. Two newly designed loss functions are also introduced to further improve the transfer performance. Extensive quantitative and qualitative analyses show the effectiveness of our CSD-MT method. Our code is available at https://github.com/Snowfallingplum/CSD-MT.","sentences":["The absence of real targets to guide the model training is one of the main problems with the makeup transfer task.","Most existing methods tackle this problem by synthesizing pseudo ground truths (PGTs).","However, the generated PGTs are often sub-optimal and their imprecision will eventually lead to performance degradation.","To alleviate this issue, in this paper, we propose a novel Content-Style Decoupled Makeup Transfer (CSD-MT) method, which works in a purely unsupervised manner and thus eliminates the negative effects of generating PGTs.","Specifically, based on the frequency characteristics analysis, we assume that the low-frequency (LF) component of a face image is more associated with its makeup style information, while the high-frequency (HF) component is more related to its content details.","This assumption allows CSD-MT to decouple the content and makeup style information in each face image through the frequency decomposition.","After that, CSD-MT realizes makeup transfer by maximizing the consistency of these two types of information between the transferred result and input images, respectively.","Two newly designed loss functions are also introduced to further improve the transfer performance.","Extensive quantitative and qualitative analyses show the effectiveness of our CSD-MT method.","Our code is available at https://github.com/Snowfallingplum/CSD-MT."],"url":"http://arxiv.org/abs/2405.17240v1","category":"cs.CV"}
{"created":"2024-05-27 14:53:35","title":"LLM-Assisted Static Analysis for Detecting Security Vulnerabilities","abstract":"Software is prone to security vulnerabilities. Program analysis tools to detect them have limited effectiveness in practice. While large language models (or LLMs) have shown impressive code generation capabilities, they cannot do complex reasoning over code to detect such vulnerabilities, especially because this task requires whole-repository analysis. In this work, we propose IRIS, the first approach that systematically combines LLMs with static analysis to perform whole-repository reasoning to detect security vulnerabilities. We curate a new dataset, CWE-Bench-Java, comprising 120 manually validated security vulnerabilities in real-world Java projects. These projects are complex, with an average of 300,000 lines of code and a maximum of up to 7 million. Out of 120 vulnerabilities in CWE-Bench-Java, IRIS detects 69 using GPT-4, while the state-of-the-art static analysis tool only detects 27. Further, IRIS also significantly reduces the number of false alarms (by more than 80% in the best case).","sentences":["Software is prone to security vulnerabilities.","Program analysis tools to detect them have limited effectiveness in practice.","While large language models (or LLMs) have shown impressive code generation capabilities, they cannot do complex reasoning over code to detect such vulnerabilities, especially because this task requires whole-repository analysis.","In this work, we propose IRIS, the first approach that systematically combines LLMs with static analysis to perform whole-repository reasoning to detect security vulnerabilities.","We curate a new dataset, CWE-Bench-Java, comprising 120 manually validated security vulnerabilities in real-world Java projects.","These projects are complex, with an average of 300,000 lines of code and a maximum of up to 7 million.","Out of 120 vulnerabilities in CWE-Bench-Java, IRIS detects 69 using GPT-4, while the state-of-the-art static analysis tool only detects 27.","Further, IRIS also significantly reduces the number of false alarms (by more than 80% in the best case)."],"url":"http://arxiv.org/abs/2405.17238v1","category":"cs.CR"}
{"created":"2024-05-27 14:51:33","title":"From Text to Blueprint: Leveraging Text-to-Image Tools for Floor Plan Creation","abstract":"Artificial intelligence is revolutionizing architecture through text-to-image synthesis, converting textual descriptions into detailed visual representations. We explore AI-assisted floor plan design, focusing on technical background, practical methods, and future directions. Using tools like, Stable Diffusion, AI leverages models such as Generative Adversarial Networks and Variational Autoencoders to generate complex and functional floorplans designs. We evaluates these AI models' effectiveness in generating residential floor plans from text prompts. Through experiments with reference images, text prompts, and sketches, we assess the strengths and limitations of current text-to-image technology in architectural visualization. Architects can use these AI tools to streamline design processes, create multiple design options, and enhance creativity and collaboration. We highlight AI's potential to drive smarter, more efficient floorplan design, contributing to ongoing discussions on AI integration in the design profession and its future impact.","sentences":["Artificial intelligence is revolutionizing architecture through text-to-image synthesis, converting textual descriptions into detailed visual representations.","We explore AI-assisted floor plan design, focusing on technical background, practical methods, and future directions.","Using tools like, Stable Diffusion, AI leverages models such as Generative Adversarial Networks and Variational Autoencoders to generate complex and functional floorplans designs.","We evaluates these AI models' effectiveness in generating residential floor plans from text prompts.","Through experiments with reference images, text prompts, and sketches, we assess the strengths and limitations of current text-to-image technology in architectural visualization.","Architects can use these AI tools to streamline design processes, create multiple design options, and enhance creativity and collaboration.","We highlight AI's potential to drive smarter, more efficient floorplan design, contributing to ongoing discussions on AI integration in the design profession and its future impact."],"url":"http://arxiv.org/abs/2405.17236v1","category":"cs.HC"}
{"created":"2024-05-27 14:50:48","title":"A study of centaur (54598) Bienor from multiple stellar occultations and rotational light curves","abstract":"Centaurs, distinguished by their volatile-rich compositions, play a pivotal role in understanding the formation and evolution of the early solar system, as they represent remnants of the primordial material that populated the outer regions. Stellar occultations offer a means to investigate their physical properties, including shape, rotational state, or the potential presence of satellites and rings.   This work aims to conduct a detailed study of the centaur (54598) Bienor through stellar occultations and rotational light curves from photometric data collected during recent years.   We successfully predicted three stellar occultations by Bienor, which were observed from Japan, Eastern Europe, and the USA. In addition, we organized observational campaigns from Spain to obtain rotational light curves. At the same time, we develop software to generate synthetic light curves from three-dimensional shape models, enabling us to validate the outcomes through computer simulations.   We resolve Bienor's projected ellipse for December 26, 2022, determine a prograde sense of rotation, and confirm an asymmetric rotational light curve. We also retrieve the axes of its triaxial ellipsoid shape as a = (127 $\\pm$ 5) km, b = (55 $\\pm$ 4) km, and c = (45 $\\pm$ 4) km. Moreover, we refine the rotation period to 9.1736 $\\pm$ 0.0002 hours and determine a geometric albedo of (6.5 $\\pm$ 0.5) %, higher than previously determined by other methods. Finally, by comparing our findings with previous results and simulated rotational light curves, we analyze whether an irregular or contact-binary shape, the presence of an additional element such as a satellite, or significant albedo variations on Bienor's surface, may be present.","sentences":["Centaurs, distinguished by their volatile-rich compositions, play a pivotal role in understanding the formation and evolution of the early solar system, as they represent remnants of the primordial material that populated the outer regions.","Stellar occultations offer a means to investigate their physical properties, including shape, rotational state, or the potential presence of satellites and rings.   ","This work aims to conduct a detailed study of the centaur (54598) Bienor through stellar occultations and rotational light curves from photometric data collected during recent years.   ","We successfully predicted three stellar occultations by Bienor, which were observed from Japan, Eastern Europe, and the USA.","In addition, we organized observational campaigns from Spain to obtain rotational light curves.","At the same time, we develop software to generate synthetic light curves from three-dimensional shape models, enabling us to validate the outcomes through computer simulations.   ","We resolve Bienor's projected ellipse for December 26, 2022, determine a prograde sense of rotation, and confirm an asymmetric rotational light curve.","We also retrieve the axes of its triaxial ellipsoid shape as a = (127 $\\pm$ 5) km, b = (55 $\\pm$ 4) km, and c = (45 $\\pm$ 4) km.","Moreover, we refine the rotation period to 9.1736 $\\pm$ 0.0002 hours and determine a geometric albedo of (6.5 $\\pm$ 0.5) %, higher than previously determined by other methods.","Finally, by comparing our findings with previous results and simulated rotational light curves, we analyze whether an irregular or contact-binary shape, the presence of an additional element such as a satellite, or significant albedo variations on Bienor's surface, may be present."],"url":"http://arxiv.org/abs/2405.17235v1","category":"astro-ph.EP"}
{"created":"2024-05-27 14:50:42","title":"Benchmarking General Purpose In-Context Learning","abstract":"In-context learning (ICL) capabilities is becoming increasingly appealing towards building general intelligence. Taking this concept one step further, we draw a parallel to humans and many animals, who inherit primarily learning capabilities but refine their memory and acquire diverse skills and knowledge through extensive lifelong experiences. This parallel inspires our approach to general purpose in-context learning (GPICL). This paper introduces two lightweight but insightful benchmarks specifically crafted to train and evaluate GPICL functionalities. Each benchmark encompasses a wide range of diverse tasks characterized by generation and interaction, minimal transferable knowledge, and long-term dependency. These features present significant challenges for models that primarily rely on context or interactions to enhance their proficiency. We hope that these benchmarks will not only advance research in GPICL but also contribute significantly to the broader field of general intelligence.","sentences":["In-context learning (ICL) capabilities is becoming increasingly appealing towards building general intelligence.","Taking this concept one step further, we draw a parallel to humans and many animals, who inherit primarily learning capabilities but refine their memory and acquire diverse skills and knowledge through extensive lifelong experiences.","This parallel inspires our approach to general purpose in-context learning (GPICL).","This paper introduces two lightweight but insightful benchmarks specifically crafted to train and evaluate GPICL functionalities.","Each benchmark encompasses a wide range of diverse tasks characterized by generation and interaction, minimal transferable knowledge, and long-term dependency.","These features present significant challenges for models that primarily rely on context or interactions to enhance their proficiency.","We hope that these benchmarks will not only advance research in GPICL but also contribute significantly to the broader field of general intelligence."],"url":"http://arxiv.org/abs/2405.17234v1","category":"cs.AI"}
{"created":"2024-05-27 14:49:39","title":"CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs","abstract":"Parameter quantization for Large Language Models (LLMs) has attracted increasing attentions recently in reducing memory costs and improving computational efficiency. Early approaches have been widely adopted. However, the existing methods suffer from poor performance in low-bit (such as 2 to 3 bits) scenarios. In this paper, we present a novel and effective Column-Level Adaptive weight Quantization (CLAQ) framework by introducing three different types of adaptive strategies for LLM quantization. Firstly, a K-Means clustering based algorithm is proposed that allows dynamic generation of quantization centroids for each column of a parameter matrix. Secondly, we design an outlier-guided adaptive precision search strategy which can dynamically assign varying bit-widths to different columns. Finally, a dynamic outlier reservation scheme is developed to retain some parameters in their original float point precision, in trade off of boosted model performance. Experiments on various mainstream open source LLMs including LLaMA-1, LLaMA-2 and Yi demonstrate that our methods achieve the state-of-the-art results across different bit settings, especially in extremely low-bit scenarios. Code will be released soon.","sentences":["Parameter quantization for Large Language Models (LLMs) has attracted increasing attentions recently in reducing memory costs and improving computational efficiency.","Early approaches have been widely adopted.","However, the existing methods suffer from poor performance in low-bit (such as 2 to 3 bits) scenarios.","In this paper, we present a novel and effective Column-Level Adaptive weight Quantization (CLAQ) framework by introducing three different types of adaptive strategies for LLM quantization.","Firstly, a K-Means clustering based algorithm is proposed that allows dynamic generation of quantization centroids for each column of a parameter matrix.","Secondly, we design an outlier-guided adaptive precision search strategy which can dynamically assign varying bit-widths to different columns.","Finally, a dynamic outlier reservation scheme is developed to retain some parameters in their original float point precision, in trade off of boosted model performance.","Experiments on various mainstream open source LLMs including LLaMA-1, LLaMA-2 and Yi demonstrate that our methods achieve the state-of-the-art results across different bit settings, especially in extremely low-bit scenarios.","Code will be released soon."],"url":"http://arxiv.org/abs/2405.17233v1","category":"cs.LG"}
{"created":"2024-05-27 14:48:32","title":"Atomically sharp 1D interfaces in 2D lateral heterostructures of VSe$_2$-NbSe$_2$ monolayers","abstract":"Van der Waals heterostructures have emerged as an ideal platform for creating engineered artificial electronic states. While vertical heterostructures have been extensively studied, realizing high-quality lateral heterostructures with atomically sharp interfaces remains a major experimental challenge. Here, we advance a one-pot two-step molecular beam lateral epitaxy approach and successfully synthesize atomically well-defined 1T-VSe$_2$ -- 1H-NbSe$_2$ lateral heterostructures. We demonstrate the formation of defect-free lateral heterostructures and characterize their electronic structure using scanning tunnelling microscopy and spectroscopy together with density functional theory calculations. We find additional electronic states at the one-dimensional interface as well as signatures of Kondo resonances in a side-coupled geometry. Our experiments explore the full potential of lateral heterostructures for realizing exotic electronic states in low-dimensional systems for further studies of artificial designer quantum materials.","sentences":["Van der Waals heterostructures have emerged as an ideal platform for creating engineered artificial electronic states.","While vertical heterostructures have been extensively studied, realizing high-quality lateral heterostructures with atomically sharp interfaces remains a major experimental challenge.","Here, we advance a one-pot two-step molecular beam lateral epitaxy approach and successfully synthesize atomically well-defined 1T-VSe$_2$ -- 1H-NbSe$_2$ lateral heterostructures.","We demonstrate the formation of defect-free lateral heterostructures and characterize their electronic structure using scanning tunnelling microscopy and spectroscopy together with density functional theory calculations.","We find additional electronic states at the one-dimensional interface as well as signatures of Kondo resonances in a side-coupled geometry.","Our experiments explore the full potential of lateral heterostructures for realizing exotic electronic states in low-dimensional systems for further studies of artificial designer quantum materials."],"url":"http://arxiv.org/abs/2405.17231v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-27 14:44:07","title":"Learning Generic and Dynamic Locomotion of Humanoids Across Discrete Terrains","abstract":"This paper addresses the challenge of terrain-adaptive dynamic locomotion in humanoid robots, a problem traditionally tackled by optimization-based methods or reinforcement learning (RL). Optimization-based methods, such as model-predictive control, excel in finding optimal reaction forces and achieving agile locomotion, especially in quadruped, but struggle with the nonlinear hybrid dynamics of legged systems and the real-time computation of step location, timing, and reaction forces. Conversely, RL-based methods show promise in navigating dynamic and rough terrains but are limited by their extensive data requirements. We introduce a novel locomotion architecture that integrates a neural network policy, trained through RL in simplified environments, with a state-of-the-art motion controller combining model-predictive control (MPC) and whole-body impulse control (WBIC). The policy efficiently learns high-level locomotion strategies, such as gait selection and step positioning, without the need for full dynamics simulations. This control architecture enables humanoid robots to dynamically navigate discrete terrains, making strategic locomotion decisions (e.g., walking, jumping, and leaping) based on ground height maps. Our results demonstrate that this integrated control architecture achieves dynamic locomotion with significantly fewer training samples than conventional RL-based methods and can be transferred to different humanoid platforms without additional training. The control architecture has been extensively tested in dynamic simulations, accomplishing terrain height-based dynamic locomotion for three different robots.","sentences":["This paper addresses the challenge of terrain-adaptive dynamic locomotion in humanoid robots, a problem traditionally tackled by optimization-based methods or reinforcement learning (RL).","Optimization-based methods, such as model-predictive control, excel in finding optimal reaction forces and achieving agile locomotion, especially in quadruped, but struggle with the nonlinear hybrid dynamics of legged systems and the real-time computation of step location, timing, and reaction forces.","Conversely, RL-based methods show promise in navigating dynamic and rough terrains but are limited by their extensive data requirements.","We introduce a novel locomotion architecture that integrates a neural network policy, trained through RL in simplified environments, with a state-of-the-art motion controller combining model-predictive control (MPC) and whole-body impulse control (WBIC).","The policy efficiently learns high-level locomotion strategies, such as gait selection and step positioning, without the need for full dynamics simulations.","This control architecture enables humanoid robots to dynamically navigate discrete terrains, making strategic locomotion decisions (e.g., walking, jumping, and leaping) based on ground height maps.","Our results demonstrate that this integrated control architecture achieves dynamic locomotion with significantly fewer training samples than conventional RL-based methods and can be transferred to different humanoid platforms without additional training.","The control architecture has been extensively tested in dynamic simulations, accomplishing terrain height-based dynamic locomotion for three different robots."],"url":"http://arxiv.org/abs/2405.17227v1","category":"cs.RO"}
{"created":"2024-05-27 14:44:06","title":"On the regularity of the structure of branched immersions","abstract":"We show the existence of some special coordinate systems for expressing maps with branch points. These coordinates allow obtaining an explicit representation formula of branch immersions and understanding the regularity of some fundamental elements in the theory of R. D. Gulliver, R. Osserman and H. L. Royden. For conformal maps, we prove that these coordinates exist with certain regularity if and only if the mean curvature vector extends and is in a particular H\\\"older space. In the general case, we characterize this type of coordinate system. Also, using these coordinates, we study the regularity and behavior of curvatures near branch points.","sentences":["We show the existence of some special coordinate systems for expressing maps with branch points.","These coordinates allow obtaining an explicit representation formula of branch immersions and understanding the regularity of some fundamental elements in the theory of R. D. Gulliver, R. Osserman and H. L. Royden.","For conformal maps, we prove that these coordinates exist with certain regularity if and only if the mean curvature vector extends and is in a particular H\\\"older space.","In the general case, we characterize this type of coordinate system.","Also, using these coordinates, we study the regularity and behavior of curvatures near branch points."],"url":"http://arxiv.org/abs/2405.17226v1","category":"math.DG"}
{"created":"2024-05-27 14:40:08","title":"Analysing the interactions between demand side and supply side investment decisions in an oligopolistic electricity market using a stochastic mixed complementarity problem","abstract":"To meet carbon emission targets, governments around the world seek electricity consumers to invest in self-sufficiency technologies such as solar photovoltaic and battery storage. Such behaviour is sought in markets typically characterised by an oligopoly amongst generating firms. In this work, we study the interactions between investment decisions on the demand side and the supply side, and we investigate how price-making behaviour on the supply side affects these interactions compared to a situation with perfect competition. To do so, we introduce a novel stochastic mixed complementarity problem to model several players in an oligopolistic electricity market. On the supply side, we consider generating firms who make operational and investment decisions. On the demand side, we consider both industrial and residential consumers, each of whom may invest in self-sufficiency technologies. The uncertainties of wind and solar generation are the sources of the model's stochasticity. We apply the model to a case study of a stylised Irish electricity system in 2030. Our results demonstrate that price-making on the supply side increases investment in self-sufficiency on the demand side, leading to a reduction in prices and carbon emissions. We also find that both market power and self-sufficiency alter the investment and decommissioning decisions made by generation firms. Counter-intuitively, we also observe that the absence of a feed-in premium increases investment in solar generation on the demand side. Our findings highlight the importance of including both demand and supply side investment in models of electricity markets characterised by an oligopoly.","sentences":["To meet carbon emission targets, governments around the world seek electricity consumers to invest in self-sufficiency technologies such as solar photovoltaic and battery storage.","Such behaviour is sought in markets typically characterised by an oligopoly amongst generating firms.","In this work, we study the interactions between investment decisions on the demand side and the supply side, and we investigate how price-making behaviour on the supply side affects these interactions compared to a situation with perfect competition.","To do so, we introduce a novel stochastic mixed complementarity problem to model several players in an oligopolistic electricity market.","On the supply side, we consider generating firms who make operational and investment decisions.","On the demand side, we consider both industrial and residential consumers, each of whom may invest in self-sufficiency technologies.","The uncertainties of wind and solar generation are the sources of the model's stochasticity.","We apply the model to a case study of a stylised Irish electricity system in 2030.","Our results demonstrate that price-making on the supply side increases investment in self-sufficiency on the demand side, leading to a reduction in prices and carbon emissions.","We also find that both market power and self-sufficiency alter the investment and decommissioning decisions made by generation firms.","Counter-intuitively, we also observe that the absence of a feed-in premium increases investment in solar generation on the demand side.","Our findings highlight the importance of including both demand and supply side investment in models of electricity markets characterised by an oligopoly."],"url":"http://arxiv.org/abs/2405.17223v1","category":"math.OC"}
{"created":"2024-05-27 14:36:39","title":"How likely is the interstellar origin of CNEOS14? On the reliability of the CNEOS database","abstract":"This paper investigates the likelihood that the CNEOS 2014-01-08 superbolide (CNEOS14) was caused by an interstellar object. This issue has remained controversial due to lack of information on the capabilities of the classified satellite sensors that recorded the fireball. We critically evaluate previous studies, specifically addressing the reliability of the CNEOS database and the associated measurement uncertainties. With proper statistical analysis of existing data and the addition of a relevant new event (the 2024 Iberian superbolide), we disprove some claims in previous work, such as: a) the existence of a purported correlation between CNEOS velocity errors and bolide speed; b) the presence of large velocity errors of 10-15 km/s in the CNEOS database; and c) the assertion that CNEOS14 is most likely a solar system object with a hyperbolic trajectory due to measurement errors. We present a quantitative estimate of the probability that CNEOS14 is interstellar. If its measurement errors are drawn from the same underlying distribution as the 18 calibrated events, then the probability that CNEOS14 is interstellar is 94.1%. This probability is lower than the 99.7% confidence (3-sigma) generally required to claim a scientific discovery. However, it is sufficiently high to be considered significant and, by far, the most likely explanation for the currently available empirical evidence.","sentences":["This paper investigates the likelihood that the CNEOS 2014-01-08 superbolide (CNEOS14) was caused by an interstellar object.","This issue has remained controversial due to lack of information on the capabilities of the classified satellite sensors that recorded the fireball.","We critically evaluate previous studies, specifically addressing the reliability of the CNEOS database and the associated measurement uncertainties.","With proper statistical analysis of existing data and the addition of a relevant new event (the 2024 Iberian superbolide), we disprove some claims in previous work, such as: a) the existence of a purported correlation between CNEOS velocity errors and bolide speed; b) the presence of large velocity errors of 10-15 km/s in the CNEOS database; and c) the assertion that CNEOS14 is most likely a solar system object with a hyperbolic trajectory due to measurement errors.","We present a quantitative estimate of the probability that CNEOS14 is interstellar.","If its measurement errors are drawn from the same underlying distribution as the 18 calibrated events, then the probability that CNEOS14 is interstellar is 94.1%.","This probability is lower than the 99.7% confidence (3-sigma) generally required to claim a scientific discovery.","However, it is sufficiently high to be considered significant and, by far, the most likely explanation for the currently available empirical evidence."],"url":"http://arxiv.org/abs/2405.17219v1","category":"astro-ph.EP"}
{"created":"2024-05-27 14:35:10","title":"Autoformalizing Euclidean Geometry","abstract":"Autoformalization involves automatically translating informal math into formal theorems and proofs that are machine-verifiable. Euclidean geometry provides an interesting and controllable domain for studying autoformalization. In this paper, we introduce a neuro-symbolic framework for autoformalizing Euclidean geometry, which combines domain knowledge, SMT solvers, and large language models (LLMs). One challenge in Euclidean geometry is that informal proofs rely on diagrams, leaving gaps in texts that are hard to formalize. To address this issue, we use theorem provers to fill in such diagrammatic information automatically, so that the LLM only needs to autoformalize the explicit textual steps, making it easier for the model. We also provide automatic semantic evaluation for autoformalized theorem statements. We construct LeanEuclid, an autoformalization benchmark consisting of problems from Euclid's Elements and the UniGeo dataset formalized in the Lean proof assistant. Experiments with GPT-4 and GPT-4V show the capability and limitations of state-of-the-art LLMs on autoformalizing geometry problems. The data and code are available at https://github.com/loganrjmurphy/LeanEuclid.","sentences":["Autoformalization involves automatically translating informal math into formal theorems and proofs that are machine-verifiable.","Euclidean geometry provides an interesting and controllable domain for studying autoformalization.","In this paper, we introduce a neuro-symbolic framework for autoformalizing Euclidean geometry, which combines domain knowledge, SMT solvers, and large language models (LLMs).","One challenge in Euclidean geometry is that informal proofs rely on diagrams, leaving gaps in texts that are hard to formalize.","To address this issue, we use theorem provers to fill in such diagrammatic information automatically, so that the LLM only needs to autoformalize the explicit textual steps, making it easier for the model.","We also provide automatic semantic evaluation for autoformalized theorem statements.","We construct LeanEuclid, an autoformalization benchmark consisting of problems from Euclid's Elements and the UniGeo dataset formalized in the Lean proof assistant.","Experiments with GPT-4 and GPT-4V show the capability and limitations of state-of-the-art LLMs on autoformalizing geometry problems.","The data and code are available at https://github.com/loganrjmurphy/LeanEuclid."],"url":"http://arxiv.org/abs/2405.17216v1","category":"cs.LG"}
{"created":"2024-05-27 14:34:27","title":"Highly inhomogeneous interactions between background climate and urban warming across typical local climate zones in heatwave and non-heatwave days","abstract":"Urban heat island (UHI) in conjunction with heatwave (HW) leads to exacerbation of thermal stress in urban areas. Prior research on UHI and HW has predominantly concentrated on examining the thermal conditions at the surface and near-surface, with few investigations extending to the radiative and dynamical interactions of UHI and HW, particularly with a focus on the inhomogeneities across local climate zones (LCZs). Here, we analyse the temperature disparity between HW and non-HW conditions across LCZs in the Sydney area by quantifying the contributions of individual radiative and dynamical processes using the coupled surface-atmosphere climate feedback-response analysis method (CFRAM). Three HW events in 2017, 2019, and 2020 are simulated using the Weather Research and Forecasting (WRF) model coupled with the Single-Layer Urban Canopy Model (SLUCM). The maximum temperature difference between HW and non-HW days may reach up to 10 K, with the increased net solar radiation during HWs being comparable to the typical level of anthropogenic heat flux in urban areas. It is also found that the reduction of clouds, the presence of vapor, and the increase of sensible heat contribute to the warming effect at different levels, with the contribution of clouds being the most dominant. Conversely, the generation of dry convection and the increase of latent heat flux lead to mitigating effects, with the latter being more dominant and capable of causing up to 10 K surface temperature difference between LCZ1 (compact high-rise) and LCZ9 (sparsely built). The differences in the contributions of climate feedback processes across different LCZs become more evident during more severe and humid HWs. These findings underscore the necessity of implementing local climate zone-tailored heat mitigation strategies.","sentences":["Urban heat island (UHI) in conjunction with heatwave (HW) leads to exacerbation of thermal stress in urban areas.","Prior research on UHI and HW has predominantly concentrated on examining the thermal conditions at the surface and near-surface, with few investigations extending to the radiative and dynamical interactions of UHI and HW, particularly with a focus on the inhomogeneities across local climate zones (LCZs).","Here, we analyse the temperature disparity between HW and non-HW conditions across LCZs in the Sydney area by quantifying the contributions of individual radiative and dynamical processes using the coupled surface-atmosphere climate feedback-response analysis method (CFRAM).","Three HW events in 2017, 2019, and 2020 are simulated using the Weather Research and Forecasting (WRF) model coupled with the Single-Layer Urban Canopy Model (SLUCM).","The maximum temperature difference between HW and non-HW days may reach up to 10 K, with the increased net solar radiation during HWs being comparable to the typical level of anthropogenic heat flux in urban areas.","It is also found that the reduction of clouds, the presence of vapor, and the increase of sensible heat contribute to the warming effect at different levels, with the contribution of clouds being the most dominant.","Conversely, the generation of dry convection and the increase of latent heat flux lead to mitigating effects, with the latter being more dominant and capable of causing up to 10 K surface temperature difference between LCZ1 (compact high-rise) and LCZ9 (sparsely built).","The differences in the contributions of climate feedback processes across different LCZs become more evident during more severe and humid HWs.","These findings underscore the necessity of implementing local climate zone-tailored heat mitigation strategies."],"url":"http://arxiv.org/abs/2405.17213v1","category":"physics.ao-ph"}
{"created":"2024-05-27 14:33:28","title":"A new parametrization of Hubble parameter and Hubble tension","abstract":"We present a new Hubble parameterization method and employ observational data from Hubble, Pantheon, and Baryon Acoustic Oscillations to constrain model parameters. The proposed method is thoroughly validated against these datasets, demonstrating a robust fit to the observational Hubble, Pantheon, and BAO data. The obtained best-fit values are $H_0 = 67.5^{+1.3}_{-1.6}$ $\\text{km s}^{-1} \\text{Mpc}^{-1}$, $\\Omega_{\\rm{m0}} = 0.2764\\pm{0.0094}$, and $\\alpha = 0.33\\pm{0.22}$, consistent with the Planck 2018 results, highlighting the existence of Hubble tension.","sentences":["We present a new Hubble parameterization method and employ observational data from Hubble, Pantheon, and Baryon Acoustic Oscillations to constrain model parameters.","The proposed method is thoroughly validated against these datasets, demonstrating a robust fit to the observational Hubble, Pantheon, and BAO data.","The obtained best-fit values are $H_0 = 67.5^{+1.3}_{-1.6}$ $\\text{km s}^{-1} \\text{Mpc}^{-1}$, $\\Omega_{\\rm{m0}} = 0.2764\\pm{0.0094}$, and $\\alpha = 0.33\\pm{0.22}$, consistent with the Planck 2018 results, highlighting the existence of Hubble tension."],"url":"http://arxiv.org/abs/2405.17212v1","category":"gr-qc"}
{"created":"2024-05-27 14:27:13","title":"Ultra-precise, sub-picometer tunable free spectral range in a parabolic microresonator induced by optical fiber bending","abstract":"Surface Nanoscale Axial Photonic (SNAP) microresonators are fabricated on silica optical fibers, leveraging silica's outstanding material and mechanical properties. These properties allow for precise control over the microresonator dimension, shape, and mode structure, a key feature for reconfigurable photonic circuits. Such circuits find applications in high-speed communications, optical computing, and optical frequency combs (OFC). However, consistently producing SNAP microresonators with equally spaced eigenmodes has remained challenging. In this study, we introduce a method to create a SNAP microresonator with a parabolic profile. We accomplish this by bending a silica optical fiber in a controlled manner using two linear stages. This approach achieves a uniform free spectral range (FSR) as narrow as 1 pm across more than 45 modes. We further demonstrate that the FSR of the SNAP microresonator can be continuously adjusted over a range nearly as wide as one FSR itself, specifically from 1.09 pm to 1.72 pm, with a precision of +/-0.01 pm and high repeatability. Given its compact size and tuning capability, this SNAP microresonator is highly promising for various applications, including the generation of tunable low-repetition-rate OFC and delay lines.","sentences":["Surface Nanoscale Axial Photonic (SNAP) microresonators are fabricated on silica optical fibers, leveraging silica's outstanding material and mechanical properties.","These properties allow for precise control over the microresonator dimension, shape, and mode structure, a key feature for reconfigurable photonic circuits.","Such circuits find applications in high-speed communications, optical computing, and optical frequency combs (OFC).","However, consistently producing SNAP microresonators with equally spaced eigenmodes has remained challenging.","In this study, we introduce a method to create a SNAP microresonator with a parabolic profile.","We accomplish this by bending a silica optical fiber in a controlled manner using two linear stages.","This approach achieves a uniform free spectral range (FSR) as narrow as 1 pm across more than 45 modes.","We further demonstrate that the FSR of the SNAP microresonator can be continuously adjusted over a range nearly as wide as one FSR itself, specifically from 1.09 pm to 1.72 pm, with a precision of +/-0.01 pm and high repeatability.","Given its compact size and tuning capability, this SNAP microresonator is highly promising for various applications, including the generation of tunable low-repetition-rate OFC and delay lines."],"url":"http://arxiv.org/abs/2405.17207v1","category":"physics.optics"}
{"created":"2024-05-27 14:24:47","title":"Efficient multi-prompt evaluation of LLMs","abstract":"Most popular benchmarks for comparing LLMs rely on a limited set of prompt templates, which may not fully capture the LLMs' abilities and can affect the reproducibility of results on leaderboards. Many recent works empirically verify prompt sensitivity and advocate for changes in LLM evaluation. In this paper, we consider the problem of estimating the performance distribution across many prompt variants instead of finding a single prompt to evaluate with. We introduce PromptEval, a method for estimating performance across a large set of prompts borrowing strength across prompts and examples to produce accurate estimates under practical evaluation budgets. The resulting distribution can be used to obtain performance quantiles to construct various robust performance metrics (e.g., top 95% quantile or median). We prove that PromptEval consistently estimates the performance distribution and demonstrate its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry. For example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations. Our code and data can be found at https://github.com/felipemaiapolo/prompt-eval.","sentences":["Most popular benchmarks for comparing LLMs rely on a limited set of prompt templates, which may not fully capture the LLMs' abilities and can affect the reproducibility of results on leaderboards.","Many recent works empirically verify prompt sensitivity and advocate for changes in LLM evaluation.","In this paper, we consider the problem of estimating the performance distribution across many prompt variants instead of finding a single prompt to evaluate with.","We introduce PromptEval, a method for estimating performance across a large set of prompts borrowing strength across prompts and examples to produce accurate estimates under practical evaluation budgets.","The resulting distribution can be used to obtain performance quantiles to construct various robust performance metrics (e.g., top 95% quantile or median).","We prove that PromptEval consistently estimates the performance distribution and demonstrate its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry.","For example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations.","Our code and data can be found at https://github.com/felipemaiapolo/prompt-eval."],"url":"http://arxiv.org/abs/2405.17202v1","category":"cs.CL"}
{"created":"2024-05-27 14:21:12","title":"A Mathematical Theory of Integer Quantum Hall Effectin Photonics","abstract":"This paper investigates interface modes in a square lattice of photonic crystal composed of gyromagnetic particles with $C_{4v}$ point group symmetry. The study shows that Dirac or linear degenerate points cannot occur at the three high symmetry points in the Brillouin zone where two Bloch bands touch. Instead, a touch point at the M-point has a quadratic degeneracy in the generic case. It is further proved that when a magnetic field is applied to the two sides of an interface in opposite directions, two interface modes that are supported along that interface can be bifurcated from the quadratic degenerate point. The results provide a mathematical foundation for the first experiment realization of the integer quantum Hall effect in the context of photonics.","sentences":["This paper investigates interface modes in a square lattice of photonic crystal composed of gyromagnetic particles with $C_{4v}$ point group symmetry.","The study shows that Dirac or linear degenerate points cannot occur at the three high symmetry points in the Brillouin zone where two Bloch bands touch.","Instead, a touch point at the M-point has a quadratic degeneracy in the generic case.","It is further proved that when a magnetic field is applied to the two sides of an interface in opposite directions, two interface modes that are supported along that interface can be bifurcated from the quadratic degenerate point.","The results provide a mathematical foundation for the first experiment realization of the integer quantum Hall effect in the context of photonics."],"url":"http://arxiv.org/abs/2405.17200v1","category":"math-ph"}
{"created":"2024-05-27 14:19:53","title":"Convex Relaxation for Solving Large-Margin Classifiers in Hyperbolic Space","abstract":"Hyperbolic spaces have increasingly been recognized for their outstanding performance in handling data with inherent hierarchical structures compared to their Euclidean counterparts. However, learning in hyperbolic spaces poses significant challenges. In particular, extending support vector machines to hyperbolic spaces is in general a constrained non-convex optimization problem. Previous and popular attempts to solve hyperbolic SVMs, primarily using projected gradient descent, are generally sensitive to hyperparameters and initializations, often leading to suboptimal solutions. In this work, by first rewriting the problem into a polynomial optimization, we apply semidefinite relaxation and sparse moment-sum-of-squares relaxation to effectively approximate the optima. From extensive empirical experiments, these methods are shown to perform better than the projected gradient descent approach.","sentences":["Hyperbolic spaces have increasingly been recognized for their outstanding performance in handling data with inherent hierarchical structures compared to their Euclidean counterparts.","However, learning in hyperbolic spaces poses significant challenges.","In particular, extending support vector machines to hyperbolic spaces is in general a constrained non-convex optimization problem.","Previous and popular attempts to solve hyperbolic SVMs, primarily using projected gradient descent, are generally sensitive to hyperparameters and initializations, often leading to suboptimal solutions.","In this work, by first rewriting the problem into a polynomial optimization, we apply semidefinite relaxation and sparse moment-sum-of-squares relaxation to effectively approximate the optima.","From extensive empirical experiments, these methods are shown to perform better than the projected gradient descent approach."],"url":"http://arxiv.org/abs/2405.17198v1","category":"cs.LG"}
{"created":"2024-05-27 14:17:33","title":"Quantum Parity Detectors: a qubit based particle detection scheme with meV thresholds for rare-event searches","abstract":"The next generation of rare-event searches, such as those aimed at determining the nature of particle dark matter or in measuring fundamental neutrino properties, will benefit from particle detectors with thresholds at the meV scale, 100-1000x lower than currently available. Quantum parity detectors (QPDs) are a novel class of proposed quantum devices that use the tremendous sensitivity of superconducting qubits to quasiparticle tunneling events as their detection concept. As envisioned, phonons generated by particle interactions within a crystalline substrate cause an eventual quasiparticle cascade within a surface patterned superconducting qubit element. This process alters the fundamental charge parity of the device in a binary manner, which can be used to deduce the initial properties of the energy deposition. We lay out the operating mechanism, noise sources, and expected sensitivity of QPDs based on a spectrum of charge-qubit types and readout mechanisms and detail an R&D pathway to demonstrating sensitivity to sub-eV energy deposits.","sentences":["The next generation of rare-event searches, such as those aimed at determining the nature of particle dark matter or in measuring fundamental neutrino properties, will benefit from particle detectors with thresholds at the meV scale, 100-1000x lower than currently available.","Quantum parity detectors (QPDs) are a novel class of proposed quantum devices that use the tremendous sensitivity of superconducting qubits to quasiparticle tunneling events as their detection concept.","As envisioned, phonons generated by particle interactions within a crystalline substrate cause an eventual quasiparticle cascade within a surface patterned superconducting qubit element.","This process alters the fundamental charge parity of the device in a binary manner, which can be used to deduce the initial properties of the energy deposition.","We lay out the operating mechanism, noise sources, and expected sensitivity of QPDs based on a spectrum of charge-qubit types and readout mechanisms and detail an R&D pathway to demonstrating sensitivity to sub-eV energy deposits."],"url":"http://arxiv.org/abs/2405.17192v1","category":"physics.ins-det"}
{"created":"2024-05-27 14:15:52","title":"MCGAN: Enhancing GAN Training with Regression-Based Generator Loss","abstract":"Generative adversarial networks (GANs) have emerged as a powerful tool for generating high-fidelity data. However, the main bottleneck of existing approaches is the lack of supervision on the generator training, which often results in undamped oscillation and unsatisfactory performance. To address this issue, we propose an algorithm called Monte Carlo GAN (MCGAN). This approach, utilizing an innovative generative loss function, termly the regression loss, reformulates the generator training as a regression task and enables the generator training by minimizing the mean squared error between the discriminator's output of real data and the expected discriminator of fake data. We demonstrate the desirable analytic properties of the regression loss, including discriminability and optimality, and show that our method requires a weaker condition on the discriminator for effective generator training. These properties justify the strength of this approach to improve the training stability while retaining the optimality of GAN by leveraging strong supervision of the regression loss. Numerical results on CIFAR-10 and CIFAR-100 datasets demonstrate that the proposed MCGAN significantly and consistently improves the existing state-of-the-art GAN models in terms of quality, accuracy, training stability, and learned latent space. Furthermore, the proposed algorithm exhibits great flexibility for integrating with a variety of backbone models to generate spatial images, temporal time-series, and spatio-temporal video data.","sentences":["Generative adversarial networks (GANs) have emerged as a powerful tool for generating high-fidelity data.","However, the main bottleneck of existing approaches is the lack of supervision on the generator training, which often results in undamped oscillation and unsatisfactory performance.","To address this issue, we propose an algorithm called Monte Carlo GAN (MCGAN).","This approach, utilizing an innovative generative loss function, termly the regression loss, reformulates the generator training as a regression task and enables the generator training by minimizing the mean squared error between the discriminator's output of real data and the expected discriminator of fake data.","We demonstrate the desirable analytic properties of the regression loss, including discriminability and optimality, and show that our method requires a weaker condition on the discriminator for effective generator training.","These properties justify the strength of this approach to improve the training stability while retaining the optimality of GAN by leveraging strong supervision of the regression loss.","Numerical results on CIFAR-10 and CIFAR-100 datasets demonstrate that the proposed MCGAN significantly and consistently improves the existing state-of-the-art GAN models in terms of quality, accuracy, training stability, and learned latent space.","Furthermore, the proposed algorithm exhibits great flexibility for integrating with a variety of backbone models to generate spatial images, temporal time-series, and spatio-temporal video data."],"url":"http://arxiv.org/abs/2405.17191v1","category":"cs.CV"}
{"created":"2024-05-27 14:14:07","title":"SoK: Leveraging Transformers for Malware Analysis","abstract":"The introduction of transformers has been an important breakthrough for AI research and application as transformers are the foundation behind Generative AI. A promising application domain for transformers is cybersecurity, in particular the malware domain analysis. The reason is the flexibility of the transformer models in handling long sequential features and understanding contextual relationships. However, as the use of transformers for malware analysis is still in the infancy stage, it is critical to evaluate, systematize, and contextualize existing literature to foster future research. This Systematization of Knowledge (SoK) paper aims to provide a comprehensive analysis of transformer-based approaches designed for malware analysis. Based on our systematic analysis of existing knowledge, we structure and propose taxonomies based on: (a) how different transformers are adapted, organized, and modified across various use cases; and (b) how diverse feature types and their representation capabilities are reflected. We also provide an inventory of datasets used to explore multiple research avenues in the use of transformers for malware analysis and discuss open challenges with future research directions. We believe that this SoK paper will assist the research community in gaining detailed insights from existing work and will serve as a foundational resource for implementing novel research using transformers for malware analysis.","sentences":["The introduction of transformers has been an important breakthrough for AI research and application as transformers are the foundation behind Generative AI.","A promising application domain for transformers is cybersecurity, in particular the malware domain analysis.","The reason is the flexibility of the transformer models in handling long sequential features and understanding contextual relationships.","However, as the use of transformers for malware analysis is still in the infancy stage, it is critical to evaluate, systematize, and contextualize existing literature to foster future research.","This Systematization of Knowledge (SoK) paper aims to provide a comprehensive analysis of transformer-based approaches designed for malware analysis.","Based on our systematic analysis of existing knowledge, we structure and propose taxonomies based on: (a) how different transformers are adapted, organized, and modified across various use cases; and (b) how diverse feature types and their representation capabilities are reflected.","We also provide an inventory of datasets used to explore multiple research avenues in the use of transformers for malware analysis and discuss open challenges with future research directions.","We believe that this SoK paper will assist the research community in gaining detailed insights from existing work and will serve as a foundational resource for implementing novel research using transformers for malware analysis."],"url":"http://arxiv.org/abs/2405.17190v1","category":"cs.CR"}
{"created":"2024-05-27 14:11:17","title":"Memorize What Matters: Emergent Scene Decomposition from Multitraverse","abstract":"Humans naturally retain memories of permanent elements, while ephemeral moments often slip through the cracks of memory. This selective retention is crucial for robotic perception, localization, and mapping. To endow robots with this capability, we introduce 3D Gaussian Mapping (3DGM), a self-supervised, camera-only offline mapping framework grounded in 3D Gaussian Splatting. 3DGM converts multitraverse RGB videos from the same region into a Gaussian-based environmental map while concurrently performing 2D ephemeral object segmentation. Our key observation is that the environment remains consistent across traversals, while objects frequently change. This allows us to exploit self-supervision from repeated traversals to achieve environment-object decomposition. More specifically, 3DGM formulates multitraverse environmental mapping as a robust differentiable rendering problem, treating pixels of the environment and objects as inliers and outliers, respectively. Using robust feature distillation, feature residuals mining, and robust optimization, 3DGM jointly performs 3D mapping and 2D segmentation without human intervention. We build the Mapverse benchmark, sourced from the Ithaca365 and nuPlan datasets, to evaluate our method in unsupervised 2D segmentation, 3D reconstruction, and neural rendering. Extensive results verify the effectiveness and potential of our method for self-driving and robotics.","sentences":["Humans naturally retain memories of permanent elements, while ephemeral moments often slip through the cracks of memory.","This selective retention is crucial for robotic perception, localization, and mapping.","To endow robots with this capability, we introduce 3D Gaussian Mapping (3DGM), a self-supervised, camera-only offline mapping framework grounded in 3D Gaussian Splatting.","3DGM converts multitraverse RGB videos from the same region into a Gaussian-based environmental map while concurrently performing 2D ephemeral object segmentation.","Our key observation is that the environment remains consistent across traversals, while objects frequently change.","This allows us to exploit self-supervision from repeated traversals to achieve environment-object decomposition.","More specifically, 3DGM formulates multitraverse environmental mapping as a robust differentiable rendering problem, treating pixels of the environment and objects as inliers and outliers, respectively.","Using robust feature distillation, feature residuals mining, and robust optimization, 3DGM jointly performs 3D mapping and 2D segmentation without human intervention.","We build the Mapverse benchmark, sourced from the Ithaca365 and nuPlan datasets, to evaluate our method in unsupervised 2D segmentation, 3D reconstruction, and neural rendering.","Extensive results verify the effectiveness and potential of our method for self-driving and robotics."],"url":"http://arxiv.org/abs/2405.17187v1","category":"cs.CV"}
{"created":"2024-05-27 14:06:39","title":"The mean solar butterfly diagram and poloidal field generation rate at the surface of the Sun","abstract":"The difference between individual solar cycles in the magnetic butterfly diagram can mostly be ascribed to the stochasticity of the emergence process. We aim to obtain the expectation value of the butterfly diagram from observations of four cycles. This allows us to further determine the generation rate of the surface radial magnetic field. We use data from Wilcox Solar Observatory to generate time-latitude diagrams spanning cycles 21 to 24 of the surface radial and toroidal magnetic fields, symmetrize them across the equator and cycle-average them. From the mean butterfly diagram and surface toroidal field we then infer the mean poloidal field generation rate at the surface of the Sun. The averaging procedure removes realization noise from individual cycles. The amount of emerging flux required to account for the evolution of the surface radial field is found to match that provided by the observed surface toroidal field and Joy's law. Cycle-averaging butterfly diagrams removes realization noise and artefacts due to imperfect scale separation, and corresponds to an ensemble average that can be interpreted in the mean-field framework. The result can then be directly compared to $\\alpha\\Omega$-type dynamo models. The Babcock-Leighton $\\alpha$-effect is consistent with observations, a result that can be appreciated only if the observational data is averaged in some way.","sentences":["The difference between individual solar cycles in the magnetic butterfly diagram can mostly be ascribed to the stochasticity of the emergence process.","We aim to obtain the expectation value of the butterfly diagram from observations of four cycles.","This allows us to further determine the generation rate of the surface radial magnetic field.","We use data from Wilcox Solar Observatory to generate time-latitude diagrams spanning cycles 21 to 24 of the surface radial and toroidal magnetic fields, symmetrize them across the equator and cycle-average them.","From the mean butterfly diagram and surface toroidal field we then infer the mean poloidal field generation rate at the surface of the Sun.","The averaging procedure removes realization noise from individual cycles.","The amount of emerging flux required to account for the evolution of the surface radial field is found to match that provided by the observed surface toroidal field and Joy's law.","Cycle-averaging butterfly diagrams removes realization noise and artefacts due to imperfect scale separation, and corresponds to an ensemble average that can be interpreted in the mean-field framework.","The result can then be directly compared to $\\alpha\\Omega$-type dynamo models.","The Babcock-Leighton $\\alpha$-effect is consistent with observations, a result that can be appreciated only if the observational data is averaged in some way."],"url":"http://arxiv.org/abs/2405.17185v1","category":"astro-ph.SR"}
{"created":"2024-05-27 14:05:51","title":"A Pioneering Roadmap for ML-Driven Algorithmic Advancements in Electrical Networks","abstract":"To advance control, operation and planning tools of electrical networks with ML is not straightforward. 110 experts were surveyed showing where and how ML algorithmis could advance. This paper assesses this survey and research environment. Then it develops an innovation roadmap that helps align our research community towards a goal-oriented realisation of the opportunities that AI upholds. This paper finds that the R\\&D environment of system operators (and the surrounding research ecosystem) needs adaptation to enable faster developments with AI while maintaining high testing quality and safety. This roadmap may interest research centre managers in system operators, academics, and labs dedicated to advancing the next generation of tooling for electrical networks.","sentences":["To advance control, operation and planning tools of electrical networks with ML is not straightforward.","110 experts were surveyed showing where and how ML algorithmis could advance.","This paper assesses this survey and research environment.","Then it develops an innovation roadmap that helps align our research community towards a goal-oriented realisation of the opportunities that AI upholds.","This paper finds that the R\\&D environment of system operators (and the surrounding research ecosystem) needs adaptation to enable faster developments with AI while maintaining high testing quality and safety.","This roadmap may interest research centre managers in system operators, academics, and labs dedicated to advancing the next generation of tooling for electrical networks."],"url":"http://arxiv.org/abs/2405.17184v1","category":"eess.SY"}
{"created":"2024-05-27 14:05:42","title":"Exact Thermodynamics For Weakly Interacting Normal-Phase Quantum Gases: Equations of State For All Partial Waves","abstract":"While the thermodynamics for bosonic systems with weak $s$-wave interactions has been known for decades, a general and systematic extension to higher partial-waves has not yet been reported. We provide closed-form expressions for the equations of state for weakly-interacting systems with arbitrary partial-waves in the normal phase. Thermodynamics, including contact, loss rate, and compressibility, are derived over the entire temperature regime. Our results offer an improved thermometer for ultracold atoms and molecules with weak high-partial wave interactions.","sentences":["While the thermodynamics for bosonic systems with weak $s$-wave interactions has been known for decades, a general and systematic extension to higher partial-waves has not yet been reported.","We provide closed-form expressions for the equations of state for weakly-interacting systems with arbitrary partial-waves in the normal phase.","Thermodynamics, including contact, loss rate, and compressibility, are derived over the entire temperature regime.","Our results offer an improved thermometer for ultracold atoms and molecules with weak high-partial wave interactions."],"url":"http://arxiv.org/abs/2405.17183v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-27 14:03:28","title":"Exploring the Performance of Continuous-Time Dynamic Link Prediction Algorithms","abstract":"Dynamic Link Prediction (DLP) addresses the prediction of future links in evolving networks. However, accurately portraying the performance of DLP algorithms poses challenges that might impede progress in the field. Importantly, common evaluation pipelines usually calculate ranking or binary classification metrics, where the scores of observed interactions (positives) are compared with those of randomly generated ones (negatives). However, a single metric is not sufficient to fully capture the differences between DLP algorithms, and is prone to overly optimistic performance evaluation. Instead, an in-depth evaluation should reflect performance variations across different nodes, edges, and time segments. In this work, we contribute tools to perform such a comprehensive evaluation. (1) We propose Birth-Death diagrams, a simple but powerful visualization technique that illustrates the effect of time-based train-test splitting on the difficulty of DLP on a given dataset. (2) We describe an exhaustive taxonomy of negative sampling methods that can be used at evaluation time. (3) We carry out an empirical study of the effect of the different negative sampling strategies. Our comparison between heuristics and state-of-the-art memory-based methods on various real-world datasets confirms a strong effect of using different negative sampling strategies on the test Area Under the Curve (AUC). Moreover, we conduct a visual exploration of the prediction, with additional insights on which different types of errors are prominent over time.","sentences":["Dynamic Link Prediction (DLP) addresses the prediction of future links in evolving networks.","However, accurately portraying the performance of DLP algorithms poses challenges that might impede progress in the field.","Importantly, common evaluation pipelines usually calculate ranking or binary classification metrics, where the scores of observed interactions (positives) are compared with those of randomly generated ones (negatives).","However, a single metric is not sufficient to fully capture the differences between DLP algorithms, and is prone to overly optimistic performance evaluation.","Instead, an in-depth evaluation should reflect performance variations across different nodes, edges, and time segments.","In this work, we contribute tools to perform such a comprehensive evaluation.","(1) We propose Birth-Death diagrams, a simple but powerful visualization technique that illustrates the effect of time-based train-test splitting on the difficulty of DLP on a given dataset.","(2) We describe an exhaustive taxonomy of negative sampling methods that can be used at evaluation time.","(3) We carry out an empirical study of the effect of the different negative sampling strategies.","Our comparison between heuristics and state-of-the-art memory-based methods on various real-world datasets confirms a strong effect of using different negative sampling strategies on the test Area Under the Curve (AUC).","Moreover, we conduct a visual exploration of the prediction, with additional insights on which different types of errors are prominent over time."],"url":"http://arxiv.org/abs/2405.17182v1","category":"cs.SI"}
{"created":"2024-05-27 14:01:42","title":"Spectral regularization for adversarially-robust representation learning","abstract":"The vulnerability of neural network classifiers to adversarial attacks is a major obstacle to their deployment in safety-critical applications. Regularization of network parameters during training can be used to improve adversarial robustness and generalization performance. Usually, the network is regularized end-to-end, with parameters at all layers affected by regularization. However, in settings where learning representations is key, such as self-supervised learning (SSL), layers after the feature representation will be discarded when performing inference. For these models, regularizing up to the feature space is more suitable. To this end, we propose a new spectral regularizer for representation learning that encourages black-box adversarial robustness in downstream classification tasks. In supervised classification settings, we show empirically that this method is more effective in boosting test accuracy and robustness than previously-proposed methods that regularize all layers of the network. We then show that this method improves the adversarial robustness of classifiers using representations learned with self-supervised training or transferred from another classification task. In all, our work begins to unveil how representational structure affects adversarial robustness.","sentences":["The vulnerability of neural network classifiers to adversarial attacks is a major obstacle to their deployment in safety-critical applications.","Regularization of network parameters during training can be used to improve adversarial robustness and generalization performance.","Usually, the network is regularized end-to-end, with parameters at all layers affected by regularization.","However, in settings where learning representations is key, such as self-supervised learning (SSL), layers after the feature representation will be discarded when performing inference.","For these models, regularizing up to the feature space is more suitable.","To this end, we propose a new spectral regularizer for representation learning that encourages black-box adversarial robustness in downstream classification tasks.","In supervised classification settings, we show empirically that this method is more effective in boosting test accuracy and robustness than previously-proposed methods that regularize all layers of the network.","We then show that this method improves the adversarial robustness of classifiers using representations learned with self-supervised training or transferred from another classification task.","In all, our work begins to unveil how representational structure affects adversarial robustness."],"url":"http://arxiv.org/abs/2405.17181v1","category":"cs.LG"}
{"created":"2024-05-27 13:59:16","title":"Statistical Mechanism Design: Robust Pricing, Estimation, and Inference","abstract":"This paper tackles challenges in pricing and revenue projections due to consumer uncertainty. We propose a novel data-based approach for firms facing unknown consumer type distributions. Unlike existing methods, we assume firms only observe a finite sample of consumers' types. We introduce \\emph{empirically optimal mechanisms}, a simple and intuitive class of sample-based mechanisms with strong finite-sample revenue guarantees. Furthermore, we leverage our results to develop a toolkit for statistical inference on profits. Our approach allows to reliably estimate the profits associated for any particular mechanism, to construct confidence intervals, and to, more generally, conduct valid hypothesis testing.","sentences":["This paper tackles challenges in pricing and revenue projections due to consumer uncertainty.","We propose a novel data-based approach for firms facing unknown consumer type distributions.","Unlike existing methods, we assume firms only observe a finite sample of consumers' types.","We introduce \\emph{empirically optimal mechanisms}, a simple and intuitive class of sample-based mechanisms with strong finite-sample revenue guarantees.","Furthermore, we leverage our results to develop a toolkit for statistical inference on profits.","Our approach allows to reliably estimate the profits associated for any particular mechanism, to construct confidence intervals, and to, more generally, conduct valid hypothesis testing."],"url":"http://arxiv.org/abs/2405.17178v1","category":"econ.TH"}
{"created":"2024-05-27 13:57:01","title":"Unorientable topological gravity and orthogonal random matrix universality","abstract":"The duality of Jackiw-Teitelboim (JT) gravity and a double scaled matrix integral has led to studies of the canonical spectral form factor (SFF) in the so called $\\tau-$scaled limit of large times, $t \\to \\infty$, and fixed temperature in order to demonstrate agreement with universal random matrix theory (RMT). Though this has been established for the unitary case, extensions to other symmetry classes requires the inclusion of unorientable manifolds in the sum over geometries, necessary to address time reversal invariance, and regularization of the corresponding prime geometrical objects, the Weil-Petersson (WP) volumes. We report here how universal signatures of quantum chaos, witnessed by the fidelity to the Gaussian orthogonal ensemble, emerge for the low-energy limit of unorientable JT gravity, i.e. the Airy model/topological gravity. To this end, we implement the loop equations for the corresponding dual (double-scaled) matrix model and find the generic form of the Airy WP volumes, supported by calculations using unorientable Kontsevich graphs. In an apparent violation of the gravity/chaos duality, the $\\tau-$scaled SFF on the gravity side acquires both logarithmic and power law contributions in $t$, not manifestly present on the RMT side. We show the expressions can be made to agree by means of bootstrapping-like relations hidden in the asymptotic expansions of generalized hypergeometric functions. Thus, we are able to establish strong evidence of the quantum chaotic nature of unorientable topological gravity.","sentences":["The duality of Jackiw-Teitelboim (JT) gravity and a double scaled matrix integral has led to studies of the canonical spectral form factor (SFF) in the so called $\\tau-$scaled limit of large times, $t \\to \\infty$, and fixed temperature in order to demonstrate agreement with universal random matrix theory (RMT).","Though this has been established for the unitary case, extensions to other symmetry classes requires the inclusion of unorientable manifolds in the sum over geometries, necessary to address time reversal invariance, and regularization of the corresponding prime geometrical objects, the Weil-Petersson (WP) volumes.","We report here how universal signatures of quantum chaos, witnessed by the fidelity to the Gaussian orthogonal ensemble, emerge for the low-energy limit of unorientable JT gravity, i.e. the Airy model/topological gravity.","To this end, we implement the loop equations for the corresponding dual (double-scaled) matrix model and find the generic form of the Airy WP volumes, supported by calculations using unorientable Kontsevich graphs.","In an apparent violation of the gravity/chaos duality, the $\\tau-$scaled SFF on the gravity side acquires both logarithmic and power law contributions in $t$, not manifestly present on the RMT side.","We show the expressions can be made to agree by means of bootstrapping-like relations hidden in the asymptotic expansions of generalized hypergeometric functions.","Thus, we are able to establish strong evidence of the quantum chaotic nature of unorientable topological gravity."],"url":"http://arxiv.org/abs/2405.17177v1","category":"hep-th"}
{"created":"2024-05-27 13:55:08","title":"DreamMat: High-quality PBR Material Generation with Geometry- and Light-aware Diffusion Models","abstract":"2D diffusion model, which often contains unwanted baked-in shading effects and results in unrealistic rendering effects in the downstream applications. Generating Physically Based Rendering (PBR) materials instead of just RGB textures would be a promising solution. However, directly distilling the PBR material parameters from 2D diffusion models still suffers from incorrect material decomposition, such as baked-in shading effects in albedo. We introduce DreamMat, an innovative approach to resolve the aforementioned problem, to generate high-quality PBR materials from text descriptions. We find out that the main reason for the incorrect material distillation is that large-scale 2D diffusion models are only trained to generate final shading colors, resulting in insufficient constraints on material decomposition during distillation. To tackle this problem, we first finetune a new light-aware 2D diffusion model to condition on a given lighting environment and generate the shading results on this specific lighting condition. Then, by applying the same environment lights in the material distillation, DreamMat can generate high-quality PBR materials that are not only consistent with the given geometry but also free from any baked-in shading effects in albedo. Extensive experiments demonstrate that the materials produced through our methods exhibit greater visual appeal to users and achieve significantly superior rendering quality compared to baseline methods, which are preferable for downstream tasks such as game and film production.","sentences":["2D diffusion model, which often contains unwanted baked-in shading effects and results in unrealistic rendering effects in the downstream applications.","Generating Physically Based Rendering (PBR) materials instead of just RGB textures would be a promising solution.","However, directly distilling the PBR material parameters from 2D diffusion models still suffers from incorrect material decomposition, such as baked-in shading effects in albedo.","We introduce DreamMat, an innovative approach to resolve the aforementioned problem, to generate high-quality PBR materials from text descriptions.","We find out that the main reason for the incorrect material distillation is that large-scale 2D diffusion models are only trained to generate final shading colors, resulting in insufficient constraints on material decomposition during distillation.","To tackle this problem, we first finetune a new light-aware 2D diffusion model to condition on a given lighting environment and generate the shading results on this specific lighting condition.","Then, by applying the same environment lights in the material distillation, DreamMat can generate high-quality PBR materials that are not only consistent with the given geometry but also free from any baked-in shading effects in albedo.","Extensive experiments demonstrate that the materials produced through our methods exhibit greater visual appeal to users and achieve significantly superior rendering quality compared to baseline methods, which are preferable for downstream tasks such as game and film production."],"url":"http://arxiv.org/abs/2405.17176v1","category":"cs.GR"}
{"created":"2024-05-27 13:51:21","title":"Partitioning complete geometric graphs into plane subgraphs","abstract":"A \\emph{complete geometric graph} consists of a set $P$ of $n$ points in the plane, in general position, and all segments (edges) connecting them. It is a well known question of Bose, Hurtado, Rivera-Campo, and Wood, whether there exists a positive constant $c<1$, such that every complete geometric graph on $n$ points can be partitioned into at most $cn$ plane graphs (that is, noncrossing subgraphs). We answer this question in the affirmative in the special case where the underlying point set $P$ is \\emph{dense}, which means that the ratio between the maximum and the minimum distances in $P$ is of the order of $\\Theta(\\sqrt{n})$.","sentences":["A \\emph{complete geometric graph} consists of a set $P$ of $n$ points in the plane, in general position, and all segments (edges) connecting them.","It is a well known question of Bose, Hurtado, Rivera-Campo, and Wood, whether there exists a positive constant $c<1$, such that every complete geometric graph on $n$ points can be partitioned into at most $cn$ plane graphs (that is, noncrossing subgraphs).","We answer this question in the affirmative in the special case where the underlying point set $P$ is \\emph{dense}, which means that the ratio between the maximum and the minimum distances in $P$ is of the order of $\\Theta(\\sqrt{n})$."],"url":"http://arxiv.org/abs/2405.17172v1","category":"math.CO"}
{"created":"2024-05-27 13:47:52","title":"Janus and RG-flow interfaces from matter-coupled F(4) gauged supergravity","abstract":"We study supersymmetric Janus solutions from matter-coupled $F(4)$ gauged supergravity coupled to three vector multiplets and $SO(4)\\sim SO(3)\\times SO(3)$ gauge group. There are two supersymmetric $AdS_6$ vacua preserving all supersymmetries with $SO(3)\\times SO(3)$ and $SO(3)_{\\textrm{diag}}$ symmetries dual to $N=2$ SCFTs in five dimensions. We consider a truncation to $SO(2)_{\\textrm{diag}}\\subset SO(3)_{\\textrm{diag}}$ singlet scalars and find a number of new supersymmetric Janus solutions preserving eight supercharges. These solutions holographcally describe conformal interfaces within $N=2$ five-dimensional SCFTs involving deformations by source terms and vacuum expectation values of relevant and irrelevant operators. Apart from the Janus solutions interpolating between $SO(3)\\times SO(3)$ $AdS_6$ vacua, some of the solutions have $SO(3)_{\\textrm{diag}}$ $AdS_6$ vacua generated by holographic RG flows from the $SO(3)\\times SO(3)$ phases on both sides. We also provide an evidence for solutions describing RG-flow interfaces with $SO(3)\\times SO(3)$ $AdS_6$ vacuum on one side and $SO(3)_{\\textrm{diag}}$ $AdS_6$ vacuum on the other side. The solutions also provide first examples of Janus solutions involving more than one $AdS_6$ vacuum in six-dimensional gauged supergravity.","sentences":["We study supersymmetric Janus solutions from matter-coupled $F(4)$ gauged supergravity coupled to three vector multiplets and $SO(4)\\sim SO(3)\\times SO(3)$ gauge group.","There are two supersymmetric $AdS_6$ vacua preserving all supersymmetries with $SO(3)\\times SO(3)$ and $SO(3)_{\\textrm{diag}}$ symmetries dual to $N=2$ SCFTs in five dimensions.","We consider a truncation to $SO(2)_{\\textrm{diag}}\\subset SO(3)_{\\textrm{diag}}$ singlet scalars and find a number of new supersymmetric Janus solutions preserving eight supercharges.","These solutions holographcally describe conformal interfaces within $N=2$ five-dimensional SCFTs involving deformations by source terms and vacuum expectation values of relevant and irrelevant operators.","Apart from the Janus solutions interpolating between $SO(3)\\times SO(3)$ $AdS_6$ vacua, some of the solutions have $SO(3)_{\\textrm{diag}}$ $AdS_6$ vacua generated by holographic RG flows from the $SO(3)\\times SO(3)$ phases on both sides.","We also provide an evidence for solutions describing RG-flow interfaces with $SO(3)\\times SO(3)$ $AdS_6$ vacuum on one side and $SO(3)_{\\textrm{diag}}$ $AdS_6$ vacuum on the other side.","The solutions also provide first examples of Janus solutions involving more than one $AdS_6$ vacuum in six-dimensional gauged supergravity."],"url":"http://arxiv.org/abs/2405.17169v1","category":"hep-th"}
{"created":"2024-05-27 13:47:26","title":"Double-layer Thin-film LiNbO3 Longitudinally Excited Shear Wave Resonators with Ultra-large Electromechanical Coupling Coefficient and Spurious-Free Performance","abstract":"This work proposes a double-layer thin-film lithium niobate (LiNbO3) longitudinally excited shear wave resonator with a theoretical electromechanical coupling coefficient exceeding 60%, RaR close to 28%, and no spurious modes. This ultra-large electromechanical coupling coefficient, which is close to the upper limit of LiNbO3, is much larger than all microwave acoustic resonators reported so far. Based on X-cut thin-film LiNbO3, when the film thickness is in the order of hundreds of nanometers, the frequency of the fundamental mode of the resonator can cover 1GHz to10GHz. The resonator design is convenient and flexible. The resonant frequency can be modulated monotonically by changing either the electrode or the thickness of the thin-film LiNbO3 without introducing additional spurious modes. This ideal resonator architecture is also applicable to LiTaO3. With the development of the new generation of mobile communications, this resonator is expected to become a key solution for future high-performance, ultra-wide-bandwidth acoustic filters.","sentences":["This work proposes a double-layer thin-film lithium niobate (LiNbO3) longitudinally excited shear wave resonator with a theoretical electromechanical coupling coefficient exceeding 60%, RaR close to 28%, and no spurious modes.","This ultra-large electromechanical coupling coefficient, which is close to the upper limit of LiNbO3, is much larger than all microwave acoustic resonators reported so far.","Based on X-cut thin-film LiNbO3, when the film thickness is in the order of hundreds of nanometers, the frequency of the fundamental mode of the resonator can cover 1GHz to10GHz.","The resonator design is convenient and flexible.","The resonant frequency can be modulated monotonically by changing either the electrode or the thickness of the thin-film LiNbO3 without introducing additional spurious modes.","This ideal resonator architecture is also applicable to LiTaO3.","With the development of the new generation of mobile communications, this resonator is expected to become a key solution for future high-performance, ultra-wide-bandwidth acoustic filters."],"url":"http://arxiv.org/abs/2405.17168v1","category":"physics.app-ph"}
{"created":"2024-05-27 13:39:57","title":"Statistical conformal Killing Vector Fields for FLRW Space-Time","abstract":"The classification of conformal Killing vector fields for FLRW space-time from Riemannian point of view was done by Maartens-Maharaj in \\cite{Maartens1986}. In this paper, we introduce conformal Killing vector fields from a new point of view for the FLRW space-time. In particular, we consider three cases for the conformal factor. Then, it is shown that there exist nine conformal vector fields on FLRW in total, such that six of them are Killing and the rest being non-Killing conformal vector fields. Consequently, by recalling the concept of statistical conformal Killing vector fields introduced in \\cite{SP}, we classify statistical structures with repsect to which these vector fields are conformal Killing. We also obtain the form of affine connections that feature a vanishing Lie derivative with respect to these conformal Killing vector fields. Imposing the torsion-free and the Codazzi conditions on these connections, we study statistical structures on FLRW. Finally, for torsionful connections we study the vanishing of the Lie derivative of the torsion tensor with respect to these conformal Killing vector fields and derive the conditions under which this is valid.","sentences":["The classification of conformal Killing vector fields for FLRW space-time from Riemannian point of view was done by Maartens-Maharaj in \\cite{Maartens1986}.","In this paper, we introduce conformal Killing vector fields from a new point of view for the FLRW space-time.","In particular, we consider three cases for the conformal factor.","Then, it is shown that there exist nine conformal vector fields on FLRW in total, such that six of them are Killing and the rest being non-Killing conformal vector fields.","Consequently, by recalling the concept of statistical conformal Killing vector fields introduced in \\cite{SP}, we classify statistical structures with repsect to which these vector fields are conformal Killing.","We also obtain the form of affine connections that feature a vanishing Lie derivative with respect to these conformal Killing vector fields.","Imposing the torsion-free and the Codazzi conditions on these connections, we study statistical structures on FLRW.","Finally, for torsionful connections we study the vanishing of the Lie derivative of the torsion tensor with respect to these conformal Killing vector fields and derive the conditions under which this is valid."],"url":"http://arxiv.org/abs/2405.17165v1","category":"gr-qc"}
{"created":"2024-05-27 13:36:50","title":"Injecting Hamiltonian Architectural Bias into Deep Graph Networks for Long-Range Propagation","abstract":"The dynamics of information diffusion within graphs is a critical open issue that heavily influences graph representation learning, especially when considering long-range propagation. This calls for principled approaches that control and regulate the degree of propagation and dissipation of information throughout the neural flow. Motivated by this, we introduce (port-)Hamiltonian Deep Graph Networks, a novel framework that models neural information flow in graphs by building on the laws of conservation of Hamiltonian dynamical systems. We reconcile under a single theoretical and practical framework both non-dissipative long-range propagation and non-conservative behaviors, introducing tools from mechanical systems to gauge the equilibrium between the two components. Our approach can be applied to general message-passing architectures, and it provides theoretical guarantees on information conservation in time. Empirical results prove the effectiveness of our port-Hamiltonian scheme in pushing simple graph convolutional architectures to state-of-the-art performance in long-range benchmarks.","sentences":["The dynamics of information diffusion within graphs is a critical open issue that heavily influences graph representation learning, especially when considering long-range propagation.","This calls for principled approaches that control and regulate the degree of propagation and dissipation of information throughout the neural flow.","Motivated by this, we introduce (port-)Hamiltonian Deep Graph Networks, a novel framework that models neural information flow in graphs by building on the laws of conservation of Hamiltonian dynamical systems.","We reconcile under a single theoretical and practical framework both non-dissipative long-range propagation and non-conservative behaviors, introducing tools from mechanical systems to gauge the equilibrium between the two components.","Our approach can be applied to general message-passing architectures, and it provides theoretical guarantees on information conservation in time.","Empirical results prove the effectiveness of our port-Hamiltonian scheme in pushing simple graph convolutional architectures to state-of-the-art performance in long-range benchmarks."],"url":"http://arxiv.org/abs/2405.17163v1","category":"cs.LG"}
{"created":"2024-05-27 13:34:35","title":"Gravitational Wave Cosmology: Be Careful of the Black Hole Mass Spectrum","abstract":"Gravitational waves (GWs) from compact binary coalescences (CBCs) offer insights into the universe expansion. The spectral siren method, used without electromagnetic counterparts (EMC), infers cosmic expansion (Hubble constant) by relating detector and source frame masses of black hole (BH) mergers. However, heuristic mass models (broken power law, power law plus peak, multipeak) may introduce biases in the Hubble constant estimation, potentially up to 3 sigma with 2000 detected GW mergers. These biases stem from the models inability to consider redshift evolution and unexpected mass features. Future GW cosmology studies should employ adaptable source mass models to address these issues.","sentences":["Gravitational waves (GWs) from compact binary coalescences (CBCs) offer insights into the universe expansion.","The spectral siren method, used without electromagnetic counterparts (EMC), infers cosmic expansion (Hubble constant) by relating detector and source frame masses of black hole (BH) mergers.","However, heuristic mass models (broken power law, power law plus peak, multipeak) may introduce biases in the Hubble constant estimation, potentially up to 3 sigma with 2000 detected GW mergers.","These biases stem from the models inability to consider redshift evolution and unexpected mass features.","Future GW cosmology studies should employ adaptable source mass models to address these issues."],"url":"http://arxiv.org/abs/2405.17161v1","category":"gr-qc"}
{"created":"2024-05-27 13:31:46","title":"PatchScaler: An Efficient Patch-independent Diffusion Model for Super-Resolution","abstract":"Diffusion models significantly improve the quality of super-resolved images with their impressive content generation capabilities. However, the huge computational costs limit the applications of these methods.Recent efforts have explored reasonable inference acceleration to reduce the number of sampling steps, but the computational cost remains high as each step is performed on the entire image.This paper introduces PatchScaler, a patch-independent diffusion-based single image super-resolution (SR) method, designed to enhance the efficiency of the inference process.The proposed method is motivated by the observation that not all the image patches within an image need the same sampling steps for reconstructing high-resolution images.Based on this observation, we thus develop a Patch-adaptive Group Sampling (PGS) to divide feature patches into different groups according to the patch-level reconstruction difficulty and dynamically assign an appropriate sampling configuration for each group so that the inference speed can be better accelerated.In addition, to improve the denoising ability at each step of the sampling, we develop a texture prompt to guide the estimations of the diffusion model by retrieving high-quality texture priors from a patch-independent reference texture memory.Experiments show that our PatchScaler achieves favorable performance in both quantitative and qualitative evaluations with fast inference speed.Our code and model are available at \\url{https://github.com/yongliuy/PatchScaler}.","sentences":["Diffusion models significantly improve the quality of super-resolved images with their impressive content generation capabilities.","However, the huge computational costs limit the applications of these methods.","Recent efforts have explored reasonable inference acceleration to reduce the number of sampling steps, but the computational cost remains high as each step is performed on the entire image.","This paper introduces PatchScaler, a patch-independent diffusion-based single image super-resolution (SR) method, designed to enhance the efficiency of the inference process.","The proposed method is motivated by the observation that not all the image patches within an image need the same sampling steps for reconstructing high-resolution images.","Based on this observation, we thus develop a Patch-adaptive Group Sampling (PGS) to divide feature patches into different groups according to the patch-level reconstruction difficulty and dynamically assign an appropriate sampling configuration for each group so that the inference speed can be better accelerated.","In addition, to improve the denoising ability at each step of the sampling, we develop a texture prompt to guide the estimations of the diffusion model by retrieving high-quality texture priors from a patch-independent reference texture memory.","Experiments show that our PatchScaler achieves favorable performance in both quantitative and qualitative evaluations with fast inference speed.","Our code and model are available at \\url{https://github.com/yongliuy/PatchScaler}."],"url":"http://arxiv.org/abs/2405.17158v1","category":"cs.CV"}
{"created":"2024-05-27 13:31:28","title":"Generation and robustness of non-local correlations induced by Heisenberg XYZ and intrinsic decoherence models: (x,y)-spin-orbit interactions and $x$- magnetic field","abstract":"In this work, the Milburn intrinsic decoherence model is used to investigate the role of spin-spin Heisenberg XYZ interaction supported by spin-orbit Dzyaloshinsky Moriya (DM) interactions of x and y directions together in the non-local correlation (NLC) dynamics of Local quantum Fisher information (LQFI), local quantum uncertainty (LQU), and Log-negativity's entanglement. The two-qubit Heisenberg XYZ (non-X) states' nonlocal correlation generations are explored under the effects of the uniformity and the inhomogeneity of an applied x-direction external inhomogeneous magnetic field (EIMF). Our meticulous exploration of the obtained results shows that the spin-spin Heisenberg XYZ and x,y-spin-orbit interactions have a high capability to raise non-local correlations in the presence of a weak external magnetic field. The raised non-local correlation can be improved by strengthening the spin-spin and x,y spin-orbit interactions and increasing the EIMF's inhomogeneity and uniformity. Non-local correlation oscillations' amplitudes and fluctuations are increased. The degradations of the NLCs' generations in the presence of intrinsic decoherence (NLCs' robustness against intrinsic decoherence) can be decreased by strengthening the spin-spin interactions. They can be increased by increasing the intensities of x,y spin-orbit interactions as well as increasing the EIMF's inhomogeneity and uniformity.","sentences":["In this work, the Milburn intrinsic decoherence model is used to investigate the role of spin-spin Heisenberg XYZ interaction supported by spin-orbit Dzyaloshinsky Moriya (DM) interactions of x and y directions together in the non-local correlation (NLC) dynamics of Local quantum Fisher information (LQFI), local quantum uncertainty (LQU), and Log-negativity's entanglement.","The two-qubit Heisenberg XYZ (non-X) states' nonlocal correlation generations are explored under the effects of the uniformity and the inhomogeneity of an applied x-direction external inhomogeneous magnetic field (EIMF).","Our meticulous exploration of the obtained results shows that the spin-spin Heisenberg XYZ and x,y-spin-orbit interactions have a high capability to raise non-local correlations in the presence of a weak external magnetic field.","The raised non-local correlation can be improved by strengthening the spin-spin and x,y spin-orbit interactions and increasing the EIMF's inhomogeneity and uniformity.","Non-local correlation oscillations' amplitudes and fluctuations are increased.","The degradations of the NLCs' generations in the presence of intrinsic decoherence (NLCs' robustness against intrinsic decoherence) can be decreased by strengthening the spin-spin interactions.","They can be increased by increasing the intensities of x,y spin-orbit interactions as well as increasing the EIMF's inhomogeneity and uniformity."],"url":"http://arxiv.org/abs/2405.17157v1","category":"quant-ph"}
{"created":"2024-05-27 13:31:03","title":"The Scaling Law in Stellar Light Curves","abstract":"Analyzing time series of fluxes from stars, known as stellar light curves, can reveal valuable information about stellar properties. However, most current methods rely on extracting summary statistics, and studies using deep learning have been limited to supervised approaches. In this research, we investigate the scaling law properties that emerge when learning from astronomical time series data using self-supervised techniques. By employing the GPT-2 architecture, we show the learned representation improves as the number of parameters increases from $10^4$ to $10^9$, with no signs of performance plateauing. We demonstrate that a self-supervised Transformer model achieves 3-10 times the sample efficiency compared to the state-of-the-art supervised learning model when inferring the surface gravity of stars as a downstream task. Our research lays the groundwork for analyzing stellar light curves by examining them through large-scale auto-regressive generative models.","sentences":["Analyzing time series of fluxes from stars, known as stellar light curves, can reveal valuable information about stellar properties.","However, most current methods rely on extracting summary statistics, and studies using deep learning have been limited to supervised approaches.","In this research, we investigate the scaling law properties that emerge when learning from astronomical time series data using self-supervised techniques.","By employing the GPT-2 architecture, we show the learned representation improves as the number of parameters increases from $10^4$ to $10^9$, with no signs of performance plateauing.","We demonstrate that a self-supervised Transformer model achieves 3-10 times the sample efficiency compared to the state-of-the-art supervised learning model when inferring the surface gravity of stars as a downstream task.","Our research lays the groundwork for analyzing stellar light curves by examining them through large-scale auto-regressive generative models."],"url":"http://arxiv.org/abs/2405.17156v1","category":"astro-ph.IM"}
{"created":"2024-05-27 13:27:45","title":"Quantitative phase mixing for Hamiltonians with trapping","abstract":"We prove quantitative decay estimates of macroscopic quantities generated by the solutions to linear transport equations driven by a general family of Hamiltonians. The associated particle trajectories are all trapped in a compact region of phase-space and feature a non-degenerate elliptic stagnation point. The analysis covers a large class of Hamiltonians generated by the radially symmetric compactly supported equilibria of the gravitational Vlasov-Poisson system. Working in radial symmetry, our analysis features both the 1+2-dimensional case and the harder 1+1-dimensional case, where all the particles have the same value of the modulus of angular momentum. The latter case is also of importance in both the plasma physics case and two dimensional incompressible fluid flows.","sentences":["We prove quantitative decay estimates of macroscopic quantities generated by the solutions to linear transport equations driven by a general family of Hamiltonians.","The associated particle trajectories are all trapped in a compact region of phase-space and feature a non-degenerate elliptic stagnation point.","The analysis covers a large class of Hamiltonians generated by the radially symmetric compactly supported equilibria of the gravitational Vlasov-Poisson system.","Working in radial symmetry, our analysis features both the 1+2-dimensional case and the harder 1+1-dimensional case, where all the particles have the same value of the modulus of angular momentum.","The latter case is also of importance in both the plasma physics case and two dimensional incompressible fluid flows."],"url":"http://arxiv.org/abs/2405.17153v1","category":"math.AP"}
{"created":"2024-05-27 13:26:59","title":"CoSLight: Co-optimizing Collaborator Selection and Decision-making to Enhance Traffic Signal Control","abstract":"Effective multi-intersection collaboration is pivotal for reinforcement-learning-based traffic signal control to alleviate congestion. Existing work mainly chooses neighboring intersections as collaborators. However, quite an amount of congestion, even some wide-range congestion, is caused by non-neighbors failing to collaborate. To address these issues, we propose to separate the collaborator selection as a second policy to be learned, concurrently being updated with the original signal-controlling policy. Specifically, the selection policy in real-time adaptively selects the best teammates according to phase- and intersection-level features. Empirical results on both synthetic and real-world datasets provide robust validation for the superiority of our approach, offering significant improvements over existing state-of-the-art methods. The code is available at https://github.com/AnonymousAccountss/CoSLight.","sentences":["Effective multi-intersection collaboration is pivotal for reinforcement-learning-based traffic signal control to alleviate congestion.","Existing work mainly chooses neighboring intersections as collaborators.","However, quite an amount of congestion, even some wide-range congestion, is caused by non-neighbors failing to collaborate.","To address these issues, we propose to separate the collaborator selection as a second policy to be learned, concurrently being updated with the original signal-controlling policy.","Specifically, the selection policy in real-time adaptively selects the best teammates according to phase- and intersection-level features.","Empirical results on both synthetic and real-world datasets provide robust validation for the superiority of our approach, offering significant improvements over existing state-of-the-art methods.","The code is available at https://github.com/AnonymousAccountss/CoSLight."],"url":"http://arxiv.org/abs/2405.17152v1","category":"cs.MA"}
{"created":"2024-05-27 13:26:34","title":"Smoke and Mirrors in Causal Downstream Tasks","abstract":"Machine Learning and AI have the potential to transform data-driven scientific discovery, enabling accurate predictions for several scientific phenomena. As many scientific questions are inherently causal, this paper looks at the causal inference task of treatment effect estimation, where we assume binary effects that are recorded as high-dimensional images in a Randomized Controlled Trial (RCT). Despite being the simplest possible setting and a perfect fit for deep learning, we theoretically find that many common choices in the literature may lead to biased estimates. To test the practical impact of these considerations, we recorded the first real-world benchmark for causal inference downstream tasks on high-dimensional observations as an RCT studying how garden ants (Lasius neglectus) respond to microparticles applied onto their colony members by hygienic grooming. Comparing 6 480 models fine-tuned from state-of-the-art visual backbones, we find that the sampling and modeling choices significantly affect the accuracy of the causal estimate, and that classification accuracy is not a proxy thereof. We further validated the analysis, repeating it on a synthetically generated visual data set controlling the causal model. Our results suggest that future benchmarks should carefully consider real downstream scientific questions, especially causal ones. Further, we highlight guidelines for representation learning methods to help answer causal questions in the sciences. All code and data will be released.","sentences":["Machine Learning and AI have the potential to transform data-driven scientific discovery, enabling accurate predictions for several scientific phenomena.","As many scientific questions are inherently causal, this paper looks at the causal inference task of treatment effect estimation, where we assume binary effects that are recorded as high-dimensional images in a Randomized Controlled Trial (RCT).","Despite being the simplest possible setting and a perfect fit for deep learning, we theoretically find that many common choices in the literature may lead to biased estimates.","To test the practical impact of these considerations, we recorded the first real-world benchmark for causal inference downstream tasks on high-dimensional observations as an RCT studying how garden ants (Lasius neglectus) respond to microparticles applied onto their colony members by hygienic grooming.","Comparing 6 480 models fine-tuned from state-of-the-art visual backbones, we find that the sampling and modeling choices significantly affect the accuracy of the causal estimate, and that classification accuracy is not a proxy thereof.","We further validated the analysis, repeating it on a synthetically generated visual data set controlling the causal model.","Our results suggest that future benchmarks should carefully consider real downstream scientific questions, especially causal ones.","Further, we highlight guidelines for representation learning methods to help answer causal questions in the sciences.","All code and data will be released."],"url":"http://arxiv.org/abs/2405.17151v1","category":"cs.LG"}
{"created":"2024-05-27 13:23:23","title":"Deep Learning-based Joint Channel Prediction and Multibeam Precoding for LEO Satellite Internet of Things","abstract":"Low earth orbit (LEO) satellite internet of things (IoT) is a promising way achieving global Internet of Everything, and thus has been widely recognized as an important component of sixth-generation (6G) wireless networks. Yet, due to high-speed movement of the LEO satellite, it is challenging to acquire timely channel state information (CSI) and design effective multibeam precoding for various IoT applications. To this end, this paper provides a deep learning (DL)-based joint channel prediction and multibeam precoding scheme under adverse environments, e.g., high Doppler shift, long propagation delay, and low satellite payload. {Specifically, this paper first designs a DL-based channel prediction scheme by using convolutional neural networks (CNN) and long short term memory (LSTM), which predicts the CSI of current time slot according to that of previous time slots. With the predicted CSI, this paper designs a DL-based robust multibeam precoding scheme by using a channel augmentation method based on variational auto-encoder (VAE).} Finally, extensive simulation results confirm the effectiveness and robustness of the proposed scheme in LEO satellite IoT.","sentences":["Low earth orbit (LEO) satellite internet of things (IoT) is a promising way achieving global Internet of Everything, and thus has been widely recognized as an important component of sixth-generation (6G) wireless networks.","Yet, due to high-speed movement of the LEO satellite, it is challenging to acquire timely channel state information (CSI) and design effective multibeam precoding for various IoT applications.","To this end, this paper provides a deep learning (DL)-based joint channel prediction and multibeam precoding scheme under adverse environments, e.g., high Doppler shift, long propagation delay, and low satellite payload.","{Specifically, this paper first designs a DL-based channel prediction scheme by using convolutional neural networks (CNN) and long short term memory (LSTM), which predicts the CSI of current time slot according to that of previous time slots.","With the predicted CSI, this paper designs a DL-based robust multibeam precoding scheme by using a channel augmentation method based on variational auto-encoder (VAE).}","Finally, extensive simulation results confirm the effectiveness and robustness of the proposed scheme in LEO satellite IoT."],"url":"http://arxiv.org/abs/2405.17150v1","category":"cs.IT"}
{"created":"2024-05-27 13:16:29","title":"Large Language Models (LLMs): Deployment, Tokenomics and Sustainability","abstract":"The rapid advancement of Large Language Models (LLMs) has significantly impacted human-computer interaction, epitomized by the release of GPT-4o, which introduced comprehensive multi-modality capabilities. In this paper, we first explored the deployment strategies, economic considerations, and sustainability challenges associated with the state-of-the-art LLMs. More specifically, we discussed the deployment debate between Retrieval-Augmented Generation (RAG) and fine-tuning, highlighting their respective advantages and limitations. After that, we quantitatively analyzed the requirement of xPUs in training and inference. Additionally, for the tokenomics of LLM services, we examined the balance between performance and cost from the quality of experience (QoE)'s perspective of end users. Lastly, we envisioned the future hybrid architecture of LLM processing and its corresponding sustainability concerns, particularly in the environmental carbon footprint impact. Through these discussions, we provided a comprehensive overview of the operational and strategic considerations essential for the responsible development and deployment of LLMs.","sentences":["The rapid advancement of Large Language Models (LLMs) has significantly impacted human-computer interaction, epitomized by the release of GPT-4o, which introduced comprehensive multi-modality capabilities.","In this paper, we first explored the deployment strategies, economic considerations, and sustainability challenges associated with the state-of-the-art LLMs.","More specifically, we discussed the deployment debate between Retrieval-Augmented Generation (RAG) and fine-tuning, highlighting their respective advantages and limitations.","After that, we quantitatively analyzed the requirement of xPUs in training and inference.","Additionally, for the tokenomics of LLM services, we examined the balance between performance and cost from the quality of experience (QoE)'s perspective of end users.","Lastly, we envisioned the future hybrid architecture of LLM processing and its corresponding sustainability concerns, particularly in the environmental carbon footprint impact.","Through these discussions, we provided a comprehensive overview of the operational and strategic considerations essential for the responsible development and deployment of LLMs."],"url":"http://arxiv.org/abs/2405.17147v1","category":"cs.MM"}
{"created":"2024-05-27 13:09:23","title":"Compressed-Language Models for Understanding Compressed File Formats: a JPEG Exploration","abstract":"This study investigates whether Compressed-Language Models (CLMs), i.e. language models operating on raw byte streams from Compressed File Formats~(CFFs), can understand files compressed by CFFs. We focus on the JPEG format as a representative CFF, given its commonality and its representativeness of key concepts in compression, such as entropy coding and run-length encoding. We test if CLMs understand the JPEG format by probing their capabilities to perform along three axes: recognition of inherent file properties, handling of files with anomalies, and generation of new files. Our findings demonstrate that CLMs can effectively perform these tasks. These results suggest that CLMs can understand the semantics of compressed data when directly operating on the byte streams of files produced by CFFs. The possibility to directly operate on raw compressed files offers the promise to leverage some of their remarkable characteristics, such as their ubiquity, compactness, multi-modality and segment-nature.","sentences":["This study investigates whether Compressed-Language Models (CLMs), i.e. language models operating on raw byte streams from Compressed File Formats~(CFFs), can understand files compressed by CFFs.","We focus on the JPEG format as a representative CFF, given its commonality and its representativeness of key concepts in compression, such as entropy coding and run-length encoding.","We test if CLMs understand the JPEG format by probing their capabilities to perform along three axes: recognition of inherent file properties, handling of files with anomalies, and generation of new files.","Our findings demonstrate that CLMs can effectively perform these tasks.","These results suggest that CLMs can understand the semantics of compressed data when directly operating on the byte streams of files produced by CFFs.","The possibility to directly operate on raw compressed files offers the promise to leverage some of their remarkable characteristics, such as their ubiquity, compactness, multi-modality and segment-nature."],"url":"http://arxiv.org/abs/2405.17146v1","category":"cs.CV"}
{"created":"2024-05-27 13:05:32","title":"Brightened Optical Transition as Indicator of Multiferroicity in a Layered Antiferromagnet","abstract":"Two-dimensional van der Waals magnets show strong interconnection between their electrical, magnetic, and structural properties. Here we reveal the emergence of a luminescent transition upon crossing the N\\'eel transition temperature of CrPS$_4$, a layered antiferromagnetic semiconductor. This luminescent transition occurs above the lowest absorption level. We attribute the optical transitions to excited states of the t$_{\\rm 2g}$ orbitals of the Cr$^{3+}$ ions, which are influenced by the distortion of the octahedral crystal field. Specifically, we find at the crossing of the N\\'eel temperature changes the distortion from an anti-polar to polar arrangement, thereby not only activating an additional luminescent pathway but also inducing a significant in-plane static dipole moment detected by a marked enhancement in the intensity of the second harmonic generation. Our findings suggest the presence of a multiferroic state in CrPS$_4$ below the N\\'eel temperature.","sentences":["Two-dimensional van der Waals magnets show strong interconnection between their electrical, magnetic, and structural properties.","Here we reveal the emergence of a luminescent transition upon crossing the N\\'eel transition temperature of CrPS$_4$, a layered antiferromagnetic semiconductor.","This luminescent transition occurs above the lowest absorption level.","We attribute the optical transitions to excited states of the t$_{\\rm 2g}$ orbitals of the Cr$^{3+}$ ions, which are influenced by the distortion of the octahedral crystal field.","Specifically, we find at the crossing of the N\\'eel temperature changes the distortion from an anti-polar to polar arrangement, thereby not only activating an additional luminescent pathway but also inducing a significant in-plane static dipole moment detected by a marked enhancement in the intensity of the second harmonic generation.","Our findings suggest the presence of a multiferroic state in CrPS$_4$ below the N\\'eel temperature."],"url":"http://arxiv.org/abs/2405.17144v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-27 13:04:29","title":"Second Law of Thermodynamics without Einstein Relation","abstract":"Materials that are constantly driven out of thermodynamic equilibrium, such as active and living systems, typically violate the Einstein relation. This may arise from active contributions to particle fluctuations which are unrelated to the dissipative resistance of the surrounding medium. We show that in these cases the widely used relation between informatic entropy production and heat dissipation does not hold. Consequently, fluctuation relations for the mechanical work, such as the Jarzynski and Crooks theorems, are invalid. We relate the breaking of the correspondence between entropy production and heat dissipation to the breaking of the fluctuation-dissipation theorem. We propose a temperature-like variable which restores this correspondence and gives rise to a generalized second law of thermodynamics, whereby the dissipated heat is necessarily nonnegative and vanishes at equilibrium. The Clausius inequality, Carnot maximum efficiency theorem, and relation between the maximum extractable work and the change of free energy are recovered as well.","sentences":["Materials that are constantly driven out of thermodynamic equilibrium, such as active and living systems, typically violate the Einstein relation.","This may arise from active contributions to particle fluctuations which are unrelated to the dissipative resistance of the surrounding medium.","We show that in these cases the widely used relation between informatic entropy production and heat dissipation does not hold.","Consequently, fluctuation relations for the mechanical work, such as the Jarzynski and Crooks theorems, are invalid.","We relate the breaking of the correspondence between entropy production and heat dissipation to the breaking of the fluctuation-dissipation theorem.","We propose a temperature-like variable which restores this correspondence and gives rise to a generalized second law of thermodynamics, whereby the dissipated heat is necessarily nonnegative and vanishes at equilibrium.","The Clausius inequality, Carnot maximum efficiency theorem, and relation between the maximum extractable work and the change of free energy are recovered as well."],"url":"http://arxiv.org/abs/2405.17142v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-27 12:59:35","title":"Synergy and Diversity in CLIP: Enhancing Performance Through Adaptive Backbone Ensembling","abstract":"Contrastive Language-Image Pretraining (CLIP) stands out as a prominent method for image representation learning. Various architectures, from vision transformers (ViTs) to convolutional networks (ResNets) have been trained with CLIP to serve as general solutions to diverse vision tasks. This paper explores the differences across various CLIP-trained vision backbones. Despite using the same data and training objective, we find that these architectures have notably different representations, different classification performance across datasets, and different robustness properties to certain types of image perturbations. Our findings indicate a remarkable possible synergy across backbones by leveraging their respective strengths. In principle, classification accuracy could be improved by over 40 percentage with an informed selection of the optimal backbone per test example.Using this insight, we develop a straightforward yet powerful approach to adaptively ensemble multiple backbones. The approach uses as few as one labeled example per class to tune the adaptive combination of backbones. On a large collection of datasets, the method achieves a remarkable increase in accuracy of up to 39.1% over the best single backbone, well beyond traditional ensembles","sentences":["Contrastive Language-Image Pretraining (CLIP) stands out as a prominent method for image representation learning.","Various architectures, from vision transformers (ViTs) to convolutional networks (ResNets) have been trained with CLIP to serve as general solutions to diverse vision tasks.","This paper explores the differences across various CLIP-trained vision backbones.","Despite using the same data and training objective, we find that these architectures have notably different representations, different classification performance across datasets, and different robustness properties to certain types of image perturbations.","Our findings indicate a remarkable possible synergy across backbones by leveraging their respective strengths.","In principle, classification accuracy could be improved by over 40 percentage with an informed selection of the optimal backbone per test example.","Using this insight, we develop a straightforward yet powerful approach to adaptively ensemble multiple backbones.","The approach uses as few as one labeled example per class to tune the adaptive combination of backbones.","On a large collection of datasets, the method achieves a remarkable increase in accuracy of up to 39.1% over the best single backbone, well beyond traditional ensembles"],"url":"http://arxiv.org/abs/2405.17139v1","category":"cs.CV"}
{"created":"2024-05-27 12:54:09","title":"Jump-teaching: Ultra Efficient and Robust Learning with Noisy Label","abstract":"Sample selection is the most straightforward technique to combat label noise, aiming to distinguish mislabeled samples during training and avoid the degradation of the robustness of the model. In the workflow, $\\textit{selecting possibly clean data}$ and $\\textit{model update}$ are iterative. However, their interplay and intrinsic characteristics hinder the robustness and efficiency of learning with noisy labels: 1)~The model chooses clean data with selection bias, leading to the accumulated error in the model update. 2) Most selection strategies leverage partner networks or supplementary information to mitigate label corruption, albeit with increased computation resources and lower throughput speed. Therefore, we employ only one network with the jump manner update to decouple the interplay and mine more semantic information from the loss for a more precise selection. Specifically, the selection of clean data for each model update is based on one of the prior models, excluding the last iteration. The strategy of model update exhibits a jump behavior in the form. Moreover, we map the outputs of the network and labels into the same semantic feature space, respectively. In this space, a detailed and simple loss distribution is generated to distinguish clean samples more effectively. Our proposed approach achieves almost up to $2.53\\times$ speedup, $0.46\\times$ peak memory footprint, and superior robustness over state-of-the-art works with various noise settings.","sentences":["Sample selection is the most straightforward technique to combat label noise, aiming to distinguish mislabeled samples during training and avoid the degradation of the robustness of the model.","In the workflow, $\\textit{selecting possibly clean data}$ and $\\textit{model update}$ are iterative.","However, their interplay and intrinsic characteristics hinder the robustness and efficiency of learning with noisy labels: 1)~The model chooses clean data with selection bias, leading to the accumulated error in the model update.","2) Most selection strategies leverage partner networks or supplementary information to mitigate label corruption, albeit with increased computation resources and lower throughput speed.","Therefore, we employ only one network with the jump manner update to decouple the interplay and mine more semantic information from the loss for a more precise selection.","Specifically, the selection of clean data for each model update is based on one of the prior models, excluding the last iteration.","The strategy of model update exhibits a jump behavior in the form.","Moreover, we map the outputs of the network and labels into the same semantic feature space, respectively.","In this space, a detailed and simple loss distribution is generated to distinguish clean samples more effectively.","Our proposed approach achieves almost up to $2.53\\times$ speedup, $0.46\\times$ peak memory footprint, and superior robustness over state-of-the-art works with various noise settings."],"url":"http://arxiv.org/abs/2405.17137v1","category":"cs.CV"}
{"created":"2024-05-27 12:54:05","title":"PanoTree: Autonomous Photo-Spot Explorer in Virtual Reality Scenes","abstract":"Social VR platforms enable social, economic, and creative activities by allowing users to create and share their own virtual spaces. In social VR, photography within a VR scene is an important indicator of visitors' activities. Although automatic identification of photo spots within a VR scene can facilitate the process of creating a VR scene and enhance the visitor experience, there are challenges in quantitatively evaluating photos taken in the VR scene and efficiently exploring the large VR scene. We propose PanoTree, an automated photo-spot explorer in VR scenes. To assess the aesthetics of images captured in VR scenes, a deep scoring network is trained on a large dataset of photos collected by a social VR platform to determine whether humans are likely to take similar photos. Furthermore, we propose a Hierarchical Optimistic Optimization (HOO)-based search algorithm to efficiently explore 3D VR spaces with the reward from the scoring network. Our user study shows that the scoring network achieves human-level performance in distinguishing randomly taken images from those taken by humans. In addition, we show applications using the explored photo spots, such as automatic thumbnail generation, support for VR world creation, and visitor flow planning within a VR scene.","sentences":["Social VR platforms enable social, economic, and creative activities by allowing users to create and share their own virtual spaces.","In social VR, photography within a VR scene is an important indicator of visitors' activities.","Although automatic identification of photo spots within a VR scene can facilitate the process of creating a VR scene and enhance the visitor experience, there are challenges in quantitatively evaluating photos taken in the VR scene and efficiently exploring the large VR scene.","We propose PanoTree, an automated photo-spot explorer in VR scenes.","To assess the aesthetics of images captured in VR scenes, a deep scoring network is trained on a large dataset of photos collected by a social VR platform to determine whether humans are likely to take similar photos.","Furthermore, we propose a Hierarchical Optimistic Optimization (HOO)-based search algorithm to efficiently explore 3D VR spaces with the reward from the scoring network.","Our user study shows that the scoring network achieves human-level performance in distinguishing randomly taken images from those taken by humans.","In addition, we show applications using the explored photo spots, such as automatic thumbnail generation, support for VR world creation, and visitor flow planning within a VR scene."],"url":"http://arxiv.org/abs/2405.17136v1","category":"cs.CV"}
{"created":"2024-05-27 12:48:30","title":"Exploiting the Layered Intrinsic Dimensionality of Deep Models for Practical Adversarial Training","abstract":"Despite being a heavily researched topic, Adversarial Training (AT) is rarely, if ever, deployed in practical AI systems for two primary reasons: (i) the gained robustness is frequently accompanied by a drop in generalization and (ii) generating adversarial examples (AEs) is computationally prohibitively expensive. To address these limitations, we propose SMAAT, a new AT algorithm that leverages the manifold conjecture, stating that off-manifold AEs lead to better robustness while on-manifold AEs result in better generalization. Specifically, SMAAT aims at generating a higher proportion of off-manifold AEs by perturbing the intermediate deepnet layer with the lowest intrinsic dimension. This systematically results in better scalability compared to classical AT as it reduces the PGD chains length required for generating the AEs. Additionally, our study provides, to the best of our knowledge, the first explanation for the difference in the generalization and robustness trends between vision and language models, ie., AT results in a drop in generalization in vision models whereas, in encoder-based language models, generalization either improves or remains unchanged. We show that vision transformers and decoder-based models tend to have low intrinsic dimensionality in the earlier layers of the network (more off-manifold AEs), while encoder-based models have low intrinsic dimensionality in the later layers. We demonstrate the efficacy of SMAAT; on several tasks, including robustifying (i) sentiment classifiers, (ii) safety filters in decoder-based models, and (iii) retrievers in RAG setups. SMAAT requires only 25-33% of the GPU time compared to standard AT, while significantly improving robustness across all applications and maintaining comparable generalization.","sentences":["Despite being a heavily researched topic, Adversarial Training (AT) is rarely, if ever, deployed in practical AI systems for two primary reasons: (i) the gained robustness is frequently accompanied by a drop in generalization and (ii) generating adversarial examples (AEs) is computationally prohibitively expensive.","To address these limitations, we propose SMAAT, a new AT algorithm that leverages the manifold conjecture, stating that off-manifold AEs lead to better robustness while on-manifold AEs result in better generalization.","Specifically, SMAAT aims at generating a higher proportion of off-manifold AEs by perturbing the intermediate deepnet layer with the lowest intrinsic dimension.","This systematically results in better scalability compared to classical AT as it reduces the PGD chains length required for generating the AEs.","Additionally, our study provides, to the best of our knowledge, the first explanation for the difference in the generalization and robustness trends between vision and language models, ie., AT results in a drop in generalization in vision models whereas, in encoder-based language models, generalization either improves or remains unchanged.","We show that vision transformers and decoder-based models tend to have low intrinsic dimensionality in the earlier layers of the network (more off-manifold AEs), while encoder-based models have low intrinsic dimensionality in the later layers.","We demonstrate the efficacy of SMAAT; on several tasks, including robustifying (i) sentiment classifiers, (ii) safety filters in decoder-based models, and (iii) retrievers in RAG setups.","SMAAT requires only 25-33% of the GPU time compared to standard AT, while significantly improving robustness across all applications and maintaining comparable generalization."],"url":"http://arxiv.org/abs/2405.17130v1","category":"cs.LG"}
{"created":"2024-05-27 12:47:40","title":"TEII: Think, Explain, Interact and Iterate with Large Language Models to Solve Cross-lingual Emotion Detection","abstract":"Cross-lingual emotion detection allows us to analyze global trends, public opinion, and social phenomena at scale. We participated in the Explainability of Cross-lingual Emotion Detection (EXALT) shared task, achieving an F1-score of 0.6046 on the evaluation set for the emotion detection sub-task. Our system outperformed the baseline by more than 0.16 F1-score absolute, and ranked second amongst competing systems. We conducted experiments using fine-tuning, zero-shot learning, and few-shot learning for Large Language Model (LLM)-based models as well as embedding-based BiLSTM and KNN for non-LLM-based techniques. Additionally, we introduced two novel methods: the Multi-Iteration Agentic Workflow and the Multi-Binary-Classifier Agentic Workflow. We found that LLM-based approaches provided good performance on multilingual emotion detection. Furthermore, ensembles combining all our experimented models yielded higher F1-scores than any single approach alone.","sentences":["Cross-lingual emotion detection allows us to analyze global trends, public opinion, and social phenomena at scale.","We participated in the Explainability of Cross-lingual Emotion Detection (EXALT) shared task, achieving an F1-score of 0.6046 on the evaluation set for the emotion detection sub-task.","Our system outperformed the baseline by more than 0.16 F1-score absolute, and ranked second amongst competing systems.","We conducted experiments using fine-tuning, zero-shot learning, and few-shot learning for Large Language Model (LLM)-based models as well as embedding-based BiLSTM and KNN for non-LLM-based techniques.","Additionally, we introduced two novel methods: the Multi-Iteration Agentic Workflow and the Multi-Binary-Classifier Agentic Workflow.","We found that LLM-based approaches provided good performance on multilingual emotion detection.","Furthermore, ensembles combining all our experimented models yielded higher F1-scores than any single approach alone."],"url":"http://arxiv.org/abs/2405.17129v1","category":"cs.CL"}
{"created":"2024-05-27 12:47:15","title":"The response of the inner dark matter halo to stellar bars","abstract":"Barred galaxies constitute about two thirds of observed disc galaxies. Bars affect not only the mass distribution of gas and stars, but also that of the dark matter. An elongation of the inner dark matter halo is known as the halo bar. We aim to characterise the structure of the halo bars, with the goal of correlating them with the properties of the stellar bars. We use a suite of simulated galaxies with various bar strengths, including gas and star formation. We quantify strengths, shapes, and densities of these simulated stellar bars. We carry out numerical experiments with frozen and analytic potentials in order to understand the role played by a live responsive stellar bar. We find that the halo bar generally follows the trends of the disc bar. The strengths of the halo and stellar bars are tightly correlated. Stronger bars induce a slight increase of dark matter density within the inner halo. Numerical experiments show that a non-responsive frozen stellar bar would be capable of inducing a dark matter bar, but it would be weaker than the live case by a factor of roughly two.","sentences":["Barred galaxies constitute about two thirds of observed disc galaxies.","Bars affect not only the mass distribution of gas and stars, but also that of the dark matter.","An elongation of the inner dark matter halo is known as the halo bar.","We aim to characterise the structure of the halo bars, with the goal of correlating them with the properties of the stellar bars.","We use a suite of simulated galaxies with various bar strengths, including gas and star formation.","We quantify strengths, shapes, and densities of these simulated stellar bars.","We carry out numerical experiments with frozen and analytic potentials in order to understand the role played by a live responsive stellar bar.","We find that the halo bar generally follows the trends of the disc bar.","The strengths of the halo and stellar bars are tightly correlated.","Stronger bars induce a slight increase of dark matter density within the inner halo.","Numerical experiments show that a non-responsive frozen stellar bar would be capable of inducing a dark matter bar, but it would be weaker than the live case by a factor of roughly two."],"url":"http://arxiv.org/abs/2405.17128v1","category":"astro-ph.GA"}
{"created":"2024-05-27 12:45:37","title":"Motion Primitives Planning For Center-Articulated Vehicles","abstract":"Autonomous navigation across unstructured terrains, including forests and construction areas, faces unique challenges due to intricate obstacles and the element of the unknown. Lacking pre-existing maps, these scenarios necessitate a motion planning approach that combines agility with efficiency. Critically, it must also incorporate the robot's kinematic constraints to navigate more effectively through complex environments. This work introduces a novel planning method for center-articulated vehicles (CAV), leveraging motion primitives within a receding horizon planning framework using onboard sensing. The approach commences with the offline creation of motion primitives, generated through forward simulations that reflect the distinct kinematic model of center-articulated vehicles. These primitives undergo evaluation through a heuristic-based scoring function, facilitating the selection of the most suitable path for real-time navigation. To augment this planning process, we develop a pose-stabilizing controller, tailored to the kinematic specifications of center-articulated vehicles. During experiments, our method demonstrates a $67\\%$ improvement in SPL (Success Rate weighted by Path Length) performance over existing strategies. Furthermore, its efficacy was validated through real-world experiments conducted with a tree harvester vehicle - SAHA.","sentences":["Autonomous navigation across unstructured terrains, including forests and construction areas, faces unique challenges due to intricate obstacles and the element of the unknown.","Lacking pre-existing maps, these scenarios necessitate a motion planning approach that combines agility with efficiency.","Critically, it must also incorporate the robot's kinematic constraints to navigate more effectively through complex environments.","This work introduces a novel planning method for center-articulated vehicles (CAV), leveraging motion primitives within a receding horizon planning framework using onboard sensing.","The approach commences with the offline creation of motion primitives, generated through forward simulations that reflect the distinct kinematic model of center-articulated vehicles.","These primitives undergo evaluation through a heuristic-based scoring function, facilitating the selection of the most suitable path for real-time navigation.","To augment this planning process, we develop a pose-stabilizing controller, tailored to the kinematic specifications of center-articulated vehicles.","During experiments, our method demonstrates a $67\\%$ improvement in SPL (Success Rate weighted by Path Length) performance over existing strategies.","Furthermore, its efficacy was validated through real-world experiments conducted with a tree harvester vehicle - SAHA."],"url":"http://arxiv.org/abs/2405.17127v1","category":"cs.RO"}
{"created":"2024-05-27 12:44:38","title":"Smoothing effects and extinction in finite time for fractional fast diffusions on Riemannian manifolds","abstract":"We study nonnegative solutions to the Cauchy problem for the Fractional Fast Diffusion Equation on a suitable class of connected, noncompact Riemannian manifolds. This parabolic equation is both singular and nonlocal: the diffusion is driven by the (spectral) fractional Laplacian on the manifold, while the nonlinearity is a concave power makes the diffusion singular, so that solutions lose mass and may extinguish in finite time. Existence of mild solutions follows by nowadays standard nonlinear semigroups techniques, and we use these solutions as the building blocks for a more general class of so-called weak dual solutions, which allow for data both in the usual $L^1$ space and in a larger weighted space, determined in terms of the fractional Green function. We focus in particular on a priori smoothing estimates (also in weighted $L^p$ spaces) for a quite large class of weak dual solutions. We also show pointwise lower bounds for solutions, showing in particular that solutions have infinite speed of propagation. Finally, we start the study of how solutions extinguish in finite time, providing suitable sharp extinction rates.","sentences":["We study nonnegative solutions to the Cauchy problem for the Fractional Fast Diffusion Equation on a suitable class of connected, noncompact Riemannian manifolds.","This parabolic equation is both singular and nonlocal: the diffusion is driven by the (spectral) fractional Laplacian on the manifold, while the nonlinearity is a concave power makes the diffusion singular, so that solutions lose mass and may extinguish in finite time.","Existence of mild solutions follows by nowadays standard nonlinear semigroups techniques, and we use these solutions as the building blocks for a more general class of so-called weak dual solutions, which allow for data both in the usual $L^1$ space and in a larger weighted space, determined in terms of the fractional Green function.","We focus in particular on a priori smoothing estimates (also in weighted $L^p$ spaces) for a quite large class of weak dual solutions.","We also show pointwise lower bounds for solutions, showing in particular that solutions have infinite speed of propagation.","Finally, we start the study of how solutions extinguish in finite time, providing suitable sharp extinction rates."],"url":"http://arxiv.org/abs/2405.17126v1","category":"math.AP"}
{"created":"2024-05-27 12:43:07","title":"Strategies to enhance THz harmonic generation combining multilayered, gated, and metamaterial-based architectures","abstract":"Graphene has unique properties paving the way for groundbreaking future applications. Its large optical nonlinearity and ease of integration in devices notably makes it an ideal candidate to become a key component for all-optical switching and frequency conversion applications. In the terahertz (THz) region, various approaches have been independently demonstrated to optimize the nonlinear effects in graphene, addressing a critical limitation arising from the atomically thin interaction length. Here, we demonstrate sample architectures that combine strategies to enhance THz nonlinearities in graphene-based structures. We achieve this by increasing the interaction length through a multilayered design, controlling carrier density with an electrical gate, and modulating the THz field spatial distribution with a metallic metasurface substrate. Our study specifically investigates third harmonic generation (THG) using a table-top high-field THz source. We measure THG enhancement factors exceeding thirty and propose architectures capable of achieving a two-order-of-magnitude increase. These findings highlight the potential of engineered graphene-based samples in advancing THz frequency conversion technologies for signal processing and wireless communication applications.","sentences":["Graphene has unique properties paving the way for groundbreaking future applications.","Its large optical nonlinearity and ease of integration in devices notably makes it an ideal candidate to become a key component for all-optical switching and frequency conversion applications.","In the terahertz (THz) region, various approaches have been independently demonstrated to optimize the nonlinear effects in graphene, addressing a critical limitation arising from the atomically thin interaction length.","Here, we demonstrate sample architectures that combine strategies to enhance THz nonlinearities in graphene-based structures.","We achieve this by increasing the interaction length through a multilayered design, controlling carrier density with an electrical gate, and modulating the THz field spatial distribution with a metallic metasurface substrate.","Our study specifically investigates third harmonic generation (THG) using a table-top high-field THz source.","We measure THG enhancement factors exceeding thirty and propose architectures capable of achieving a two-order-of-magnitude increase.","These findings highlight the potential of engineered graphene-based samples in advancing THz frequency conversion technologies for signal processing and wireless communication applications."],"url":"http://arxiv.org/abs/2405.17125v1","category":"physics.optics"}
{"created":"2024-05-27 12:41:58","title":"Drag, lift, and torque correlations for axi-symmetric rod-like non-spherical particles in linear wall-bounded shear flow","abstract":"This paper presents novel correlations to predict the drag, lift, and torque coefficients of axi-symmetric non-spherical rod-like particles in a wall-bounded linear shear flow. The particle position and orientation relative to the wall are varied to systematically investigate the influence of the wall on the hydrodynamic forces. The newly derived correlations for drag, lift, and torque on the particle depend on various parameters, including the particle Reynolds number, the orientation angle between the major axis of the particle and the main local flow direction, the aspect ratio of the particle, and the dimensionless distance from the particle centre to the wall. The coefficients of the correlations are determined through a fitting process utilizing the data generated from our previous study on the interaction forces between a locally uniform flow and an axi-symmetric non-spherical rod-like particles, as well as from data of novel direct numerical simulations (DNS) performed in this work of flow past axi-symmetric rod-like particles near a wall. The proposed correlations exhibit a good agreement compared to the DNS results, with median errors of 2.89%, 5.37%, and 11.00%, and correlation coefficients of 0.99, 0.99, and 0.96 for the correlations accounting for changes in drag, lift, and torque coefficients due to the wall-bounded linear shear flow profile, respectively. These correlations can be used in large-scale simulations using an Eulerian-Lagrangian or a CFD/DEM framework to predict the behaviour of axi-symmetric rod-like non-spherical particles in wall-bounded flows, shear flows, as well as uniform flows.","sentences":["This paper presents novel correlations to predict the drag, lift, and torque coefficients of axi-symmetric non-spherical rod-like particles in a wall-bounded linear shear flow.","The particle position and orientation relative to the wall are varied to systematically investigate the influence of the wall on the hydrodynamic forces.","The newly derived correlations for drag, lift, and torque on the particle depend on various parameters, including the particle Reynolds number, the orientation angle between the major axis of the particle and the main local flow direction, the aspect ratio of the particle, and the dimensionless distance from the particle centre to the wall.","The coefficients of the correlations are determined through a fitting process utilizing the data generated from our previous study on the interaction forces between a locally uniform flow and an axi-symmetric non-spherical rod-like particles, as well as from data of novel direct numerical simulations (DNS) performed in this work of flow past axi-symmetric rod-like particles near a wall.","The proposed correlations exhibit a good agreement compared to the DNS results, with median errors of 2.89%, 5.37%, and 11.00%, and correlation coefficients of 0.99, 0.99, and 0.96 for the correlations accounting for changes in drag, lift, and torque coefficients due to the wall-bounded linear shear flow profile, respectively.","These correlations can be used in large-scale simulations using an Eulerian-Lagrangian or a CFD/DEM framework to predict the behaviour of axi-symmetric rod-like non-spherical particles in wall-bounded flows, shear flows, as well as uniform flows."],"url":"http://arxiv.org/abs/2405.17124v1","category":"physics.flu-dyn"}
{"created":"2024-05-27 12:41:06","title":"The Witten Index of massless $(d+1)$-Dirac-Schr\u00f6dinger Operators","abstract":"We calculate the Witten index of a class of (non-Fredholm) Dirac-Schr\\\"odinger operators over $\\mathbb{R}^{d+1}$ for $d\\geq 3$ odd, and thus generalize known results for the case $d=1$. For a concrete example of the potential, we give a more explicit index formula, showing that the Witten index assumes any real number on this class of operators.","sentences":["We calculate the Witten index of a class of (non-Fredholm) Dirac-Schr\\\"odinger operators over $\\mathbb{R}^{d+1}$ for $d\\geq 3$ odd, and thus generalize known results for the case $d=1$. For a concrete example of the potential, we give a more explicit index formula, showing that the Witten index assumes any real number on this class of operators."],"url":"http://arxiv.org/abs/2405.17123v1","category":"math.FA"}
{"created":"2024-05-27 12:40:01","title":"Relative stationary dynamical systems","abstract":"Let $G$ be a locally compact second countable group equipped with an admissible non-degenerate Borel probability measure $\\mu$. We generalize the notion of $\\mu$-stationary systems to $\\mu$-stationary $G$-factor maps $\\pi: (X,\\nu)\\to (Y,\\eta)$. For these stationary relations between dynamical systems, we provide a structure theorem, which generalizes the structure theorem of Furstenberg-Glasner. Furthermore, we show the existence and uniqueness of a relative version of the Poisson boundary in this setup.","sentences":["Let $G$ be a locally compact second countable group equipped with an admissible non-degenerate Borel probability measure $\\mu$. We generalize the notion of $\\mu$-stationary systems to $\\mu$-stationary $G$-factor maps $\\pi: (X,\\nu)\\to (Y,\\eta)$. For these stationary relations between dynamical systems, we provide a structure theorem, which generalizes the structure theorem of Furstenberg-Glasner.","Furthermore, we show the existence and uniqueness of a relative version of the Poisson boundary in this setup."],"url":"http://arxiv.org/abs/2405.17122v1","category":"math.DS"}
{"created":"2024-05-27 12:39:08","title":"Entanglement signature in quantum work statistics in the slow-driving regime","abstract":"In slowly driven classical systems, work is a stochastic quantity and its probability distribution is known to satisfy the work fluctuation-dissipation relation, which states that the mean and variance of the dissipated work are linearly related. Recently, it was shown that generation of quantum coherence in the instantaneous energy eigenbasis leads to a correction to this linear relation in the slow-driving regime. Here, we go even further by investigating nonclassical features of work fluctuations in setups with more than one system. To do this, we first generalize slow control protocols to encompass multipartite systems, allowing for the generation of quantum correlations during the driving process. Then, focussing on two-qubit systems, we show that entanglement generation leads to a positive contribution to the dissipated work, which is distinct from the quantum correction due to local coherence generation known from previous work. Our results show that entanglement generated during slow control protocols, e.g. as an unavoidable consequence of qubit crosstalk, comes at the cost of increased dissipation.","sentences":["In slowly driven classical systems, work is a stochastic quantity and its probability distribution is known to satisfy the work fluctuation-dissipation relation, which states that the mean and variance of the dissipated work are linearly related.","Recently, it was shown that generation of quantum coherence in the instantaneous energy eigenbasis leads to a correction to this linear relation in the slow-driving regime.","Here, we go even further by investigating nonclassical features of work fluctuations in setups with more than one system.","To do this, we first generalize slow control protocols to encompass multipartite systems, allowing for the generation of quantum correlations during the driving process.","Then, focussing on two-qubit systems, we show that entanglement generation leads to a positive contribution to the dissipated work, which is distinct from the quantum correction due to local coherence generation known from previous work.","Our results show that entanglement generated during slow control protocols, e.g. as an unavoidable consequence of qubit crosstalk, comes at the cost of increased dissipation."],"url":"http://arxiv.org/abs/2405.17121v1","category":"quant-ph"}
{"created":"2024-05-27 12:38:25","title":"Dual VC Dimension Obstructs Sample Compression by Embeddings","abstract":"This work studies embedding of arbitrary VC classes in well-behaved VC classes, focusing particularly on extremal classes. Our main result expresses an impossibility: such embeddings necessarily require a significant increase in dimension. In particular, we prove that for every $d$ there is a class with VC dimension $d$ that cannot be embedded in any extremal class of VC dimension smaller than exponential in $d$.   In addition to its independent interest, this result has an important implication in learning theory, as it reveals a fundamental limitation of one of the most extensively studied approaches to tackling the long-standing sample compression conjecture. Concretely, the approach proposed by Floyd and Warmuth entails embedding any given VC class into an extremal class of a comparable dimension, and then applying an optimal sample compression scheme for extremal classes. However, our results imply that this strategy would in some cases result in a sample compression scheme at least exponentially larger than what is predicted by the sample compression conjecture.   The above implications follow from a general result we prove: any extremal class with VC dimension $d$ has dual VC dimension at most $2d+1$. This bound is exponentially smaller than the classical bound $2^{d+1}-1$ of Assouad, which applies to general concept classes (and is known to be unimprovable for some classes). We in fact prove a stronger result, establishing that $2d+1$ upper bounds the dual Radon number of extremal classes. This theorem represents an abstraction of the classical Radon theorem for convex sets, extending its applicability to a wider combinatorial framework, without relying on the specifics of Euclidean convexity. The proof utilizes the topological method and is primarily based on variants of the Topological Radon Theorem.","sentences":["This work studies embedding of arbitrary VC classes in well-behaved VC classes, focusing particularly on extremal classes.","Our main result expresses an impossibility: such embeddings necessarily require a significant increase in dimension.","In particular, we prove that for every $d$ there is a class with VC dimension $d$ that cannot be embedded in any extremal class of VC dimension smaller than exponential in $d$.   ","In addition to its independent interest, this result has an important implication in learning theory, as it reveals a fundamental limitation of one of the most extensively studied approaches to tackling the long-standing sample compression conjecture.","Concretely, the approach proposed by Floyd and Warmuth entails embedding any given VC class into an extremal class of a comparable dimension, and then applying an optimal sample compression scheme for extremal classes.","However, our results imply that this strategy would in some cases result in a sample compression scheme at least exponentially larger than what is predicted by the sample compression conjecture.   ","The above implications follow from a general result we prove: any extremal class with VC dimension $d$ has dual VC dimension at most $2d+1$. This bound is exponentially smaller than the classical bound $2^{d+1}-1$ of Assouad, which applies to general concept classes (and is known to be unimprovable for some classes).","We in fact prove a stronger result, establishing that $2d+1$ upper bounds the dual Radon number of extremal classes.","This theorem represents an abstraction of the classical Radon theorem for convex sets, extending its applicability to a wider combinatorial framework, without relying on the specifics of Euclidean convexity.","The proof utilizes the topological method and is primarily based on variants of the Topological Radon Theorem."],"url":"http://arxiv.org/abs/2405.17120v1","category":"cs.DM"}
{"created":"2024-05-27 12:35:13","title":"On $\u03c8$-lattices in modular $(\\varphi,\u0393)$-modules","abstract":"Let $F/{\\mathbb Q}_p$ be a finite field extension, let $k$ be a finite field extension of the residue field of $F$. Generalizing the $\\psi$-lattices which Colmez constructed in \\'{e}tale $(\\varphi,\\Gamma)$-modules over $k[[t]][t^{-1}]$, we define, study and exemplify $\\psi$-lattices in \\'{e}tale $(\\varphi,\\Gamma)$-modules over $k[[t_1,\\ldots,t_d]][\\prod_it_i^{-1}]$ for arbitrary $d\\in{\\mathbb N}$.","sentences":["Let $F/{\\mathbb Q}_p$ be a finite field extension, let $k$ be a finite field extension of the residue field of $F$. Generalizing the $\\psi$-lattices which Colmez constructed in \\'{e}tale $(\\varphi,\\Gamma)$-modules over $k[[t]][t^{-1}]$, we define, study and exemplify $\\psi$-lattices in \\'{e}tale $(\\varphi,\\Gamma)$-modules over $k[[t_1,\\ldots,t_d]][\\prod_it_i^{-1}]$ for arbitrary $d\\in{\\mathbb N}$."],"url":"http://arxiv.org/abs/2405.17118v1","category":"math.NT"}
{"created":"2024-05-27 12:33:47","title":"Mixtures of Unsupervised Lexicon Classification","abstract":"This paper presents a mixture version of the method-of-moment unsupervised lexicon classification by an incorporation of a Dirichlet process.","sentences":["This paper presents a mixture version of the method-of-moment unsupervised lexicon classification by an incorporation of a Dirichlet process."],"url":"http://arxiv.org/abs/2405.17116v1","category":"cs.CL"}
{"created":"2024-05-27 12:33:03","title":"Holographic MIMO Systems, Their Channel Estimation and Performance","abstract":"Holographic multiple-input multiple-output (MIMO) systems constitute a promising technology in support of next-generation wireless communications, thus paving the way for a smart programmable radio environment. However, despite its significant potential, further fundamental issues remain to be addressed, such as the acquisition of accurate channel information. Indeed, the conventional angular-domain channel representation is no longer adequate for characterizing the sparsity inherent in holographic MIMO channels. To fill this knowledge gap, in this article, we conceive a decomposition and reconstruction (DeRe)-based framework for facilitating the estimation of sparse channels in holographic MIMOs. In particular, the channel parameters involved in the steering vector, namely the azimuth and elevation angles plus the distance (AED), are decomposed for independently constructing their own covariance matrices. Then, the acquisition of each parameter can be formulated as a compressive sensing (CS) problem by harnessing the covariance matrix associated with each individual parameter. We demonstrate that our solution exhibits an improved performance and imposes a reduced pilot overhead, despite its reduced complexity. Finally, promising open research topics are highlighted to bridge the gap between the theory and the practical employment of holographic MIMO schemes.","sentences":["Holographic multiple-input multiple-output (MIMO) systems constitute a promising technology in support of next-generation wireless communications, thus paving the way for a smart programmable radio environment.","However, despite its significant potential, further fundamental issues remain to be addressed, such as the acquisition of accurate channel information.","Indeed, the conventional angular-domain channel representation is no longer adequate for characterizing the sparsity inherent in holographic MIMO channels.","To fill this knowledge gap, in this article, we conceive a decomposition and reconstruction (DeRe)-based framework for facilitating the estimation of sparse channels in holographic MIMOs.","In particular, the channel parameters involved in the steering vector, namely the azimuth and elevation angles plus the distance (AED), are decomposed for independently constructing their own covariance matrices.","Then, the acquisition of each parameter can be formulated as a compressive sensing (CS) problem by harnessing the covariance matrix associated with each individual parameter.","We demonstrate that our solution exhibits an improved performance and imposes a reduced pilot overhead, despite its reduced complexity.","Finally, promising open research topics are highlighted to bridge the gap between the theory and the practical employment of holographic MIMO schemes."],"url":"http://arxiv.org/abs/2405.17114v1","category":"cs.IT"}
{"created":"2024-05-27 12:28:17","title":"Diffusion Bridge AutoEncoders for Unsupervised Representation Learning","abstract":"Diffusion-based representation learning has achieved substantial attention due to its promising capabilities in latent representation and sample generation. Recent studies have employed an auxiliary encoder to identify a corresponding representation from a sample and to adjust the dimensionality of a latent variable z. Meanwhile, this auxiliary structure invokes information split problem because the diffusion and the auxiliary encoder would divide the information from the sample into two representations for each model. Particularly, the information modeled by the diffusion becomes over-regularized because of the static prior distribution on xT. To address this problem, we introduce Diffusion Bridge AuteEncoders (DBAE), which enable z-dependent endpoint xT inference through a feed-forward architecture. This structure creates an information bottleneck at z, so xT becomes dependent on z in its generation. This results in two consequences: 1) z holds the full information of samples, and 2) xT becomes a learnable distribution, not static any further. We propose an objective function for DBAE to enable both reconstruction and generative modeling, with their theoretical justification. Empirical evidence supports the effectiveness of the intended design in DBAE, which notably enhances downstream inference quality, reconstruction, and disentanglement. Additionally, DBAE generates high-fidelity samples in the unconditional generation.","sentences":["Diffusion-based representation learning has achieved substantial attention due to its promising capabilities in latent representation and sample generation.","Recent studies have employed an auxiliary encoder to identify a corresponding representation from a sample and to adjust the dimensionality of a latent variable z.","Meanwhile, this auxiliary structure invokes information split problem because the diffusion and the auxiliary encoder would divide the information from the sample into two representations for each model.","Particularly, the information modeled by the diffusion becomes over-regularized because of the static prior distribution on xT. To address this problem, we introduce Diffusion Bridge AuteEncoders (DBAE), which enable z-dependent endpoint xT inference through a feed-forward architecture.","This structure creates an information bottleneck at z, so xT becomes dependent on z in its generation.","This results in two consequences: 1) z holds the full information of samples, and 2) xT becomes a learnable distribution, not static any further.","We propose an objective function for DBAE to enable both reconstruction and generative modeling, with their theoretical justification.","Empirical evidence supports the effectiveness of the intended design in DBAE, which notably enhances downstream inference quality, reconstruction, and disentanglement.","Additionally, DBAE generates high-fidelity samples in the unconditional generation."],"url":"http://arxiv.org/abs/2405.17111v1","category":"cs.LG"}
{"created":"2024-05-27 12:26:49","title":"Superpixelwise Low-rank Approximation based Partial Label Learning for Hyperspectral Image Classification","abstract":"Insufficient prior knowledge of a captured hyperspectral image (HSI) scene may lead the experts or the automatic labeling systems to offer incorrect labels or ambiguous labels (i.e., assigning each training sample to a group of candidate labels, among which only one of them is valid; this is also known as partial label learning) during the labeling process. Accordingly, how to learn from such data with ambiguous labels is a problem of great practical importance. In this paper, we propose a novel superpixelwise low-rank approximation (LRA)-based partial label learning method, namely SLAP, which is the first to take into account partial label learning in HSI classification. SLAP is mainly composed of two phases: disambiguating the training labels and acquiring the predictive model. Specifically, in the first phase, we propose a superpixelwise LRA-based model, preparing the affinity graph for the subsequent label propagation process while extracting the discriminative representation to enhance the following classification task of the second phase. Then to disambiguate the training labels, label propagation propagates the labeling information via the affinity graph of training pixels. In the second phase, we take advantage of the resulting disambiguated training labels and the discriminative representations to enhance the classification performance. The extensive experiments validate the advantage of the proposed SLAP method over state-of-the-art methods.","sentences":["Insufficient prior knowledge of a captured hyperspectral image (HSI) scene may lead the experts or the automatic labeling systems to offer incorrect labels or ambiguous labels (i.e., assigning each training sample to a group of candidate labels, among which only one of them is valid; this is also known as partial label learning) during the labeling process.","Accordingly, how to learn from such data with ambiguous labels is a problem of great practical importance.","In this paper, we propose a novel superpixelwise low-rank approximation (LRA)-based partial label learning method, namely SLAP, which is the first to take into account partial label learning in HSI classification.","SLAP is mainly composed of two phases: disambiguating the training labels and acquiring the predictive model.","Specifically, in the first phase, we propose a superpixelwise LRA-based model, preparing the affinity graph for the subsequent label propagation process while extracting the discriminative representation to enhance the following classification task of the second phase.","Then to disambiguate the training labels, label propagation propagates the labeling information via the affinity graph of training pixels.","In the second phase, we take advantage of the resulting disambiguated training labels and the discriminative representations to enhance the classification performance.","The extensive experiments validate the advantage of the proposed SLAP method over state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.17110v1","category":"cs.CV"}
{"created":"2024-05-27 12:25:34","title":"Left-Linear Completion with AC Axioms","abstract":"We revisit completion modulo equational theories for left-linear term rewrite systems where unification modulo the theory is avoided and the normal rewrite relation can be used in order to decide validity questions. To that end, we give a new correctness proof for finite runs and establish a simulation result between the two inference systems known from the literature. Given a concrete reduction order, novel canonicity results show that the resulting complete systems are unique up to the representation of their rules' right-hand sides. Furthermore, we show how left-linear AC completion can be simulated by general AC completion. In particular, this result allows us to switch from the former to the latter at any point during a completion process.","sentences":["We revisit completion modulo equational theories for left-linear term rewrite systems where unification modulo the theory is avoided and the normal rewrite relation can be used in order to decide validity questions.","To that end, we give a new correctness proof for finite runs and establish a simulation result between the two inference systems known from the literature.","Given a concrete reduction order, novel canonicity results show that the resulting complete systems are unique up to the representation of their rules' right-hand sides.","Furthermore, we show how left-linear AC completion can be simulated by general AC completion.","In particular, this result allows us to switch from the former to the latter at any point during a completion process."],"url":"http://arxiv.org/abs/2405.17109v1","category":"cs.LO"}
{"created":"2024-05-27 12:24:14","title":"Finding good policies in average-reward Markov Decision Processes without prior knowledge","abstract":"We revisit the identification of an $\\varepsilon$-optimal policy in average-reward Markov Decision Processes (MDP). In such MDPs, two measures of complexity have appeared in the literature: the diameter, $D$, and the optimal bias span, $H$, which satisfy $H\\leq D$. Prior work have studied the complexity of $\\varepsilon$-optimal policy identification only when a generative model is available. In this case, it is known that there exists an MDP with $D \\simeq H$ for which the sample complexity to output an $\\varepsilon$-optimal policy is $\\Omega(SAD/\\varepsilon^2)$ where $S$ and $A$ are the sizes of the state and action spaces. Recently, an algorithm with a sample complexity of order $SAH/\\varepsilon^2$ has been proposed, but it requires the knowledge of $H$. We first show that the sample complexity required to estimate $H$ is not bounded by any function of $S,A$ and $H$, ruling out the possibility to easily make the previous algorithm agnostic to $H$. By relying instead on a diameter estimation procedure, we propose the first algorithm for $(\\varepsilon,\\delta)$-PAC policy identification that does not need any form of prior knowledge on the MDP. Its sample complexity scales in $SAD/\\varepsilon^2$ in the regime of small $\\varepsilon$, which is near-optimal. In the online setting, our first contribution is a lower bound which implies that a sample complexity polynomial in $H$ cannot be achieved in this setting. Then, we propose an online algorithm with a sample complexity in $SAD^2/\\varepsilon^2$, as well as a novel approach based on a data-dependent stopping rule that we believe is promising to further reduce this bound.","sentences":["We revisit the identification of an $\\varepsilon$-optimal policy in average-reward Markov Decision Processes (MDP).","In such MDPs, two measures of complexity have appeared in the literature: the diameter, $D$, and the optimal bias span, $H$, which satisfy $H\\leq D$. Prior work have studied the complexity of $\\varepsilon$-optimal policy identification only when a generative model is available.","In this case, it is known that there exists an MDP with $D \\simeq H$ for which the sample complexity to output an $\\varepsilon$-optimal policy is $\\Omega(SAD/\\varepsilon^2)$ where $S$ and $A$ are the sizes of the state and action spaces.","Recently, an algorithm with a sample complexity of order $SAH/\\varepsilon^2$ has been proposed, but it requires the knowledge of $H$. We first show that the sample complexity required to estimate $H$ is not bounded by any function of $S,A$ and $H$, ruling out the possibility to easily make the previous algorithm agnostic to $H$. By relying instead on a diameter estimation procedure, we propose the first algorithm for $(\\varepsilon,\\delta)$-PAC policy identification that does not need any form of prior knowledge on the MDP.","Its sample complexity scales in $SAD/\\varepsilon^2$ in the regime of small $\\varepsilon$, which is near-optimal.","In the online setting, our first contribution is a lower bound which implies that a sample complexity polynomial in $H$ cannot be achieved in this setting.","Then, we propose an online algorithm with a sample complexity in $SAD^2/\\varepsilon^2$, as well as a novel approach based on a data-dependent stopping rule that we believe is promising to further reduce this bound."],"url":"http://arxiv.org/abs/2405.17108v1","category":"cs.LG"}
{"created":"2024-05-27 12:23:59","title":"A sharp quantitative estimate of critical sets","abstract":"The paper establishes a sharp quantitative estimate for the $(d-1)$-Hausdorff measure of the critical set of $\\mathcal{C}^1$ vector-valued functions on $\\mathbb{R}^d$. Additionally, we prove that for a generic $\\mathcal{C}^2$ function where ``generic\" is understood in the topological sense of Baire category, the critical set has a locally finite $(d-1)$-Hausdorff measure.","sentences":["The paper establishes a sharp quantitative estimate for the $(d-1)$-Hausdorff measure of the critical set of $\\mathcal{C}^1$ vector-valued functions on $\\mathbb{R}^d$. Additionally, we prove that for a generic $\\mathcal{C}^2$ function where ``generic\" is understood in the topological sense of Baire category, the critical set has a locally finite $(d-1)$-Hausdorff measure."],"url":"http://arxiv.org/abs/2405.17107v1","category":"math.FA"}
{"created":"2024-05-27 12:23:45","title":"Commutator-based operator splitting for linear port-Hamiltonian systems","abstract":"The port-Hamiltonian approach offers a modeling of dynamic systems with an energy-conserving and a dissipative part. Port-Hamiltonian (pH) systems are passive. That means no energy can be generated within the system. A passive system cannot store more energy than it receives. The exact solution of the pH system fulfills the dissipation inequality. In this paper, we deal with operator splitting that considers the energy-conserving and dissipative parts separately. We aim at high-order splitting schemes that preserve the dissipation inequality. Fourth-order methods for linear pHs-ODE are derived and an extension to sixth-order methods is discussed.","sentences":["The port-Hamiltonian approach offers a modeling of dynamic systems with an energy-conserving and a dissipative part.","Port-Hamiltonian (pH) systems are passive.","That means no energy can be generated within the system.","A passive system cannot store more energy than it receives.","The exact solution of the pH system fulfills the dissipation inequality.","In this paper, we deal with operator splitting that considers the energy-conserving and dissipative parts separately.","We aim at high-order splitting schemes that preserve the dissipation inequality.","Fourth-order methods for linear pHs-ODE are derived and an extension to sixth-order methods is discussed."],"url":"http://arxiv.org/abs/2405.17106v1","category":"math-ph"}
{"created":"2024-05-27 12:23:08","title":"LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding","abstract":"Visual grounding is an essential tool that links user-provided text queries with query-specific regions within an image. Despite advancements in visual grounding models, their ability to comprehend complex queries remains limited. To overcome this limitation, we introduce LLM-Optic, an innovative method that utilizes Large Language Models (LLMs) as an optical lens to enhance existing visual grounding models in comprehending complex text queries involving intricate text structures, multiple objects, or object spatial relationships, situations that current models struggle with. LLM-Optic first employs an LLM as a Text Grounder to interpret complex text queries and accurately identify objects the user intends to locate. Then a pre-trained visual grounding model is used to generate candidate bounding boxes given the refined query by the Text Grounder. After that, LLM-Optic annotates the candidate bounding boxes with numerical marks to establish a connection between text and specific image regions, thereby linking two distinct modalities. Finally, it employs a Large Multimodal Model (LMM) as a Visual Grounder to select the marked candidate objects that best correspond to the original text query. Through LLM-Optic, we have achieved universal visual grounding, which allows for the detection of arbitrary objects specified by arbitrary human language input. Importantly, our method achieves this enhancement without requiring additional training or fine-tuning. Extensive experiments across various challenging benchmarks demonstrate that LLM-Optic achieves state-of-the-art zero-shot visual grounding capabilities.","sentences":["Visual grounding is an essential tool that links user-provided text queries with query-specific regions within an image.","Despite advancements in visual grounding models, their ability to comprehend complex queries remains limited.","To overcome this limitation, we introduce LLM-Optic, an innovative method that utilizes Large Language Models (LLMs) as an optical lens to enhance existing visual grounding models in comprehending complex text queries involving intricate text structures, multiple objects, or object spatial relationships, situations that current models struggle with.","LLM-Optic first employs an LLM as a Text Grounder to interpret complex text queries and accurately identify objects the user intends to locate.","Then a pre-trained visual grounding model is used to generate candidate bounding boxes given the refined query by the Text Grounder.","After that, LLM-Optic annotates the candidate bounding boxes with numerical marks to establish a connection between text and specific image regions, thereby linking two distinct modalities.","Finally, it employs a Large Multimodal Model (LMM) as a Visual Grounder to select the marked candidate objects that best correspond to the original text query.","Through LLM-Optic, we have achieved universal visual grounding, which allows for the detection of arbitrary objects specified by arbitrary human language input.","Importantly, our method achieves this enhancement without requiring additional training or fine-tuning.","Extensive experiments across various challenging benchmarks demonstrate that LLM-Optic achieves state-of-the-art zero-shot visual grounding capabilities."],"url":"http://arxiv.org/abs/2405.17104v1","category":"cs.CV"}
{"created":"2024-05-27 12:21:48","title":"Empowering Character-level Text Infilling by Eliminating Sub-Tokens","abstract":"In infilling tasks, sub-tokens, representing instances where a complete token is segmented into two parts, often emerge at the boundaries of prefixes, middles, and suffixes. Traditional methods focused on training models at the token level, leading to sub-optimal performance in character-level infilling tasks during the inference stage. Alternately, some approaches considered character-level infilling, but they relied on predicting sub-tokens in inference, yet this strategy diminished ability in character-level infilling tasks due to the large perplexity of the model on sub-tokens. In this paper, we introduce FIM-SE, which stands for Fill-In-the-Middle with both Starting and Ending character constraints. The proposed method addresses character-level infilling tasks by utilizing a line-level format to avoid predicting any sub-token in inference. In addition, we incorporate two special tokens to signify the rest of the incomplete lines, thereby enhancing generation guidance. Extensive experiments demonstrate that our proposed approach surpasses previous methods, offering a significant advantage. Code is available at https://github.com/SenseLLM/FIM-SE.","sentences":["In infilling tasks, sub-tokens, representing instances where a complete token is segmented into two parts, often emerge at the boundaries of prefixes, middles, and suffixes.","Traditional methods focused on training models at the token level, leading to sub-optimal performance in character-level infilling tasks during the inference stage.","Alternately, some approaches considered character-level infilling, but they relied on predicting sub-tokens in inference, yet this strategy diminished ability in character-level infilling tasks due to the large perplexity of the model on sub-tokens.","In this paper, we introduce FIM-SE, which stands for Fill-In-the-Middle with both Starting and Ending character constraints.","The proposed method addresses character-level infilling tasks by utilizing a line-level format to avoid predicting any sub-token in inference.","In addition, we incorporate two special tokens to signify the rest of the incomplete lines, thereby enhancing generation guidance.","Extensive experiments demonstrate that our proposed approach surpasses previous methods, offering a significant advantage.","Code is available at https://github.com/SenseLLM/FIM-SE."],"url":"http://arxiv.org/abs/2405.17103v1","category":"cs.CL"}
{"created":"2024-05-27 12:12:26","title":"Evaluation of Multi-task Uncertainties in Joint Semantic Segmentation and Monocular Depth Estimation","abstract":"While a number of promising uncertainty quantification methods have been proposed to address the prevailing shortcomings of deep neural networks like overconfidence and lack of explainability, quantifying predictive uncertainties in the context of joint semantic segmentation and monocular depth estimation has not been explored yet. Since many real-world applications are multi-modal in nature and, hence, have the potential to benefit from multi-task learning, this is a substantial gap in current literature. To this end, we conduct a comprehensive series of experiments to study how multi-task learning influences the quality of uncertainty estimates in comparison to solving both tasks separately.","sentences":["While a number of promising uncertainty quantification methods have been proposed to address the prevailing shortcomings of deep neural networks like overconfidence and lack of explainability, quantifying predictive uncertainties in the context of joint semantic segmentation and monocular depth estimation has not been explored yet.","Since many real-world applications are multi-modal in nature and, hence, have the potential to benefit from multi-task learning, this is a substantial gap in current literature.","To this end, we conduct a comprehensive series of experiments to study how multi-task learning influences the quality of uncertainty estimates in comparison to solving both tasks separately."],"url":"http://arxiv.org/abs/2405.17097v1","category":"cs.CV"}
{"created":"2024-05-27 12:11:13","title":"Projective modules over Rees-like algebras and its monoid extensions","abstract":"Let $A$ be a Rees-like algebra of dimension $d$ and $N$ a commutative partially cancellative torsion-free seminormal monoid. We prove the following results. \\begin{enumerate}   \\item Let $P$ be a finitely generated projective $A$-module of $\\rank\\geq d$. Then $(i)$ $P$ has a unimodular element; $(ii)$ The action of $\\EL(A\\oplus P)$ on $\\Um(A\\oplus P)$ is transitive.   \\item Let $P$ be a finitely generated projective $A[N]$-module of $\\rank~r$. Then $(i)$ $P$ has a unimodular element for $r\\geq\\max\\{3,d\\}$; $(ii)$ The action of $\\EL(A[N]\\oplus P)$ on $\\Um(A[N]\\oplus P)$ is transitive for $r\\geq\\max\\{2,d\\}$. \\end{enumerate} These improve the classical results of Serre \\cite{Se58} and Bass \\cite{Ba64}.","sentences":["Let $A$ be a Rees-like algebra of dimension $d$ and $N$ a commutative partially cancellative torsion-free seminormal monoid.","We prove the following results.","\\begin{enumerate}   \\item Let $P$ be a finitely generated projective $A$-module of $\\rank\\geq d$. Then $(i)$ $P$ has a unimodular element; $(ii)$ The action of $\\EL(A\\oplus P)$ on $\\Um(A\\oplus P)$ is transitive.   ","\\item Let $P$ be a finitely generated projective $A[N]$-module of $\\rank~r$.","Then $(i)$ $P$ has a unimodular element for $r\\geq\\max\\{3,d\\}$; $(ii)$ The action of $\\EL(A[N]\\oplus P)$ on $\\Um(A[N]\\oplus P)$ is transitive for $r\\geq\\max\\{2,d\\}$. \\end{enumerate} These improve the classical results of Serre \\cite{Se58} and Bass \\cite{Ba64}."],"url":"http://arxiv.org/abs/2405.17096v1","category":"math.AC"}
{"created":"2024-05-27 12:08:49","title":"Hydrodynamic modes in holographic multiple-axion model","abstract":"In this paper we investigate the shear viscoelasticity and the hydrodynamic modes in a holographic solid model with several sets of axions that all break the translations spontaneously on boundary. Comparing with the single-axion model, the shear modulus is enhanced at high temperatures and the shear viscosity is always suppressed in the presence of additional axions. However, the different sets of axions exhibit competitive relationship in determining the shear modulus at low temperatures. Furthermore, by calculating the black hole quasi-normal modes, it is found that adding more axions only increases the amount of diffusive modes. The number of the sound modes always remains unchanged.","sentences":["In this paper we investigate the shear viscoelasticity and the hydrodynamic modes in a holographic solid model with several sets of axions that all break the translations spontaneously on boundary.","Comparing with the single-axion model, the shear modulus is enhanced at high temperatures and the shear viscosity is always suppressed in the presence of additional axions.","However, the different sets of axions exhibit competitive relationship in determining the shear modulus at low temperatures.","Furthermore, by calculating the black hole quasi-normal modes, it is found that adding more axions only increases the amount of diffusive modes.","The number of the sound modes always remains unchanged."],"url":"http://arxiv.org/abs/2405.17092v1","category":"hep-th"}
{"created":"2024-05-27 12:04:36","title":"Phase Transitions in the Output Distribution of Large Language Models","abstract":"In a physical system, changing parameters such as temperature can induce a phase transition: an abrupt change from one state of matter to another. Analogous phenomena have recently been observed in large language models. Typically, the task of identifying phase transitions requires human analysis and some prior understanding of the system to narrow down which low-dimensional properties to monitor and analyze. Statistical methods for the automated detection of phase transitions from data have recently been proposed within the physics community. These methods are largely system agnostic and, as shown here, can be adapted to study the behavior of large language models. In particular, we quantify distributional changes in the generated output via statistical distances, which can be efficiently estimated with access to the probability distribution over next-tokens. This versatile approach is capable of discovering new phases of behavior and unexplored transitions -- an ability that is particularly exciting in light of the rapid development of language models and their emergent capabilities.","sentences":["In a physical system, changing parameters such as temperature can induce a phase transition: an abrupt change from one state of matter to another.","Analogous phenomena have recently been observed in large language models.","Typically, the task of identifying phase transitions requires human analysis and some prior understanding of the system to narrow down which low-dimensional properties to monitor and analyze.","Statistical methods for the automated detection of phase transitions from data have recently been proposed within the physics community.","These methods are largely system agnostic and, as shown here, can be adapted to study the behavior of large language models.","In particular, we quantify distributional changes in the generated output via statistical distances, which can be efficiently estimated with access to the probability distribution over next-tokens.","This versatile approach is capable of discovering new phases of behavior and unexplored transitions -- an ability that is particularly exciting in light of the rapid development of language models and their emergent capabilities."],"url":"http://arxiv.org/abs/2405.17088v1","category":"cs.LG"}
{"created":"2024-05-27 11:55:35","title":"Ensembling Diffusion Models via Adaptive Feature Aggregation","abstract":"The success of the text-guided diffusion model has inspired the development and release of numerous powerful diffusion models within the open-source community. These models are typically fine-tuned on various expert datasets, showcasing diverse denoising capabilities. Leveraging multiple high-quality models to produce stronger generation ability is valuable, but has not been extensively studied. Existing methods primarily adopt parameter merging strategies to produce a new static model. However, they overlook the fact that the divergent denoising capabilities of the models may dynamically change across different states, such as when experiencing different prompts, initial noises, denoising steps, and spatial locations. In this paper, we propose a novel ensembling method, Adaptive Feature Aggregation (AFA), which dynamically adjusts the contributions of multiple models at the feature level according to various states (i.e., prompts, initial noises, denoising steps, and spatial locations), thereby keeping the advantages of multiple diffusion models, while suppressing their disadvantages. Specifically, we design a lightweight Spatial-Aware Block-Wise (SABW) feature aggregator that adaptive aggregates the block-wise intermediate features from multiple U-Net denoisers into a unified one. The core idea lies in dynamically producing an individual attention map for each model's features by comprehensively considering various states. It is worth noting that only SABW is trainable with about 50 million parameters, while other models are frozen. Both the quantitative and qualitative experiments demonstrate the effectiveness of our proposed Adaptive Feature Aggregation method. The code is available at https://github.com/tenvence/afa/.","sentences":["The success of the text-guided diffusion model has inspired the development and release of numerous powerful diffusion models within the open-source community.","These models are typically fine-tuned on various expert datasets, showcasing diverse denoising capabilities.","Leveraging multiple high-quality models to produce stronger generation ability is valuable, but has not been extensively studied.","Existing methods primarily adopt parameter merging strategies to produce a new static model.","However, they overlook the fact that the divergent denoising capabilities of the models may dynamically change across different states, such as when experiencing different prompts, initial noises, denoising steps, and spatial locations.","In this paper, we propose a novel ensembling method, Adaptive Feature Aggregation (AFA), which dynamically adjusts the contributions of multiple models at the feature level according to various states (i.e., prompts, initial noises, denoising steps, and spatial locations), thereby keeping the advantages of multiple diffusion models, while suppressing their disadvantages.","Specifically, we design a lightweight Spatial-Aware Block-Wise (SABW) feature aggregator that adaptive aggregates the block-wise intermediate features from multiple U-Net denoisers into a unified one.","The core idea lies in dynamically producing an individual attention map for each model's features by comprehensively considering various states.","It is worth noting that only SABW is trainable with about 50 million parameters, while other models are frozen.","Both the quantitative and qualitative experiments demonstrate the effectiveness of our proposed Adaptive Feature Aggregation method.","The code is available at https://github.com/tenvence/afa/."],"url":"http://arxiv.org/abs/2405.17082v1","category":"cs.CV"}
{"created":"2024-05-27 11:53:47","title":"A Two-Level Stochastic Model for the Lateral Movement of Vehicles Within Their Lane Under Homogeneous Traffic Conditions","abstract":"The lateral position of vehicles within their lane is a decisive factor for the range of vision of vehicle sensors. This, in turn, is crucial for a vehicle's ability to perceive its environment and gain a high situational awareness by processing the collected information. When aiming for increasing levels of vehicle autonomy, this situational awareness becomes more and more important. Thus, when validating an autonomous driving function the representativeness of the submicroscopic behavior such as the lateral offset has to be ensured. With simulations being an essential part of the validation of autonomous driving functions, models describing these phenomena are required. Possible applications are the enhancement of microscopic traffic simulations and the maneuver-based approach for scenario-based testing. This paper presents a two-level stochastic approach to model the lateral movement of vehicles within their lane during road-following maneuvers under homogeneous traffic conditions. A Markov model generates the coarse lateral offset profile. It is superposed with a noise model for the fine movements. Both models are set up using real-world data. The evaluation of the model shows promising qualitative and quantitative results, the potential for enhancements and extreme low computation times (10000 times faster than real time).","sentences":["The lateral position of vehicles within their lane is a decisive factor for the range of vision of vehicle sensors.","This, in turn, is crucial for a vehicle's ability to perceive its environment and gain a high situational awareness by processing the collected information.","When aiming for increasing levels of vehicle autonomy, this situational awareness becomes more and more important.","Thus, when validating an autonomous driving function the representativeness of the submicroscopic behavior such as the lateral offset has to be ensured.","With simulations being an essential part of the validation of autonomous driving functions, models describing these phenomena are required.","Possible applications are the enhancement of microscopic traffic simulations and the maneuver-based approach for scenario-based testing.","This paper presents a two-level stochastic approach to model the lateral movement of vehicles within their lane during road-following maneuvers under homogeneous traffic conditions.","A Markov model generates the coarse lateral offset profile.","It is superposed with a noise model for the fine movements.","Both models are set up using real-world data.","The evaluation of the model shows promising qualitative and quantitative results, the potential for enhancements and extreme low computation times (10000 times faster than real time)."],"url":"http://arxiv.org/abs/2405.17080v1","category":"cs.RO"}
{"created":"2024-05-27 11:47:21","title":"Leveraging small language models for Text2SPARQL tasks to improve the resilience of AI assistance","abstract":"In this work we will show that language models with less than one billion parameters can be used to translate natural language to SPARQL queries after fine-tuning. Using three different datasets ranging from academic to real world, we identify prerequisites that the training data must fulfill in order for the training to be successful. The goal is to empower users of semantic web technology to use AI assistance with affordable commodity hardware, making them more resilient against external factors.","sentences":["In this work we will show that language models with less than one billion parameters can be used to translate natural language to SPARQL queries after fine-tuning.","Using three different datasets ranging from academic to real world, we identify prerequisites that the training data must fulfill in order for the training to be successful.","The goal is to empower users of semantic web technology to use AI assistance with affordable commodity hardware, making them more resilient against external factors."],"url":"http://arxiv.org/abs/2405.17076v1","category":"cs.AI"}
{"created":"2024-05-27 11:42:46","title":"A novel framework for systematic propositional formula simplification based on existential graphs","abstract":"This paper presents a novel simplification calculus for propositional logic derived from Peirce's existential graphs' rules of inference and implication graphs. Our rules can be applied to propositional logic formulae in nested form, are equivalence-preserving, guarantee a monotonically decreasing number of variables, clauses and literals, and maximise the preservation of structural problem information. Our techniques can also be seen as higher-level SAT preprocessing, and we show how one of our rules (TWSR) generalises and streamlines most of the known equivalence-preserving SAT preprocessing methods. In addition, we propose a simplification procedure based on the systematic application of two of our rules (EPR and TWSR) which is solver-agnostic and can be used to simplify large Boolean satisfiability problems and propositional formulae in arbitrary form, and we provide a formal analysis of its algorithmic complexity in terms of space and time. Finally, we show how our rules can be further extended with a novel n-ary implication graph to capture all known equivalence-preserving preprocessing procedures.","sentences":["This paper presents a novel simplification calculus for propositional logic derived from Peirce's existential graphs' rules of inference and implication graphs.","Our rules can be applied to propositional logic formulae in nested form, are equivalence-preserving, guarantee a monotonically decreasing number of variables, clauses and literals, and maximise the preservation of structural problem information.","Our techniques can also be seen as higher-level SAT preprocessing, and we show how one of our rules (TWSR) generalises and streamlines most of the known equivalence-preserving SAT preprocessing methods.","In addition, we propose a simplification procedure based on the systematic application of two of our rules (EPR and TWSR) which is solver-agnostic and can be used to simplify large Boolean satisfiability problems and propositional formulae in arbitrary form, and we provide a formal analysis of its algorithmic complexity in terms of space and time.","Finally, we show how our rules can be further extended with a novel n-ary implication graph to capture all known equivalence-preserving preprocessing procedures."],"url":"http://arxiv.org/abs/2405.17072v1","category":"cs.LO"}
{"created":"2024-05-27 11:41:41","title":"Efficient mid-term forecasting of hourly electricity load using generalized additive models","abstract":"Accurate mid-term (weeks to one year) hourly electricity load forecasts are essential for strategic decision-making in power plant operation, ensuring supply security and grid stability, and energy trading. While numerous models effectively predict short-term (hours to a few days) hourly load, mid-term forecasting solutions remain scarce. In mid-term load forecasting, besides daily, weekly, and annual seasonal and autoregressive effects, capturing weather and holiday effects, as well as socio-economic non-stationarities in the data, poses significant modeling challenges. To address these challenges, we propose a novel forecasting method using Generalized Additive Models (GAMs) built from interpretable P-splines and enhanced with autoregressive post-processing. This model uses smoothed temperatures, Error-Trend-Seasonal (ETS) modeled non-stationary states, a nuanced representation of holiday effects with weekday variations, and seasonal information as input. The proposed model is evaluated on load data from 24 European countries. This analysis demonstrates that the model not only has significantly enhanced forecasting accuracy compared to state-of-the-art methods but also offers valuable insights into the influence of individual components on predicted load, given its full interpretability. Achieving performance akin to day-ahead TSO forecasts in fast computation times of a few seconds for several years of hourly data underscores the model's potential for practical application in the power system industry.","sentences":["Accurate mid-term (weeks to one year) hourly electricity load forecasts are essential for strategic decision-making in power plant operation, ensuring supply security and grid stability, and energy trading.","While numerous models effectively predict short-term (hours to a few days) hourly load, mid-term forecasting solutions remain scarce.","In mid-term load forecasting, besides daily, weekly, and annual seasonal and autoregressive effects, capturing weather and holiday effects, as well as socio-economic non-stationarities in the data, poses significant modeling challenges.","To address these challenges, we propose a novel forecasting method using Generalized Additive Models (GAMs) built from interpretable P-splines and enhanced with autoregressive post-processing.","This model uses smoothed temperatures, Error-Trend-Seasonal (ETS) modeled non-stationary states, a nuanced representation of holiday effects with weekday variations, and seasonal information as input.","The proposed model is evaluated on load data from 24 European countries.","This analysis demonstrates that the model not only has significantly enhanced forecasting accuracy compared to state-of-the-art methods but also offers valuable insights into the influence of individual components on predicted load, given its full interpretability.","Achieving performance akin to day-ahead TSO forecasts in fast computation times of a few seconds for several years of hourly data underscores the model's potential for practical application in the power system industry."],"url":"http://arxiv.org/abs/2405.17070v1","category":"stat.AP"}
{"created":"2024-05-27 11:40:50","title":"Training-free Editioning of Text-to-Image Models","abstract":"Inspired by the software industry's practice of offering different editions or versions of a product tailored to specific user groups or use cases, we propose a novel task, namely, training-free editioning, for text-to-image models. Specifically, we aim to create variations of a base text-to-image model without retraining, enabling the model to cater to the diverse needs of different user groups or to offer distinct features and functionalities. To achieve this, we propose that different editions of a given text-to-image model can be formulated as concept subspaces in the latent space of its text encoder (e.g., CLIP). In such a concept subspace, all points satisfy a specific user need (e.g., generating images of a cat lying on the grass/ground/falling leaves). Technically, we apply Principal Component Analysis (PCA) to obtain the desired concept subspaces from representative text embedding that correspond to a specific user need or requirement. Projecting the text embedding of a given prompt into these low-dimensional subspaces enables efficient model editioning without retraining. Intuitively, our proposed editioning paradigm enables a service provider to customize the base model into its \"cat edition\" (or other editions) that restricts image generation to cats, regardless of the user's prompt (e.g., dogs, people, etc.). This introduces a new dimension for product differentiation, targeted functionality, and pricing strategies, unlocking novel business models for text-to-image generators. Extensive experimental results demonstrate the validity of our approach and its potential to enable a wide range of customized text-to-image model editions across various domains and applications.","sentences":["Inspired by the software industry's practice of offering different editions or versions of a product tailored to specific user groups or use cases, we propose a novel task, namely, training-free editioning, for text-to-image models.","Specifically, we aim to create variations of a base text-to-image model without retraining, enabling the model to cater to the diverse needs of different user groups or to offer distinct features and functionalities.","To achieve this, we propose that different editions of a given text-to-image model can be formulated as concept subspaces in the latent space of its text encoder (e.g., CLIP).","In such a concept subspace, all points satisfy a specific user need (e.g., generating images of a cat lying on the grass/ground/falling leaves).","Technically, we apply Principal Component Analysis (PCA) to obtain the desired concept subspaces from representative text embedding that correspond to a specific user need or requirement.","Projecting the text embedding of a given prompt into these low-dimensional subspaces enables efficient model editioning without retraining.","Intuitively, our proposed editioning paradigm enables a service provider to customize the base model into its \"cat edition\" (or other editions) that restricts image generation to cats, regardless of the user's prompt (e.g., dogs, people, etc.).","This introduces a new dimension for product differentiation, targeted functionality, and pricing strategies, unlocking novel business models for text-to-image generators.","Extensive experimental results demonstrate the validity of our approach and its potential to enable a wide range of customized text-to-image model editions across various domains and applications."],"url":"http://arxiv.org/abs/2405.17069v1","category":"cs.CV"}
{"created":"2024-05-27 11:40:42","title":"The Poisson Midpoint Method for Langevin Dynamics: Provably Efficient Discretization for Diffusion Models","abstract":"Langevin Dynamics is a Stochastic Differential Equation (SDE) central to sampling and generative modeling and is implemented via time discretization. Langevin Monte Carlo (LMC), based on the Euler-Maruyama discretization, is the simplest and most studied algorithm. LMC can suffer from slow convergence - requiring a large number of steps of small step-size to obtain good quality samples. This becomes stark in the case of diffusion models where a large number of steps gives the best samples, but the quality degrades rapidly with smaller number of steps. Randomized Midpoint Method has been recently proposed as a better discretization of Langevin dynamics for sampling from strongly log-concave distributions. However, important applications such as diffusion models involve non-log concave densities and contain time varying drift. We propose its variant, the Poisson Midpoint Method, which approximates a small step-size LMC with large step-sizes. We prove that this can obtain a quadratic speed up of LMC under very weak assumptions. We apply our method to diffusion models for image generation and show that it maintains the quality of DDPM with 1000 neural network calls with just 50-80 neural network calls and outperforms ODE based methods with similar compute.","sentences":["Langevin Dynamics is a Stochastic Differential Equation (SDE) central to sampling and generative modeling and is implemented via time discretization.","Langevin Monte Carlo (LMC), based on the Euler-Maruyama discretization, is the simplest and most studied algorithm.","LMC can suffer from slow convergence - requiring a large number of steps of small step-size to obtain good quality samples.","This becomes stark in the case of diffusion models where a large number of steps gives the best samples, but the quality degrades rapidly with smaller number of steps.","Randomized Midpoint Method has been recently proposed as a better discretization of Langevin dynamics for sampling from strongly log-concave distributions.","However, important applications such as diffusion models involve non-log concave densities and contain time varying drift.","We propose its variant, the Poisson Midpoint Method, which approximates a small step-size LMC with large step-sizes.","We prove that this can obtain a quadratic speed up of LMC under very weak assumptions.","We apply our method to diffusion models for image generation and show that it maintains the quality of DDPM with 1000 neural network calls with just 50-80 neural network calls and outperforms ODE based methods with similar compute."],"url":"http://arxiv.org/abs/2405.17068v1","category":"cs.LG"}
{"created":"2024-05-27 11:39:59","title":"Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization","abstract":"Large Language Models (LLMs) have shown remarkable capabilities in language understanding and generation. Nonetheless, it was also witnessed that LLMs tend to produce inaccurate responses to specific queries. This deficiency can be traced to the tokenization step LLMs must undergo, which is an inevitable limitation inherent to all LLMs. In fact, incorrect tokenization is the critical point that hinders LLMs in understanding the input precisely, thus leading to unsatisfactory output. To demonstrate this flaw of LLMs, we construct an adversarial dataset, named as $\\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which draws upon the vocabularies of various open-source LLMs to challenge LLMs' tokenization. ADT consists of two subsets: the manually constructed ADT-Human and the automatically generated ADT-Auto. Our empirical results reveal that our ADT is highly effective on challenging the tokenization of leading LLMs, including GPT-4o, Llama-3, Qwen2.5-max and so on, thus degrading these LLMs' capabilities. Moreover, our method of automatic data generation has been proven efficient and robust, which can be applied to any open-source LLMs. To the best of our knowledge, our study is the first to investigating LLMs' vulnerability in terms of challenging their token segmentation, which will shed light on the subsequent research of improving LLMs' capabilities through optimizing their tokenization process and algorithms.","sentences":["Large Language Models (LLMs) have shown remarkable capabilities in language understanding and generation.","Nonetheless, it was also witnessed that LLMs tend to produce inaccurate responses to specific queries.","This deficiency can be traced to the tokenization step LLMs must undergo, which is an inevitable limitation inherent to all LLMs.","In fact, incorrect tokenization is the critical point that hinders LLMs in understanding the input precisely, thus leading to unsatisfactory output.","To demonstrate this flaw of LLMs, we construct an adversarial dataset, named as $\\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which draws upon the vocabularies of various open-source LLMs to challenge LLMs' tokenization.","ADT consists of two subsets: the manually constructed ADT-Human and the automatically generated ADT-Auto.","Our empirical results reveal that our ADT is highly effective on challenging the tokenization of leading LLMs, including GPT-4o, Llama-3, Qwen2.5-max and so on, thus degrading these LLMs' capabilities.","Moreover, our method of automatic data generation has been proven efficient and robust, which can be applied to any open-source LLMs.","To the best of our knowledge, our study is the first to investigating LLMs' vulnerability in terms of challenging their token segmentation, which will shed light on the subsequent research of improving LLMs' capabilities through optimizing their tokenization process and algorithms."],"url":"http://arxiv.org/abs/2405.17067v1","category":"cs.CL"}
{"created":"2024-05-27 11:37:36","title":"Saturn: Sample-efficient Generative Molecular Design using Memory Manipulation","abstract":"Generative molecular design for drug discovery has very recently achieved a wave of experimental validation, with language-based backbones being the most common architectures employed. The most important factor for downstream success is whether an in silico oracle is well correlated with the desired end-point. To this end, current methods use cheaper proxy oracles with higher throughput before evaluating the most promising subset with high-fidelity oracles. The ability to directly optimize high-fidelity oracles would greatly enhance generative design and be expected to improve hit rates. However, current models are not efficient enough to consider such a prospect, exemplifying the sample efficiency problem. In this work, we introduce Saturn, which leverages the Augmented Memory algorithm and demonstrates the first application of the Mamba architecture for generative molecular design. We elucidate how experience replay with data augmentation improves sample efficiency and how Mamba synergistically exploits this mechanism. Saturn outperforms 22 models on multi-parameter optimization tasks relevant to drug discovery and may possess sufficient sample efficiency to consider the prospect of directly optimizing high-fidelity oracles.","sentences":["Generative molecular design for drug discovery has very recently achieved a wave of experimental validation, with language-based backbones being the most common architectures employed.","The most important factor for downstream success is whether an in silico oracle is well correlated with the desired end-point.","To this end, current methods use cheaper proxy oracles with higher throughput before evaluating the most promising subset with high-fidelity oracles.","The ability to directly optimize high-fidelity oracles would greatly enhance generative design and be expected to improve hit rates.","However, current models are not efficient enough to consider such a prospect, exemplifying the sample efficiency problem.","In this work, we introduce Saturn, which leverages the Augmented Memory algorithm and demonstrates the first application of the Mamba architecture for generative molecular design.","We elucidate how experience replay with data augmentation improves sample efficiency and how Mamba synergistically exploits this mechanism.","Saturn outperforms 22 models on multi-parameter optimization tasks relevant to drug discovery and may possess sufficient sample efficiency to consider the prospect of directly optimizing high-fidelity oracles."],"url":"http://arxiv.org/abs/2405.17066v1","category":"q-bio.BM"}
{"created":"2024-05-27 11:35:46","title":"The Probability of Improved Prediction: a new concept in statistical inference","abstract":"In an attempt to provide an answer to the increasing criticism against p-values and to bridge the gap between statistical inference and prediction modelling, we introduce the probability of improved prediction (PIP). In general, the PIP is a probabilistic measure for comparing two competing models. Three versions of the PIP and several estimators are introduced and the relationships between them, p-values and the mean squared error are investigated. The performance of the estimators is assessed in a simulation study. An application shows how the PIP can support p-values to strengthen the conclusions or possibly point at issues with e.g. replicability.","sentences":["In an attempt to provide an answer to the increasing criticism against p-values and to bridge the gap between statistical inference and prediction modelling, we introduce the probability of improved prediction (PIP).","In general, the PIP is a probabilistic measure for comparing two competing models.","Three versions of the PIP and several estimators are introduced and the relationships between them, p-values and the mean squared error are investigated.","The performance of the estimators is assessed in a simulation study.","An application shows how the PIP can support p-values to strengthen the conclusions or possibly point at issues with e.g. replicability."],"url":"http://arxiv.org/abs/2405.17064v1","category":"stat.ME"}
{"created":"2024-05-27 11:33:47","title":"On Laplace equation solution in orthogonal similar oblate spheroidal coordinates","abstract":"Orthogonal coordinate systems enable expressing the boundary conditions of differential equations in accord with the physical boundaries of the problem. It can significantly simplify calculations. The orthogonal similar oblate spheroidal (SOS) coordinate system can be particularly useful for a physical processes description inside or in the vicinity of the bodies or particles with the geometry of an oblate spheroid. The interior solution of the Laplace equation in the SOS coordinates was recently found; however, the exterior solution was missing. The exterior solution of the azimuthally symmetric Laplace equation in the SOS coordinates is derived. In the steps leading to this solution, important formulas of the SOS algebra are found. Various forms of the Laplace operator in the SOS coordinates in azimuthally symmetric case are shown. General transformation between two different SOS coordinate systems is derived. It is determined that the SOS harmonics are physically the same as the solid harmonics. Further, a formula expressing any generalized Legendre polynomial as a finite sum of monomials is found. The reported relations have potential application in geophysics, astrophysics, electrostatics and solid state physics (e.g. ferroic inclusions).","sentences":["Orthogonal coordinate systems enable expressing the boundary conditions of differential equations in accord with the physical boundaries of the problem.","It can significantly simplify calculations.","The orthogonal similar oblate spheroidal (SOS) coordinate system can be particularly useful for a physical processes description inside or in the vicinity of the bodies or particles with the geometry of an oblate spheroid.","The interior solution of the Laplace equation in the SOS coordinates was recently found; however, the exterior solution was missing.","The exterior solution of the azimuthally symmetric Laplace equation in the SOS coordinates is derived.","In the steps leading to this solution, important formulas of the SOS algebra are found.","Various forms of the Laplace operator in the SOS coordinates in azimuthally symmetric case are shown.","General transformation between two different SOS coordinate systems is derived.","It is determined that the SOS harmonics are physically the same as the solid harmonics.","Further, a formula expressing any generalized Legendre polynomial as a finite sum of monomials is found.","The reported relations have potential application in geophysics, astrophysics, electrostatics and solid state physics (e.g. ferroic inclusions)."],"url":"http://arxiv.org/abs/2405.17063v1","category":"physics.class-ph"}
{"created":"2024-05-27 11:31:58","title":"Unifying Demonstration Selection and Compression for In-Context Learning","abstract":"In-context learning (ICL) facilitates large language models (LLMs) exhibiting spectacular emergent capabilities in various scenarios. Unfortunately, introducing demonstrations easily makes the prompt length explode, bringing a significant burden to hardware. In addition, random demonstrations usually achieve limited improvements in ICL, necessitating demonstration selection among accessible candidates. Previous studies introduce extra modules to perform demonstration compression or selection independently. In this paper, we propose an ICL framework UniICL, which Unifies demonstration selection and compression, and final response generation via a single frozen LLM. Specifically, UniICL first projects actual demonstrations and inference text inputs into short virtual tokens, respectively. Then, virtual tokens are applied to select suitable demonstrations by measuring semantic similarity within latent space among candidate demonstrations and inference input. Finally, inference text inputs together with selected virtual demonstrations are fed into the same frozen LLM for response generation. Notably, UniICL is a parameter-efficient framework that only contains 17M trainable parameters originating from the projection layer. We conduct experiments and analysis over in- and out-domain datasets of both generative and understanding tasks, encompassing ICL scenarios with plentiful and limited demonstration candidates. Results show that UniICL effectively unifies $12 \\times$ compression, demonstration selection, and response generation, efficiently scaling up the baseline from 4-shot to 64-shot ICL in IMDb with 24 GB CUDA allocation","sentences":["In-context learning (ICL) facilitates large language models (LLMs) exhibiting spectacular emergent capabilities in various scenarios.","Unfortunately, introducing demonstrations easily makes the prompt length explode, bringing a significant burden to hardware.","In addition, random demonstrations usually achieve limited improvements in ICL, necessitating demonstration selection among accessible candidates.","Previous studies introduce extra modules to perform demonstration compression or selection independently.","In this paper, we propose an ICL framework UniICL, which Unifies demonstration selection and compression, and final response generation via a single frozen LLM.","Specifically, UniICL first projects actual demonstrations and inference text inputs into short virtual tokens, respectively.","Then, virtual tokens are applied to select suitable demonstrations by measuring semantic similarity within latent space among candidate demonstrations and inference input.","Finally, inference text inputs together with selected virtual demonstrations are fed into the same frozen LLM for response generation.","Notably, UniICL is a parameter-efficient framework that only contains 17M trainable parameters originating from the projection layer.","We conduct experiments and analysis over in- and out-domain datasets of both generative and understanding tasks, encompassing ICL scenarios with plentiful and limited demonstration candidates.","Results show that UniICL effectively unifies $12 \\times$ compression, demonstration selection, and response generation, efficiently scaling up the baseline from 4-shot to 64-shot ICL in IMDb with 24 GB CUDA allocation"],"url":"http://arxiv.org/abs/2405.17062v1","category":"cs.CL"}
{"created":"2024-05-27 11:31:08","title":"Graph Neural Networks on Quantum Computers","abstract":"Graph Neural Networks (GNNs) are powerful machine learning models that excel at analyzing structured data represented as graphs, demonstrating remarkable performance in applications like social network analysis and recommendation systems. However, classical GNNs face scalability challenges when dealing with large-scale graphs. This paper proposes frameworks for implementing GNNs on quantum computers to potentially address the challenges. We devise quantum algorithms corresponding to the three fundamental types of classical GNNs: Graph Convolutional Networks, Graph Attention Networks, and Message-Passing GNNs. A complexity analysis of our quantum implementation of the Simplified Graph Convolutional (SGC) Network shows potential quantum advantages over its classical counterpart, with significant improvements in time and space complexities. Our complexities can have trade-offs between the two: when optimizing for minimal circuit depth, our quantum SGC achieves logarithmic time complexity in the input sizes (albeit at the cost of linear space complexity). When optimizing for minimal qubit usage, the quantum SGC exhibits space complexity logarithmic in the input sizes, offering an exponential reduction compared to classical SGCs, while still maintaining better time complexity. These results suggest our Quantum GNN frameworks could efficiently process large-scale graphs. This work paves the way for implementing more advanced Graph Neural Network models on quantum computers, opening new possibilities in quantum machine learning for analyzing graph-structured data.","sentences":["Graph Neural Networks (GNNs) are powerful machine learning models that excel at analyzing structured data represented as graphs, demonstrating remarkable performance in applications like social network analysis and recommendation systems.","However, classical GNNs face scalability challenges when dealing with large-scale graphs.","This paper proposes frameworks for implementing GNNs on quantum computers to potentially address the challenges.","We devise quantum algorithms corresponding to the three fundamental types of classical GNNs: Graph Convolutional Networks, Graph Attention Networks, and Message-Passing GNNs.","A complexity analysis of our quantum implementation of the Simplified Graph Convolutional (SGC) Network shows potential quantum advantages over its classical counterpart, with significant improvements in time and space complexities.","Our complexities can have trade-offs between the two: when optimizing for minimal circuit depth, our quantum SGC achieves logarithmic time complexity in the input sizes (albeit at the cost of linear space complexity).","When optimizing for minimal qubit usage, the quantum SGC exhibits space complexity logarithmic in the input sizes, offering an exponential reduction compared to classical SGCs, while still maintaining better time complexity.","These results suggest our Quantum GNN frameworks could efficiently process large-scale graphs.","This work paves the way for implementing more advanced Graph Neural Network models on quantum computers, opening new possibilities in quantum machine learning for analyzing graph-structured data."],"url":"http://arxiv.org/abs/2405.17060v1","category":"quant-ph"}
{"created":"2024-05-27 11:27:00","title":"ReflectionCoder: Learning from Reflection Sequence for Enhanced One-off Code Generation","abstract":"Code generation plays a crucial role in various tasks, such as code auto-completion and mathematical reasoning. Previous work has proposed numerous methods to enhance code generation performance, including integrating feedback from the compiler. Inspired by this, we present ReflectionCoder, a novel approach that effectively leverages reflection sequences constructed by integrating compiler feedback to improve one-off code generation performance. Furthermore, we propose reflection self-distillation and dynamically masked distillation to effectively utilize these reflection sequences. Extensive experiments on three benchmarks, i.e., HumanEval (+), MBPP (+), and MultiPl-E, demonstrate that models fine-tuned with our method achieve state-of-the-art performance. Notably, ReflectionCoder-DeepSeek-Coder-33B reaches pass@1 of 82.9 (76.8) on HumanEval (+) and 84.1 (72.0) on MBPP (+), on par with GPT-3.5-Turbo and Claude-3-opus, and surpasses early GPT-4. Beyond the code domain, we believe this approach can benefit other domains that focus on final results and require long reasoning paths. Code and data are available at https://github.com/SenseLLM/ReflectionCoder.","sentences":["Code generation plays a crucial role in various tasks, such as code auto-completion and mathematical reasoning.","Previous work has proposed numerous methods to enhance code generation performance, including integrating feedback from the compiler.","Inspired by this, we present ReflectionCoder, a novel approach that effectively leverages reflection sequences constructed by integrating compiler feedback to improve one-off code generation performance.","Furthermore, we propose reflection self-distillation and dynamically masked distillation to effectively utilize these reflection sequences.","Extensive experiments on three benchmarks, i.e., HumanEval (+), MBPP (+), and MultiPl-E, demonstrate that models fine-tuned with our method achieve state-of-the-art performance.","Notably, ReflectionCoder-DeepSeek-Coder-33B reaches pass@1 of 82.9 (76.8) on HumanEval (+) and 84.1 (72.0) on MBPP (+), on par with GPT-3.5-Turbo and Claude-3-opus, and surpasses early GPT-4.","Beyond the code domain, we believe this approach can benefit other domains that focus on final results and require long reasoning paths.","Code and data are available at https://github.com/SenseLLM/ReflectionCoder."],"url":"http://arxiv.org/abs/2405.17057v1","category":"cs.CL"}
{"created":"2024-05-27 11:25:41","title":"Massive twistor worldline in electromagnetic fields","abstract":"We study the (ambi-)twistor model for spinning particles interacting via electromagnetic field, as a toy model for studying classical dynamics of gravitating bodies including effects of both spins to all orders. We compute the momentum kick and spin kick up to one-loop order and show precisely how they are encoded in the classical eikonal. The all-orders-in-spin effects are encoded as a dynamical implementation of the Newman-Janis shift, and we find that the expansion in both spins can be resummed to simple expressions in special kinematic configurations, at least up to one-loop order. We confirm that the classical eikonal can be understood as the generator of canonical transformations that map the in-states of a scattering process to the out-states. We also show that cut contributions for converting worldline propagators from time-symmetric to retarded amount to the iterated action of the leading eikonal at one-loop order.","sentences":["We study the (ambi-)twistor model for spinning particles interacting via electromagnetic field, as a toy model for studying classical dynamics of gravitating bodies including effects of both spins to all orders.","We compute the momentum kick and spin kick up to one-loop order and show precisely how they are encoded in the classical eikonal.","The all-orders-in-spin effects are encoded as a dynamical implementation of the Newman-Janis shift, and we find that the expansion in both spins can be resummed to simple expressions in special kinematic configurations, at least up to one-loop order.","We confirm that the classical eikonal can be understood as the generator of canonical transformations that map the in-states of a scattering process to the out-states.","We also show that cut contributions for converting worldline propagators from time-symmetric to retarded amount to the iterated action of the leading eikonal at one-loop order."],"url":"http://arxiv.org/abs/2405.17056v1","category":"hep-th"}
{"created":"2024-05-27 11:18:25","title":"WirelessLLM: Empowering Large Language Models Towards Wireless Intelligence","abstract":"The rapid evolution of wireless technologies and the growing complexity of network infrastructures necessitate a paradigm shift in how communication networks are designed, configured, and managed. Recent advancements in Large Language Models (LLMs) have sparked interest in their potential to revolutionize wireless communication systems. However, existing studies on LLMs for wireless systems are limited to a direct application for telecom language understanding. To empower LLMs with knowledge and expertise in the wireless domain, this paper proposes WirelessLLM, a comprehensive framework for adapting and enhancing LLMs to address the unique challenges and requirements of wireless communication networks. We first identify three foundational principles that underpin WirelessLLM: knowledge alignment, knowledge fusion, and knowledge evolution. Then, we investigate the enabling technologies to build WirelessLLM, including prompt engineering, retrieval augmented generation, tool usage, multi-modal pre-training, and domain-specific fine-tuning. Moreover, we present three case studies to demonstrate the practical applicability and benefits of WirelessLLM for solving typical problems in wireless networks. Finally, we conclude this paper by highlighting key challenges and outlining potential avenues for future research.","sentences":["The rapid evolution of wireless technologies and the growing complexity of network infrastructures necessitate a paradigm shift in how communication networks are designed, configured, and managed.","Recent advancements in Large Language Models (LLMs) have sparked interest in their potential to revolutionize wireless communication systems.","However, existing studies on LLMs for wireless systems are limited to a direct application for telecom language understanding.","To empower LLMs with knowledge and expertise in the wireless domain, this paper proposes WirelessLLM, a comprehensive framework for adapting and enhancing LLMs to address the unique challenges and requirements of wireless communication networks.","We first identify three foundational principles that underpin WirelessLLM: knowledge alignment, knowledge fusion, and knowledge evolution.","Then, we investigate the enabling technologies to build WirelessLLM, including prompt engineering, retrieval augmented generation, tool usage, multi-modal pre-training, and domain-specific fine-tuning.","Moreover, we present three case studies to demonstrate the practical applicability and benefits of WirelessLLM for solving typical problems in wireless networks.","Finally, we conclude this paper by highlighting key challenges and outlining potential avenues for future research."],"url":"http://arxiv.org/abs/2405.17053v1","category":"cs.NI"}
{"created":"2024-05-27 11:14:55","title":"SelfCP: Compressing Long Prompt to 1/12 Using the Frozen Large Language Model Itself","abstract":"Long prompt leads to huge hardware costs when using Large Language Models (LLMs). Unfortunately, many tasks, such as summarization, inevitably introduce long task-inputs, and the wide application of in-context learning easily makes the prompt length explode. Inspired by the language understanding ability of LLMs, this paper proposes SelfCP, which uses the LLM \\textbf{itself} to \\textbf{C}ompress long \\textbf{P}rompt into compact virtual tokens. SelfCP applies a general frozen LLM twice, first as an encoder to compress the prompt and then as a decoder to generate responses. Specifically, given a long prompt, we place special tokens within the lengthy segment for compression and signal the LLM to generate $k$ virtual tokens. Afterward, the virtual tokens concatenate with the uncompressed prompt and are fed into the same LLM to generate the response. In general, SelfCP facilitates the unconditional and conditional compression of prompts, fitting both standard tasks and those with specific objectives. Since the encoder and decoder are frozen, SelfCP only contains 17M trainable parameters and allows for convenient adaptation across various backbones. We implement SelfCP with two LLM backbones and evaluate it in both in- and out-domain tasks. Results show that the compressed virtual tokens can substitute $12 \\times$ larger original prompts effectively","sentences":["Long prompt leads to huge hardware costs when using Large Language Models (LLMs).","Unfortunately, many tasks, such as summarization, inevitably introduce long task-inputs, and the wide application of in-context learning easily makes the prompt length explode.","Inspired by the language understanding ability of LLMs, this paper proposes SelfCP, which uses the LLM \\textbf{itself} to \\textbf{C}ompress long \\textbf{P}rompt into compact virtual tokens.","SelfCP applies a general frozen LLM twice, first as an encoder to compress the prompt and then as a decoder to generate responses.","Specifically, given a long prompt, we place special tokens within the lengthy segment for compression and signal the LLM to generate $k$ virtual tokens.","Afterward, the virtual tokens concatenate with the uncompressed prompt and are fed into the same LLM to generate the response.","In general, SelfCP facilitates the unconditional and conditional compression of prompts, fitting both standard tasks and those with specific objectives.","Since the encoder and decoder are frozen, SelfCP only contains 17M trainable parameters and allows for convenient adaptation across various backbones.","We implement SelfCP with two LLM backbones and evaluate it in both in- and out-domain tasks.","Results show that the compressed virtual tokens can substitute $12 \\times$ larger original prompts effectively"],"url":"http://arxiv.org/abs/2405.17052v1","category":"cs.CL"}
{"created":"2024-05-27 11:07:47","title":"BeamVQ: Aligning Space-Time Forecasting Model via Self-training on Physics-aware Metrics","abstract":"Data-driven deep learning has emerged as the new paradigm to model complex physical space-time systems. These data-driven methods learn patterns by optimizing statistical metrics and tend to overlook the adherence to physical laws, unlike traditional model-driven numerical methods. Thus, they often generate predictions that are not physically realistic. On the other hand, by sampling a large amount of high quality predictions from a data-driven model, some predictions will be more physically plausible than the others and closer to what will happen in the future. Based on this observation, we propose \\emph{Beam search by Vector Quantization} (BeamVQ) to enhance the physical alignment of data-driven space-time forecasting models. The key of BeamVQ is to train model on self-generated samples filtered with physics-aware metrics. To be flexibly support different backbone architectures, BeamVQ leverages a code bank to transform any encoder-decoder model to the continuous state space into discrete codes. Afterwards, it iteratively employs beam search to sample high-quality sequences, retains those with the highest physics-aware scores, and trains model on the new dataset. Comprehensive experiments show that BeamVQ not only gave an average statistical skill score boost for more than 32% for ten backbones on five datasets, but also significantly enhances physics-aware metrics.","sentences":["Data-driven deep learning has emerged as the new paradigm to model complex physical space-time systems.","These data-driven methods learn patterns by optimizing statistical metrics and tend to overlook the adherence to physical laws, unlike traditional model-driven numerical methods.","Thus, they often generate predictions that are not physically realistic.","On the other hand, by sampling a large amount of high quality predictions from a data-driven model, some predictions will be more physically plausible than the others and closer to what will happen in the future.","Based on this observation, we propose \\emph{Beam search by Vector Quantization} (BeamVQ) to enhance the physical alignment of data-driven space-time forecasting models.","The key of BeamVQ is to train model on self-generated samples filtered with physics-aware metrics.","To be flexibly support different backbone architectures, BeamVQ leverages a code bank to transform any encoder-decoder model to the continuous state space into discrete codes.","Afterwards, it iteratively employs beam search to sample high-quality sequences, retains those with the highest physics-aware scores, and trains model on the new dataset.","Comprehensive experiments show that BeamVQ not only gave an average statistical skill score boost for more than 32% for ten backbones on five datasets, but also significantly enhances physics-aware metrics."],"url":"http://arxiv.org/abs/2405.17051v1","category":"cs.LG"}
{"created":"2024-05-27 11:02:20","title":"Modified Six State Cryptographic Protocol with Entangled Ancilla Component States","abstract":"In a realistic situation, it is very difficult to communicate securely between two distant parties without introducing any disturbances. These disturbances might occur either due to external noise or may be due to the interference of an eavesdropper sitting in between the sender and the receiver. In this work, we probe here the existence of the possibility of the situation of generation of a secret key even if the eavesdropper is able to construct an entangled ancilla state in such a way that she can extract information from the intercepted qubit. To achieve this task, we consider and modify the six-state QKD protocol in which Eve can construct the unitary transformation that may make all ancilla components entangled at the output. Then, we calculate the mutual information between Alice and Bob and Alice and Eve, and identify the region where the secret key is generated even in the presence of Eve. We find that, in general, the mutual information of Alice and Eve depends not only on the disturbance D, but here we have shown that it also depends on the concurrence of the ancilla component states. We have further shown that it is possible to derive the disturbance-free mutual information of Alice and Eve, if Eve manipulates her entangled ancilla state in a particular manner. Thus, in this way, we are able to show that a secret key can be generated between Alice and Bob even if the disturbance is large enough. Moreover, we show that Bruss's six state QKD protocol failed to generate the secret key in the region where the modified six-state QKD protocol can generate the secret key.","sentences":["In a realistic situation, it is very difficult to communicate securely between two distant parties without introducing any disturbances.","These disturbances might occur either due to external noise or may be due to the interference of an eavesdropper sitting in between the sender and the receiver.","In this work, we probe here the existence of the possibility of the situation of generation of a secret key even if the eavesdropper is able to construct an entangled ancilla state in such a way that she can extract information from the intercepted qubit.","To achieve this task, we consider and modify the six-state QKD protocol in which Eve can construct the unitary transformation that may make all ancilla components entangled at the output.","Then, we calculate the mutual information between Alice and Bob and Alice and Eve, and identify the region where the secret key is generated even in the presence of Eve.","We find that, in general, the mutual information of Alice and Eve depends not only on the disturbance D, but here we have shown that it also depends on the concurrence of the ancilla component states.","We have further shown that it is possible to derive the disturbance-free mutual information of Alice and Eve, if Eve manipulates her entangled ancilla state in a particular manner.","Thus, in this way, we are able to show that a secret key can be generated between Alice and Bob even if the disturbance is large enough.","Moreover, we show that Bruss's six state QKD protocol failed to generate the secret key in the region where the modified six-state QKD protocol can generate the secret key."],"url":"http://arxiv.org/abs/2405.17046v1","category":"quant-ph"}
{"created":"2024-05-27 11:00:51","title":"Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models","abstract":"Advanced artificial intelligence (AI) systems with access to millions of research papers could inspire new research ideas that may not be conceived by humans alone. However, how interesting are these AI-generated ideas, and how can we improve their quality? Here, we introduce SciMuse, a system that uses an evolving knowledge graph built from more than 58 million scientific papers to generate personalized research ideas via an interface to GPT-4. We conducted a large-scale human evaluation with over 100 research group leaders from the Max Planck Society, who ranked more than 4,000 personalized research ideas based on their level of interest. This evaluation allows us to understand the relationships between scientific interest and the core properties of the knowledge graph. We find that data-efficient machine learning can predict research interest with high precision, allowing us to optimize the interest-level of generated research ideas. This work represents a step towards an artificial scientific muse that could catalyze unforeseen collaborations and suggest interesting avenues for scientists.","sentences":["Advanced artificial intelligence (AI) systems with access to millions of research papers could inspire new research ideas that may not be conceived by humans alone.","However, how interesting are these AI-generated ideas, and how can we improve their quality?","Here, we introduce SciMuse, a system that uses an evolving knowledge graph built from more than 58 million scientific papers to generate personalized research ideas via an interface to GPT-4.","We conducted a large-scale human evaluation with over 100 research group leaders from the Max Planck Society, who ranked more than 4,000 personalized research ideas based on their level of interest.","This evaluation allows us to understand the relationships between scientific interest and the core properties of the knowledge graph.","We find that data-efficient machine learning can predict research interest with high precision, allowing us to optimize the interest-level of generated research ideas.","This work represents a step towards an artificial scientific muse that could catalyze unforeseen collaborations and suggest interesting avenues for scientists."],"url":"http://arxiv.org/abs/2405.17044v1","category":"cs.AI"}
{"created":"2024-05-27 11:00:10","title":"Action of Weyl group on equivariant K-theory of flag varieties","abstract":"We describe the action of the Weyl group of a semi simple linear group $G$ on cohomological and K-theoretic invariants of the generalized flag variety $G/B$. We study the automorphism $s_i$, induced by the reflection in the simple root, on the equivariant $K$-theory ring $K_T(G/B)$ using divided difference operators. Using the localization theorem for torus action and Borel presentation for the equivariant K-theory ring, we calculate the formula for this automorphism. Moreover, we expand this formula in the basis consisting of structure sheaves classes of Schubert varieties. We provide effective formula (applying properties of Weyl groups) for the approximation of this expansion, more specifically for the part corresponding to Schubert varieties with the fixed dimension, which in the case of $G$ being a special linear group is more exact. Finally, we discuss the above-mentioned formula in the basis of motivic Chern classes of Schubert varieties.","sentences":["We describe the action of the Weyl group of a semi simple linear group $G$ on cohomological and K-theoretic invariants of the generalized flag variety $G/B$.","We study the automorphism $s_i$, induced by the reflection in the simple root, on the equivariant $K$-theory ring $K_T(G/B)$ using divided difference operators.","Using the localization theorem for torus action and Borel presentation for the equivariant K-theory ring, we calculate the formula for this automorphism.","Moreover, we expand this formula in the basis consisting of structure sheaves classes of Schubert varieties.","We provide effective formula (applying properties of Weyl groups) for the approximation of this expansion, more specifically for the part corresponding to Schubert varieties with the fixed dimension, which in the case of $G$ being a special linear group is more exact.","Finally, we discuss the above-mentioned formula in the basis of motivic Chern classes of Schubert varieties."],"url":"http://arxiv.org/abs/2405.17043v1","category":"math.AG"}
{"created":"2024-05-27 10:52:48","title":"Solving marginals of the LDP for the directed landscape","abstract":"We prove the upper-tail Large Deviation Principle (LDP) for the parabolic Airy process and characterize the limit shape of the directed landscape under the upper-tail conditioning. The LDP result answers Conjecture 10.1 in Das, Dauvergne, and Vir\\'{a}g (2024). The starting point of our proof is the metric-level LDP for the directed landscape from Das, Dauvergne, and Vir\\'{a}g (2024) that reduces our work to solving a variational problem. Our proof is PDE-based and uses geometric arguments, connecting the variational problem to the weak solutions of Burgers' equation. Further, our method may generalize to the setting of the upper-tail LDP for the KPZ fixed point under the multi-wedge initial data, and we prove a decomposition result in this direction.","sentences":["We prove the upper-tail Large Deviation Principle (LDP) for the parabolic Airy process and characterize the limit shape of the directed landscape under the upper-tail conditioning.","The LDP result answers Conjecture 10.1 in Das, Dauvergne, and Vir\\'{a}g (2024).","The starting point of our proof is the metric-level LDP for the directed landscape from Das, Dauvergne, and Vir\\'{a}g (2024) that reduces our work to solving a variational problem.","Our proof is PDE-based and uses geometric arguments, connecting the variational problem to the weak solutions of Burgers' equation.","Further, our method may generalize to the setting of the upper-tail LDP for the KPZ fixed point under the multi-wedge initial data, and we prove a decomposition result in this direction."],"url":"http://arxiv.org/abs/2405.17041v1","category":"math.PR"}
{"created":"2024-05-27 10:45:49","title":"BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation","abstract":"Large language models (LLMs) have catalyzed a paradigm shift in natural language processing, yet their limited controllability poses a significant challenge for downstream applications. We aim to address this by drawing inspiration from the neural mechanisms of the human brain, specifically Broca's and Wernicke's areas, which are crucial for language generation and comprehension, respectively. In particular, Broca's area receives cognitive decision signals from Wernicke's area, treating the language generation as an intricate decision-making process, which differs from the fully auto-regressive language generation of existing LLMs. In a similar vein, our proposed system, the BWArea model, conceptualizes language generation as a decision-making task. This model has three components: a language world model, an inverse dynamics model, and a cognitive policy. Like Wernicke's area, the inverse dynamics model is designed to deduce the underlying cognitive intentions, or latent actions, behind each token. The BWArea model is amenable to both pre-training and fine-tuning like existing LLMs. With 30B clean pre-training tokens, we have trained a BWArea model, which achieves competitive performance with LLMs of equal size (1B parameters). Unlike fully auto-regressive LLMs, its pre-training performance does not degenerate if dirty data unintentionally appears. This shows the advantage of a decomposed structure of BWArea model in reducing efforts in laborious data selection and labeling. Finally, we reveal that the BWArea model offers enhanced controllability via fine-tuning the cognitive policy with downstream reward metrics, thereby facilitating alignment with greater simplicity. On 9 out of 10 tasks from two suites, TextWorld and BigBench Hard, our method shows superior performance to auto-regressive LLMs.","sentences":["Large language models (LLMs) have catalyzed a paradigm shift in natural language processing, yet their limited controllability poses a significant challenge for downstream applications.","We aim to address this by drawing inspiration from the neural mechanisms of the human brain, specifically Broca's and Wernicke's areas, which are crucial for language generation and comprehension, respectively.","In particular, Broca's area receives cognitive decision signals from Wernicke's area, treating the language generation as an intricate decision-making process, which differs from the fully auto-regressive language generation of existing LLMs.","In a similar vein, our proposed system, the BWArea model, conceptualizes language generation as a decision-making task.","This model has three components: a language world model, an inverse dynamics model, and a cognitive policy.","Like Wernicke's area, the inverse dynamics model is designed to deduce the underlying cognitive intentions, or latent actions, behind each token.","The BWArea model is amenable to both pre-training and fine-tuning like existing LLMs.","With 30B clean pre-training tokens, we have trained a BWArea model, which achieves competitive performance with LLMs of equal size (1B parameters).","Unlike fully auto-regressive LLMs, its pre-training performance does not degenerate if dirty data unintentionally appears.","This shows the advantage of a decomposed structure of BWArea model in reducing efforts in laborious data selection and labeling.","Finally, we reveal that the BWArea model offers enhanced controllability via fine-tuning the cognitive policy with downstream reward metrics, thereby facilitating alignment with greater simplicity.","On 9 out of 10 tasks from two suites, TextWorld and BigBench Hard, our method shows superior performance to auto-regressive LLMs."],"url":"http://arxiv.org/abs/2405.17039v1","category":"cs.CL"}
{"created":"2024-05-27 10:44:27","title":"Advancements in Tactile Hand Gesture Recognition for Enhanced Human-Machine Interaction","abstract":"Motivated by the growing interest in enhancing intuitive physical Human-Machine Interaction (HRI/HVI), this study aims to propose a robust tactile hand gesture recognition system. We performed a comprehensive evaluation of different hand gesture recognition approaches for a large area tactile sensing interface (touch interface) constructed from conductive textiles. Our evaluation encompassed traditional feature engineering methods, as well as contemporary deep learning techniques capable of real-time interpretation of a range of hand gestures, accommodating variations in hand sizes, movement velocities, applied pressure levels, and interaction points. Our extensive analysis of the various methods makes a significant contribution to tactile-based gesture recognition in the field of human-machine interaction.","sentences":["Motivated by the growing interest in enhancing intuitive physical Human-Machine Interaction (HRI/HVI), this study aims to propose a robust tactile hand gesture recognition system.","We performed a comprehensive evaluation of different hand gesture recognition approaches for a large area tactile sensing interface (touch interface) constructed from conductive textiles.","Our evaluation encompassed traditional feature engineering methods, as well as contemporary deep learning techniques capable of real-time interpretation of a range of hand gestures, accommodating variations in hand sizes, movement velocities, applied pressure levels, and interaction points.","Our extensive analysis of the various methods makes a significant contribution to tactile-based gesture recognition in the field of human-machine interaction."],"url":"http://arxiv.org/abs/2405.17038v1","category":"cs.HC"}
{"created":"2024-05-27 10:42:13","title":"Glauber Generative Model: Discrete Diffusion Models via Binary Classification","abstract":"We introduce the Glauber Generative Model (GGM), a new class of discrete diffusion models, to obtain new samples from a distribution given samples from a discrete space. GGM deploys a discrete Markov chain called the heat bath dynamics (or the Glauber dynamics) to denoise a sequence of noisy tokens to a sample from a joint distribution of discrete tokens. Our novel conceptual framework provides an exact reduction of the task of learning the denoising Markov chain to solving a class of binary classification tasks. More specifically, the model learns to classify a given token in a noisy sequence as signal or noise. In contrast, prior works on discrete diffusion models either solve regression problems to learn importance ratios, or minimize loss functions given by variational approximations. We apply GGM to language modeling and image generation, where images are discretized using image tokenizers like VQGANs. We show that it outperforms existing discrete diffusion models in language generation, and demonstrates strong performance for image generation without using dataset-specific image tokenizers. We also show that our model is capable of performing well in zero-shot control settings like text and image infilling.","sentences":["We introduce the Glauber Generative Model (GGM), a new class of discrete diffusion models, to obtain new samples from a distribution given samples from a discrete space.","GGM deploys a discrete Markov chain called the heat bath dynamics (or the Glauber dynamics) to denoise a sequence of noisy tokens to a sample from a joint distribution of discrete tokens.","Our novel conceptual framework provides an exact reduction of the task of learning the denoising Markov chain to solving a class of binary classification tasks.","More specifically, the model learns to classify a given token in a noisy sequence as signal or noise.","In contrast, prior works on discrete diffusion models either solve regression problems to learn importance ratios, or minimize loss functions given by variational approximations.","We apply GGM to language modeling and image generation, where images are discretized using image tokenizers like VQGANs.","We show that it outperforms existing discrete diffusion models in language generation, and demonstrates strong performance for image generation without using dataset-specific image tokenizers.","We also show that our model is capable of performing well in zero-shot control settings like text and image infilling."],"url":"http://arxiv.org/abs/2405.17035v1","category":"cs.LG"}
{"created":"2024-05-27 10:40:21","title":"FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks","abstract":"Fairness-aware Graph Neural Networks (GNNs) often face a challenging trade-off, where prioritizing fairness may require compromising utility. In this work, we re-examine fairness through the lens of spectral graph theory, aiming to reconcile fairness and utility within the framework of spectral graph learning. We explore the correlation between sensitive features and spectrum in GNNs, using theoretical analysis to delineate the similarity between original sensitive features and those after convolution under different spectrum. Our analysis reveals a reduction in the impact of similarity when the eigenvectors associated with the largest magnitude eigenvalue exhibit directional similarity. Based on these theoretical insights, we propose FUGNN, a novel spectral graph learning approach that harmonizes the conflict between fairness and utility. FUGNN ensures algorithmic fairness and utility by truncating the spectrum and optimizing eigenvector distribution during the encoding process. The fairness-aware eigenvector selection reduces the impact of convolution on sensitive features while concurrently minimizing the sacrifice of utility. FUGNN further optimizes the distribution of eigenvectors through a transformer architecture. By incorporating the optimized spectrum into the graph convolution network, FUGNN effectively learns node representations. Experiments on six real-world datasets demonstrate the superiority of FUGNN over baseline methods. The codes are available at https://github.com/yushuowiki/FUGNN.","sentences":["Fairness-aware Graph Neural Networks (GNNs) often face a challenging trade-off, where prioritizing fairness may require compromising utility.","In this work, we re-examine fairness through the lens of spectral graph theory, aiming to reconcile fairness and utility within the framework of spectral graph learning.","We explore the correlation between sensitive features and spectrum in GNNs, using theoretical analysis to delineate the similarity between original sensitive features and those after convolution under different spectrum.","Our analysis reveals a reduction in the impact of similarity when the eigenvectors associated with the largest magnitude eigenvalue exhibit directional similarity.","Based on these theoretical insights, we propose FUGNN, a novel spectral graph learning approach that harmonizes the conflict between fairness and utility.","FUGNN ensures algorithmic fairness and utility by truncating the spectrum and optimizing eigenvector distribution during the encoding process.","The fairness-aware eigenvector selection reduces the impact of convolution on sensitive features while concurrently minimizing the sacrifice of utility.","FUGNN further optimizes the distribution of eigenvectors through a transformer architecture.","By incorporating the optimized spectrum into the graph convolution network, FUGNN effectively learns node representations.","Experiments on six real-world datasets demonstrate the superiority of FUGNN over baseline methods.","The codes are available at https://github.com/yushuowiki/FUGNN."],"url":"http://arxiv.org/abs/2405.17034v1","category":"cs.LG"}
{"created":"2024-05-27 10:31:26","title":"SCaRL- A Synthetic Multi-Modal Dataset for Autonomous Driving","abstract":"We present a novel synthetically generated multi-modal dataset, SCaRL, to enable the training and validation of autonomous driving solutions. Multi-modal datasets are essential to attain the robustness and high accuracy required by autonomous systems in applications such as autonomous driving. As deep learning-based solutions are becoming more prevalent for object detection, classification, and tracking tasks, there is great demand for datasets combining camera, lidar, and radar sensors. Existing real/synthetic datasets for autonomous driving lack synchronized data collection from a complete sensor suite. SCaRL provides synchronized Synthetic data from RGB, semantic/instance, and depth Cameras; Range-Doppler-Azimuth/Elevation maps and raw data from Radar; and 3D point clouds/2D maps of semantic, depth and Doppler data from coherent Lidar. SCaRL is a large dataset based on the CARLA Simulator, which provides data for diverse, dynamic scenarios and traffic conditions. SCaRL is the first dataset to include synthetic synchronized data from coherent Lidar and MIMO radar sensors.   The dataset can be accessed here: https://fhr-ihs-sva.pages.fraunhofer.de/asp/scarl/","sentences":["We present a novel synthetically generated multi-modal dataset, SCaRL, to enable the training and validation of autonomous driving solutions.","Multi-modal datasets are essential to attain the robustness and high accuracy required by autonomous systems in applications such as autonomous driving.","As deep learning-based solutions are becoming more prevalent for object detection, classification, and tracking tasks, there is great demand for datasets combining camera, lidar, and radar sensors.","Existing real/synthetic datasets for autonomous driving lack synchronized data collection from a complete sensor suite.","SCaRL provides synchronized Synthetic data from RGB, semantic/instance, and depth Cameras; Range-Doppler-Azimuth/Elevation maps and raw data from Radar; and 3D point clouds/2D maps of semantic, depth and Doppler data from coherent Lidar.","SCaRL is a large dataset based on the CARLA Simulator, which provides data for diverse, dynamic scenarios and traffic conditions.","SCaRL is the first dataset to include synthetic synchronized data from coherent Lidar and MIMO radar sensors.   ","The dataset can be accessed here: https://fhr-ihs-sva.pages.fraunhofer.de/asp/scarl/"],"url":"http://arxiv.org/abs/2405.17030v1","category":"cs.CV"}
{"created":"2024-05-27 10:30:54","title":"RSET: Remapping-based Sorting Method for Emotion Transfer Speech Synthesis","abstract":"Although current Text-To-Speech (TTS) models are able to generate high-quality speech samples, there are still challenges in developing emotion intensity controllable TTS. Most existing TTS models achieve emotion intensity control by extracting intensity information from reference speeches. Unfortunately, limited by the lack of modeling for intra-class emotion intensity and the model's information decoupling capability, the generated speech cannot achieve fine-grained emotion intensity control and suffers from information leakage issues. In this paper, we propose an emotion transfer TTS model, which defines a remapping-based sorting method to model intra-class relative intensity information, combined with Mutual Information (MI) to decouple speaker and emotion information, and synthesizes expressive speeches with perceptible intensity differences. Experiments show that our model achieves fine-grained emotion control while preserving speaker information.","sentences":["Although current Text-To-Speech (TTS) models are able to generate high-quality speech samples, there are still challenges in developing emotion intensity controllable TTS.","Most existing TTS models achieve emotion intensity control by extracting intensity information from reference speeches.","Unfortunately, limited by the lack of modeling for intra-class emotion intensity and the model's information decoupling capability, the generated speech cannot achieve fine-grained emotion intensity control and suffers from information leakage issues.","In this paper, we propose an emotion transfer TTS model, which defines a remapping-based sorting method to model intra-class relative intensity information, combined with Mutual Information (MI) to decouple speaker and emotion information, and synthesizes expressive speeches with perceptible intensity differences.","Experiments show that our model achieves fine-grained emotion control while preserving speaker information."],"url":"http://arxiv.org/abs/2405.17028v1","category":"cs.SD"}
{"created":"2024-05-27 10:30:21","title":"Supervised Batch Normalization","abstract":"Batch Normalization (BN), a widely-used technique in neural networks, enhances generalization and expedites training by normalizing each mini-batch to the same mean and variance. However, its effectiveness diminishes when confronted with diverse data distributions. To address this challenge, we propose Supervised Batch Normalization (SBN), a pioneering approach. We expand normalization beyond traditional single mean and variance parameters, enabling the identification of data modes prior to training. This ensures effective normalization for samples sharing common features. We define contexts as modes, categorizing data with similar characteristics. These contexts are explicitly defined, such as domains in domain adaptation or modalities in multimodal systems, or implicitly defined through clustering algorithms based on data similarity. We illustrate the superiority of our approach over BN and other commonly employed normalization techniques through various experiments on both single and multi-task datasets. Integrating SBN with Vision Transformer results in a remarkable \\textit{15.13}\\% accuracy enhancement on CIFAR-100. Additionally, in domain adaptation scenarios, employing AdaMatch demonstrates an impressive \\textit{22.25}\\% accuracy improvement on MNIST and SVHN compared to BN.","sentences":["Batch Normalization (BN), a widely-used technique in neural networks, enhances generalization and expedites training by normalizing each mini-batch to the same mean and variance.","However, its effectiveness diminishes when confronted with diverse data distributions.","To address this challenge, we propose Supervised Batch Normalization (SBN), a pioneering approach.","We expand normalization beyond traditional single mean and variance parameters, enabling the identification of data modes prior to training.","This ensures effective normalization for samples sharing common features.","We define contexts as modes, categorizing data with similar characteristics.","These contexts are explicitly defined, such as domains in domain adaptation or modalities in multimodal systems, or implicitly defined through clustering algorithms based on data similarity.","We illustrate the superiority of our approach over BN and other commonly employed normalization techniques through various experiments on both single and multi-task datasets.","Integrating SBN with Vision Transformer results in a remarkable \\textit{15.13}\\% accuracy enhancement on CIFAR-100.","Additionally, in domain adaptation scenarios, employing AdaMatch demonstrates an impressive \\textit{22.25}\\% accuracy improvement on MNIST and SVHN compared to BN."],"url":"http://arxiv.org/abs/2405.17027v1","category":"cs.LG"}
{"created":"2024-05-27 10:25:08","title":"SWAT: Scalable and Efficient Window Attention-based Transformers Acceleration on FPGAs","abstract":"Efficiently supporting long context length is crucial for Transformer models. The quadratic complexity of the self-attention computation plagues traditional Transformers. Sliding window-based static sparse attention mitigates the problem by limiting the attention scope of the input tokens, reducing the theoretical complexity from quadratic to linear. Although the sparsity induced by window attention is highly structured, it does not align perfectly with the microarchitecture of the conventional accelerators, leading to suboptimal implementation. In response, we propose a dataflow-aware FPGA-based accelerator design, SWAT, that efficiently leverages the sparsity to achieve scalable performance for long input. The proposed microarchitecture is based on a design that maximizes data reuse by using a combination of row-wise dataflow, kernel fusion optimization, and an input-stationary design considering the distributed memory and computation resources of FPGA. Consequently, it achieves up to 22$\\times$ and 5.7$\\times$ improvement in latency and energy efficiency compared to the baseline FPGA-based accelerator and 15$\\times$ energy efficiency compared to GPU-based solution.","sentences":["Efficiently supporting long context length is crucial for Transformer models.","The quadratic complexity of the self-attention computation plagues traditional Transformers.","Sliding window-based static sparse attention mitigates the problem by limiting the attention scope of the input tokens, reducing the theoretical complexity from quadratic to linear.","Although the sparsity induced by window attention is highly structured, it does not align perfectly with the microarchitecture of the conventional accelerators, leading to suboptimal implementation.","In response, we propose a dataflow-aware FPGA-based accelerator design, SWAT, that efficiently leverages the sparsity to achieve scalable performance for long input.","The proposed microarchitecture is based on a design that maximizes data reuse by using a combination of row-wise dataflow, kernel fusion optimization, and an input-stationary design considering the distributed memory and computation resources of FPGA.","Consequently, it achieves up to 22$\\times$ and 5.7$\\times$ improvement in latency and energy efficiency compared to the baseline FPGA-based accelerator and 15$\\times$ energy efficiency compared to GPU-based solution."],"url":"http://arxiv.org/abs/2405.17025v1","category":"cs.AR"}
{"created":"2024-05-27 10:21:38","title":"Compositional Few-Shot Class-Incremental Learning","abstract":"Few-shot class-incremental learning (FSCIL) is proposed to continually learn from novel classes with only a few samples after the (pre-)training on base classes with sufficient data. However, this remains a challenge. In contrast, humans can easily recognize novel classes with a few samples. Cognitive science demonstrates that an important component of such human capability is compositional learning. This involves identifying visual primitives from learned knowledge and then composing new concepts using these transferred primitives, making incremental learning both effective and interpretable. To imitate human compositional learning, we propose a cognitive-inspired method for the FSCIL task. We define and build a compositional model based on set similarities, and then equip it with a primitive composition module and a primitive reuse module. In the primitive composition module, we propose to utilize the Centered Kernel Alignment (CKA) similarity to approximate the similarity between primitive sets, allowing the training and evaluation based on primitive compositions. In the primitive reuse module, we enhance primitive reusability by classifying inputs based on primitives replaced with the closest primitives from other classes. Experiments on three datasets validate our method, showing it outperforms current state-of-the-art methods with improved interpretability. Our code is available at https://github.com/Zoilsen/Comp-FSCIL.","sentences":["Few-shot class-incremental learning (FSCIL) is proposed to continually learn from novel classes with only a few samples after the (pre-)training on base classes with sufficient data.","However, this remains a challenge.","In contrast, humans can easily recognize novel classes with a few samples.","Cognitive science demonstrates that an important component of such human capability is compositional learning.","This involves identifying visual primitives from learned knowledge and then composing new concepts using these transferred primitives, making incremental learning both effective and interpretable.","To imitate human compositional learning, we propose a cognitive-inspired method for the FSCIL task.","We define and build a compositional model based on set similarities, and then equip it with a primitive composition module and a primitive reuse module.","In the primitive composition module, we propose to utilize the Centered Kernel Alignment (CKA) similarity to approximate the similarity between primitive sets, allowing the training and evaluation based on primitive compositions.","In the primitive reuse module, we enhance primitive reusability by classifying inputs based on primitives replaced with the closest primitives from other classes.","Experiments on three datasets validate our method, showing it outperforms current state-of-the-art methods with improved interpretability.","Our code is available at https://github.com/Zoilsen/Comp-FSCIL."],"url":"http://arxiv.org/abs/2405.17022v1","category":"cs.CV"}
{"created":"2024-05-27 10:20:49","title":"Truncated Modular Exponentiation Operators: A Strategy for Quantum Factoring","abstract":"Modular exponentiation (ME) operators are one of the fundamental components of Shor's algorithm, and the place where most of the quantum resources are deployed. I propose a method for constructing the ME operators that relies upon the simple observation that the work register starts in state $\\vert 1 \\rangle$. Therefore, we do not have to create an ME operator $U$ that accepts a general input, but rather, one that takes an input from the periodic sequence of states $\\vert f(x) \\rangle$ for $x \\in \\{0, 1, \\cdots, r-1\\}$, where $f(x)$ is the ME function with period $r$. The operator $U$ can be partitioned into $r$ levels, where the gates in level $x \\in \\{0, 1, \\cdots, r-1\\}$ increment the state $\\vert f(x) \\rangle$ to the state $\\vert f(x+1) \\rangle$. The gates below $x$ do not affect the state $\\vert f(x+1) \\rangle$. The obvious problem with this method is that it is self-defeating: If we knew the operator $U$, then we would know the period $r$ of the ME function, and there would be no need for Shor's algorithm. I show, however, that the ME operators are very forgiving, and truncated approximate forms in which levels have been omitted are able to extract factors just as well as the exact operators. I demonstrate this by factoring the numbers $N = 21, 33, 35, 143, 247$ by using less than half the requisite number of levels in the ME operators. This procedure works because the method of continued fractions only requires an approximate phase value. This is the basis for a factorization strategy in which we fill the circuits for the ME operators with more and more gates, and the correlations between the various composite operators $U^p$ (where $p$ is a power of two) compensate for the missing levels.","sentences":["Modular exponentiation (ME) operators are one of the fundamental components of Shor's algorithm, and the place where most of the quantum resources are deployed.","I propose a method for constructing the ME operators that relies upon the simple observation that the work register starts in state $\\vert 1 \\rangle$.","Therefore, we do not have to create an ME operator $U$ that accepts a general input, but rather, one that takes an input from the periodic sequence of states $\\vert f(x) \\rangle$ for $x \\in \\{0, 1, \\cdots, r-1\\}$, where $f(x)$ is the ME function with period $r$. The operator $U$ can be partitioned into $r$ levels, where the gates in level $x \\in \\{0, 1, \\cdots, r-1\\}$ increment the state $\\vert f(x) \\rangle$ to the state $\\vert f(x+1) \\rangle$.","The gates below $x$ do not affect the state $\\vert f(x+1) \\rangle$.","The obvious problem with this method is that it is self-defeating: If we knew the operator $U$, then we would know the period $r$ of the ME function, and there would be no need for Shor's algorithm.","I show, however, that the ME operators are very forgiving, and truncated approximate forms in which levels have been omitted are able to extract factors just as well as the exact operators.","I demonstrate this by factoring the numbers $N = 21, 33, 35, 143, 247$ by using less than half the requisite number of levels in the ME operators.","This procedure works because the method of continued fractions only requires an approximate phase value.","This is the basis for a factorization strategy in which we fill the circuits for the ME operators with more and more gates, and the correlations between the various composite operators $U^p$ (where $p$ is a power of two) compensate for the missing levels."],"url":"http://arxiv.org/abs/2405.17021v1","category":"quant-ph"}
{"created":"2024-05-27 10:15:35","title":"From Compliant to Rigid Contact Simulation: a Unified and Efficient Approach","abstract":"Whether rigid or compliant, contact interactions are inherent to robot motions, enabling them to move or manipulate things. Contact interactions result from complex physical phenomena, that can be mathematically cast as Nonlinear Complementarity Problems (NCPs) in the context of rigid or compliant point contact interactions. Such a class of complementarity problems is, in general, difficult to solve both from an optimization and numerical perspective. Over the past decades, dedicated and specialized contact solvers, implemented in modern robotics simulators (e.g., Bullet, Drake, MuJoCo, DART, Raisim) have emerged. Yet, most of these solvers tend either to solve a relaxed formulation of the original contact problems (at the price of physical inconsistencies) or to scale poorly with the problem dimension or its numerical conditioning (e.g., a robotic hand manipulating a paper sheet). In this paper, we introduce a unified and efficient approach to solving NCPs in the context of contact simulation. It relies on a sound combination of the Alternating Direction Method of Multipliers (ADMM) and proximal algorithms to account for both compliant and rigid contact interfaces in a unified way. To handle ill-conditioned problems and accelerate the convergence rate, we also propose an efficient update strategy to adapt the ADMM hyperparameters automatically. By leveraging proximal methods, we also propose new algorithmic solutions to efficiently evaluate the inverse dynamics involving rigid and compliant contact interactions, extending the approach developed in MuJoCo. We validate the efficiency and robustness of our contact solver against several alternative contact methods of the literature and benchmark them on various robotics and granular mechanics scenarios. Our code is made open-source at https://github.com/Simple-Robotics/Simple.","sentences":["Whether rigid or compliant, contact interactions are inherent to robot motions, enabling them to move or manipulate things.","Contact interactions result from complex physical phenomena, that can be mathematically cast as Nonlinear Complementarity Problems (NCPs) in the context of rigid or compliant point contact interactions.","Such a class of complementarity problems is, in general, difficult to solve both from an optimization and numerical perspective.","Over the past decades, dedicated and specialized contact solvers, implemented in modern robotics simulators (e.g., Bullet, Drake, MuJoCo, DART, Raisim) have emerged.","Yet, most of these solvers tend either to solve a relaxed formulation of the original contact problems (at the price of physical inconsistencies) or to scale poorly with the problem dimension or its numerical conditioning (e.g., a robotic hand manipulating a paper sheet).","In this paper, we introduce a unified and efficient approach to solving NCPs in the context of contact simulation.","It relies on a sound combination of the Alternating Direction Method of Multipliers (ADMM) and proximal algorithms to account for both compliant and rigid contact interfaces in a unified way.","To handle ill-conditioned problems and accelerate the convergence rate, we also propose an efficient update strategy to adapt the ADMM hyperparameters automatically.","By leveraging proximal methods, we also propose new algorithmic solutions to efficiently evaluate the inverse dynamics involving rigid and compliant contact interactions, extending the approach developed in MuJoCo.","We validate the efficiency and robustness of our contact solver against several alternative contact methods of the literature and benchmark them on various robotics and granular mechanics scenarios.","Our code is made open-source at https://github.com/Simple-Robotics/Simple."],"url":"http://arxiv.org/abs/2405.17020v1","category":"cs.RO"}
{"created":"2024-05-27 10:15:16","title":"Bounding Random Test Set Size with Computational Learning Theory","abstract":"Random testing approaches work by generating inputs at random, or by selecting inputs randomly from some pre-defined operational profile. One long-standing question that arises in this and other testing contexts is as follows: When can we stop testing? At what point can we be certain that executing further tests in this manner will not explore previously untested (and potentially buggy) software behaviors? This is analogous to the question in Machine Learning, of how many training examples are required in order to infer an accurate model. In this paper we show how probabilistic approaches to answer this question in Machine Learning (arising from Computational Learning Theory) can be applied in our testing context. This enables us to produce an upper bound on the number of tests that are required to achieve a given level of adequacy. We are the first to enable this from only knowing the number of coverage targets (e.g. lines of code) in the source code, without needing to observe a sample test executions. We validate this bound on a large set of Java units, and an autonomous driving system.","sentences":["Random testing approaches work by generating inputs at random, or by selecting inputs randomly from some pre-defined operational profile.","One long-standing question that arises in this and other testing contexts is as follows: When can we stop testing?","At what point can we be certain that executing further tests in this manner will not explore previously untested (and potentially buggy) software behaviors?","This is analogous to the question in Machine Learning, of how many training examples are required in order to infer an accurate model.","In this paper we show how probabilistic approaches to answer this question in Machine Learning (arising from Computational Learning Theory) can be applied in our testing context.","This enables us to produce an upper bound on the number of tests that are required to achieve a given level of adequacy.","We are the first to enable this from only knowing the number of coverage targets (e.g. lines of code) in the source code, without needing to observe a sample test executions.","We validate this bound on a large set of Java units, and an autonomous driving system."],"url":"http://arxiv.org/abs/2405.17019v1","category":"cs.SE"}
{"created":"2024-05-27 10:01:52","title":"Analysis of Multiscale Reinforcement Q-Learning Algorithms for Mean Field Control Games","abstract":"Mean Field Control Games (MFCG), introduced in [Angiuli et al., 2022a], represent competitive games between a large number of large collaborative groups of agents in the infinite limit of number and size of groups. In this paper, we prove the convergence of a three-timescale Reinforcement Q-Learning (RL) algorithm to solve MFCG in a model-free approach from the point of view of representative agents. Our analysis uses a Q-table for finite state and action spaces updated at each discrete time-step over an infinite horizon. In [Angiuli et al., 2023], we proved convergence of two-timescale algorithms for MFG and MFC separately highlighting the need to follow multiple population distributions in the MFC case. Here, we integrate this feature for MFCG as well as three rates of update decreasing to zero in the proper ratios. Our technique of proof uses a generalization to three timescales of the two-timescale analysis in [Borkar, 1997]. We give a simple example satisfying the various hypothesis made in the proof of convergence and illustrating the performance of the algorithm.","sentences":["Mean Field Control Games (MFCG), introduced in [Angiuli et al., 2022a], represent competitive games between a large number of large collaborative groups of agents in the infinite limit of number and size of groups.","In this paper, we prove the convergence of a three-timescale Reinforcement Q-Learning (RL) algorithm to solve MFCG in a model-free approach from the point of view of representative agents.","Our analysis uses a Q-table for finite state and action spaces updated at each discrete time-step over an infinite horizon.","In [Angiuli et al., 2023], we proved convergence of two-timescale algorithms for MFG and MFC separately highlighting the need to follow multiple population distributions in the MFC case.","Here, we integrate this feature for MFCG as well as three rates of update decreasing to zero in the proper ratios.","Our technique of proof uses a generalization to three timescales of the two-timescale analysis in [Borkar, 1997].","We give a simple example satisfying the various hypothesis made in the proof of convergence and illustrating the performance of the algorithm."],"url":"http://arxiv.org/abs/2405.17017v1","category":"math.OC"}
{"created":"2024-05-27 10:01:36","title":"$\\text{Di}^2\\text{Pose}$: Discrete Diffusion Model for Occluded 3D Human Pose Estimation","abstract":"Continuous diffusion models have demonstrated their effectiveness in addressing the inherent uncertainty and indeterminacy in monocular 3D human pose estimation (HPE). Despite their strengths, the need for large search spaces and the corresponding demand for substantial training data make these models prone to generating biomechanically unrealistic poses. This challenge is particularly noticeable in occlusion scenarios, where the complexity of inferring 3D structures from 2D images intensifies. In response to these limitations, we introduce the Discrete Diffusion Pose ($\\text{Di}^2\\text{Pose}$), a novel framework designed for occluded 3D HPE that capitalizes on the benefits of a discrete diffusion model. Specifically, $\\text{Di}^2\\text{Pose}$ employs a two-stage process: it first converts 3D poses into a discrete representation through a \\emph{pose quantization step}, which is subsequently modeled in latent space through a \\emph{discrete diffusion process}. This methodological innovation restrictively confines the search space towards physically viable configurations and enhances the model's capability to comprehend how occlusions affect human pose within the latent space. Extensive evaluations conducted on various benchmarks (e.g., Human3.6M, 3DPW, and 3DPW-Occ) have demonstrated its effectiveness.","sentences":["Continuous diffusion models have demonstrated their effectiveness in addressing the inherent uncertainty and indeterminacy in monocular 3D human pose estimation (HPE).","Despite their strengths, the need for large search spaces and the corresponding demand for substantial training data make these models prone to generating biomechanically unrealistic poses.","This challenge is particularly noticeable in occlusion scenarios, where the complexity of inferring 3D structures from 2D images intensifies.","In response to these limitations, we introduce the Discrete Diffusion Pose ($\\text{Di}^2\\text{Pose}$), a novel framework designed for occluded 3D HPE that capitalizes on the benefits of a discrete diffusion model.","Specifically, $\\text{Di}^2\\text{Pose}$ employs a two-stage process: it first converts 3D poses into a discrete representation through a \\emph{pose quantization step}, which is subsequently modeled in latent space through a \\emph{discrete diffusion process}.","This methodological innovation restrictively confines the search space towards physically viable configurations and enhances the model's capability to comprehend how occlusions affect human pose within the latent space.","Extensive evaluations conducted on various benchmarks (e.g., Human3.6M, 3DPW, and 3DPW-Occ) have demonstrated its effectiveness."],"url":"http://arxiv.org/abs/2405.17016v1","category":"cs.CV"}
{"created":"2024-05-27 09:57:51","title":"MotionLLM: Multimodal Motion-Language Learning with Large Language Models","abstract":"Recent advancements in Multimodal Large Language Models (MM-LLMs) have demonstrated promising potential in terms of generalization and robustness when applied to different modalities. While previous works have already achieved 3D human motion generation using various approaches including language modeling, they mostly % are mostly carefully designed use specialized architecture and are restricted to single-human motion generation. Inspired by the success of MM-LLMs, we propose MotionLLM, a simple and general framework that can achieve single-human, multi-human motion generation, and motion captioning by fine-tuning pre-trained LLMs. Specifically, we encode and quantize motions into discrete LLM-understandable tokens, which results in a unified vocabulary consisting of both motion and text tokens. With only 1--3% parameters of the LLMs trained by using adapters, our single-human motion generation achieves comparable results to those diffusion models and other trained-from-scratch transformer-based models. Additionally, we show that our approach is scalable and flexible, allowing easy extension to multi-human motion generation through autoregressive generation of single-human motions. Project page: https://knoxzhao.github.io/MotionLLM","sentences":["Recent advancements in Multimodal Large Language Models (MM-LLMs) have demonstrated promising potential in terms of generalization and robustness when applied to different modalities.","While previous works have already achieved 3D human motion generation using various approaches including language modeling, they mostly % are mostly carefully designed use specialized architecture and are restricted to single-human motion generation.","Inspired by the success of MM-LLMs, we propose MotionLLM, a simple and general framework that can achieve single-human, multi-human motion generation, and motion captioning by fine-tuning pre-trained LLMs.","Specifically, we encode and quantize motions into discrete LLM-understandable tokens, which results in a unified vocabulary consisting of both motion and text tokens.","With only 1--3% parameters of the LLMs trained by using adapters, our single-human motion generation achieves comparable results to those diffusion models and other trained-from-scratch transformer-based models.","Additionally, we show that our approach is scalable and flexible, allowing easy extension to multi-human motion generation through autoregressive generation of single-human motions.","Project page: https://knoxzhao.github.io/MotionLLM"],"url":"http://arxiv.org/abs/2405.17013v1","category":"cs.CV"}
{"created":"2024-05-27 17:38:13","title":"Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention","abstract":"We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption. Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at github.com/OpenNLPLab/TransnormerLLM.","sentences":["We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption.","Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting.","However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention.","Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks.","This eliminates the need for cumsum in the linear attention calculation.","Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware.","To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention.","We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths.","TNL is notably more efficient than other language models.","In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures.","The source code is released at github.com/OpenNLPLab/TransnormerLLM."],"url":"http://arxiv.org/abs/2405.17381v1","category":"cs.CL"}
{"created":"2024-05-27 16:50:54","title":"Comprehensive analysis of local and nonlocal amplitudes in the $B^0\\rightarrow K^{*0}\u03bc^+\u03bc^-$ decay","abstract":"A comprehensive study of the local and nonlocal amplitudes contributing to the decay $B^0\\rightarrow K^{*0}(\\to K^+\\pi^-) \\mu^+\\mu^-$ is performed by analysing the phase-space distribution of the decay products. The analysis is based on \\proton\\proton collision data corresponding to an integrated luminosity of 8.4fb$^{-1}$ collected by the LHCb experiment. This measurement employs for the first time a model of both one-particle and two-particle nonlocal amplitudes, and utilises the complete dimuon mass spectrum without any veto regions around the narrow charmonium resonances. In this way it is possible to explicitly isolate the local and nonlocal contributions and capture the interference between them. The results show that interference with nonlocal contributions, although larger than predicted, only has a minor impact on the Wilson Coefficients determined from the fit to the data. For the local contributions, the Wilson Coefficient $C_9$, responsible for vector dimuon currents, exhibits a $2.1\\sigma$ deviation from the Standard Model expectation. The Wilson Coefficients $C_{10}$, $C_{9}'$ and $C_{10}'$ are all in better agreement than $C_{9}$ with the Standard Model and the global significance is at the level of $1.5\\sigma$. The model used also accounts for nonlocal contributions from $B^{0}\\to K^{*0}\\left[\\tau^+\\tau^-\\to \\mu^+\\mu^-\\right]$ rescattering, resulting in the first direct measurement of the $b s\\tau\\tau$ vector effective-coupling $C_{9\\tau}$.","sentences":["A comprehensive study of the local and nonlocal amplitudes contributing to the decay $B^0\\rightarrow K^{*0}(\\to K^+\\pi^-) \\mu^+\\mu^-$ is performed by analysing the phase-space distribution of the decay products.","The analysis is based on \\proton\\proton collision data corresponding to an integrated luminosity of 8.4fb$^{-1}$ collected by the LHCb experiment.","This measurement employs for the first time a model of both one-particle and two-particle nonlocal amplitudes, and utilises the complete dimuon mass spectrum without any veto regions around the narrow charmonium resonances.","In this way it is possible to explicitly isolate the local and nonlocal contributions and capture the interference between them.","The results show that interference with nonlocal contributions, although larger than predicted, only has a minor impact on the Wilson Coefficients determined from the fit to the data.","For the local contributions, the Wilson Coefficient $C_9$, responsible for vector dimuon currents, exhibits a $2.1\\sigma$ deviation from the Standard Model expectation.","The Wilson Coefficients $C_{10}$, $C_{9}'$ and $C_{10}'$ are all in better agreement than $C_{9}$ with the Standard Model and the global significance is at the level of $1.5\\sigma$. The model used also accounts for nonlocal contributions from $B^{0}\\to K^{*0}\\left[\\tau^+\\tau^-\\to \\mu^+\\mu^-\\right]$ rescattering, resulting in the first direct measurement of the $b s\\tau\\tau$ vector effective-coupling $C_{9\\tau}$."],"url":"http://arxiv.org/abs/2405.17347v1","category":"hep-ex"}
{"created":"2024-05-27 15:31:35","title":"Towards Accurate Ego-lane Identification with Early Time Series Classification","abstract":"Accurate and timely determination of a vehicle's current lane within a map is a critical task in autonomous driving systems. This paper utilizes an Early Time Series Classification (ETSC) method to achieve precise and rapid ego-lane identification in real-world driving data. The method begins by assessing the similarities between map and lane markings perceived by the vehicle's camera using measurement model quality metrics. These metrics are then fed into a selected ETSC method, comprising a probabilistic classifier and a tailored trigger function, optimized via multi-objective optimization to strike a balance between early prediction and accuracy. Our solution has been evaluated on a comprehensive dataset consisting of 114 hours of real-world traffic data, collected across 5 different countries by our test vehicles. Results show that by leveraging road lane-marking geometry and lane-marking type derived solely from a camera, our solution achieves an impressive accuracy of 99.6%, with an average prediction time of only 0.84 seconds.","sentences":["Accurate and timely determination of a vehicle's current lane within a map is a critical task in autonomous driving systems.","This paper utilizes an Early Time Series Classification (ETSC) method to achieve precise and rapid ego-lane identification in real-world driving data.","The method begins by assessing the similarities between map and lane markings perceived by the vehicle's camera using measurement model quality metrics.","These metrics are then fed into a selected ETSC method, comprising a probabilistic classifier and a tailored trigger function, optimized via multi-objective optimization to strike a balance between early prediction and accuracy.","Our solution has been evaluated on a comprehensive dataset consisting of 114 hours of real-world traffic data, collected across 5 different countries by our test vehicles.","Results show that by leveraging road lane-marking geometry and lane-marking type derived solely from a camera, our solution achieves an impressive accuracy of 99.6%, with an average prediction time of only 0.84 seconds."],"url":"http://arxiv.org/abs/2405.17270v1","category":"eess.SP"}
{"created":"2024-05-27 15:27:53","title":"Suppressing defection by increasing temptation: the impact of smart cooperators on a social dilemma situation","abstract":"In a social dilemma situation, where individual and collective interests are in conflict, it sounds a reasonable assumption that the presence of super or smart players, who simultaneously punish defection and reward cooperation without allowing exploitation, could solve the basic problem. The behavior of such a multi-strategy system, however, is more subtle than it is firstly anticipated. When exploring the complete parameter space, we find that the emergence of cyclic dominance among strategies is rather common, which results in several counter-intuitive phenomena. For example, the defection level can be lowered at higher temptation, or weaker punishment provides better conditions for smart players. Our study indicates that smart cooperators can unexpectedly thrive under high temptation, emphasizing the complexity of strategic interactions. This study suggests that the principles governing these interactions can be applied to other moral behaviors, such as truth-telling and honesty, providing valuable insights for future research in multi-agent systems.","sentences":["In a social dilemma situation, where individual and collective interests are in conflict, it sounds a reasonable assumption that the presence of super or smart players, who simultaneously punish defection and reward cooperation without allowing exploitation, could solve the basic problem.","The behavior of such a multi-strategy system, however, is more subtle than it is firstly anticipated.","When exploring the complete parameter space, we find that the emergence of cyclic dominance among strategies is rather common, which results in several counter-intuitive phenomena.","For example, the defection level can be lowered at higher temptation, or weaker punishment provides better conditions for smart players.","Our study indicates that smart cooperators can unexpectedly thrive under high temptation, emphasizing the complexity of strategic interactions.","This study suggests that the principles governing these interactions can be applied to other moral behaviors, such as truth-telling and honesty, providing valuable insights for future research in multi-agent systems."],"url":"http://arxiv.org/abs/2405.17268v1","category":"physics.soc-ph"}
{"created":"2024-05-27 14:58:56","title":"Anomalous Entropic Behavior Observed in Quasar 3C 273","abstract":"We investigate the flux intensities spanning from radio waves to $\\gamma$-rays across 36 light curves of Quasar 3C 273, utilizing publicly available data collected by the Integral Science Data Centre (ISDC) database. Our analysis reveals a consistent adherence of all light curves from this quasar to $q$-Gaussian distribution. This compelling finding strongly suggests a nonextensive behavior exhibited by Quasar 3C 273. Moreover, we derive the $q$ entropic indices for these light curves, providing insights into the degree of nonextensivity, where cases with $q>1$ were primarily observed. Utilizing this index, we estimate the nonextensive entropy ($S_{q}$) and explore its correlation with the energy (in eV) and the $q$ index. Notably, we observe a tendency for the $q$ value to increase as the Tsallis entropy decreases. Remarkably, our most significant observation pertains to the relationship between the entropy $S_{q}$ and the energy of the source. We identify an anomalous behavior in entropy, particularly evident in the infrared and $\\gamma$-ray wavebands.","sentences":["We investigate the flux intensities spanning from radio waves to $\\gamma$-rays across 36 light curves of Quasar 3C 273, utilizing publicly available data collected by the Integral Science Data Centre (ISDC) database.","Our analysis reveals a consistent adherence of all light curves from this quasar to $q$-Gaussian distribution.","This compelling finding strongly suggests a nonextensive behavior exhibited by Quasar 3C 273.","Moreover, we derive the $q$ entropic indices for these light curves, providing insights into the degree of nonextensivity, where cases with $q>1$ were primarily observed.","Utilizing this index, we estimate the nonextensive entropy ($S_{q}$) and explore its correlation with the energy (in eV) and the $q$ index.","Notably, we observe a tendency for the $q$ value to increase as the Tsallis entropy decreases.","Remarkably, our most significant observation pertains to the relationship between the entropy $S_{q}$ and the energy of the source.","We identify an anomalous behavior in entropy, particularly evident in the infrared and $\\gamma$-ray wavebands."],"url":"http://arxiv.org/abs/2405.17244v1","category":"astro-ph.HE"}
{"created":"2024-05-27 12:21:31","title":"DINO-SD: Champion Solution for ICRA 2024 RoboDepth Challenge","abstract":"Surround-view depth estimation is a crucial task aims to acquire the depth maps of the surrounding views. It has many applications in real world scenarios such as autonomous driving, AR/VR and 3D reconstruction, etc. However, given that most of the data in the autonomous driving dataset is collected in daytime scenarios, this leads to poor depth model performance in the face of out-of-distribution(OoD) data. While some works try to improve the robustness of depth model under OoD data, these methods either require additional training data or lake generalizability. In this report, we introduce the DINO-SD, a novel surround-view depth estimation model. Our DINO-SD does not need additional data and has strong robustness. Our DINO-SD get the best performance in the track4 of ICRA 2024 RoboDepth Challenge.","sentences":["Surround-view depth estimation is a crucial task aims to acquire the depth maps of the surrounding views.","It has many applications in real world scenarios such as autonomous driving, AR/VR and 3D reconstruction, etc.","However, given that most of the data in the autonomous driving dataset is collected in daytime scenarios, this leads to poor depth model performance in the face of out-of-distribution(OoD) data.","While some works try to improve the robustness of depth model under OoD data, these methods either require additional training data or lake generalizability.","In this report, we introduce the DINO-SD, a novel surround-view depth estimation model.","Our DINO-SD does not need additional data and has strong robustness.","Our DINO-SD get the best performance in the track4 of ICRA 2024 RoboDepth Challenge."],"url":"http://arxiv.org/abs/2405.17102v1","category":"cs.CV"}
{"created":"2024-05-27 12:08:47","title":"Non-invertible quasihomogeneus singularities and their Landau-Ginzburg orbifolds","abstract":"According to the classification of quasihomogeneus singularities, any polynomial $f$ defining such singularity has a decomposition $f = f_\\kappa + f_{add}$. The polynomial $f_\\kappa$ is of the certain form while $f_{add}$ is only restricted by the condition that the singularity of $f$ should be isolated. The polynomial $f_{add}$ is zero if and only if $f$ is invertible, and in the non-invertible case $f_{add}$ is arbitrary complicated. In this paper we investigate all possible polynomials $f_{add}$ for a given non-invertible $f$. For a given $f_\\kappa$ we introduce the specific small collection of monomials that build up $f_{add}$ such that the polynomial $f = f_\\kappa + f_{add}$ defines an isolated quasihomogeneus singularity. If $(f,\\mathbf{Z}/2\\mathbf{Z})$ is Landau-Ginzburg orbifold with such non-invertible polynomial $f$, we provide the quasihomogeneus polynomial $\\bar{f}$ such that the orbifold equivalence $(f,\\mathbf{Z}/2\\mathbf{Z}) \\sim (\\bar{f}, \\{id\\})$ holds. We also give the explicit isomorphism between the corresponding Frobenius algebras.","sentences":["According to the classification of quasihomogeneus singularities, any polynomial $f$ defining such singularity has a decomposition $f = f_\\kappa","+","f_{add}$. The polynomial $f_\\kappa$ is of the certain form while $f_{add}$ is only restricted by the condition that the singularity of $f$ should be isolated.","The polynomial $f_{add}$ is zero if and only if $f$ is invertible, and in the non-invertible case $f_{add}$ is arbitrary complicated.","In this paper we investigate all possible polynomials $f_{add}$ for a given non-invertible $f$. For a given $f_\\kappa$ we introduce the specific small collection of monomials that build up $f_{add}$ such that the polynomial $f = f_\\kappa + f_{add}$ defines an isolated quasihomogeneus singularity.","If $(f,\\mathbf{Z}/2\\mathbf{Z})$ is Landau-Ginzburg orbifold with such non-invertible polynomial $f$, we provide the quasihomogeneus polynomial $\\bar{f}$ such that the orbifold equivalence $(f,\\mathbf{Z}/2\\mathbf{Z}) \\sim (\\bar{f}, \\{id\\})$ holds.","We also give the explicit isomorphism between the corresponding Frobenius algebras."],"url":"http://arxiv.org/abs/2405.17091v1","category":"math.AG"}
{"created":"2024-05-27 11:59:31","title":"Inverse reinforcement learning by expert imitation for the stochastic linear-quadratic optimal control problem","abstract":"This article studies inverse reinforcement learning (IRL) for the stochastic linear-quadratic optimal control problem, where two agents are considered. A learner agent does not know the expert agent's performance cost function, but it imitates the behavior of the expert agent by constructing an underlying cost function that obtains the same optimal feedback control as the expert's. We first develop a model-based IRL algorithm, which consists of a policy correction and a policy update from the policy iteration in reinforcement learning, as well as a cost function weight reconstruction based on the inverse optimal control. Then, under this scheme, we propose a model-free off-policy IRL algorithm, which does not need to know or identify the system and only needs to collect the behavior data of the expert agent and learner agent once during the iteration process. Moreover, the proofs of the algorithm's convergence, stability, and non-unique solutions are given. Finally, a simulation example is provided to verify the effectiveness of the proposed algorithm.","sentences":["This article studies inverse reinforcement learning (IRL) for the stochastic linear-quadratic optimal control problem, where two agents are considered.","A learner agent does not know the expert agent's performance cost function, but it imitates the behavior of the expert agent by constructing an underlying cost function that obtains the same optimal feedback control as the expert's.","We first develop a model-based IRL algorithm, which consists of a policy correction and a policy update from the policy iteration in reinforcement learning, as well as a cost function weight reconstruction based on the inverse optimal control.","Then, under this scheme, we propose a model-free off-policy IRL algorithm, which does not need to know or identify the system and only needs to collect the behavior data of the expert agent and learner agent once during the iteration process.","Moreover, the proofs of the algorithm's convergence, stability, and non-unique solutions are given.","Finally, a simulation example is provided to verify the effectiveness of the proposed algorithm."],"url":"http://arxiv.org/abs/2405.17085v1","category":"math.OC"}
{"created":"2024-05-27 11:29:54","title":"Comparative Study of Machine Learning Algorithms in Detecting Cardiovascular Diseases","abstract":"The detection of cardiovascular diseases (CVD) using machine learning techniques represents a significant advancement in medical diagnostics, aiming to enhance early detection, accuracy, and efficiency. This study explores a comparative analysis of various machine learning algorithms, including Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and XGBoost. By utilising a structured workflow encompassing data collection, preprocessing, model selection and hyperparameter tuning, training, evaluation, and choice of the optimal model, this research addresses the critical need for improved diagnostic tools. The findings highlight the efficacy of ensemble methods and advanced algorithms in providing reliable predictions, thereby offering a comprehensive framework for CVD detection that can be readily implemented and adapted in clinical settings.","sentences":["The detection of cardiovascular diseases (CVD) using machine learning techniques represents a significant advancement in medical diagnostics, aiming to enhance early detection, accuracy, and efficiency.","This study explores a comparative analysis of various machine learning algorithms, including Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and XGBoost.","By utilising a structured workflow encompassing data collection, preprocessing, model selection and hyperparameter tuning, training, evaluation, and choice of the optimal model, this research addresses the critical need for improved diagnostic tools.","The findings highlight the efficacy of ensemble methods and advanced algorithms in providing reliable predictions, thereby offering a comprehensive framework for CVD detection that can be readily implemented and adapted in clinical settings."],"url":"http://arxiv.org/abs/2405.17059v1","category":"cs.LG"}
{"created":"2024-05-27 10:39:18","title":"Exact phylodynamic likelihood via structured Markov genealogy processes","abstract":"We consider genealogies arising from a Markov population process in which individuals are categorized into a discrete collection of compartments, with the requirement that individuals within the same compartment are statistically exchangeable. When equipped with a sampling process, each such population process induces a time-evolving tree-valued process defined as the genealogy of all sampled individuals. We provide a construction of this genealogy process and derive exact expressions for the likelihood of an observed genealogy in terms of filter equations. These filter equations can be numerically solved using standard Monte Carlo integration methods. Thus, we obtain statistically efficient likelihood-based inference for essentially arbitrary compartment models based on an observed genealogy of individuals sampled from the population.","sentences":["We consider genealogies arising from a Markov population process in which individuals are categorized into a discrete collection of compartments, with the requirement that individuals within the same compartment are statistically exchangeable.","When equipped with a sampling process, each such population process induces a time-evolving tree-valued process defined as the genealogy of all sampled individuals.","We provide a construction of this genealogy process and derive exact expressions for the likelihood of an observed genealogy in terms of filter equations.","These filter equations can be numerically solved using standard Monte Carlo integration methods.","Thus, we obtain statistically efficient likelihood-based inference for essentially arbitrary compartment models based on an observed genealogy of individuals sampled from the population."],"url":"http://arxiv.org/abs/2405.17032v1","category":"q-bio.QM"}
{"created":"2024-05-27 10:25:03","title":"Beware of Overestimated Decoding Performance Arising from Temporal Autocorrelations in Electroencephalogram Signals","abstract":"Researchers have reported high decoding accuracy (>95%) using non-invasive Electroencephalogram (EEG) signals for brain-computer interface (BCI) decoding tasks like image decoding, emotion recognition, auditory spatial attention detection, etc. Since these EEG data were usually collected with well-designed paradigms in labs, the reliability and robustness of the corresponding decoding methods were doubted by some researchers, and they argued that such decoding accuracy was overestimated due to the inherent temporal autocorrelation of EEG signals. However, the coupling between the stimulus-driven neural responses and the EEG temporal autocorrelations makes it difficult to confirm whether this overestimation exists in truth. Furthermore, the underlying pitfalls behind overestimated decoding accuracy have not been fully explained due to a lack of appropriate formulation. In this work, we formulate the pitfall in various EEG decoding tasks in a unified framework. EEG data were recorded from watermelons to remove stimulus-driven neural responses. Labels were assigned to continuous EEG according to the experimental design for EEG recording of several typical datasets, and then the decoding methods were conducted. The results showed the label can be successfully decoded as long as continuous EEG data with the same label were split into training and test sets. Further analysis indicated that high accuracy of various BCI decoding tasks could be achieved by associating labels with EEG intrinsic temporal autocorrelation features. These results underscore the importance of choosing the right experimental designs and data splits in BCI decoding tasks to prevent inflated accuracies due to EEG temporal autocorrelation.","sentences":["Researchers have reported high decoding accuracy (>95%) using non-invasive Electroencephalogram (EEG) signals for brain-computer interface (BCI) decoding tasks like image decoding, emotion recognition, auditory spatial attention detection, etc.","Since these EEG data were usually collected with well-designed paradigms in labs, the reliability and robustness of the corresponding decoding methods were doubted by some researchers, and they argued that such decoding accuracy was overestimated due to the inherent temporal autocorrelation of EEG signals.","However, the coupling between the stimulus-driven neural responses and the EEG temporal autocorrelations makes it difficult to confirm whether this overestimation exists in truth.","Furthermore, the underlying pitfalls behind overestimated decoding accuracy have not been fully explained due to a lack of appropriate formulation.","In this work, we formulate the pitfall in various EEG decoding tasks in a unified framework.","EEG data were recorded from watermelons to remove stimulus-driven neural responses.","Labels were assigned to continuous EEG according to the experimental design for EEG recording of several typical datasets, and then the decoding methods were conducted.","The results showed the label can be successfully decoded as long as continuous EEG data with the same label were split into training and test sets.","Further analysis indicated that high accuracy of various BCI decoding tasks could be achieved by associating labels with EEG intrinsic temporal autocorrelation features.","These results underscore the importance of choosing the right experimental designs and data splits in BCI decoding tasks to prevent inflated accuracies due to EEG temporal autocorrelation."],"url":"http://arxiv.org/abs/2405.17024v1","category":"eess.SP"}
{"created":"2024-05-27 09:54:50","title":"Position: Foundation Agents as the Paradigm Shift for Decision Making","abstract":"Decision making demands intricate interplay between perception, memory, and reasoning to discern optimal policies. Conventional approaches to decision making face challenges related to low sample efficiency and poor generalization. In contrast, foundation models in language and vision has showcased rapid adaptation to diverse new tasks. Therefore, we advocate for the construction of foundation agents as a transformative shift in the learning paradigm of agents. This proposal is underpinned by the formulation of foundation agents with its fundamental characteristics and challenges motivated by the success of large language models (LLMs). Moreover, we specify the roadmap of foundation agents from large interactive data collection or generation, to self-supervised pretraining and adaptation, and knowledge and value alignment with LLMs. Lastly, we pinpoint critical research questions derived from the formulation and delineate trends for foundation agents supported by real-world use cases, addressing both technical and theoretical aspects to propel the field towards a more comprehensive and impactful future.","sentences":["Decision making demands intricate interplay between perception, memory, and reasoning to discern optimal policies.","Conventional approaches to decision making face challenges related to low sample efficiency and poor generalization.","In contrast, foundation models in language and vision has showcased rapid adaptation to diverse new tasks.","Therefore, we advocate for the construction of foundation agents as a transformative shift in the learning paradigm of agents.","This proposal is underpinned by the formulation of foundation agents with its fundamental characteristics and challenges motivated by the success of large language models (LLMs).","Moreover, we specify the roadmap of foundation agents from large interactive data collection or generation, to self-supervised pretraining and adaptation, and knowledge and value alignment with LLMs.","Lastly, we pinpoint critical research questions derived from the formulation and delineate trends for foundation agents supported by real-world use cases, addressing both technical and theoretical aspects to propel the field towards a more comprehensive and impactful future."],"url":"http://arxiv.org/abs/2405.17009v1","category":"cs.AI"}
{"created":"2024-05-27 09:42:04","title":"Vision-and-Language Navigation Generative Pretrained Transformer","abstract":"In the Vision-and-Language Navigation (VLN) field, agents are tasked with navigating real-world scenes guided by linguistic instructions. Enabling the agent to adhere to instructions throughout the process of navigation represents a significant challenge within the domain of VLN. To address this challenge, common approaches often rely on encoders to explicitly record past locations and actions, increasing model complexity and resource consumption.   Our proposal, the Vision-and-Language Navigation Generative Pretrained Transformer (VLN-GPT), adopts a transformer decoder model (GPT2) to model trajectory sequence dependencies, bypassing the need for historical encoding modules. This method allows for direct historical information access through trajectory sequence, enhancing efficiency. Furthermore, our model separates the training process into offline pre-training with imitation learning and online fine-tuning with reinforcement learning. This distinction allows for more focused training objectives and improved performance.   Performance assessments on the VLN dataset reveal that VLN-GPT surpasses complex state-of-the-art encoder-based models.","sentences":["In the Vision-and-Language Navigation (VLN) field, agents are tasked with navigating real-world scenes guided by linguistic instructions.","Enabling the agent to adhere to instructions throughout the process of navigation represents a significant challenge within the domain of VLN.","To address this challenge, common approaches often rely on encoders to explicitly record past locations and actions, increasing model complexity and resource consumption.   ","Our proposal, the Vision-and-Language Navigation Generative Pretrained Transformer (VLN-GPT), adopts a transformer decoder model (GPT2) to model trajectory sequence dependencies, bypassing the need for historical encoding modules.","This method allows for direct historical information access through trajectory sequence, enhancing efficiency.","Furthermore, our model separates the training process into offline pre-training with imitation learning and online fine-tuning with reinforcement learning.","This distinction allows for more focused training objectives and improved performance.   ","Performance assessments on the VLN dataset reveal that VLN-GPT surpasses complex state-of-the-art encoder-based models."],"url":"http://arxiv.org/abs/2405.16994v1","category":"cs.AI"}
{"created":"2024-05-27 09:11:10","title":"Inherent quantum resources in the stationary spin chains","abstract":"The standard way to generate many-body quantum correlations is via a dynamical protocol: an initial product state is transformed by interactions that generate non-classical correlations at later times. Here, we show that many-body Bell correlations are inherently present in the eigenstates of a variety of spin-1/2 chains. In particular, we show that the eigenstates and thermal states of the collective Lipkin-Meshkov-Glick model possess many-body Bell correlations. We demonstrate that the Bell correlations can take on quantized values that change discontinuously with variations in the total magnetization. Finally, we show that these many-body Bell correlations persist even in the presence of both diagonal and off-diagonal disorder.","sentences":["The standard way to generate many-body quantum correlations is via a dynamical protocol: an initial product state is transformed by interactions that generate non-classical correlations at later times.","Here, we show that many-body Bell correlations are inherently present in the eigenstates of a variety of spin-1/2 chains.","In particular, we show that the eigenstates and thermal states of the collective Lipkin-Meshkov-Glick model possess many-body Bell correlations.","We demonstrate that the Bell correlations can take on quantized values that change discontinuously with variations in the total magnetization.","Finally, we show that these many-body Bell correlations persist even in the presence of both diagonal and off-diagonal disorder."],"url":"http://arxiv.org/abs/2405.16974v1","category":"quant-ph"}
{"created":"2024-05-27 09:08:55","title":"Collective Perception Datasets for Autonomous Driving: A Comprehensive Review","abstract":"To ensure safe operation of autonomous vehicles in complex urban environments, complete perception of the environment is necessary. However, due to environmental conditions, sensor limitations, and occlusions, this is not always possible from a single point of view. To address this issue, collective perception is an effective method. Realistic and large-scale datasets are essential for training and evaluating collective perception methods. This paper provides the first comprehensive technical review of collective perception datasets in the context of autonomous driving. The survey analyzes existing V2V and V2X datasets, categorizing them based on different criteria such as sensor modalities, environmental conditions, and scenario variety. The focus is on their applicability for the development of connected automated vehicles. This study aims to identify the key criteria of all datasets and to present their strengths, weaknesses, and anomalies. Finally, this survey concludes by making recommendations regarding which dataset is most suitable for collective 3D object detection, tracking, and semantic segmentation.","sentences":["To ensure safe operation of autonomous vehicles in complex urban environments, complete perception of the environment is necessary.","However, due to environmental conditions, sensor limitations, and occlusions, this is not always possible from a single point of view.","To address this issue, collective perception is an effective method.","Realistic and large-scale datasets are essential for training and evaluating collective perception methods.","This paper provides the first comprehensive technical review of collective perception datasets in the context of autonomous driving.","The survey analyzes existing V2V and V2X datasets, categorizing them based on different criteria such as sensor modalities, environmental conditions, and scenario variety.","The focus is on their applicability for the development of connected automated vehicles.","This study aims to identify the key criteria of all datasets and to present their strengths, weaknesses, and anomalies.","Finally, this survey concludes by making recommendations regarding which dataset is most suitable for collective 3D object detection, tracking, and semantic segmentation."],"url":"http://arxiv.org/abs/2405.16973v1","category":"cs.CV"}
{"created":"2024-05-27 08:57:04","title":"Exploring the LLM Journey from Cognition to Expression with Linear Representations","abstract":"This paper presents an in-depth examination of the evolution and interplay of cognitive and expressive capabilities in large language models (LLMs), with a specific focus on Baichuan-7B and Baichuan-33B, an advanced bilingual (Chinese and English) LLM series. We define and explore the model's cognitive and expressive capabilities through linear representations across three critical phases: Pretraining, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). Cognitive capability is defined as the quantity and quality of information conveyed by the neuron output vectors within the network, similar to the neural signal processing in human cognition. Expressive capability is defined as the model's capability to produce word-level output. Our findings unveil a sequential development pattern, where cognitive abilities are largely established during Pretraining, whereas expressive abilities predominantly advance during SFT and RLHF. Statistical analyses confirm a significant correlation between the two capabilities, suggesting that cognitive capacity may limit expressive potential. The paper also explores the theoretical underpinnings of these divergent developmental trajectories and their connection to the LLMs' architectural design. Moreover, we evaluate various optimization-independent strategies, such as few-shot learning and repeated sampling, which bridge the gap between cognitive and expressive capabilities. This research reveals the potential connection between the hidden space and the output space, contributing valuable insights into the interpretability and controllability of their training processes.","sentences":["This paper presents an in-depth examination of the evolution and interplay of cognitive and expressive capabilities in large language models (LLMs), with a specific focus on Baichuan-7B and Baichuan-33B, an advanced bilingual (Chinese and English) LLM series.","We define and explore the model's cognitive and expressive capabilities through linear representations across three critical phases: Pretraining, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF).","Cognitive capability is defined as the quantity and quality of information conveyed by the neuron output vectors within the network, similar to the neural signal processing in human cognition.","Expressive capability is defined as the model's capability to produce word-level output.","Our findings unveil a sequential development pattern, where cognitive abilities are largely established during Pretraining, whereas expressive abilities predominantly advance during SFT and RLHF.","Statistical analyses confirm a significant correlation between the two capabilities, suggesting that cognitive capacity may limit expressive potential.","The paper also explores the theoretical underpinnings of these divergent developmental trajectories and their connection to the LLMs' architectural design.","Moreover, we evaluate various optimization-independent strategies, such as few-shot learning and repeated sampling, which bridge the gap between cognitive and expressive capabilities.","This research reveals the potential connection between the hidden space and the output space, contributing valuable insights into the interpretability and controllability of their training processes."],"url":"http://arxiv.org/abs/2405.16964v1","category":"cs.CL"}
{"created":"2024-05-27 08:55:22","title":"Blind Data Adaptation to tackle Covariate Shift in Operational Steganalysis","abstract":"The proliferation of image manipulation for unethical purposes poses significant challenges in social networks. One particularly concerning method is Image Steganography, allowing individuals to hide illegal information in digital images without arousing suspicions. Such a technique pose severe security risks, making it crucial to develop effective steganalysis methods enabling to detect manipulated images for clandestine communications. Although significant advancements have been achieved with machine learning models, a critical issue remains: the disparity between the controlled datasets used to train steganalysis models against real-world datasets of forensic practitioners, undermining severely the practical effectiveness of standardized steganalysis models. In this paper, we address this issue focusing on a realistic scenario where practitioners lack crucial information about the limited target set of images under analysis, including details about their development process and even whereas it contains manipulated images or not. By leveraging geometric alignment and distribution matching of source and target residuals, we develop TADA (Target Alignment through Data Adaptation), a novel methodology enabling to emulate sources aligned with specific targets in steganalysis, which is also relevant for highly unbalanced targets. The emulator is represented by a light convolutional network trained to align distributions of image residuals. Experimental validation demonstrates the potential of our strategy over traditional methods fighting covariate shift in steganalysis.","sentences":["The proliferation of image manipulation for unethical purposes poses significant challenges in social networks.","One particularly concerning method is Image Steganography, allowing individuals to hide illegal information in digital images without arousing suspicions.","Such a technique pose severe security risks, making it crucial to develop effective steganalysis methods enabling to detect manipulated images for clandestine communications.","Although significant advancements have been achieved with machine learning models, a critical issue remains: the disparity between the controlled datasets used to train steganalysis models against real-world datasets of forensic practitioners, undermining severely the practical effectiveness of standardized steganalysis models.","In this paper, we address this issue focusing on a realistic scenario where practitioners lack crucial information about the limited target set of images under analysis, including details about their development process and even whereas it contains manipulated images or not.","By leveraging geometric alignment and distribution matching of source and target residuals, we develop TADA (Target Alignment through Data Adaptation), a novel methodology enabling to emulate sources aligned with specific targets in steganalysis, which is also relevant for highly unbalanced targets.","The emulator is represented by a light convolutional network trained to align distributions of image residuals.","Experimental validation demonstrates the potential of our strategy over traditional methods fighting covariate shift in steganalysis."],"url":"http://arxiv.org/abs/2405.16961v1","category":"eess.IV"}
{"created":"2024-05-27 08:46:57","title":"Functional Programming Paradigm of Python for Scientific Computation Pipeline Integration","abstract":"The advent of modern data processing has led to an increasing tendency towards interdisciplinarity, which frequently involves the importation of different technical approaches. Consequently, there is an urgent need for a unified data control system to facilitate the integration of varying libraries. This integration is of profound significance in accelerating prototype verification, optimising algorithm performance and minimising maintenance costs. This paper presents a novel functional programming (FP) paradigm based on the Python architecture and associated suites in programming practice, designed for the integration of pipelines of different data mapping operations. In particular, the solution is intended for the integration of scientific computation flows, which affords a robust yet flexible solution for the aforementioned challenges.","sentences":["The advent of modern data processing has led to an increasing tendency towards interdisciplinarity, which frequently involves the importation of different technical approaches.","Consequently, there is an urgent need for a unified data control system to facilitate the integration of varying libraries.","This integration is of profound significance in accelerating prototype verification, optimising algorithm performance and minimising maintenance costs.","This paper presents a novel functional programming (FP) paradigm based on the Python architecture and associated suites in programming practice, designed for the integration of pipelines of different data mapping operations.","In particular, the solution is intended for the integration of scientific computation flows, which affords a robust yet flexible solution for the aforementioned challenges."],"url":"http://arxiv.org/abs/2405.16956v1","category":"cs.LG"}
{"created":"2024-05-27 08:42:42","title":"Fast ML-driven Analog Circuit Layout using Reinforcement Learning and Steiner Trees","abstract":"This paper presents an artificial intelligence driven methodology to reduce the bottleneck often encountered in the analog ICs layout phase. We frame the floorplanning problem as a Markov Decision Process and leverage reinforcement learning for automatic placement generation under established topological constraints. Consequently, we introduce Steiner tree-based methods for the global routing step and generate guiding paths to be used to connect every circuit block. Finally, by integrating these solutions into a procedural generation framework, we present a unified pipeline that bridges the divide between circuit design and verification steps. Experimental results demonstrate the efficacy in generating complete layouts, eventually reducing runtimes to 1.5% compared to manual efforts.","sentences":["This paper presents an artificial intelligence driven methodology to reduce the bottleneck often encountered in the analog ICs layout phase.","We frame the floorplanning problem as a Markov Decision Process and leverage reinforcement learning for automatic placement generation under established topological constraints.","Consequently, we introduce Steiner tree-based methods for the global routing step and generate guiding paths to be used to connect every circuit block.","Finally, by integrating these solutions into a procedural generation framework, we present a unified pipeline that bridges the divide between circuit design and verification steps.","Experimental results demonstrate the efficacy in generating complete layouts, eventually reducing runtimes to 1.5% compared to manual efforts."],"url":"http://arxiv.org/abs/2405.16951v1","category":"cs.LG"}
{"created":"2024-05-27 08:38:17","title":"Biological Neurons Compete with Deep Reinforcement Learning in Sample Efficiency in a Simulated Gameworld","abstract":"How do biological systems and machine learning algorithms compare in the number of samples required to show significant improvements in completing a task? We compared the learning efficiency of in vitro biological neural networks to the state-of-the-art deep reinforcement learning (RL) algorithms in a simplified simulation of the game `Pong'. Using DishBrain, a system that embodies in vitro neural networks with in silico computation using a high-density multi-electrode array, we contrasted the learning rate and the performance of these biological systems against time-matched learning from three state-of-the-art deep RL algorithms (i.e., DQN, A2C, and PPO) in the same game environment. This allowed a meaningful comparison between biological neural systems and deep RL. We find that when samples are limited to a real-world time course, even these very simple biological cultures outperformed deep RL algorithms across various game performance characteristics, implying a higher sample efficiency. Ultimately, even when tested across multiple types of information input to assess the impact of higher dimensional data input, biological neurons showcased faster learning than all deep reinforcement learning agents.","sentences":["How do biological systems and machine learning algorithms compare in the number of samples required to show significant improvements in completing a task?","We compared the learning efficiency of in vitro biological neural networks to the state-of-the-art deep reinforcement learning (RL) algorithms in a simplified simulation of the game `Pong'.","Using DishBrain, a system that embodies in vitro neural networks with in silico computation using a high-density multi-electrode array, we contrasted the learning rate and the performance of these biological systems against time-matched learning from three state-of-the-art deep RL algorithms (i.e., DQN, A2C, and PPO) in the same game environment.","This allowed a meaningful comparison between biological neural systems and deep RL.","We find that when samples are limited to a real-world time course, even these very simple biological cultures outperformed deep RL algorithms across various game performance characteristics, implying a higher sample efficiency.","Ultimately, even when tested across multiple types of information input to assess the impact of higher dimensional data input, biological neurons showcased faster learning than all deep reinforcement learning agents."],"url":"http://arxiv.org/abs/2405.16946v1","category":"q-bio.NC"}
{"created":"2024-05-27 08:24:42","title":"From Obstacle to Opportunity: Enhancing Semi-supervised Learning with Synthetic Data","abstract":"Semi-supervised learning (SSL) can utilize unlabeled data to enhance model performance. In recent years, with increasingly powerful generative models becoming available, a large number of synthetic images have been uploaded to public image sets. Therefore, when collecting unlabeled data from these sources, the inclusion of synthetic images is inevitable. This prompts us to consider the impact of unlabeled data mixed with real and synthetic images on SSL. In this paper, we set up a new task, Real and Synthetic hybrid SSL (RS-SSL), to investigate this problem. We discover that current SSL methods are unable to fully utilize synthetic data and are sometimes negatively affected. Then, by analyzing the issues caused by synthetic images, we propose a new SSL method, RSMatch, to tackle the RS-SSL problem. Extensive experimental results show that RSMatch can better utilize the synthetic data in unlabeled images to improve the SSL performance. The effectiveness is further verified through ablation studies and visualization.","sentences":["Semi-supervised learning (SSL) can utilize unlabeled data to enhance model performance.","In recent years, with increasingly powerful generative models becoming available, a large number of synthetic images have been uploaded to public image sets.","Therefore, when collecting unlabeled data from these sources, the inclusion of synthetic images is inevitable.","This prompts us to consider the impact of unlabeled data mixed with real and synthetic images on SSL.","In this paper, we set up a new task, Real and Synthetic hybrid SSL (RS-SSL), to investigate this problem.","We discover that current SSL methods are unable to fully utilize synthetic data and are sometimes negatively affected.","Then, by analyzing the issues caused by synthetic images, we propose a new SSL method, RSMatch, to tackle the RS-SSL problem.","Extensive experimental results show that RSMatch can better utilize the synthetic data in unlabeled images to improve the SSL performance.","The effectiveness is further verified through ablation studies and visualization."],"url":"http://arxiv.org/abs/2405.16930v1","category":"cs.CV"}
{"created":"2024-05-27 08:22:52","title":"Uncertainty Management in the Construction of Knowledge Graphs: a Survey","abstract":"Knowledge Graphs (KGs) are a major asset for companies thanks to their great flexibility in data representation and their numerous applications, e.g., vocabulary sharing, Q/A or recommendation systems. To build a KG it is a common practice to rely on automatic methods for extracting knowledge from various heterogeneous sources. But in a noisy and uncertain world, knowledge may not be reliable and conflicts between data sources may occur. Integrating unreliable data would directly impact the use of the KG, therefore such conflicts must be resolved. This could be done manually by selecting the best data to integrate. This first approach is highly accurate, but costly and time-consuming. That is why recent efforts focus on automatic approaches, which represents a challenging task since it requires handling the uncertainty of extracted knowledge throughout its integration into the KG. We survey state-of-the-art approaches in this direction and present constructions of both open and enterprise KGs and how their quality is maintained. We then describe different knowledge extraction methods, introducing additional uncertainty. We also discuss downstream tasks after knowledge acquisition, including KG completion using embedding models, knowledge alignment, and knowledge fusion in order to address the problem of knowledge uncertainty in KG construction. We conclude with a discussion on the remaining challenges and perspectives when constructing a KG taking into account uncertainty.","sentences":["Knowledge Graphs (KGs) are a major asset for companies thanks to their great flexibility in data representation and their numerous applications, e.g., vocabulary sharing, Q/A or recommendation systems.","To build a KG it is a common practice to rely on automatic methods for extracting knowledge from various heterogeneous sources.","But in a noisy and uncertain world, knowledge may not be reliable and conflicts between data sources may occur.","Integrating unreliable data would directly impact the use of the KG, therefore such conflicts must be resolved.","This could be done manually by selecting the best data to integrate.","This first approach is highly accurate, but costly and time-consuming.","That is why recent efforts focus on automatic approaches, which represents a challenging task since it requires handling the uncertainty of extracted knowledge throughout its integration into the KG.","We survey state-of-the-art approaches in this direction and present constructions of both open and enterprise KGs and how their quality is maintained.","We then describe different knowledge extraction methods, introducing additional uncertainty.","We also discuss downstream tasks after knowledge acquisition, including KG completion using embedding models, knowledge alignment, and knowledge fusion in order to address the problem of knowledge uncertainty in KG construction.","We conclude with a discussion on the remaining challenges and perspectives when constructing a KG taking into account uncertainty."],"url":"http://arxiv.org/abs/2405.16929v1","category":"cs.AI"}
{"created":"2024-05-27 08:20:50","title":"Is Cambodia the World's Largest Cashew Producer?","abstract":"Cambodia's agricultural landscape is rapidly transforming, particularly in the cashew sector. Despite the country's rapid emergence and ambition to become the largest cashew producer, comprehensive data on plantation areas and the environmental impacts of this expansion are lacking. This study addresses the gap in detailed land use data for cashew plantations in Cambodia and assesses the implications of agricultural advancements. We collected over 80,000 training polygons across Cambodia to train a convolutional neural network using high-resolution optical and SAR satellite data for precise cashew plantation mapping. Our findings indicate that Cambodia ranks among the top five in terms of cultivated area and the top three in global cashew production, driven by high yields. Significant cultivated areas are located in Kampong Thom, Kratie, and Ratanak Kiri provinces. Balancing rapid agricultural expansion with environmental stewardship, particularly forest conservation, is crucial. Cambodia's cashew production is poised for further growth, driven by high-yielding trees and premium nuts. However, sustainable expansion requires integrating agricultural practices with economic and environmental strategies to enhance local value and protect forested areas. Advanced mapping technologies offer comprehensive tools to support these objectives and ensure the sustainable development of Cambodia's cashew industry.","sentences":["Cambodia's agricultural landscape is rapidly transforming, particularly in the cashew sector.","Despite the country's rapid emergence and ambition to become the largest cashew producer, comprehensive data on plantation areas and the environmental impacts of this expansion are lacking.","This study addresses the gap in detailed land use data for cashew plantations in Cambodia and assesses the implications of agricultural advancements.","We collected over 80,000 training polygons across Cambodia to train a convolutional neural network using high-resolution optical and SAR satellite data for precise cashew plantation mapping.","Our findings indicate that Cambodia ranks among the top five in terms of cultivated area and the top three in global cashew production, driven by high yields.","Significant cultivated areas are located in Kampong Thom, Kratie, and Ratanak Kiri provinces.","Balancing rapid agricultural expansion with environmental stewardship, particularly forest conservation, is crucial.","Cambodia's cashew production is poised for further growth, driven by high-yielding trees and premium nuts.","However, sustainable expansion requires integrating agricultural practices with economic and environmental strategies to enhance local value and protect forested areas.","Advanced mapping technologies offer comprehensive tools to support these objectives and ensure the sustainable development of Cambodia's cashew industry."],"url":"http://arxiv.org/abs/2405.16926v1","category":"cs.CY"}
{"created":"2024-05-27 08:13:39","title":"Theories of synaptic memory consolidation and intelligent plasticity for continual learning","abstract":"Humans and animals learn throughout life. Such continual learning is crucial for intelligence. In this chapter, we examine the pivotal role plasticity mechanisms with complex internal synaptic dynamics could play in enabling this ability in neural networks. By surveying theoretical research, we highlight two fundamental enablers for continual learning. First, synaptic plasticity mechanisms must maintain and evolve an internal state over several behaviorally relevant timescales. Second, plasticity algorithms must leverage the internal state to intelligently regulate plasticity at individual synapses to facilitate the seamless integration of new memories while avoiding detrimental interference with existing ones. Our chapter covers successful applications of these principles to deep neural networks and underscores the significance of synaptic metaplasticity in sustaining continual learning capabilities. Finally, we outline avenues for further research to understand the brain's superb continual learning abilities and harness similar mechanisms for artificial intelligence systems.","sentences":["Humans and animals learn throughout life.","Such continual learning is crucial for intelligence.","In this chapter, we examine the pivotal role plasticity mechanisms with complex internal synaptic dynamics could play in enabling this ability in neural networks.","By surveying theoretical research, we highlight two fundamental enablers for continual learning.","First, synaptic plasticity mechanisms must maintain and evolve an internal state over several behaviorally relevant timescales.","Second, plasticity algorithms must leverage the internal state to intelligently regulate plasticity at individual synapses to facilitate the seamless integration of new memories while avoiding detrimental interference with existing ones.","Our chapter covers successful applications of these principles to deep neural networks and underscores the significance of synaptic metaplasticity in sustaining continual learning capabilities.","Finally, we outline avenues for further research to understand the brain's superb continual learning abilities and harness similar mechanisms for artificial intelligence systems."],"url":"http://arxiv.org/abs/2405.16922v1","category":"q-bio.NC"}
{"created":"2024-05-27 08:12:00","title":"VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models","abstract":"While large multi-modal models (LMMs) have exhibited impressive capabilities across diverse tasks, their effectiveness in handling complex tasks has been limited by the prevailing single-step reasoning paradigm. To this end, this paper proposes VoCoT, a multi-step Visually grounded object-centric Chain-of-Thought reasoning framework tailored for inference with LMMs. VoCoT is characterized by two key features: (1) object-centric reasoning paths that revolve around cross-modal shared object-level information, and (2) visually grounded representation of object concepts in a multi-modal interleaved and aligned manner, which effectively bridges the modality gap within LMMs during long-term generation. Additionally, we construct an instruction dataset to facilitate LMMs in adapting to reasoning with VoCoT. By introducing VoCoT into the prevalent open-source LMM architecture, we introduce VolCano. With only 7B parameters and limited input resolution, VolCano demonstrates excellent performance across various scenarios, surpassing SOTA models, including GPT-4V, in tasks requiring complex reasoning. Our code, data and model will be available at https://github.com/RupertLuo/VoCoT.","sentences":["While large multi-modal models (LMMs) have exhibited impressive capabilities across diverse tasks, their effectiveness in handling complex tasks has been limited by the prevailing single-step reasoning paradigm.","To this end, this paper proposes VoCoT, a multi-step Visually grounded object-centric Chain-of-Thought reasoning framework tailored for inference with LMMs.","VoCoT is characterized by two key features: (1) object-centric reasoning paths that revolve around cross-modal shared object-level information, and (2) visually grounded representation of object concepts in a multi-modal interleaved and aligned manner, which effectively bridges the modality gap within LMMs during long-term generation.","Additionally, we construct an instruction dataset to facilitate LMMs in adapting to reasoning with VoCoT. By introducing VoCoT into the prevalent open-source LMM architecture, we introduce VolCano.","With only 7B parameters and limited input resolution, VolCano demonstrates excellent performance across various scenarios, surpassing SOTA models, including GPT-4V, in tasks requiring complex reasoning.","Our code, data and model will be available at https://github.com/RupertLuo/VoCoT."],"url":"http://arxiv.org/abs/2405.16919v1","category":"cs.CV"}
{"created":"2024-05-27 07:55:45","title":"GTA: Generative Trajectory Augmentation with Guidance for Offline Reinforcement Learning","abstract":"Offline Reinforcement Learning (Offline RL) presents challenges of learning effective decision-making policies from static datasets without any online interactions. Data augmentation techniques, such as noise injection and data synthesizing, aim to improve Q-function approximation by smoothing the learned state-action region. However, these methods often fall short of directly improving the quality of offline datasets, leading to suboptimal results. In response, we introduce \\textbf{GTA}, Generative Trajectory Augmentation, a novel generative data augmentation approach designed to enrich offline data by augmenting trajectories to be both high-rewarding and dynamically plausible. GTA applies a diffusion model within the data augmentation framework. GTA partially noises original trajectories and then denoises them with classifier-free guidance via conditioning on amplified return value. Our results show that GTA, as a general data augmentation strategy, enhances the performance of widely used offline RL algorithms in both dense and sparse reward settings. Furthermore, we conduct a quality analysis of data augmented by GTA and demonstrate that GTA improves the quality of the data. Our code is available at https://github.com/Jaewoopudding/GTA","sentences":["Offline Reinforcement Learning (Offline RL) presents challenges of learning effective decision-making policies from static datasets without any online interactions.","Data augmentation techniques, such as noise injection and data synthesizing, aim to improve Q-function approximation by smoothing the learned state-action region.","However, these methods often fall short of directly improving the quality of offline datasets, leading to suboptimal results.","In response, we introduce \\textbf{GTA}, Generative Trajectory Augmentation, a novel generative data augmentation approach designed to enrich offline data by augmenting trajectories to be both high-rewarding and dynamically plausible.","GTA applies a diffusion model within the data augmentation framework.","GTA partially noises original trajectories and then denoises them with classifier-free guidance via conditioning on amplified return value.","Our results show that GTA, as a general data augmentation strategy, enhances the performance of widely used offline RL algorithms in both dense and sparse reward settings.","Furthermore, we conduct a quality analysis of data augmented by GTA and demonstrate that GTA improves the quality of the data.","Our code is available at https://github.com/Jaewoopudding/GTA"],"url":"http://arxiv.org/abs/2405.16907v1","category":"cs.AI"}
{"created":"2024-05-27 07:49:30","title":"Recurrent and Convolutional Neural Networks in Classification of EEG Signal for Guided Imagery and Mental Workload Detection","abstract":"The Guided Imagery technique is reported to be used by therapists all over the world in order to increase the comfort of patients suffering from a variety of disorders from mental to oncology ones and proved to be successful in numerous of ways. Possible support for the therapists can be estimation of the time at which subject goes into deep relaxation. This paper presents the results of the investigations of a cohort of 26 students exposed to Guided Imagery relaxation technique and mental task workloads conducted with the use of dense array electroencephalographic amplifier. The research reported herein aimed at verification whether it is possible to detect differences between those two states and to classify them using deep learning methods and recurrent neural networks such as EEGNet, Long Short-Term Memory-based classifier, 1D Convolutional Neural Network and hybrid model of 1D Convolutional Neural Network and Long Short-Term Memory. The data processing pipeline was presented from the data acquisition, through the initial data cleaning, preprocessing and postprocessing. The classification was based on two datasets: one of them using 26 so-called cognitive electrodes and the other one using signal collected from 256 channels. So far there have not been such comparisons in the application being discussed. The classification results are presented by the validation metrics such as: accuracy, recall, precision, F1-score and loss for each case. It turned out that it is not necessary to collect signals from all electrodes as classification of the cognitive ones gives the results similar to those obtained for the full signal and extending input to 256 channels does not add much value. In Disscussion there were proposed an optimal classifier as well as some suggestions concerning the prospective development of the project.","sentences":["The Guided Imagery technique is reported to be used by therapists all over the world in order to increase the comfort of patients suffering from a variety of disorders from mental to oncology ones and proved to be successful in numerous of ways.","Possible support for the therapists can be estimation of the time at which subject goes into deep relaxation.","This paper presents the results of the investigations of a cohort of 26 students exposed to Guided Imagery relaxation technique and mental task workloads conducted with the use of dense array electroencephalographic amplifier.","The research reported herein aimed at verification whether it is possible to detect differences between those two states and to classify them using deep learning methods and recurrent neural networks such as EEGNet, Long Short-Term Memory-based classifier, 1D Convolutional Neural Network and hybrid model of 1D Convolutional Neural Network and Long Short-Term Memory.","The data processing pipeline was presented from the data acquisition, through the initial data cleaning, preprocessing and postprocessing.","The classification was based on two datasets: one of them using 26 so-called cognitive electrodes and the other one using signal collected from 256 channels.","So far there have not been such comparisons in the application being discussed.","The classification results are presented by the validation metrics such as: accuracy, recall, precision, F1-score and loss for each case.","It turned out that it is not necessary to collect signals from all electrodes as classification of the cognitive ones gives the results similar to those obtained for the full signal and extending input to 256 channels does not add much value.","In Disscussion there were proposed an optimal classifier as well as some suggestions concerning the prospective development of the project."],"url":"http://arxiv.org/abs/2405.16901v1","category":"cs.LG"}
{"created":"2024-05-27 07:46:36","title":"Partial Models for Building Adaptive Model-Based Reinforcement Learning Agents","abstract":"In neuroscience, one of the key behavioral tests for determining whether a subject of study exhibits model-based behavior is to study its adaptiveness to local changes in the environment. In reinforcement learning, however, recent studies have shown that modern model-based agents display poor adaptivity to such changes. The main reason for this is that modern agents are typically designed to improve sample efficiency in single task settings and thus do not take into account the challenges that can arise in other settings. In local adaptation settings, one particularly important challenge is in quickly building and maintaining a sufficiently accurate model after a local change. This is challenging for deep model-based agents as their models and replay buffers are monolithic structures lacking distribution shift handling capabilities. In this study, we show that the conceptually simple idea of partial models can allow deep model-based agents to overcome this challenge and thus allow for building locally adaptive model-based agents. By modeling the different parts of the state space through different models, the agent can not only maintain a model that is accurate across the state space, but it can also quickly adapt it in the presence of a local change in the environment. We demonstrate this by showing that the use of partial models in agents such as deep Dyna-Q, PlaNet and Dreamer can allow for them to effectively adapt to the local changes in their environments.","sentences":["In neuroscience, one of the key behavioral tests for determining whether a subject of study exhibits model-based behavior is to study its adaptiveness to local changes in the environment.","In reinforcement learning, however, recent studies have shown that modern model-based agents display poor adaptivity to such changes.","The main reason for this is that modern agents are typically designed to improve sample efficiency in single task settings and thus do not take into account the challenges that can arise in other settings.","In local adaptation settings, one particularly important challenge is in quickly building and maintaining a sufficiently accurate model after a local change.","This is challenging for deep model-based agents as their models and replay buffers are monolithic structures lacking distribution shift handling capabilities.","In this study, we show that the conceptually simple idea of partial models can allow deep model-based agents to overcome this challenge and thus allow for building locally adaptive model-based agents.","By modeling the different parts of the state space through different models, the agent can not only maintain a model that is accurate across the state space, but it can also quickly adapt it in the presence of a local change in the environment.","We demonstrate this by showing that the use of partial models in agents such as deep Dyna-Q, PlaNet and Dreamer can allow for them to effectively adapt to the local changes in their environments."],"url":"http://arxiv.org/abs/2405.16899v1","category":"cs.LG"}
{"created":"2024-05-27 07:27:41","title":"Cross Far- and Near-Field Channel Measurement and Modeling in Extremely Large-scale Antenna Array (ELAA) Systems","abstract":"Technologies like ultra-massive multiple-input-multiple-output (UM-MIMO) and reconfigurable intelligent surfaces (RISs) are of special interest to meet the key performance indicators of future wireless systems including ubiquitous connectivity and lightning-fast data rates. One of their common features, the extremely large-scale antenna array (ELAA) systems with hundreds or thousands of antennas, give rise to near-field (NF) propagation and bring new challenges to channel modeling and characterization. In this paper, a cross-field channel model for ELAA systems is proposed, which improves the statistical model in 3GPP TR 38.901 by refining the propagation path with its first and last bounces and differentiating the characterization of parameters like path loss, delay, and angles in near- and far-fields. A comprehensive analysis of cross-field boundaries and closed-form expressions of corresponding NF or FF parameters are provided. Furthermore, cross-field experiments carried out in a typical indoor scenario at 300 GHz verify the variation of MPC parameters across the antenna array, and demonstrate the distinction of channels between different antenna elements. Finally, detailed generation procedures of the cross-field channel model are provided, based on which simulations and analysis on NF probabilities and channel coefficients are conducted for $4\\times4$, $8\\times8$, $16\\times16$, and $9\\times21$ uniform planar arrays at different frequency bands.","sentences":["Technologies like ultra-massive multiple-input-multiple-output (UM-MIMO) and reconfigurable intelligent surfaces (RISs) are of special interest to meet the key performance indicators of future wireless systems including ubiquitous connectivity and lightning-fast data rates.","One of their common features, the extremely large-scale antenna array (ELAA) systems with hundreds or thousands of antennas, give rise to near-field (NF) propagation and bring new challenges to channel modeling and characterization.","In this paper, a cross-field channel model for ELAA systems is proposed, which improves the statistical model in 3GPP TR 38.901 by refining the propagation path with its first and last bounces and differentiating the characterization of parameters like path loss, delay, and angles in near- and far-fields.","A comprehensive analysis of cross-field boundaries and closed-form expressions of corresponding NF or FF parameters are provided.","Furthermore, cross-field experiments carried out in a typical indoor scenario at 300 GHz verify the variation of MPC parameters across the antenna array, and demonstrate the distinction of channels between different antenna elements.","Finally, detailed generation procedures of the cross-field channel model are provided, based on which simulations and analysis on NF probabilities and channel coefficients are conducted for $4\\times4$, $8\\times8$, $16\\times16$, and $9\\times21$ uniform planar arrays at different frequency bands."],"url":"http://arxiv.org/abs/2405.16893v1","category":"cs.IT"}
{"created":"2024-05-27 07:10:04","title":"A Large Language Model-based multi-agent manufacturing system for intelligent shopfloor","abstract":"As productivity advances, the demand of customers for multi-variety and small-batch production is increasing, thereby putting forward higher requirements for manufacturing systems. When production tasks frequent changes due to this demand, traditional manufacturing systems often cannot response promptly. The multi-agent manufacturing system is proposed to address this problem. However, because of technical limitations, the negotiation among agents in this kind of system is realized through predefined heuristic rules, which is not intelligent enough to deal with the multi-variety and small batch production. To this end, a Large Language Model-based (LLM-based) multi-agent manufacturing system for intelligent shopfloor is proposed in the present study. This system delineates the diverse agents and defines their collaborative methods. The roles of the agents encompass Machine Server Agent (MSA), Bid Inviter Agent (BIA), Bidder Agent (BA), Thinking Agent (TA), and Decision Agent (DA). Due to the support of LLMs, TA and DA acquire the ability of analyzing the shopfloor condition and choosing the most suitable machine, as opposed to executing a predefined program artificially. The negotiation between BAs and BIA is the most crucial step in connecting manufacturing resources. With the support of TA and DA, BIA will finalize the distribution of orders, relying on the information of each machine returned by BA. MSAs bears the responsibility for connecting the agents with the physical shopfloor. This system aims to distribute and transmit workpieces through the collaboration of the agents with these distinct roles, distinguishing it from other scheduling approaches. Comparative experiments were also conducted to validate the performance of this system.","sentences":["As productivity advances, the demand of customers for multi-variety and small-batch production is increasing, thereby putting forward higher requirements for manufacturing systems.","When production tasks frequent changes due to this demand, traditional manufacturing systems often cannot response promptly.","The multi-agent manufacturing system is proposed to address this problem.","However, because of technical limitations, the negotiation among agents in this kind of system is realized through predefined heuristic rules, which is not intelligent enough to deal with the multi-variety and small batch production.","To this end, a Large Language Model-based (LLM-based) multi-agent manufacturing system for intelligent shopfloor is proposed in the present study.","This system delineates the diverse agents and defines their collaborative methods.","The roles of the agents encompass Machine Server Agent (MSA), Bid Inviter Agent (BIA), Bidder Agent (BA), Thinking Agent (TA), and Decision Agent (DA).","Due to the support of LLMs, TA and DA acquire the ability of analyzing the shopfloor condition and choosing the most suitable machine, as opposed to executing a predefined program artificially.","The negotiation between BAs and BIA is the most crucial step in connecting manufacturing resources.","With the support of TA and DA, BIA will finalize the distribution of orders, relying on the information of each machine returned by BA.","MSAs bears the responsibility for connecting the agents with the physical shopfloor.","This system aims to distribute and transmit workpieces through the collaboration of the agents with these distinct roles, distinguishing it from other scheduling approaches.","Comparative experiments were also conducted to validate the performance of this system."],"url":"http://arxiv.org/abs/2405.16887v1","category":"cs.AI"}
{"created":"2024-05-27 06:59:20","title":"Scorch: A Library for Sparse Deep Learning","abstract":"The rapid growth in the size of deep learning models strains the capabilities of traditional dense computation paradigms. Leveraging sparse computation has become increasingly popular for training and deploying large-scale models, but existing deep learning frameworks lack extensive support for sparse operations. To bridge this gap, we introduce Scorch, a library that seamlessly integrates efficient sparse tensor computation into the PyTorch ecosystem, with an initial focus on inference workloads on CPUs. Scorch provides a flexible and intuitive interface for sparse tensors, supporting diverse sparse data structures. Scorch introduces a compiler stack that automates key optimizations, including automatic loop ordering, tiling, and format inference. Combined with a runtime that adapts its execution to both dense and sparse data, Scorch delivers substantial speedups over hand-written PyTorch Sparse (torch.sparse) operations without sacrificing usability. More importantly, Scorch enables efficient computation of complex sparse operations that lack hand-optimized PyTorch implementations. This flexibility is crucial for exploring novel sparse architectures. We demonstrate Scorch's ease of use and performance gains on diverse deep learning models across multiple domains. With only minimal code changes, Scorch achieves 1.05-5.78x speedups over PyTorch Sparse on end-to-end tasks. Scorch's seamless integration and performance gains make it a valuable addition to the PyTorch ecosystem. We believe Scorch will enable wider exploration of sparsity as a tool for scaling deep learning and inform the development of other sparse libraries.","sentences":["The rapid growth in the size of deep learning models strains the capabilities of traditional dense computation paradigms.","Leveraging sparse computation has become increasingly popular for training and deploying large-scale models, but existing deep learning frameworks lack extensive support for sparse operations.","To bridge this gap, we introduce Scorch, a library that seamlessly integrates efficient sparse tensor computation into the PyTorch ecosystem, with an initial focus on inference workloads on CPUs.","Scorch provides a flexible and intuitive interface for sparse tensors, supporting diverse sparse data structures.","Scorch introduces a compiler stack that automates key optimizations, including automatic loop ordering, tiling, and format inference.","Combined with a runtime that adapts its execution to both dense and sparse data, Scorch delivers substantial speedups over hand-written PyTorch Sparse (torch.sparse) operations without sacrificing usability.","More importantly, Scorch enables efficient computation of complex sparse operations that lack hand-optimized PyTorch implementations.","This flexibility is crucial for exploring novel sparse architectures.","We demonstrate Scorch's ease of use and performance gains on diverse deep learning models across multiple domains.","With only minimal code changes, Scorch achieves 1.05-5.78x speedups over PyTorch Sparse on end-to-end tasks.","Scorch's seamless integration and performance gains make it a valuable addition to the PyTorch ecosystem.","We believe Scorch will enable wider exploration of sparsity as a tool for scaling deep learning and inform the development of other sparse libraries."],"url":"http://arxiv.org/abs/2405.16883v1","category":"cs.LG"}
{"created":"2024-05-27 06:50:01","title":"Systematic Literature Review of Commercial Participation in Open Source Software","abstract":"Open source software (OSS) has been playing a fundamental role in not only information technology but also our social lives. Attracted by various advantages of OSS, increasing commercial companies take extensive participation in open source development and have had a broad impact. This paper provides a comprehensive systematic literature review (SLR) of existing research on company participation in OSS. We collected 92 papers and organized them based on their research topics, which cover three main directions, i.e., participation motivation, contribution model, and impact on OSS development. We found the explored motivations of companies are mainly from economic, technological, and social aspects. Existing studies categorize companies' contribution models in OSS projects mainly through their objectives and how they shape OSS communities. Researchers also explored how commercial participation affects OSS development. We conclude with research challenges and promising research directions on commercial participation in OSS. This study contributes to a comprehensive understanding of commercial participation in OSS development.","sentences":["Open source software (OSS) has been playing a fundamental role in not only information technology but also our social lives.","Attracted by various advantages of OSS, increasing commercial companies take extensive participation in open source development and have had a broad impact.","This paper provides a comprehensive systematic literature review (SLR) of existing research on company participation in OSS.","We collected 92 papers and organized them based on their research topics, which cover three main directions, i.e., participation motivation, contribution model, and impact on OSS development.","We found the explored motivations of companies are mainly from economic, technological, and social aspects.","Existing studies categorize companies' contribution models in OSS projects mainly through their objectives and how they shape OSS communities.","Researchers also explored how commercial participation affects OSS development.","We conclude with research challenges and promising research directions on commercial participation in OSS.","This study contributes to a comprehensive understanding of commercial participation in OSS development."],"url":"http://arxiv.org/abs/2405.16880v1","category":"cs.SE"}
{"created":"2024-05-27 06:50:00","title":"Unsupervised Generative Feature Transformation via Graph Contrastive Pre-training and Multi-objective Fine-tuning","abstract":"Feature transformation is to derive a new feature set from original features to augment the AI power of data. In many science domains such as material performance screening, while feature transformation can model material formula interactions and compositions and discover performance drivers, supervised labels are collected from expensive and lengthy experiments. This issue motivates an Unsupervised Feature Transformation Learning (UFTL) problem. Prior literature, such as manual transformation, supervised feedback guided search, and PCA, either relies on domain knowledge or expensive supervised feedback, or suffers from large search space, or overlooks non-linear feature-feature interactions. UFTL imposes a major challenge on existing methods: how to design a new unsupervised paradigm that captures complex feature interactions and avoids large search space? To fill this gap, we connect graph, contrastive, and generative learning to develop a measurement-pretrain-finetune paradigm for UFTL. For unsupervised feature set utility measurement, we propose a feature value consistency preservation perspective and develop a mean discounted cumulative gain like unsupervised metric to evaluate feature set utility. For unsupervised feature set representation pretraining, we regard a feature set as a feature-feature interaction graph, and develop an unsupervised graph contrastive learning encoder to embed feature sets into vectors. For generative transformation finetuning, we regard a feature set as a feature cross sequence and feature transformation as sequential generation. We develop a deep generative feature transformation model that coordinates the pretrained feature set encoder and the gradient information extracted from a feature set utility evaluator to optimize a transformed feature generator.","sentences":["Feature transformation is to derive a new feature set from original features to augment the AI power of data.","In many science domains such as material performance screening, while feature transformation can model material formula interactions and compositions and discover performance drivers, supervised labels are collected from expensive and lengthy experiments.","This issue motivates an Unsupervised Feature Transformation Learning (UFTL) problem.","Prior literature, such as manual transformation, supervised feedback guided search, and PCA, either relies on domain knowledge or expensive supervised feedback, or suffers from large search space, or overlooks non-linear feature-feature interactions.","UFTL imposes a major challenge on existing methods: how to design a new unsupervised paradigm that captures complex feature interactions and avoids large search space?","To fill this gap, we connect graph, contrastive, and generative learning to develop a measurement-pretrain-finetune paradigm for UFTL.","For unsupervised feature set utility measurement, we propose a feature value consistency preservation perspective and develop a mean discounted cumulative gain like unsupervised metric to evaluate feature set utility.","For unsupervised feature set representation pretraining, we regard a feature set as a feature-feature interaction graph, and develop an unsupervised graph contrastive learning encoder to embed feature sets into vectors.","For generative transformation finetuning, we regard a feature set as a feature cross sequence and feature transformation as sequential generation.","We develop a deep generative feature transformation model that coordinates the pretrained feature set encoder and the gradient information extracted from a feature set utility evaluator to optimize a transformed feature generator."],"url":"http://arxiv.org/abs/2405.16879v1","category":"cs.LG"}
{"created":"2024-05-27 06:49:39","title":"Are Self-Attentions Effective for Time Series Forecasting?","abstract":"Time series forecasting is crucial for applications across multiple domains and various scenarios. Although Transformer models have dramatically shifted the landscape of forecasting, their effectiveness remains debated. Recent findings have indicated that simpler linear models might outperform complex Transformer-based approaches, highlighting the potential for more streamlined architectures. In this paper, we shift focus from the overall architecture of the Transformer to the effectiveness of self-attentions for time series forecasting. To this end, we introduce a new architecture, Cross-Attention-only Time Series transformer (CATS), that rethinks the traditional Transformer framework by eliminating self-attention and leveraging cross-attention mechanisms instead. By establishing future horizon-dependent parameters as queries and enhanced parameter sharing, our model not only improves long-term forecasting accuracy but also reduces the number of parameters and memory usage. Extensive experiment across various datasets demonstrates that our model achieves superior performance with the lowest mean squared error and uses fewer parameters compared to existing models.","sentences":["Time series forecasting is crucial for applications across multiple domains and various scenarios.","Although Transformer models have dramatically shifted the landscape of forecasting, their effectiveness remains debated.","Recent findings have indicated that simpler linear models might outperform complex Transformer-based approaches, highlighting the potential for more streamlined architectures.","In this paper, we shift focus from the overall architecture of the Transformer to the effectiveness of self-attentions for time series forecasting.","To this end, we introduce a new architecture, Cross-Attention-only Time Series transformer (CATS), that rethinks the traditional Transformer framework by eliminating self-attention and leveraging cross-attention mechanisms instead.","By establishing future horizon-dependent parameters as queries and enhanced parameter sharing, our model not only improves long-term forecasting accuracy but also reduces the number of parameters and memory usage.","Extensive experiment across various datasets demonstrates that our model achieves superior performance with the lowest mean squared error and uses fewer parameters compared to existing models."],"url":"http://arxiv.org/abs/2405.16877v1","category":"cs.LG"}
{"created":"2024-05-27 06:48:58","title":"Transfer Learning for Diffusion Models","abstract":"Diffusion models, a specific type of generative model, have achieved unprecedented performance in recent years and consistently produce high-quality synthetic samples. A critical prerequisite for their notable success lies in the presence of a substantial number of training samples, which can be impractical in real-world applications due to high collection costs or associated risks. Consequently, various finetuning and regularization approaches have been proposed to transfer knowledge from existing pre-trained models to specific target domains with limited data. This paper introduces the Transfer Guided Diffusion Process (TGDP), a novel approach distinct from conventional finetuning and regularization methods. We prove that the optimal diffusion model for the target domain integrates pre-trained diffusion models on the source domain with additional guidance from a domain classifier. We further extend TGDP to a conditional version for modeling the joint distribution of data and its corresponding labels, together with two additional regularization terms to enhance the model performance. We validate the effectiveness of TGDP on Gaussian mixture simulations and on real electrocardiogram (ECG) datasets.","sentences":["Diffusion models, a specific type of generative model, have achieved unprecedented performance in recent years and consistently produce high-quality synthetic samples.","A critical prerequisite for their notable success lies in the presence of a substantial number of training samples, which can be impractical in real-world applications due to high collection costs or associated risks.","Consequently, various finetuning and regularization approaches have been proposed to transfer knowledge from existing pre-trained models to specific target domains with limited data.","This paper introduces the Transfer Guided Diffusion Process (TGDP), a novel approach distinct from conventional finetuning and regularization methods.","We prove that the optimal diffusion model for the target domain integrates pre-trained diffusion models on the source domain with additional guidance from a domain classifier.","We further extend TGDP to a conditional version for modeling the joint distribution of data and its corresponding labels, together with two additional regularization terms to enhance the model performance.","We validate the effectiveness of TGDP on Gaussian mixture simulations and on real electrocardiogram (ECG) datasets."],"url":"http://arxiv.org/abs/2405.16876v1","category":"cs.LG"}
{"created":"2024-05-27 06:36:17","title":"Mixture of Modality Knowledge Experts for Robust Multi-modal Knowledge Graph Completion","abstract":"Multi-modal knowledge graph completion (MMKGC) aims to automatically discover new knowledge triples in the given multi-modal knowledge graphs (MMKGs), which is achieved by collaborative modeling the structural information concealed in massive triples and the multi-modal features of the entities. Existing methods tend to focus on crafting elegant entity-wise multi-modal fusion strategies, yet they overlook the utilization of multi-perspective features concealed within the modalities under diverse relational contexts. To address this issue, we introduce a novel MMKGC framework with Mixture of Modality Knowledge experts (MoMoK for short) to learn adaptive multi-modal embedding under intricate relational contexts. We design relation-guided modality knowledge experts to acquire relation-aware modality embeddings and integrate the predictions from multi-modalities to achieve comprehensive decisions. Additionally, we disentangle the experts by minimizing their mutual information. Experiments on four public MMKG benchmarks demonstrate the outstanding performance of MoMoK under complex scenarios.","sentences":["Multi-modal knowledge graph completion (MMKGC) aims to automatically discover new knowledge triples in the given multi-modal knowledge graphs (MMKGs), which is achieved by collaborative modeling the structural information concealed in massive triples and the multi-modal features of the entities.","Existing methods tend to focus on crafting elegant entity-wise multi-modal fusion strategies, yet they overlook the utilization of multi-perspective features concealed within the modalities under diverse relational contexts.","To address this issue, we introduce a novel MMKGC framework with Mixture of Modality Knowledge experts (MoMoK for short) to learn adaptive multi-modal embedding under intricate relational contexts.","We design relation-guided modality knowledge experts to acquire relation-aware modality embeddings and integrate the predictions from multi-modalities to achieve comprehensive decisions.","Additionally, we disentangle the experts by minimizing their mutual information.","Experiments on four public MMKG benchmarks demonstrate the outstanding performance of MoMoK under complex scenarios."],"url":"http://arxiv.org/abs/2405.16869v1","category":"cs.AI"}
{"created":"2024-05-27 06:33:25","title":"Clustering-based Learning for UAV Tracking and Pose Estimation","abstract":"UAV tracking and pose estimation plays an imperative role in various UAV-related missions, such as formation control and anti-UAV measures. Accurately detecting and tracking UAVs in a 3D space remains a particularly challenging problem, as it requires extracting sparse features of micro UAVs from different flight environments and continuously matching correspondences, especially during agile flight. Generally, cameras and LiDARs are the two main types of sensors used to capture UAV trajectories in flight. However, both sensors have limitations in UAV classification and pose estimation. This technical report briefly introduces the method proposed by our team \"NTU-ICG\" for the CVPR 2024 UG2+ Challenge Track 5. This work develops a clustering-based learning detection approach, CL-Det, for UAV tracking and pose estimation using two types of LiDARs, namely Livox Avia and LiDAR 360. We combine the information from the two data sources to locate drones in 3D. We first align the timestamps of Livox Avia data and LiDAR 360 data and then separate the point cloud of objects of interest (OOIs) from the environment. The point cloud of OOIs is clustered using the DBSCAN method, with the midpoint of the largest cluster assumed to be the UAV position. Furthermore, we utilize historical estimations to fill in missing data. The proposed method shows competitive pose estimation performance and ranks 5th on the final leaderboard of the CVPR 2024 UG2+ Challenge.","sentences":["UAV tracking and pose estimation plays an imperative role in various UAV-related missions, such as formation control and anti-UAV measures.","Accurately detecting and tracking UAVs in a 3D space remains a particularly challenging problem, as it requires extracting sparse features of micro UAVs from different flight environments and continuously matching correspondences, especially during agile flight.","Generally, cameras and LiDARs are the two main types of sensors used to capture UAV trajectories in flight.","However, both sensors have limitations in UAV classification and pose estimation.","This technical report briefly introduces the method proposed by our team \"NTU-ICG\" for the CVPR 2024 UG2+ Challenge Track 5.","This work develops a clustering-based learning detection approach, CL-Det, for UAV tracking and pose estimation using two types of LiDARs, namely Livox Avia and LiDAR 360.","We combine the information from the two data sources to locate drones in 3D.","We first align the timestamps of Livox Avia data and LiDAR 360 data and then separate the point cloud of objects of interest (OOIs) from the environment.","The point cloud of OOIs is clustered using the DBSCAN method, with the midpoint of the largest cluster assumed to be the UAV position.","Furthermore, we utilize historical estimations to fill in missing data.","The proposed method shows competitive pose estimation performance and ranks 5th on the final leaderboard of the CVPR 2024 UG2+ Challenge."],"url":"http://arxiv.org/abs/2405.16867v1","category":"cs.RO"}
{"created":"2024-05-27 06:20:58","title":"Think Before You Act: A Two-Stage Framework for Mitigating Gender Bias Towards Vision-Language Tasks","abstract":"Gender bias in vision-language models (VLMs) can reinforce harmful stereotypes and discrimination. In this paper, we focus on mitigating gender bias towards vision-language tasks. We identify object hallucination as the essence of gender bias in VLMs. Existing VLMs tend to focus on salient or familiar attributes in images but ignore contextualized nuances. Moreover, most VLMs rely on the co-occurrence between specific objects and gender attributes to infer the ignored features, ultimately resulting in gender bias. We propose GAMA, a task-agnostic generation framework to mitigate gender bias. GAMA consists of two stages: narrative generation and answer inference. During narrative generation, GAMA yields all-sided but gender-obfuscated narratives, which prevents premature concentration on localized image features, especially gender attributes. During answer inference, GAMA integrates the image, generated narrative, and a task-specific question prompt to infer answers for different vision-language tasks. This approach allows the model to rethink gender attributes and answers. We conduct extensive experiments on GAMA, demonstrating its debiasing and generalization ability.","sentences":["Gender bias in vision-language models (VLMs) can reinforce harmful stereotypes and discrimination.","In this paper, we focus on mitigating gender bias towards vision-language tasks.","We identify object hallucination as the essence of gender bias in VLMs.","Existing VLMs tend to focus on salient or familiar attributes in images but ignore contextualized nuances.","Moreover, most VLMs rely on the co-occurrence between specific objects and gender attributes to infer the ignored features, ultimately resulting in gender bias.","We propose GAMA, a task-agnostic generation framework to mitigate gender bias.","GAMA consists of two stages: narrative generation and answer inference.","During narrative generation, GAMA yields all-sided but gender-obfuscated narratives, which prevents premature concentration on localized image features, especially gender attributes.","During answer inference, GAMA integrates the image, generated narrative, and a task-specific question prompt to infer answers for different vision-language tasks.","This approach allows the model to rethink gender attributes and answers.","We conduct extensive experiments on GAMA, demonstrating its debiasing and generalization ability."],"url":"http://arxiv.org/abs/2405.16860v1","category":"cs.CV"}
{"created":"2024-05-27 05:55:22","title":"EM Distillation for One-step Diffusion Models","abstract":"While diffusion models can learn complex distributions, sampling requires a computationally expensive iterative process. Existing distillation methods enable efficient sampling, but have notable limitations, such as performance degradation with very few sampling steps, reliance on training data access, or mode-seeking optimization that may fail to capture the full distribution. We propose EM Distillation (EMD), a maximum likelihood-based approach that distills a diffusion model to a one-step generator model with minimal loss of perceptual quality. Our approach is derived through the lens of Expectation-Maximization (EM), where the generator parameters are updated using samples from the joint distribution of the diffusion teacher prior and inferred generator latents. We develop a reparametrized sampling scheme and a noise cancellation technique that together stabilizes the distillation process. We further reveal an interesting connection of our method with existing methods that minimize mode-seeking KL. EMD outperforms existing one-step generative methods in terms of FID scores on ImageNet-64 and ImageNet-128, and compares favorably with prior work on distilling text-to-image diffusion models.","sentences":["While diffusion models can learn complex distributions, sampling requires a computationally expensive iterative process.","Existing distillation methods enable efficient sampling, but have notable limitations, such as performance degradation with very few sampling steps, reliance on training data access, or mode-seeking optimization that may fail to capture the full distribution.","We propose EM Distillation (EMD), a maximum likelihood-based approach that distills a diffusion model to a one-step generator model with minimal loss of perceptual quality.","Our approach is derived through the lens of Expectation-Maximization (EM), where the generator parameters are updated using samples from the joint distribution of the diffusion teacher prior and inferred generator latents.","We develop a reparametrized sampling scheme and a noise cancellation technique that together stabilizes the distillation process.","We further reveal an interesting connection of our method with existing methods that minimize mode-seeking KL.","EMD outperforms existing one-step generative methods in terms of FID scores on ImageNet-64 and ImageNet-128, and compares favorably with prior work on distilling text-to-image diffusion models."],"url":"http://arxiv.org/abs/2405.16852v1","category":"cs.LG"}
{"created":"2024-05-27 05:53:30","title":"Temporal Spiking Neural Networks with Synaptic Delay for Graph Reasoning","abstract":"Spiking neural networks (SNNs) are investigated as biologically inspired models of neural computation, distinguished by their computational capability and energy efficiency due to precise spiking times and sparse spikes with event-driven computation. A significant question is how SNNs can emulate human-like graph-based reasoning of concepts and relations, especially leveraging the temporal domain optimally. This paper reveals that SNNs, when amalgamated with synaptic delay and temporal coding, are proficient in executing (knowledge) graph reasoning. It is elucidated that spiking time can function as an additional dimension to encode relation properties via a neural-generalized path formulation. Empirical results highlight the efficacy of temporal delay in relation processing and showcase exemplary performance in diverse graph reasoning tasks. The spiking model is theoretically estimated to achieve $20\\times$ energy savings compared to non-spiking counterparts, deepening insights into the capabilities and potential of biologically inspired SNNs for efficient reasoning. The code is available at https://github.com/pkuxmq/GRSNN.","sentences":["Spiking neural networks (SNNs) are investigated as biologically inspired models of neural computation, distinguished by their computational capability and energy efficiency due to precise spiking times and sparse spikes with event-driven computation.","A significant question is how SNNs can emulate human-like graph-based reasoning of concepts and relations, especially leveraging the temporal domain optimally.","This paper reveals that SNNs, when amalgamated with synaptic delay and temporal coding, are proficient in executing (knowledge) graph reasoning.","It is elucidated that spiking time can function as an additional dimension to encode relation properties via a neural-generalized path formulation.","Empirical results highlight the efficacy of temporal delay in relation processing and showcase exemplary performance in diverse graph reasoning tasks.","The spiking model is theoretically estimated to achieve $20\\times$ energy savings compared to non-spiking counterparts, deepening insights into the capabilities and potential of biologically inspired SNNs for efficient reasoning.","The code is available at https://github.com/pkuxmq/GRSNN."],"url":"http://arxiv.org/abs/2405.16851v1","category":"cs.NE"}
{"created":"2024-05-27 05:45:51","title":"TokenUnify: Scalable Autoregressive Visual Pre-training with Mixture Token Prediction","abstract":"Autoregressive next-token prediction is a standard pretraining method for large-scale language models, but its application to vision tasks is hindered by the non-sequential nature of image data, leading to cumulative errors. Most vision models employ masked autoencoder (MAE) based pretraining, which faces scalability issues. To address these challenges, we introduce \\textbf{TokenUnify}, a novel pretraining method that integrates random token prediction, next-token prediction, and next-all token prediction. We provide theoretical evidence demonstrating that TokenUnify mitigates cumulative errors in visual autoregression. Cooperated with TokenUnify, we have assembled a large-scale electron microscopy (EM) image dataset with ultra-high resolution, ideal for creating spatially correlated long sequences. This dataset includes over 120 million annotated voxels, making it the largest neuron segmentation dataset to date and providing a unified benchmark for experimental validation. Leveraging the Mamba network inherently suited for long-sequence modeling on this dataset, TokenUnify not only reduces the computational complexity but also leads to a significant 45\\% improvement in segmentation performance on downstream EM neuron segmentation tasks compared to existing methods. Furthermore, TokenUnify demonstrates superior scalability over MAE and traditional autoregressive methods, effectively bridging the gap between pretraining strategies for language and vision models. Code is available at \\url{https://github.com/ydchen0806/TokenUnify}.","sentences":["Autoregressive next-token prediction is a standard pretraining method for large-scale language models, but its application to vision tasks is hindered by the non-sequential nature of image data, leading to cumulative errors.","Most vision models employ masked autoencoder (MAE) based pretraining, which faces scalability issues.","To address these challenges, we introduce \\textbf{TokenUnify}, a novel pretraining method that integrates random token prediction, next-token prediction, and next-all token prediction.","We provide theoretical evidence demonstrating that TokenUnify mitigates cumulative errors in visual autoregression.","Cooperated with TokenUnify, we have assembled a large-scale electron microscopy (EM) image dataset with ultra-high resolution, ideal for creating spatially correlated long sequences.","This dataset includes over 120 million annotated voxels, making it the largest neuron segmentation dataset to date and providing a unified benchmark for experimental validation.","Leveraging the Mamba network inherently suited for long-sequence modeling on this dataset, TokenUnify not only reduces the computational complexity but also leads to a significant 45\\% improvement in segmentation performance on downstream EM neuron segmentation tasks compared to existing methods.","Furthermore, TokenUnify demonstrates superior scalability over MAE and traditional autoregressive methods, effectively bridging the gap between pretraining strategies for language and vision models.","Code is available at \\url{https://github.com/ydchen0806/TokenUnify}."],"url":"http://arxiv.org/abs/2405.16847v1","category":"cs.CV"}
{"created":"2024-05-27 05:06:24","title":"Enhancing Fast Feed Forward Networks with Load Balancing and a Master Leaf Node","abstract":"Fast feedforward networks (FFFs) are a class of neural networks that exploit the observation that different regions of the input space activate distinct subsets of neurons in wide networks. FFFs partition the input space into separate sections using a differentiable binary tree of neurons and during inference descend the binary tree in order to improve computational efficiency. Inspired by Mixture of Experts (MoE) research, we propose the incorporation of load balancing and Master Leaf techniques into the FFF architecture to improve performance and simplify the training process. We reproduce experiments found in literature and present results on FFF models enhanced using these techniques. The proposed architecture and training recipe achieves up to 16.3% and 3% absolute classification accuracy increase in training and test accuracy, respectively, compared to the original FFF architecture. Additionally, we observe a smaller variance in the results compared to those reported in prior research. These findings demonstrate the potential of integrating MoE-inspired techniques into FFFs for developing more accurate and efficient models.","sentences":["Fast feedforward networks (FFFs) are a class of neural networks that exploit the observation that different regions of the input space activate distinct subsets of neurons in wide networks.","FFFs partition the input space into separate sections using a differentiable binary tree of neurons and during inference descend the binary tree in order to improve computational efficiency.","Inspired by Mixture of Experts (MoE) research, we propose the incorporation of load balancing and Master Leaf techniques into the FFF architecture to improve performance and simplify the training process.","We reproduce experiments found in literature and present results on FFF models enhanced using these techniques.","The proposed architecture and training recipe achieves up to 16.3% and 3% absolute classification accuracy increase in training and test accuracy, respectively, compared to the original FFF architecture.","Additionally, we observe a smaller variance in the results compared to those reported in prior research.","These findings demonstrate the potential of integrating MoE-inspired techniques into FFFs for developing more accurate and efficient models."],"url":"http://arxiv.org/abs/2405.16836v1","category":"cs.LG"}
{"created":"2024-05-27 04:53:09","title":"Structured Graph Network for Constrained Robot Crowd Navigation with Low Fidelity Simulation","abstract":"We investigate the feasibility of deploying reinforcement learning (RL) policies for constrained crowd navigation using a low-fidelity simulator. We introduce a representation of the dynamic environment, separating human and obstacle representations. Humans are represented through detected states, while obstacles are represented as computed point clouds based on maps and robot localization. This representation enables RL policies trained in a low-fidelity simulator to deploy in real world with a reduced sim2real gap. Additionally, we propose a spatio-temporal graph to model the interactions between agents and obstacles. Based on the graph, we use attention mechanisms to capture the robot-human, human-human, and human-obstacle interactions. Our method significantly improves navigation performance in both simulated and real-world environments. Video demonstrations can be found at https://sites.google.com/view/constrained-crowdnav/home.","sentences":["We investigate the feasibility of deploying reinforcement learning (RL) policies for constrained crowd navigation using a low-fidelity simulator.","We introduce a representation of the dynamic environment, separating human and obstacle representations.","Humans are represented through detected states, while obstacles are represented as computed point clouds based on maps and robot localization.","This representation enables RL policies trained in a low-fidelity simulator to deploy in real world with a reduced sim2real gap.","Additionally, we propose a spatio-temporal graph to model the interactions between agents and obstacles.","Based on the graph, we use attention mechanisms to capture the robot-human, human-human, and human-obstacle interactions.","Our method significantly improves navigation performance in both simulated and real-world environments.","Video demonstrations can be found at https://sites.google.com/view/constrained-crowdnav/home."],"url":"http://arxiv.org/abs/2405.16830v1","category":"cs.RO"}
{"created":"2024-05-27 04:52:21","title":"PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting","abstract":"Neural Radiance Fields (NeRFs) have demonstrated remarkable proficiency in synthesizing photorealistic images of large-scale scenes. However, they are often plagued by a loss of fine details and long rendering durations. 3D Gaussian Splatting has recently been introduced as a potent alternative, achieving both high-fidelity visual results and accelerated rendering performance. Nonetheless, scaling 3D Gaussian Splatting is fraught with challenges. Specifically, large-scale scenes grapples with the integration of objects across multiple scales and disparate viewpoints, which often leads to compromised efficacy as the Gaussians need to balance between detail levels. Furthermore, the generation of initialization points via COLMAP from large-scale dataset is both computationally demanding and prone to incomplete reconstructions. To address these challenges, we present Pyramidal 3D Gaussian Splatting (PyGS) with NeRF Initialization. Our approach represent the scene with a hierarchical assembly of Gaussians arranged in a pyramidal fashion. The top level of the pyramid is composed of a few large Gaussians, while each subsequent layer accommodates a denser collection of smaller Gaussians. We effectively initialize these pyramidal Gaussians through sampling a rapidly trained grid-based NeRF at various frequencies. We group these pyramidal Gaussians into clusters and use a compact weighting network to dynamically determine the influence of each pyramid level of each cluster considering camera viewpoint during rendering. Our method achieves a significant performance leap across multiple large-scale datasets and attains a rendering time that is over 400 times faster than current state-of-the-art approaches.","sentences":["Neural Radiance Fields (NeRFs) have demonstrated remarkable proficiency in synthesizing photorealistic images of large-scale scenes.","However, they are often plagued by a loss of fine details and long rendering durations.","3D Gaussian Splatting has recently been introduced as a potent alternative, achieving both high-fidelity visual results and accelerated rendering performance.","Nonetheless, scaling 3D Gaussian Splatting is fraught with challenges.","Specifically, large-scale scenes grapples with the integration of objects across multiple scales and disparate viewpoints, which often leads to compromised efficacy as the Gaussians need to balance between detail levels.","Furthermore, the generation of initialization points via COLMAP from large-scale dataset is both computationally demanding and prone to incomplete reconstructions.","To address these challenges, we present Pyramidal 3D Gaussian Splatting (PyGS) with NeRF Initialization.","Our approach represent the scene with a hierarchical assembly of Gaussians arranged in a pyramidal fashion.","The top level of the pyramid is composed of a few large Gaussians, while each subsequent layer accommodates a denser collection of smaller Gaussians.","We effectively initialize these pyramidal Gaussians through sampling a rapidly trained grid-based NeRF at various frequencies.","We group these pyramidal Gaussians into clusters and use a compact weighting network to dynamically determine the influence of each pyramid level of each cluster considering camera viewpoint during rendering.","Our method achieves a significant performance leap across multiple large-scale datasets and attains a rendering time that is over 400 times faster than current state-of-the-art approaches."],"url":"http://arxiv.org/abs/2405.16829v1","category":"cs.CV"}
{"created":"2024-05-27 04:44:36","title":"Unified Editing of Panorama, 3D Scenes, and Videos Through Disentangled Self-Attention Injection","abstract":"While text-to-image models have achieved impressive capabilities in image generation and editing, their application across various modalities often necessitates training separate models. Inspired by existing method of single image editing with self attention injection and video editing with shared attention, we propose a novel unified editing framework that combines the strengths of both approaches by utilizing only a basic 2D image text-to-image (T2I) diffusion model. Specifically, we design a sampling method that facilitates editing consecutive images while maintaining semantic consistency utilizing shared self-attention features during both reference and consecutive image sampling processes. Experimental results confirm that our method enables editing across diverse modalities including 3D scenes, videos, and panorama images.","sentences":["While text-to-image models have achieved impressive capabilities in image generation and editing, their application across various modalities often necessitates training separate models.","Inspired by existing method of single image editing with self attention injection and video editing with shared attention, we propose a novel unified editing framework that combines the strengths of both approaches by utilizing only a basic 2D image text-to-image (T2I) diffusion model.","Specifically, we design a sampling method that facilitates editing consecutive images while maintaining semantic consistency utilizing shared self-attention features during both reference and consecutive image sampling processes.","Experimental results confirm that our method enables editing across diverse modalities including 3D scenes, videos, and panorama images."],"url":"http://arxiv.org/abs/2405.16823v1","category":"cs.CV"}
{"created":"2024-05-27 04:38:10","title":"Laboratory-Scale AI: Open-Weight Models are Competitive with ChatGPT Even in Low-Resource Settings","abstract":"The rapid proliferation of generative AI has raised questions about the competitiveness of lower-parameter, locally tunable, open-weight models relative to high-parameter, API-guarded, closed-weight models in terms of performance, domain adaptation, cost, and generalization. Centering under-resourced yet risk-intolerant settings in government, research, and healthcare, we see for-profit closed-weight models as incompatible with requirements for transparency, privacy, adaptability, and standards of evidence. Yet the performance penalty in using open-weight models, especially in low-data and low-resource settings, is unclear.   We assess the feasibility of using smaller, open-weight models to replace GPT-4-Turbo in zero-shot, few-shot, and fine-tuned regimes, assuming access to only a single, low-cost GPU. We assess value-sensitive issues around bias, privacy, and abstention on three additional tasks relevant to those topics. We find that with relatively low effort, very low absolute monetary cost, and relatively little data for fine-tuning, small open-weight models can achieve competitive performance in domain-adapted tasks without sacrificing generality. We then run experiments considering practical issues in bias, privacy, and hallucination risk, finding that open models offer several benefits over closed models. We intend this work as a case study in understanding the opportunity cost of reproducibility and transparency over for-profit state-of-the-art zero shot performance, finding this cost to be marginal under realistic settings.","sentences":["The rapid proliferation of generative AI has raised questions about the competitiveness of lower-parameter, locally tunable, open-weight models relative to high-parameter, API-guarded, closed-weight models in terms of performance, domain adaptation, cost, and generalization.","Centering under-resourced yet risk-intolerant settings in government, research, and healthcare, we see for-profit closed-weight models as incompatible with requirements for transparency, privacy, adaptability, and standards of evidence.","Yet the performance penalty in using open-weight models, especially in low-data and low-resource settings, is unclear.   ","We assess the feasibility of using smaller, open-weight models to replace GPT-4-Turbo in zero-shot, few-shot, and fine-tuned regimes, assuming access to only a single, low-cost GPU.","We assess value-sensitive issues around bias, privacy, and abstention on three additional tasks relevant to those topics.","We find that with relatively low effort, very low absolute monetary cost, and relatively little data for fine-tuning, small open-weight models can achieve competitive performance in domain-adapted tasks without sacrificing generality.","We then run experiments considering practical issues in bias, privacy, and hallucination risk, finding that open models offer several benefits over closed models.","We intend this work as a case study in understanding the opportunity cost of reproducibility and transparency over for-profit state-of-the-art zero shot performance, finding this cost to be marginal under realistic settings."],"url":"http://arxiv.org/abs/2405.16820v1","category":"cs.LG"}
{"created":"2024-05-27 03:54:09","title":"Extreme Compression of Adaptive Neural Images","abstract":"Implicit Neural Representations (INRs) and Neural Fields are a novel paradigm for signal representation, from images and audio to 3D scenes and videos. The fundamental idea is to represent a signal as a continuous and differentiable neural network. This idea offers unprecedented benefits such as continuous resolution and memory efficiency, enabling new compression techniques. However, representing data as neural networks poses new challenges. For instance, given a 2D image as a neural network, how can we further compress such a neural image?. In this work, we present a novel analysis on compressing neural fields, with the focus on images. We also introduce Adaptive Neural Images (ANI), an efficient neural representation that enables adaptation to different inference or transmission requirements. Our proposed method allows to reduce the bits-per-pixel (bpp) of the neural image by 4x, without losing sensitive details or harming fidelity. We achieve this thanks to our successful implementation of 4-bit neural representations. Our work offers a new framework for developing compressed neural fields.","sentences":["Implicit Neural Representations (INRs) and Neural Fields are a novel paradigm for signal representation, from images and audio to 3D scenes and videos.","The fundamental idea is to represent a signal as a continuous and differentiable neural network.","This idea offers unprecedented benefits such as continuous resolution and memory efficiency, enabling new compression techniques.","However, representing data as neural networks poses new challenges.","For instance, given a 2D image as a neural network, how can we further compress such a neural image?.","In this work, we present a novel analysis on compressing neural fields, with the focus on images.","We also introduce Adaptive Neural Images (ANI), an efficient neural representation that enables adaptation to different inference or transmission requirements.","Our proposed method allows to reduce the bits-per-pixel (bpp) of the neural image by 4x, without losing sensitive details or harming fidelity.","We achieve this thanks to our successful implementation of 4-bit neural representations.","Our work offers a new framework for developing compressed neural fields."],"url":"http://arxiv.org/abs/2405.16807v1","category":"cs.CV"}
{"created":"2024-05-27 03:52:55","title":"Entity Alignment with Noisy Annotations from Large Language Models","abstract":"Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying equivalent entity pairs. While existing methods heavily rely on human-generated labels, it is prohibitively expensive to incorporate cross-domain experts for annotation in real-world scenarios. The advent of Large Language Models (LLMs) presents new avenues for automating EA with annotations, inspired by their comprehensive capability to process semantic information. However, it is nontrivial to directly apply LLMs for EA since the annotation space in real-world KGs is large. LLMs could also generate noisy labels that may mislead the alignment. To this end, we propose a unified framework, LLM4EA, to effectively leverage LLMs for EA. Specifically, we design a novel active learning policy to significantly reduce the annotation space by prioritizing the most valuable entities based on the entire inter-KG and intra-KG structure. Moreover, we introduce an unsupervised label refiner to continuously enhance label accuracy through in-depth probabilistic reasoning. We iteratively optimize the policy based on the feedback from a base EA model. Extensive experiments demonstrate the advantages of LLM4EA on four benchmark datasets in terms of effectiveness, robustness, and efficiency.","sentences":["Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying equivalent entity pairs.","While existing methods heavily rely on human-generated labels, it is prohibitively expensive to incorporate cross-domain experts for annotation in real-world scenarios.","The advent of Large Language Models (LLMs) presents new avenues for automating EA with annotations, inspired by their comprehensive capability to process semantic information.","However, it is nontrivial to directly apply LLMs for EA since the annotation space in real-world KGs is large.","LLMs could also generate noisy labels that may mislead the alignment.","To this end, we propose a unified framework, LLM4EA, to effectively leverage LLMs for EA.","Specifically, we design a novel active learning policy to significantly reduce the annotation space by prioritizing the most valuable entities based on the entire inter-KG and intra-KG structure.","Moreover, we introduce an unsupervised label refiner to continuously enhance label accuracy through in-depth probabilistic reasoning.","We iteratively optimize the policy based on the feedback from a base EA model.","Extensive experiments demonstrate the advantages of LLM4EA on four benchmark datasets in terms of effectiveness, robustness, and efficiency."],"url":"http://arxiv.org/abs/2405.16806v1","category":"cs.CL"}
{"created":"2024-05-27 03:40:16","title":"TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing Graph and Text Mutual Transformations","abstract":"Text-Attributed Graphs (TAGs) enhance graph structures with natural language descriptions, enabling detailed representation of data and their relationships across a broad spectrum of real-world scenarios. Despite the potential for deeper insights, existing TAG representation learning primarily relies on supervised methods, necessitating extensive labeled data and limiting applicability across diverse contexts. This paper introduces a new self-supervised learning framework, Text-And-Graph Multi-View Alignment (TAGA), which overcomes these constraints by integrating TAGs' structural and semantic dimensions. TAGA constructs two complementary views: Text-of-Graph view, which organizes node texts into structured documents based on graph topology, and the Graph-of-Text view, which converts textual nodes and connections into graph data. By aligning representations from both views, TAGA captures joint textual and structural information. In addition, a novel structure-preserving random walk algorithm is proposed for efficient training on large-sized TAGs. Our framework demonstrates strong performance in zero-shot and few-shot scenarios across eight real-world datasets.","sentences":["Text-Attributed Graphs (TAGs) enhance graph structures with natural language descriptions, enabling detailed representation of data and their relationships across a broad spectrum of real-world scenarios.","Despite the potential for deeper insights, existing TAG representation learning primarily relies on supervised methods, necessitating extensive labeled data and limiting applicability across diverse contexts.","This paper introduces a new self-supervised learning framework, Text-And-Graph Multi-View Alignment (TAGA), which overcomes these constraints by integrating TAGs' structural and semantic dimensions.","TAGA constructs two complementary views: Text-of-Graph view, which organizes node texts into structured documents based on graph topology, and the Graph-of-Text view, which converts textual nodes and connections into graph data.","By aligning representations from both views, TAGA captures joint textual and structural information.","In addition, a novel structure-preserving random walk algorithm is proposed for efficient training on large-sized TAGs.","Our framework demonstrates strong performance in zero-shot and few-shot scenarios across eight real-world datasets."],"url":"http://arxiv.org/abs/2405.16800v1","category":"cs.LG"}
{"created":"2024-05-27 03:31:16","title":"A Real-Time Voice Activity Detection Based On Lightweight Neural","abstract":"Voice activity detection (VAD) is the task of detecting speech in an audio stream, which is challenging due to numerous unseen noises and low signal-to-noise ratios in real environments. Recently, neural network-based VADs have alleviated the degradation of performance to some extent. However, the majority of existing studies have employed excessively large models and incorporated future context, while neglecting to evaluate the operational efficiency and latency of the models. In this paper, we propose a lightweight and real-time neural network called MagicNet, which utilizes casual and depth separable 1-D convolutions and GRU. Without relying on future features as input, our proposed model is compared with two state-of-the-art algorithms on synthesized in-domain and out-domain test datasets. The evaluation results demonstrate that MagicNet can achieve improved performance and robustness with fewer parameter costs.","sentences":["Voice activity detection (VAD) is the task of detecting speech in an audio stream, which is challenging due to numerous unseen noises and low signal-to-noise ratios in real environments.","Recently, neural network-based VADs have alleviated the degradation of performance to some extent.","However, the majority of existing studies have employed excessively large models and incorporated future context, while neglecting to evaluate the operational efficiency and latency of the models.","In this paper, we propose a lightweight and real-time neural network called MagicNet, which utilizes casual and depth separable 1-D convolutions and GRU.","Without relying on future features as input, our proposed model is compared with two state-of-the-art algorithms on synthesized in-domain and out-domain test datasets.","The evaluation results demonstrate that MagicNet can achieve improved performance and robustness with fewer parameter costs."],"url":"http://arxiv.org/abs/2405.16797v1","category":"cs.SD"}
{"created":"2024-05-27 03:26:01","title":"Laurel: Generating Dafny Assertions Using Large Language Models","abstract":"Dafny is a popular verification language, which automates proofs by outsourcing them to an SMT solver. This automation is not perfect, however, and the solver often requires guidance in the form of helper assertions creating a burden for the proof engineer. In this paper, we propose Laurel, a tool that uses large language models (LLMs) to automatically generate helper assertions for Dafny programs. To improve the success rate of LLMs in this task, we design two domain-specific prompting techniques. First, we help the LLM determine the location of the missing assertion by analyzing the verifier's error message and inserting an assertion placeholder at that location. Second, we provide the LLM with example assertions from the same codebase, which we select based on a new lemma similarity metric. We evaluate our techniques on a dataset of helper assertions we extracted from three real-world Dafny codebases. Our evaluation shows that Laurel is able to generate over 50% of the required helper assertions given only a few attempts, making LLMs a usable and affordable tool to further automate practical program verification.","sentences":["Dafny is a popular verification language, which automates proofs by outsourcing them to an SMT solver.","This automation is not perfect, however, and the solver often requires guidance in the form of helper assertions creating a burden for the proof engineer.","In this paper, we propose Laurel, a tool that uses large language models (LLMs) to automatically generate helper assertions for Dafny programs.","To improve the success rate of LLMs in this task, we design two domain-specific prompting techniques.","First, we help the LLM determine the location of the missing assertion by analyzing the verifier's error message and inserting an assertion placeholder at that location.","Second, we provide the LLM with example assertions from the same codebase, which we select based on a new lemma similarity metric.","We evaluate our techniques on a dataset of helper assertions we extracted from three real-world Dafny codebases.","Our evaluation shows that Laurel is able to generate over 50% of the required helper assertions given only a few attempts, making LLMs a usable and affordable tool to further automate practical program verification."],"url":"http://arxiv.org/abs/2405.16792v1","category":"cs.LO"}
{"created":"2024-05-27 03:24:01","title":"NoteLLM-2: Multimodal Large Representation Models for Recommendation","abstract":"Large Language Models (LLMs) have demonstrated exceptional text understanding. Existing works explore their application in text embedding tasks. However, there are few works utilizing LLMs to assist multimodal representation tasks. In this work, we investigate the potential of LLMs to enhance multimodal representation in multimodal item-to-item (I2I) recommendations. One feasible method is the transfer of Multimodal Large Language Models (MLLMs) for representation tasks. However, pre-training MLLMs usually requires collecting high-quality, web-scale multimodal data, resulting in complex training procedures and high costs. This leads the community to rely heavily on open-source MLLMs, hindering customized training for representation scenarios. Therefore, we aim to design an end-to-end training method that customizes the integration of any existing LLMs and vision encoders to construct efficient multimodal representation models. Preliminary experiments show that fine-tuned LLMs in this end-to-end method tend to overlook image content. To overcome this challenge, we propose a novel training framework, NoteLLM-2, specifically designed for multimodal representation. We propose two ways to enhance the focus on visual information. The first method is based on the prompt viewpoint, which separates multimodal content into visual content and textual content. NoteLLM-2 adopts the multimodal In-Content Learning method to teach LLMs to focus on both modalities and aggregate key information. The second method is from the model architecture, utilizing a late fusion mechanism to directly fuse visual information into textual information. Extensive experiments have been conducted to validate the effectiveness of our method.","sentences":["Large Language Models (LLMs) have demonstrated exceptional text understanding.","Existing works explore their application in text embedding tasks.","However, there are few works utilizing LLMs to assist multimodal representation tasks.","In this work, we investigate the potential of LLMs to enhance multimodal representation in multimodal item-to-item (I2I) recommendations.","One feasible method is the transfer of Multimodal Large Language Models (MLLMs) for representation tasks.","However, pre-training MLLMs usually requires collecting high-quality, web-scale multimodal data, resulting in complex training procedures and high costs.","This leads the community to rely heavily on open-source MLLMs, hindering customized training for representation scenarios.","Therefore, we aim to design an end-to-end training method that customizes the integration of any existing LLMs and vision encoders to construct efficient multimodal representation models.","Preliminary experiments show that fine-tuned LLMs in this end-to-end method tend to overlook image content.","To overcome this challenge, we propose a novel training framework, NoteLLM-2, specifically designed for multimodal representation.","We propose two ways to enhance the focus on visual information.","The first method is based on the prompt viewpoint, which separates multimodal content into visual content and textual content.","NoteLLM-2 adopts the multimodal In-Content Learning method to teach LLMs to focus on both modalities and aggregate key information.","The second method is from the model architecture, utilizing a late fusion mechanism to directly fuse visual information into textual information.","Extensive experiments have been conducted to validate the effectiveness of our method."],"url":"http://arxiv.org/abs/2405.16789v1","category":"cs.IR"}
{"created":"2024-05-27 03:10:57","title":"TrojFM: Resource-efficient Backdoor Attacks against Very Large Foundation Models","abstract":"One key challenge in backdoor attacks against large foundation models is the resource limits. Backdoor attacks usually require retraining the target model, which is impractical for very large foundation models. Existing backdoor attacks are mainly designed for supervised classifiers or small foundation models (e.g., BERT). None of these attacks has successfully compromised a very large foundation model, such as Llama-3-70B, especially with limited computational resources. In this paper, we propose TrojFM, a novel backdoor attack tailored for very large foundation models. Our primary technical contribution is the development of a novel backdoor injection method. This method forces a backdoored model to generate similar hidden representations for poisoned inputs regardless of their actual semantics. Our approach injects such backdoors by fine-tuning only a very small proportion of model parameters. This enables TrojFM to efficiently launch downstream task-agnostic backdoor attacks against very large foundation models under limited computational resources. Moreover, we optimize the fine-tuning process with our customized QLoRA technique, enabling launching our attack via only~\\textit{one A100 GPU}. Furthermore, we design a new trigger injection method to ensure our attack stealthiness. Through extensive experiments, we first demonstrate that TrojFM can launch effective backdoor attacks against widely used large GPT-style models without jeopardizing their normal functionalities (and outperforming existing attacks on BERT-style models). Furthermore, we show that TrojFM is resilient to SOTA defenses and is insensitive to changes in key hyper-parameters. Finally, we conduct a resource analysis to quantify that our method can significantly save computational and memory costs compared to existing backdoor attacks.","sentences":["One key challenge in backdoor attacks against large foundation models is the resource limits.","Backdoor attacks usually require retraining the target model, which is impractical for very large foundation models.","Existing backdoor attacks are mainly designed for supervised classifiers or small foundation models (e.g., BERT).","None of these attacks has successfully compromised a very large foundation model, such as Llama-3-70B, especially with limited computational resources.","In this paper, we propose TrojFM, a novel backdoor attack tailored for very large foundation models.","Our primary technical contribution is the development of a novel backdoor injection method.","This method forces a backdoored model to generate similar hidden representations for poisoned inputs regardless of their actual semantics.","Our approach injects such backdoors by fine-tuning only a very small proportion of model parameters.","This enables TrojFM to efficiently launch downstream task-agnostic backdoor attacks against very large foundation models under limited computational resources.","Moreover, we optimize the fine-tuning process with our customized QLoRA technique, enabling launching our attack via only~\\textit{one A100 GPU}.","Furthermore, we design a new trigger injection method to ensure our attack stealthiness.","Through extensive experiments, we first demonstrate that TrojFM can launch effective backdoor attacks against widely used large GPT-style models without jeopardizing their normal functionalities (and outperforming existing attacks on BERT-style models).","Furthermore, we show that TrojFM is resilient to SOTA defenses and is insensitive to changes in key hyper-parameters.","Finally, we conduct a resource analysis to quantify that our method can significantly save computational and memory costs compared to existing backdoor attacks."],"url":"http://arxiv.org/abs/2405.16783v1","category":"cs.CR"}
{"created":"2024-05-27 02:27:28","title":"Reframing the Relationship in Out-of-Distribution Detection","abstract":"The remarkable achievements of Large Language Models (LLMs) have captivated the attention of both academia and industry, transcending their initial role in dialogue generation. The utilization of LLMs as intermediary agents in various tasks has yielded promising results, sparking a wave of innovation in artificial intelligence. Building on these breakthroughs, we introduce a novel approach that integrates the agent paradigm into the Out-of-distribution (OOD) detection task, aiming to enhance its robustness and adaptability. Our proposed method, Concept Matching with Agent (CMA), employs neutral prompts as agents to augment the CLIP-based OOD detection process. These agents function as dynamic observers and communication hubs, interacting with both In-distribution (ID) labels and data inputs to form vector triangle relationships. This triangular framework offers a more nuanced approach than the traditional binary relationship, allowing for better separation and identification of ID and OOD inputs. Our extensive experimental results showcase the superior performance of CMA over both zero-shot and training-required methods in a diverse array of real-world scenarios.","sentences":["The remarkable achievements of Large Language Models (LLMs) have captivated the attention of both academia and industry, transcending their initial role in dialogue generation.","The utilization of LLMs as intermediary agents in various tasks has yielded promising results, sparking a wave of innovation in artificial intelligence.","Building on these breakthroughs, we introduce a novel approach that integrates the agent paradigm into the Out-of-distribution (OOD) detection task, aiming to enhance its robustness and adaptability.","Our proposed method, Concept Matching with Agent (CMA), employs neutral prompts as agents to augment the CLIP-based OOD detection process.","These agents function as dynamic observers and communication hubs, interacting with both In-distribution (ID) labels and data inputs to form vector triangle relationships.","This triangular framework offers a more nuanced approach than the traditional binary relationship, allowing for better separation and identification of ID and OOD inputs.","Our extensive experimental results showcase the superior performance of CMA over both zero-shot and training-required methods in a diverse array of real-world scenarios."],"url":"http://arxiv.org/abs/2405.16766v1","category":"cs.CV"}
{"created":"2024-05-27 02:20:55","title":"Masked Face Recognition with Generative-to-Discriminative Representations","abstract":"Masked face recognition is important for social good but challenged by diverse occlusions that cause insufficient or inaccurate representations. In this work, we propose a unified deep network to learn generative-to-discriminative representations for facilitating masked face recognition. To this end, we split the network into three modules and learn them on synthetic masked faces in a greedy module-wise pretraining manner. First, we leverage a generative encoder pretrained for face inpainting and finetune it to represent masked faces into category-aware descriptors. Attribute to the generative encoder's ability in recovering context information, the resulting descriptors can provide occlusion-robust representations for masked faces, mitigating the effect of diverse masks. Then, we incorporate a multi-layer convolutional network as a discriminative reformer and learn it to convert the category-aware descriptors into identity-aware vectors, where the learning is effectively supervised by distilling relation knowledge from off-the-shelf face recognition model. In this way, the discriminative reformer together with the generative encoder serves as the pretrained backbone, providing general and discriminative representations towards masked faces. Finally, we cascade one fully-connected layer following by one softmax layer into a feature classifier and finetune it to identify the reformed identity-aware vectors. Extensive experiments on synthetic and realistic datasets demonstrate the effectiveness of our approach in recognizing masked faces.","sentences":["Masked face recognition is important for social good but challenged by diverse occlusions that cause insufficient or inaccurate representations.","In this work, we propose a unified deep network to learn generative-to-discriminative representations for facilitating masked face recognition.","To this end, we split the network into three modules and learn them on synthetic masked faces in a greedy module-wise pretraining manner.","First, we leverage a generative encoder pretrained for face inpainting and finetune it to represent masked faces into category-aware descriptors.","Attribute to the generative encoder's ability in recovering context information, the resulting descriptors can provide occlusion-robust representations for masked faces, mitigating the effect of diverse masks.","Then, we incorporate a multi-layer convolutional network as a discriminative reformer and learn it to convert the category-aware descriptors into identity-aware vectors, where the learning is effectively supervised by distilling relation knowledge from off-the-shelf face recognition model.","In this way, the discriminative reformer together with the generative encoder serves as the pretrained backbone, providing general and discriminative representations towards masked faces.","Finally, we cascade one fully-connected layer following by one softmax layer into a feature classifier and finetune it to identify the reformed identity-aware vectors.","Extensive experiments on synthetic and realistic datasets demonstrate the effectiveness of our approach in recognizing masked faces."],"url":"http://arxiv.org/abs/2405.16761v1","category":"cs.CV"}
{"created":"2024-05-27 01:54:16","title":"CHESS: Contextual Harnessing for Efficient SQL Synthesis","abstract":"Utilizing large language models (LLMs) for transforming natural language questions into SQL queries (text-to-SQL) is a promising yet challenging approach, particularly when applied to real-world databases with complex and extensive schemas. In particular, effectively incorporating data catalogs and database values for SQL generation remains an obstacle, leading to suboptimal solutions. We address this problem by proposing a new pipeline that effectively retrieves relevant data and context, selects an efficient schema, and synthesizes correct and efficient SQL queries. To increase retrieval precision, our pipeline introduces a hierarchical retrieval method leveraging model-generated keywords, locality-sensitive hashing indexing, and vector databases. Additionally, we have developed an adaptive schema pruning technique that adjusts based on the complexity of the problem and the model's context size. Our approach generalizes to both frontier proprietary models like GPT-4 and open-source models such as Llama-3-70B. Through a series of ablation studies, we demonstrate the effectiveness of each component of our pipeline and its impact on the end-to-end performance. Our method achieves new state-of-the-art performance on the cross-domain challenging BIRD dataset.","sentences":["Utilizing large language models (LLMs) for transforming natural language questions into SQL queries (text-to-SQL) is a promising yet challenging approach, particularly when applied to real-world databases with complex and extensive schemas.","In particular, effectively incorporating data catalogs and database values for SQL generation remains an obstacle, leading to suboptimal solutions.","We address this problem by proposing a new pipeline that effectively retrieves relevant data and context, selects an efficient schema, and synthesizes correct and efficient SQL queries.","To increase retrieval precision, our pipeline introduces a hierarchical retrieval method leveraging model-generated keywords, locality-sensitive hashing indexing, and vector databases.","Additionally, we have developed an adaptive schema pruning technique that adjusts based on the complexity of the problem and the model's context size.","Our approach generalizes to both frontier proprietary models like GPT-4 and open-source models such as Llama-3-70B. Through a series of ablation studies, we demonstrate the effectiveness of each component of our pipeline and its impact on the end-to-end performance.","Our method achieves new state-of-the-art performance on the cross-domain challenging BIRD dataset."],"url":"http://arxiv.org/abs/2405.16755v1","category":"cs.LG"}
{"created":"2024-05-27 01:53:53","title":"Multi-answer Constrained Optimal Querying: Maximum Information Gain Coding","abstract":"As the rapidly developments of artificial intelligence and machine learning, behavior tree design in multiagent system or AI game become more important. The behavior tree design problem is highly related to the source coding in information theory. \"Twenty Questions\" problem is a typical example for the behavior tree design, usually used to explain the source coding application in information theory and can be solved by Huffman coding. In some realistic scenarios, there are some constraints on the asked questions. However, for general question set, finding the minimum expected querying length is an open problem, belongs to NP-hard. Recently, a new coding scheme has been proposed to provide a near optimal solution for binary cases with some constraints, named greedy binary separation coding (GBSC). In this work, we shall generalize it to D-ary cases and propose maximum information gain coding (MIGC) approach to solve the multi-answer decision constrained querying problem. The optimality of the proposed MIGC is discussed in theory. Later on, we also apply MIGC to discuss three practical scenarios and showcase that MIGC has better performance than GBSC and Shannon Coding in terms of bits persymbol.","sentences":["As the rapidly developments of artificial intelligence and machine learning, behavior tree design in multiagent system or AI game become more important.","The behavior tree design problem is highly related to the source coding in information theory.","\"Twenty Questions\" problem is a typical example for the behavior tree design, usually used to explain the source coding application in information theory and can be solved by Huffman coding.","In some realistic scenarios, there are some constraints on the asked questions.","However, for general question set, finding the minimum expected querying length is an open problem, belongs to NP-hard.","Recently, a new coding scheme has been proposed to provide a near optimal solution for binary cases with some constraints, named greedy binary separation coding (GBSC).","In this work, we shall generalize it to D-ary cases and propose maximum information gain coding (MIGC) approach to solve the multi-answer decision constrained querying problem.","The optimality of the proposed MIGC is discussed in theory.","Later on, we also apply MIGC to discuss three practical scenarios and showcase that MIGC has better performance than GBSC and Shannon Coding in terms of bits persymbol."],"url":"http://arxiv.org/abs/2405.16753v1","category":"cs.IT"}
{"created":"2024-05-27 01:48:07","title":"Model Ensembling for Constrained Optimization","abstract":"There is a long history in machine learning of model ensembling, beginning with boosting and bagging and continuing to the present day. Much of this history has focused on combining models for classification and regression, but recently there is interest in more complex settings such as ensembling policies in reinforcement learning. Strong connections have also emerged between ensembling and multicalibration techniques. In this work, we further investigate these themes by considering a setting in which we wish to ensemble models for multidimensional output predictions that are in turn used for downstream optimization. More precisely, we imagine we are given a number of models mapping a state space to multidimensional real-valued predictions. These predictions form the coefficients of a linear objective that we would like to optimize under specified constraints. The fundamental question we address is how to improve and combine such models in a way that outperforms the best of them in the downstream optimization problem. We apply multicalibration techniques that lead to two provably efficient and convergent algorithms. The first of these (the white box approach) requires being given models that map states to output predictions, while the second (the \\emph{black box} approach) requires only policies (mappings from states to solutions to the optimization problem). For both, we provide convergence and utility guarantees. We conclude by investigating the performance and behavior of the two algorithms in a controlled experimental setting.","sentences":["There is a long history in machine learning of model ensembling, beginning with boosting and bagging and continuing to the present day.","Much of this history has focused on combining models for classification and regression, but recently there is interest in more complex settings such as ensembling policies in reinforcement learning.","Strong connections have also emerged between ensembling and multicalibration techniques.","In this work, we further investigate these themes by considering a setting in which we wish to ensemble models for multidimensional output predictions that are in turn used for downstream optimization.","More precisely, we imagine we are given a number of models mapping a state space to multidimensional real-valued predictions.","These predictions form the coefficients of a linear objective that we would like to optimize under specified constraints.","The fundamental question we address is how to improve and combine such models in a way that outperforms the best of them in the downstream optimization problem.","We apply multicalibration techniques that lead to two provably efficient and convergent algorithms.","The first of these (the white box approach) requires being given models that map states to output predictions, while the second (the \\emph{black box} approach) requires only policies (mappings from states to solutions to the optimization problem).","For both, we provide convergence and utility guarantees.","We conclude by investigating the performance and behavior of the two algorithms in a controlled experimental setting."],"url":"http://arxiv.org/abs/2405.16752v1","category":"cs.LG"}
{"created":"2024-05-27 01:47:14","title":"LLM-Based Cooperative Agents using Information Relevance and Plan Validation","abstract":"We address the challenge of multi-agent cooperation, where agents achieve a common goal by interacting with a 3D scene and cooperating with decentralized agents under complex partial observations. This involves managing communication costs and optimizing interaction trajectories in dynamic environments. Our research focuses on three primary limitations of existing cooperative agent systems. Firstly, current systems demonstrate inefficiency in managing acquired information through observation, resulting in declining planning performance as the environment becomes more complex with additional objects or goals. Secondly, the neglect of false plans in partially observable settings leads to suboptimal cooperative performance, as agents struggle to adapt to environmental changes influenced by the unseen actions of other agents. Lastly, the failure to incorporate spatial data into decision-making processes restricts the agent's ability to construct optimized trajectories. To overcome these limitations, we propose the RElevance and Validation-Enhanced Cooperative Language Agent (REVECA), a novel cognitive architecture powered by GPT-3.5. REVECA leverages relevance assessment, plan validation, and spatial information to enhance the efficiency and robustness of agent cooperation in dynamic and partially observable environments while minimizing continuous communication costs and effectively managing irrelevant dummy objects. Our extensive experiments demonstrate the superiority of REVECA over previous approaches, including those driven by GPT-4.0. Additionally, a user study highlights REVECA's potential for achieving trustworthy human-AI cooperation. We expect that REVECA will have significant applications in gaming, XR applications, educational tools, and humanoid robots, contributing to substantial economic, commercial, and academic advancements.","sentences":["We address the challenge of multi-agent cooperation, where agents achieve a common goal by interacting with a 3D scene and cooperating with decentralized agents under complex partial observations.","This involves managing communication costs and optimizing interaction trajectories in dynamic environments.","Our research focuses on three primary limitations of existing cooperative agent systems.","Firstly, current systems demonstrate inefficiency in managing acquired information through observation, resulting in declining planning performance as the environment becomes more complex with additional objects or goals.","Secondly, the neglect of false plans in partially observable settings leads to suboptimal cooperative performance, as agents struggle to adapt to environmental changes influenced by the unseen actions of other agents.","Lastly, the failure to incorporate spatial data into decision-making processes restricts the agent's ability to construct optimized trajectories.","To overcome these limitations, we propose the RElevance and Validation-Enhanced Cooperative Language Agent (REVECA), a novel cognitive architecture powered by GPT-3.5.","REVECA leverages relevance assessment, plan validation, and spatial information to enhance the efficiency and robustness of agent cooperation in dynamic and partially observable environments while minimizing continuous communication costs and effectively managing irrelevant dummy objects.","Our extensive experiments demonstrate the superiority of REVECA over previous approaches, including those driven by GPT-4.0.","Additionally, a user study highlights REVECA's potential for achieving trustworthy human-AI cooperation.","We expect that REVECA will have significant applications in gaming, XR applications, educational tools, and humanoid robots, contributing to substantial economic, commercial, and academic advancements."],"url":"http://arxiv.org/abs/2405.16751v1","category":"cs.AI"}
{"created":"2024-05-27 01:31:30","title":"Ecosystem of Large Language Models for Code","abstract":"The availability of vast amounts of publicly accessible data of source code and the advances in modern language models, coupled with increasing computational resources, have led to a remarkable surge in the development of large language models for code (LLM4Code, for short). The interaction between code datasets and models gives rise to a complex ecosystem characterized by intricate dependencies that are worth studying. This paper introduces a pioneering analysis of the code model ecosystem. Utilizing Hugging Face -- the premier hub for transformer-based models -- as our primary source, we curate a list of datasets and models that are manually confirmed to be relevant to software engineering. By analyzing the ecosystem, we first identify the popular and influential datasets, models, and contributors. The popularity is quantified by various metrics, including the number of downloads, the number of likes, the number of reuses, etc. The ecosystem follows a power-law distribution, indicating that users prefer widely recognized models and datasets. Then, we manually categorize how models in the ecosystem are reused into nine categories, analyzing prevalent model reuse practices. The top 3 most popular reuse types are fine-tuning, architecture sharing, and quantization. We also explore the practices surrounding the publication of LLM4Code, specifically focusing on documentation practice and license selection. We find that the documentation in the ecosystem contains less information than that in general artificial intelligence (AI)-related repositories hosted on GitHub. Additionally, the license usage is also different from other software repositories. Models in the ecosystem adopt some AI-specific licenses, e.g., RAIL (Responsible AI Licenses) and AI model license agreement.","sentences":["The availability of vast amounts of publicly accessible data of source code and the advances in modern language models, coupled with increasing computational resources, have led to a remarkable surge in the development of large language models for code (LLM4Code, for short).","The interaction between code datasets and models gives rise to a complex ecosystem characterized by intricate dependencies that are worth studying.","This paper introduces a pioneering analysis of the code model ecosystem.","Utilizing Hugging Face -- the premier hub for transformer-based models -- as our primary source, we curate a list of datasets and models that are manually confirmed to be relevant to software engineering.","By analyzing the ecosystem, we first identify the popular and influential datasets, models, and contributors.","The popularity is quantified by various metrics, including the number of downloads, the number of likes, the number of reuses, etc.","The ecosystem follows a power-law distribution, indicating that users prefer widely recognized models and datasets.","Then, we manually categorize how models in the ecosystem are reused into nine categories, analyzing prevalent model reuse practices.","The top 3 most popular reuse types are fine-tuning, architecture sharing, and quantization.","We also explore the practices surrounding the publication of LLM4Code, specifically focusing on documentation practice and license selection.","We find that the documentation in the ecosystem contains less information than that in general artificial intelligence (AI)-related repositories hosted on GitHub.","Additionally, the license usage is also different from other software repositories.","Models in the ecosystem adopt some AI-specific licenses, e.g., RAIL (Responsible AI Licenses) and AI model license agreement."],"url":"http://arxiv.org/abs/2405.16746v1","category":"cs.SE"}
{"created":"2024-05-27 01:08:23","title":"Oracle-Efficient Reinforcement Learning for Max Value Ensembles","abstract":"Reinforcement learning (RL) in large or infinite state spaces is notoriously challenging, both theoretically (where worst-case sample and computational complexities must scale with state space cardinality) and experimentally (where function approximation and policy gradient techniques often scale poorly and suffer from instability and high variance). One line of research attempting to address these difficulties makes the natural assumption that we are given a collection of heuristic base or $\\textit{constituent}$ policies upon which we would like to improve in a scalable manner. In this work we aim to compete with the $\\textit{max-following policy}$, which at each state follows the action of whichever constituent policy has the highest value. The max-following policy is always at least as good as the best constituent policy, and may be considerably better. Our main result is an efficient algorithm that learns to compete with the max-following policy, given only access to the constituent policies (but not their value functions). In contrast to prior work in similar settings, our theoretical results require only the minimal assumption of an ERM oracle for value function approximation for the constituent policies (and not the global optimal policy or the max-following policy itself) on samplable distributions. We illustrate our algorithm's experimental effectiveness and behavior on several robotic simulation testbeds.","sentences":["Reinforcement learning (RL) in large or infinite state spaces is notoriously challenging, both theoretically (where worst-case sample and computational complexities must scale with state space cardinality) and experimentally (where function approximation and policy gradient techniques often scale poorly and suffer from instability and high variance).","One line of research attempting to address these difficulties makes the natural assumption that we are given a collection of heuristic base or $\\textit{constituent}$ policies upon which we would like to improve in a scalable manner.","In this work we aim to compete with the $\\textit{max-following policy}$, which at each state follows the action of whichever constituent policy has the highest value.","The max-following policy is always at least as good as the best constituent policy, and may be considerably better.","Our main result is an efficient algorithm that learns to compete with the max-following policy, given only access to the constituent policies (but not their value functions).","In contrast to prior work in similar settings, our theoretical results require only the minimal assumption of an ERM oracle for value function approximation for the constituent policies (and not the global optimal policy or the max-following policy itself) on samplable distributions.","We illustrate our algorithm's experimental effectiveness and behavior on several robotic simulation testbeds."],"url":"http://arxiv.org/abs/2405.16739v1","category":"cs.LG"}
{"created":"2024-05-27 00:11:53","title":"Latent Energy-Based Odyssey: Black-Box Optimization via Expanded Exploration in the Energy-Based Latent Space","abstract":"Offline Black-Box Optimization (BBO) aims at optimizing a black-box function using the knowledge from a pre-collected offline dataset of function values and corresponding input designs. However, the high-dimensional and highly-multimodal input design space of black-box function pose inherent challenges for most existing methods that model and operate directly upon input designs. These issues include but are not limited to high sample complexity, which relates to inaccurate approximation of black-box function; and insufficient coverage and exploration of input design modes, which leads to suboptimal proposal of new input designs. In this work, we consider finding a latent space that serves as a compressed yet accurate representation of the design-value joint space, enabling effective latent exploration of high-value input design modes. To this end, we formulate an learnable energy-based latent space, and propose Noise-intensified Telescoping density-Ratio Estimation (NTRE) scheme for variational learning of an accurate latent space model without costly Markov Chain Monte Carlo. The optimization process is then exploration of high-value designs guided by the learned energy-based model in the latent space, formulated as gradient-based sampling from a latent-variable-parameterized inverse model. We show that our particular parameterization encourages expanded exploration around high-value design modes, motivated by inversion thinking of a fundamental result of conditional covariance matrix typically used for variance reduction. We observe that our method, backed by an accurately learned informative latent space and an expanding-exploration model design, yields significant improvements over strong previous methods on both synthetic and real world datasets such as the design-bench suite.","sentences":["Offline Black-Box Optimization (BBO) aims at optimizing a black-box function using the knowledge from a pre-collected offline dataset of function values and corresponding input designs.","However, the high-dimensional and highly-multimodal input design space of black-box function pose inherent challenges for most existing methods that model and operate directly upon input designs.","These issues include but are not limited to high sample complexity, which relates to inaccurate approximation of black-box function; and insufficient coverage and exploration of input design modes, which leads to suboptimal proposal of new input designs.","In this work, we consider finding a latent space that serves as a compressed yet accurate representation of the design-value joint space, enabling effective latent exploration of high-value input design modes.","To this end, we formulate an learnable energy-based latent space, and propose Noise-intensified Telescoping density-Ratio Estimation (NTRE) scheme for variational learning of an accurate latent space model without costly Markov Chain Monte Carlo.","The optimization process is then exploration of high-value designs guided by the learned energy-based model in the latent space, formulated as gradient-based sampling from a latent-variable-parameterized inverse model.","We show that our particular parameterization encourages expanded exploration around high-value design modes, motivated by inversion thinking of a fundamental result of conditional covariance matrix typically used for variance reduction.","We observe that our method, backed by an accurately learned informative latent space and an expanding-exploration model design, yields significant improvements over strong previous methods on both synthetic and real world datasets such as the design-bench suite."],"url":"http://arxiv.org/abs/2405.16730v1","category":"cs.LG"}
{"created":"2024-05-26 23:56:45","title":"Towards Multi-Task Multi-Modal Models: A Video Generative Perspective","abstract":"Advancements in language foundation models have primarily fueled the recent surge in artificial intelligence. In contrast, generative learning of non-textual modalities, especially videos, significantly trails behind language modeling. This thesis chronicles our endeavor to build multi-task models for generating videos and other modalities under diverse conditions, as well as for understanding and compression applications. Given the high dimensionality of visual data, we pursue concise and accurate latent representations. Our video-native spatial-temporal tokenizers preserve high fidelity. We unveil a novel approach to mapping bidirectionally between visual observation and interpretable lexical terms. Furthermore, our scalable visual token representation proves beneficial across generation, compression, and understanding tasks. This achievement marks the first instances of language models surpassing diffusion models in visual synthesis and a video tokenizer outperforming industry-standard codecs. Within these multi-modal latent spaces, we study the design of multi-task generative models. Our masked multi-task transformer excels at the quality, efficiency, and flexibility of video generation. We enable a frozen language model, trained solely on text, to generate visual content. Finally, we build a scalable generative multi-modal transformer trained from scratch, enabling the generation of videos containing high-fidelity motion with the corresponding audio given diverse conditions. Throughout the course, we have shown the effectiveness of integrating multiple tasks, crafting high-fidelity latent representation, and generating multiple modalities. This work suggests intriguing potential for future exploration in generating non-textual data and enabling real-time, interactive experiences across various media forms.","sentences":["Advancements in language foundation models have primarily fueled the recent surge in artificial intelligence.","In contrast, generative learning of non-textual modalities, especially videos, significantly trails behind language modeling.","This thesis chronicles our endeavor to build multi-task models for generating videos and other modalities under diverse conditions, as well as for understanding and compression applications.","Given the high dimensionality of visual data, we pursue concise and accurate latent representations.","Our video-native spatial-temporal tokenizers preserve high fidelity.","We unveil a novel approach to mapping bidirectionally between visual observation and interpretable lexical terms.","Furthermore, our scalable visual token representation proves beneficial across generation, compression, and understanding tasks.","This achievement marks the first instances of language models surpassing diffusion models in visual synthesis and a video tokenizer outperforming industry-standard codecs.","Within these multi-modal latent spaces, we study the design of multi-task generative models.","Our masked multi-task transformer excels at the quality, efficiency, and flexibility of video generation.","We enable a frozen language model, trained solely on text, to generate visual content.","Finally, we build a scalable generative multi-modal transformer trained from scratch, enabling the generation of videos containing high-fidelity motion with the corresponding audio given diverse conditions.","Throughout the course, we have shown the effectiveness of integrating multiple tasks, crafting high-fidelity latent representation, and generating multiple modalities.","This work suggests intriguing potential for future exploration in generating non-textual data and enabling real-time, interactive experiences across various media forms."],"url":"http://arxiv.org/abs/2405.16728v1","category":"cs.CV"}
{"created":"2024-05-26 23:14:37","title":"Amortized Active Causal Induction with Deep Reinforcement Learning","abstract":"We present Causal Amortized Active Structure Learning (CAASL), an active intervention design policy that can select interventions that are adaptive, real-time and that does not require access to the likelihood. This policy, an amortized network based on the transformer, is trained with reinforcement learning on a simulator of the design environment, and a reward function that measures how close the true causal graph is to a causal graph posterior inferred from the gathered data. On synthetic data and a single-cell gene expression simulator, we demonstrate empirically that the data acquired through our policy results in a better estimate of the underlying causal graph than alternative strategies. Our design policy successfully achieves amortized intervention design on the distribution of the training environment while also generalizing well to distribution shifts in test-time design environments. Further, our policy also demonstrates excellent zero-shot generalization to design environments with dimensionality higher than that during training, and to intervention types that it has not been trained on.","sentences":["We present Causal Amortized Active Structure Learning (CAASL), an active intervention design policy that can select interventions that are adaptive, real-time and that does not require access to the likelihood.","This policy, an amortized network based on the transformer, is trained with reinforcement learning on a simulator of the design environment, and a reward function that measures how close the true causal graph is to a causal graph posterior inferred from the gathered data.","On synthetic data and a single-cell gene expression simulator, we demonstrate empirically that the data acquired through our policy results in a better estimate of the underlying causal graph than alternative strategies.","Our design policy successfully achieves amortized intervention design on the distribution of the training environment while also generalizing well to distribution shifts in test-time design environments.","Further, our policy also demonstrates excellent zero-shot generalization to design environments with dimensionality higher than that during training, and to intervention types that it has not been trained on."],"url":"http://arxiv.org/abs/2405.16718v1","category":"cs.LG"}
{"created":"2024-05-26 22:31:09","title":"Coil Reweighting to Suppress Motion Artifacts in Real-Time Exercise Cine Imaging","abstract":"Background: Accelerated real-time cine (RT-Cine) imaging enables cardiac function assessment without the need for breath-holding. However, when performed during in-magnet exercise, RT-Cine images may exhibit significant motion artifacts. Methods: By projecting the time-averaged images to the subspace spanned by the coil sensitivity maps, we propose a coil reweighting (CR) method to automatically suppress a subset of receive coils that introduces a high level of artifacts in the reconstructed image. RT-Cine data collected at rest and during exercise from ten healthy volunteers and six patients were utilized to assess the performance of the proposed method. One short-axis and one two-chamber RT-Cine series reconstructed with and without CR from each subject were visually scored by two cardiologists in terms of the level of artifacts on a scale of 1 (worst) to 5 (best). Results: For healthy volunteers, applying CR to RT-Cine images collected at rest did not significantly change the image quality score (p=1). In contrast, for RT-Cine images collected during exercise, CR significantly improved the score from 3.9 to 4.68 (p<0.001). Similarly, in patients, CR did not significantly change the score for images collected at rest (p=0.031) but markedly improved the score from 3.15 to 4.42 (p<0.001) for images taken during exercise. Despite lower image quality scores in the patient cohort compared to healthy subjects, likely due to larger body habitus and the difficulty of limiting body motion during exercise, CR effectively suppressed motion artifacts, with all image series from the patient cohort receiving a score of four or higher. Conclusion: Using data from healthy subjects and patients, we demonstrate that the motion artifacts in the reconstructed RT-Cine images can be effectively suppressed significantly with the proposed CR method.","sentences":["Background: Accelerated real-time cine (RT-Cine) imaging enables cardiac function assessment without the need for breath-holding.","However, when performed during in-magnet exercise, RT-Cine images may exhibit significant motion artifacts.","Methods: By projecting the time-averaged images to the subspace spanned by the coil sensitivity maps, we propose a coil reweighting (CR) method to automatically suppress a subset of receive coils that introduces a high level of artifacts in the reconstructed image.","RT-Cine data collected at rest and during exercise from ten healthy volunteers and six patients were utilized to assess the performance of the proposed method.","One short-axis and one two-chamber RT-Cine series reconstructed with and without CR from each subject were visually scored by two cardiologists in terms of the level of artifacts on a scale of 1 (worst) to 5 (best).","Results: For healthy volunteers, applying CR to RT-Cine images collected at rest did not significantly change the image quality score (p=1).","In contrast, for RT-Cine images collected during exercise, CR significantly improved the score from 3.9 to 4.68 (p<0.001).","Similarly, in patients, CR did not significantly change the score for images collected at rest (p=0.031) but markedly improved the score from 3.15 to 4.42 (p<0.001) for images taken during exercise.","Despite lower image quality scores in the patient cohort compared to healthy subjects, likely due to larger body habitus and the difficulty of limiting body motion during exercise, CR effectively suppressed motion artifacts, with all image series from the patient cohort receiving a score of four or higher.","Conclusion: Using data from healthy subjects and patients, we demonstrate that the motion artifacts in the reconstructed RT-Cine images can be effectively suppressed significantly with the proposed CR method."],"url":"http://arxiv.org/abs/2405.16715v1","category":"eess.SP"}
{"created":"2024-05-26 22:30:29","title":"Crafting Interpretable Embeddings by Asking LLMs Questions","abstract":"Large language models (LLMs) have rapidly improved text embeddings for a growing array of natural-language processing tasks. However, their opaqueness and proliferation into scientific domains such as neuroscience have created a growing need for interpretability. Here, we ask whether we can obtain interpretable embeddings through LLM prompting. We introduce question-answering embeddings (QA-Emb), embeddings where each feature represents an answer to a yes/no question asked to an LLM. Training QA-Emb reduces to selecting a set of underlying questions rather than learning model weights.   We use QA-Emb to flexibly generate interpretable models for predicting fMRI voxel responses to language stimuli. QA-Emb significantly outperforms an established interpretable baseline, and does so while requiring very few questions. This paves the way towards building flexible feature spaces that can concretize and evaluate our understanding of semantic brain representations. We additionally find that QA-Emb can be effectively approximated with an efficient model, and we explore broader applications in simple NLP tasks.","sentences":["Large language models (LLMs) have rapidly improved text embeddings for a growing array of natural-language processing tasks.","However, their opaqueness and proliferation into scientific domains such as neuroscience have created a growing need for interpretability.","Here, we ask whether we can obtain interpretable embeddings through LLM prompting.","We introduce question-answering embeddings (QA-Emb), embeddings where each feature represents an answer to a yes/no question asked to an LLM.","Training QA-Emb reduces to selecting a set of underlying questions rather than learning model weights.   ","We use QA-Emb to flexibly generate interpretable models for predicting fMRI voxel responses to language stimuli.","QA-Emb significantly outperforms an established interpretable baseline, and does so while requiring very few questions.","This paves the way towards building flexible feature spaces that can concretize and evaluate our understanding of semantic brain representations.","We additionally find that QA-Emb can be effectively approximated with an efficient model, and we explore broader applications in simple NLP tasks."],"url":"http://arxiv.org/abs/2405.16714v1","category":"cs.CL"}
{"created":"2024-05-26 22:23:02","title":"Zamba: A Compact 7B SSM Hybrid Model","abstract":"In this technical report, we present Zamba, a novel 7B SSM-transformer hybrid model which achieves competitive performance against leading open-weight models at a comparable scale. Zamba is trained on 1T tokens from openly available datasets and is the best non-transformer model at this scale. Zamba pioneers a unique architecture combining a Mamba backbone with a single shared attention module, thus obtaining the benefits of attention at minimal parameter cost. Due to its architecture, Zamba is significantly faster at inference than comparable transformer models and requires substantially less memory for generation of long sequences. Zamba is pretrained in two phases: the first phase is based on existing web datasets, while the second one consists of annealing the model over high-quality instruct and synthetic datasets, and is characterized by a rapid learning rate decay. We open-source the weights and all checkpoints for Zamba, through both phase 1 and annealing phases.","sentences":["In this technical report, we present Zamba, a novel 7B SSM-transformer hybrid model which achieves competitive performance against leading open-weight models at a comparable scale.","Zamba is trained on 1T tokens from openly available datasets and is the best non-transformer model at this scale.","Zamba pioneers a unique architecture combining a Mamba backbone with a single shared attention module, thus obtaining the benefits of attention at minimal parameter cost.","Due to its architecture, Zamba is significantly faster at inference than comparable transformer models and requires substantially less memory for generation of long sequences.","Zamba is pretrained in two phases: the first phase is based on existing web datasets, while the second one consists of annealing the model over high-quality instruct and synthetic datasets, and is characterized by a rapid learning rate decay.","We open-source the weights and all checkpoints for Zamba, through both phase 1 and annealing phases."],"url":"http://arxiv.org/abs/2405.16712v1","category":"cs.LG"}
{"created":"2024-05-26 22:18:38","title":"The AI-DEC: A Card-based Design Method for User-centered AI Explanations","abstract":"Increasing evidence suggests that many deployed AI systems do not sufficiently support end-user interaction and information needs. Engaging end-users in the design of these systems can reveal user needs and expectations, yet effective ways of engaging end-users in the AI explanation design remain under-explored. To address this gap, we developed a design method, called AI-DEC, that defines four dimensions of AI explanations that are critical for the integration of AI systems -- communication content, modality, frequency, and direction -- and offers design examples for end-users to design AI explanations that meet their needs. We evaluated this method through co-design sessions with workers in healthcare, finance, and management industries who regularly use AI systems in their daily work. Findings indicate that the AI-DEC effectively supported workers in designing explanations that accommodated diverse levels of performance and autonomy needs, which varied depending on the AI system's workplace role and worker values. We discuss the implications of using the AI-DEC for the user-centered design of AI explanations in real-world systems.","sentences":["Increasing evidence suggests that many deployed AI systems do not sufficiently support end-user interaction and information needs.","Engaging end-users in the design of these systems can reveal user needs and expectations, yet effective ways of engaging end-users in the AI explanation design remain under-explored.","To address this gap, we developed a design method, called AI-DEC, that defines four dimensions of AI explanations that are critical for the integration of AI systems -- communication content, modality, frequency, and direction -- and offers design examples for end-users to design AI explanations that meet their needs.","We evaluated this method through co-design sessions with workers in healthcare, finance, and management industries who regularly use AI systems in their daily work.","Findings indicate that the AI-DEC effectively supported workers in designing explanations that accommodated diverse levels of performance and autonomy needs, which varied depending on the AI system's workplace role and worker values.","We discuss the implications of using the AI-DEC for the user-centered design of AI explanations in real-world systems."],"url":"http://arxiv.org/abs/2405.16711v1","category":"cs.HC"}
{"created":"2024-05-26 21:58:32","title":"Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning","abstract":"This demo paper examines the susceptibility of Federated Learning (FL) systems to targeted data poisoning attacks, presenting a novel system for visualizing and mitigating such threats. We simulate targeted data poisoning attacks via label flipping and analyze the impact on model performance, employing a five-component system that includes Simulation and Data Generation, Data Collection and Upload, User-friendly Interface, Analysis and Insight, and Advisory System. Observations from three demo modules: label manipulation, attack timing, and malicious attack availability, and two analysis components: utility and analytical behavior of local model updates highlight the risks to system integrity and offer insight into the resilience of FL systems. The demo is available at https://github.com/CathyXueqingZhang/DataPoisoningVis.","sentences":["This demo paper examines the susceptibility of Federated Learning (FL) systems to targeted data poisoning attacks, presenting a novel system for visualizing and mitigating such threats.","We simulate targeted data poisoning attacks via label flipping and analyze the impact on model performance, employing a five-component system that includes Simulation and Data Generation, Data Collection and Upload, User-friendly Interface, Analysis and Insight, and Advisory System.","Observations from three demo modules: label manipulation, attack timing, and malicious attack availability, and two analysis components: utility and analytical behavior of local model updates highlight the risks to system integrity and offer insight into the resilience of FL systems.","The demo is available at https://github.com/CathyXueqingZhang/DataPoisoningVis."],"url":"http://arxiv.org/abs/2405.16707v1","category":"cs.CR"}
{"created":"2024-05-26 20:58:12","title":"Detection of decision-making manipulation in the pairwise comparisons method","abstract":"Most decision-making models, including the pairwise comparison method, assume the decision-makers honesty. However, it is easy to imagine a situation where a decision-maker tries to manipulate the ranking results. This paper presents three simple manipulation methods in the pairwise comparison method. We then try to detect these methods using appropriately constructed neural networks. Experimental results accompany the proposed solutions on the generated data, showing a considerable manipulation detection level.","sentences":["Most decision-making models, including the pairwise comparison method, assume the decision-makers honesty.","However, it is easy to imagine a situation where a decision-maker tries to manipulate the ranking results.","This paper presents three simple manipulation methods in the pairwise comparison method.","We then try to detect these methods using appropriately constructed neural networks.","Experimental results accompany the proposed solutions on the generated data, showing a considerable manipulation detection level."],"url":"http://arxiv.org/abs/2405.16693v1","category":"cs.AI"}
{"created":"2024-05-26 20:02:29","title":"Aggregation-Diffusion Equations for Collective Behaviour in the Sciences","abstract":"This is a survey article based on the content of the plenary lecture given by Jos\\'e A. Carrillo at the ICIAM23 conference in Tokyo. It is devoted to produce a snapshot of the state of the art in the analysis, numerical analysis, simulation, and applications of the vast area of aggregation-diffusion equations. We also discuss the implications in mathematical biology explaining cell sorting in tissue growth as an example of this modelling framework. This modelling strategy is quite successful in other timely applications such as global optimisation, parameter estimation and machine learning.","sentences":["This is a survey article based on the content of the plenary lecture given by Jos\\'e A. Carrillo at the ICIAM23 conference in Tokyo.","It is devoted to produce a snapshot of the state of the art in the analysis, numerical analysis, simulation, and applications of the vast area of aggregation-diffusion equations.","We also discuss the implications in mathematical biology explaining cell sorting in tissue growth as an example of this modelling framework.","This modelling strategy is quite successful in other timely applications such as global optimisation, parameter estimation and machine learning."],"url":"http://arxiv.org/abs/2405.16679v1","category":"math.AP"}
{"created":"2024-05-26 19:44:42","title":"iDIGIT4L. Nuevos ecosistemas de digitalizaci\u00f3n y aprendizaje hombre-m\u00e1quina para sistemas de fabricaci\u00f3n industrial heredados","abstract":"The digitization of the productive ecosystem related to human-machine interaction has become a priority for small and medium-sized enterprises. Particularly to face the challenges of Industry 4.0 and advanced digital skills in the workplace. From the research point of view, digitization opens a global scenario for the generation of opportunities for learning in existing manufacturing systems. Mainly, traditional environments must deal with competitive pressures to incorporate new technologies and adapt the skills of workers. In this paper is presented the iDIGIT4L project, which was envisaged to research and develop a digitization ecosystem where people and systems interact in order to transform industrial processes in an intelligent and predictive way. The project contributes with a three-tier human-machine learning methodology that provides augmented and bi-directional interaction in a traditional manufacturing scenario. It is based on the implementation of a non-intrusively integrated digital twin, characterizing an old industrial milling machine for learning through knowledge models supported by the experience of skilled workers. As a result, it has been possible to simultaneously update the functionalities of the industrial system and the digital skills of the workers, becoming an integral part of the digital twin.","sentences":["The digitization of the productive ecosystem related to human-machine interaction has become a priority for small and medium-sized enterprises.","Particularly to face the challenges of Industry 4.0 and advanced digital skills in the workplace.","From the research point of view, digitization opens a global scenario for the generation of opportunities for learning in existing manufacturing systems.","Mainly, traditional environments must deal with competitive pressures to incorporate new technologies and adapt the skills of workers.","In this paper is presented the iDIGIT4L project, which was envisaged to research and develop a digitization ecosystem where people and systems interact in order to transform industrial processes in an intelligent and predictive way.","The project contributes with a three-tier human-machine learning methodology that provides augmented and bi-directional interaction in a traditional manufacturing scenario.","It is based on the implementation of a non-intrusively integrated digital twin, characterizing an old industrial milling machine for learning through knowledge models supported by the experience of skilled workers.","As a result, it has been possible to simultaneously update the functionalities of the industrial system and the digital skills of the workers, becoming an integral part of the digital twin."],"url":"http://arxiv.org/abs/2405.16676v1","category":"cs.HC"}
{"created":"2024-05-26 19:25:08","title":"Mixture of Experts Using Tensor Products","abstract":"In multi-task learning, the conventional approach involves training a model on multiple tasks simultaneously. However, the training signals from different tasks can interfere with one another, potentially leading to \\textit{negative transfer}. To mitigate this, we investigate if modular language models can facilitate positive transfer and systematic generalization. Specifically, we propose a novel modular language model (\\texttt{TensorPoly}), that balances parameter efficiency with nuanced routing methods. For \\textit{modules}, we reparameterize Low-Rank Adaptation (\\texttt{LoRA}) by employing an entangled tensor through the use of tensor product operations and name the resulting approach \\texttt{TLoRA}. For \\textit{routing function}, we tailor two innovative routing functions according to the granularity: \\texttt{TensorPoly-I} which directs to each rank within the entangled tensor while \\texttt{TensorPoly-II} offers a finer-grained routing approach targeting each order of the entangled tensor. The experimental results from the multi-task T0-benchmark demonstrate that: 1) all modular LMs surpass the corresponding dense approaches, highlighting the potential of modular language models to mitigate negative inference in multi-task learning and deliver superior outcomes. 2) \\texttt{TensorPoly-I} achieves higher parameter efficiency in adaptation and outperforms other modular LMs, which shows the potential of our approach in multi-task transfer learning.","sentences":["In multi-task learning, the conventional approach involves training a model on multiple tasks simultaneously.","However, the training signals from different tasks can interfere with one another, potentially leading to \\textit{negative transfer}.","To mitigate this, we investigate if modular language models can facilitate positive transfer and systematic generalization.","Specifically, we propose a novel modular language model (\\texttt{TensorPoly}), that balances parameter efficiency with nuanced routing methods.","For \\textit{modules}, we reparameterize Low-Rank Adaptation (\\texttt{LoRA}) by employing an entangled tensor through the use of tensor product operations and name the resulting approach \\texttt{TLoRA}.","For \\textit{routing function}, we tailor two innovative routing functions according to the granularity: \\texttt{TensorPoly-I} which directs to each rank within the entangled tensor while \\texttt{TensorPoly-II} offers a finer-grained routing approach targeting each order of the entangled tensor.","The experimental results from the multi-task T0-benchmark demonstrate that: 1) all modular LMs surpass the corresponding dense approaches, highlighting the potential of modular language models to mitigate negative inference in multi-task learning and deliver superior outcomes.","2) \\texttt{TensorPoly-I} achieves higher parameter efficiency in adaptation and outperforms other modular LMs, which shows the potential of our approach in multi-task transfer learning."],"url":"http://arxiv.org/abs/2405.16671v1","category":"cs.LG"}
{"created":"2024-05-26 18:49:59","title":"RLSF: Reinforcement Learning via Symbolic Feedback","abstract":"In recent years, large language models (LLMs) have had a dramatic impact on various sub-fields of AI, most notably on natural language understanding tasks. However, there is widespread agreement that the logical reasoning capabilities of contemporary LLMs are, at best, fragmentary (i.e., may work well on some problem instances but fail dramatically on others). While traditional LLM fine-tuning approaches (e.g., those that use human feedback) do address this problem to some degree, they suffer from many issues, including unsound black-box reward models, difficulties in collecting preference data, and sparse scalar reward values.   To address these challenges, we propose a new training/fine-tuning paradigm we refer to as Reinforcement Learning via Symbolic Feedback (RLSF), which is aimed at enhancing the reasoning capabilities of LLMs. In the RLSF setting, the LLM that is being trained/fine-tuned is considered as the RL agent, while the environment is allowed access to reasoning or domain knowledge tools (e.g., solvers, algebra systems). Crucially, in RLSF, these reasoning tools can provide feedback to the LLMs via poly-sized certificates (e.g., proofs), that characterize errors in the LLM-generated object with respect to some correctness specification. The ability of RLSF-based training/fine-tuning to leverage certificate-generating symbolic tools enables sound fine-grained (token-level) reward signals to LLMs, and thus addresses the limitations of traditional reward models mentioned above. Via extensive evaluations, we show that our RLSF-based fine-tuning of LLMs outperforms traditional approaches on two different applications, namely, program synthesis from natural language pseudo-code to programming language (C++) and solving the Game of 24.","sentences":["In recent years, large language models (LLMs) have had a dramatic impact on various sub-fields of AI, most notably on natural language understanding tasks.","However, there is widespread agreement that the logical reasoning capabilities of contemporary LLMs are, at best, fragmentary (i.e., may work well on some problem instances but fail dramatically on others).","While traditional LLM fine-tuning approaches (e.g., those that use human feedback) do address this problem to some degree, they suffer from many issues, including unsound black-box reward models, difficulties in collecting preference data, and sparse scalar reward values.   ","To address these challenges, we propose a new training/fine-tuning paradigm we refer to as Reinforcement Learning via Symbolic Feedback (RLSF), which is aimed at enhancing the reasoning capabilities of LLMs.","In the RLSF setting, the LLM that is being trained/fine-tuned is considered as the RL agent, while the environment is allowed access to reasoning or domain knowledge tools (e.g., solvers, algebra systems).","Crucially, in RLSF, these reasoning tools can provide feedback to the LLMs via poly-sized certificates (e.g., proofs), that characterize errors in the LLM-generated object with respect to some correctness specification.","The ability of RLSF-based training/fine-tuning to leverage certificate-generating symbolic tools enables sound fine-grained (token-level) reward signals to LLMs, and thus addresses the limitations of traditional reward models mentioned above.","Via extensive evaluations, we show that our RLSF-based fine-tuning of LLMs outperforms traditional approaches on two different applications, namely, program synthesis from natural language pseudo-code to programming language (C++) and solving the Game of 24."],"url":"http://arxiv.org/abs/2405.16661v1","category":"cs.CL"}
{"created":"2024-05-26 18:29:24","title":"Acceleration of Grokking in Learning Arithmetic Operations via Kolmogorov-Arnold Representation","abstract":"We propose novel methodologies aimed at accelerating the grokking phenomenon, which refers to the rapid increment of test accuracy after a long period of overfitting as reported in~\\cite{power2022grokking}. Focusing on the grokking phenomenon that arises in learning arithmetic binary operations via the transformer model, we begin with a discussion on data augmentation in the case of commutative binary operations. To further accelerate, we elucidate arithmetic operations through the lens of the Kolmogorov-Arnold (KA) representation theorem, revealing its correspondence to the transformer architecture: embedding, decoder block, and classifier. Observing the shared structure between KA representations associated with binary operations, we suggest various transfer learning mechanisms that expedite grokking. This interpretation is substantiated through a series of rigorous experiments. In addition, our approach is successful in learning two nonstandard arithmetic tasks: composition of operations and a system of equations. Furthermore, we reveal that the model is capable of learning arithmetic operations using a limited number of tokens under embedding transfer, which is supported by a set of experiments as well.","sentences":["We propose novel methodologies aimed at accelerating the grokking phenomenon, which refers to the rapid increment of test accuracy after a long period of overfitting as reported in~\\cite{power2022grokking}.","Focusing on the grokking phenomenon that arises in learning arithmetic binary operations via the transformer model, we begin with a discussion on data augmentation in the case of commutative binary operations.","To further accelerate, we elucidate arithmetic operations through the lens of the Kolmogorov-Arnold (KA) representation theorem, revealing its correspondence to the transformer architecture: embedding, decoder block, and classifier.","Observing the shared structure between KA representations associated with binary operations, we suggest various transfer learning mechanisms that expedite grokking.","This interpretation is substantiated through a series of rigorous experiments.","In addition, our approach is successful in learning two nonstandard arithmetic tasks: composition of operations and a system of equations.","Furthermore, we reveal that the model is capable of learning arithmetic operations using a limited number of tokens under embedding transfer, which is supported by a set of experiments as well."],"url":"http://arxiv.org/abs/2405.16658v1","category":"cs.LG"}
{"created":"2024-05-26 18:17:46","title":"Predicting Likely-Vulnerable Code Changes: Machine Learning-based Vulnerability Protections for Android Open Source Project","abstract":"This paper presents a framework that selectively triggers security reviews for incoming source code changes. Functioning as a review bot within a code review service, the framework can automatically request additional security reviews at pre-submit time before the code changes are submitted to a source code repository. Because performing such secure code reviews add cost, the framework employs a classifier trained to identify code changes with a high likelihood of vulnerabilities. The online classifier leverages various types of input features to analyze the review patterns, track the software engineering process, and mine specific text patterns within given code changes. The classifier and its features are meticulously chosen and optimized using data from the submitted code changes and reported vulnerabilities in Android Open Source Project (AOSP). The evaluation results demonstrate that our Vulnerability Prevention (VP) framework identifies approximately 80% of the vulnerability-inducing code changes in the dataset with a precision ratio of around 98% and a false positive rate of around 1.7%. We discuss the implications of deploying the VP framework in multi-project settings and future directions for Android security research. This paper explores and validates our approach to code change-granularity vulnerability prediction, offering a preventive technique for software security by preemptively detecting vulnerable code changes before submission.","sentences":["This paper presents a framework that selectively triggers security reviews for incoming source code changes.","Functioning as a review bot within a code review service, the framework can automatically request additional security reviews at pre-submit time before the code changes are submitted to a source code repository.","Because performing such secure code reviews add cost, the framework employs a classifier trained to identify code changes with a high likelihood of vulnerabilities.","The online classifier leverages various types of input features to analyze the review patterns, track the software engineering process, and mine specific text patterns within given code changes.","The classifier and its features are meticulously chosen and optimized using data from the submitted code changes and reported vulnerabilities in Android Open Source Project (AOSP).","The evaluation results demonstrate that our Vulnerability Prevention (VP) framework identifies approximately 80% of the vulnerability-inducing code changes in the dataset with a precision ratio of around 98% and a false positive rate of around 1.7%.","We discuss the implications of deploying the VP framework in multi-project settings and future directions for Android security research.","This paper explores and validates our approach to code change-granularity vulnerability prediction, offering a preventive technique for software security by preemptively detecting vulnerable code changes before submission."],"url":"http://arxiv.org/abs/2405.16655v1","category":"cs.CR"}
{"created":"2024-05-26 18:10:49","title":"A two-speed actuator for robotics with fast seamless gear shifting","abstract":"This paper present a novel dual-speed actuator adapted to robotics. In many applications, robots have to bear large loads while moving slowly and also have to move quickly through the air with almost no load. This lead to conflicting requirements for their actuators. Multiple gear ratios address this issue by allowing an effective use of power over a wide range of torque-speed load conditions. Furthermore, very different gear ratios also lead to drastic changes of the intrinsic impedance, enabling a non-back-drivable mode for stiff position control and a back-drivable mode for force control. The proposed actuator consists of two electric motors coupled to a differential; one has a large gear ratio while the other is almost direct-drive and equipped with a brake. During the high-force mode the brake is locked, only one motor is used, and the actuator behaves like a regular highly-geared servo-motor. During the high-speed mode the brake is open, both motor are used at the same time, and the actuator behaves like a direct drive motor. A dynamic model is developed and novel controllers are proposed for synergic use of both motors. The redundancy of motors is exploited for maintaining full control of the output during mode transitions, allowing for fast and seamless switching even when interacting with unknown environments. Results are demonstrated with a proof-of-concept linear actuator.","sentences":["This paper present a novel dual-speed actuator adapted to robotics.","In many applications, robots have to bear large loads while moving slowly and also have to move quickly through the air with almost no load.","This lead to conflicting requirements for their actuators.","Multiple gear ratios address this issue by allowing an effective use of power over a wide range of torque-speed load conditions.","Furthermore, very different gear ratios also lead to drastic changes of the intrinsic impedance, enabling a non-back-drivable mode for stiff position control and a back-drivable mode for force control.","The proposed actuator consists of two electric motors coupled to a differential; one has a large gear ratio while the other is almost direct-drive and equipped with a brake.","During the high-force mode the brake is locked, only one motor is used, and the actuator behaves like a regular highly-geared servo-motor.","During the high-speed mode the brake is open, both motor are used at the same time, and the actuator behaves like a direct drive motor.","A dynamic model is developed and novel controllers are proposed for synergic use of both motors.","The redundancy of motors is exploited for maintaining full control of the output during mode transitions, allowing for fast and seamless switching even when interacting with unknown environments.","Results are demonstrated with a proof-of-concept linear actuator."],"url":"http://arxiv.org/abs/2405.16652v1","category":"cs.RO"}
{"created":"2024-05-26 17:38:44","title":"Pick up the PACE: A Parameter-Free Optimizer for Lifelong Reinforcement Learning","abstract":"A key challenge in lifelong reinforcement learning (RL) is the loss of plasticity, where previous learning progress hinders an agent's adaptation to new tasks. While regularization and resetting can help, they require precise hyperparameter selection at the outset and environment-dependent adjustments. Building on the principled theory of online convex optimization, we present a parameter-free optimizer for lifelong RL, called PACE, which requires no tuning or prior knowledge about the distribution shifts. Extensive experiments on Procgen, Atari, and Gym Control environments show that PACE works surprisingly well$\\unicode{x2013}$mitigating loss of plasticity and rapidly adapting to challenging distribution shifts$\\unicode{x2013}$despite the underlying optimization problem being nonconvex and nonstationary.","sentences":["A key challenge in lifelong reinforcement learning (RL) is the loss of plasticity, where previous learning progress hinders an agent's adaptation to new tasks.","While regularization and resetting can help, they require precise hyperparameter selection at the outset and environment-dependent adjustments.","Building on the principled theory of online convex optimization, we present a parameter-free optimizer for lifelong RL, called PACE, which requires no tuning or prior knowledge about the distribution shifts.","Extensive experiments on Procgen, Atari, and Gym Control environments show that PACE works surprisingly well$\\unicode{x2013}$mitigating loss of plasticity and rapidly adapting to challenging distribution shifts$\\unicode{x2013}$despite the underlying optimization problem being nonconvex and nonstationary."],"url":"http://arxiv.org/abs/2405.16642v1","category":"cs.LG"}
{"created":"2024-05-26 17:31:21","title":"A Survey of Multimodal Large Language Model from A Data-centric Perspective","abstract":"Human beings perceive the world through diverse senses such as sight, smell, hearing, and touch. Similarly, multimodal large language models (MLLMs) enhance the capabilities of traditional large language models by integrating and processing data from multiple modalities including text, vision, audio, video, and 3D environments. Data plays a pivotal role in the development and refinement of these models. In this survey, we comprehensively review the literature on MLLMs from a data-centric perspective. Specifically, we explore methods for preparing multimodal data during the pretraining and adaptation phases of MLLMs. Additionally, we analyze the evaluation methods for datasets and review benchmarks for evaluating MLLMs. Our survey also outlines potential future research directions. This work aims to provide researchers with a detailed understanding of the data-driven aspects of MLLMs, fostering further exploration and innovation in this field.","sentences":["Human beings perceive the world through diverse senses such as sight, smell, hearing, and touch.","Similarly, multimodal large language models (MLLMs) enhance the capabilities of traditional large language models by integrating and processing data from multiple modalities including text, vision, audio, video, and 3D environments.","Data plays a pivotal role in the development and refinement of these models.","In this survey, we comprehensively review the literature on MLLMs from a data-centric perspective.","Specifically, we explore methods for preparing multimodal data during the pretraining and adaptation phases of MLLMs.","Additionally, we analyze the evaluation methods for datasets and review benchmarks for evaluating MLLMs.","Our survey also outlines potential future research directions.","This work aims to provide researchers with a detailed understanding of the data-driven aspects of MLLMs, fostering further exploration and innovation in this field."],"url":"http://arxiv.org/abs/2405.16640v1","category":"cs.AI"}
{"created":"2024-05-26 17:08:04","title":"Bayesian Inference with Deep Weakly Nonlinear Networks","abstract":"We show at a physics level of rigor that Bayesian inference with a fully connected neural network and a shaped nonlinearity of the form $\\phi(t) = t + \\psi t^3/L$ is (perturbatively) solvable in the regime where the number of training datapoints $P$ , the input dimension $N_0$, the network layer widths $N$, and the network depth $L$ are simultaneously large. Our results hold with weak assumptions on the data; the main constraint is that $P < N_0$. We provide techniques to compute the model evidence and posterior to arbitrary order in $1/N$ and at arbitrary temperature. We report the following results from the first-order computation:   1. When the width $N$ is much larger than the depth $L$ and training set size $P$, neural network Bayesian inference coincides with Bayesian inference using a kernel. The value of $\\psi$ determines the curvature of a sphere, hyperbola, or plane into which the training data is implicitly embedded under the feature map.   2. When $LP/N$ is a small constant, neural network Bayesian inference departs from the kernel regime. At zero temperature, neural network Bayesian inference is equivalent to Bayesian inference using a data-dependent kernel, and $LP/N$ serves as an effective depth that controls the extent of feature learning.   3. In the restricted case of deep linear networks ($\\psi=0$) and noisy data, we show a simple data model for which evidence and generalization error are optimal at zero temperature. As $LP/N$ increases, both evidence and generalization further improve, demonstrating the benefit of depth in benign overfitting.","sentences":["We show at a physics level of rigor that Bayesian inference with a fully connected neural network and a shaped nonlinearity of the form $\\phi(t) = t + \\psi t^3/L$ is (perturbatively) solvable in the regime where the number of training datapoints $P$ , the input dimension $N_0$, the network layer widths $N$, and the network depth $L$ are simultaneously large.","Our results hold with weak assumptions on the data; the main constraint is that $P <","N_0$. We provide techniques to compute the model evidence and posterior to arbitrary order in $1/N$ and at arbitrary temperature.","We report the following results from the first-order computation:   1.","When the width $N$ is much larger than the depth $L$ and training set size $P$, neural network Bayesian inference coincides with Bayesian inference using a kernel.","The value of $\\psi$ determines the curvature of a sphere, hyperbola, or plane into which the training data is implicitly embedded under the feature map.   ","2.","When $LP/N$ is a small constant, neural network Bayesian inference departs from the kernel regime.","At zero temperature, neural network Bayesian inference is equivalent to Bayesian inference using a data-dependent kernel, and $LP/N$ serves as an effective depth that controls the extent of feature learning.   ","3.","In the restricted case of deep linear networks ($\\psi=0$) and noisy data, we show a simple data model for which evidence and generalization error are optimal at zero temperature.","As $LP/N$ increases, both evidence and generalization further improve, demonstrating the benefit of depth in benign overfitting."],"url":"http://arxiv.org/abs/2405.16630v1","category":"stat.ML"}
{"created":"2024-05-26 16:39:19","title":"Graph neural networks with configuration cross-attention for tensor compilers","abstract":"With the recent popularity of neural networks comes the need for efficient serving of inference workloads. A neural network inference workload can be represented as a computational graph with nodes as operators transforming multidimensional tensors. The tensors can be transposed and/or tiled in a combinatorially large number of ways, some configurations leading to accelerated inference. We propose TGraph, a neural graph architecture that allows screening for fast configurations of the target computational graph, thus representing an artificial intelligence (AI) tensor compiler in contrast to the traditional heuristics-based compilers. The proposed solution improves mean Kendall's $\\tau$ across layout collections of TpuGraphs from 29.8% of the reliable baseline to 67.4% of TGraph. We estimate the potential CO$_2$ emission reduction associated with our work to be equivalent to over 50% of the total household emissions in the areas hosting AI-oriented data centers.","sentences":["With the recent popularity of neural networks comes the need for efficient serving of inference workloads.","A neural network inference workload can be represented as a computational graph with nodes as operators transforming multidimensional tensors.","The tensors can be transposed and/or tiled in a combinatorially large number of ways, some configurations leading to accelerated inference.","We propose TGraph, a neural graph architecture that allows screening for fast configurations of the target computational graph, thus representing an artificial intelligence (AI) tensor compiler in contrast to the traditional heuristics-based compilers.","The proposed solution improves mean Kendall's $\\tau$ across layout collections of TpuGraphs from 29.8% of the reliable baseline to 67.4% of TGraph.","We estimate the potential CO$_2$ emission reduction associated with our work to be equivalent to over 50% of the total household emissions in the areas hosting AI-oriented data centers."],"url":"http://arxiv.org/abs/2405.16623v1","category":"cs.LG"}
{"created":"2024-05-26 16:34:52","title":"Mimicry and the Emergence of Cooperative Communication","abstract":"In many situations, communication between agents is a critical component of cooperative multi-agent systems, however, it can be difficult to learn or evolve. In this paper, we investigate a simple way in which the emergence of communication may be facilitated. Namely, we explore the effects of when agents can mimic preexisting, externally generated useful signals. The key idea here is that these signals incentivise listeners to develop positive responses, that can then also be invoked by speakers mimicking those signals. This investigation starts with formalising this problem, and demonstrating that this form of mimicry changes optimisation dynamics and may provide the opportunity to escape non-communicative local optima. We then explore the problem empirically with a simulation in which spatially situated agents must communicate to collect resources. Our results show that both evolutionary optimisation and reinforcement learning may benefit from this intervention.","sentences":["In many situations, communication between agents is a critical component of cooperative multi-agent systems, however, it can be difficult to learn or evolve.","In this paper, we investigate a simple way in which the emergence of communication may be facilitated.","Namely, we explore the effects of when agents can mimic preexisting, externally generated useful signals.","The key idea here is that these signals incentivise listeners to develop positive responses, that can then also be invoked by speakers mimicking those signals.","This investigation starts with formalising this problem, and demonstrating that this form of mimicry changes optimisation dynamics and may provide the opportunity to escape non-communicative local optima.","We then explore the problem empirically with a simulation in which spatially situated agents must communicate to collect resources.","Our results show that both evolutionary optimisation and reinforcement learning may benefit from this intervention."],"url":"http://arxiv.org/abs/2405.16622v1","category":"cs.MA"}
{"created":"2024-05-26 16:30:43","title":"Error Performance Analysis of UAV-Mounted RIS for NOMA Systems with Practical Constraints","abstract":"Uncrewed aerial vehicles (UAVs) have attracted recent attention for sixth-generation (6G) networks due to their low cost and flexible deployment. In order to maximize the ever-increasing data rates, spectral efficiency, and wider coverage, technologies such as reconfigurable intelligent surface (RIS) and non-orthogonal multiple access (NOMA) are adapted with UAVs (UAV-RIS NOMA). However, the error performance of UAV-RIS NOMA has not been considered, yet. In this letter, we investigate the error probability of UAV-RIS NOMA systems. We also consider the practical constraints of hardware impairments (HWI) at the transceivers, inter-cell interference (ICI), and imperfect successive interference cancellation (SIC). The analytical derivations are validated by Monte-Carlo simulations. Our results demonstrate that our proposed system achieves higher performance gain (more than 5 dB with increasing the number of RIS elements) with less error probability compared to UAVs without RIS. Moreover, it is found that the HWI, ICI, and imperfect SIC have shown a negative impact on the system performance.","sentences":["Uncrewed aerial vehicles (UAVs) have attracted recent attention for sixth-generation (6G) networks due to their low cost and flexible deployment.","In order to maximize the ever-increasing data rates, spectral efficiency, and wider coverage, technologies such as reconfigurable intelligent surface (RIS) and non-orthogonal multiple access (NOMA) are adapted with UAVs (UAV-RIS NOMA).","However, the error performance of UAV-RIS NOMA has not been considered, yet.","In this letter, we investigate the error probability of UAV-RIS NOMA systems.","We also consider the practical constraints of hardware impairments (HWI) at the transceivers, inter-cell interference (ICI), and imperfect successive interference cancellation (SIC).","The analytical derivations are validated by Monte-Carlo simulations.","Our results demonstrate that our proposed system achieves higher performance gain (more than 5 dB with increasing the number of RIS elements) with less error probability compared to UAVs without RIS.","Moreover, it is found that the HWI, ICI, and imperfect SIC have shown a negative impact on the system performance."],"url":"http://arxiv.org/abs/2405.16620v1","category":"eess.SP"}
{"created":"2024-05-26 15:44:53","title":"The devil is in discretization discrepancy. Robustifying Differentiable NAS with Single-Stage Searching Protocol","abstract":"Neural Architecture Search (NAS) has been widely adopted to design neural networks for various computer vision tasks. One of its most promising subdomains is differentiable NAS (DNAS), where the optimal architecture is found in a differentiable manner. However, gradient-based methods suffer from the discretization error, which can severely damage the process of obtaining the final architecture. In our work, we first study the risk of discretization error and show how it affects an unregularized supernet. Then, we present that penalizing high entropy, a common technique of architecture regularization, can hinder the supernet's performance. Therefore, to robustify the DNAS framework, we introduce a novel single-stage searching protocol, which is not reliant on decoding a continuous architecture. Our results demonstrate that this approach outperforms other DNAS methods by achieving 75.3% in the searching stage on the Cityscapes validation dataset and attains performance 1.1% higher than the optimal network of DCNAS on the non-dense search space comprising short connections. The entire training process takes only 5.5 GPU days due to the weight reuse, and yields a computationally efficient architecture. Additionally, we propose a new dataset split procedure, which substantially improves results and prevents architecture degeneration in DARTS.","sentences":["Neural Architecture Search (NAS) has been widely adopted to design neural networks for various computer vision tasks.","One of its most promising subdomains is differentiable NAS (DNAS), where the optimal architecture is found in a differentiable manner.","However, gradient-based methods suffer from the discretization error, which can severely damage the process of obtaining the final architecture.","In our work, we first study the risk of discretization error and show how it affects an unregularized supernet.","Then, we present that penalizing high entropy, a common technique of architecture regularization, can hinder the supernet's performance.","Therefore, to robustify the DNAS framework, we introduce a novel single-stage searching protocol, which is not reliant on decoding a continuous architecture.","Our results demonstrate that this approach outperforms other DNAS methods by achieving 75.3% in the searching stage on the Cityscapes validation dataset and attains performance 1.1% higher than the optimal network of DCNAS on the non-dense search space comprising short connections.","The entire training process takes only 5.5 GPU days due to the weight reuse, and yields a computationally efficient architecture.","Additionally, we propose a new dataset split procedure, which substantially improves results and prevents architecture degeneration in DARTS."],"url":"http://arxiv.org/abs/2405.16610v1","category":"cs.CV"}
{"created":"2024-05-26 15:30:34","title":"Intelligence as Computation","abstract":"This paper proposes a specific conceptualization of intelligence as computation. This conceptualization is intended to provide a unified view for all disciplines of intelligence research. Already, it unifies several conceptualizations currently under investigation, including physical, neural, embodied, morphological, and mechanical intelligences. To achieve this, the proposed conceptualization explains the differences among existing views by different computational paradigms, such as digital, analog, mechanical, or morphological computation. Viewing intelligence as a composition of computations from different paradigms, the challenges posed by previous conceptualizations are resolved. Intelligence is hypothesized as a multi-paradigmatic computation relying on specific computational principles. These principles distinguish intelligence from other, non-intelligent computations. The proposed conceptualization implies a multi-disciplinary research agenda that is intended to lead to unified science of intelligence.","sentences":["This paper proposes a specific conceptualization of intelligence as computation.","This conceptualization is intended to provide a unified view for all disciplines of intelligence research.","Already, it unifies several conceptualizations currently under investigation, including physical, neural, embodied, morphological, and mechanical intelligences.","To achieve this, the proposed conceptualization explains the differences among existing views by different computational paradigms, such as digital, analog, mechanical, or morphological computation.","Viewing intelligence as a composition of computations from different paradigms, the challenges posed by previous conceptualizations are resolved.","Intelligence is hypothesized as a multi-paradigmatic computation relying on specific computational principles.","These principles distinguish intelligence from other, non-intelligent computations.","The proposed conceptualization implies a multi-disciplinary research agenda that is intended to lead to unified science of intelligence."],"url":"http://arxiv.org/abs/2405.16604v1","category":"cs.AI"}
{"created":"2024-05-26 15:25:26","title":"Image-Text-Image Knowledge Transferring for Lifelong Person Re-Identification with Hybrid Clothing States","abstract":"With the continuous expansion of intelligent surveillance networks, lifelong person re-identification (LReID) has received widespread attention, pursuing the need of self-evolution across different domains. However, existing LReID studies accumulate knowledge with the assumption that people would not change their clothes. In this paper, we propose a more practical task, namely lifelong person re-identification with hybrid clothing states (LReID-Hybrid), which takes a series of cloth-changing and cloth-consistent domains into account during lifelong learning. To tackle the challenges of knowledge granularity mismatch and knowledge presentation mismatch that occurred in LReID-Hybrid, we take advantage of the consistency and generalization of the text space, and propose a novel framework, dubbed $Teata$, to effectively align, transfer and accumulate knowledge in an \"image-text-image\" closed loop. Concretely, to achieve effective knowledge transfer, we design a Structured Semantic Prompt (SSP) learning to decompose the text prompt into several structured pairs to distill knowledge from the image space with a unified granularity of text description. Then, we introduce a Knowledge Adaptation and Projection strategy (KAP), which tunes text knowledge via a slow-paced learner to adapt to different tasks without catastrophic forgetting. Extensive experiments demonstrate the superiority of our proposed $Teata$ for LReID-Hybrid as well as on conventional LReID benchmarks over advanced methods.","sentences":["With the continuous expansion of intelligent surveillance networks, lifelong person re-identification (LReID) has received widespread attention, pursuing the need of self-evolution across different domains.","However, existing LReID studies accumulate knowledge with the assumption that people would not change their clothes.","In this paper, we propose a more practical task, namely lifelong person re-identification with hybrid clothing states (LReID-Hybrid), which takes a series of cloth-changing and cloth-consistent domains into account during lifelong learning.","To tackle the challenges of knowledge granularity mismatch and knowledge presentation mismatch that occurred in LReID-Hybrid, we take advantage of the consistency and generalization of the text space, and propose a novel framework, dubbed $Teata$, to effectively align, transfer and accumulate knowledge in an \"image-text-image\" closed loop.","Concretely, to achieve effective knowledge transfer, we design a Structured Semantic Prompt (SSP) learning to decompose the text prompt into several structured pairs to distill knowledge from the image space with a unified granularity of text description.","Then, we introduce a Knowledge Adaptation and Projection strategy (KAP), which tunes text knowledge via a slow-paced learner to adapt to different tasks without catastrophic forgetting.","Extensive experiments demonstrate the superiority of our proposed $Teata$ for LReID-Hybrid as well as on conventional LReID benchmarks over advanced methods."],"url":"http://arxiv.org/abs/2405.16600v1","category":"cs.CV"}
{"created":"2024-05-26 15:11:45","title":"An Evolutionary Framework for Connect-4 as Test-Bed for Comparison of Advanced Minimax, Q-Learning and MCTS","abstract":"A major challenge in decision making domains with large state spaces is to effectively select actions which maximize utility. In recent years, approaches such as reinforcement learning (RL) and search algorithms have been successful to tackle this issue, despite their differences. RL defines a learning framework that an agent explores and interacts with. Search algorithms provide a formalism to search for a solution. However, it is often difficult to evaluate the performances of such approaches in a practical way. Motivated by this problem, we focus on one game domain, i.e., Connect-4, and develop a novel evolutionary framework to evaluate three classes of algorithms: RL, Minimax and Monte Carlo tree search (MCTS). The contribution of this paper is threefold: i) we implement advanced versions of these algorithms and provide a systematic comparison with their standard counterpart, ii) we develop a novel evaluation framework, which we call the Evolutionary Tournament, and iii) we conduct an extensive evaluation of the relative performance of each algorithm to compare our findings. We evaluate different metrics and show that MCTS achieves the best results in terms of win percentage, whereas Minimax and Q-Learning are ranked in second and third place, respectively, although the latter is shown to be the fastest to make a decision.","sentences":["A major challenge in decision making domains with large state spaces is to effectively select actions which maximize utility.","In recent years, approaches such as reinforcement learning (RL) and search algorithms have been successful to tackle this issue, despite their differences.","RL defines a learning framework that an agent explores and interacts with.","Search algorithms provide a formalism to search for a solution.","However, it is often difficult to evaluate the performances of such approaches in a practical way.","Motivated by this problem, we focus on one game domain, i.e., Connect-4, and develop a novel evolutionary framework to evaluate three classes of algorithms: RL, Minimax and Monte Carlo tree search (MCTS).","The contribution of this paper is threefold: i) we implement advanced versions of these algorithms and provide a systematic comparison with their standard counterpart, ii) we develop a novel evaluation framework, which we call the Evolutionary Tournament, and iii) we conduct an extensive evaluation of the relative performance of each algorithm to compare our findings.","We evaluate different metrics and show that MCTS achieves the best results in terms of win percentage, whereas Minimax and Q-Learning are ranked in second and third place, respectively, although the latter is shown to be the fastest to make a decision."],"url":"http://arxiv.org/abs/2405.16595v1","category":"cs.AI"}
{"created":"2024-05-26 14:42:49","title":"Attaining Human`s Desirable Outcomes in Human-AI Interaction via Structural Causal Games","abstract":"In human-AI interaction, a prominent goal is to attain human`s desirable outcome with the assistance of AI agents, which can be ideally delineated as a problem of seeking the optimal Nash Equilibrium that matches the human`s desirable outcome. However, reaching the outcome is usually challenging due to the existence of multiple Nash Equilibria that are related to the assisting task but do not correspond to the human`s desirable outcome. To tackle this issue, we employ a theoretical framework called structural causal game (SCG) to formalize the human-AI interactive process. Furthermore, we introduce a strategy referred to as pre-policy intervention on the SCG to steer AI agents towards attaining the human`s desirable outcome. In more detail, a pre-policy is learned as a generalized intervention to guide the agents` policy selection, under a transparent and interpretable procedure determined by the SCG. To make the framework practical, we propose a reinforcement learning-like algorithm to search out this pre-policy. The proposed algorithm is tested in both gridworld environments and realistic dialogue scenarios with large language models, demonstrating its adaptability in a broader class of problems and potential effectiveness in real-world situations.","sentences":["In human-AI interaction, a prominent goal is to attain human`s desirable outcome with the assistance of AI agents, which can be ideally delineated as a problem of seeking the optimal Nash Equilibrium that matches the human`s desirable outcome.","However, reaching the outcome is usually challenging due to the existence of multiple Nash Equilibria that are related to the assisting task but do not correspond to the human`s desirable outcome.","To tackle this issue, we employ a theoretical framework called structural causal game (SCG) to formalize the human-AI interactive process.","Furthermore, we introduce a strategy referred to as pre-policy intervention on the SCG to steer AI agents towards attaining the human`s desirable outcome.","In more detail, a pre-policy is learned as a generalized intervention to guide the agents` policy selection, under a transparent and interpretable procedure determined by the SCG.","To make the framework practical, we propose a reinforcement learning-like algorithm to search out this pre-policy.","The proposed algorithm is tested in both gridworld environments and realistic dialogue scenarios with large language models, demonstrating its adaptability in a broader class of problems and potential effectiveness in real-world situations."],"url":"http://arxiv.org/abs/2405.16588v1","category":"cs.AI"}
{"created":"2024-05-26 14:38:24","title":"Cost-Effective Online Multi-LLM Selection with Versatile Reward Models","abstract":"With the rapid advancement of large language models (LLMs), the diversity of multi-LLM tasks and the variability in their pricing structures have become increasingly important, as costs can vary greatly between different LLMs. To tackle these challenges, we introduce the \\textit{C2MAB-V}, a \\underline{C}ost-effective \\underline{C}ombinatorial \\underline{M}ulti-armed \\underline{B}andit with \\underline{V}ersatile reward models for optimal LLM selection and usage. This online model differs from traditional static approaches or those reliant on a single LLM without cost consideration. With multiple LLMs deployed on a scheduling cloud and a local server dedicated to handling user queries, \\textit{C2MAB-V} facilitates the selection of multiple LLMs over a combinatorial search space, specifically tailored for various collaborative task types with different reward models. Based on our designed online feedback mechanism and confidence bound technique, \\textit{C2MAB-V} can effectively address the multi-LLM selection challenge by managing the exploration-exploitation trade-off across different models, while also balancing cost and reward for diverse tasks. The NP-hard integer linear programming problem for selecting multiple LLMs with trade-off dilemmas is addressed by: i) decomposing the integer problem into a relaxed form by the local server, ii) utilizing a discretization rounding scheme that provides optimal LLM combinations by the scheduling cloud, and iii) continual online updates based on feedback. Theoretically, we prove that \\textit{C2MAB-V} offers strict guarantees over versatile reward models, matching state-of-the-art results for regret and violations in some degenerate cases. Empirically, we show that \\textit{C2MAB-V} effectively balances performance and cost-efficiency with nine LLMs for three application scenarios.","sentences":["With the rapid advancement of large language models (LLMs), the diversity of multi-LLM tasks and the variability in their pricing structures have become increasingly important, as costs can vary greatly between different LLMs.","To tackle these challenges, we introduce the \\textit{C2MAB-V}, a \\underline{C}ost-effective \\underline{C}ombinatorial \\underline{M}ulti-armed \\underline{B}andit with \\underline{V}ersatile reward models for optimal LLM selection and usage.","This online model differs from traditional static approaches or those reliant on a single LLM without cost consideration.","With multiple LLMs deployed on a scheduling cloud and a local server dedicated to handling user queries, \\textit{C2MAB-V} facilitates the selection of multiple LLMs over a combinatorial search space, specifically tailored for various collaborative task types with different reward models.","Based on our designed online feedback mechanism and confidence bound technique, \\textit{C2MAB-V} can effectively address the multi-LLM selection challenge by managing the exploration-exploitation trade-off across different models, while also balancing cost and reward for diverse tasks.","The NP-hard integer linear programming problem for selecting multiple LLMs with trade-off dilemmas is addressed by: i) decomposing the integer problem into a relaxed form by the local server, ii) utilizing a discretization rounding scheme that provides optimal LLM combinations by the scheduling cloud, and iii) continual online updates based on feedback.","Theoretically, we prove that \\textit{C2MAB-V} offers strict guarantees over versatile reward models, matching state-of-the-art results for regret and violations in some degenerate cases.","Empirically, we show that \\textit{C2MAB-V} effectively balances performance and cost-efficiency with nine LLMs for three application scenarios."],"url":"http://arxiv.org/abs/2405.16587v1","category":"cs.LG"}
{"created":"2024-05-26 14:29:10","title":"Fair Federated Learning under Domain Skew with Local Consistency and Domain Diversity","abstract":"Federated learning (FL) has emerged as a new paradigm for privacy-preserving collaborative training. Under domain skew, the current FL approaches are biased and face two fairness problems. 1) Parameter Update Conflict: data disparity among clients leads to varying parameter importance and inconsistent update directions. These two disparities cause important parameters to potentially be overwhelmed by unimportant ones of dominant updates. It consequently results in significant performance decreases for lower-performing clients. 2) Model Aggregation Bias: existing FL approaches introduce unfair weight allocation and neglect domain diversity. It leads to biased model convergence objective and distinct performance among domains. We discover a pronounced directional update consistency in Federated Learning and propose a novel framework to tackle above issues. First, leveraging the discovered characteristic, we selectively discard unimportant parameter updates to prevent updates from clients with lower performance overwhelmed by unimportant parameters, resulting in fairer generalization performance. Second, we propose a fair aggregation objective to prevent global model bias towards some domains, ensuring that the global model continuously aligns with an unbiased model. The proposed method is generic and can be combined with other existing FL methods to enhance fairness. Comprehensive experiments on Digits and Office-Caltech demonstrate the high fairness and performance of our method.","sentences":["Federated learning (FL) has emerged as a new paradigm for privacy-preserving collaborative training.","Under domain skew, the current FL approaches are biased and face two fairness problems.","1) Parameter Update Conflict: data disparity among clients leads to varying parameter importance and inconsistent update directions.","These two disparities cause important parameters to potentially be overwhelmed by unimportant ones of dominant updates.","It consequently results in significant performance decreases for lower-performing clients.","2) Model Aggregation Bias: existing FL approaches introduce unfair weight allocation and neglect domain diversity.","It leads to biased model convergence objective and distinct performance among domains.","We discover a pronounced directional update consistency in Federated Learning and propose a novel framework to tackle above issues.","First, leveraging the discovered characteristic, we selectively discard unimportant parameter updates to prevent updates from clients with lower performance overwhelmed by unimportant parameters, resulting in fairer generalization performance.","Second, we propose a fair aggregation objective to prevent global model bias towards some domains, ensuring that the global model continuously aligns with an unbiased model.","The proposed method is generic and can be combined with other existing FL methods to enhance fairness.","Comprehensive experiments on Digits and Office-Caltech demonstrate the high fairness and performance of our method."],"url":"http://arxiv.org/abs/2405.16585v1","category":"cs.LG"}
{"created":"2024-05-26 13:36:45","title":"ID-to-3D: Expressive ID-guided 3D Heads via Score Distillation Sampling","abstract":"We propose ID-to-3D, a method to generate identity- and text-guided 3D human heads with disentangled expressions, starting from even a single casually captured in-the-wild image of a subject. The foundation of our approach is anchored in compositionality, alongside the use of task-specific 2D diffusion models as priors for optimization. First, we extend a foundational model with a lightweight expression-aware and ID-aware architecture, and create 2D priors for geometry and texture generation, via fine-tuning only 0.2% of its available training parameters. Then, we jointly leverage a neural parametric representation for the expressions of each subject and a multi-stage generation of highly detailed geometry and albedo texture. This combination of strong face identity embeddings and our neural representation enables accurate reconstruction of not only facial features but also accessories and hair and can be meshed to provide render-ready assets for gaming and telepresence. Our results achieve an unprecedented level of identity-consistent and high-quality texture and geometry generation, generalizing to a ``world'' of unseen 3D identities, without relying on large 3D captured datasets of human assets. Explore our 3D results at: https://https://idto3d.github.io.","sentences":["We propose ID-to-3D, a method to generate identity- and text-guided 3D human heads with disentangled expressions, starting from even a single casually captured in-the-wild image of a subject.","The foundation of our approach is anchored in compositionality, alongside the use of task-specific 2D diffusion models as priors for optimization.","First, we extend a foundational model with a lightweight expression-aware and ID-aware architecture, and create 2D priors for geometry and texture generation, via fine-tuning only 0.2% of its available training parameters.","Then, we jointly leverage a neural parametric representation for the expressions of each subject and a multi-stage generation of highly detailed geometry and albedo texture.","This combination of strong face identity embeddings and our neural representation enables accurate reconstruction of not only facial features but also accessories and hair and can be meshed to provide render-ready assets for gaming and telepresence.","Our results achieve an unprecedented level of identity-consistent and high-quality texture and geometry generation, generalizing to a ``world'' of unseen 3D identities, without relying on large 3D captured datasets of human assets.","Explore our 3D results at: https://https://idto3d.github.io."],"url":"http://arxiv.org/abs/2405.16570v1","category":"cs.CV"}
{"created":"2024-05-26 13:32:24","title":"Automatic Jailbreaking of the Text-to-Image Generative AI Systems","abstract":"Recent AI systems have shown extremely powerful performance, even surpassing human performance, on various tasks such as information retrieval, language generation, and image generation based on large language models (LLMs). At the same time, there are diverse safety risks that can cause the generation of malicious contents by circumventing the alignment in LLMs, which are often referred to as jailbreaking. However, most of the previous works only focused on the text-based jailbreaking in LLMs, and the jailbreaking of the text-to-image (T2I) generation system has been relatively overlooked. In this paper, we first evaluate the safety of the commercial T2I generation systems, such as ChatGPT, Copilot, and Gemini, on copyright infringement with naive prompts. From this empirical study, we find that Copilot and Gemini block only 12\\% and 17\\% of the attacks with naive prompts, respectively, while ChatGPT blocks 84\\% of them. Then, we further propose a stronger automated jailbreaking pipeline for T2I generation systems, which produces prompts that bypass their safety guards. Our automated jailbreaking framework leverages an LLM optimizer to generate prompts to maximize degree of violation from the generated images without any weight updates or gradient computation. Surprisingly, our simple yet effective approach successfully jailbreaks the ChatGPT with 11.0\\% block rate, making it generate copyrighted contents in 76\\% of the time. Finally, we explore various defense strategies, such as post-generation filtering and machine unlearning techniques, but found that they were inadequate, which suggests the necessity of stronger defense mechanisms.","sentences":["Recent AI systems have shown extremely powerful performance, even surpassing human performance, on various tasks such as information retrieval, language generation, and image generation based on large language models (LLMs).","At the same time, there are diverse safety risks that can cause the generation of malicious contents by circumventing the alignment in LLMs, which are often referred to as jailbreaking.","However, most of the previous works only focused on the text-based jailbreaking in LLMs, and the jailbreaking of the text-to-image (T2I) generation system has been relatively overlooked.","In this paper, we first evaluate the safety of the commercial T2I generation systems, such as ChatGPT, Copilot, and Gemini, on copyright infringement with naive prompts.","From this empirical study, we find that Copilot and Gemini block only 12\\% and 17\\% of the attacks with naive prompts, respectively, while ChatGPT blocks 84\\% of them.","Then, we further propose a stronger automated jailbreaking pipeline for T2I generation systems, which produces prompts that bypass their safety guards.","Our automated jailbreaking framework leverages an LLM optimizer to generate prompts to maximize degree of violation from the generated images without any weight updates or gradient computation.","Surprisingly, our simple yet effective approach successfully jailbreaks the ChatGPT with 11.0\\% block rate, making it generate copyrighted contents in 76\\% of the time.","Finally, we explore various defense strategies, such as post-generation filtering and machine unlearning techniques, but found that they were inadequate, which suggests the necessity of stronger defense mechanisms."],"url":"http://arxiv.org/abs/2405.16567v1","category":"cs.AI"}
{"created":"2024-05-26 13:31:23","title":"Study of WH production through vector boson scattering and extraction of the relative sign of the W and Z couplings to the Higgs boson in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search for the production of a W boson and a Higgs boson through vector boson scattering (VBS) is presented, using CMS data from proton-proton collisions at $\\sqrt{s}$ = 13 TeV collected from 2016 to 2018. The integrated luminosity of the data sample is 138 fb$^{-1}$. Selected events must be consistent with the presence of two jets originating from VBS, the leptonic decay of the W boson to an electron or muon, and a Higgs boson decaying into a pair of b quarks, reconstructed as either a single merged jet or two resolved jets. A measurement of the process as predicted by the standard model (SM) is performed alongside a study of beyond-the-SM (BSM) scenarios. The SM analysis sets an observed (expected) 95% confidence level upper limit of 14.3 (9.0) on the ratio of the measured VBS WH cross section to that expected by the SM. The BSM analysis, conducted within the so-called $\\kappa$ framework, excludes all scenarios with $\\lambda_\\mathrm{WZ}$ $\\lt$ 0 that are consistent with current measurements, where $\\lambda_\\mathrm{WZ}$ = $\\kappa_\\mathrm{W}/\\kappa_\\mathrm{Z}$ and $\\kappa_\\mathrm{W}$ and $\\kappa_\\mathrm{Z}$ are the HWW and HZZ coupling modifiers, respectively. The signficance of the exclusion is beyond 5 standard deviations, and it is consistent with the SM expectation of $\\lambda_\\mathrm{WZ}$ = 1.","sentences":["A search for the production of a W boson and a Higgs boson through vector boson scattering (VBS) is presented, using CMS data from proton-proton collisions at $\\sqrt{s}$ = 13 TeV collected from 2016 to 2018.","The integrated luminosity of the data sample is 138 fb$^{-1}$. Selected events must be consistent with the presence of two jets originating from VBS, the leptonic decay of the W boson to an electron or muon, and a Higgs boson decaying into a pair of b quarks, reconstructed as either a single merged jet or two resolved jets.","A measurement of the process as predicted by the standard model (SM) is performed alongside a study of beyond-the-SM (BSM) scenarios.","The SM analysis sets an observed (expected) 95% confidence level upper limit of 14.3 (9.0) on the ratio of the measured VBS WH cross section to that expected by the SM.","The BSM analysis, conducted within the so-called $\\kappa$ framework, excludes all scenarios with $\\lambda_\\mathrm{WZ}$ $\\lt$ 0 that are consistent with current measurements, where $\\lambda_\\mathrm{WZ}$ = $\\kappa_\\mathrm{W}/\\kappa_\\mathrm{Z}$ and $\\kappa_\\mathrm{W}$ and $\\kappa_\\mathrm{Z}$ are the HWW and HZZ coupling modifiers, respectively.","The signficance of the exclusion is beyond 5 standard deviations, and it is consistent with the SM expectation of $\\lambda_\\mathrm{WZ}$ = 1."],"url":"http://arxiv.org/abs/2405.16566v1","category":"hep-ex"}
{"created":"2024-05-26 13:11:55","title":"Task Groupings Regularization: Data-Free Meta-Learning with Heterogeneous Pre-trained Models","abstract":"Data-Free Meta-Learning (DFML) aims to derive knowledge from a collection of pre-trained models without accessing their original data, enabling the rapid adaptation to new unseen tasks. Current methods often overlook the heterogeneity among pre-trained models, which leads to performance degradation due to task conflicts. In this paper, we empirically and theoretically identify and analyze the model heterogeneity in DFML. We find that model heterogeneity introduces a heterogeneity-homogeneity trade-off, where homogeneous models reduce task conflicts but also increase the overfitting risk. Balancing this trade-off is crucial for learning shared representations across tasks. Based on our findings, we propose Task Groupings Regularization, a novel approach that benefits from model heterogeneity by grouping and aligning conflicting tasks. Specifically, we embed pre-trained models into a task space to compute dissimilarity, and group heterogeneous models together based on this measure. Then, we introduce implicit gradient regularization within each group to mitigate potential conflicts. By encouraging a gradient direction suitable for all tasks, the meta-model captures shared representations that generalize across tasks. Comprehensive experiments showcase the superiority of our approach in multiple benchmarks, effectively tackling the model heterogeneity in challenging multi-domain and multi-architecture scenarios.","sentences":["Data-Free Meta-Learning (DFML) aims to derive knowledge from a collection of pre-trained models without accessing their original data, enabling the rapid adaptation to new unseen tasks.","Current methods often overlook the heterogeneity among pre-trained models, which leads to performance degradation due to task conflicts.","In this paper, we empirically and theoretically identify and analyze the model heterogeneity in DFML.","We find that model heterogeneity introduces a heterogeneity-homogeneity trade-off, where homogeneous models reduce task conflicts but also increase the overfitting risk.","Balancing this trade-off is crucial for learning shared representations across tasks.","Based on our findings, we propose Task Groupings Regularization, a novel approach that benefits from model heterogeneity by grouping and aligning conflicting tasks.","Specifically, we embed pre-trained models into a task space to compute dissimilarity, and group heterogeneous models together based on this measure.","Then, we introduce implicit gradient regularization within each group to mitigate potential conflicts.","By encouraging a gradient direction suitable for all tasks, the meta-model captures shared representations that generalize across tasks.","Comprehensive experiments showcase the superiority of our approach in multiple benchmarks, effectively tackling the model heterogeneity in challenging multi-domain and multi-architecture scenarios."],"url":"http://arxiv.org/abs/2405.16560v1","category":"cs.LG"}
{"created":"2024-05-26 13:06:45","title":"Scalable Numerical Embeddings for Multivariate Time Series: Enhancing Healthcare Data Representation Learning","abstract":"Multivariate time series (MTS) data, when sampled irregularly and asynchronously, often present extensive missing values. Conventional methodologies for MTS analysis tend to rely on temporal embeddings based on timestamps that necessitate subsequent imputations, yet these imputed values frequently deviate substantially from their actual counterparts, thereby compromising prediction accuracy. Furthermore, these methods typically fail to provide robust initial embeddings for values infrequently observed or even absent within the training set, posing significant challenges to model generalizability. In response to these challenges, we propose SCAlable Numerical Embedding (SCANE), a novel framework that treats each feature value as an independent token, effectively bypassing the need for imputation. SCANE regularizes the traits of distinct feature embeddings and enhances representational learning through a scalable embedding mechanism. Coupling SCANE with the Transformer Encoder architecture, we develop the Scalable nUMerical eMbeddIng Transformer (SUMMIT), which is engineered to deliver precise predictive outputs for MTS characterized by prevalent missing entries. Our experimental validation, conducted across three disparate electronic health record (EHR) datasets marked by elevated missing value frequencies, confirms the superior performance of SUMMIT over contemporary state-of-the-art approaches addressing similar challenges. These results substantiate the efficacy of SCANE and SUMMIT, underscoring their potential applicability across a broad spectrum of MTS data analytical tasks.","sentences":["Multivariate time series (MTS) data, when sampled irregularly and asynchronously, often present extensive missing values.","Conventional methodologies for MTS analysis tend to rely on temporal embeddings based on timestamps that necessitate subsequent imputations, yet these imputed values frequently deviate substantially from their actual counterparts, thereby compromising prediction accuracy.","Furthermore, these methods typically fail to provide robust initial embeddings for values infrequently observed or even absent within the training set, posing significant challenges to model generalizability.","In response to these challenges, we propose SCAlable Numerical Embedding (SCANE), a novel framework that treats each feature value as an independent token, effectively bypassing the need for imputation.","SCANE regularizes the traits of distinct feature embeddings and enhances representational learning through a scalable embedding mechanism.","Coupling SCANE with the Transformer Encoder architecture, we develop the Scalable nUMerical eMbeddIng Transformer (SUMMIT), which is engineered to deliver precise predictive outputs for MTS characterized by prevalent missing entries.","Our experimental validation, conducted across three disparate electronic health record (EHR) datasets marked by elevated missing value frequencies, confirms the superior performance of SUMMIT over contemporary state-of-the-art approaches addressing similar challenges.","These results substantiate the efficacy of SCANE and SUMMIT, underscoring their potential applicability across a broad spectrum of MTS data analytical tasks."],"url":"http://arxiv.org/abs/2405.16557v1","category":"cs.LG"}
{"created":"2024-05-26 13:04:20","title":"Gibbs states and Brownian models for coexisting haze and cloud droplets","abstract":"Clouds microphysics describes the formation and evolution of cloud droplets, rain, and ice particles. It is among the most critical factors in determining the cloud's size, lifetime, precipitation, and radiative effect. Among all cloud types, the small clouds, characterized by weak updrafts, that are close to the haze-to-cloud transition pose challenges in measuring them and understanding their properties. They are superabundant but hard to capture by satellites and often falsely regarded as aerosols. K\\\"ohler's theory explains droplet activation and their growth by condensation at the earliest stages of cloud development. It fully describes the thermodynamic state of a single drop but falls short when explaining the collective behavior of large populations of particles. This is especially important when the supersaturation pool is limited. We present an analytical framework to extend K\\\"ohler's theory to coexisting cloud droplets. Our results suggest hysteresis and asymmetry in the process of droplet activation and deactivation. The turbulent nature of clouds is incorporated into our model formulation as Brownian noise to provide explicit droplet size distributions and activation timescales. The theoretical findings are confronted with experimental data stemming from laboratory clouds created in the Pi convection chamber, suggesting a new way of understanding haze-to-cloud transitions and small cloud formation processes.","sentences":["Clouds microphysics describes the formation and evolution of cloud droplets, rain, and ice particles.","It is among the most critical factors in determining the cloud's size, lifetime, precipitation, and radiative effect.","Among all cloud types, the small clouds, characterized by weak updrafts, that are close to the haze-to-cloud transition pose challenges in measuring them and understanding their properties.","They are superabundant but hard to capture by satellites and often falsely regarded as aerosols.","K\\\"ohler's theory explains droplet activation and their growth by condensation at the earliest stages of cloud development.","It fully describes the thermodynamic state of a single drop but falls short when explaining the collective behavior of large populations of particles.","This is especially important when the supersaturation pool is limited.","We present an analytical framework to extend K\\\"ohler's theory to coexisting cloud droplets.","Our results suggest hysteresis and asymmetry in the process of droplet activation and deactivation.","The turbulent nature of clouds is incorporated into our model formulation as Brownian noise to provide explicit droplet size distributions and activation timescales.","The theoretical findings are confronted with experimental data stemming from laboratory clouds created in the Pi convection chamber, suggesting a new way of understanding haze-to-cloud transitions and small cloud formation processes."],"url":"http://arxiv.org/abs/2405.16556v1","category":"physics.ao-ph"}
{"created":"2024-05-26 12:43:18","title":"SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation","abstract":"Existing Large Language Models (LLMs) generate text through unidirectional autoregressive decoding methods to respond to various user queries. These methods tend to consider token selection in a simple sequential manner, making it easy to fall into suboptimal options when encountering uncertain tokens, referred to as chaotic points in our work. Many chaotic points exist in texts generated by LLMs, and they often significantly affect the quality of subsequently generated tokens, which can interfere with LLMs' generation. This paper proposes Self-Evaluation Decoding, SED, a decoding method for enhancing model generation. Analogous to the human decision-making process, SED integrates speculation and evaluation steps into the decoding process, allowing LLMs to make more careful decisions and thus optimize token selection at chaotic points. Experimental results across various tasks using different LLMs demonstrate SED's effectiveness.","sentences":["Existing Large Language Models (LLMs) generate text through unidirectional autoregressive decoding methods to respond to various user queries.","These methods tend to consider token selection in a simple sequential manner, making it easy to fall into suboptimal options when encountering uncertain tokens, referred to as chaotic points in our work.","Many chaotic points exist in texts generated by LLMs, and they often significantly affect the quality of subsequently generated tokens, which can interfere with LLMs' generation.","This paper proposes Self-Evaluation Decoding, SED, a decoding method for enhancing model generation.","Analogous to the human decision-making process, SED integrates speculation and evaluation steps into the decoding process, allowing LLMs to make more careful decisions and thus optimize token selection at chaotic points.","Experimental results across various tasks using different LLMs demonstrate SED's effectiveness."],"url":"http://arxiv.org/abs/2405.16552v1","category":"cs.CL"}
{"created":"2024-05-26 12:40:23","title":"ReCODE: Modeling Repeat Consumption with Neural ODE","abstract":"In real-world recommender systems, such as in the music domain, repeat consumption is a common phenomenon where users frequently listen to a small set of preferred songs or artists repeatedly. The key point of modeling repeat consumption is capturing the temporal patterns between a user's repeated consumption of the items. Existing studies often rely on heuristic assumptions, such as assuming an exponential distribution for the temporal gaps. However, due to the high complexity of real-world recommender systems, these pre-defined distributions may fail to capture the intricate dynamic user consumption patterns, leading to sub-optimal performance. Drawing inspiration from the flexibility of neural ordinary differential equations (ODE) in capturing the dynamics of complex systems, we propose ReCODE, a novel model-agnostic framework that utilizes neural ODE to model repeat consumption. ReCODE comprises two essential components: a user's static preference prediction module and the modeling of user dynamic repeat intention. By considering both immediate choices and historical consumption patterns, ReCODE offers comprehensive modeling of user preferences in the target context. Moreover, ReCODE seamlessly integrates with various existing recommendation models, including collaborative-based and sequential-based models, making it easily applicable in different scenarios. Experimental results on two real-world datasets consistently demonstrate that ReCODE significantly improves the performance of base models and outperforms other baseline methods.","sentences":["In real-world recommender systems, such as in the music domain, repeat consumption is a common phenomenon where users frequently listen to a small set of preferred songs or artists repeatedly.","The key point of modeling repeat consumption is capturing the temporal patterns between a user's repeated consumption of the items.","Existing studies often rely on heuristic assumptions, such as assuming an exponential distribution for the temporal gaps.","However, due to the high complexity of real-world recommender systems, these pre-defined distributions may fail to capture the intricate dynamic user consumption patterns, leading to sub-optimal performance.","Drawing inspiration from the flexibility of neural ordinary differential equations (ODE) in capturing the dynamics of complex systems, we propose ReCODE, a novel model-agnostic framework that utilizes neural ODE to model repeat consumption.","ReCODE comprises two essential components: a user's static preference prediction module and the modeling of user dynamic repeat intention.","By considering both immediate choices and historical consumption patterns, ReCODE offers comprehensive modeling of user preferences in the target context.","Moreover, ReCODE seamlessly integrates with various existing recommendation models, including collaborative-based and sequential-based models, making it easily applicable in different scenarios.","Experimental results on two real-world datasets consistently demonstrate that ReCODE significantly improves the performance of base models and outperforms other baseline methods."],"url":"http://arxiv.org/abs/2405.16550v1","category":"cs.IR"}
{"created":"2024-05-26 12:26:03","title":"Mamba4KT:An Efficient and Effective Mamba-based Knowledge Tracing Model","abstract":"Knowledge tracing (KT) enhances student learning by leveraging past performance to predict future performance. Current research utilizes models based on attention mechanisms and recurrent neural network structures to capture long-term dependencies and correlations between exercises, aiming to improve model accuracy. Due to the growing amount of data in smart education scenarios, this poses a challenge in terms of time and space consumption for knowledge tracing models. However, existing research often overlooks the efficiency of model training and inference and the constraints of training resources. Recognizing the significance of prioritizing model efficiency and resource usage in knowledge tracing, we introduce Mamba4KT. This novel model is the first to explore enhanced efficiency and resource utilization in knowledge tracing. We also examine the interpretability of the Mamba structure both sequence-level and exercise-level to enhance model interpretability. Experimental findings across three public datasets demonstrate that Mamba4KT achieves comparable prediction accuracy to state-of-the-art models while significantly improving training and inference efficiency and resource utilization. As educational data continues to grow, our work suggests a promising research direction for knowledge tracing that improves model prediction accuracy, model efficiency, resource utilization, and interpretability simultaneously.","sentences":["Knowledge tracing (KT) enhances student learning by leveraging past performance to predict future performance.","Current research utilizes models based on attention mechanisms and recurrent neural network structures to capture long-term dependencies and correlations between exercises, aiming to improve model accuracy.","Due to the growing amount of data in smart education scenarios, this poses a challenge in terms of time and space consumption for knowledge tracing models.","However, existing research often overlooks the efficiency of model training and inference and the constraints of training resources.","Recognizing the significance of prioritizing model efficiency and resource usage in knowledge tracing, we introduce Mamba4KT.","This novel model is the first to explore enhanced efficiency and resource utilization in knowledge tracing.","We also examine the interpretability of the Mamba structure both sequence-level and exercise-level to enhance model interpretability.","Experimental findings across three public datasets demonstrate that Mamba4KT achieves comparable prediction accuracy to state-of-the-art models while significantly improving training and inference efficiency and resource utilization.","As educational data continues to grow, our work suggests a promising research direction for knowledge tracing that improves model prediction accuracy, model efficiency, resource utilization, and interpretability simultaneously."],"url":"http://arxiv.org/abs/2405.16542v1","category":"cs.AI"}
{"created":"2024-05-26 12:01:30","title":"Gamified AI Approch for Early Detection of Dementia","abstract":"This paper aims to develop a new deep learning-inspired gaming approach for early detection of dementia. This research integrates a robust convolutional neural network (CNN)-based model for early dementia detection using health metrics data as well as facial image data through a cognitive assessment-based gaming application. We have collected 1000 data samples of health metrics dataset from Apollo Diagnostic Center Kolkata that is labeled as either demented or non-demented for the training of MOD-1D-CNN for the game level 1 and another dataset of facial images containing 1800 facial data that are labeled as either demented or non-demented is collected by our research team for the training of MOD-2D-CNN model in-game level 2. In our work, the loss for the proposed MOD-1D-CNN model is 0.2692 and the highest accuracy is 70.50% for identifying the dementia traits using real-life health metrics data. Similarly, the proposed MOD-2D-CNN model loss is 0.1755 and the highest accuracy is obtained here 95.72% for recognizing the dementia status using real-life face-based image data. Therefore, a rule-based weightage method is applied to combine both the proposed methods to achieve the final decision. The MOD-1D-CNN and MOD-2D-CNN models are more lightweight and computationally efficient alternatives because they have a significantly lower number of parameters when compared to the other state-of-the-art models. We have compared their accuracies and parameters with the other state-of-the-art deep learning models.","sentences":["This paper aims to develop a new deep learning-inspired gaming approach for early detection of dementia.","This research integrates a robust convolutional neural network (CNN)-based model for early dementia detection using health metrics data as well as facial image data through a cognitive assessment-based gaming application.","We have collected 1000 data samples of health metrics dataset from Apollo Diagnostic Center Kolkata that is labeled as either demented or non-demented for the training of MOD-1D-CNN for the game level 1 and another dataset of facial images containing 1800 facial data that are labeled as either demented or non-demented is collected by our research team for the training of MOD-2D-CNN model in-game level 2.","In our work, the loss for the proposed MOD-1D-CNN model is 0.2692 and the highest accuracy is 70.50% for identifying the dementia traits using real-life health metrics data.","Similarly, the proposed MOD-2D-CNN model loss is 0.1755 and the highest accuracy is obtained here 95.72% for recognizing the dementia status using real-life face-based image data.","Therefore, a rule-based weightage method is applied to combine both the proposed methods to achieve the final decision.","The MOD-1D-CNN and MOD-2D-CNN models are more lightweight and computationally efficient alternatives because they have a significantly lower number of parameters when compared to the other state-of-the-art models.","We have compared their accuracies and parameters with the other state-of-the-art deep learning models."],"url":"http://arxiv.org/abs/2405.16538v1","category":"cs.CV"}
{"created":"2024-05-26 11:22:47","title":"Past, Present, and Future of Citation Practices in HCI","abstract":"Science is a complex system comprised of many scientists who individually make collective decisions that, due to the size and nature of the academic system, largely do not affect the system as a whole. However, certain decisions at the meso-level of research communities, such as the Human-Computer Interaction (HCI) community, may result in deep and long-lasting behavioral changes in scientists. In this article, we provide evidence on how a change in editorial policies introduced at the ACM CHI Conference in 2016 launched the CHI community on an expansive path, denoted by a year-by-year increase in the mean number of references included in CHI articles. If this near-linear trend continues undisrupted, an article in CHI 2030 will include on average almost 130 references. Our meta-research provides insights into how the nature and meaning of citation practices in HCI have changed, influenced by factors such as digital accessibility of resources and academic pressures. The observed trend towards more citations reflects a citation culture where quantity is prioritized over quality, contributing to both author and peer reviewer fatigue. This article underscores the value of meta-research for research communities and the profound impact that meso-level policy adjustments have on the evolution of scientific fields and disciplines, urging stakeholders to carefully consider the broader implications of such changes.","sentences":["Science is a complex system comprised of many scientists who individually make collective decisions that, due to the size and nature of the academic system, largely do not affect the system as a whole.","However, certain decisions at the meso-level of research communities, such as the Human-Computer Interaction (HCI) community, may result in deep and long-lasting behavioral changes in scientists.","In this article, we provide evidence on how a change in editorial policies introduced at the ACM CHI Conference in 2016 launched the CHI community on an expansive path, denoted by a year-by-year increase in the mean number of references included in CHI articles.","If this near-linear trend continues undisrupted, an article in CHI 2030 will include on average almost 130 references.","Our meta-research provides insights into how the nature and meaning of citation practices in HCI have changed, influenced by factors such as digital accessibility of resources and academic pressures.","The observed trend towards more citations reflects a citation culture where quantity is prioritized over quality, contributing to both author and peer reviewer fatigue.","This article underscores the value of meta-research for research communities and the profound impact that meso-level policy adjustments have on the evolution of scientific fields and disciplines, urging stakeholders to carefully consider the broader implications of such changes."],"url":"http://arxiv.org/abs/2405.16526v1","category":"cs.HC"}
{"created":"2024-05-26 11:17:49","title":"Multi-State TD Target for Model-Free Reinforcement Learning","abstract":"Temporal difference (TD) learning is a fundamental technique in reinforcement learning that updates value estimates for states or state-action pairs using a TD target. This target represents an improved estimate of the true value by incorporating both immediate rewards and the estimated value of subsequent states. Traditionally, TD learning relies on the value of a single subsequent state. We propose an enhanced multi-state TD (MSTD) target that utilizes the estimated values of multiple subsequent states. Building on this new MSTD concept, we develop complete actor-critic algorithms that include management of replay buffers in two modes, and integrate with deep deterministic policy optimization (DDPG) and soft actor-critic (SAC). Experimental results demonstrate that algorithms employing the MSTD target significantly improve learning performance compared to traditional methods.","sentences":["Temporal difference (TD) learning is a fundamental technique in reinforcement learning that updates value estimates for states or state-action pairs using a TD target.","This target represents an improved estimate of the true value by incorporating both immediate rewards and the estimated value of subsequent states.","Traditionally, TD learning relies on the value of a single subsequent state.","We propose an enhanced multi-state TD (MSTD) target that utilizes the estimated values of multiple subsequent states.","Building on this new MSTD concept, we develop complete actor-critic algorithms that include management of replay buffers in two modes, and integrate with deep deterministic policy optimization (DDPG) and soft actor-critic (SAC).","Experimental results demonstrate that algorithms employing the MSTD target significantly improve learning performance compared to traditional methods."],"url":"http://arxiv.org/abs/2405.16522v1","category":"cs.LG"}
{"created":"2024-05-26 10:57:23","title":"Adaptive estimation of $\\mathbb{L}_2$-norm of a probability density and related topics I. Lower bounds","abstract":"We deal with the problem of the adaptive estimation of the $\\mathbb{L}_2$-norm of a probability density on $\\mathbb{R}^d$, $d\\geq 1$, from independent observations. The unknown density is assumed to be uniformly bounded and to belong to the union of balls in the isotropic/anisotropic Nikolskii's spaces. We will show that the optimally adaptive estimators over the collection of considered functional classes do no exist. Also, in the framework of an abstract density model we present several generic lower bounds related to the adaptive estimation of an arbitrary functional of a probability density. These results having independent interest have no analogue in the existing literature. In the companion paper Cleanthous et al (2024) we prove that established lower bounds are tight and provide with explicit construction of adaptive estimators of $\\mathbb{L}_2$-norm of the density.","sentences":["We deal with the problem of the adaptive estimation of the $\\mathbb{L}_2$-norm of a probability density on $\\mathbb{R}^d$, $d\\geq 1$, from independent observations.","The unknown density is assumed to be uniformly bounded and to belong to the union of balls in the isotropic/anisotropic Nikolskii's spaces.","We will show that the optimally adaptive estimators over the collection of considered functional classes do no exist.","Also, in the framework of an abstract density model we present several generic lower bounds related to the adaptive estimation of an arbitrary functional of a probability density.","These results having independent interest have no analogue in the existing literature.","In the companion paper Cleanthous et al (2024) we prove that established lower bounds are tight and provide with explicit construction of adaptive estimators of $\\mathbb{L}_2$-norm of the density."],"url":"http://arxiv.org/abs/2405.16515v1","category":"math.ST"}
{"created":"2024-05-26 10:43:16","title":"SE3Set: Harnessing equivariant hypergraph neural networks for molecular representation learning","abstract":"In this paper, we develop SE3Set, an SE(3) equivariant hypergraph neural network architecture tailored for advanced molecular representation learning. Hypergraphs are not merely an extension of traditional graphs; they are pivotal for modeling high-order relationships, a capability that conventional equivariant graph-based methods lack due to their inherent limitations in representing intricate many-body interactions. To achieve this, we first construct hypergraphs via proposing a new fragmentation method that considers both chemical and three-dimensional spatial information of molecular system. We then design SE3Set, which incorporates equivariance into the hypergragh neural network. This ensures that the learned molecular representations are invariant to spatial transformations, thereby providing robustness essential for accurate prediction of molecular properties. SE3Set has shown performance on par with state-of-the-art (SOTA) models for small molecule datasets like QM9 and MD17. It excels on the MD22 dataset, achieving a notable improvement of approximately 20% in accuracy across all molecules, which highlights the prevalence of complex many-body interactions in larger molecules. This exceptional performance of SE3Set across diverse molecular structures underscores its transformative potential in computational chemistry, offering a route to more accurate and physically nuanced modeling.","sentences":["In this paper, we develop SE3Set, an SE(3) equivariant hypergraph neural network architecture tailored for advanced molecular representation learning.","Hypergraphs are not merely an extension of traditional graphs; they are pivotal for modeling high-order relationships, a capability that conventional equivariant graph-based methods lack due to their inherent limitations in representing intricate many-body interactions.","To achieve this, we first construct hypergraphs via proposing a new fragmentation method that considers both chemical and three-dimensional spatial information of molecular system.","We then design SE3Set, which incorporates equivariance into the hypergragh neural network.","This ensures that the learned molecular representations are invariant to spatial transformations, thereby providing robustness essential for accurate prediction of molecular properties.","SE3Set has shown performance on par with state-of-the-art (SOTA) models for small molecule datasets like QM9 and MD17.","It excels on the MD22 dataset, achieving a notable improvement of approximately 20% in accuracy across all molecules, which highlights the prevalence of complex many-body interactions in larger molecules.","This exceptional performance of SE3Set across diverse molecular structures underscores its transformative potential in computational chemistry, offering a route to more accurate and physically nuanced modeling."],"url":"http://arxiv.org/abs/2405.16511v1","category":"cs.LG"}
{"created":"2024-05-26 10:33:17","title":"Meta-Task Planning for Language Agents","abstract":"The rapid advancement of neural language models has sparked a new surge of intelligent agent research. Unlike traditional agents, large language model-based agents (LLM agents) have emerged as a promising paradigm for achieving artificial general intelligence (AGI) due to their superior reasoning and generalization capabilities. Effective planning is crucial for the success of LLM agents in real-world tasks, making it a highly pursued topic in the community. Current planning methods typically translate tasks into executable action sequences. However, determining a feasible or optimal sequence for complex tasks at fine granularity, which often requires compositing long chains of heterogeneous actions, remains challenging. This paper introduces Meta-Task Planning (MTP), a zero-shot methodology for collaborative LLM-based multi-agent systems that simplifies complex task planning by decomposing it into a hierarchy of subordinate tasks, or meta-tasks. Each meta-task is then mapped into executable actions. MTP was assessed on two rigorous benchmarks, TravelPlanner and API-Bank. Notably, MTP achieved an average $\\sim40\\%$ success rate on TravelPlanner, significantly higher than the state-of-the-art (SOTA) baseline ($2.92\\%$), and outperforming $LLM_{api}$-4 with ReAct on API-Bank by $\\sim14\\%$, showing the immense potential of integrating LLM with multi-agent systems.","sentences":["The rapid advancement of neural language models has sparked a new surge of intelligent agent research.","Unlike traditional agents, large language model-based agents (LLM agents) have emerged as a promising paradigm for achieving artificial general intelligence (AGI) due to their superior reasoning and generalization capabilities.","Effective planning is crucial for the success of LLM agents in real-world tasks, making it a highly pursued topic in the community.","Current planning methods typically translate tasks into executable action sequences.","However, determining a feasible or optimal sequence for complex tasks at fine granularity, which often requires compositing long chains of heterogeneous actions, remains challenging.","This paper introduces Meta-Task Planning (MTP), a zero-shot methodology for collaborative LLM-based multi-agent systems that simplifies complex task planning by decomposing it into a hierarchy of subordinate tasks, or meta-tasks.","Each meta-task is then mapped into executable actions.","MTP was assessed on two rigorous benchmarks, TravelPlanner and API-Bank.","Notably, MTP achieved an average $\\sim40\\%$ success rate on TravelPlanner, significantly higher than the state-of-the-art (SOTA) baseline ($2.92\\%$), and outperforming $LLM_{api}$-4 with ReAct on API-Bank by $\\sim14\\%$, showing the immense potential of integrating LLM with multi-agent systems."],"url":"http://arxiv.org/abs/2405.16510v1","category":"cs.AI"}
{"created":"2024-05-26 10:15:20","title":"Causal Concept Embedding Models: Beyond Causal Opacity in Deep Learning","abstract":"Causal opacity denotes the difficulty in understanding the \"hidden\" causal structure underlying a deep neural network's (DNN) reasoning. This leads to the inability to rely on and verify state-of-the-art DNN-based systems especially in high-stakes scenarios. For this reason, causal opacity represents a key open challenge at the intersection of deep learning, interpretability, and causality. This work addresses this gap by introducing Causal Concept Embedding Models (Causal CEMs), a class of interpretable models whose decision-making process is causally transparent by design. The results of our experiments show that Causal CEMs can: (i) match the generalization performance of causally-opaque models, (ii) support the analysis of interventional and counterfactual scenarios, thereby improving the model's causal interpretability and supporting the effective verification of its reliability and fairness, and (iii) enable human-in-the-loop corrections to mispredicted intermediate reasoning steps, boosting not just downstream accuracy after corrections but also accuracy of the explanation provided for a specific instance.","sentences":["Causal opacity denotes the difficulty in understanding the \"hidden\" causal structure underlying a deep neural network's (DNN) reasoning.","This leads to the inability to rely on and verify state-of-the-art DNN-based systems especially in high-stakes scenarios.","For this reason, causal opacity represents a key open challenge at the intersection of deep learning, interpretability, and causality.","This work addresses this gap by introducing Causal Concept Embedding Models (Causal CEMs), a class of interpretable models whose decision-making process is causally transparent by design.","The results of our experiments show that Causal CEMs can: (i) match the generalization performance of causally-opaque models, (ii) support the analysis of interventional and counterfactual scenarios, thereby improving the model's causal interpretability and supporting the effective verification of its reliability and fairness, and (iii) enable human-in-the-loop corrections to mispredicted intermediate reasoning steps, boosting not just downstream accuracy after corrections but also accuracy of the explanation provided for a specific instance."],"url":"http://arxiv.org/abs/2405.16507v1","category":"cs.LG"}
{"created":"2024-05-26 09:47:17","title":"Integrating GNN and Neural ODEs for Estimating Two-Body Interactions in Mixed-Species Collective Motion","abstract":"Analyzing the motion of multiple biological agents, be it cells or individual animals, is pivotal for the understanding of complex collective behaviors. With the advent of advanced microscopy, detailed images of complex tissue formations involving multiple cell types have become more accessible in recent years. However, deciphering the underlying rules that govern cell movements is far from trivial. Here, we present a novel deep learning framework to estimate the underlying equations of motion from observed trajectories, a pivotal step in decoding such complex dynamics. Our framework integrates graph neural networks with neural differential equations, enabling effective prediction of two-body interactions based on the states of the interacting entities. We demonstrate the efficacy of our approach through two numerical experiments. First, we used a simulated data from a toy model to tune the hyperparameters. Based on the obtained hyperparameters, we then applied this approach to a more complex model that describes interacting cells of cellular slime molds. Our results show that the proposed method can accurately estimate the function of two-body interactions, thereby precisely replicating both individual and collective behaviors within these systems.","sentences":["Analyzing the motion of multiple biological agents, be it cells or individual animals, is pivotal for the understanding of complex collective behaviors.","With the advent of advanced microscopy, detailed images of complex tissue formations involving multiple cell types have become more accessible in recent years.","However, deciphering the underlying rules that govern cell movements is far from trivial.","Here, we present a novel deep learning framework to estimate the underlying equations of motion from observed trajectories, a pivotal step in decoding such complex dynamics.","Our framework integrates graph neural networks with neural differential equations, enabling effective prediction of two-body interactions based on the states of the interacting entities.","We demonstrate the efficacy of our approach through two numerical experiments.","First, we used a simulated data from a toy model to tune the hyperparameters.","Based on the obtained hyperparameters, we then applied this approach to a more complex model that describes interacting cells of cellular slime molds.","Our results show that the proposed method can accurately estimate the function of two-body interactions, thereby precisely replicating both individual and collective behaviors within these systems."],"url":"http://arxiv.org/abs/2405.16503v1","category":"physics.bio-ph"}
{"created":"2024-05-26 09:16:34","title":"Exploring a Multimodal Fusion-based Deep Learning Network for Detecting Facial Palsy","abstract":"Algorithmic detection of facial palsy offers the potential to improve current practices, which usually involve labor-intensive and subjective assessment by clinicians. In this paper, we present a multimodal fusion-based deep learning model that utilizes unstructured data (i.e. an image frame with facial line segments) and structured data (i.e. features of facial expressions) to detect facial palsy. We then contribute to a study to analyze the effect of different data modalities and the benefits of a multimodal fusion-based approach using videos of 21 facial palsy patients. Our experimental results show that among various data modalities (i.e. unstructured data - RGB images and images of facial line segments and structured data - coordinates of facial landmarks and features of facial expressions), the feed-forward neural network using features of facial expression achieved the highest precision of 76.22 while the ResNet-based model using images of facial line segments achieved the highest recall of 83.47. When we leveraged both images of facial line segments and features of facial expressions, our multimodal fusion-based deep learning model slightly improved the precision score to 77.05 at the expense of a decrease in the recall score.","sentences":["Algorithmic detection of facial palsy offers the potential to improve current practices, which usually involve labor-intensive and subjective assessment by clinicians.","In this paper, we present a multimodal fusion-based deep learning model that utilizes unstructured data (i.e. an image frame with facial line segments) and structured data (i.e. features of facial expressions) to detect facial palsy.","We then contribute to a study to analyze the effect of different data modalities and the benefits of a multimodal fusion-based approach using videos of 21 facial palsy patients.","Our experimental results show that among various data modalities (i.e. unstructured data - RGB images and images of facial line segments and structured data - coordinates of facial landmarks and features of facial expressions), the feed-forward neural network using features of facial expression achieved the highest precision of 76.22 while the ResNet-based model using images of facial line segments achieved the highest recall of 83.47.","When we leveraged both images of facial line segments and features of facial expressions, our multimodal fusion-based deep learning model slightly improved the precision score to 77.05 at the expense of a decrease in the recall score."],"url":"http://arxiv.org/abs/2405.16496v1","category":"cs.CV"}
{"created":"2024-05-26 08:55:22","title":"Causal-Aware Graph Neural Architecture Search under Distribution Shifts","abstract":"Graph NAS has emerged as a promising approach for autonomously designing GNN architectures by leveraging the correlations between graphs and architectures. Existing methods fail to generalize under distribution shifts that are ubiquitous in real-world graph scenarios, mainly because the graph-architecture correlations they exploit might be spurious and varying across distributions. We propose to handle the distribution shifts in the graph architecture search process by discovering and exploiting the causal relationship between graphs and architectures to search for the optimal architectures that can generalize under distribution shifts. The problem remains unexplored with following challenges: how to discover the causal graph-architecture relationship that has stable predictive abilities across distributions, and how to handle distribution shifts with the discovered causal graph-architecture relationship to search the generalized graph architectures. To address these challenges, we propose Causal-aware Graph Neural Architecture Search (CARNAS), which is able to capture the causal graph-architecture relationship during the architecture search process and discover the generalized graph architecture under distribution shifts. Specifically, we propose Disentangled Causal Subgraph Identification to capture the causal subgraphs that have stable prediction abilities across distributions. Then, we propose Graph Embedding Intervention to intervene on causal subgraphs within the latent space, ensuring that these subgraphs encapsulate essential features for prediction while excluding non-causal elements. Additionally, we propose Invariant Architecture Customization to reinforce the causal invariant nature of the causal subgraphs, which are utilized to tailor generalized graph architectures. Extensive experiments demonstrate that CARNAS achieves advanced out-of-distribution generalization ability.","sentences":["Graph NAS has emerged as a promising approach for autonomously designing GNN architectures by leveraging the correlations between graphs and architectures.","Existing methods fail to generalize under distribution shifts that are ubiquitous in real-world graph scenarios, mainly because the graph-architecture correlations they exploit might be spurious and varying across distributions.","We propose to handle the distribution shifts in the graph architecture search process by discovering and exploiting the causal relationship between graphs and architectures to search for the optimal architectures that can generalize under distribution shifts.","The problem remains unexplored with following challenges: how to discover the causal graph-architecture relationship that has stable predictive abilities across distributions, and how to handle distribution shifts with the discovered causal graph-architecture relationship to search the generalized graph architectures.","To address these challenges, we propose Causal-aware Graph Neural Architecture Search (CARNAS), which is able to capture the causal graph-architecture relationship during the architecture search process and discover the generalized graph architecture under distribution shifts.","Specifically, we propose Disentangled Causal Subgraph Identification to capture the causal subgraphs that have stable prediction abilities across distributions.","Then, we propose Graph Embedding Intervention to intervene on causal subgraphs within the latent space, ensuring that these subgraphs encapsulate essential features for prediction while excluding non-causal elements.","Additionally, we propose Invariant Architecture Customization to reinforce the causal invariant nature of the causal subgraphs, which are utilized to tailor generalized graph architectures.","Extensive experiments demonstrate that CARNAS achieves advanced out-of-distribution generalization ability."],"url":"http://arxiv.org/abs/2405.16489v1","category":"cs.LG"}
{"created":"2024-05-26 08:51:39","title":"Decomposing the Neurons: Activation Sparsity via Mixture of Experts for Continual Test Time Adaptation","abstract":"Continual Test-Time Adaptation (CTTA), which aims to adapt the pre-trained model to ever-evolving target domains, emerges as an important task for vision models. As current vision models appear to be heavily biased towards texture, continuously adapting the model from one domain distribution to another can result in serious catastrophic forgetting. Drawing inspiration from the human visual system's adeptness at processing both shape and texture according to the famous Trichromatic Theory, we explore the integration of a Mixture-of-Activation-Sparsity-Experts (MoASE) as an adapter for the CTTA task. Given the distinct reaction of neurons with low/high activation to domain-specific/agnostic features, MoASE decomposes the neural activation into high-activation and low-activation components with a non-differentiable Spatial Differentiate Dropout (SDD). Based on the decomposition, we devise a multi-gate structure comprising a Domain-Aware Gate (DAG) that utilizes domain information to adaptive combine experts that process the post-SDD sparse activations of different strengths, and the Activation Sparsity Gate (ASG) that adaptively assigned feature selection threshold of the SDD for different experts for more precise feature decomposition. Finally, we introduce a Homeostatic-Proximal (HP) loss to bypass the error accumulation problem when continuously adapting the model. Extensive experiments on four prominent benchmarks substantiate that our methodology achieves state-of-the-art performance in both classification and segmentation CTTA tasks. Our code is now available at https://github.com/RoyZry98/MoASE-Pytorch.","sentences":["Continual Test-Time Adaptation (CTTA), which aims to adapt the pre-trained model to ever-evolving target domains, emerges as an important task for vision models.","As current vision models appear to be heavily biased towards texture, continuously adapting the model from one domain distribution to another can result in serious catastrophic forgetting.","Drawing inspiration from the human visual system's adeptness at processing both shape and texture according to the famous Trichromatic Theory, we explore the integration of a Mixture-of-Activation-Sparsity-Experts (MoASE) as an adapter for the CTTA task.","Given the distinct reaction of neurons with low/high activation to domain-specific/agnostic features, MoASE decomposes the neural activation into high-activation and low-activation components with a non-differentiable Spatial Differentiate Dropout (SDD).","Based on the decomposition, we devise a multi-gate structure comprising a Domain-Aware Gate (DAG) that utilizes domain information to adaptive combine experts that process the post-SDD sparse activations of different strengths, and the Activation Sparsity Gate (ASG) that adaptively assigned feature selection threshold of the SDD for different experts for more precise feature decomposition.","Finally, we introduce a Homeostatic-Proximal (HP) loss to bypass the error accumulation problem when continuously adapting the model.","Extensive experiments on four prominent benchmarks substantiate that our methodology achieves state-of-the-art performance in both classification and segmentation CTTA tasks.","Our code is now available at https://github.com/RoyZry98/MoASE-Pytorch."],"url":"http://arxiv.org/abs/2405.16486v1","category":"cs.CV"}
{"created":"2024-05-26 08:47:53","title":"Make Safe Decisions in Power System: Safe Reinforcement Learning Based Pre-decision Making for Voltage Stability Emergency Control","abstract":"The high penetration of renewable energy and power electronic equipment bring significant challenges to the efficient construction of adaptive emergency control strategies against various presumed contingencies in today's power systems. Traditional model-based emergency control methods have difficulty in adapt well to various complicated operating conditions in practice. Fr emerging artificial intelligence-based approaches, i.e., reinforcement learning-enabled solutions, they are yet to provide solid safety assurances under strict constraints in practical power systems. To address these research gaps, this paper develops a safe reinforcement learning (SRL)-based pre-decision making framework against short-term voltage collapse. Our proposed framework employs neural networks for pre-decision formulation, security margin estimation, and corrective action implementation, without reliance on precise system parameters. Leveraging the gradient projection, we propose a security projecting correction algorithm that offers theoretical security assurances to amend risky actions. The applicability of the algorithm is further enhanced through the incorporation of active learning, which expedites the training process and improves security estimation accuracy. Extensive numerical tests on the New England 39-bus system and the realistic Guangdong Provincal Power Grid demonstrate the effectiveness of the proposed framework.","sentences":["The high penetration of renewable energy and power electronic equipment bring significant challenges to the efficient construction of adaptive emergency control strategies against various presumed contingencies in today's power systems.","Traditional model-based emergency control methods have difficulty in adapt well to various complicated operating conditions in practice.","Fr emerging artificial intelligence-based approaches, i.e., reinforcement learning-enabled solutions, they are yet to provide solid safety assurances under strict constraints in practical power systems.","To address these research gaps, this paper develops a safe reinforcement learning (SRL)-based pre-decision making framework against short-term voltage collapse.","Our proposed framework employs neural networks for pre-decision formulation, security margin estimation, and corrective action implementation, without reliance on precise system parameters.","Leveraging the gradient projection, we propose a security projecting correction algorithm that offers theoretical security assurances to amend risky actions.","The applicability of the algorithm is further enhanced through the incorporation of active learning, which expedites the training process and improves security estimation accuracy.","Extensive numerical tests on the New England 39-bus system and the realistic Guangdong Provincal Power Grid demonstrate the effectiveness of the proposed framework."],"url":"http://arxiv.org/abs/2405.16485v1","category":"eess.SY"}
{"created":"2024-05-26 08:03:51","title":"Vision-Based Approach for Food Weight Estimation from 2D Images","abstract":"In response to the increasing demand for efficient and non-invasive methods to estimate food weight, this paper presents a vision-based approach utilizing 2D images. The study employs a dataset of 2380 images comprising fourteen different food types in various portions, orientations, and containers. The proposed methodology integrates deep learning and computer vision techniques, specifically employing Faster R-CNN for food detection and MobileNetV3 for weight estimation. The detection model achieved a mean average precision (mAP) of 83.41\\%, an average Intersection over Union (IoU) of 91.82\\%, and a classification accuracy of 100\\%. For weight estimation, the model demonstrated a root mean squared error (RMSE) of 6.3204, a mean absolute percentage error (MAPE) of 0.0640\\%, and an R-squared value of 98.65\\%. The study underscores the potential applications of this technology in healthcare for nutrition counseling, fitness and wellness for dietary intake assessment, and smart food storage solutions to reduce waste. The results indicate that the combination of Faster R-CNN and MobileNetV3 provides a robust framework for accurate food weight estimation from 2D images, showcasing the synergy of computer vision and deep learning in practical applications.","sentences":["In response to the increasing demand for efficient and non-invasive methods to estimate food weight, this paper presents a vision-based approach utilizing 2D images.","The study employs a dataset of 2380 images comprising fourteen different food types in various portions, orientations, and containers.","The proposed methodology integrates deep learning and computer vision techniques, specifically employing Faster R-CNN for food detection and MobileNetV3 for weight estimation.","The detection model achieved a mean average precision (mAP) of 83.41\\%, an average Intersection over Union (IoU) of 91.82\\%, and a classification accuracy of 100\\%.","For weight estimation, the model demonstrated a root mean squared error (RMSE) of 6.3204, a mean absolute percentage error (MAPE) of 0.0640\\%, and an R-squared value of 98.65\\%.","The study underscores the potential applications of this technology in healthcare for nutrition counseling, fitness and wellness for dietary intake assessment, and smart food storage solutions to reduce waste.","The results indicate that the combination of Faster R-CNN and MobileNetV3 provides a robust framework for accurate food weight estimation from 2D images, showcasing the synergy of computer vision and deep learning in practical applications."],"url":"http://arxiv.org/abs/2405.16478v1","category":"cs.CV"}
{"created":"2024-05-26 07:58:51","title":"Looks Too Good To Be True: An Information-Theoretic Analysis of Hallucinations in Generative Restoration Models","abstract":"The pursuit of high perceptual quality in image restoration has driven the development of revolutionary generative models, capable of producing results often visually indistinguishable from real data. However, as their perceptual quality continues to improve, these models also exhibit a growing tendency to generate hallucinations - realistic-looking details that do not exist in the ground truth images. The presence of hallucinations introduces uncertainty regarding the reliability of the models' predictions, raising major concerns about their practical application. In this paper, we employ information-theory tools to investigate this phenomenon, revealing a fundamental tradeoff between uncertainty and perception. We rigorously analyze the relationship between these two factors, proving that the global minimal uncertainty in generative models grows in tandem with perception. In particular, we define the inherent uncertainty of the restoration problem and show that attaining perfect perceptual quality entails at least twice this uncertainty. Additionally, we establish a relation between mean squared-error distortion, uncertainty and perception, through which we prove the aforementioned uncertainly-perception tradeoff induces the well-known perception-distortion tradeoff. This work uncovers fundamental limitations of generative models in achieving both high perceptual quality and reliable predictions for image restoration. We demonstrate our theoretical findings through an analysis of single image super-resolution algorithms. Our work aims to raise awareness among practitioners about this inherent tradeoff, empowering them to make informed decisions and potentially prioritize safety over perceptual performance.","sentences":["The pursuit of high perceptual quality in image restoration has driven the development of revolutionary generative models, capable of producing results often visually indistinguishable from real data.","However, as their perceptual quality continues to improve, these models also exhibit a growing tendency to generate hallucinations - realistic-looking details that do not exist in the ground truth images.","The presence of hallucinations introduces uncertainty regarding the reliability of the models' predictions, raising major concerns about their practical application.","In this paper, we employ information-theory tools to investigate this phenomenon, revealing a fundamental tradeoff between uncertainty and perception.","We rigorously analyze the relationship between these two factors, proving that the global minimal uncertainty in generative models grows in tandem with perception.","In particular, we define the inherent uncertainty of the restoration problem and show that attaining perfect perceptual quality entails at least twice this uncertainty.","Additionally, we establish a relation between mean squared-error distortion, uncertainty and perception, through which we prove the aforementioned uncertainly-perception tradeoff induces the well-known perception-distortion tradeoff.","This work uncovers fundamental limitations of generative models in achieving both high perceptual quality and reliable predictions for image restoration.","We demonstrate our theoretical findings through an analysis of single image super-resolution algorithms.","Our work aims to raise awareness among practitioners about this inherent tradeoff, empowering them to make informed decisions and potentially prioritize safety over perceptual performance."],"url":"http://arxiv.org/abs/2405.16475v1","category":"cs.LG"}
{"created":"2024-05-26 07:56:30","title":"M$^3$CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought","abstract":"Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge from both textual and visual modalities for step-by-step reasoning, which gains increasing attention. Nevertheless, the current MCoT benchmark still faces some challenges: (1) absence of visual modal reasoning, (2) single-step visual modal reasoning, and (3) Domain missing, thereby hindering the development of MCoT. Motivated by this, we introduce a novel benchmark (M$^3$CoT) to address the above challenges, advancing the multi-domain, multi-step, and multi-modal CoT. Additionally, we conduct a thorough evaluation involving abundant MCoT approaches on Vision Large Language Models (VLLMs). In addition, we highlight that the current VLLMs still struggle to correctly reason in M$^3$CoT and there remains a large gap between existing VLLMs and human performance in M$^3$CoT, despite their superior results on previous MCoT benchmarks. To our knowledge, we take the first meaningful step toward the multi-domain, multi-step, and multi-modal scenario in MCoT. We hope that M$^3$CoT can serve as a valuable resource, providing a pioneering foundation in multi-domain, multi-step, multi-modal chain-of-thought research.","sentences":["Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge from both textual and visual modalities for step-by-step reasoning, which gains increasing attention.","Nevertheless, the current MCoT benchmark still faces some challenges: (1) absence of visual modal reasoning, (2) single-step visual modal reasoning, and (3) Domain missing, thereby hindering the development of MCoT. Motivated by this, we introduce a novel benchmark (M$^3$CoT) to address the above challenges, advancing the multi-domain, multi-step, and multi-modal CoT.","Additionally, we conduct a thorough evaluation involving abundant MCoT approaches on Vision Large Language Models (VLLMs).","In addition, we highlight that the current VLLMs still struggle to correctly reason in M$^3$CoT and there remains a large gap between existing VLLMs and human performance in M$^3$CoT, despite their superior results on previous MCoT benchmarks.","To our knowledge, we take the first meaningful step toward the multi-domain, multi-step, and multi-modal scenario in MCoT.","We hope that M$^3$CoT can serve as a valuable resource, providing a pioneering foundation in multi-domain, multi-step, multi-modal chain-of-thought research."],"url":"http://arxiv.org/abs/2405.16473v1","category":"cs.CV"}
{"created":"2024-05-26 07:08:13","title":"Probabilistic Contrastive Learning with Explicit Concentration on the Hypersphere","abstract":"Self-supervised contrastive learning has predominantly adopted deterministic methods, which are not suited for environments characterized by uncertainty and noise. This paper introduces a new perspective on incorporating uncertainty into contrastive learning by embedding representations within a spherical space, inspired by the von Mises-Fisher distribution (vMF). We introduce an unnormalized form of vMF and leverage the concentration parameter, kappa, as a direct, interpretable measure to quantify uncertainty explicitly. This approach not only provides a probabilistic interpretation of the embedding space but also offers a method to calibrate model confidence against varying levels of data corruption and characteristics. Our empirical results demonstrate that the estimated concentration parameter correlates strongly with the degree of unforeseen data corruption encountered at test time, enables failure analysis, and enhances existing out-of-distribution detection methods.","sentences":["Self-supervised contrastive learning has predominantly adopted deterministic methods, which are not suited for environments characterized by uncertainty and noise.","This paper introduces a new perspective on incorporating uncertainty into contrastive learning by embedding representations within a spherical space, inspired by the von Mises-Fisher distribution (vMF).","We introduce an unnormalized form of vMF and leverage the concentration parameter, kappa, as a direct, interpretable measure to quantify uncertainty explicitly.","This approach not only provides a probabilistic interpretation of the embedding space but also offers a method to calibrate model confidence against varying levels of data corruption and characteristics.","Our empirical results demonstrate that the estimated concentration parameter correlates strongly with the degree of unforeseen data corruption encountered at test time, enables failure analysis, and enhances existing out-of-distribution detection methods."],"url":"http://arxiv.org/abs/2405.16460v1","category":"cs.LG"}
{"created":"2024-05-26 07:06:02","title":"Pattern formation in three-state systems: Towards understanding morphology formation in the presence of evaporation","abstract":"Inspired by experimental evidence collected when processing thin films from ternary solutions made of two solutes, typically polymers, and one solvent, we computationally study the morphology formation of domains obtained in three-state systems using both a lattice model and a continuum counterpart. The lattice-based approach relies on the Blume-Capel nearest neighbor model with bulk conservative Kawasaki dynamics, whereas as continuum system we consider a coupled system of evolution equations that is derived as hydrodynamic limit when replacing the nearest neighbor interaction in the lattice case by a suitable Kac potential. We explore how the obtained morphology depends on the solvent content in the mixture. In particular, we study how these scenarios change when the solvent is allowed to evaporate.","sentences":["Inspired by experimental evidence collected when processing thin films from ternary solutions made of two solutes, typically polymers, and one solvent, we computationally study the morphology formation of domains obtained in three-state systems using both a lattice model and a continuum counterpart.","The lattice-based approach relies on the Blume-Capel nearest neighbor model with bulk conservative Kawasaki dynamics, whereas as continuum system we consider a coupled system of evolution equations that is derived as hydrodynamic limit when replacing the nearest neighbor interaction in the lattice case by a suitable Kac potential.","We explore how the obtained morphology depends on the solvent content in the mixture.","In particular, we study how these scenarios change when the solvent is allowed to evaporate."],"url":"http://arxiv.org/abs/2405.16459v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-26 07:00:12","title":"Dominant Shuffle: A Simple Yet Powerful Data Augmentation for Time-series Prediction","abstract":"Recent studies have suggested frequency-domain Data augmentation (DA) is effec tive for time series prediction. Existing frequency-domain augmentations disturb the original data with various full-spectrum noises, leading to excess domain gap between augmented and original data. Although impressive performance has been achieved in certain cases, frequency-domain DA has yet to be generalized to time series prediction datasets. In this paper, we found that frequency-domain augmentations can be significantly improved by two modifications that limit the perturbations. First, we found that limiting the perturbation to only dominant frequencies significantly outperforms full-spectrum perturbations. Dominant fre quencies represent the main periodicity and trends of the signal and are more important than other frequencies. Second, we found that simply shuffling the dominant frequency components is superior over sophisticated designed random perturbations. Shuffle rearranges the original components (magnitudes and phases) and limits the external noise. With these two modifications, we proposed dominant shuffle, a simple yet effective data augmentation for time series prediction. Our method is very simple yet powerful and can be implemented with just a few lines of code. Extensive experiments with eight datasets and six popular time series models demonstrate that our method consistently improves the baseline performance under various settings and significantly outperforms other DA methods. Code can be accessed at https://kaizhao.net/time-series.","sentences":["Recent studies have suggested frequency-domain Data augmentation (DA) is effec tive for time series prediction.","Existing frequency-domain augmentations disturb the original data with various full-spectrum noises, leading to excess domain gap between augmented and original data.","Although impressive performance has been achieved in certain cases, frequency-domain DA has yet to be generalized to time series prediction datasets.","In this paper, we found that frequency-domain augmentations can be significantly improved by two modifications that limit the perturbations.","First, we found that limiting the perturbation to only dominant frequencies significantly outperforms full-spectrum perturbations.","Dominant fre quencies represent the main periodicity and trends of the signal and are more important than other frequencies.","Second, we found that simply shuffling the dominant frequency components is superior over sophisticated designed random perturbations.","Shuffle rearranges the original components (magnitudes and phases) and limits the external noise.","With these two modifications, we proposed dominant shuffle, a simple yet effective data augmentation for time series prediction.","Our method is very simple yet powerful and can be implemented with just a few lines of code.","Extensive experiments with eight datasets and six popular time series models demonstrate that our method consistently improves the baseline performance under various settings and significantly outperforms other DA methods.","Code can be accessed at https://kaizhao.net/time-series."],"url":"http://arxiv.org/abs/2405.16456v1","category":"cs.LG"}
{"created":"2024-05-26 06:52:56","title":"A Slices Perspective for Incremental Nonparametric Inference in High Dimensional State Spaces","abstract":"We introduce an innovative method for incremental nonparametric probabilistic inference in high-dimensional state spaces. Our approach leverages \\slices from high-dimensional surfaces to efficiently approximate posterior distributions of any shape. Unlike many existing graph-based methods, our \\slices perspective eliminates the need for additional intermediate reconstructions, maintaining a more accurate representation of posterior distributions. Additionally, we propose a novel heuristic to balance between accuracy and efficiency, enabling real-time operation in nonparametric scenarios. In empirical evaluations on synthetic and real-world datasets, our \\slices approach consistently outperforms other state-of-the-art methods. It demonstrates superior accuracy and achieves a significant reduction in computational complexity, often by an order of magnitude.","sentences":["We introduce an innovative method for incremental nonparametric probabilistic inference in high-dimensional state spaces.","Our approach leverages \\slices from high-dimensional surfaces to efficiently approximate posterior distributions of any shape.","Unlike many existing graph-based methods, our \\slices perspective eliminates the need for additional intermediate reconstructions, maintaining a more accurate representation of posterior distributions.","Additionally, we propose a novel heuristic to balance between accuracy and efficiency, enabling real-time operation in nonparametric scenarios.","In empirical evaluations on synthetic and real-world datasets, our \\slices approach consistently outperforms other state-of-the-art methods.","It demonstrates superior accuracy and achieves a significant reduction in computational complexity, often by an order of magnitude."],"url":"http://arxiv.org/abs/2405.16453v1","category":"cs.AI"}
{"created":"2024-05-26 06:42:06","title":"From Macro to Micro: Boosting micro-expression recognition via pre-training on macro-expression videos","abstract":"Micro-expression recognition (MER) has drawn increasing attention in recent years due to its potential applications in intelligent medical and lie detection. However, the shortage of annotated data has been the major obstacle to further improve deep-learning based MER methods. Intuitively, utilizing sufficient macro-expression data to promote MER performance seems to be a feasible solution. However, the facial patterns of macro-expressions and micro-expressions are significantly different, which makes naive transfer learning methods difficult to deploy directly. To tacle this issue, we propose a generalized transfer learning paradigm, called \\textbf{MA}cro-expression \\textbf{TO} \\textbf{MI}cro-expression (MA2MI). Under our paradigm, networks can learns the ability to represent subtle facial movement by reconstructing future frames. In addition, we also propose a two-branch micro-action network (MIACNet) to decouple facial position features and facial action features, which can help the network more accurately locate facial action locations. Extensive experiments on three popular MER benchmarks demonstrate the superiority of our method.","sentences":["Micro-expression recognition (MER) has drawn increasing attention in recent years due to its potential applications in intelligent medical and lie detection.","However, the shortage of annotated data has been the major obstacle to further improve deep-learning based MER methods.","Intuitively, utilizing sufficient macro-expression data to promote MER performance seems to be a feasible solution.","However, the facial patterns of macro-expressions and micro-expressions are significantly different, which makes naive transfer learning methods difficult to deploy directly.","To tacle this issue, we propose a generalized transfer learning paradigm, called \\textbf{MA}cro-expression \\textbf{TO} \\textbf{MI}cro-expression (MA2MI).","Under our paradigm, networks can learns the ability to represent subtle facial movement by reconstructing future frames.","In addition, we also propose a two-branch micro-action network (MIACNet) to decouple facial position features and facial action features, which can help the network more accurately locate facial action locations.","Extensive experiments on three popular MER benchmarks demonstrate the superiority of our method."],"url":"http://arxiv.org/abs/2405.16451v1","category":"cs.CV"}
{"created":"2024-05-26 06:33:48","title":"Synthesizing Programmatic Reinforcement Learning Policies with Large Language Model Guided Search","abstract":"Programmatic reinforcement learning (PRL) has been explored for representing policies through programs as a means to achieve interpretability and generalization. Despite promising outcomes, current state-of-the-art PRL methods are hindered by sample inefficiency, necessitating tens of millions of program-environment interactions. To tackle this challenge, we introduce a novel LLM-guided search framework (LLM-GS). Our key insight is to leverage the programming expertise and common sense reasoning of LLMs to enhance the efficiency of assumption-free, random-guessing search methods. We address the challenge of LLMs' inability to generate precise and grammatically correct programs in domain-specific languages (DSLs) by proposing a Pythonic-DSL strategy - an LLM is instructed to initially generate Python codes and then convert them into DSL programs. To further optimize the LLM-generated programs, we develop a search algorithm named Scheduled Hill Climbing, designed to efficiently explore the programmatic search space to consistently improve the programs. Experimental results in the Karel domain demonstrate the superior effectiveness and efficiency of our LLM-GS framework. Extensive ablation studies further verify the critical role of our Pythonic-DSL strategy and Scheduled Hill Climbing algorithm.","sentences":["Programmatic reinforcement learning (PRL) has been explored for representing policies through programs as a means to achieve interpretability and generalization.","Despite promising outcomes, current state-of-the-art PRL methods are hindered by sample inefficiency, necessitating tens of millions of program-environment interactions.","To tackle this challenge, we introduce a novel LLM-guided search framework (LLM-GS).","Our key insight is to leverage the programming expertise and common sense reasoning of LLMs to enhance the efficiency of assumption-free, random-guessing search methods.","We address the challenge of LLMs' inability to generate precise and grammatically correct programs in domain-specific languages (DSLs) by proposing a Pythonic-DSL strategy - an LLM is instructed to initially generate Python codes and then convert them into DSL programs.","To further optimize the LLM-generated programs, we develop a search algorithm named Scheduled Hill Climbing, designed to efficiently explore the programmatic search space to consistently improve the programs.","Experimental results in the Karel domain demonstrate the superior effectiveness and efficiency of our LLM-GS framework.","Extensive ablation studies further verify the critical role of our Pythonic-DSL strategy and Scheduled Hill Climbing algorithm."],"url":"http://arxiv.org/abs/2405.16450v1","category":"cs.LG"}
{"created":"2024-05-26 05:50:17","title":"MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting","abstract":"In recent years, Transformers have become the de-facto architecture for long-term sequence forecasting (LTSF), but faces challenges such as quadratic complexity and permutation invariant bias. A recent model, Mamba, based on selective state space models (SSMs), has emerged as a competitive alternative to Transformer, offering comparable performance with higher throughput and linear complexity related to sequence length. In this study, we analyze the limitations of current Mamba in LTSF and propose four targeted improvements, leading to MambaTS. We first introduce variable scan along time to arrange the historical information of all the variables together. We suggest that causal convolution in Mamba is not necessary for LTSF and propose the Temporal Mamba Block (TMB). We further incorporate a dropout mechanism for selective parameters of TMB to mitigate model overfitting. Moreover, we tackle the issue of variable scan order sensitivity by introducing variable permutation training. We further propose variable-aware scan along time to dynamically discover variable relationships during training and decode the optimal variable scan order by solving the shortest path visiting all nodes problem during inference. Extensive experiments conducted on eight public datasets demonstrate that MambaTS achieves new state-of-the-art performance.","sentences":["In recent years, Transformers have become the de-facto architecture for long-term sequence forecasting (LTSF), but faces challenges such as quadratic complexity and permutation invariant bias.","A recent model, Mamba, based on selective state space models (SSMs), has emerged as a competitive alternative to Transformer, offering comparable performance with higher throughput and linear complexity related to sequence length.","In this study, we analyze the limitations of current Mamba in LTSF and propose four targeted improvements, leading to MambaTS.","We first introduce variable scan along time to arrange the historical information of all the variables together.","We suggest that causal convolution in Mamba is not necessary for LTSF and propose the Temporal Mamba Block (TMB).","We further incorporate a dropout mechanism for selective parameters of TMB to mitigate model overfitting.","Moreover, we tackle the issue of variable scan order sensitivity by introducing variable permutation training.","We further propose variable-aware scan along time to dynamically discover variable relationships during training and decode the optimal variable scan order by solving the shortest path visiting all nodes problem during inference.","Extensive experiments conducted on eight public datasets demonstrate that MambaTS achieves new state-of-the-art performance."],"url":"http://arxiv.org/abs/2405.16440v1","category":"cs.LG"}
{"created":"2024-05-26 05:48:21","title":"Towards Imitation Learning in Real World Unstructured Social Mini-Games in Pedestrian Crowds","abstract":"Imitation Learning (IL) strategies are used to generate policies for robot motion planning and navigation by learning from human trajectories. Recently, there has been a lot of excitement in applying IL in social interactions arising in urban environments such as university campuses, restaurants, grocery stores, and hospitals. However, obtaining numerous expert demonstrations in social settings might be expensive, risky, or even impossible. Current approaches therefore, focus only on simulated social interaction scenarios. This raises the question: \\textit{How can a robot learn to imitate an expert demonstrator from real world multi-agent social interaction scenarios}? It remains unknown which, if any, IL methods perform well and what assumptions they require. We benchmark representative IL methods in real world social interaction scenarios on a motion planning task, using a novel pedestrian intersection dataset collected at the University of Texas at Austin campus. Our evaluation reveals two key findings: first, learning multi-agent cost functions is required for learning the diverse behavior modes of agents in tightly coupled interactions and second, conditioning the training of IL methods on partial state information or providing global information in simulation can improve imitation learning, especially in real world social interaction scenarios.","sentences":["Imitation Learning (IL) strategies are used to generate policies for robot motion planning and navigation by learning from human trajectories.","Recently, there has been a lot of excitement in applying IL in social interactions arising in urban environments such as university campuses, restaurants, grocery stores, and hospitals.","However, obtaining numerous expert demonstrations in social settings might be expensive, risky, or even impossible.","Current approaches therefore, focus only on simulated social interaction scenarios.","This raises the question: \\textit{How can a robot learn to imitate an expert demonstrator from real world multi-agent social interaction scenarios}?","It remains unknown which, if any, IL methods perform well and what assumptions they require.","We benchmark representative IL methods in real world social interaction scenarios on a motion planning task, using a novel pedestrian intersection dataset collected at the University of Texas at Austin campus.","Our evaluation reveals two key findings: first, learning multi-agent cost functions is required for learning the diverse behavior modes of agents in tightly coupled interactions and second, conditioning the training of IL methods on partial state information or providing global information in simulation can improve imitation learning, especially in real world social interaction scenarios."],"url":"http://arxiv.org/abs/2405.16439v1","category":"cs.RO"}
{"created":"2024-05-26 05:38:50","title":"Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer","abstract":"Aligning generative models with human preference via RLHF typically suffers from overoptimization, where an imperfectly learned reward model can misguide the generative model to output undesired responses. We investigate this problem in a principled manner by identifying the source of the misalignment as a form of distributional shift and uncertainty in learning human preferences. To mitigate overoptimization, we first propose a theoretical algorithm that chooses the best policy for an adversarially chosen reward model; one that simultaneously minimizes the maximum likelihood estimation of the loss and a reward penalty term. Here, the reward penalty term is introduced to prevent the policy from choosing actions with spurious high proxy rewards, resulting in provable sample efficiency of the algorithm under a partial coverage style condition. Moving from theory to practice, the proposed algorithm further enjoys an equivalent but surprisingly easy-to-implement reformulation. Using the equivalence between reward models and the corresponding optimal policy, the algorithm features a simple objective that combines: (i) a preference optimization loss that directly aligns the policy with human preference, and (ii) a supervised learning loss that explicitly imitates the policy with a (suitable) baseline distribution. In the context of aligning large language models (LLM), this objective fuses the direct preference optimization (DPO) loss with the supervised fune-tuning (SFT) loss to help mitigate the overoptimization towards undesired responses, for which we name the algorithm Regularized Preference Optimization (RPO). Experiments of aligning LLMs demonstrate the improved performance of RPO compared with DPO baselines. Our work sheds light on the interplay between preference optimization and SFT in tuning LLMs with both theoretical guarantees and empirical evidence.","sentences":["Aligning generative models with human preference via RLHF typically suffers from overoptimization, where an imperfectly learned reward model can misguide the generative model to output undesired responses.","We investigate this problem in a principled manner by identifying the source of the misalignment as a form of distributional shift and uncertainty in learning human preferences.","To mitigate overoptimization, we first propose a theoretical algorithm that chooses the best policy for an adversarially chosen reward model; one that simultaneously minimizes the maximum likelihood estimation of the loss and a reward penalty term.","Here, the reward penalty term is introduced to prevent the policy from choosing actions with spurious high proxy rewards, resulting in provable sample efficiency of the algorithm under a partial coverage style condition.","Moving from theory to practice, the proposed algorithm further enjoys an equivalent but surprisingly easy-to-implement reformulation.","Using the equivalence between reward models and the corresponding optimal policy, the algorithm features a simple objective that combines: (i) a preference optimization loss that directly aligns the policy with human preference, and (ii) a supervised learning loss that explicitly imitates the policy with a (suitable) baseline distribution.","In the context of aligning large language models (LLM), this objective fuses the direct preference optimization (DPO) loss with the supervised fune-tuning (SFT) loss to help mitigate the overoptimization towards undesired responses, for which we name the algorithm Regularized Preference Optimization (RPO).","Experiments of aligning LLMs demonstrate the improved performance of RPO compared with DPO baselines.","Our work sheds light on the interplay between preference optimization and SFT in tuning LLMs with both theoretical guarantees and empirical evidence."],"url":"http://arxiv.org/abs/2405.16436v1","category":"cs.LG"}
{"created":"2024-05-26 05:22:35","title":"The Importance of Directional Feedback for LLM-based Optimizers","abstract":"We study the potential of using large language models (LLMs) as an interactive optimizer for solving maximization problems in a text space using natural language and numerical feedback. Inspired by the classical optimization literature, we classify the natural language feedback into directional and non-directional, where the former is a generalization of the first-order feedback to the natural language space. We find that LLMs are especially capable of optimization when they are provided with {directional feedback}. Based on this insight, we design a new LLM-based optimizer that synthesizes directional feedback from the historical optimization trace to achieve reliable improvement over iterations. Empirically, we show our LLM-based optimizer is more stable and efficient in solving optimization problems, from maximizing mathematical functions to optimizing prompts for writing poems, compared with existing techniques.","sentences":["We study the potential of using large language models (LLMs) as an interactive optimizer for solving maximization problems in a text space using natural language and numerical feedback.","Inspired by the classical optimization literature, we classify the natural language feedback into directional and non-directional, where the former is a generalization of the first-order feedback to the natural language space.","We find that LLMs are especially capable of optimization when they are provided with {directional feedback}.","Based on this insight, we design a new LLM-based optimizer that synthesizes directional feedback from the historical optimization trace to achieve reliable improvement over iterations.","Empirically, we show our LLM-based optimizer is more stable and efficient in solving optimization problems, from maximizing mathematical functions to optimizing prompts for writing poems, compared with existing techniques."],"url":"http://arxiv.org/abs/2405.16434v1","category":"cs.AI"}
{"created":"2024-05-26 05:18:00","title":"CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling","abstract":"Using large language models (LLMs) to assist psychological counseling is a significant but challenging task at present. Attempts have been made on improving empathetic conversations or acting as effective assistants in the treatment with LLMs. However, the existing datasets lack consulting knowledge, resulting in LLMs lacking professional consulting competence. Moreover, how to automatically evaluate multi-turn dialogues within the counseling process remains an understudied area. To bridge the gap, we propose CPsyCoun, a report-based multi-turn dialogue reconstruction and evaluation framework for Chinese psychological counseling. To fully exploit psychological counseling reports, a two-phase approach is devised to construct high-quality dialogues while a comprehensive evaluation benchmark is developed for the effective automatic evaluation of multi-turn psychological consultations. Competitive experimental results demonstrate the effectiveness of our proposed framework in psychological counseling. We open-source the datasets and model for future research at https://github.com/CAS-SIAT-XinHai/CPsyCoun","sentences":["Using large language models (LLMs) to assist psychological counseling is a significant but challenging task at present.","Attempts have been made on improving empathetic conversations or acting as effective assistants in the treatment with LLMs.","However, the existing datasets lack consulting knowledge, resulting in LLMs lacking professional consulting competence.","Moreover, how to automatically evaluate multi-turn dialogues within the counseling process remains an understudied area.","To bridge the gap, we propose CPsyCoun, a report-based multi-turn dialogue reconstruction and evaluation framework for Chinese psychological counseling.","To fully exploit psychological counseling reports, a two-phase approach is devised to construct high-quality dialogues while a comprehensive evaluation benchmark is developed for the effective automatic evaluation of multi-turn psychological consultations.","Competitive experimental results demonstrate the effectiveness of our proposed framework in psychological counseling.","We open-source the datasets and model for future research at https://github.com/CAS-SIAT-XinHai/CPsyCoun"],"url":"http://arxiv.org/abs/2405.16433v1","category":"cs.CL"}
{"created":"2024-05-26 05:03:06","title":"GAMEOPT+: Improving Fuel Efficiency in Unregulated Heterogeneous Traffic Intersections via Optimal Multi-agent Cooperative Control","abstract":"Better fuel efficiency leads to better financial security as well as a cleaner environment. We propose a novel approach for improving fuel efficiency in unstructured and unregulated traffic environments. Existing intelligent transportation solutions for improving fuel efficiency, however, apply only to traffic intersections with sparse traffic or traffic where drivers obey the regulations, or both. We propose GameOpt+, a novel hybrid approach for cooperative intersection control in dynamic, multi-lane, unsignalized intersections. GameOpt+ is a hybrid solution that combines an auction mechanism and an optimization-based trajectory planner. It generates a priority entrance sequence for each agent and computes velocity controls in real-time, taking less than 10 milliseconds even in high-density traffic with over 10,000 vehicles per hour. Compared to fully optimization-based methods, it operates 100 times faster while ensuring fairness, safety, and efficiency. Tested on the SUMO simulator, our algorithm improves throughput by at least 25%, reduces the time to reach the goal by at least 70%, and decreases fuel consumption by 50% compared to auction-based and signaled approaches using traffic lights and stop signs. GameOpt+ is also unaffected by unbalanced traffic inflows, whereas some of the other baselines encountered a decrease in performance in unbalanced traffic inflow environments.","sentences":["Better fuel efficiency leads to better financial security as well as a cleaner environment.","We propose a novel approach for improving fuel efficiency in unstructured and unregulated traffic environments.","Existing intelligent transportation solutions for improving fuel efficiency, however, apply only to traffic intersections with sparse traffic or traffic where drivers obey the regulations, or both.","We propose GameOpt+, a novel hybrid approach for cooperative intersection control in dynamic, multi-lane, unsignalized intersections.","GameOpt+ is a hybrid solution that combines an auction mechanism and an optimization-based trajectory planner.","It generates a priority entrance sequence for each agent and computes velocity controls in real-time, taking less than 10 milliseconds even in high-density traffic with over 10,000 vehicles per hour.","Compared to fully optimization-based methods, it operates 100 times faster while ensuring fairness, safety, and efficiency.","Tested on the SUMO simulator, our algorithm improves throughput by at least 25%, reduces the time to reach the goal by at least 70%, and decreases fuel consumption by 50% compared to auction-based and signaled approaches using traffic lights and stop signs.","GameOpt+ is also unaffected by unbalanced traffic inflows, whereas some of the other baselines encountered a decrease in performance in unbalanced traffic inflow environments."],"url":"http://arxiv.org/abs/2405.16430v1","category":"cs.RO"}
{"created":"2024-05-26 04:41:17","title":"Segmentation of Maya hieroglyphs through fine-tuned foundation models","abstract":"The study of Maya hieroglyphic writing unlocks the rich history of cultural and societal knowledge embedded within this ancient civilization's visual narrative. Artificial Intelligence (AI) offers a novel lens through which we can translate these inscriptions, with the potential to allow non-specialists access to reading these texts and to aid in the decipherment of those hieroglyphs which continue to elude comprehensive interpretation. Toward this, we leverage a foundational model to segment Maya hieroglyphs from an open-source digital library dedicated to Maya artifacts. Despite the initial promise of publicly available foundational segmentation models, their effectiveness in accurately segmenting Maya hieroglyphs was initially limited. Addressing this challenge, our study involved the meticulous curation of image and label pairs with the assistance of experts in Maya art and history, enabling the fine-tuning of these foundational models. This process significantly enhanced model performance, illustrating the potential of fine-tuning approaches and the value of our expanding dataset. We plan to open-source this dataset for encouraging future research, and eventually to help make the hieroglyphic texts legible to a broader community, particularly for Maya heritage community members.","sentences":["The study of Maya hieroglyphic writing unlocks the rich history of cultural and societal knowledge embedded within this ancient civilization's visual narrative.","Artificial Intelligence (AI) offers a novel lens through which we can translate these inscriptions, with the potential to allow non-specialists access to reading these texts and to aid in the decipherment of those hieroglyphs which continue to elude comprehensive interpretation.","Toward this, we leverage a foundational model to segment Maya hieroglyphs from an open-source digital library dedicated to Maya artifacts.","Despite the initial promise of publicly available foundational segmentation models, their effectiveness in accurately segmenting Maya hieroglyphs was initially limited.","Addressing this challenge, our study involved the meticulous curation of image and label pairs with the assistance of experts in Maya art and history, enabling the fine-tuning of these foundational models.","This process significantly enhanced model performance, illustrating the potential of fine-tuning approaches and the value of our expanding dataset.","We plan to open-source this dataset for encouraging future research, and eventually to help make the hieroglyphic texts legible to a broader community, particularly for Maya heritage community members."],"url":"http://arxiv.org/abs/2405.16426v1","category":"cs.CV"}
{"created":"2024-05-26 04:30:17","title":"Improving Health Professionals' Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision Making","abstract":"With advanced AI/ML, there has been growing research on explainable AI (XAI) and studies on how humans interact with AI and XAI for effective human-AI collaborative decision-making. However, we still have a lack of understanding of how AI systems and XAI should be first presented to users without technical backgrounds. In this paper, we present the findings of semi-structured interviews with health professionals (n=12) and students (n=4) majoring in medicine and health to study how to improve onboarding with AI and XAI. For the interviews, we built upon human-AI interaction guidelines to create onboarding materials of an AI system for stroke rehabilitation assessment and AI explanations and introduce them to the participants. Our findings reveal that beyond presenting traditional performance metrics on AI, participants desired benchmark information, the practical benefits of AI, and interaction trials to better contextualize AI performance, and refine the objectives and performance of AI. Based on these findings, we highlight directions for improving onboarding with AI and XAI and human-AI collaborative decision-making.","sentences":["With advanced AI/ML, there has been growing research on explainable AI (XAI) and studies on how humans interact with AI and XAI for effective human-AI collaborative decision-making.","However, we still have a lack of understanding of how AI systems and XAI should be first presented to users without technical backgrounds.","In this paper, we present the findings of semi-structured interviews with health professionals (n=12) and students (n=4) majoring in medicine and health to study how to improve onboarding with AI and XAI.","For the interviews, we built upon human-AI interaction guidelines to create onboarding materials of an AI system for stroke rehabilitation assessment and AI explanations and introduce them to the participants.","Our findings reveal that beyond presenting traditional performance metrics on AI, participants desired benchmark information, the practical benefits of AI, and interaction trials to better contextualize AI performance, and refine the objectives and performance of AI.","Based on these findings, we highlight directions for improving onboarding with AI and XAI and human-AI collaborative decision-making."],"url":"http://arxiv.org/abs/2405.16424v1","category":"cs.HC"}
{"created":"2024-05-26 04:26:07","title":"AI-Generated Text Detection and Classification Based on BERT Deep Learning Algorithm","abstract":"AI-generated text detection plays an increasingly important role in various fields. In this study, we developed an efficient AI-generated text detection model based on the BERT algorithm, which provides new ideas and methods for solving related problems. In the data preprocessing stage, a series of steps were taken to process the text, including operations such as converting to lowercase, word splitting, removing stop words, stemming extraction, removing digits, and eliminating redundant spaces, to ensure data quality and accuracy. By dividing the dataset into a training set and a test set in the ratio of 60% and 40%, and observing the changes in the accuracy and loss values during the training process, we found that the model performed well during the training process. The accuracy increases steadily from the initial 94.78% to 99.72%, while the loss value decreases from 0.261 to 0.021 and converges gradually, which indicates that the BERT model is able to detect AI-generated text with high accuracy and the prediction results are gradually approaching the real classification results. Further analysis of the results of the training and test sets reveals that in terms of loss value, the average loss of the training set is 0.0565, while the average loss of the test set is 0.0917, showing a slightly higher loss value. As for the accuracy, the average accuracy of the training set reaches 98.1%, while the average accuracy of the test set is 97.71%, which is not much different from each other, indicating that the model has good generalisation ability. In conclusion, the AI-generated text detection model based on the BERT algorithm proposed in this study shows high accuracy and stability in experiments, providing an effective solution for related fields.","sentences":["AI-generated text detection plays an increasingly important role in various fields.","In this study, we developed an efficient AI-generated text detection model based on the BERT algorithm, which provides new ideas and methods for solving related problems.","In the data preprocessing stage, a series of steps were taken to process the text, including operations such as converting to lowercase, word splitting, removing stop words, stemming extraction, removing digits, and eliminating redundant spaces, to ensure data quality and accuracy.","By dividing the dataset into a training set and a test set in the ratio of 60% and 40%, and observing the changes in the accuracy and loss values during the training process, we found that the model performed well during the training process.","The accuracy increases steadily from the initial 94.78% to 99.72%, while the loss value decreases from 0.261 to 0.021 and converges gradually, which indicates that the BERT model is able to detect AI-generated text with high accuracy and the prediction results are gradually approaching the real classification results.","Further analysis of the results of the training and test sets reveals that in terms of loss value, the average loss of the training set is 0.0565, while the average loss of the test set is 0.0917, showing a slightly higher loss value.","As for the accuracy, the average accuracy of the training set reaches 98.1%, while the average accuracy of the test set is 97.71%, which is not much different from each other, indicating that the model has good generalisation ability.","In conclusion, the AI-generated text detection model based on the BERT algorithm proposed in this study shows high accuracy and stability in experiments, providing an effective solution for related fields."],"url":"http://arxiv.org/abs/2405.16422v1","category":"cs.CL"}
{"created":"2024-05-26 04:05:01","title":"Towards Sustainable IoT: Challenges, Solutions, and Future Directions for Device Longevity","abstract":"In an era dominated by the Internet of Things, ensuring the longevity and sustainability of IoT devices has emerged as a pressing concern. This study explores the various complex difficulties which contributed to the early decommissioning of IoT devices and suggests methods to improve their lifespan management. By examining factors such as security vulnerabilities, user awareness gaps, and the influence of fashion-driven technology trends, the paper underscores the need for legislative interventions, consumer education, and industry accountability. Additionally, it explores innovative approaches to improving IoT longevity, including the integration of sustainability considerations into architectural design through requirements engineering methodologies. Furthermore, the paper discusses the potential of distributed ledger technology, or blockchain, to promote transparent and decentralized processes for device provisioning and tracking. This study promotes a sustainable IoT ecosystem by integrating technology innovation, legal change, and social awareness to reduce environmental impact and enhance resilience for the digital future","sentences":["In an era dominated by the Internet of Things, ensuring the longevity and sustainability of IoT devices has emerged as a pressing concern.","This study explores the various complex difficulties which contributed to the early decommissioning of IoT devices and suggests methods to improve their lifespan management.","By examining factors such as security vulnerabilities, user awareness gaps, and the influence of fashion-driven technology trends, the paper underscores the need for legislative interventions, consumer education, and industry accountability.","Additionally, it explores innovative approaches to improving IoT longevity, including the integration of sustainability considerations into architectural design through requirements engineering methodologies.","Furthermore, the paper discusses the potential of distributed ledger technology, or blockchain, to promote transparent and decentralized processes for device provisioning and tracking.","This study promotes a sustainable IoT ecosystem by integrating technology innovation, legal change, and social awareness to reduce environmental impact and enhance resilience for the digital future"],"url":"http://arxiv.org/abs/2405.16421v1","category":"cs.CR"}
{"created":"2024-05-26 03:41:40","title":"Enhancing Feature Diversity Boosts Channel-Adaptive Vision Transformers","abstract":"Multi-Channel Imaging (MCI) contains an array of challenges for encoding useful feature representations not present in traditional images. For example, images from two different satellites may both contain RGB channels, but the remaining channels can be different for each imaging source. Thus, MCI models must support a variety of channel configurations at test time. Recent work has extended traditional visual encoders for MCI, such as Vision Transformers (ViT), by supplementing pixel information with an encoding representing the channel configuration. However, these methods treat each channel equally, i.e., they do not consider the unique properties of each channel type, which can result in needless and potentially harmful redundancies in the learned features. For example, if RGB channels are always present, the other channels can focus on extracting information that cannot be captured by the RGB channels. To this end, we propose DiChaViT, which aims to enhance the diversity in the learned features of MCI-ViT models. This is achieved through a novel channel sampling strategy that encourages the selection of more distinct channel sets for training. Additionally, we employ regularization and initialization techniques to increase the likelihood that new information is learned from each channel. Many of our improvements are architecture agnostic and could be incorporated into new architectures as they are developed. Experiments on both satellite and cell microscopy datasets, CHAMMI, JUMP-CP, and So2Sat, report DiChaViT yields a 1.5-5.0% gain over the state-of-the-art.","sentences":["Multi-Channel Imaging (MCI) contains an array of challenges for encoding useful feature representations not present in traditional images.","For example, images from two different satellites may both contain RGB channels, but the remaining channels can be different for each imaging source.","Thus, MCI models must support a variety of channel configurations at test time.","Recent work has extended traditional visual encoders for MCI, such as Vision Transformers (ViT), by supplementing pixel information with an encoding representing the channel configuration.","However, these methods treat each channel equally, i.e., they do not consider the unique properties of each channel type, which can result in needless and potentially harmful redundancies in the learned features.","For example, if RGB channels are always present, the other channels can focus on extracting information that cannot be captured by the RGB channels.","To this end, we propose DiChaViT, which aims to enhance the diversity in the learned features of MCI-ViT models.","This is achieved through a novel channel sampling strategy that encourages the selection of more distinct channel sets for training.","Additionally, we employ regularization and initialization techniques to increase the likelihood that new information is learned from each channel.","Many of our improvements are architecture agnostic and could be incorporated into new architectures as they are developed.","Experiments on both satellite and cell microscopy datasets, CHAMMI, JUMP-CP, and So2Sat, report DiChaViT yields a 1.5-5.0% gain over the state-of-the-art."],"url":"http://arxiv.org/abs/2405.16419v1","category":"cs.CV"}
{"created":"2024-05-26 03:32:27","title":"Unraveling the Smoothness Properties of Diffusion Models: A Gaussian Mixture Perspective","abstract":"Diffusion models have made rapid progress in generating high-quality samples across various domains. However, a theoretical understanding of the Lipschitz continuity and second momentum properties of the diffusion process is still lacking. In this paper, we bridge this gap by providing a detailed examination of these smoothness properties for the case where the target data distribution is a mixture of Gaussians, which serves as a universal approximator for smooth densities such as image data. We prove that if the target distribution is a $k$-mixture of Gaussians, the density of the entire diffusion process will also be a $k$-mixture of Gaussians. We then derive tight upper bounds on the Lipschitz constant and second momentum that are independent of the number of mixture components $k$. Finally, we apply our analysis to various diffusion solvers, both SDE and ODE based, to establish concrete error guarantees in terms of the total variation distance and KL divergence between the target and learned distributions. Our results provide deeper theoretical insights into the dynamics of the diffusion process under common data distributions.","sentences":["Diffusion models have made rapid progress in generating high-quality samples across various domains.","However, a theoretical understanding of the Lipschitz continuity and second momentum properties of the diffusion process is still lacking.","In this paper, we bridge this gap by providing a detailed examination of these smoothness properties for the case where the target data distribution is a mixture of Gaussians, which serves as a universal approximator for smooth densities such as image data.","We prove that if the target distribution is a $k$-mixture of Gaussians, the density of the entire diffusion process will also be a $k$-mixture of Gaussians.","We then derive tight upper bounds on the Lipschitz constant and second momentum that are independent of the number of mixture components $k$.","Finally, we apply our analysis to various diffusion solvers, both SDE and ODE based, to establish concrete error guarantees in terms of the total variation distance and KL divergence between the target and learned distributions.","Our results provide deeper theoretical insights into the dynamics of the diffusion process under common data distributions."],"url":"http://arxiv.org/abs/2405.16418v1","category":"cs.LG"}
{"created":"2024-05-26 03:05:10","title":"Augmented Risk Prediction for the Onset of Alzheimer's Disease from Electronic Health Records with Large Language Models","abstract":"Alzheimer's disease (AD) is the fifth-leading cause of death among Americans aged 65 and older. Screening and early detection of AD and related dementias (ADRD) are critical for timely intervention and for identifying clinical trial participants. The widespread adoption of electronic health records (EHRs) offers an important resource for developing ADRD screening tools such as machine learning based predictive models. Recent advancements in large language models (LLMs) demonstrate their unprecedented capability of encoding knowledge and performing reasoning, which offers them strong potential for enhancing risk prediction. This paper proposes a novel pipeline that augments risk prediction by leveraging the few-shot inference power of LLMs to make predictions on cases where traditional supervised learning methods (SLs) may not excel. Specifically, we develop a collaborative pipeline that combines SLs and LLMs via a confidence-driven decision-making mechanism, leveraging the strengths of SLs in clear-cut cases and LLMs in more complex scenarios. We evaluate this pipeline using a real-world EHR data warehouse from Oregon Health \\& Science University (OHSU) Hospital, encompassing EHRs from over 2.5 million patients and more than 20 million patient encounters. Our results show that our proposed approach effectively combines the power of SLs and LLMs, offering significant improvements in predictive performance. This advancement holds promise for revolutionizing ADRD screening and early detection practices, with potential implications for better strategies of patient management and thus improving healthcare.","sentences":["Alzheimer's disease (AD) is the fifth-leading cause of death among Americans aged 65 and older.","Screening and early detection of AD and related dementias (ADRD) are critical for timely intervention and for identifying clinical trial participants.","The widespread adoption of electronic health records (EHRs) offers an important resource for developing ADRD screening tools such as machine learning based predictive models.","Recent advancements in large language models (LLMs) demonstrate their unprecedented capability of encoding knowledge and performing reasoning, which offers them strong potential for enhancing risk prediction.","This paper proposes a novel pipeline that augments risk prediction by leveraging the few-shot inference power of LLMs to make predictions on cases where traditional supervised learning methods (SLs) may not excel.","Specifically, we develop a collaborative pipeline that combines SLs and LLMs via a confidence-driven decision-making mechanism, leveraging the strengths of SLs in clear-cut cases and LLMs in more complex scenarios.","We evaluate this pipeline using a real-world EHR data warehouse from Oregon Health \\& Science University (OHSU) Hospital, encompassing EHRs from over 2.5 million patients and more than 20 million patient encounters.","Our results show that our proposed approach effectively combines the power of SLs and LLMs, offering significant improvements in predictive performance.","This advancement holds promise for revolutionizing ADRD screening and early detection practices, with potential implications for better strategies of patient management and thus improving healthcare."],"url":"http://arxiv.org/abs/2405.16413v1","category":"cs.AI"}
{"created":"2024-05-26 02:59:13","title":"Tensor Attention Training: Provably Efficient Learning of Higher-order Transformers","abstract":"Tensor Attention, a multi-view attention that is able to capture high-order correlations among multiple modalities, can overcome the representational limitations of classical matrix attention. However, the $\\Omega(n^3)$ time complexity of tensor attention poses a significant obstacle to its practical implementation in transformers, where $n$ is the input sequence length. In this work, we prove that the backward gradient of tensor attention training can be computed in almost linear $n^{1+o(1)}$ time, the same complexity as its forward computation under a bounded entries assumption. We provide a closed-form solution for the gradient and propose a fast computation method utilizing polynomial approximation methods and tensor algebraic tricks. Furthermore, we prove the necessity and tightness of our assumption through hardness analysis, showing that slightly weakening it renders the gradient problem unsolvable in truly subcubic time. Our theoretical results establish the feasibility of efficient higher-order transformer training and may facilitate practical applications of tensor attention architectures.","sentences":["Tensor Attention, a multi-view attention that is able to capture high-order correlations among multiple modalities, can overcome the representational limitations of classical matrix attention.","However, the $\\Omega(n^3)$ time complexity of tensor attention poses a significant obstacle to its practical implementation in transformers, where $n$ is the input sequence length.","In this work, we prove that the backward gradient of tensor attention training can be computed in almost linear $n^{1+o(1)}$ time, the same complexity as its forward computation under a bounded entries assumption.","We provide a closed-form solution for the gradient and propose a fast computation method utilizing polynomial approximation methods and tensor algebraic tricks.","Furthermore, we prove the necessity and tightness of our assumption through hardness analysis, showing that slightly weakening it renders the gradient problem unsolvable in truly subcubic time.","Our theoretical results establish the feasibility of efficient higher-order transformer training and may facilitate practical applications of tensor attention architectures."],"url":"http://arxiv.org/abs/2405.16411v1","category":"cs.LG"}
{"created":"2024-05-27 17:59:59","title":"Crystalline invariants of fractional Chern insulators","abstract":"In the presence of crystalline symmetry, topologically ordered states can acquire a host of symmetry-protected invariants. These determine the patterns of crystalline symmetry fractionalization of the anyons in addition to fractionally quantized responses to lattice defects. Here we show how ground state expectation values of partial rotations centered at high symmetry points can be used to extract crystalline invariants. Using methods from conformal field theory and G-crossed braided tensor categories, we develop a theory of invariants obtained from partial rotations, which apply to both Abelian and non-Abelian topological orders. We then perform numerical Monte Carlo calculations for projected parton wave functions of fractional Chern insulators, demonstrating remarkable agreement between theory and numerics. For the topological orders we consider, we show that the Hall conductivity, filling fraction, and partial rotation invariants fully characterize the crystalline invariants of the system. Our results also yield invariants of continuum fractional quantum Hall states protected by spatial rotational symmetry.","sentences":["In the presence of crystalline symmetry, topologically ordered states can acquire a host of symmetry-protected invariants.","These determine the patterns of crystalline symmetry fractionalization of the anyons in addition to fractionally quantized responses to lattice defects.","Here we show how ground state expectation values of partial rotations centered at high symmetry points can be used to extract crystalline invariants.","Using methods from conformal field theory and G-crossed braided tensor categories, we develop a theory of invariants obtained from partial rotations, which apply to both Abelian and non-Abelian topological orders.","We then perform numerical Monte Carlo calculations for projected parton wave functions of fractional Chern insulators, demonstrating remarkable agreement between theory and numerics.","For the topological orders we consider, we show that the Hall conductivity, filling fraction, and partial rotation invariants fully characterize the crystalline invariants of the system.","Our results also yield invariants of continuum fractional quantum Hall states protected by spatial rotational symmetry."],"url":"http://arxiv.org/abs/2405.17431v1","category":"cond-mat.str-el"}
{"created":"2024-05-27 17:59:07","title":"MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds","abstract":"We introduce 4D Motion Scaffolds (MoSca), a neural information processing system designed to reconstruct and synthesize novel views of dynamic scenes from monocular videos captured casually in the wild. To address such a challenging and ill-posed inverse problem, we leverage prior knowledge from foundational vision models, lift the video data to a novel Motion Scaffold (MoSca) representation, which compactly and smoothly encodes the underlying motions / deformations. The scene geometry and appearance are then disentangled from the deformation field, and are encoded by globally fusing the Gaussians anchored onto the MoSca and optimized via Gaussian Splatting. Additionally, camera poses can be seamlessly initialized and refined during the dynamic rendering process, without the need for other pose estimation tools. Experiments demonstrate state-of-the-art performance on dynamic rendering benchmarks.","sentences":["We introduce 4D Motion Scaffolds (MoSca), a neural information processing system designed to reconstruct and synthesize novel views of dynamic scenes from monocular videos captured casually in the wild.","To address such a challenging and ill-posed inverse problem, we leverage prior knowledge from foundational vision models, lift the video data to a novel Motion Scaffold (MoSca) representation, which compactly and smoothly encodes the underlying motions / deformations.","The scene geometry and appearance are then disentangled from the deformation field, and are encoded by globally fusing the Gaussians anchored onto the MoSca and optimized via Gaussian Splatting.","Additionally, camera poses can be seamlessly initialized and refined during the dynamic rendering process, without the need for other pose estimation tools.","Experiments demonstrate state-of-the-art performance on dynamic rendering benchmarks."],"url":"http://arxiv.org/abs/2405.17421v1","category":"cs.CV"}
{"created":"2024-05-27 17:58:18","title":"Splitting aspects of holomorphic distributions with locally free tangent sheaf","abstract":"In this work, we mainly deal with a two-dimensional singular holomorphic distribution $\\mathcal{D}$ defined on $M$, in the two situations $M=\\mathbb{P}^n$ or $M=(\\mathbb{C}^n,0)$, tangent to a one-dimensional foliation $\\mathcal{G}$ on $M$, and whose tangent sheaf $T_{\\mathcal{D}}$ is locally free. We provide sufficient conditions on $\\mathcal{G}$ so that there is another one-dimensional foliation $\\mathcal{H}$ on $M$ tangent to $\\mathcal{D}$, such that their respective tangent sheaves satisfy the splitting relation $T_{\\mathcal{D}}=T_{\\mathcal{G}} \\oplus T_{\\mathcal{H}}$. As an application, we show that if $\\mathcal{F}$ is a codimension one holomorphic foliation on $\\mathbb{P}^3$ with locally free tangent sheaf and tangent to a nontrivial holomorphic vector field on $\\mathbb{P}^3$, then $T_{\\mathcal{F}}$ splits. Some results on division of holomorphic differential forms by tangent vector fields are also obtained.","sentences":["In this work, we mainly deal with a two-dimensional singular holomorphic distribution $\\mathcal{D}$ defined on $M$, in the two situations $M=\\mathbb{P}^n$ or $M=(\\mathbb{C}^n,0)$, tangent to a one-dimensional foliation $\\mathcal{G}$ on $M$, and whose tangent sheaf $T_{\\mathcal{D}}$ is locally free.","We provide sufficient conditions on $\\mathcal{G}$ so that there is another one-dimensional foliation $\\mathcal{H}$ on $M$ tangent to $\\mathcal{D}$, such that their respective tangent sheaves satisfy the splitting relation $T_{\\mathcal{D}}=T_{\\mathcal{G}} \\oplus T_{\\mathcal{H}}$. As an application, we show that if $\\mathcal{F}$ is a codimension one holomorphic foliation on $\\mathbb{P}^3$ with locally free tangent sheaf and tangent to a nontrivial holomorphic vector field on $\\mathbb{P}^3$, then $T_{\\mathcal{F}}$ splits.","Some results on division of holomorphic differential forms by tangent vector fields are also obtained."],"url":"http://arxiv.org/abs/2405.17415v1","category":"math.CV"}
{"created":"2024-05-27 17:48:32","title":"CrEIMBO: Cross Ensemble Interactions in Multi-view Brain Observations","abstract":"Modern recordings of neural activity provide diverse observations of neurons across brain areas, behavioral conditions, and subjects -- thus presenting an exciting opportunity to reveal the fundamentals of brain-wide dynamics underlying cognitive function. Current methods, however, often fail to fully harness the richness of such data as they either provide an uninterpretable representation (e.g., via \"black box\" deep networks) or over-simplify the model (e.g., assume stationary dynamics or analyze each session independently). Here, instead of regarding asynchronous recordings that lack alignment in neural identity or brain areas as a limitation, we exploit these diverse views of the same brain system to learn a unified model of brain dynamics. We assume that brain observations stem from the joint activity of a set of functional neural ensembles (groups of co-active neurons) that are similar in functionality across recordings, and propose to discover the ensemble and their non-stationary dynamical interactions in a new model we term CrEIMBO (Cross-Ensemble Interactions in Multi-view Brain Observations). CrEIMBO identifies the composition of the per-session neural ensembles through graph-driven dictionary learning and models the ensemble dynamics as a latent sparse time-varying decomposition of global sub-circuits, thereby capturing non-stationary dynamics. CrEIMBO identifies multiple co-active sub-circuits while maintaining representation interpretability due to sharing sub-circuits across sessions. CrEIMBO distinguishes session-specific from global (session-invariant) computations by exploring when distinct sub-circuits are active. We demonstrate CrEIMBO's ability to recover ground truth components in synthetic data and uncover meaningful brain dynamics, capturing cross-subject and inter- and intra-area variability, in high-density electrode recordings of humans performing a memory task.","sentences":["Modern recordings of neural activity provide diverse observations of neurons across brain areas, behavioral conditions, and subjects -- thus presenting an exciting opportunity to reveal the fundamentals of brain-wide dynamics underlying cognitive function.","Current methods, however, often fail to fully harness the richness of such data as they either provide an uninterpretable representation (e.g., via \"black box\" deep networks) or over-simplify the model (e.g., assume stationary dynamics or analyze each session independently).","Here, instead of regarding asynchronous recordings that lack alignment in neural identity or brain areas as a limitation, we exploit these diverse views of the same brain system to learn a unified model of brain dynamics.","We assume that brain observations stem from the joint activity of a set of functional neural ensembles (groups of co-active neurons) that are similar in functionality across recordings, and propose to discover the ensemble and their non-stationary dynamical interactions in a new model we term CrEIMBO (Cross-Ensemble Interactions in Multi-view Brain Observations).","CrEIMBO identifies the composition of the per-session neural ensembles through graph-driven dictionary learning and models the ensemble dynamics as a latent sparse time-varying decomposition of global sub-circuits, thereby capturing non-stationary dynamics.","CrEIMBO identifies multiple co-active sub-circuits while maintaining representation interpretability due to sharing sub-circuits across sessions.","CrEIMBO distinguishes session-specific from global (session-invariant) computations by exploring when distinct sub-circuits are active.","We demonstrate CrEIMBO's ability to recover ground truth components in synthetic data and uncover meaningful brain dynamics, capturing cross-subject and inter- and intra-area variability, in high-density electrode recordings of humans performing a memory task."],"url":"http://arxiv.org/abs/2405.17395v1","category":"q-bio.NC"}
{"created":"2024-05-27 17:45:21","title":"Global existence, fast signal diffusion limit, and $L^\\infty$-in-time convergence rates in a competitive chemotaxis system","abstract":"We study a chemotaxis system that includes two competitive prey and one predator species in a two-dimensional domain, where the movement of prey (resp. predators) is driven by chemicals secreted by predators (resp. prey), called mutually repulsive (resp. mutually attractive) chemotaxis effect. The kinetics for all species are chosen according to the competitive Lotka-Volterra equations for prey and to a Holling type functional response for the predator. Under the biologically relevant scenario that the chemicals diffuse much faster than the individual diffusion of all species and a suitable re-scaling, equations for chemical concentrations are parabolic with slow evolution of coefficient $0<\\varepsilon\\ll 1$. In the first main result, we show the global existence of a unique classical solution to the system for each $\\varepsilon$. Secondly, we study rigorously the so-called fast signal diffusion limit, passing from the system including parabolic equations with the slow evolution to the one with all elliptic equations for chemical concentrations, i.e. the limit as $\\varepsilon \\to 0$. This explains why elliptic equations can be proposed for chemical concentration instead of parabolic ones with slow evolution. Thirdly, the $L^\\infty$-in-time convergence rates for the fast signal diffusion limit are estimated, where the effect of the initial layer is carefully treated. Finally, the differences between the systems with and without the slow evolution, and between the systems with one or two preys are discussed due to numerical simulations.","sentences":["We study a chemotaxis system that includes two competitive prey and one predator species in a two-dimensional domain, where the movement of prey (resp.","predators) is driven by chemicals secreted by predators (resp.","prey), called mutually repulsive (resp.","mutually attractive) chemotaxis effect.","The kinetics for all species are chosen according to the competitive Lotka-Volterra equations for prey and to a Holling type functional response for the predator.","Under the biologically relevant scenario that the chemicals diffuse much faster than the individual diffusion of all species and a suitable re-scaling, equations for chemical concentrations are parabolic with slow evolution of coefficient $0<\\varepsilon\\ll 1$.","In the first main result, we show the global existence of a unique classical solution to the system for each $\\varepsilon$. Secondly, we study rigorously the so-called fast signal diffusion limit, passing from the system including parabolic equations with the slow evolution to the one with all elliptic equations for chemical concentrations, i.e. the limit as $\\varepsilon \\to 0$.","This explains why elliptic equations can be proposed for chemical concentration instead of parabolic ones with slow evolution.","Thirdly, the $L^\\infty$-in-time convergence rates for the fast signal diffusion limit are estimated, where the effect of the initial layer is carefully treated.","Finally, the differences between the systems with and without the slow evolution, and between the systems with one or two preys are discussed due to numerical simulations."],"url":"http://arxiv.org/abs/2405.17392v1","category":"math.AP"}
{"created":"2024-05-27 17:42:50","title":"Turbulent circumnuclear disc and cold gas outflow in the newborn radio source 4C 31.04","abstract":"We present deep kpc- and pc-scale neutral atomic hydrogen (HI) absorption observations of a very young radio source (< 5000 yrs), 4C 31.04, using the WSRT and the Global VLBI array. Using $z=0.0598$, we detect a broad absorption feature centred at the systemic velocity, and narrow absorption redshifted by 220 km/s both previously observed. Additionally, we detect a new blueshifted, broad, shallow absorption wing. At pc scales, the broad absorption at the systemic velocity is detected across the entire radio source while the shallow wing is only seen against part of the eastern lobe. The velocity dispersion of the gas is overall high ($\\geq$40 km/s), and is highest (>60 km/s) in the region including the outflow and the radio hot spot. While we detect a velocity gradient along the western lobe and parts of the eastern lobe, most of the gas along the rest of the eastern lobe exhibits no signs of rotation. We therefore conclude that the radio lobes of 4C 31.04 are expanding into a circumnuclear disc, partially disrupting it and making the gas highly turbulent. The distribution of gas is predominantly smooth at the spatial resolution of ~4 pc studied here. However, clumps of gas are also present, particularly along the eastern lobe. This lobe is strongly interacting with the clouds and driving an outflow ~35 pc from the radio core, with a mass-outflow rate of $0.3 \\leq \\dot{M} \\leq 1.4$ M$_\\odot$/yr. We compare our observations with a model on the survival of atomic gas clouds in radio-jet-driven outflows and find that the existence of a sub-kpc outflow implies high gas density and inefficient mixing of the cold gas with the hot medium, leading to shorter cooling times. Overall, this provides further evidence of the strong impact of young radio jets on cold ISM and supports the predictions of simulations regarding jet$-$ISM interactions and the nature of the gas into which the jets expand.","sentences":["We present deep kpc- and pc-scale neutral atomic hydrogen (HI) absorption observations of a very young radio source (< 5000 yrs), 4C 31.04, using the WSRT and the Global VLBI array.","Using $z=0.0598$, we detect a broad absorption feature centred at the systemic velocity, and narrow absorption redshifted by 220 km/s both previously observed.","Additionally, we detect a new blueshifted, broad, shallow absorption wing.","At pc scales, the broad absorption at the systemic velocity is detected across the entire radio source while the shallow wing is only seen against part of the eastern lobe.","The velocity dispersion of the gas is overall high ($\\geq$40 km/s), and is highest (>60 km/s) in the region including the outflow and the radio hot spot.","While we detect a velocity gradient along the western lobe and parts of the eastern lobe, most of the gas along the rest of the eastern lobe exhibits no signs of rotation.","We therefore conclude that the radio lobes of 4C 31.04 are expanding into a circumnuclear disc, partially disrupting it and making the gas highly turbulent.","The distribution of gas is predominantly smooth at the spatial resolution of ~4 pc studied here.","However, clumps of gas are also present, particularly along the eastern lobe.","This lobe is strongly interacting with the clouds and driving an outflow ~35 pc from the radio core, with a mass-outflow rate of $0.3 \\leq \\dot{M} \\leq 1.4$ M$_\\odot$/yr.","We compare our observations with a model on the survival of atomic gas clouds in radio-jet-driven outflows and find that the existence of a sub-kpc outflow implies high gas density and inefficient mixing of the cold gas with the hot medium, leading to shorter cooling times.","Overall, this provides further evidence of the strong impact of young radio jets on cold ISM and supports the predictions of simulations regarding jet$-$ISM interactions and the nature of the gas into which the jets expand."],"url":"http://arxiv.org/abs/2405.17389v1","category":"astro-ph.GA"}
{"created":"2024-05-27 17:42:00","title":"Batteryless BLE and Light-based IoT Sensor Nodes for Reliable Environmental Sensing","abstract":"The sustainable design of Internet of Things (IoT) networks encompasses considerations related to energy efficiency and autonomy as well as considerations related to reliable communications, ensuring no energy is wasted on undelivered data. Under these considerations, this work proposes the design and implementation of energy-efficient Bluetooth Low Energy (BLE) and Light-based IoT (LIoT) batteryless IoT sensor nodes powered by an indoor light Energy Harvesting Unit (EHU). Our design intends to integrate these nodes into a sensing network to improve its reliability by combining both technologies and taking advantage of their features. The nodes incorporate state-of-the-art components, such as low-power sensors and efficient System-on-Chips (SoCs). Moreover, we design a strategy for adaptive switching between active and sleep cycles as a function of the available energy, allowing the IoT nodes to continuously operate without batteries. Our results show that by adapting the duty cycle of the BLE and LIoT nodes depending on the environment's light intensity, we can ensure a continuous and reliable node operation. In particular, measurements show that our proposed BLE and LIoT node designs are able to communicate with an IoT gateway in a bidirectional way, every 19.3 and 624.6 seconds, respectively, in an energy-autonomous and reliable manner.","sentences":["The sustainable design of Internet of Things (IoT) networks encompasses considerations related to energy efficiency and autonomy as well as considerations related to reliable communications, ensuring no energy is wasted on undelivered data.","Under these considerations, this work proposes the design and implementation of energy-efficient Bluetooth Low Energy (BLE) and Light-based IoT (LIoT) batteryless IoT sensor nodes powered by an indoor light Energy Harvesting Unit (EHU).","Our design intends to integrate these nodes into a sensing network to improve its reliability by combining both technologies and taking advantage of their features.","The nodes incorporate state-of-the-art components, such as low-power sensors and efficient System-on-Chips (SoCs).","Moreover, we design a strategy for adaptive switching between active and sleep cycles as a function of the available energy, allowing the IoT nodes to continuously operate without batteries.","Our results show that by adapting the duty cycle of the BLE and LIoT nodes depending on the environment's light intensity, we can ensure a continuous and reliable node operation.","In particular, measurements show that our proposed BLE and LIoT node designs are able to communicate with an IoT gateway in a bidirectional way, every 19.3 and 624.6 seconds, respectively, in an energy-autonomous and reliable manner."],"url":"http://arxiv.org/abs/2405.17387v1","category":"eess.SP"}
{"created":"2024-05-27 17:40:39","title":"Thermalization and Criticality on an Analog-Digital Quantum Simulator","abstract":"Understanding how interacting particles approach thermal equilibrium is a major challenge of quantum simulators. Unlocking the full potential of such systems toward this goal requires flexible initial state preparation, precise time evolution, and extensive probes for final state characterization. We present a quantum simulator comprising 69 superconducting qubits which supports both universal quantum gates and high-fidelity analog evolution, with performance beyond the reach of classical simulation in cross-entropy benchmarking experiments. Emulating a two-dimensional (2D) XY quantum magnet, we leverage a wide range of measurement techniques to study quantum states after ramps from an antiferromagnetic initial state. We observe signatures of the classical Kosterlitz-Thouless phase transition, as well as strong deviations from Kibble-Zurek scaling predictions attributed to the interplay between quantum and classical coarsening of the correlated domains. This interpretation is corroborated by injecting variable energy density into the initial state, which enables studying the effects of the eigenstate thermalization hypothesis (ETH) in targeted parts of the eigenspectrum. Finally, we digitally prepare the system in pairwise-entangled dimer states and image the transport of energy and vorticity during thermalization. These results establish the efficacy of superconducting analog-digital quantum processors for preparing states across many-body spectra and unveiling their thermalization dynamics.","sentences":["Understanding how interacting particles approach thermal equilibrium is a major challenge of quantum simulators.","Unlocking the full potential of such systems toward this goal requires flexible initial state preparation, precise time evolution, and extensive probes for final state characterization.","We present a quantum simulator comprising 69 superconducting qubits which supports both universal quantum gates and high-fidelity analog evolution, with performance beyond the reach of classical simulation in cross-entropy benchmarking experiments.","Emulating a two-dimensional (2D) XY quantum magnet, we leverage a wide range of measurement techniques to study quantum states after ramps from an antiferromagnetic initial state.","We observe signatures of the classical Kosterlitz-Thouless phase transition, as well as strong deviations from Kibble-Zurek scaling predictions attributed to the interplay between quantum and classical coarsening of the correlated domains.","This interpretation is corroborated by injecting variable energy density into the initial state, which enables studying the effects of the eigenstate thermalization hypothesis (ETH) in targeted parts of the eigenspectrum.","Finally, we digitally prepare the system in pairwise-entangled dimer states and image the transport of energy and vorticity during thermalization.","These results establish the efficacy of superconducting analog-digital quantum processors for preparing states across many-body spectra and unveiling their thermalization dynamics."],"url":"http://arxiv.org/abs/2405.17385v1","category":"quant-ph"}
{"created":"2024-05-27 17:39:01","title":"Supernova Remnants in Gamma Rays","abstract":"In the 1960s, the remnants of supernova explosions (SNRs) were indicated as a possible source of galactic cosmic rays through the Diffusive Shock Acceleration (DSA) mechanism. Since then, the observation of gamma-ray emission from relativistic ions in these objects has been one of the main goals of high-energy astrophysics. A few dozen SNRs have been detected at GeV and TeV photon energies in the last two decades. However, these observations have shown a complex phenomenology that is not easy to reduce to the standard paradigm based on DSA acceleration. Although the understanding of these objects has greatly increased, and their nature as efficient electron and proton accelerators has been observed, it remains to be clarified whether these objects are the main contributors to galactic cosmic rays. Here, we review the observations of {\\gamma}-ray emission from SNRs and the perspectives for the future.","sentences":["In the 1960s, the remnants of supernova explosions (SNRs) were indicated as a possible source of galactic cosmic rays through the Diffusive Shock Acceleration (DSA) mechanism.","Since then, the observation of gamma-ray emission from relativistic ions in these objects has been one of the main goals of high-energy astrophysics.","A few dozen SNRs have been detected at GeV and TeV photon energies in the last two decades.","However, these observations have shown a complex phenomenology that is not easy to reduce to the standard paradigm based on DSA acceleration.","Although the understanding of these objects has greatly increased, and their nature as efficient electron and proton accelerators has been observed, it remains to be clarified whether these objects are the main contributors to galactic cosmic rays.","Here, we review the observations of {\\gamma}-ray emission from SNRs and the perspectives for the future."],"url":"http://arxiv.org/abs/2405.17384v1","category":"astro-ph.HE"}
{"created":"2024-05-27 17:38:55","title":"Unlocking the Secrets of Linear Complexity Sequence Model from A Unified Perspective","abstract":"We present the Linear Complexity Sequence Model (LCSM), a comprehensive solution that unites various sequence modeling techniques with linear complexity, including linear attention, state space model, long convolution, and linear RNN, within a single framework. The goal is to enhance comprehension of these models by analyzing the impact of each component from a cohesive and streamlined viewpoint. Specifically, we segment the modeling processes of these models into three distinct stages: Expand, Oscillation, and Shrink (EOS), with each model having its own specific settings. The Expand stage involves projecting the input signal onto a high-dimensional memory state. This is followed by recursive operations performed on the memory state in the Oscillation stage. Finally, the memory state is projected back to a low-dimensional space in the Shrink stage. We perform comprehensive experiments to analyze the impact of different stage settings on language modeling and retrieval tasks. Our results show that data-driven methods are crucial for the effectiveness of the three stages in language modeling, whereas hand-crafted methods yield better performance in retrieval tasks.","sentences":["We present the Linear Complexity Sequence Model (LCSM), a comprehensive solution that unites various sequence modeling techniques with linear complexity, including linear attention, state space model, long convolution, and linear RNN, within a single framework.","The goal is to enhance comprehension of these models by analyzing the impact of each component from a cohesive and streamlined viewpoint.","Specifically, we segment the modeling processes of these models into three distinct stages: Expand, Oscillation, and Shrink (EOS), with each model having its own specific settings.","The Expand stage involves projecting the input signal onto a high-dimensional memory state.","This is followed by recursive operations performed on the memory state in the Oscillation stage.","Finally, the memory state is projected back to a low-dimensional space in the Shrink stage.","We perform comprehensive experiments to analyze the impact of different stage settings on language modeling and retrieval tasks.","Our results show that data-driven methods are crucial for the effectiveness of the three stages in language modeling, whereas hand-crafted methods yield better performance in retrieval tasks."],"url":"http://arxiv.org/abs/2405.17383v1","category":"cs.CL"}
{"created":"2024-05-27 17:31:56","title":"Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models","abstract":"Safety alignment is the key to guiding the behaviors of large language models (LLMs) that are in line with human preferences and restrict harmful behaviors at inference time, but recent studies show that it can be easily compromised by finetuning with only a few adversarially designed training examples. We aim to measure the risks in finetuning LLMs through navigating the LLM safety landscape. We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as \"safety basin\": randomly perturbing model weights maintains the safety level of the original aligned model in its local neighborhood. Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. LLM safety landscape also highlights the system prompt's critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide new insights for future work on LLM safety community.","sentences":["Safety alignment is the key to guiding the behaviors of large language models (LLMs) that are in line with human preferences and restrict harmful behaviors at inference time, but recent studies show that it can be easily compromised by finetuning with only a few adversarially designed training examples.","We aim to measure the risks in finetuning LLMs through navigating the LLM safety landscape.","We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as \"safety basin\": randomly perturbing model weights maintains the safety level of the original aligned model in its local neighborhood.","Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape.","Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin.","LLM safety landscape also highlights the system prompt's critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin.","These observations from our safety landscape research provide new insights for future work on LLM safety community."],"url":"http://arxiv.org/abs/2405.17374v1","category":"cs.LG"}
{"created":"2024-05-27 17:26:36","title":"Model-Agnostic Zeroth-Order Policy Optimization for Meta-Learning of Ergodic Linear Quadratic Regulators","abstract":"Meta-learning has been proposed as a promising machine learning topic in recent years, with important applications to image classification, robotics, computer games, and control systems. In this paper, we study the problem of using meta-learning to deal with uncertainty and heterogeneity in ergodic linear quadratic regulators. We integrate the zeroth-order optimization technique with a typical meta-learning method, proposing an algorithm that omits the estimation of policy Hessian, which applies to tasks of learning a set of heterogeneous but similar linear dynamic systems. The induced meta-objective function inherits important properties of the original cost function when the set of linear dynamic systems are meta-learnable, allowing the algorithm to optimize over a learnable landscape without projection onto the feasible set. We provide a convergence result for the exact gradient descent process by analyzing the boundedness and smoothness of the gradient for the meta-objective, which justify the proposed algorithm with gradient estimation error being small. We also provide a numerical example to corroborate this perspective.","sentences":["Meta-learning has been proposed as a promising machine learning topic in recent years, with important applications to image classification, robotics, computer games, and control systems.","In this paper, we study the problem of using meta-learning to deal with uncertainty and heterogeneity in ergodic linear quadratic regulators.","We integrate the zeroth-order optimization technique with a typical meta-learning method, proposing an algorithm that omits the estimation of policy Hessian, which applies to tasks of learning a set of heterogeneous but similar linear dynamic systems.","The induced meta-objective function inherits important properties of the original cost function when the set of linear dynamic systems are meta-learnable, allowing the algorithm to optimize over a learnable landscape without projection onto the feasible set.","We provide a convergence result for the exact gradient descent process by analyzing the boundedness and smoothness of the gradient for the meta-objective, which justify the proposed algorithm with gradient estimation error being small.","We also provide a numerical example to corroborate this perspective."],"url":"http://arxiv.org/abs/2405.17370v1","category":"eess.SY"}
{"created":"2024-05-27 17:21:04","title":"Finite Fractal Dimension of uniform attractors for non-autonomous dynamical systems with infinite dimensional symbol space","abstract":"The aim of this paper is to find an upper bound for the box-counting dimension of uniform attractors for non-autonomous dynamical systems. Contrary to the results in literature, we do not ask the symbol space to have finite box-counting dimension. Instead, we ask a condition on the semi-continuity of pullback attractors of the system as time goes to infinity. This semi-continuity can be achieved if we suppose the existence of finite-dimensional exponential uniform attractors for the limit symbols. After showing these new results, we apply them to study the box-counting dimension of the uniform attractor for a reaction-diffusion equation, and we find a specific forcing term such that the symbol space has infinite box-counting dimension but the uniform attractor has finite box-counting dimension anyway.","sentences":["The aim of this paper is to find an upper bound for the box-counting dimension of uniform attractors for non-autonomous dynamical systems.","Contrary to the results in literature, we do not ask the symbol space to have finite box-counting dimension.","Instead, we ask a condition on the semi-continuity of pullback attractors of the system as time goes to infinity.","This semi-continuity can be achieved if we suppose the existence of finite-dimensional exponential uniform attractors for the limit symbols.","After showing these new results, we apply them to study the box-counting dimension of the uniform attractor for a reaction-diffusion equation, and we find a specific forcing term such that the symbol space has infinite box-counting dimension but the uniform attractor has finite box-counting dimension anyway."],"url":"http://arxiv.org/abs/2405.17367v1","category":"math.DS"}
{"created":"2024-05-27 17:12:59","title":"Optimized thread-block arrangement in a GPU implementation of a linear solver for atmospheric chemistry mechanisms","abstract":"Earth system models (ESM) demand significant hardware resources and energy consumption to solve atmospheric chemistry processes. Recent studies have shown improved performance from running these models on GPU accelerators. Nonetheless, there is room for improvement in exploiting even more GPU resources.   This study proposes an optimized distribution of the chemical solver's computational load on the GPU, named Block-cells. Additionally, we evaluate different configurations for distributing the computational load in an NVIDIA GPU.   We use the linear solver from the Chemistry Across Multiple Phases (CAMP) framework as our test bed. An intermediate-complexity chemical mechanism under typical atmospheric conditions is used. Results demonstrate a 35x speedup compared to the single-CPU thread reference case. Even using the full resources of the node (40 physical cores) on the reference case, the Block-cells version outperforms them by 50%. The Block-cells approach shows promise in alleviating the computational burden of chemical solvers on GPU architectures.","sentences":["Earth system models (ESM) demand significant hardware resources and energy consumption to solve atmospheric chemistry processes.","Recent studies have shown improved performance from running these models on GPU accelerators.","Nonetheless, there is room for improvement in exploiting even more GPU resources.   ","This study proposes an optimized distribution of the chemical solver's computational load on the GPU, named Block-cells.","Additionally, we evaluate different configurations for distributing the computational load in an NVIDIA GPU.   ","We use the linear solver from the Chemistry Across Multiple Phases (CAMP) framework as our test bed.","An intermediate-complexity chemical mechanism under typical atmospheric conditions is used.","Results demonstrate a 35x speedup compared to the single-CPU thread reference case.","Even using the full resources of the node (40 physical cores) on the reference case, the Block-cells version outperforms them by 50%.","The Block-cells approach shows promise in alleviating the computational burden of chemical solvers on GPU architectures."],"url":"http://arxiv.org/abs/2405.17363v1","category":"cs.AR"}
{"created":"2024-05-27 16:57:27","title":"Large-scale time-multiplexed nanophotonic parametric oscillators","abstract":"Arrays of nonlinear resonators offer a fertile ground for a wide range of complex phenomena and opportunities for advanced photonic sensing and computing. Recently, significant attention has focused on studying coupled resonators in special-purpose configurations either on chips or in table-top experiments. However, a path to realizing a large-scale programmable network of nonlinear photonic resonators remains elusive because of the challenges associated with simultaneously achieving strong nonlinearity, independent operation of the resonators, and programmability of the couplings. In this work, we break these barriers by realizing large-scale, time-multiplexed optical parametric oscillators (OPOs) on a single lithium niobate nanophotonic chip. We show independent operation of 70 identical OPOs in an ultrafast nanophotonic circuit. The OPOs exhibit an ultra-low threshold of a few picojoules, substantially surpassing the strength of nonlinearity of other platforms. Using our ultrafast nanophotonic circuit, a network of N OPOs with programmable all-to-all couplings requires only a few additional components. The time-multiplexed nanophotonic OPOs can enable myriad applications, including ultrafast classical and quantum information processing.","sentences":["Arrays of nonlinear resonators offer a fertile ground for a wide range of complex phenomena and opportunities for advanced photonic sensing and computing.","Recently, significant attention has focused on studying coupled resonators in special-purpose configurations either on chips or in table-top experiments.","However, a path to realizing a large-scale programmable network of nonlinear photonic resonators remains elusive because of the challenges associated with simultaneously achieving strong nonlinearity, independent operation of the resonators, and programmability of the couplings.","In this work, we break these barriers by realizing large-scale, time-multiplexed optical parametric oscillators (OPOs) on a single lithium niobate nanophotonic chip.","We show independent operation of 70 identical OPOs in an ultrafast nanophotonic circuit.","The OPOs exhibit an ultra-low threshold of a few picojoules, substantially surpassing the strength of nonlinearity of other platforms.","Using our ultrafast nanophotonic circuit, a network of N OPOs with programmable all-to-all couplings requires only a few additional components.","The time-multiplexed nanophotonic OPOs can enable myriad applications, including ultrafast classical and quantum information processing."],"url":"http://arxiv.org/abs/2405.17355v1","category":"physics.optics"}
{"created":"2024-05-27 16:53:42","title":"Kondo-Zeno crossover in the dynamics of a monitored quantum dot","abstract":"We study the dynamics of a quantum dot coupled to a metallic bath and subject to continuous monitoring of its charge density. The dynamics averaged over measurement noise is described by a dissipative Anderson impurity model with local Markovian dephasing, that we solve using an extension of the Non-Crossing Approximation in the vectorized Hilbert space. We show that the decay time scale of an initially polarised spin which is suddenly coupled to the bath and to the monitoring protocol displays a crossover from Kondo screening, with a lifetime controlled by interactions, to Quantum Zeno effect, with a lifetime which decreases with bare dissipation as the dephasing or monitoring rate is increased. Using a Schrieffer-Wolff transformation on the Lindbladian we derive an effective model for the long-time dynamics which is described at weak dissipation by a non-Hermitian Kondo model with complex-valued spin-spin exchange. As the dephasing is increased heating due to doublon production takes over and control the spin decay.","sentences":["We study the dynamics of a quantum dot coupled to a metallic bath and subject to continuous monitoring of its charge density.","The dynamics averaged over measurement noise is described by a dissipative Anderson impurity model with local Markovian dephasing, that we solve using an extension of the Non-Crossing Approximation in the vectorized Hilbert space.","We show that the decay time scale of an initially polarised spin which is suddenly coupled to the bath and to the monitoring protocol displays a crossover from Kondo screening, with a lifetime controlled by interactions, to Quantum Zeno effect, with a lifetime which decreases with bare dissipation as the dephasing or monitoring rate is increased.","Using a Schrieffer-Wolff transformation on the Lindbladian we derive an effective model for the long-time dynamics which is described at weak dissipation by a non-Hermitian Kondo model with complex-valued spin-spin exchange.","As the dephasing is increased heating due to doublon production takes over and control the spin decay."],"url":"http://arxiv.org/abs/2405.17348v1","category":"cond-mat.str-el"}
{"created":"2024-05-27 16:49:07","title":"Boundary conditions and the two-point function plateau for the hierarchical $|\\varphi|^4$ model in dimensions 4 and higher","abstract":"We obtain precise plateau estimates for the two-point function of the finite-volume weakly-coupled hierarchical $|\\varphi|^4$ model in dimensions $d \\ge 4$, for both free and periodic boundary conditions, and for any number $n \\ge 1$ of components of the field $\\varphi$. We prove that, within a critical window around their respective effective critical points, the two-point functions for both free and periodic boundary conditions have a plateau, in the sense that they decay as $|x|^{-(d-2)}$ until reaching a constant plateau value of order $V^{-1/2}$ (with a logarithmic correction for $d=4$), where $V$ is size of the finite volume. The two critical windows for free and periodic boundary conditions do not overlap. The dependence of the plateau height on the location within the critical window is governed by an explicit $n$-dependent universal profile which is independent of the dimension. The proof is based on a rigorous renormalisation group method and extends the method used by Michta, Park and Slade (arXiv:2306.00896) to study the finite-volume susceptibility and related quantities. Our results lead to precise conjectures concerning Euclidean (non-hierarchical) models of spin systems and self-avoiding walk in dimensions $d \\ge 4$.","sentences":["We obtain precise plateau estimates for the two-point function of the finite-volume weakly-coupled hierarchical $|\\varphi|^4$ model in dimensions $d \\ge 4$, for both free and periodic boundary conditions, and for any number $n \\ge 1$ of components of the field $\\varphi$. We prove that, within a critical window around their respective effective critical points, the two-point functions for both free and periodic boundary conditions have a plateau, in the sense that they decay as $|x|^{-(d-2)}$ until reaching a constant plateau value of order $V^{-1/2}$ (with a logarithmic correction for $d=4$), where $V$ is size of the finite volume.","The two critical windows for free and periodic boundary conditions do not overlap.","The dependence of the plateau height on the location within the critical window is governed by an explicit $n$-dependent universal profile which is independent of the dimension.","The proof is based on a rigorous renormalisation group method and extends the method used by Michta, Park and Slade (arXiv:2306.00896) to study the finite-volume susceptibility and related quantities.","Our results lead to precise conjectures concerning Euclidean (non-hierarchical) models of spin systems and self-avoiding walk in dimensions $d \\ge 4$."],"url":"http://arxiv.org/abs/2405.17344v1","category":"math-ph"}
{"created":"2024-05-27 16:41:38","title":"From Seed to Supermassive Black Holes: Capture, Growth, Migration, and Pairing in Dense Proto-Bulge Environments","abstract":"The origins and mergers of supermassive black holes (BHs) remain a mystery. We describe a scenario from a novel multi-physics simulation featuring rapid ($\\lesssim 1\\,$Myr) hyper-Eddington gas capture by a $\\sim 1000\\,{\\rm M}_{\\odot}$ ``seed'' BH up to supermassive ($\\gtrsim 10^{6}\\,M_{\\odot}$) masses, in a massive, dense molecular cloud complex typical of high-redshift starbursts. Due to the high cloud density, stellar feedback is inefficient and most of the gas turns into stars in star clusters which rapidly merge hierarchically, creating deep potential wells. Relatively low-mass BH seeds at random positions can be ``captured'' by merging sub-clusters and migrate to the center in $\\sim1$ free-fall time (vastly faster than dynamical friction). This also efficiently produces a paired BH binary with $\\sim 0.1$\\,pc separation. The centrally-concentrated stellar density profile (akin to a ``proto-bulge'') allows the cluster as a whole to capture and retain gas and build up a large (pc-scale) circum-binary accretion disk with gas coherently funnelled to the central BH (even when the BH radius of influence is small). The disk is ``hyper-magnetized'' and ``flux-frozen'': dominated by a toroidal magnetic field with plasma $\\beta \\sim 10^{-3}$, with the fields amplified by flux-freezing. This drives hyper-Eddington inflow rates $\\gtrsim 1\\,\\rm M_\\odot yr^{-1}$, which also drive the two BHs to nearly-equal masses. The late-stage system appears remarkably similar to recently-observed high-redshift ``little red dots.'' This scenario can provide an explanation for rapid SMBH formation, growth and mergers in high-redshift galaxies.","sentences":["The origins and mergers of supermassive black holes (BHs) remain a mystery.","We describe a scenario from a novel multi-physics simulation featuring rapid ($\\lesssim 1\\,$Myr) hyper-Eddington gas capture by a $\\sim 1000\\,{\\rm M}_{\\odot}$ ``seed'' BH up to supermassive ($\\gtrsim 10^{6}\\,M_{\\odot}$) masses, in a massive, dense molecular cloud complex typical of high-redshift starbursts.","Due to the high cloud density, stellar feedback is inefficient and most of the gas turns into stars in star clusters which rapidly merge hierarchically, creating deep potential wells.","Relatively low-mass BH seeds at random positions can be ``captured'' by merging sub-clusters and migrate to the center in $\\sim1$ free-fall time (vastly faster than dynamical friction).","This also efficiently produces a paired BH binary with $\\sim 0.1$\\,pc separation.","The centrally-concentrated stellar density profile (akin to a ``proto-bulge'') allows the cluster as a whole to capture and retain gas and build up a large (pc-scale) circum-binary accretion disk with gas coherently funnelled to the central BH (even when the BH radius of influence is small).","The disk is ``hyper-magnetized'' and ``flux-frozen'': dominated by a toroidal magnetic field with plasma $\\beta \\sim 10^{-3}$, with the fields amplified by flux-freezing.","This drives hyper-Eddington inflow rates $\\gtrsim 1\\,\\rm M_\\odot yr^{-1}$, which also drive the two BHs to nearly-equal masses.","The late-stage system appears remarkably similar to recently-observed high-redshift ``little red dots.''","This scenario can provide an explanation for rapid SMBH formation, growth and mergers in high-redshift galaxies."],"url":"http://arxiv.org/abs/2405.17338v1","category":"astro-ph.GA"}
{"created":"2024-05-27 16:31:15","title":"Emergent time crystal from a fractional Langevin equation with white and colored noise","abstract":"We study the fractional Langevin equation with fractional $\\alpha$-order and linear friction terms of a system coupled to white and colored thermal baths using both analytical and numerical methods. We find analytical expressions for the position and the mean squared displacement (MSD) of the system using the Prabhakar-Mittag-Leffler function. The MSD exhibits long-term sub-diffusive regimes $t^{\\alpha}$ driven by colored noise and $t^{2\\alpha-1}$ driven by white noise. When the linear friction is neglected, periodic ordered phases emerge for small fractional orders $\\alpha \\lesssim 0.1$. In particular, the zero-linear friction system driven only by colored noise manifests the properties of a time crystal, with a ground state satisfying the fluctuation-dissipation theorem and a periodicity proportional to $2\\pi$. On the other hand, the zero-linear friction system driven only by white noise displays an out-of-equilibrium time glass phase with periodicity proportional to $\\pi$. A mixed phase with contributions from both ground and out-of-equilibrium states is encountered when the system couples to both baths. In that case, the periodicity deviates from $2\\pi$ due to damping effects. We test all the analytical results numerically by implementing a discrete recursive expression, where the random forces of the system are modeled as the derivative of the fractional Brownian motion. A microscopic description for the system is also provided by an extension of the Caldeira-Leggett model.","sentences":["We study the fractional Langevin equation with fractional $\\alpha$-order and linear friction terms of a system coupled to white and colored thermal baths using both analytical and numerical methods.","We find analytical expressions for the position and the mean squared displacement (MSD) of the system using the Prabhakar-Mittag-Leffler function.","The MSD exhibits long-term sub-diffusive regimes $t^{\\alpha}$ driven by colored noise and $t^{2\\alpha-1}$ driven by white noise.","When the linear friction is neglected, periodic ordered phases emerge for small fractional orders $\\alpha \\lesssim 0.1$.","In particular, the zero-linear friction system driven only by colored noise manifests the properties of a time crystal, with a ground state satisfying the fluctuation-dissipation theorem and a periodicity proportional to $2\\pi$. On the other hand, the zero-linear friction system driven only by white noise displays an out-of-equilibrium time glass phase with periodicity proportional to $\\pi$. A mixed phase with contributions from both ground and out-of-equilibrium states is encountered when the system couples to both baths.","In that case, the periodicity deviates from $2\\pi$ due to damping effects.","We test all the analytical results numerically by implementing a discrete recursive expression, where the random forces of the system are modeled as the derivative of the fractional Brownian motion.","A microscopic description for the system is also provided by an extension of the Caldeira-Leggett model."],"url":"http://arxiv.org/abs/2405.17331v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-27 16:27:57","title":"Magnetic properties of diluted hexaferrites","abstract":"We revisit the magnetic properties of the hexagonal ferrite PbFe$_{12-x}$Ga$_x$O$_{19}$. Recent experiments have reported puzzling dependencies of the ordering temperature and the saturation magnetization on the Ga concentration $x$. To explain these observations, we perform large-scale Monte Carlo simulations, focusing on the effects of an unequal distribution of the Ga impurities over the five distinct Fe sublattices. Ab-initio density-functional calculations predict that the Ga ions preferably occupy the $12k$ sublattice and (to a lesser extent) the $2a$ sublattice. We incorporate this insight into a nonuniform model of the Ga distribution. Monte Carlo simulations using this model lead to an excellent agreement between the theoretical and experimental values of the ordering temperature and saturation magnetization, indicating that the unequal distribution of the Ga impurities is the main reason for the unusual magnetic properties of PbFe$_{12-x}$Ga$_x$O$_{19}$. We also compute the temperature and concentration dependencies of the sublattice magnetizations, and we study the character of the zero-temperature transition that takes place when the ordering temperature is tuned to zero.","sentences":["We revisit the magnetic properties of the hexagonal ferrite PbFe$_{12-x}$Ga$_x$O$_{19}$.","Recent experiments have reported puzzling dependencies of the ordering temperature and the saturation magnetization on the Ga concentration $x$. To explain these observations, we perform large-scale Monte Carlo simulations, focusing on the effects of an unequal distribution of the Ga impurities over the five distinct Fe sublattices.","Ab-initio density-functional calculations predict that the Ga ions preferably occupy the $12k$ sublattice and (to a lesser extent) the $2a$ sublattice.","We incorporate this insight into a nonuniform model of the Ga distribution.","Monte Carlo simulations using this model lead to an excellent agreement between the theoretical and experimental values of the ordering temperature and saturation magnetization, indicating that the unequal distribution of the Ga impurities is the main reason for the unusual magnetic properties of PbFe$_{12-x}$Ga$_x$O$_{19}$. We also compute the temperature and concentration dependencies of the sublattice magnetizations, and we study the character of the zero-temperature transition that takes place when the ordering temperature is tuned to zero."],"url":"http://arxiv.org/abs/2405.17328v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-27 16:16:55","title":"Stoner instabilities and Ising excitonic states in twisted transition metal dichalcogenides","abstract":"Moir\\'e transition metal dichalcogenide (TMD) systems provide a tunable platform for studying electron-correlation driven quantum phases. Such phases have so far been found at rational fillings of the moir\\'e superlattice, and it is believed that lattice commensurability plays a key role in their stability. In this work, we show via magnetotransport measurements on twisted WSe2 that new correlated electronic phases can exist away from commensurability. The first phase is an antiferromagnetic metal that is driven by proximity to the van Hove singularity. The second is a re-entrant magnetic field-driven insulator. This insulator is formed from a small and equal density of electrons and holes with opposite spin projections - an Ising excitonic insulator.","sentences":["Moir\\'e transition metal dichalcogenide (TMD) systems provide a tunable platform for studying electron-correlation driven quantum phases.","Such phases have so far been found at rational fillings of the moir\\'e superlattice, and it is believed that lattice commensurability plays a key role in their stability.","In this work, we show via magnetotransport measurements on twisted WSe2 that new correlated electronic phases can exist away from commensurability.","The first phase is an antiferromagnetic metal that is driven by proximity to the van Hove singularity.","The second is a re-entrant magnetic field-driven insulator.","This insulator is formed from a small and equal density of electrons and holes with opposite spin projections - an Ising excitonic insulator."],"url":"http://arxiv.org/abs/2405.17316v1","category":"cond-mat.str-el"}
{"created":"2024-05-27 16:16:50","title":"Maximizing Phylogenetic Diversity under Ecological Constraints: A Parameterized Complexity Study","abstract":"In the NP-hard Optimizing PD with Dependencies (PDD) problem, the input consists of a phylogenetic tree $T$ over a set of taxa $X$, a food-web that describes the prey-predator relationships in $X$, and integers $k$ and $D$. The task is to find a set $S$ of $k$ species that is viable in the food-web such that the subtree of $T$ obtained by retaining only the vertices of $S$ has total edge weight at least $D$. Herein, viable means that for every predator taxon of $S$, the set $S$ contains at least one prey taxon. We provide the first systematic analysis of PDD and its special case s-PDD from a parameterized complexity perspective. For solution-size related parameters, we show that PDD is FPT with respect to $D$ and with respect to $k$ plus the height of the phylogenetic tree. Moreover, we consider structural parameterizations of the food-web. For example, we show an FPT-algorithm for the parameter that measures the vertex deletion distance to graphs where every connected component is a complete graph. Finally, we show that s-PDD admits an FPT-algorithm for the treewidth of the food-web. This disproves a conjecture of Faller et al. [Annals of Combinatorics, 2011] who conjectured that s-PDD is NP-hard even when the food-web is a tree.","sentences":["In the NP-hard Optimizing PD with Dependencies (PDD) problem, the input consists of a phylogenetic tree $T$ over a set of taxa $X$, a food-web that describes the prey-predator relationships in $X$, and integers $k$ and $D$. The task is to find a set $S$ of $k$ species that is viable in the food-web such that the subtree of $T$ obtained by retaining only the vertices of $S$ has total edge weight at least $D$. Herein, viable means that for every predator taxon of $S$, the set $S$ contains at least one prey taxon.","We provide the first systematic analysis of PDD and its special case s-PDD from a parameterized complexity perspective.","For solution-size related parameters, we show that PDD is FPT with respect to $D$ and with respect to $k$ plus the height of the phylogenetic tree.","Moreover, we consider structural parameterizations of the food-web.","For example, we show an FPT-algorithm for the parameter that measures the vertex deletion distance to graphs where every connected component is a complete graph.","Finally, we show that s-PDD admits an FPT-algorithm for the treewidth of the food-web.","This disproves a conjecture of Faller et al.","[Annals of Combinatorics, 2011] who conjectured that s-PDD is NP-hard even when the food-web is a tree."],"url":"http://arxiv.org/abs/2405.17314v1","category":"cs.CC"}
{"created":"2024-05-27 16:11:49","title":"Probabilistic Graph Rewiring via Virtual Nodes","abstract":"Message-passing graph neural networks (MPNNs) have emerged as a powerful paradigm for graph-based machine learning. Despite their effectiveness, MPNNs face challenges such as under-reaching and over-squashing, where limited receptive fields and structural bottlenecks hinder information flow in the graph. While graph transformers hold promise in addressing these issues, their scalability is limited due to quadratic complexity regarding the number of nodes, rendering them impractical for larger graphs. Here, we propose \\emph{implicitly rewired message-passing neural networks} (IPR-MPNNs), a novel approach that integrates \\emph{implicit} probabilistic graph rewiring into MPNNs. By introducing a small number of virtual nodes, i.e., adding additional nodes to a given graph and connecting them to existing nodes, in a differentiable, end-to-end manner, IPR-MPNNs enable long-distance message propagation, circumventing quadratic complexity. Theoretically, we demonstrate that IPR-MPNNs surpass the expressiveness of traditional MPNNs. Empirically, we validate our approach by showcasing its ability to mitigate under-reaching and over-squashing effects, achieving state-of-the-art performance across multiple graph datasets. Notably, IPR-MPNNs outperform graph transformers while maintaining significantly faster computational efficiency.","sentences":["Message-passing graph neural networks (MPNNs) have emerged as a powerful paradigm for graph-based machine learning.","Despite their effectiveness, MPNNs face challenges such as under-reaching and over-squashing, where limited receptive fields and structural bottlenecks hinder information flow in the graph.","While graph transformers hold promise in addressing these issues, their scalability is limited due to quadratic complexity regarding the number of nodes, rendering them impractical for larger graphs.","Here, we propose \\emph{implicitly rewired message-passing neural networks} (IPR-MPNNs), a novel approach that integrates \\emph{implicit} probabilistic graph rewiring into MPNNs.","By introducing a small number of virtual nodes, i.e., adding additional nodes to a given graph and connecting them to existing nodes, in a differentiable, end-to-end manner, IPR-MPNNs enable long-distance message propagation, circumventing quadratic complexity.","Theoretically, we demonstrate that IPR-MPNNs surpass the expressiveness of traditional MPNNs.","Empirically, we validate our approach by showcasing its ability to mitigate under-reaching and over-squashing effects, achieving state-of-the-art performance across multiple graph datasets.","Notably, IPR-MPNNs outperform graph transformers while maintaining significantly faster computational efficiency."],"url":"http://arxiv.org/abs/2405.17311v1","category":"cs.LG"}
{"created":"2024-05-27 16:09:25","title":"Peer2PIR: Private Queries for IPFS","abstract":"The InterPlanetary File System (IPFS) is a peer-to-peer network for storing data in a distributed file system, hosting over 190,000 peers spanning 152 countries. Despite its prominence, the privacy properties that IPFS offers to peers are severely limited. Any query within the network leaks to other peers the content for which a peer is querying. We address IPFS' privacy leakage across three functionalities (peer routing, provider advertisements, and content retrieval), ultimately empowering peers to privately navigate and retrieve content in the network. We argue that private information retrieval (PIR) is the most suitable tool for our task. Our work highlights and addresses novel challenges inherent to integrating PIR into distributed systems. We present our new, private protocols and demonstrate that they incur minimal overheads compared to IPFS today. We also include a systematic comparison of state-of-art PIR protocols in the context of distributed systems which may be of independent interest.","sentences":["The InterPlanetary File System (IPFS) is a peer-to-peer network for storing data in a distributed file system, hosting over 190,000 peers spanning 152 countries.","Despite its prominence, the privacy properties that IPFS offers to peers are severely limited.","Any query within the network leaks to other peers the content for which a peer is querying.","We address IPFS' privacy leakage across three functionalities (peer routing, provider advertisements, and content retrieval), ultimately empowering peers to privately navigate and retrieve content in the network.","We argue that private information retrieval (PIR) is the most suitable tool for our task.","Our work highlights and addresses novel challenges inherent to integrating PIR into distributed systems.","We present our new, private protocols and demonstrate that they incur minimal overheads compared to IPFS today.","We also include a systematic comparison of state-of-art PIR protocols in the context of distributed systems which may be of independent interest."],"url":"http://arxiv.org/abs/2405.17307v1","category":"cs.CR"}
{"created":"2024-05-27 16:07:47","title":"Non-Abelian Hopf-Euler insulators","abstract":"We discuss a class of three-band non-Abelian topological insulators in three dimensions which carry a single bulk Hopf index protected by spatiotemporal ($\\mathcal{PT}$) inversion symmetry. These phases may also host subdimensional topological invariants given by the Euler characteristic class, resulting in real Hopf-Euler insulators. Such systems naturally realize helical nodal structures in the 3D Brillouin zone, providing a physical manifestation of the linking number described by the Hopf invariant. We show that, by opening a gap between the valence bands of these systems, one finds a fully-gapped `flag' phase, which displays a three-band multi-gap Pontryagin invariant. Unlike the previously reported $\\mathcal{PT}$-symmetric four-band real Hopf insulator, which hosts a $\\mathbb{Z} \\oplus \\mathbb{Z}$ invariant, these phases are not unitarily equivalent to two copies of a complex two-band Hopf insulator. We show that these uncharted phases can be obtained through dimensional extension of two-dimensional Euler insulators, and that they support (1) an optical bulk integrated circular shift effect quantized by the Hopf invariant, (2) quantum-geometric breathing in the real space Wannier functions, and (3) surface Euler topology on boundaries. Consequently, our findings pave a way for novel experimental realizations of real-space quantum-geometry, as these systems may be directly simulated by utilizing synthethic dimensions in metamaterials or ultracold atoms.","sentences":["We discuss a class of three-band non-Abelian topological insulators in three dimensions which carry a single bulk Hopf index protected by spatiotemporal ($\\mathcal{PT}$) inversion symmetry.","These phases may also host subdimensional topological invariants given by the Euler characteristic class, resulting in real Hopf-Euler insulators.","Such systems naturally realize helical nodal structures in the 3D Brillouin zone, providing a physical manifestation of the linking number described by the Hopf invariant.","We show that, by opening a gap between the valence bands of these systems, one finds a fully-gapped `flag' phase, which displays a three-band multi-gap Pontryagin invariant.","Unlike the previously reported $\\mathcal{PT}$-symmetric four-band real Hopf insulator, which hosts a $\\mathbb{Z} \\oplus \\mathbb{Z}$ invariant, these phases are not unitarily equivalent to two copies of a complex two-band Hopf insulator.","We show that these uncharted phases can be obtained through dimensional extension of two-dimensional Euler insulators, and that they support (1) an optical bulk integrated circular shift effect quantized by the Hopf invariant, (2) quantum-geometric breathing in the real space Wannier functions, and (3) surface Euler topology on boundaries.","Consequently, our findings pave a way for novel experimental realizations of real-space quantum-geometry, as these systems may be directly simulated by utilizing synthethic dimensions in metamaterials or ultracold atoms."],"url":"http://arxiv.org/abs/2405.17305v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-27 16:00:21","title":"Coupling Light with Matter for Identifying Dominant Subnetworks","abstract":"We present a novel light-matter platform that uses complex-valued oscillator networks, a form of physical neural networks, to identify dominant subnetworks and uncover indirect correlations within larger networks. This approach offers significant advantages, including low energy consumption, high processing speed, and the immediate identification of co- and counter-regulated nodes without post-processing. The effectiveness of this approach is demonstrated through its application to biological networks, and we also propose its applicability to a wide range of other network types.","sentences":["We present a novel light-matter platform that uses complex-valued oscillator networks, a form of physical neural networks, to identify dominant subnetworks and uncover indirect correlations within larger networks.","This approach offers significant advantages, including low energy consumption, high processing speed, and the immediate identification of co- and counter-regulated nodes without post-processing.","The effectiveness of this approach is demonstrated through its application to biological networks, and we also propose its applicability to a wide range of other network types."],"url":"http://arxiv.org/abs/2405.17296v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-27 15:59:34","title":"Non-Detections of Helium in the Young Sub-Jovian Planets K2-100b, HD 63433b, & V1298 Tau c","abstract":"We search for excess in-transit absorption of neutral helium at 1.083 $\\mu$m in the atmospheres of the young (<800 Myr) sub-Jovian (0.2-0.5 $\\rm R_{J}$) planets HD 63433b, K2-100b, and V1298 Tau c using high-resolution (R~25,000) transit observations taken with Keck II/NIRSPEC. Our observations do not show evidence of helium absorption for any of the planets in our sample. We calculate 3$\\sigma$ upper limits on the planets' excess helium absorption of <0.47% for HD 63433b, <0.56% for K2-100b, and <1.13% for V1298 Tau c. In terms of equivalent width, we constrain these to <2.52, <4.44, and <8.49 mA for HD 63433b, K2-100b, and V1298 Tau c, respectively. We fit our transmission spectra with one-dimensional Parker wind models to determine upper limits on the planets' mass-loss rates of <7.9$\\times10^{10}$, <1.25$\\times10^{11}$, and <$7.9\\times10^{11}$g s$^{-1}$. Our non-detections align with expectations from one-dimensional hydrodynamic escape models, magnetic fields, and stellar wind confinement. The upper limits we measure for these planets are consistent with predicted trends in system age and He equivalent width from 1D hydrodynamic models.","sentences":["We search for excess in-transit absorption of neutral helium at 1.083 $\\mu$m in the atmospheres of the young (<800 Myr) sub-Jovian (0.2-0.5 $\\rm R_{J}$) planets HD 63433b, K2-100b, and V1298","Tau c using high-resolution (R~25,000) transit observations taken with Keck II/NIRSPEC.","Our observations do not show evidence of helium absorption for any of the planets in our sample.","We calculate 3$\\sigma$ upper limits on the planets' excess helium absorption of <0.47% for HD 63433b, <0.56% for K2-100b, and <1.13% for V1298 Tau c. In terms of equivalent width, we constrain these to <2.52, <4.44, and <8.49 mA for HD 63433b, K2-100b, and V1298","Tau c, respectively.","We fit our transmission spectra with one-dimensional Parker wind models to determine upper limits on the planets' mass-loss rates of <7.9$\\times10^{10}$, <1.25$\\times10^{11}$, and <$7.9\\times10^{11}$g s$^{-1}$. Our non-detections align with expectations from one-dimensional hydrodynamic escape models, magnetic fields, and stellar wind confinement.","The upper limits we measure for these planets are consistent with predicted trends in system age and He equivalent width from 1D hydrodynamic models."],"url":"http://arxiv.org/abs/2405.17294v1","category":"astro-ph.EP"}
{"created":"2024-05-27 15:55:33","title":"Revised Optimal design of power electronic transformer based on hybrid MMC under over-modulation operation","abstract":"The bridge arm of the hybrid modular multilevel converter (MMC) is composed of half-bridge and full-bridge sub-modules cascaded together. Compared with the half-bridge MMC, it can operate in the boost-AC mode, where the modulation index can be higher than 1, and the DC voltage and the AC voltage level are no longer mutually constrained; compared with the full-bridge MMC, it has lower switching device costs and losses. When the hybrid MMC boost-AC mode is used in the power electronic transformer, the degree of freedom in system design is improved, and the cost and volume of the power electronic transformer system can be further reduced. This paper analyzes how to make full use of the newly added modulation index of freedom introduced by the boost-AC hybrid MMC to optimize the power electronic transformer system, and finally gives the optimal modulation index selection scheme of the hybrid MMC for different optimization objectives.","sentences":["The bridge arm of the hybrid modular multilevel converter (MMC) is composed of half-bridge and full-bridge sub-modules cascaded together.","Compared with the half-bridge MMC, it can operate in the boost-AC mode, where the modulation index can be higher than 1, and the DC voltage and the AC voltage level are no longer mutually constrained; compared with the full-bridge MMC, it has lower switching device costs and losses.","When the hybrid MMC boost-AC mode is used in the power electronic transformer, the degree of freedom in system design is improved, and the cost and volume of the power electronic transformer system can be further reduced.","This paper analyzes how to make full use of the newly added modulation index of freedom introduced by the boost-AC hybrid MMC to optimize the power electronic transformer system, and finally gives the optimal modulation index selection scheme of the hybrid MMC for different optimization objectives."],"url":"http://arxiv.org/abs/2405.17291v1","category":"eess.SY"}
{"created":"2024-05-27 15:53:10","title":"On the equilibrium solutions in a model for electro-energy-reaction-diffusion systems","abstract":"Electro-energy-reaction-diffusion systems are thermodynamically consistent continuum models for reaction-diffusion processes that account for temperature and electrostatic effects in a way that total charge and energy are conserved. The question of the long-time asymptotic behavior in electro-energy-reaction-diffusion systems and the characterization of their equilibrium solutions leads to a maximization problem of the entropy on the manifold of states with fixed values for the linear charge and the nonlinear energy functional. As the main result, we establish the existence, uniqueness, and regularity of solutions to this constrained optimization problem. Important ingredients in the proof are tools from convex analysis and a reduced version of the Lagrange functional. We also derive the time-dependent PDE system in the framework of gradient systems, and we discuss the relations between stationary states, critical points, and local equilibria.","sentences":["Electro-energy-reaction-diffusion systems are thermodynamically consistent continuum models for reaction-diffusion processes that account for temperature and electrostatic effects in a way that total charge and energy are conserved.","The question of the long-time asymptotic behavior in electro-energy-reaction-diffusion systems and the characterization of their equilibrium solutions leads to a maximization problem of the entropy on the manifold of states with fixed values for the linear charge and the nonlinear energy functional.","As the main result, we establish the existence, uniqueness, and regularity of solutions to this constrained optimization problem.","Important ingredients in the proof are tools from convex analysis and a reduced version of the Lagrange functional.","We also derive the time-dependent PDE system in the framework of gradient systems, and we discuss the relations between stationary states, critical points, and local equilibria."],"url":"http://arxiv.org/abs/2405.17289v1","category":"math.AP"}
{"created":"2024-05-27 15:47:03","title":"Recurrent Complex-Weighted Autoencoders for Unsupervised Object Discovery","abstract":"Current state-of-the-art synchrony-based models encode object bindings with complex-valued activations and compute with real-valued weights in feedforward architectures. We argue for the computational advantages of a recurrent architecture with complex-valued weights. We propose a fully convolutional autoencoder, SynCx, that performs iterative constraint satisfaction: at each iteration, a hidden layer bottleneck encodes statistically regular configurations of features in particular phase relationships; over iterations, local constraints propagate and the model converges to a globally consistent configuration of phase assignments. Binding is achieved simply by the matrix-vector product operation between complex-valued weights and activations, without the need for additional mechanisms that have been incorporated into current synchrony-based models. SynCx outperforms or is strongly competitive with current models for unsupervised object discovery. SynCx also avoids certain systematic grouping errors of current models, such as the inability to separate similarly colored objects without additional supervision.","sentences":["Current state-of-the-art synchrony-based models encode object bindings with complex-valued activations and compute with real-valued weights in feedforward architectures.","We argue for the computational advantages of a recurrent architecture with complex-valued weights.","We propose a fully convolutional autoencoder, SynCx, that performs iterative constraint satisfaction: at each iteration, a hidden layer bottleneck encodes statistically regular configurations of features in particular phase relationships; over iterations, local constraints propagate and the model converges to a globally consistent configuration of phase assignments.","Binding is achieved simply by the matrix-vector product operation between complex-valued weights and activations, without the need for additional mechanisms that have been incorporated into current synchrony-based models.","SynCx outperforms or is strongly competitive with current models for unsupervised object discovery.","SynCx also avoids certain systematic grouping errors of current models, such as the inability to separate similarly colored objects without additional supervision."],"url":"http://arxiv.org/abs/2405.17283v1","category":"cs.LG"}
{"created":"2024-05-27 15:46:52","title":"R-ODE: Ricci Curvature Tells When You Will be Informed","abstract":"Information diffusion prediction is fundamental to understand the structure and organization of the online social networks, and plays a crucial role to blocking rumor spread, influence maximization, political propaganda, etc. So far, most existing solutions primarily predict the next user who will be informed with historical cascades, but ignore an important factor in the diffusion process - the time. Such limitation motivates us to pose the problem of the time-aware personalized information diffusion prediction for the first time, telling the time when the target user will be informed. In this paper, we address this problem from a fresh geometric perspective of Ricci curvature, and propose a novel Ricci-curvature regulated Ordinary Differential Equation (R-ODE). In the diffusion process, R-ODE considers that the inter-correlated users are organized in a dynamic system in the representation space, and the cascades give the observations sampled from the continuous realm. At each infection time, the message diffuses along the largest Ricci curvature, signifying less transportation effort. In the continuous realm, the message triggers users' movement, whose trajectory in the space is parameterized by an ODE with graph neural network. Consequently, R-ODE predicts the infection time of a target user by the movement trajectory learnt from the observations. Extensive experiments evaluate the personalized time prediction ability of R-ODE, and show R-ODE outperforms the state-of-the-art baselines.","sentences":["Information diffusion prediction is fundamental to understand the structure and organization of the online social networks, and plays a crucial role to blocking rumor spread, influence maximization, political propaganda, etc.","So far, most existing solutions primarily predict the next user who will be informed with historical cascades, but ignore an important factor in the diffusion process - the time.","Such limitation motivates us to pose the problem of the time-aware personalized information diffusion prediction for the first time, telling the time when the target user will be informed.","In this paper, we address this problem from a fresh geometric perspective of Ricci curvature, and propose a novel Ricci-curvature regulated Ordinary Differential Equation (R-ODE).","In the diffusion process, R-ODE considers that the inter-correlated users are organized in a dynamic system in the representation space, and the cascades give the observations sampled from the continuous realm.","At each infection time, the message diffuses along the largest Ricci curvature, signifying less transportation effort.","In the continuous realm, the message triggers users' movement, whose trajectory in the space is parameterized by an ODE with graph neural network.","Consequently, R-ODE predicts the infection time of a target user by the movement trajectory learnt from the observations.","Extensive experiments evaluate the personalized time prediction ability of R-ODE, and show R-ODE outperforms the state-of-the-art baselines."],"url":"http://arxiv.org/abs/2405.17282v1","category":"cs.SI"}
{"created":"2024-05-27 15:40:24","title":"EF-Calib: Spatiotemporal Calibration of Event- and Frame-Based Cameras Using Continuous-Time Trajectories","abstract":"Event camera, a bio-inspired asynchronous triggered camera, offers promising prospects for fusion with frame-based cameras owing to its low latency and high dynamic range. However, calibrating stereo vision systems that incorporate both event and frame-based cameras remains a significant challenge. In this letter, we present EF-Calib, a spatiotemporal calibration framework for event- and frame-based cameras using continuous-time trajectories. A novel calibration pattern applicable to both camera types and the corresponding event recognition algorithm is proposed. Leveraging the asynchronous nature of events, a derivable piece-wise B-spline to represent camera pose continuously is introduced, enabling calibration for intrinsic parameters, extrinsic parameters, and time offset, with analytical Jacobians provided. Various experiments are carried out to evaluate the calibration performance of EF-Calib, including calibration experiments for intrinsic parameters, extrinsic parameters, and time offset. Experimental results show that EF-Calib achieves the most accurate intrinsic parameters compared to current SOTA, the close accuracy of the extrinsic parameters compared to the frame-based results, and accurate time offset estimation. EF-Calib provides a convenient and accurate toolbox for calibrating the system that fuses events and frames. The code of this paper will also be open-sourced at: https://github.com/wsakobe/EF-Calib.","sentences":["Event camera, a bio-inspired asynchronous triggered camera, offers promising prospects for fusion with frame-based cameras owing to its low latency and high dynamic range.","However, calibrating stereo vision systems that incorporate both event and frame-based cameras remains a significant challenge.","In this letter, we present EF-Calib, a spatiotemporal calibration framework for event-","and frame-based cameras using continuous-time trajectories.","A novel calibration pattern applicable to both camera types and the corresponding event recognition algorithm is proposed.","Leveraging the asynchronous nature of events, a derivable piece-wise B-spline to represent camera pose continuously is introduced, enabling calibration for intrinsic parameters, extrinsic parameters, and time offset, with analytical Jacobians provided.","Various experiments are carried out to evaluate the calibration performance of EF-Calib, including calibration experiments for intrinsic parameters, extrinsic parameters, and time offset.","Experimental results show that EF-Calib achieves the most accurate intrinsic parameters compared to current SOTA, the close accuracy of the extrinsic parameters compared to the frame-based results, and accurate time offset estimation.","EF-Calib provides a convenient and accurate toolbox for calibrating the system that fuses events and frames.","The code of this paper will also be open-sourced at: https://github.com/wsakobe/EF-Calib."],"url":"http://arxiv.org/abs/2405.17278v1","category":"cs.RO"}
{"created":"2024-05-27 15:39:45","title":"Gradients of Functions of Large Matrices","abstract":"Tuning scientific and probabilistic machine learning models -- for example, partial differential equations, Gaussian processes, or Bayesian neural networks -- often relies on evaluating functions of matrices whose size grows with the data set or the number of parameters. While the state-of-the-art for evaluating these quantities is almost always based on Lanczos and Arnoldi iterations, the present work is the first to explain how to differentiate these workhorses of numerical linear algebra efficiently. To get there, we derive previously unknown adjoint systems for Lanczos and Arnoldi iterations, implement them in JAX, and show that the resulting code can compete with Diffrax when it comes to differentiating PDEs, GPyTorch for selecting Gaussian process models and beats standard factorisation methods for calibrating Bayesian neural networks. All this is achieved without any problem-specific code optimisation. Find the code at https://github.com/pnkraemer/experiments-lanczos-adjoints and install the library with pip install matfree.","sentences":["Tuning scientific and probabilistic machine learning models -- for example, partial differential equations, Gaussian processes, or Bayesian neural networks -- often relies on evaluating functions of matrices whose size grows with the data set or the number of parameters.","While the state-of-the-art for evaluating these quantities is almost always based on Lanczos and Arnoldi iterations, the present work is the first to explain how to differentiate these workhorses of numerical linear algebra efficiently.","To get there, we derive previously unknown adjoint systems for Lanczos and Arnoldi iterations, implement them in JAX, and show that the resulting code can compete with Diffrax when it comes to differentiating PDEs, GPyTorch for selecting Gaussian process models and beats standard factorisation methods for calibrating Bayesian neural networks.","All this is achieved without any problem-specific code optimisation.","Find the code at https://github.com/pnkraemer/experiments-lanczos-adjoints and install the library with pip install matfree."],"url":"http://arxiv.org/abs/2405.17277v1","category":"cs.LG"}
{"created":"2024-05-27 15:22:22","title":"ReStorEdge: An edge computing system with reuse semantics","abstract":"This paper investigates an edge computing system where requests are processed by a set of replicated edge servers. We investigate a class of applications where similar queries produce identical results. To reduce processing overhead on the edge servers we store the results of previous computations and return them when new queries are sufficiently similar to earlier ones that produced the results, avoiding the necessity of processing every new query. We implement a similarity-based data classification system, which we evaluate based on real-world datasets of images and voice queries. We evaluate a range of orchestration strategies to distribute queries and cached results between edge nodes and show that the throughput of queries over a system of distributed edge nodes can be increased by 25-33%, increasing its capacity for higher workloads.","sentences":["This paper investigates an edge computing system where requests are processed by a set of replicated edge servers.","We investigate a class of applications where similar queries produce identical results.","To reduce processing overhead on the edge servers we store the results of previous computations and return them when new queries are sufficiently similar to earlier ones that produced the results, avoiding the necessity of processing every new query.","We implement a similarity-based data classification system, which we evaluate based on real-world datasets of images and voice queries.","We evaluate a range of orchestration strategies to distribute queries and cached results between edge nodes and show that the throughput of queries over a system of distributed edge nodes can be increased by 25-33%, increasing its capacity for higher workloads."],"url":"http://arxiv.org/abs/2405.17263v1","category":"cs.ET"}
{"created":"2024-05-27 15:14:47","title":"Surface reconstruction of sampled textiles via Morse theory","abstract":"In this work, we study the perception problem for garments using tools from computational topology: the identification of their geometry and position in space from point-cloud samples, as obtained e.g. with 3D scanners. We present a reconstruction algorithm based on a direct topological study of the sampled textile surface that allows us to obtain a cellular decomposition of it via a Morse function. No intermediate triangulation or local implicit equations are used, avoiding reconstruction-induced artifices. No a priori knowledge of the surface topology, density or regularity of the point-sample is required to run the algorithm. The results are a piecewise decomposition of the surface as a union of Morse cells (i.e. topological disks), suitable for tasks such as noise-filtering or mesh-independent reparametrization, and a cell complex of small rank determining the surface topology. This algorithm can be applied to smooth surfaces with or without boundary, embedded in an ambient space of any dimension.","sentences":["In this work, we study the perception problem for garments using tools from computational topology: the identification of their geometry and position in space from point-cloud samples, as obtained e.g. with 3D scanners.","We present a reconstruction algorithm based on a direct topological study of the sampled textile surface that allows us to obtain a cellular decomposition of it via a Morse function.","No intermediate triangulation or local implicit equations are used, avoiding reconstruction-induced artifices.","No a priori knowledge of the surface topology, density or regularity of the point-sample is required to run the algorithm.","The results are a piecewise decomposition of the surface as a union of Morse cells (i.e. topological disks), suitable for tasks such as noise-filtering or mesh-independent reparametrization, and a cell complex of small rank determining the surface topology.","This algorithm can be applied to smooth surfaces with or without boundary, embedded in an ambient space of any dimension."],"url":"http://arxiv.org/abs/2405.17257v1","category":"cs.CG"}
{"created":"2024-05-27 15:13:09","title":"Emergent asymmetry in confined bioconvection","abstract":"Bioconvection is the prototypical active matter system for hydrodynamic instabilities and pattern formation in suspensions of biased swimming microorganisms, particularly at the dilute end of the concentration spectrum where cell-cell interactions typically are neglected. Confinement is an inherent characteristic of such systems, including those that are naturally-occurring or industrially-exploited, so it is important to understand the impact of boundaries on the hydrodynamic instabilities. Despite recent interest in this area we note that commonly-adopted symmetry assumptions in the literature, such as for a vertical channel or pipe, are uncorroborated and potentially unjustified. Therefore, by employing a combination of analytical and numerical techniques, we investigate whether confinement itself can drive asymmetric plume formation in a suspension of bottom-heavy swimming microorganisms (gyrotactic cells). For a class of solutions in a vertical channel we establish the existence of a first integral of motion, and reveal that asymptotic asymmetry is plausible. Furthermore, numerical simulations from both Lagrangian and Eulerian perspectives demonstrate with remarkable agreement that asymmetric solutions can indeed be more stable than symmetric; asymmetric solutions are in fact dominant for a large, practically-important region of parameter space. In addition, we verify the presence of blip and varicose instabilities for an experimentally accessible parameter range. Finally, we extend our study to a vertical Hele-Shaw geometry to explore whether a simple linear drag approximation can be justified. We find that although two-dimensional bioconvective structures and associated bulk properties have some similarities with experimental observations, approximating near wall physics in even the simplest confined systems remains challenging.","sentences":["Bioconvection is the prototypical active matter system for hydrodynamic instabilities and pattern formation in suspensions of biased swimming microorganisms, particularly at the dilute end of the concentration spectrum where cell-cell interactions typically are neglected.","Confinement is an inherent characteristic of such systems, including those that are naturally-occurring or industrially-exploited, so it is important to understand the impact of boundaries on the hydrodynamic instabilities.","Despite recent interest in this area we note that commonly-adopted symmetry assumptions in the literature, such as for a vertical channel or pipe, are uncorroborated and potentially unjustified.","Therefore, by employing a combination of analytical and numerical techniques, we investigate whether confinement itself can drive asymmetric plume formation in a suspension of bottom-heavy swimming microorganisms (gyrotactic cells).","For a class of solutions in a vertical channel we establish the existence of a first integral of motion, and reveal that asymptotic asymmetry is plausible.","Furthermore, numerical simulations from both Lagrangian and Eulerian perspectives demonstrate with remarkable agreement that asymmetric solutions can indeed be more stable than symmetric; asymmetric solutions are in fact dominant for a large, practically-important region of parameter space.","In addition, we verify the presence of blip and varicose instabilities for an experimentally accessible parameter range.","Finally, we extend our study to a vertical Hele-Shaw geometry to explore whether a simple linear drag approximation can be justified.","We find that although two-dimensional bioconvective structures and associated bulk properties have some similarities with experimental observations, approximating near wall physics in even the simplest confined systems remains challenging."],"url":"http://arxiv.org/abs/2405.17256v1","category":"cond-mat.soft"}
{"created":"2024-05-27 14:58:22","title":"Modulus of Continuity of Solutions to Complex Monge-Amp\u00e8re Equations on Stein Spaces","abstract":"In this paper, we study the modulus of continuity of solutions to Dirichlet problems for complex Monge-Amp\\`ere equations with $L^p$ densities on Stein spaces with isolated singularities. In particular, we prove such solutions are H\\\"older continuous outside singular points if the boundary data is H\\\"older continuous.","sentences":["In this paper, we study the modulus of continuity of solutions to Dirichlet problems for complex Monge-Amp\\`ere equations with $L^p$ densities on Stein spaces with isolated singularities.","In particular, we prove such solutions are H\\\"older continuous outside singular points if the boundary data is H\\\"older continuous."],"url":"http://arxiv.org/abs/2405.17242v1","category":"math.CV"}
{"created":"2024-05-27 14:49:33","title":"K\u00e4hler families of Green's functions","abstract":"In a remarkable series of works, Guo, Phong, Song, and Sturm have obtained key uniform estimates for the Green's functions associated with certain K\\\"ahler metrics. In this note, we broaden the scope of their techniques by removing one of their assumptions and allowing the complex structure to vary. We apply our results to various families of canonical K\\\"ahler metrics.","sentences":["In a remarkable series of works, Guo, Phong, Song, and Sturm have obtained key uniform estimates for the Green's functions associated with certain K\\\"ahler metrics.","In this note, we broaden the scope of their techniques by removing one of their assumptions and allowing the complex structure to vary.","We apply our results to various families of canonical K\\\"ahler metrics."],"url":"http://arxiv.org/abs/2405.17232v1","category":"math.CV"}
{"created":"2024-05-27 14:47:00","title":"InsigHTable: Insight-driven Hierarchical Table Visualization with Reinforcement Learning","abstract":"Embedding visual representations within original hierarchical tables can mitigate additional cognitive load stemming from the division of users' attention. The created hierarchical table visualizations can help users understand and explore complex data with multi-level attributes. However, because of many options available for transforming hierarchical tables and selecting subsets for embedding, the design space of hierarchical table visualizations becomes vast, and the construction process turns out to be tedious, hindering users from constructing hierarchical table visualizations with many data insights efficiently. We propose InsigHTable, a mixed-initiative and insight-driven hierarchical table transformation and visualization system. We first define data insights within hierarchical tables, which consider the hierarchical structure in the table headers. Since hierarchical table visualization construction is a sequential decision-making process, InsigHTable integrates a deep reinforcement learning framework incorporating an auxiliary rewards mechanism. This mechanism addresses the challenge of sparse rewards in constructing hierarchical table visualizations. Within the deep reinforcement learning framework, the agent continuously optimizes its decision-making process to create hierarchical table visualizations to uncover more insights by collaborating with analysts. We demonstrate the usability and effectiveness of InsigHTable through two case studies and sets of experiments. The results validate the effectiveness of the deep reinforcement learning framework and show that InsigHTable can facilitate users to construct hierarchical table visualizations and understand underlying data insights.","sentences":["Embedding visual representations within original hierarchical tables can mitigate additional cognitive load stemming from the division of users' attention.","The created hierarchical table visualizations can help users understand and explore complex data with multi-level attributes.","However, because of many options available for transforming hierarchical tables and selecting subsets for embedding, the design space of hierarchical table visualizations becomes vast, and the construction process turns out to be tedious, hindering users from constructing hierarchical table visualizations with many data insights efficiently.","We propose InsigHTable, a mixed-initiative and insight-driven hierarchical table transformation and visualization system.","We first define data insights within hierarchical tables, which consider the hierarchical structure in the table headers.","Since hierarchical table visualization construction is a sequential decision-making process, InsigHTable integrates a deep reinforcement learning framework incorporating an auxiliary rewards mechanism.","This mechanism addresses the challenge of sparse rewards in constructing hierarchical table visualizations.","Within the deep reinforcement learning framework, the agent continuously optimizes its decision-making process to create hierarchical table visualizations to uncover more insights by collaborating with analysts.","We demonstrate the usability and effectiveness of InsigHTable through two case studies and sets of experiments.","The results validate the effectiveness of the deep reinforcement learning framework and show that InsigHTable can facilitate users to construct hierarchical table visualizations and understand underlying data insights."],"url":"http://arxiv.org/abs/2405.17229v1","category":"cs.HC"}
{"created":"2024-05-27 14:46:13","title":"Antifragility of stochastic transport on networks with damage","abstract":"A system is called antifragile when damage acts as a constructive element improving the performance of a global function. In this letter, we analyze the emergence of antifragility in the movement of random walkers on networks with modular structures or communities. The random walker hops considering the capacity of transport of each link, whereas the links are susceptible to random damage that accumulates over time. We show that in networks with communities and high modularity, the localization of damage in specific groups of nodes leads to a global antifragile response of the system improving the capacity of stochastic transport to more easily reach the nodes of a network. Our findings give evidence of the mechanisms behind antifragile response in complex systems and pave the way for their quantitative exploration in different fields.","sentences":["A system is called antifragile when damage acts as a constructive element improving the performance of a global function.","In this letter, we analyze the emergence of antifragility in the movement of random walkers on networks with modular structures or communities.","The random walker hops considering the capacity of transport of each link, whereas the links are susceptible to random damage that accumulates over time.","We show that in networks with communities and high modularity, the localization of damage in specific groups of nodes leads to a global antifragile response of the system improving the capacity of stochastic transport to more easily reach the nodes of a network.","Our findings give evidence of the mechanisms behind antifragile response in complex systems and pave the way for their quantitative exploration in different fields."],"url":"http://arxiv.org/abs/2405.17228v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-27 14:40:03","title":"A Retrospective of the Tutorial on Opportunities and Challenges of Online Deep Learning","abstract":"Machine learning algorithms have become indispensable in today's world. They support and accelerate the way we make decisions based on the data at hand. This acceleration means that data structures that were valid at one moment could no longer be valid in the future. With these changing data structures, it is necessary to adapt machine learning (ML) systems incrementally to the new data. This is done with the use of online learning or continuous ML technologies. While deep learning technologies have shown exceptional performance on predefined datasets, they have not been widely applied to online, streaming, and continuous learning. In this retrospective of our tutorial titled Opportunities and Challenges of Online Deep Learning held at ECML PKDD 2023, we provide a brief overview of the opportunities but also the potential pitfalls for the application of neural networks in online learning environments using the frameworks River and Deep-River.","sentences":["Machine learning algorithms have become indispensable in today's world.","They support and accelerate the way we make decisions based on the data at hand.","This acceleration means that data structures that were valid at one moment could no longer be valid in the future.","With these changing data structures, it is necessary to adapt machine learning (ML) systems incrementally to the new data.","This is done with the use of online learning or continuous ML technologies.","While deep learning technologies have shown exceptional performance on predefined datasets, they have not been widely applied to online, streaming, and continuous learning.","In this retrospective of our tutorial titled Opportunities and Challenges of Online Deep Learning held at ECML PKDD 2023, we provide a brief overview of the opportunities but also the potential pitfalls for the application of neural networks in online learning environments using the frameworks River and Deep-River."],"url":"http://arxiv.org/abs/2405.17222v1","category":"cs.LG"}
{"created":"2024-05-27 14:30:09","title":"Flow control of three-dimensional cylinders transitioning to turbulence via multi-agent reinforcement learning","abstract":"Designing active-flow-control (AFC) strategies for three-dimensional (3D) bluff bodies is a challenging task with critical industrial implications. In this study we explore the potential of discovering novel control strategies for drag reduction using deep reinforcement learning. We introduce a high-dimensional AFC setup on a 3D cylinder, considering Reynolds numbers ($Re_D$) from $100$ to $400$, which is a range including the transition to 3D wake instabilities. The setup involves multiple zero-net-mass-flux jets positioned on the top and bottom surfaces, aligned into two slots. The method relies on coupling the computational-fluid-dynamics solver with a multi-agent reinforcement-learning (MARL) framework based on the proximal-policy-optimization algorithm. MARL offers several advantages: it exploits local invariance, adaptable control across geometries, facilitates transfer learning and cross-application of agents, and results in a significant training speedup. For instance, our results demonstrate $21\\%$ drag reduction for $Re_D=300$, outperforming classical periodic control, which yields up to $6\\%$ reduction. To the authors' knowledge, the present MARL-based framework represents the first time where training is conducted in 3D cylinders. This breakthrough paves the way for conducting AFC on progressively more complex turbulent-flow configurations.","sentences":["Designing active-flow-control (AFC) strategies for three-dimensional (3D) bluff bodies is a challenging task with critical industrial implications.","In this study we explore the potential of discovering novel control strategies for drag reduction using deep reinforcement learning.","We introduce a high-dimensional AFC setup on a 3D cylinder, considering Reynolds numbers ($Re_D$) from $100$ to $400$, which is a range including the transition to 3D wake instabilities.","The setup involves multiple zero-net-mass-flux jets positioned on the top and bottom surfaces, aligned into two slots.","The method relies on coupling the computational-fluid-dynamics solver with a multi-agent reinforcement-learning (MARL) framework based on the proximal-policy-optimization algorithm.","MARL offers several advantages: it exploits local invariance, adaptable control across geometries, facilitates transfer learning and cross-application of agents, and results in a significant training speedup.","For instance, our results demonstrate $21\\%$ drag reduction for $Re_D=300$, outperforming classical periodic control, which yields up to $6\\%$ reduction.","To the authors' knowledge, the present MARL-based framework represents the first time where training is conducted in 3D cylinders.","This breakthrough paves the way for conducting AFC on progressively more complex turbulent-flow configurations."],"url":"http://arxiv.org/abs/2405.17210v1","category":"physics.flu-dyn"}
{"created":"2024-05-27 14:25:05","title":"Numerical solution of the boundary value problem of elliptic equation by Levi function scheme","abstract":"For boundary value problem of an elliptic equation with variable coefficients describing the physical field distribution in inhomogeneous media, the Levi function can represent the solution in terms of volume and surface potentials, with the drawback that the volume potential involving in the solution expression requires heavy computational costs as well as the solvability of the integral equations with respect to the density pair. We introduce an modified integral expression for the solution to an elliptic equation in divergence form under the Levi function framework. The well-posedness of the linear integral system with respect to the density functions to be determined is rigorously proved. Based on the singularity decomposition for the Levi function, we propose two schemes to deal with the volume integrals so that the density functions can be solved efficiently. One method is an adaptive discretization scheme (ADS) for computing the integrals with continuous integrands, leading to the uniform accuracy of the integrals in the whole domain, and consequently the efficient computations for the density functions. The other method is the dual reciprocity method (DRM) which is a meshless approach converting the volume integrals into boundary integrals equivalently by expressing the volume density as the combination of the radial basis functions determined by the interior grids. The proposed schemes are justified numerically to be of satisfactory computation costs. Numerical examples in 2-dimensional and 3-dimensional cases are presented to show the validity of the proposed schemes.","sentences":["For boundary value problem of an elliptic equation with variable coefficients describing the physical field distribution in inhomogeneous media, the Levi function can represent the solution in terms of volume and surface potentials, with the drawback that the volume potential involving in the solution expression requires heavy computational costs as well as the solvability of the integral equations with respect to the density pair.","We introduce an modified integral expression for the solution to an elliptic equation in divergence form under the Levi function framework.","The well-posedness of the linear integral system with respect to the density functions to be determined is rigorously proved.","Based on the singularity decomposition for the Levi function, we propose two schemes to deal with the volume integrals so that the density functions can be solved efficiently.","One method is an adaptive discretization scheme (ADS) for computing the integrals with continuous integrands, leading to the uniform accuracy of the integrals in the whole domain, and consequently the efficient computations for the density functions.","The other method is the dual reciprocity method (DRM) which is a meshless approach converting the volume integrals into boundary integrals equivalently by expressing the volume density as the combination of the radial basis functions determined by the interior grids.","The proposed schemes are justified numerically to be of satisfactory computation costs.","Numerical examples in 2-dimensional and 3-dimensional cases are presented to show the validity of the proposed schemes."],"url":"http://arxiv.org/abs/2405.17204v1","category":"math.NA"}
{"created":"2024-05-27 14:20:37","title":"Physically Consistent Modeling & Identification of Nonlinear Friction with Dissipative Gaussian Processes","abstract":"Friction modeling has always been a challenging problem due to the complexity of real physical systems. Although a few state-of-the-art structured data-driven methods show their efficiency in nonlinear system modeling, deterministic passivity as one of the significant characteristics of friction is rarely considered in these methods. To address this issue, we propose a Gaussian Process based model that preserves the inherent structural properties such as passivity. A matrix-vector physical structure is considered in our approaches to ensure physical consistency, in particular, enabling a guarantee of positive semi-definiteness of the damping matrix. An aircraft benchmark simulation is employed to demonstrate the efficacy of our methodology. Estimation accuracy and data efficiency are increased substantially by considering and enforcing more structured physical knowledge. Also, the fulfillment of the dissipative nature of the aerodynamics is validated numerically.","sentences":["Friction modeling has always been a challenging problem due to the complexity of real physical systems.","Although a few state-of-the-art structured data-driven methods show their efficiency in nonlinear system modeling, deterministic passivity as one of the significant characteristics of friction is rarely considered in these methods.","To address this issue, we propose a Gaussian Process based model that preserves the inherent structural properties such as passivity.","A matrix-vector physical structure is considered in our approaches to ensure physical consistency, in particular, enabling a guarantee of positive semi-definiteness of the damping matrix.","An aircraft benchmark simulation is employed to demonstrate the efficacy of our methodology.","Estimation accuracy and data efficiency are increased substantially by considering and enforcing more structured physical knowledge.","Also, the fulfillment of the dissipative nature of the aerodynamics is validated numerically."],"url":"http://arxiv.org/abs/2405.17199v1","category":"eess.SY"}
{"created":"2024-05-27 14:18:04","title":"Trace field degrees in the Torelli group","abstract":"We show that for $g\\ge 2$, all integers $1 \\le d \\le 3g-3$ arise as trace field degrees of pseudo-Anosov mapping classes in the Torelli group of the closed orientable surface of genus $g$. Our method uses the Thurston-Veech construction of pseudo-Anosov maps, and we provide examples where the stretch factor has algebraic degree any even number between two and $6g-6$. This validates a claim by Thurston from the 1980s.","sentences":["We show that for $g\\ge 2$, all integers $1 \\le d \\le 3g-3$ arise as trace field degrees of pseudo-Anosov mapping classes in the Torelli group of the closed orientable surface of genus $g$. Our method uses the Thurston-Veech construction of pseudo-Anosov maps, and we provide examples where the stretch factor has algebraic degree any even number between two and $6g-6$. This validates a claim by Thurston from the 1980s."],"url":"http://arxiv.org/abs/2405.17194v1","category":"math.GT"}
{"created":"2024-05-27 14:08:48","title":"Wellposedness of solution for an $N$-D chemotaxis-convection model during tumor angiogenesis","abstract":"In this paper, we consider the following parabolic-parabolic-elliptic system } \\begin{align*} \\left\\{\\aligned & u_t=\\Delta u-\\nabla\\cdot(u\\nabla v)+\\xi\\nabla\\cdot(u\\nabla w)+au-\\mu u^{\\alpha}, && x\\in\\Omega, t>0,\\\\ & v_t=\\Delta v+\\nabla\\cdot(v\\nabla w)-v+u,&& x\\in\\Omega, t>0,\\\\ & 0=\\Delta w-w+u,&& x\\in\\Omega, t>0\\\\ \\endaligned\\right. \\end{align*} on a bounded domain $\\Omega\\subset \\mathbb{R}^{N}$ ($N\\geq1$) with smooth boundary $\\partial \\Omega$, where $\\mu$, $a$, $\\alpha$ are positive constants and $\\xi\\in\\mathbb{R}$. If one of the following cases holds:\\\\ (i) $N\\geq4$ and $\\alpha>\\frac{4N-4+N\\sqrt{2N^2-6N+8}}{2N}$;\\\\ (ii) $N=3$, $\\alpha>2$, for any $\\mu>0$ or $\\alpha=2$, the index $\\mu$ should be suitably big;\\\\ (iii) $N=2$, $\\alpha\\geq2$, for any $\\mu>0$.\\\\ Without any restriction on the index $\\xi$, for any given suitably regular initial data, the corresponding Neumann initial-boundary problem admits a unique global and bounded classical solution.","sentences":["In this paper, we consider the following parabolic-parabolic-elliptic system } \\begin{align*} \\left\\{\\aligned & u_t=\\Delta u-\\nabla\\cdot(u\\nabla v)+\\xi\\nabla\\cdot(u\\nabla w)+au-\\mu u^{\\alpha}, && x\\in\\Omega, t>0,\\\\ & v_t=\\Delta v+\\nabla\\cdot(v\\nabla w)-v+u,&& x\\in\\Omega, t>0,\\\\ & 0=\\Delta w-w+u,&& x\\in\\Omega, t>0\\\\ \\endaligned\\right.","\\end{align*} on a bounded domain $\\Omega\\subset \\mathbb{R}^{N}$ ($N\\geq1$) with smooth boundary $\\partial \\Omega$, where $\\mu$, $a$, $\\alpha$ are positive constants and $\\xi\\in\\mathbb{R}$. If one of the following cases holds:\\\\ (i) $N\\geq4$ and $\\alpha>\\frac{4N-4+N\\sqrt{2N^2-6N+8}}{2N}$;\\\\ (ii) $N=3$, $\\alpha>2$, for any $\\mu>0$ or $\\alpha=2$, the index $\\mu$ should be suitably big;\\\\ (iii) $N=2$, $\\alpha\\geq2$, for any $\\mu>0$.\\\\ Without any restriction on the index $\\xi$, for any given suitably regular initial data, the corresponding Neumann initial-boundary problem admits a unique global and bounded classical solution."],"url":"http://arxiv.org/abs/2405.17186v1","category":"math.AP"}
{"created":"2024-05-27 13:54:23","title":"Some further progress for existence and boundedness of solutions to a two-dimensional chemotaxis-(Navier-)Stokes system modeling coral fertilization","abstract":"In this paper, we investigate the effects exerted by the interplay among Laplacian diffusion, chemotaxis cross diffusion and the fluid dynamic mechanism on global existence and boundedness of the solutions. The mathematical model considered herein appears as \\begin{align}\\left\\{ \\begin{array}{l} n_t+u\\cdot\\nabla n=\\Delta n-\\nabla\\cdot( nS(n)\\nabla c)-nm,\\quad x\\in \\Omega, t>0, \\disp{ c_{ t}+u\\cdot\\nabla c=\\Delta c-c+w},\\quad x\\in \\Omega, t>0, \\disp{w_{t}+u\\cdot\\nabla w=\\Delta w-nw},\\quad x\\in \\Omega, t>0,\\\\ u_t+\\kappa(u \\cdot \\nabla)u+\\nabla P=\\Delta u+(n+m)\\nabla \\phi,\\quad x\\in \\Omega, t>0,\\\\ \\nabla\\cdot u=0,\\quad x\\in \\Omega, t>0,\\\\ \\end{array}\\right.\\eqno(KSNF) \\end{align} in a bounded domain $\\Omega\\subset \\mathbb{R}^2$ with a smooth boundary, which describes the process of coral fertilization occurring in ocean flow. Here $\\kappa\\in \\mathbb{R}$ is a given constant, $\\phi\\in W^{2,\\infty}(\\Omega)$and $S(n) $ is a scalar function satisfies $|S(n)|\\leq C_S(1+n)^{-\\alpha}$ {for all} $n\\geq 0$ with some $C_S>0$ and $\\alpha\\in\\mathbb{R}$. It is proved that if either $\\alpha>-1,\\kappa=0$ or $\\alpha\\geq-\\frac{1}{2},\\kappa\\in\\mathbb{R}$ is satisfied,then for any reasonably smooth initial data, the corresponding Neumann-Neumann-Neumann-Dirichlet initial-boundary problem $(KSNF)$ possesses a globally classical solution. In case of the stronger assumption $\\alpha>-1,\\kappa = 0$ or $\\alpha>-\\frac{1}{2},\\kappa \\in\\mathbb{R},$ we moreover show that the corresponding initial-boundary problem admits a unique global classical solution which is uniformly bounded on $\\Omega\\times(0,\\infty)$.","sentences":["In this paper, we investigate the effects exerted by the interplay among Laplacian diffusion, chemotaxis cross diffusion and the fluid dynamic mechanism on global existence and boundedness of the solutions.","The mathematical model considered herein appears as \\begin{align}\\left\\{ \\begin{array}{l} n_t+u\\cdot\\nabla n=\\Delta n-\\nabla\\cdot( nS(n)\\nabla c)-nm,\\quad x\\in \\Omega, t>0, \\disp{ c_{ t}+u\\cdot\\nabla c=\\Delta","c-c+w},\\quad x\\in \\Omega, t>0, \\disp{w_{t}+u\\cdot\\nabla w=\\Delta w-nw},\\quad x\\in \\Omega, t>0,\\\\ u_t+\\kappa(u \\cdot \\nabla)u+\\nabla P=\\Delta u+(n+m)\\nabla","\\phi,\\quad x\\in \\Omega, t>0,\\\\ \\nabla\\cdot u=0,\\quad x\\in \\Omega, t>0,\\\\ \\end{array}\\right.\\eqno(KSNF) \\end{align} in a bounded domain $\\Omega\\subset \\mathbb{R}^2$ with a smooth boundary, which describes the process of coral fertilization occurring in ocean flow.","Here $\\kappa\\in \\mathbb{R}$ is a given constant, $\\phi\\in W^{2,\\infty}(\\Omega)$and $S(n) $ is a scalar function satisfies $|S(n)|\\leq C_S(1+n)^{-\\alpha}$ {for all} $n\\geq 0$ with some $C_S>0$ and $\\alpha\\in\\mathbb{R}$. It is proved that if either $\\alpha>-1,\\kappa=0$ or $\\alpha\\geq-\\frac{1}{2},\\kappa\\in\\mathbb{R}$ is satisfied,then for any reasonably smooth initial data, the corresponding Neumann-Neumann-Neumann-Dirichlet initial-boundary problem $(KSNF)$ possesses a globally classical solution.","In case of the stronger assumption $\\alpha>-1,\\kappa = 0$ or $\\alpha>-\\frac{1}{2},\\kappa \\in\\mathbb{R},$ we moreover show that the corresponding initial-boundary problem admits a unique global classical solution which is uniformly bounded on $\\Omega\\times(0,\\infty)$."],"url":"http://arxiv.org/abs/2405.17175v1","category":"math.AP"}
{"created":"2024-05-27 13:51:50","title":"Iteration problem for several chaos in non-autonomous discrete system","abstract":"In this paper we investigate the iteration problem for several chaos in non-autonomous discrete system. Firstly, we prove that the Li-Yorke chaos of a non-autonomous discrete dynamical system is preserved under iterations when $f_{1,\\infty}$ converges to $f$, which weakens the condition in the literature that $f_{1,\\infty}$ uniformly converges to $f$. Besides, we prove that both DC2' and Kato's chaos of a non-autonomous discrete dynamical system are iteration invariants. Additionally, we give a sufficient condition for non-autonomous discrete dynamical system to be Li-Yorke chaos. Finally, we give an example to show that the DC3 of a non-autonomous discrete dynamical system is not inherited under iterations, which partly answers an open question proposed by Wu and Zhu(Chaos in a class of non-autonomous discrete systems, Appl.Math.Lett. 2013,26:431-436).","sentences":["In this paper we investigate the iteration problem for several chaos in non-autonomous discrete system.","Firstly, we prove that the Li-Yorke chaos of a non-autonomous discrete dynamical system is preserved under iterations when $f_{1,\\infty}$ converges to $f$, which weakens the condition in the literature that $f_{1,\\infty}$ uniformly converges to $f$. Besides, we prove that both DC2' and Kato's chaos of a non-autonomous discrete dynamical system are iteration invariants.","Additionally, we give a sufficient condition for non-autonomous discrete dynamical system to be Li-Yorke chaos.","Finally, we give an example to show that the DC3 of a non-autonomous discrete dynamical system is not inherited under iterations, which partly answers an open question proposed by Wu and Zhu(Chaos in a class of non-autonomous discrete systems, Appl.","Math.","Lett.","2013,26:431-436)."],"url":"http://arxiv.org/abs/2405.17173v1","category":"math.DS"}
{"created":"2024-05-27 13:50:17","title":"Coordinating robotized construction using advanced robotic simulation: The case of collaborative brick wall assembly","abstract":"Utilizing robotic systems in the construction industry is gaining popularity due to their build time, precision, and efficiency. In this paper, we introduce a system that allows the coordination of multiple manipulator robots for construction activities. As a case study, we chose robotic brick wall assembly. By utilizing a multi robot system where arm manipulators collaborate with each other, the entirety of a potentially long wall can be assembled simultaneously. However, the reduction of overall bricklaying time is dependent on the minimization of time required for each individual manipulator. In this paper, we execute the simulation with various placements of material and the robots base, as well as different robot configurations, to determine the optimal position of the robot and material and the best configuration for the robot. The simulation results provide users with insights into how to find the best placement of robots and raw materials for brick wall assembly.","sentences":["Utilizing robotic systems in the construction industry is gaining popularity due to their build time, precision, and efficiency.","In this paper, we introduce a system that allows the coordination of multiple manipulator robots for construction activities.","As a case study, we chose robotic brick wall assembly.","By utilizing a multi robot system where arm manipulators collaborate with each other, the entirety of a potentially long wall can be assembled simultaneously.","However, the reduction of overall bricklaying time is dependent on the minimization of time required for each individual manipulator.","In this paper, we execute the simulation with various placements of material and the robots base, as well as different robot configurations, to determine the optimal position of the robot and material and the best configuration for the robot.","The simulation results provide users with insights into how to find the best placement of robots and raw materials for brick wall assembly."],"url":"http://arxiv.org/abs/2405.17171v1","category":"cs.RO"}
{"created":"2024-05-27 13:49:24","title":"Forecasting Four Business Cycle Phases Using Machine Learning: A Case Study of US and EuroZone","abstract":"Understanding the business cycle is crucial for building economic stability, guiding business planning, and informing investment decisions. The business cycle refers to the recurring pattern of expansion and contraction in economic activity over time. Economic analysis is inherently complex, incorporating a myriad of factors (such as macroeconomic indicators, political decisions). This complexity makes it challenging to fully account for all variables when determining the current state of the economy and predicting its future trajectory in the upcoming months. The objective of this study is to investigate the capacity of machine learning models in automatically analyzing the state of the economic, with the goal of forecasting business phases (expansion, slowdown, recession and recovery) in the United States and the EuroZone. We compared three different machine learning approaches to classify the phases of the business cycle, and among them, the Multinomial Logistic Regression (MLR) achieved the best results. Specifically, MLR got the best results by achieving the accuracy of 65.25% (Top1) and 84.74% (Top2) for the EuroZone and 75% (Top1) and 92.14% (Top2) for the United States. These results demonstrate the potential of machine learning techniques to predict business cycles accurately, which can aid in making informed decisions in the fields of economics and finance.","sentences":["Understanding the business cycle is crucial for building economic stability, guiding business planning, and informing investment decisions.","The business cycle refers to the recurring pattern of expansion and contraction in economic activity over time.","Economic analysis is inherently complex, incorporating a myriad of factors (such as macroeconomic indicators, political decisions).","This complexity makes it challenging to fully account for all variables when determining the current state of the economy and predicting its future trajectory in the upcoming months.","The objective of this study is to investigate the capacity of machine learning models in automatically analyzing the state of the economic, with the goal of forecasting business phases (expansion, slowdown, recession and recovery) in the United States and the EuroZone.","We compared three different machine learning approaches to classify the phases of the business cycle, and among them, the Multinomial Logistic Regression (MLR) achieved the best results.","Specifically, MLR got the best results by achieving the accuracy of 65.25% (Top1) and 84.74% (Top2) for the EuroZone and 75% (Top1) and 92.14% (Top2) for the United States.","These results demonstrate the potential of machine learning techniques to predict business cycles accurately, which can aid in making informed decisions in the fields of economics and finance."],"url":"http://arxiv.org/abs/2405.17170v1","category":"cs.LG"}
{"created":"2024-05-27 13:28:54","title":"A low-mass sub-Neptune planet transiting the bright active star HD 73344","abstract":"Context. Planets with radii of between 2-4 RE closely orbiting solar-type stars are of significant importance for studying the transition from rocky to giant planets.   Aims. Our goal is to determine the mass of a transiting planet around the very bright F6 star HD 73344 . This star exhibits high activity and has a rotation period that is close to the orbital period of the planet.   Methods. The transiting planet, initially a K2 candidate, is confirmed through TESS observations . We refined its parameters and rule out a false positive with Spitzer observations. We analyzed high-precision RV data from the SOPHIE and HIRES spectrographs. We conducted separate and joint analyses using the PASTIS software. We used a novel observing strategy, targeting the star at high cadence for two consecutive nights with SOPHIE to understand the short-term stellar variability. We modeled stellar noise with two Gaussian processes.   Results. High-cadence RV observations provide better constraints on stellar variability and precise orbital parameters for the transiting planet. The derived mean density suggests a sub-Neptune-type composition, but uncertainties in the planet's mass prevent a detailed characterization. In addition, we find a periodic signal in the RV data that we attribute to the signature of a nontransiting exoplanet, without totally excluding the possibility of a nonplanetary origin. Dynamical analyses confirm the stability of the two-planet system and provide constraints on the inclination of the candidate planet; these findings favor a near-coplanar system.   Conclusions. While the transiting planet orbits the bright star at a short period, stellar activity prevented us from precise mass measurements. Long-term RV tracking of this planet could improve this measurement, as well as our understanding of the activity of the host star.","sentences":["Context.","Planets with radii of between 2-4 RE closely orbiting solar-type stars are of significant importance for studying the transition from rocky to giant planets.   ","Aims.","Our goal is to determine the mass of a transiting planet around the very bright F6 star HD 73344 .","This star exhibits high activity and has a rotation period that is close to the orbital period of the planet.   Methods.","The transiting planet, initially a K2 candidate, is confirmed through TESS observations .","We refined its parameters and rule out a false positive with Spitzer observations.","We analyzed high-precision RV data from the SOPHIE and HIRES spectrographs.","We conducted separate and joint analyses using the PASTIS software.","We used a novel observing strategy, targeting the star at high cadence for two consecutive nights with SOPHIE to understand the short-term stellar variability.","We modeled stellar noise with two Gaussian processes.   Results.","High-cadence RV observations provide better constraints on stellar variability and precise orbital parameters for the transiting planet.","The derived mean density suggests a sub-Neptune-type composition, but uncertainties in the planet's mass prevent a detailed characterization.","In addition, we find a periodic signal in the RV data that we attribute to the signature of a nontransiting exoplanet, without totally excluding the possibility of a nonplanetary origin.","Dynamical analyses confirm the stability of the two-planet system and provide constraints on the inclination of the candidate planet; these findings favor a near-coplanar system.   Conclusions.","While the transiting planet orbits the bright star at a short period, stellar activity prevented us from precise mass measurements.","Long-term RV tracking of this planet could improve this measurement, as well as our understanding of the activity of the host star."],"url":"http://arxiv.org/abs/2405.17155v1","category":"astro-ph.EP"}
{"created":"2024-05-27 13:19:23","title":"LCM: Locally Constrained Compact Point Cloud Model for Masked Point Modeling","abstract":"The pre-trained point cloud model based on Masked Point Modeling (MPM) has exhibited substantial improvements across various tasks. However, these models heavily rely on the Transformer, leading to quadratic complexity and limited decoder, hindering their practice application. To address this limitation, we first conduct a comprehensive analysis of existing Transformer-based MPM, emphasizing the idea that redundancy reduction is crucial for point cloud analysis. To this end, we propose a Locally constrained Compact point cloud Model (LCM) consisting of a locally constrained compact encoder and a locally constrained Mamba-based decoder. Our encoder replaces self-attention with our local aggregation layers to achieve an elegant balance between performance and efficiency. Considering the varying information density between masked and unmasked patches in the decoder inputs of MPM, we introduce a locally constrained Mamba-based decoder. This decoder ensures linear complexity while maximizing the perception of point cloud geometry information from unmasked patches with higher information density. Extensive experimental results show that our compact model significantly surpasses existing Transformer-based models in both performance and efficiency, especially our LCM-based Point-MAE model, compared to the Transformer-based model, achieved an improvement of 2.24%, 0.87%, and 0.94% in performance on the three variants of ScanObjectNN while reducing parameters by 88% and computation by 73%.","sentences":["The pre-trained point cloud model based on Masked Point Modeling (MPM) has exhibited substantial improvements across various tasks.","However, these models heavily rely on the Transformer, leading to quadratic complexity and limited decoder, hindering their practice application.","To address this limitation, we first conduct a comprehensive analysis of existing Transformer-based MPM, emphasizing the idea that redundancy reduction is crucial for point cloud analysis.","To this end, we propose a Locally constrained Compact point cloud Model (LCM) consisting of a locally constrained compact encoder and a locally constrained Mamba-based decoder.","Our encoder replaces self-attention with our local aggregation layers to achieve an elegant balance between performance and efficiency.","Considering the varying information density between masked and unmasked patches in the decoder inputs of MPM, we introduce a locally constrained Mamba-based decoder.","This decoder ensures linear complexity while maximizing the perception of point cloud geometry information from unmasked patches with higher information density.","Extensive experimental results show that our compact model significantly surpasses existing Transformer-based models in both performance and efficiency, especially our LCM-based Point-MAE model, compared to the Transformer-based model, achieved an improvement of 2.24%, 0.87%, and 0.94% in performance on the three variants of ScanObjectNN while reducing parameters by 88% and computation by 73%."],"url":"http://arxiv.org/abs/2405.17149v1","category":"cs.CV"}
{"created":"2024-05-27 13:18:52","title":"Direct view of gate-tunable miniband dispersion in graphene superlattices near the magic twist angle","abstract":"Superlattices from twisted graphene mono- and bi-layer systems give rise to on-demand quantum many-body states such as Mott insulators, unconventional superconductors and the fractional quantum Hall effect. These phenomena are observed in transport experiments when changing the filling of the low-energy electronic bands. Their origin is broadly ascribed to a combination of flat bands and strong Coulomb interactions, yet a comprehensive understanding is lacking. This is primarily because the relevant low-energy band structure is believed to strongly change in a non-trivial way as the electron filling is varied. Here we gain direct access to the filling-dependent low energy bands of twisted bilayer graphene (TBG) and twisted double bilayer graphene (TDBG) by applying micro-focused angle-resolved photoemission spectroscopy to in situ gated devices. Our findings for the two systems are in stark contrast: The doping dependent dispersion for TBG can be described in a simple model, combining a filling-dependent rigid band shift with a many-body related bandwidth change. In TDBG, on the other hand, we find a complex behaviour of the low-energy bands, combining non-monotonous bandwidth changes and tuneable gap openings. Our work establishes the extent of electric field tunability of the low energy electronic states in twisted graphene superlattices and can serve to underpin the theoretical understanding of the resulting phenomena.","sentences":["Superlattices from twisted graphene mono- and bi-layer systems give rise to on-demand quantum many-body states such as Mott insulators, unconventional superconductors and the fractional quantum Hall effect.","These phenomena are observed in transport experiments when changing the filling of the low-energy electronic bands.","Their origin is broadly ascribed to a combination of flat bands and strong Coulomb interactions, yet a comprehensive understanding is lacking.","This is primarily because the relevant low-energy band structure is believed to strongly change in a non-trivial way as the electron filling is varied.","Here we gain direct access to the filling-dependent low energy bands of twisted bilayer graphene (TBG) and twisted double bilayer graphene (TDBG) by applying micro-focused angle-resolved photoemission spectroscopy to in situ gated devices.","Our findings for the two systems are in stark contrast: The doping dependent dispersion for TBG can be described in a simple model, combining a filling-dependent rigid band shift with a many-body related bandwidth change.","In TDBG, on the other hand, we find a complex behaviour of the low-energy bands, combining non-monotonous bandwidth changes and tuneable gap openings.","Our work establishes the extent of electric field tunability of the low energy electronic states in twisted graphene superlattices and can serve to underpin the theoretical understanding of the resulting phenomena."],"url":"http://arxiv.org/abs/2405.17148v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-27 13:09:17","title":"Disentanglement-induced multistability","abstract":"Multistability cannot be derived from any theoretical model that is based on a monostable master equation. On the other hand, multistability is experimentally-observed in a variety of quantum systems. A master equation having a nonlinear term that gives rise to disentanglement has been recently proposed . The dynamics governed by this master equation is explored for a quantum system made of coupled spins. It is found that the added nonlinear term can give rise to multistability. The spins' response to an externally applied magnetic field is evaluated, and both a phase transition and a dynamical instability are found. These findings, which originate from disentanglement-induced multistability, indirectly support the hypothesis that spontaneous disentanglement occurs in quantum systems.","sentences":["Multistability cannot be derived from any theoretical model that is based on a monostable master equation.","On the other hand, multistability is experimentally-observed in a variety of quantum systems.","A master equation having a nonlinear term that gives rise to disentanglement has been recently proposed .","The dynamics governed by this master equation is explored for a quantum system made of coupled spins.","It is found that the added nonlinear term can give rise to multistability.","The spins' response to an externally applied magnetic field is evaluated, and both a phase transition and a dynamical instability are found.","These findings, which originate from disentanglement-induced multistability, indirectly support the hypothesis that spontaneous disentanglement occurs in quantum systems."],"url":"http://arxiv.org/abs/2405.17145v1","category":"quant-ph"}
{"created":"2024-05-27 12:56:12","title":"CMOSS: A Reliable, Motif-based Columnar Molecular Storage System","abstract":"The surge in demand for cost-effective, durable long-term archival media, coupled with density limitations of contemporary magnetic media, has resulted in synthetic DNA emerging as a promising new alternative. Despite its benefits, storing data on DNA poses several challenges as the technology used for reading/writing data and achieving random access on DNA are highly error prone. In order to deal with such errors, it is important to design efficient pipelines that can carefully use redundancy to mask errors without amplifying overall cost. In this work, we present Columnar MOlecular Storage System (CMOSS), a novel, end-to-end DNA storage pipeline that can provide error-tolerant data storage at low read/write costs. CMOSS differs from SOTA on three fronts (i) a motif-based, vertical layout in contrast to nucleotide-based horizontal layout used by SOTA, (ii) merged consensus calling and decoding enabled by the vertical layout, and (iii) a flexible, fixed-size, block-based data organization for random access over DNA storage in contrast to the variable-sized, object-based access used by SOTA. Using an in-depth evaluation via simulation studies and real wet-lab experiments, we demonstrate the benefits of various CMOSS design choices. We make the entire pipeline together with the read datasets openly available to the community for faithful reproduction and furthering research.","sentences":["The surge in demand for cost-effective, durable long-term archival media, coupled with density limitations of contemporary magnetic media, has resulted in synthetic DNA emerging as a promising new alternative.","Despite its benefits, storing data on DNA poses several challenges as the technology used for reading/writing data and achieving random access on DNA are highly error prone.","In order to deal with such errors, it is important to design efficient pipelines that can carefully use redundancy to mask errors without amplifying overall cost.","In this work, we present Columnar MOlecular Storage System (CMOSS), a novel, end-to-end DNA storage pipeline that can provide error-tolerant data storage at low read/write costs.","CMOSS differs from SOTA on three fronts (i) a motif-based, vertical layout in contrast to nucleotide-based horizontal layout used by SOTA, (ii) merged consensus calling and decoding enabled by the vertical layout, and (iii) a flexible, fixed-size, block-based data organization for random access over DNA storage in contrast to the variable-sized, object-based access used by SOTA.","Using an in-depth evaluation via simulation studies and real wet-lab experiments, we demonstrate the benefits of various CMOSS design choices.","We make the entire pipeline together with the read datasets openly available to the community for faithful reproduction and furthering research."],"url":"http://arxiv.org/abs/2405.17138v1","category":"cs.DB"}
{"created":"2024-05-27 12:50:25","title":"Moir\u00e9 flat bands in alternating twisted $\\mathrm{MoTe_2}$ multilayer","abstract":"The long-awaited fractional quantum anomalous Hall (FQAH) effect recently has been observed in the twisted $\\mathrm{MoTe_2}$ homobilayers, causing a great sensation. Here, we theoretically investigate the moir\\'e band structures of a closely related system, the alternating twisted multilayer $\\mathrm{MoTe_2}$ (ATML-$\\mathrm{MoTe_2}$), where the adjacent layers have opposite twist angles. We illustrate that such ATML-$\\mathrm{MoTe_2}$ is a very unique moir\\'e system, exhibiting multiple topological flat bands highly controllable by the layer number and twist angle, which is not only an ideal platform to simulate Hubbard model, but also may host FQAH states. Specifically, an N-layer ATML-$\\mathrm{MoTe_2}$ ($N \\geq 3$) always possesses $N-2$ topological flat bands near Fermi energy $E_f$, which has an odd-even dependent decomposition rule to understand the behaviors of the moir\\'e flat bands. We predict three intriguing examples: (1) The AT3L-$\\mathrm{MoTe_2}$ ($N=3$) has one isolated moir\\'e flat band, which corresponds to a triangular lattice Hubbard model, resembling the twisted TMD heterobilayers. (2) The AT4L-$\\mathrm{MoTe_2}$ ($N=4$) has two topological flat bands that are very similar to the twisted $\\mathrm{MoTe_2}$ homobilayers, implying the possible existence of FQAH states. (3) When $N>4$, the giant density of states (DOS) induced by the multiple moir\\'e flat bands may induce exotic correlated states.","sentences":["The long-awaited fractional quantum anomalous Hall (FQAH) effect recently has been observed in the twisted $\\mathrm{MoTe_2}$ homobilayers, causing a great sensation.","Here, we theoretically investigate the moir\\'e band structures of a closely related system, the alternating twisted multilayer $\\mathrm{MoTe_2}$ (ATML-$\\mathrm{MoTe_2}$), where the adjacent layers have opposite twist angles.","We illustrate that such ATML-$\\mathrm{MoTe_2}$ is a very unique moir\\'e system, exhibiting multiple topological flat bands highly controllable by the layer number and twist angle, which is not only an ideal platform to simulate Hubbard model, but also may host FQAH states.","Specifically, an N-layer ATML-$\\mathrm{MoTe_2}$ ($N \\geq 3$) always possesses $N-2$ topological flat bands near Fermi energy $E_f$, which has an odd-even dependent decomposition rule to understand the behaviors of the moir\\'e flat bands.","We predict three intriguing examples: (1) The AT3L-$\\mathrm{MoTe_2}$ ($N=3$) has one isolated moir\\'e flat band, which corresponds to a triangular lattice Hubbard model, resembling the twisted TMD heterobilayers.","(2) The AT4L-$\\mathrm{MoTe_2}$ ($N=4$) has two topological flat bands that are very similar to the twisted $\\mathrm{MoTe_2}$ homobilayers, implying the possible existence of FQAH states.","(3) When $N>4$, the giant density of states (DOS) induced by the multiple moir\\'e flat bands may induce exotic correlated states."],"url":"http://arxiv.org/abs/2405.17134v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-27 12:49:06","title":"Digital Signal Processing Techniques for Noise Characterization of Lasers and Optical Frequency Combs: A Tutorial","abstract":"Performing noise characterizations of lasers and optical frequency combs on sampled and digitized data offers numerous advantages compared to analog measurement techniques. One of the main advantages is that the measurement setup is greatly simplified. Only a balanced detector followed by an analog-to-digital converter is needed, allowing all the complexity to be moved to the digital domain. Secondly, near-optimal phase estimators are efficiently implementable, providing accurate phase noise estimation in the presence of the measurement noise. Finally, joint processing of multiple comb lines is feasible, enabling computation of phase noise correlation matrix, which includes all information about the phase noise of the optical frequency comb. This tutorial introduces a framework based on digital signal processing for phase noise characterization of lasers and optical frequency combs. The framework is based on the extended Kalman filter (EKF) and automatic differentiation. The EKF is a near-optimal estimator of the optical phase in the presence of measurement noise, making it very suitable for phase noise measurements. Automatic differentiation is key to efficiently optimizing many parameters entering the EKF framework. More specifically, the combination of EKF and automatic differentiation enables the efficient optimization of phase noise measurement for optical frequency combs with arbitrarily complex noise dynamics that may include many free parameters. We show the framework's efficacy through simulations and experimental data, showcasing its application across various comb types and in dual-comb measurements, highlighting its accuracy and versatility. Finally, we discuss its capability for digital phase noise compensation, which is highly relevant to free-running dual-comb spectroscopy applications.","sentences":["Performing noise characterizations of lasers and optical frequency combs on sampled and digitized data offers numerous advantages compared to analog measurement techniques.","One of the main advantages is that the measurement setup is greatly simplified.","Only a balanced detector followed by an analog-to-digital converter is needed, allowing all the complexity to be moved to the digital domain.","Secondly, near-optimal phase estimators are efficiently implementable, providing accurate phase noise estimation in the presence of the measurement noise.","Finally, joint processing of multiple comb lines is feasible, enabling computation of phase noise correlation matrix, which includes all information about the phase noise of the optical frequency comb.","This tutorial introduces a framework based on digital signal processing for phase noise characterization of lasers and optical frequency combs.","The framework is based on the extended Kalman filter (EKF) and automatic differentiation.","The EKF is a near-optimal estimator of the optical phase in the presence of measurement noise, making it very suitable for phase noise measurements.","Automatic differentiation is key to efficiently optimizing many parameters entering the EKF framework.","More specifically, the combination of EKF and automatic differentiation enables the efficient optimization of phase noise measurement for optical frequency combs with arbitrarily complex noise dynamics that may include many free parameters.","We show the framework's efficacy through simulations and experimental data, showcasing its application across various comb types and in dual-comb measurements, highlighting its accuracy and versatility.","Finally, we discuss its capability for digital phase noise compensation, which is highly relevant to free-running dual-comb spectroscopy applications."],"url":"http://arxiv.org/abs/2405.17131v1","category":"physics.optics"}
{"created":"2024-05-27 12:34:55","title":"Robust Reproducible Network Exploration","abstract":"We propose a novel method of network detection that is robust against any complex dependence structure. Our goal is to conduct exploratory network detection, meaning that we attempt to detect a network composed of ``connectable'' edges that are worth investigating in detail for further modelling or precise network analysis. For a reproducible network detection, we pursuit high power while controlling the false discovery rate (FDR). In particular, we formalize the problem as a multiple testing, and propose p-variables that are used in the Benjamini-Hochberg procedure. We show that the proposed method controls the FDR under arbitrary dependence structure with any sample size, and has asymptotic power one. The validity is also confirmed by simulations and a real data example.","sentences":["We propose a novel method of network detection that is robust against any complex dependence structure.","Our goal is to conduct exploratory network detection, meaning that we attempt to detect a network composed of ``connectable'' edges that are worth investigating in detail for further modelling or precise network analysis.","For a reproducible network detection, we pursuit high power while controlling the false discovery rate (FDR).","In particular, we formalize the problem as a multiple testing, and propose p-variables that are used in the Benjamini-Hochberg procedure.","We show that the proposed method controls the FDR under arbitrary dependence structure with any sample size, and has asymptotic power one.","The validity is also confirmed by simulations and a real data example."],"url":"http://arxiv.org/abs/2405.17117v1","category":"stat.ME"}
{"created":"2024-05-27 12:31:49","title":"Rational Homotopy and Hodge Theory of Moduli Stacks of principal $G$-bundles","abstract":"For a semisimple complex algebraic group $G$ we determine the rational cohomology and the Hodge-Tate structure of the moduli stack ${\\mathscr B}un_{G,X}$ of principal $G$-bundles over a connected smooth complex projective variety $X$ of special type using the homotopy theory of the underlying topological stack.","sentences":["For a semisimple complex algebraic group $G$ we determine the rational cohomology and the Hodge-Tate structure of the moduli stack ${\\mathscr B}un_{G,X}$ of principal $G$-bundles over a connected smooth complex projective variety $X$ of special type using the homotopy theory of the underlying topological stack."],"url":"http://arxiv.org/abs/2405.17113v1","category":"math.AG"}
{"created":"2024-05-27 12:29:04","title":"Using continuation methods to analyse the difficulty of problems solved by Ising machines","abstract":"Ising machines are dedicated hardware solvers of NP-hard optimization problems. However, they do not always find the most optimal solution. The probability of finding this optimal solution depends on the problem at hand. Using continuation methods, we show that this is closely linked to the bifurcation sequence of the optimal solution. From this bifurcation analysis, we can determine the effectiveness of solution schemes. Moreover, we find that the proper choice of implementation of the Ising machine can drastically change this bifurcation sequence and therefore vastly increase the probability of finding the optimal solution.","sentences":["Ising machines are dedicated hardware solvers of NP-hard optimization problems.","However, they do not always find the most optimal solution.","The probability of finding this optimal solution depends on the problem at hand.","Using continuation methods, we show that this is closely linked to the bifurcation sequence of the optimal solution.","From this bifurcation analysis, we can determine the effectiveness of solution schemes.","Moreover, we find that the proper choice of implementation of the Ising machine can drastically change this bifurcation sequence and therefore vastly increase the probability of finding the optimal solution."],"url":"http://arxiv.org/abs/2405.17112v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-27 12:18:46","title":"Sok: Comprehensive Security Overview, Challenges, and Future Directions of Voice-Controlled Systems","abstract":"The integration of Voice Control Systems (VCS) into smart devices and their growing presence in daily life accentuate the importance of their security. Current research has uncovered numerous vulnerabilities in VCS, presenting significant risks to user privacy and security. However, a cohesive and systematic examination of these vulnerabilities and the corresponding solutions is still absent. This lack of comprehensive analysis presents a challenge for VCS designers in fully understanding and mitigating the security issues within these systems.   Addressing this gap, our study introduces a hierarchical model structure for VCS, providing a novel lens for categorizing and analyzing existing literature in a systematic manner. We classify attacks based on their technical principles and thoroughly evaluate various attributes, such as their methods, targets, vectors, and behaviors. Furthermore, we consolidate and assess the defense mechanisms proposed in current research, offering actionable recommendations for enhancing VCS security. Our work makes a significant contribution by simplifying the complexity inherent in VCS security, aiding designers in effectively identifying and countering potential threats, and setting a foundation for future advancements in VCS security research.","sentences":["The integration of Voice Control Systems (VCS) into smart devices and their growing presence in daily life accentuate the importance of their security.","Current research has uncovered numerous vulnerabilities in VCS, presenting significant risks to user privacy and security.","However, a cohesive and systematic examination of these vulnerabilities and the corresponding solutions is still absent.","This lack of comprehensive analysis presents a challenge for VCS designers in fully understanding and mitigating the security issues within these systems.   ","Addressing this gap, our study introduces a hierarchical model structure for VCS, providing a novel lens for categorizing and analyzing existing literature in a systematic manner.","We classify attacks based on their technical principles and thoroughly evaluate various attributes, such as their methods, targets, vectors, and behaviors.","Furthermore, we consolidate and assess the defense mechanisms proposed in current research, offering actionable recommendations for enhancing VCS security.","Our work makes a significant contribution by simplifying the complexity inherent in VCS security, aiding designers in effectively identifying and countering potential threats, and setting a foundation for future advancements in VCS security research."],"url":"http://arxiv.org/abs/2405.17100v1","category":"cs.CR"}
{"created":"2024-05-27 12:10:07","title":"Dual feature reduction for the sparse-group lasso and its adaptive variant","abstract":"The sparse-group lasso performs both variable and group selection, making simultaneous use of the strengths of the lasso and group lasso. It has found widespread use in genetics, a field that regularly involves the analysis of high-dimensional data, due to its sparse-group penalty, which allows it to utilize grouping information. However, the sparse-group lasso can be computationally more expensive than both the lasso and group lasso, due to the added shrinkage complexity, and its additional hyper-parameter that needs tuning. In this paper a novel dual feature reduction method, Dual Feature Reduction (DFR), is presented that uses strong screening rules for the sparse-group lasso and the adaptive sparse-group lasso to reduce their input space before optimization. DFR applies two layers of screening and is based on the dual norms of the sparse-group lasso and adaptive sparse-group lasso. Through synthetic and real numerical studies, it is shown that the proposed feature reduction approach is able to drastically reduce the computational cost in many different scenarios.","sentences":["The sparse-group lasso performs both variable and group selection, making simultaneous use of the strengths of the lasso and group lasso.","It has found widespread use in genetics, a field that regularly involves the analysis of high-dimensional data, due to its sparse-group penalty, which allows it to utilize grouping information.","However, the sparse-group lasso can be computationally more expensive than both the lasso and group lasso, due to the added shrinkage complexity, and its additional hyper-parameter that needs tuning.","In this paper a novel dual feature reduction method, Dual Feature Reduction (DFR), is presented that uses strong screening rules for the sparse-group lasso and the adaptive sparse-group lasso to reduce their input space before optimization.","DFR applies two layers of screening and is based on the dual norms of the sparse-group lasso and adaptive sparse-group lasso.","Through synthetic and real numerical studies, it is shown that the proposed feature reduction approach is able to drastically reduce the computational cost in many different scenarios."],"url":"http://arxiv.org/abs/2405.17094v1","category":"stat.ML"}
{"created":"2024-05-27 12:04:01","title":"Symbolic dynamics for the Kuramoto-Sivashinsky PDE on the line II","abstract":"We present a new algorithm for the rigorous integration of the variational equation (i.e. producing $\\mathcal C^1$ estimates) for a class of dissipative PDEs on the torus. As an application for some parameter value for the Kuramoto-Sivashinsky PDE on the line with odd and periodic boundary conditions we prove the existence of infinite number of homo- and heteroclinic orbits to two periodic orbits. The proof is computer assisted.","sentences":["We present a new algorithm for the rigorous integration of the variational equation (i.e. producing $\\mathcal C^1$ estimates) for a class of dissipative PDEs on the torus.","As an application for some parameter value for the Kuramoto-Sivashinsky PDE on the line with odd and periodic boundary conditions we prove the existence of infinite number of homo- and heteroclinic orbits to two periodic orbits.","The proof is computer assisted."],"url":"http://arxiv.org/abs/2405.17087v1","category":"math.DS"}
{"created":"2024-05-27 11:54:51","title":"Effective Layer Pruning Through Similarity Metric Perspective","abstract":"Deep neural networks have been the predominant paradigm in machine learning for solving cognitive tasks. Such models, however, are restricted by a high computational overhead, limiting their applicability and hindering advancements in the field. Extensive research demonstrated that pruning structures from these models is a straightforward approach to reducing network complexity. In this direction, most efforts focus on removing weights or filters. Studies have also been devoted to layer pruning as it promotes superior computational gains. However, layer pruning often hurts the network predictive ability (i.e., accuracy) at high compression rates. This work introduces an effective layer-pruning strategy that meets all underlying properties pursued by pruning methods. Our method estimates the relative importance of a layer using the Centered Kernel Alignment (CKA) metric, employed to measure the similarity between the representations of the unpruned model and a candidate layer for pruning. We confirm the effectiveness of our method on standard architectures and benchmarks, in which it outperforms existing layer-pruning strategies and other state-of-the-art pruning techniques. Particularly, we remove more than 75% of computation while improving predictive ability. At higher compression regimes, our method exhibits negligible accuracy drop, while other methods notably deteriorate model accuracy. Apart from these benefits, our pruned models exhibit robustness to adversarial and out-of-distribution samples.","sentences":["Deep neural networks have been the predominant paradigm in machine learning for solving cognitive tasks.","Such models, however, are restricted by a high computational overhead, limiting their applicability and hindering advancements in the field.","Extensive research demonstrated that pruning structures from these models is a straightforward approach to reducing network complexity.","In this direction, most efforts focus on removing weights or filters.","Studies have also been devoted to layer pruning as it promotes superior computational gains.","However, layer pruning often hurts the network predictive ability (i.e., accuracy) at high compression rates.","This work introduces an effective layer-pruning strategy that meets all underlying properties pursued by pruning methods.","Our method estimates the relative importance of a layer using the Centered Kernel Alignment (CKA) metric, employed to measure the similarity between the representations of the unpruned model and a candidate layer for pruning.","We confirm the effectiveness of our method on standard architectures and benchmarks, in which it outperforms existing layer-pruning strategies and other state-of-the-art pruning techniques.","Particularly, we remove more than 75% of computation while improving predictive ability.","At higher compression regimes, our method exhibits negligible accuracy drop, while other methods notably deteriorate model accuracy.","Apart from these benefits, our pruned models exhibit robustness to adversarial and out-of-distribution samples."],"url":"http://arxiv.org/abs/2405.17081v1","category":"cs.LG"}
{"created":"2024-05-27 11:52:24","title":"Learning with User-Level Local Differential Privacy","abstract":"User-level privacy is important in distributed systems. Previous research primarily focuses on the central model, while the local models have received much less attention. Under the central model, user-level DP is strictly stronger than the item-level one. However, under the local model, the relationship between user-level and item-level LDP becomes more complex, thus the analysis is crucially different. In this paper, we first analyze the mean estimation problem and then apply it to stochastic optimization, classification, and regression. In particular, we propose adaptive strategies to achieve optimal performance at all privacy levels. Moreover, we also obtain information-theoretic lower bounds, which show that the proposed methods are minimax optimal up to logarithmic factors. Unlike the central DP model, where user-level DP always leads to slower convergence, our result shows that under the local model, the convergence rates are nearly the same between user-level and item-level cases for distributions with bounded support. For heavy-tailed distributions, the user-level rate is even faster than the item-level one.","sentences":["User-level privacy is important in distributed systems.","Previous research primarily focuses on the central model, while the local models have received much less attention.","Under the central model, user-level DP is strictly stronger than the item-level one.","However, under the local model, the relationship between user-level and item-level LDP becomes more complex, thus the analysis is crucially different.","In this paper, we first analyze the mean estimation problem and then apply it to stochastic optimization, classification, and regression.","In particular, we propose adaptive strategies to achieve optimal performance at all privacy levels.","Moreover, we also obtain information-theoretic lower bounds, which show that the proposed methods are minimax optimal up to logarithmic factors.","Unlike the central DP model, where user-level DP always leads to slower convergence, our result shows that under the local model, the convergence rates are nearly the same between user-level and item-level cases for distributions with bounded support.","For heavy-tailed distributions, the user-level rate is even faster than the item-level one."],"url":"http://arxiv.org/abs/2405.17079v1","category":"stat.ML"}
{"created":"2024-05-27 11:45:08","title":"Towards Ultra-High-Definition Image Deraining: A Benchmark and An Efficient Method","abstract":"Despite significant progress has been made in image deraining, existing approaches are mostly carried out on low-resolution images. The effectiveness of these methods on high-resolution images is still unknown, especially for ultra-high-definition (UHD) images, given the continuous advancement of imaging devices. In this paper, we focus on the task of UHD image deraining, and contribute the first large-scale UHD image deraining dataset, 4K-Rain13k, that contains 13,000 image pairs at 4K resolution. Based on this dataset, we conduct a benchmark study on existing methods for processing UHD images. Furthermore, we develop an effective and efficient vision MLP-based architecture (UDR-Mixer) to better solve this task. Specifically, our method contains two building components: a spatial feature rearrangement layer that captures long-range information of UHD images, and a frequency feature modulation layer that facilitates high-quality UHD image reconstruction. Extensive experimental results demonstrate that our method performs favorably against the state-of-the-art approaches while maintaining a lower model complexity. The code and dataset will be available at https://github.com/cschenxiang/UDR-Mixer.","sentences":["Despite significant progress has been made in image deraining, existing approaches are mostly carried out on low-resolution images.","The effectiveness of these methods on high-resolution images is still unknown, especially for ultra-high-definition (UHD) images, given the continuous advancement of imaging devices.","In this paper, we focus on the task of UHD image deraining, and contribute the first large-scale UHD image deraining dataset, 4K-Rain13k, that contains 13,000 image pairs at 4K resolution.","Based on this dataset, we conduct a benchmark study on existing methods for processing UHD images.","Furthermore, we develop an effective and efficient vision MLP-based architecture (UDR-Mixer) to better solve this task.","Specifically, our method contains two building components: a spatial feature rearrangement layer that captures long-range information of UHD images, and a frequency feature modulation layer that facilitates high-quality UHD image reconstruction.","Extensive experimental results demonstrate that our method performs favorably against the state-of-the-art approaches while maintaining a lower model complexity.","The code and dataset will be available at https://github.com/cschenxiang/UDR-Mixer."],"url":"http://arxiv.org/abs/2405.17074v1","category":"cs.CV"}
{"created":"2024-05-27 11:43:05","title":"Soft Two-degree-of-freedom Dielectric Elastomer Position Sensor Exhibiting Linear Behavior","abstract":"Soft robots could bring robotic systems to new horizons, by enabling safe human-machine interaction. For precise control, these soft structures require high level position feedback that is not easily achieved through conventional one-degree-of-freedom (DOF) sensing apparatus. In this paper, a soft two-DOF dielectric elastomer (DE) sensor is specifically designed to provide accurate position feedback for a soft polymer robotic manipulator. The technology is exemplified on a soft robot intended for MRI-guided prostate interventions. DEs are chosen for their major advantages of softness, high strains, low cost and embedded multiple-DOF sensing capability, providing excellent system integration. A geometrical model of the proposed DE sensor is developed and compared to experimental results in order to understand sensor mechanics. Using a differential measurement approach, a handmade prototype provided linear sensory behavior and 0.2 mm accuracy on two-DOF. This correlates to a 0.7\\% error over the sensor's 30 mm x 30 mm planar range, demonstrating the outstanding potential of DE technology for accurate multi-DOF position sensing.","sentences":["Soft robots could bring robotic systems to new horizons, by enabling safe human-machine interaction.","For precise control, these soft structures require high level position feedback that is not easily achieved through conventional one-degree-of-freedom (DOF) sensing apparatus.","In this paper, a soft two-DOF dielectric elastomer (DE) sensor is specifically designed to provide accurate position feedback for a soft polymer robotic manipulator.","The technology is exemplified on a soft robot intended for MRI-guided prostate interventions.","DEs are chosen for their major advantages of softness, high strains, low cost and embedded multiple-DOF sensing capability, providing excellent system integration.","A geometrical model of the proposed DE sensor is developed and compared to experimental results in order to understand sensor mechanics.","Using a differential measurement approach, a handmade prototype provided linear sensory behavior and 0.2 mm accuracy on two-DOF.","This correlates to a 0.7\\% error over the sensor's 30 mm x 30 mm planar range, demonstrating the outstanding potential of DE technology for accurate multi-DOF position sensing."],"url":"http://arxiv.org/abs/2405.17073v1","category":"cs.RO"}
{"created":"2024-05-27 11:28:14","title":"Determining the Possibility of Multistationarity in a Model of the Earth Carbon Cycle with Direct Air Capture","abstract":"The existence of steady-state multiplicity or multistationarity in the Earth System raises the possibility that the Earth may reach a \"tipping point\" and rapidly transition to a warmer steady-state from which recovery may be practically impossible. In detailed Earth models that require extensive computation time, it is difficult to make an a priori prediction of the existence of steady-state multiplicity. In this study, we demonstrate Chemical Reaction Network Theory (CRNT) analysis of a simple heuristic box model of the Earth System carbon cycle with the human intervention of direct air capture. The analysis reveals necessary conditions for the combination of system parameters where steady-state multiplicity may exist. With this method, other negative emissions technologies (NET) may be screened in a relatively simple manner to aid in the priority setting by policymakers.","sentences":["The existence of steady-state multiplicity or multistationarity in the Earth System raises the possibility that the Earth may reach a \"tipping point\" and rapidly transition to a warmer steady-state from which recovery may be practically impossible.","In detailed Earth models that require extensive computation time, it is difficult to make an a priori prediction of the existence of steady-state multiplicity.","In this study, we demonstrate Chemical Reaction Network Theory (CRNT) analysis of a simple heuristic box model of the Earth System carbon cycle with the human intervention of direct air capture.","The analysis reveals necessary conditions for the combination of system parameters where steady-state multiplicity may exist.","With this method, other negative emissions technologies (NET) may be screened in a relatively simple manner to aid in the priority setting by policymakers."],"url":"http://arxiv.org/abs/2405.17058v1","category":"math.DS"}
{"created":"2024-05-27 11:03:48","title":"Verifying Properties of Binary Neural Networks Using Sparse Polynomial Optimization","abstract":"This paper explores methods for verifying the properties of Binary Neural Networks (BNNs), focusing on robustness against adversarial attacks. Despite their lower computational and memory needs, BNNs, like their full-precision counterparts, are also sensitive to input perturbations. Established methods for solving this problem are predominantly based on Satisfiability Modulo Theories and Mixed-Integer Linear Programming techniques, which are characterized by NP complexity and often face scalability issues.   We introduce an alternative approach using Semidefinite Programming relaxations derived from sparse Polynomial Optimization. Our approach, compatible with continuous input space, not only mitigates numerical issues associated with floating-point calculations but also enhances verification scalability through the strategic use of tighter first-order semidefinite relaxations. We demonstrate the effectiveness of our method in verifying robustness against both $\\|.\\|_\\infty$ and $\\|.\\|_2$-based adversarial attacks.","sentences":["This paper explores methods for verifying the properties of Binary Neural Networks (BNNs), focusing on robustness against adversarial attacks.","Despite their lower computational and memory needs, BNNs, like their full-precision counterparts, are also sensitive to input perturbations.","Established methods for solving this problem are predominantly based on Satisfiability Modulo Theories and Mixed-Integer Linear Programming techniques, which are characterized by NP complexity and often face scalability issues.   ","We introduce an alternative approach using Semidefinite Programming relaxations derived from sparse Polynomial Optimization.","Our approach, compatible with continuous input space, not only mitigates numerical issues associated with floating-point calculations but also enhances verification scalability through the strategic use of tighter first-order semidefinite relaxations.","We demonstrate the effectiveness of our method in verifying robustness against both $\\|.\\|_\\infty$ and $\\|.\\|_2$-based adversarial attacks."],"url":"http://arxiv.org/abs/2405.17049v1","category":"cs.LG"}
{"created":"2024-05-27 11:02:21","title":"Interpretable Robotic Manipulation from Language","abstract":"Humans naturally employ linguistic instructions to convey knowledge, a process that proves significantly more complex for machines, especially within the context of multitask robotic manipulation environments. Natural language, moreover, serves as the primary medium through which humans acquire new knowledge, presenting a potentially intuitive bridge for translating concepts understandable by humans into formats that can be learned by machines. In pursuit of facilitating this integration, we introduce an explainable behavior cloning agent, named Ex-PERACT, specifically designed for manipulation tasks. This agent is distinguished by its hierarchical structure, which incorporates natural language to enhance the learning process. At the top level, the model is tasked with learning a discrete skill code, while at the bottom level, the policy network translates the problem into a voxelized grid and maps the discretized actions to voxel grids. We evaluate our method across eight challenging manipulation tasks utilizing the RLBench benchmark, demonstrating that Ex-PERACT not only achieves competitive policy performance but also effectively bridges the gap between human instructions and machine execution in complex environments.","sentences":["Humans naturally employ linguistic instructions to convey knowledge, a process that proves significantly more complex for machines, especially within the context of multitask robotic manipulation environments.","Natural language, moreover, serves as the primary medium through which humans acquire new knowledge, presenting a potentially intuitive bridge for translating concepts understandable by humans into formats that can be learned by machines.","In pursuit of facilitating this integration, we introduce an explainable behavior cloning agent, named Ex-PERACT, specifically designed for manipulation tasks.","This agent is distinguished by its hierarchical structure, which incorporates natural language to enhance the learning process.","At the top level, the model is tasked with learning a discrete skill code, while at the bottom level, the policy network translates the problem into a voxelized grid and maps the discretized actions to voxel grids.","We evaluate our method across eight challenging manipulation tasks utilizing the RLBench benchmark, demonstrating that Ex-PERACT not only achieves competitive policy performance but also effectively bridges the gap between human instructions and machine execution in complex environments."],"url":"http://arxiv.org/abs/2405.17047v1","category":"cs.RO"}
{"created":"2024-05-27 11:01:47","title":"A cohomological approach to Ruelle-Pollicott resonances and speed of mixing of Anosov diffeomorphisms","abstract":"We investigate Ruelle-Pollicott resonances of Anosov diffeomorphisms with respect to the measure of maximal entropy. We highlight a profound connection between resonances and eigenvalues of the action induced by the dynamics on de Rham cohomology. We finally exploit this relation to get information about the Ruelle-Pollicott asymptotics of the correlation function and establish a cohomological bound for the speed of mixing of Anosov diffeomorphisms.","sentences":["We investigate Ruelle-Pollicott resonances of Anosov diffeomorphisms with respect to the measure of maximal entropy.","We highlight a profound connection between resonances and eigenvalues of the action induced by the dynamics on de Rham cohomology.","We finally exploit this relation to get information about the Ruelle-Pollicott asymptotics of the correlation function and establish a cohomological bound for the speed of mixing of Anosov diffeomorphisms."],"url":"http://arxiv.org/abs/2405.17045v1","category":"math.DS"}
{"created":"2024-05-27 10:44:02","title":"Exploring Superconductivity: The Interplay of Electronic Orders in Topological Quantum Materials","abstract":"Topological quantum materials hold great promise for future technological applications. Their unique electronic properties, such as protected surface states and exotic quasiparticles, offer opportunities for designing novel electronic devices, spintronics, and quantum information processing. The origin of the interplay between various electronic orders in topological quantum materials, such as superconductivity and magnetism, remains unclear, particularly whether these electronic orders cooperate, compete, or simply coexist. Since the 2000s, the combination of topology and matter has sparked a tremendous surge of interest among theoreticians and experimentalists alike. Novel theoretical descriptions and predictions, as well as complex experimental setups confirming or refuting these theories, continuously appear in renowned journals. This review aims to provide conceptual tools to understand the fundamental concepts of this ever-growing field. Superconductivity and its historical development will serve as a second pillar alongside topological materials. While the primary focus will be on topological superconductors, other topological materials, such as topological insulators and topological semimetals, will also be explained phenomenologically.","sentences":["Topological quantum materials hold great promise for future technological applications.","Their unique electronic properties, such as protected surface states and exotic quasiparticles, offer opportunities for designing novel electronic devices, spintronics, and quantum information processing.","The origin of the interplay between various electronic orders in topological quantum materials, such as superconductivity and magnetism, remains unclear, particularly whether these electronic orders cooperate, compete, or simply coexist.","Since the 2000s, the combination of topology and matter has sparked a tremendous surge of interest among theoreticians and experimentalists alike.","Novel theoretical descriptions and predictions, as well as complex experimental setups confirming or refuting these theories, continuously appear in renowned journals.","This review aims to provide conceptual tools to understand the fundamental concepts of this ever-growing field.","Superconductivity and its historical development will serve as a second pillar alongside topological materials.","While the primary focus will be on topological superconductors, other topological materials, such as topological insulators and topological semimetals, will also be explained phenomenologically."],"url":"http://arxiv.org/abs/2405.17036v1","category":"cond-mat.str-el"}
{"created":"2024-05-27 10:23:23","title":"The Impact of Cumulus Clouds and CCNs Regeneration on Aerosol Vertical Distribution and Size","abstract":"This study employs a high-resolution (10m) System for Atmospheric Modeling (SAM) coupled with the Spectral Bin Microphysical (SBM) scheme to thoroughly investigate the processes governing the evolution of aerosol properties within and outside a shallow cumulus cloud. The model encompasses the complete life cycle of cloud droplets, starting from their formation through their evolution until their complete evaporation or sedimentation to the ground. Additionally, the model tracks the aerosols' evolution both within droplets and in the air. Aerosols are transported within the droplets, grow by droplet coalescence, and are released into the atmosphere after droplet evaporation (regeneration process). The aerosol concentration increases by droplet evaporation and decreases along with falling drops. So, the effects of clouds on the surrounding aerosols depend on the microphysical and dynamic processes, which in turn depend on the amount of background aerosols; here, we compare clean and polluted conditions. It is shown that clouds significantly impact the vertical profile of aerosol concentration in the lower troposphere, as well as their size distribution, and can serve as a source of large (giant) cloud condensation nuclei. Furthermore, it is shown that both precipitating and non-precipitating boundary layer clouds contribute to a substantial increase in aerosol concentration within the inversion layer due to intense evaporation.","sentences":["This study employs a high-resolution (10m) System for Atmospheric Modeling (SAM) coupled with the Spectral Bin Microphysical (SBM) scheme to thoroughly investigate the processes governing the evolution of aerosol properties within and outside a shallow cumulus cloud.","The model encompasses the complete life cycle of cloud droplets, starting from their formation through their evolution until their complete evaporation or sedimentation to the ground.","Additionally, the model tracks the aerosols' evolution both within droplets and in the air.","Aerosols are transported within the droplets, grow by droplet coalescence, and are released into the atmosphere after droplet evaporation (regeneration process).","The aerosol concentration increases by droplet evaporation and decreases along with falling drops.","So, the effects of clouds on the surrounding aerosols depend on the microphysical and dynamic processes, which in turn depend on the amount of background aerosols; here, we compare clean and polluted conditions.","It is shown that clouds significantly impact the vertical profile of aerosol concentration in the lower troposphere, as well as their size distribution, and can serve as a source of large (giant) cloud condensation nuclei.","Furthermore, it is shown that both precipitating and non-precipitating boundary layer clouds contribute to a substantial increase in aerosol concentration within the inversion layer due to intense evaporation."],"url":"http://arxiv.org/abs/2405.17023v1","category":"physics.ao-ph"}
{"created":"2024-05-27 09:57:44","title":"Crystalline part of the Galois cohomology of crystalline representations","abstract":"For $p \\geq 3$ and an unramified extension $F/\\mathbb{Q}_p$ with perfect residue field, we define a syntomic complex with coefficients in a Wach module over a certain period ring for $F$. We show that our complex computes the crystalline part of the Galois cohomology (in the sense of Bloch and Kato) of the associated crystalline representation of the absolute Galois group of $F$. Furthermore, we establish that Wach modules of Berger naturally descend over to a smaller period ring studied by Fontaine and Wach. This enables us to define another syntomic complex with coefficients and we show that its cohomology also computes the crystalline part of the Galois cohomology of the associated representation.","sentences":["For $p \\geq 3$ and an unramified extension $F/\\mathbb{Q}_p$ with perfect residue field, we define a syntomic complex with coefficients in a Wach module over a certain period ring for $F$. We show that our complex computes the crystalline part of the Galois cohomology (in the sense of Bloch and Kato) of the associated crystalline representation of the absolute Galois group of $F$. Furthermore, we establish that Wach modules of Berger naturally descend over to a smaller period ring studied by Fontaine and Wach.","This enables us to define another syntomic complex with coefficients and we show that its cohomology also computes the crystalline part of the Galois cohomology of the associated representation."],"url":"http://arxiv.org/abs/2405.17012v1","category":"math.NT"}
{"created":"2024-05-27 09:53:59","title":"Quantum-safe Edge Applications: How to Secure Computation in Distributed Computing Systems","abstract":"The advent of distributed computing systems will offer great flexibility for application workloads, while also imposing more attention to security, where the future advent and adoption of quantum technology can introduce new security threats. For this reason, the Multi-access Edge Computing (MEC) working group at ETSI has recently started delving into security aspects, especially motivated by the upcoming reality of the MEC federation, which involves services made of application instances belonging to different systems (thus, different trust domains). On the other side, Quantum Key Distribution (QKD) can help strengthen the level of security by enabling the exchange of secure keys through an unconditionally secure protocol, e.g., to secure communication between REST clients and servers in distributed computing systems at the edge. In this paper, we propose a technical solution to achieve this goal, building on standard specifications, namely ETSI MEC and ETSI QKD, and discussing the gaps and limitations of current technology, which hamper full-fledged in-field deployment and mass adoption. Furthermore, we provide our look-ahead view on the future of secure distributed computing through the enticing option of federating edge computing domains.","sentences":["The advent of distributed computing systems will offer great flexibility for application workloads, while also imposing more attention to security, where the future advent and adoption of quantum technology can introduce new security threats.","For this reason, the Multi-access Edge Computing (MEC) working group at ETSI has recently started delving into security aspects, especially motivated by the upcoming reality of the MEC federation, which involves services made of application instances belonging to different systems (thus, different trust domains).","On the other side, Quantum Key Distribution (QKD) can help strengthen the level of security by enabling the exchange of secure keys through an unconditionally secure protocol, e.g., to secure communication between REST clients and servers in distributed computing systems at the edge.","In this paper, we propose a technical solution to achieve this goal, building on standard specifications, namely ETSI MEC and ETSI QKD, and discussing the gaps and limitations of current technology, which hamper full-fledged in-field deployment and mass adoption.","Furthermore, we provide our look-ahead view on the future of secure distributed computing through the enticing option of federating edge computing domains."],"url":"http://arxiv.org/abs/2405.17008v1","category":"cs.NI"}
{"created":"2024-05-27 09:53:17","title":"Waveforms for Computing Over the Air","abstract":"Over-the-air computation (AirComp) leverages the signal-superposition characteristic of wireless multiple access channels to perform mathematical computations. Initially introduced to enhance communication reliability in interference channels and wireless sensor networks, AirComp has more recently found applications in task-oriented communications, namely, for wireless distributed learning and in wireless control systems. Its adoption aims to address latency challenges arising from an increased number of edge devices or IoT devices accessing the constrained wireless spectrum. This paper focuses on the physical layer of these systems, specifically on the waveform and the signal processing aspects at the transmitter and receiver to meet the challenges that AirComp presents within the different contexts and use cases.","sentences":["Over-the-air computation (AirComp) leverages the signal-superposition characteristic of wireless multiple access channels to perform mathematical computations.","Initially introduced to enhance communication reliability in interference channels and wireless sensor networks, AirComp has more recently found applications in task-oriented communications, namely, for wireless distributed learning and in wireless control systems.","Its adoption aims to address latency challenges arising from an increased number of edge devices or IoT devices accessing the constrained wireless spectrum.","This paper focuses on the physical layer of these systems, specifically on the waveform and the signal processing aspects at the transmitter and receiver to meet the challenges that AirComp presents within the different contexts and use cases."],"url":"http://arxiv.org/abs/2405.17007v1","category":"eess.SP"}
{"created":"2024-05-27 09:45:06","title":"Delta-modular ILP Problems of Bounded Co-dimension, Discrepancy, and Convolution","abstract":"For $k, n \\geq 0$, and $c \\in Z^n$, we consider ILP problems \\begin{gather*}   \\max\\bigl\\{ c^\\top x \\colon A x = b,\\, x \\in Z^n_{\\geq 0} \\bigr\\}\\text{ with $A \\in Z^{k \\times n}$, $rank(A) = k$, $b \\in Z^{k}$ and}   \\max\\bigl\\{ c^\\top x \\colon A x \\leq b,\\, x \\in Z^n \\bigr\\} \\text{ with $A \\in Z^{(n+k) \\times n}$, $rank(A) = n$, $b \\in Z^{n+k}$.} \\end{gather*} The first problem is called an \\emph{ILP problem in the standard form of the codimension $k$}, and the second problem is called an \\emph{ILP problem in the canonical form with $n+k$ constraints.} We show that, for any sufficiently large $\\Delta$, both problems can be solved with $$ 2^{O(k)} \\cdot (f_{k,d} \\cdot \\Delta)^2 / 2^{\\Omega\\bigl(\\sqrt{\\log(f_{k,d} \\cdot \\Delta)}\\bigr)} $$ operations, where $   f_{k,d} = \\min \\Bigl\\{ k^{k/2},   \\bigl(\\log k \\cdot \\log (d + k)\\bigr)^{k/2}   \\Bigr\\} $, $d$ is the dimension of a corresponding polyhedron and $\\Delta$ is the maximum absolute value of $rank(A) \\times rank(A)$ sub-determinants of $A$.   As our second main result, we show that the feasibility variants of both problems can be solved with $$ 2^{O(k)} \\cdot f_{k,d} \\cdot \\Delta \\cdot \\log^3(f_{k,d} \\cdot \\Delta) $$ operations. The constant $f_{k,d}$ can be replaced by other constant $g_{k,\\Delta} = \\bigl(\\log k \\cdot \\log (k \\Delta)\\bigr)^{k/2}$ that depends only on $k$ and $\\Delta$. Additionally, we consider different partial cases with $k=0$ and $k=1$, which have interesting applications.   As a result of independent interest, we propose an $n^2/2^{\\Omega\\bigl(\\sqrt{\\log n}\\bigr)}$-time algorithm for the tropical convolution problem on sequences, indexed by elements of a finite Abelian group of the order $n$. This result is obtained, reducing the above problem to the matrix multiplication problem on a tropical semiring and using seminal algorithm by R. Williams.","sentences":["For $k, n \\geq 0$, and $c \\in Z^n$, we consider ILP problems \\begin{gather*}   \\max\\bigl\\{ c^\\top x \\colon A x = b,\\, x \\in Z^n_{\\geq 0} \\bigr\\}\\text{ with $A \\in Z^{k \\times n}$, $rank(A) = k$, $b \\in Z^{k}$ and}   \\max\\bigl\\{ c^\\top x \\colon A x \\leq b,\\, x \\in Z^n \\bigr\\} \\text{ with $A \\in Z^{(n+k)","\\times n}$, $rank(A) = n$, $b \\in Z^{n+k}$.} \\end{gather*} The first problem is called an \\emph{ILP problem in the standard form of the codimension $k$}, and the second problem is called an \\emph{ILP problem in the canonical form with $n+k$ constraints.}","We show that, for any sufficiently large $\\Delta$, both problems can be solved with $$ 2^{O(k)} \\cdot (f_{k,d} \\cdot \\Delta)^2 / 2^{\\Omega\\bigl(\\sqrt{\\log(f_{k,d} \\cdot \\Delta)}\\bigr)} $$ operations, where $   f_{k,d} = \\min \\Bigl\\{ k^{k/2},   \\bigl(\\log k \\cdot \\log (d + k)\\bigr)^{k/2}   \\Bigr\\} $, $d$ is the dimension of a corresponding polyhedron and $\\Delta$ is the maximum absolute value of $rank(A)","\\times rank(A)$ sub-determinants of $A$.   ","As our second main result, we show that the feasibility variants of both problems can be solved with $$ 2^{O(k)} \\cdot f_{k,d} \\cdot \\Delta \\cdot \\log^3(f_{k,d} \\cdot \\Delta) $$ operations.","The constant $f_{k,d}$ can be replaced by other constant $g_{k,\\Delta} = \\bigl(\\log k \\cdot \\log (k \\Delta)\\bigr)^{k/2}$ that depends only on $k$ and $\\Delta$. Additionally, we consider different partial cases with $k=0$ and $k=1$, which have interesting applications.   ","As a result of independent interest, we propose an $n^2/2^{\\Omega\\bigl(\\sqrt{\\log n}\\bigr)}$-time algorithm for the tropical convolution problem on sequences, indexed by elements of a finite Abelian group of the order $n$. This result is obtained, reducing the above problem to the matrix multiplication problem on a tropical semiring and using seminal algorithm by R. Williams."],"url":"http://arxiv.org/abs/2405.17001v1","category":"cs.CC"}
{"created":"2024-05-27 09:43:10","title":"Program Synthesis is $\u03a3_3^0$-Complete","abstract":"This paper considers program synthesis in the context of computational hardness, asking the question: How hard is it to determine whether a given synthesis problem has a solution or not?   To answer this question, this paper studies program synthesis for a basic imperative, Turing-complete language IMP, for which this paper proves that program synthesis is $\\Sigma_3^0$-\\emph{complete} in the arithmetical hierarchy. The proof of this fact relies on a fully constructive encoding of program synthesis (which is typically formulated as a second-order query) as a first-order formula in the standard model of arithmetic (i.e., Peano arithmetic). Constructing such a formula then allows us to reduce the decision problem for COF (the set of functions which diverge only on a finite set of inputs), which is well-known to be a $\\Sigma_3^0$-complete problem, into the constructed first-order representation of synthesis.   In addition to this main result, we also consider the hardness of variants of synthesis problems, such as those introduced in previous work to make program synthesis more tractable (e.g., synthesis over finite examples). To the best of our knowledge, this paper is the first to give a first-order characterization of program synthesis in general, and precisely define the computability of synthesis problems and their variants.","sentences":["This paper considers program synthesis in the context of computational hardness, asking the question: How hard is it to determine whether a given synthesis problem has a solution or not?   ","To answer this question, this paper studies program synthesis for a basic imperative, Turing-complete language IMP, for which this paper proves that program synthesis is $\\Sigma_3^0$-\\emph{complete} in the arithmetical hierarchy.","The proof of this fact relies on a fully constructive encoding of program synthesis (which is typically formulated as a second-order query) as a first-order formula in the standard model of arithmetic (i.e., Peano arithmetic).","Constructing such a formula then allows us to reduce the decision problem for COF (the set of functions which diverge only on a finite set of inputs), which is well-known to be a $\\Sigma_3^0$-complete problem, into the constructed first-order representation of synthesis.   ","In addition to this main result, we also consider the hardness of variants of synthesis problems, such as those introduced in previous work to make program synthesis more tractable (e.g., synthesis over finite examples).","To the best of our knowledge, this paper is the first to give a first-order characterization of program synthesis in general, and precisely define the computability of synthesis problems and their variants."],"url":"http://arxiv.org/abs/2405.16997v1","category":"cs.LO"}
{"created":"2024-05-27 09:42:21","title":"Electron form factors in Basis Light-front Quantization","abstract":"In this paper, we evaluate the electromagnetic and gravitational form factors as well as the corresponding generalized parton distributions of the electron using the Basis Light-front Quantization approach to QED. We compare our results with those from light-front perturbation theory. We adopt a novel basis with its scale depending on the constituents' longitudinal momentum fraction. We show that this basis improves convergence of the form factors with increasing basis dimension, compared to that calculated in the original basis with fixed scale. These results both validate the BLFQ approach and provide guidance for its efficient implementation in solving light-front Hamiltonian mass eigenstates for more complex systems in QED and QCD.","sentences":["In this paper, we evaluate the electromagnetic and gravitational form factors as well as the corresponding generalized parton distributions of the electron using the Basis Light-front Quantization approach to QED.","We compare our results with those from light-front perturbation theory.","We adopt a novel basis with its scale depending on the constituents' longitudinal momentum fraction.","We show that this basis improves convergence of the form factors with increasing basis dimension, compared to that calculated in the original basis with fixed scale.","These results both validate the BLFQ approach and provide guidance for its efficient implementation in solving light-front Hamiltonian mass eigenstates for more complex systems in QED and QCD."],"url":"http://arxiv.org/abs/2405.16995v1","category":"hep-ph"}
{"created":"2024-05-27 09:23:52","title":"Robust kernel-free quadratic surface twin support vector machine with capped $L_1$-norm distance metric","abstract":"Twin support vector machine (TSVM) is a very classical and practical classifier for pattern classification. However, the traditional TSVM has two limitations. Firstly, it uses the L_2-norm distance metric that leads to its sensitivity to outliers. Second, it needs to select the appropriate kernel function and the kernel parameters for nonlinear classification. To effectively avoid these two problems, this paper proposes a robust capped L_1-norm kernel-free quadratic surface twin support vector machine (CL_1QTSVM). The strengths of our model are briefly summarized as follows. 1) The robustness of our model is further improved by employing the capped L_1 norm distance metric. 2) Our model is a kernel-free method that avoids the time-consuming process of selecting appropriate kernel functions and kernel parameters. 3) The introduction of L_2-norm regularization term to improve the generalization ability of the model. 4) To efficiently solve the proposed model, an iterative algorithm is developed. 5) The convergence, time complexity and existence of locally optimal solutions of the developed algorithms are further discussed. Numerical experiments on numerous types of datasets validate the classification performance and robustness of the proposed model.","sentences":["Twin support vector machine (TSVM) is a very classical and practical classifier for pattern classification.","However, the traditional TSVM has two limitations.","Firstly, it uses the L_2-norm distance metric that leads to its sensitivity to outliers.","Second, it needs to select the appropriate kernel function and the kernel parameters for nonlinear classification.","To effectively avoid these two problems, this paper proposes a robust capped L_1-norm kernel-free quadratic surface twin support vector machine (CL_1QTSVM).","The strengths of our model are briefly summarized as follows.","1)","The robustness of our model is further improved by employing the capped L_1 norm distance metric.","2)","Our model is a kernel-free method that avoids the time-consuming process of selecting appropriate kernel functions and kernel parameters.","3)","The introduction of L_2-norm regularization term to improve the generalization ability of the model.","4) To efficiently solve the proposed model, an iterative algorithm is developed.","5) The convergence, time complexity and existence of locally optimal solutions of the developed algorithms are further discussed.","Numerical experiments on numerous types of datasets validate the classification performance and robustness of the proposed model."],"url":"http://arxiv.org/abs/2405.16982v1","category":"cs.IR"}
{"created":"2024-05-27 09:22:47","title":"Characterising Developer Sentiment in Software Components: An Exploratory Study of Gentoo","abstract":"Collaborative software development happens in teams, that cooperate on shared artefacts, and discuss development on online platforms. Due to the complexity of development and the variety of teams, software components often act as effective containers for parallel work and teams.   Past research has shown how communication between team members, especially in an open-source environment, can become extremely toxic, and lead to members leaving the development team. This has a direct effect on the evolution and maintenance of the project in which the former members were active in.   The purpose of our study is two-fold: first, we propose an approach to evaluate, at a finer granularity, the positive and negative emotions in the communication between developers; and second, we aim to characterise a project's development paths, or components, as more or less impacted by the emotions.   Our analysis evaluates single sentences rather than whole messages as the finest granularity of communication. The previous study found that the high positivity or negativity at the sentence level may indirectly impact the writer him/herself, or the reader. In this way, we could highlight specific paths of Gentoo as the most affected by negative emotions, and show how negative emotions have evolved and changed along the same paths.   By joining the analysis of the mailing lists, from which we derive the sentiment of the developers, with the information derived from the development logs, we obtained a longitudinal picture of how development paths have been historically affected by positive or negative emotions. Our study shows that, in recent years, negative emotions have generally decreased in the communication between Gentoo developers. We also show how file paths, as collaborative software development artefacts, were more or less impacted by the emotions of the developers.","sentences":["Collaborative software development happens in teams, that cooperate on shared artefacts, and discuss development on online platforms.","Due to the complexity of development and the variety of teams, software components often act as effective containers for parallel work and teams.   ","Past research has shown how communication between team members, especially in an open-source environment, can become extremely toxic, and lead to members leaving the development team.","This has a direct effect on the evolution and maintenance of the project in which the former members were active in.   ","The purpose of our study is two-fold: first, we propose an approach to evaluate, at a finer granularity, the positive and negative emotions in the communication between developers; and second, we aim to characterise a project's development paths, or components, as more or less impacted by the emotions.   ","Our analysis evaluates single sentences rather than whole messages as the finest granularity of communication.","The previous study found that the high positivity or negativity at the sentence level may indirectly impact the writer him/herself, or the reader.","In this way, we could highlight specific paths of Gentoo as the most affected by negative emotions, and show how negative emotions have evolved and changed along the same paths.   ","By joining the analysis of the mailing lists, from which we derive the sentiment of the developers, with the information derived from the development logs, we obtained a longitudinal picture of how development paths have been historically affected by positive or negative emotions.","Our study shows that, in recent years, negative emotions have generally decreased in the communication between Gentoo developers.","We also show how file paths, as collaborative software development artefacts, were more or less impacted by the emotions of the developers."],"url":"http://arxiv.org/abs/2405.16981v1","category":"cs.SE"}
{"created":"2024-05-27 09:16:56","title":"All iterated function systems are Lipschitz up to an equivalent metric","abstract":"A finite family $\\mathcal{F}=\\{f_1,\\ldots,f_n\\}$ of continuous selfmaps of a given metric space $X$ is called an iterated function system (shortly IFS). In a case of contractive selfmaps of a complete metric space is well-known that IFS has an unique attractor \\cite{Hu}. However, in \\cite{LS} authors studied highly non-contractive IFSs, i.e. such families $\\mathcal{F}=\\{f_1,\\ldots,f_n\\}$ of continuous selfmaps that for any remetrization of $X$ each function $f_i$ has Lipschitz constant $>1, i=1,\\ldots,n.$ They asked when one can remetrize $X$ that $\\mathcal{F}$ is Lipschitz IFS, i.e. all $f_i's$ are Lipschitz (not necessarily contractive), $ i=1,\\ldots,n$. We give a general positive answer for this problem by constructing respective new metric (equivalent to the original one) on $X$, determined by a given family $\\mathcal{F}=\\{f_1,\\ldots,f_n\\}$ of continuous selfmaps of $X$. However, our construction is valid even for some specific infinite families of continuous functions.","sentences":["A finite family $\\mathcal{F}=\\{f_1,\\ldots,f_n\\}$ of continuous selfmaps of a given metric space $X$ is called an iterated function system (shortly IFS).","In a case of contractive selfmaps of a complete metric space is well-known that IFS has an unique attractor \\cite{Hu}.","However, in \\cite{LS} authors studied highly non-contractive IFSs, i.e. such families $\\mathcal{F}=\\{f_1,\\ldots,f_n\\}$ of continuous selfmaps that for any remetrization of $X$ each function $f_i$ has Lipschitz constant $>1, i=1,\\ldots,n.$ They asked when one can remetrize $X$ that $\\mathcal{F}$ is Lipschitz IFS, i.e. all $f_i's$ are Lipschitz (not necessarily contractive), $ i=1,\\ldots,n$. We give a general positive answer for this problem by constructing respective new metric (equivalent to the original one) on $X$, determined by a given family $\\mathcal{F}=\\{f_1,\\ldots,f_n\\}$ of continuous selfmaps of $X$. However, our construction is valid even for some specific infinite families of continuous functions."],"url":"http://arxiv.org/abs/2405.16977v1","category":"math.GN"}
{"created":"2024-05-27 09:16:20","title":"Generalized hydrodynamics and approach to Generalized Gibbs equilibrium for a classical harmonic chain","abstract":"We study the evolution of a classical harmonic chain with nearest-neighbor interactions starting from domain wall initial conditions. The initial state is taken to be either a product of two Gibbs Ensembles (GEs) with unequal temperatures on the two halves of the chain or a product of two Generalized Gibbs Ensembles (GGEs) with different parameters in the two halves. For this system, we construct the Wigner function and demonstrate that its evolution defines the Generalized Hydrodynamics (GHD) describing the evolution of the conserved quantities. We solve the GHD for both finite and infinite chains and compute the evolution of conserved densities and currents. For a finite chain with fixed boundaries, we show that these quantities relax as $\\sim 1/\\sqrt{t}$ to their respective steady-state values given by the final expected GE or GGE state, depending on the initial conditions. Exact expressions for the Lagrange multipliers of the final expected GGE state are obtained in terms of the steady state densities. In the case of an infinite chain, we find that the conserved densities and currents at any finite time exhibit ballistic scaling while, at infinite time, any finite segment of the system can be described by a current-carrying non-equilibrium steady state (NESS). We compute the scaling functions analytically and show that the relaxation to the NESS occurs as $\\sim 1/t$ for the densities and as $\\sim 1/t^2$ for the currents. We compare the analytic results from hydrodynamics with those from exact microscopic numerics and find excellent agreement.","sentences":["We study the evolution of a classical harmonic chain with nearest-neighbor interactions starting from domain wall initial conditions.","The initial state is taken to be either a product of two Gibbs Ensembles (GEs) with unequal temperatures on the two halves of the chain or a product of two Generalized Gibbs Ensembles (GGEs) with different parameters in the two halves.","For this system, we construct the Wigner function and demonstrate that its evolution defines the Generalized Hydrodynamics (GHD) describing the evolution of the conserved quantities.","We solve the GHD for both finite and infinite chains and compute the evolution of conserved densities and currents.","For a finite chain with fixed boundaries, we show that these quantities relax as $\\sim 1/\\sqrt{t}$ to their respective steady-state values given by the final expected GE or GGE state, depending on the initial conditions.","Exact expressions for the Lagrange multipliers of the final expected GGE state are obtained in terms of the steady state densities.","In the case of an infinite chain, we find that the conserved densities and currents at any finite time exhibit ballistic scaling while, at infinite time, any finite segment of the system can be described by a current-carrying non-equilibrium steady state (NESS).","We compute the scaling functions analytically and show that the relaxation to the NESS occurs as $\\sim 1/t$ for the densities and as $\\sim 1/t^2$ for the currents.","We compare the analytic results from hydrodynamics with those from exact microscopic numerics and find excellent agreement."],"url":"http://arxiv.org/abs/2405.16976v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-27 09:08:08","title":"A Correlation- and Mean-Aware Loss Function and Benchmarking Framework to Improve GAN-based Tabular Data Synthesis","abstract":"Advancements in science rely on data sharing. In medicine, where personal data are often involved, synthetic tabular data generated by generative adversarial networks (GANs) offer a promising avenue. However, existing GANs struggle to capture the complexities of real-world tabular data, which often contain a mix of continuous and categorical variables with potential imbalances and dependencies. We propose a novel correlation- and mean-aware loss function designed to address these challenges as a regularizer for GANs. To ensure a rigorous evaluation, we establish a comprehensive benchmarking framework using ten real-world datasets and eight established tabular GAN baselines. The proposed loss function demonstrates statistically significant improvements over existing methods in capturing the true data distribution, significantly enhancing the quality of synthetic data generated with GANs. The benchmarking framework shows that the enhanced synthetic data quality leads to improved performance in downstream machine learning (ML) tasks, ultimately paving the way for easier data sharing.","sentences":["Advancements in science rely on data sharing.","In medicine, where personal data are often involved, synthetic tabular data generated by generative adversarial networks (GANs) offer a promising avenue.","However, existing GANs struggle to capture the complexities of real-world tabular data, which often contain a mix of continuous and categorical variables with potential imbalances and dependencies.","We propose a novel correlation- and mean-aware loss function designed to address these challenges as a regularizer for GANs.","To ensure a rigorous evaluation, we establish a comprehensive benchmarking framework using ten real-world datasets and eight established tabular GAN baselines.","The proposed loss function demonstrates statistically significant improvements over existing methods in capturing the true data distribution, significantly enhancing the quality of synthetic data generated with GANs.","The benchmarking framework shows that the enhanced synthetic data quality leads to improved performance in downstream machine learning (ML) tasks, ultimately paving the way for easier data sharing."],"url":"http://arxiv.org/abs/2405.16971v1","category":"cs.LG"}
{"created":"2024-05-27 09:06:17","title":"Remote control system of a binary tree of switches -- II. balancing for a perfect binary tree","abstract":"We study a tree coloring model introduced by Guidon (2018), initially based on an analogy with a remote control system of a rail yard, seen as switches on a binary tree. For a given binary tree, we formalize the constraints on the coloring, in particular the distribution of the nodes among colors. Following Guidon, we are interested in balanced colorings i.e. colorings which minimize the maximum size of the subsets of the tree nodes distributed by color. With his method, we present balanced colorings for trees of height up to 7. But his method seems difficult to apply for trees of greater height. Also we present another method which gives solutions for arbitrarily large trees. We illustrate it with a balanced coloring for height 8. In the appendix, we give the exact formulas and the asymptotic behavior of the number of colorings as a function of the height of the tree.","sentences":["We study a tree coloring model introduced by Guidon (2018), initially based on an analogy with a remote control system of a rail yard, seen as switches on a binary tree.","For a given binary tree, we formalize the constraints on the coloring, in particular the distribution of the nodes among colors.","Following Guidon, we are interested in balanced colorings i.e. colorings which minimize the maximum size of the subsets of the tree nodes distributed by color.","With his method, we present balanced colorings for trees of height up to 7.","But his method seems difficult to apply for trees of greater height.","Also we present another method which gives solutions for arbitrarily large trees.","We illustrate it with a balanced coloring for height 8.","In the appendix, we give the exact formulas and the asymptotic behavior of the number of colorings as a function of the height of the tree."],"url":"http://arxiv.org/abs/2405.16968v1","category":"cs.DM"}
{"created":"2024-05-27 09:01:22","title":"Imaginary past of a quantum particle moving on imaginary time","abstract":"The analytical continuation of classical equations of motion to complex times suggests that a tunnelling particle spends in the barrier an imaginary duration $i|\\mathcal T|$. Does this mean that it takes a finite time to tunnel, or should tunnelling be seen as an instantaneous process? It is well known that examination of the adiabatic limit in a small additional AC field points towards $|\\mathcal T|$ being the time it takes to traverse the barrier. However, this is only half the story. We probe the transmitted particle's history, and find that it \"remembers\" very little of the field's past behaviour, as if the transit time were close to zero. The ensuing contradiction suggests that the question is ill-posed, and we explain why.","sentences":["The analytical continuation of classical equations of motion to complex times suggests that a tunnelling particle spends in the barrier an imaginary duration","$i|\\mathcal T|$. Does this mean that it takes a finite time to tunnel, or should tunnelling be seen as an instantaneous process?","It is well known that examination of the adiabatic limit in a small additional AC field points towards $|\\mathcal T|$ being the time it takes to traverse the barrier.","However, this is only half the story.","We probe the transmitted particle's history, and find that it \"remembers\" very little of the field's past behaviour, as if the transit time were close to zero.","The ensuing contradiction suggests that the question is ill-posed, and we explain why."],"url":"http://arxiv.org/abs/2405.16967v1","category":"quant-ph"}
{"created":"2024-05-27 08:57:37","title":"Timeliness of Status Update System: The Effect of Parallel Transmission Using Heterogeneous Updating Devices","abstract":"Timely status updating is the premise of emerging interaction-based applications in the Internet of Things (IoT). Using redundant devices to update the status of interest is a promising method to improve the timeliness of information. However, parallel status updating leads to out-of-order arrivals at the monitor, significantly challenging timeliness analysis. This work studies the Age of Information (AoI) of a multi-queue status update system where multiple devices monitor the same physical process. Specifically, two systems are considered: the Basic System, which only has type-1 devices that are ad hoc devices located close to the source, and the Hybrid System, which contains additional type-2 devices that are infrastructure-based devices located in fixed points compared to the Basic System. Using the Stochastic Hybrid Systems (SHS) framework, a mathematical model that combines discrete and continuous dynamics, we derive the expressions of the average AoI of the considered two systems in closed form. Numerical results verify the accuracy of the analysis. It is shown that when the number and parameters of the type-1 devices/type-2 devices are fixed, the logarithm of average AoI will linearly decrease with the logarithm of the total arrival rate of type-2 devices or that of the number of type-1 devices under specific condition. It has also been demonstrated that the proposed systems can significantly outperform the FCFS M/M/N status update system.","sentences":["Timely status updating is the premise of emerging interaction-based applications in the Internet of Things (IoT).","Using redundant devices to update the status of interest is a promising method to improve the timeliness of information.","However, parallel status updating leads to out-of-order arrivals at the monitor, significantly challenging timeliness analysis.","This work studies the Age of Information (AoI) of a multi-queue status update system where multiple devices monitor the same physical process.","Specifically, two systems are considered: the Basic System, which only has type-1 devices that are ad hoc devices located close to the source, and the Hybrid System, which contains additional type-2 devices that are infrastructure-based devices located in fixed points compared to the Basic System.","Using the Stochastic Hybrid Systems (SHS) framework, a mathematical model that combines discrete and continuous dynamics, we derive the expressions of the average AoI of the considered two systems in closed form.","Numerical results verify the accuracy of the analysis.","It is shown that when the number and parameters of the type-1 devices/type-2 devices are fixed, the logarithm of average AoI will linearly decrease with the logarithm of the total arrival rate of type-2 devices or that of the number of type-1 devices under specific condition.","It has also been demonstrated that the proposed systems can significantly outperform the FCFS M/M/N status update system."],"url":"http://arxiv.org/abs/2405.16965v1","category":"cs.IT"}
{"created":"2024-05-27 08:55:55","title":"Log-Noetherian functions","abstract":"We introduce the class of \\emph{Log-Noetherian} (LN) functions. These are holomorphic solutions to algebraic differential equations (in several variables) with logarithmic singularities. We prove an upper bound on the number of solutions for systems of LN equations, resolving in particular Khovanskii's conjecture for Noetherian functions. Consequently, we show that the structure ${\\mathbb R}_\\text{LN}$ generated by LN-functions, as well as its expansion ${\\mathbb R}_\\text{LN,exp}$, are effectively o-minimal: definable sets in these structures admit effective bounds on their complexity in terms of the complexity of the defining formulas.   We show that ${\\mathbb R}_\\text{LN,exp}$ contains the horizontal sections of regular flat connections with quasiunipotent monodromy over algebraic varieties. It therefore contains the universal covers of Shimura varieties and period maps of polarized variations of $\\mathbb Z$-Hodge structures. We also give an effective Pila-Wilkie theorem for ${\\mathbb R}_\\text{LN,exp}$-definable sets. Thus ${\\mathbb R}_\\text{LN,exp}$ can be used as an effective variant of ${\\mathbb R}_\\text{an,exp}$ in the various applications of o-minimality to arithmetic geometry and Hodge theory.","sentences":["We introduce the class of \\emph{Log-Noetherian} (LN) functions.","These are holomorphic solutions to algebraic differential equations (in several variables) with logarithmic singularities.","We prove an upper bound on the number of solutions for systems of LN equations, resolving in particular Khovanskii's conjecture for Noetherian functions.","Consequently, we show that the structure ${\\mathbb R}_\\text{LN}$ generated by LN-functions, as well as its expansion ${\\mathbb R}_\\text{LN,exp}$, are effectively o-minimal: definable sets in these structures admit effective bounds on their complexity in terms of the complexity of the defining formulas.   ","We show that ${\\mathbb R}_\\text{LN,exp}$ contains the horizontal sections of regular flat connections with quasiunipotent monodromy over algebraic varieties.","It therefore contains the universal covers of Shimura varieties and period maps of polarized variations of $\\mathbb Z$-Hodge structures.","We also give an effective Pila-Wilkie theorem for ${\\mathbb R}_\\text{LN,exp}$-definable sets.","Thus ${\\mathbb R}_\\text{LN,exp}$ can be used as an effective variant of ${\\mathbb R}_\\text{an,exp}$ in the various applications of o-minimality to arithmetic geometry and Hodge theory."],"url":"http://arxiv.org/abs/2405.16963v1","category":"math.AG"}
{"created":"2024-05-27 08:55:49","title":"Stochastically accelerated perturbative triples correction in coupled cluster calculations","abstract":"We introduce a novel algorithm that leverages stochastic sampling techniques to compute the perturbative triples correction in the coupled-cluster (CC) framework. By combining elements of randomness and determinism, our algorithm achieves a favorable balance between accuracy and computational cost. The main advantage of this algorithm is that it allows for the calculation to be stopped at any time, providing an unbiased estimate, with a statistical error that goes to zero as the exact calculation is approached. We provide evidence that our semi-stochastic algorithm achieves substantial computational savings compared to traditional deterministic methods. Specifically, we demonstrate that a precision of 0.5 millihartree can be attained with only 10\\% of the computational effort required by the full calculation. This work opens up new avenues for efficient and accurate computations, enabling investigations of complex molecular systems that were previously computationally prohibitive.","sentences":["We introduce a novel algorithm that leverages stochastic sampling techniques to compute the perturbative triples correction in the coupled-cluster (CC) framework.","By combining elements of randomness and determinism, our algorithm achieves a favorable balance between accuracy and computational cost.","The main advantage of this algorithm is that it allows for the calculation to be stopped at any time, providing an unbiased estimate, with a statistical error that goes to zero as the exact calculation is approached.","We provide evidence that our semi-stochastic algorithm achieves substantial computational savings compared to traditional deterministic methods.","Specifically, we demonstrate that a precision of 0.5 millihartree can be attained with only 10\\% of the computational effort required by the full calculation.","This work opens up new avenues for efficient and accurate computations, enabling investigations of complex molecular systems that were previously computationally prohibitive."],"url":"http://arxiv.org/abs/2405.16962v1","category":"physics.chem-ph"}
{"created":"2024-05-27 08:54:11","title":"A Machine Learning Approach to Analyze the Effects of Alzheimer's Disease on Handwriting through Lognormal Features","abstract":"Alzheimer's disease is one of the most incisive illnesses among the neurodegenerative ones, and it causes a progressive decline in cognitive abilities that, in the worst cases, becomes severe enough to interfere with daily life. Currently, there is no cure, so an early diagnosis is strongly needed to try and slow its progression through medical treatments. Handwriting analysis is considered a potential tool for detecting and understanding certain neurological conditions, including Alzheimer's disease. While handwriting analysis alone cannot provide a definitive diagnosis of Alzheimer's, it may offer some insights and be used for a comprehensive assessment. The Sigma-lognormal model is conceived for movement analysis and can also be applied to handwriting. This model returns a set of lognormal parameters as output, which forms the basis for the computation of novel and significant features. This paper presents a machine learning approach applied to handwriting features extracted through the sigma-lognormal model. The aim is to develop a support system to help doctors in the diagnosis and study of Alzheimer, evaluate the effectiveness of the extracted features and finally study the relation among them.","sentences":["Alzheimer's disease is one of the most incisive illnesses among the neurodegenerative ones, and it causes a progressive decline in cognitive abilities that, in the worst cases, becomes severe enough to interfere with daily life.","Currently, there is no cure, so an early diagnosis is strongly needed to try and slow its progression through medical treatments.","Handwriting analysis is considered a potential tool for detecting and understanding certain neurological conditions, including Alzheimer's disease.","While handwriting analysis alone cannot provide a definitive diagnosis of Alzheimer's, it may offer some insights and be used for a comprehensive assessment.","The Sigma-lognormal model is conceived for movement analysis and can also be applied to handwriting.","This model returns a set of lognormal parameters as output, which forms the basis for the computation of novel and significant features.","This paper presents a machine learning approach applied to handwriting features extracted through the sigma-lognormal model.","The aim is to develop a support system to help doctors in the diagnosis and study of Alzheimer, evaluate the effectiveness of the extracted features and finally study the relation among them."],"url":"http://arxiv.org/abs/2405.16959v1","category":"cs.CV"}
{"created":"2024-05-27 08:45:57","title":"Evaluation of Resource-Efficient Crater Detectors on Embedded Systems","abstract":"Real-time analysis of Martian craters is crucial for mission-critical operations, including safe landings and geological exploration. This work leverages the latest breakthroughs for on-the-edge crater detection aboard spacecraft. We rigorously benchmark several YOLO networks using a Mars craters dataset, analyzing their performance on embedded systems with a focus on optimization for low-power devices. We optimize this process for a new wave of cost-effective, commercial-off-the-shelf-based smaller satellites. Implementations on diverse platforms, including Google Coral Edge TPU, AMD Versal SoC VCK190, Nvidia Jetson Nano and Jetson AGX Orin, undergo a detailed trade-off analysis. Our findings identify optimal network-device pairings, enhancing the feasibility of crater detection on resource-constrained hardware and setting a new precedent for efficient and resilient extraterrestrial imaging. Code at: https://github.com/billpsomas/mars_crater_detection.","sentences":["Real-time analysis of Martian craters is crucial for mission-critical operations, including safe landings and geological exploration.","This work leverages the latest breakthroughs for on-the-edge crater detection aboard spacecraft.","We rigorously benchmark several YOLO networks using a Mars craters dataset, analyzing their performance on embedded systems with a focus on optimization for low-power devices.","We optimize this process for a new wave of cost-effective, commercial-off-the-shelf-based smaller satellites.","Implementations on diverse platforms, including Google Coral Edge TPU, AMD Versal SoC VCK190, Nvidia Jetson Nano and Jetson AGX Orin, undergo a detailed trade-off analysis.","Our findings identify optimal network-device pairings, enhancing the feasibility of crater detection on resource-constrained hardware and setting a new precedent for efficient and resilient extraterrestrial imaging.","Code at: https://github.com/billpsomas/mars_crater_detection."],"url":"http://arxiv.org/abs/2405.16953v1","category":"cs.CV"}
{"created":"2024-05-27 08:40:43","title":"Spectral similarities in galaxies through an unsupervised classification of spaxels","abstract":"We present the first unsupervised classification of spaxels in hyperspectral images of individual galaxies. Classes identify regions by spectral similarity and thus take all the information into account that is contained in the data cubes (spatial and spectral).We used Gaussian mixture models in a latent discriminant subspace to find clusters of spaxels. The spectra were corrected for small-scale motions within the galaxy based on emission lines with an automatic algorithm. Our data consist of two MUSE/VLT data cubes of JKB 18 and NGC 1068 and one NIRSpec/JWST data cube of NGC 4151.Our classes identify many regions that are most often easily interpreted. Most of the 11 classes that we find for JKB 18 are identified as photoionised by stars. Some of them are known HII regions, but we mapped them as extended, with gradients of ionisation intensities. One compact structure has not been reported before, and according to diagnostic diagrams, it might be a planetary nebula or a denser HII region. For NGC 1068, our 16 classes are of active galactic nucleus-type (AGN) or star-forming regions. Their spatial distribution corresponds perfectly to well-known structures such as spiral arms and a ring with giant molecular clouds. A subclassification in the nuclear region reveals several structures and gradients in the AGN spectra. Our unsupervised classification of the MUSE data of NGC 1068 helps visualise the complex interaction of the AGN and the jet with the interstellar medium in a single map. The centre of NGC 4151 is very complex, but our classes can easily be related to ionisation cones, the jet, or H2 emission. We find a new elongated structure that is ionised by the AGN along the N-S axis perpendicular to the jet direction. It is rotated counterclockwise with respect to the axis of the H2 emission. Our work shows that the unsupervised classification of spaxels takes full advantage of the richness of the information in the data cubes by presenting the spectral and spatial information in a combined and synthetic way.","sentences":["We present the first unsupervised classification of spaxels in hyperspectral images of individual galaxies.","Classes identify regions by spectral similarity and thus take all the information into account that is contained in the data cubes (spatial and spectral).We used Gaussian mixture models in a latent discriminant subspace to find clusters of spaxels.","The spectra were corrected for small-scale motions within the galaxy based on emission lines with an automatic algorithm.","Our data consist of two MUSE/VLT data cubes of JKB 18 and NGC 1068 and one NIRSpec/JWST data cube of NGC 4151.Our classes identify many regions that are most often easily interpreted.","Most of the 11 classes that we find for JKB 18 are identified as photoionised by stars.","Some of them are known HII regions, but we mapped them as extended, with gradients of ionisation intensities.","One compact structure has not been reported before, and according to diagnostic diagrams, it might be a planetary nebula or a denser HII region.","For NGC 1068, our 16 classes are of active galactic nucleus-type (AGN) or star-forming regions.","Their spatial distribution corresponds perfectly to well-known structures such as spiral arms and a ring with giant molecular clouds.","A subclassification in the nuclear region reveals several structures and gradients in the AGN spectra.","Our unsupervised classification of the MUSE data of NGC 1068 helps visualise the complex interaction of the AGN and the jet with the interstellar medium in a single map.","The centre of NGC 4151 is very complex, but our classes can easily be related to ionisation cones, the jet, or H2 emission.","We find a new elongated structure that is ionised by the AGN along the N-S axis perpendicular to the jet direction.","It is rotated counterclockwise with respect to the axis of the H2 emission.","Our work shows that the unsupervised classification of spaxels takes full advantage of the richness of the information in the data cubes by presenting the spectral and spatial information in a combined and synthetic way."],"url":"http://arxiv.org/abs/2405.16949v1","category":"astro-ph.GA"}
{"created":"2024-05-27 08:37:26","title":"Joint Channel, Data and Radar Parameter Estimation for AFDM Systems in Doubly-Dispersive Channels","abstract":"We propose new schemes for joint channel and data estimation (JCDE) and radar parameter estimation (RPE) in doubly-dispersive channels, such that integrated sensing and communications (ISAC) is enabled by user equipment (UE) independently performing JCDE, and base stations (BSs) performing RPE. The contributed JCDE and RPE schemes are designed for waveforms known to perform well in doubly-dispersive channels, under a unified model that captures the features of either legacy orthogonal frequency division multiplexing (OFDM), state-of-the-art (SotA) orthogonal time frequency space (OTFS), and next-generation affine frequency division multiplexing (AFDM) systems. The proposed JCDE algorithm is based on a Bayesian parametric bilinear Gaussian belief propagation (PBiGaBP) framework first proposed for OTFS and here shown to apply to all aforementioned waveforms, while the RPE scheme is based on a new probabilistic data association (PDA) approach incorporating a Bernoulli-Gaussian denoising, optimized via expectation maximization (EM). Simulation results demonstrate that JCDE in AFDM systems utilizing a single pilot per block significantly outperforms the SotA alternative even if the latter is granted a substantial power advantage. Similarly, the AFDM-based RPE scheme is found to outperform the OTFS-based approach, as well as the sparse Bayesian learning (SBL) technique, regardless of the waveform used.","sentences":["We propose new schemes for joint channel and data estimation (JCDE) and radar parameter estimation (RPE) in doubly-dispersive channels, such that integrated sensing and communications (ISAC) is enabled by user equipment (UE) independently performing JCDE, and base stations (BSs) performing RPE.","The contributed JCDE and RPE schemes are designed for waveforms known to perform well in doubly-dispersive channels, under a unified model that captures the features of either legacy orthogonal frequency division multiplexing (OFDM), state-of-the-art (SotA) orthogonal time frequency space (OTFS), and next-generation affine frequency division multiplexing (AFDM) systems.","The proposed JCDE algorithm is based on a Bayesian parametric bilinear Gaussian belief propagation (PBiGaBP) framework first proposed for OTFS and here shown to apply to all aforementioned waveforms, while the RPE scheme is based on a new probabilistic data association (PDA) approach incorporating a Bernoulli-Gaussian denoising, optimized via expectation maximization (EM).","Simulation results demonstrate that JCDE in AFDM systems utilizing a single pilot per block significantly outperforms the SotA alternative even if the latter is granted a substantial power advantage.","Similarly, the AFDM-based RPE scheme is found to outperform the OTFS-based approach, as well as the sparse Bayesian learning (SBL) technique, regardless of the waveform used."],"url":"http://arxiv.org/abs/2405.16945v1","category":"eess.SP"}
{"created":"2024-05-27 08:37:16","title":"Even- and Odd-denominator Fractional Quantum Anomalous Hall Effect in Graphene Moire Superlattices","abstract":"Fractional quantum anomalous hall effect (FQAHE), a transport effect with fractionally quantized Hall plateau emerging under zero magnetic field, provides a radically new opportunity to engineer topological quantum electronics. By construction of topological flat band with moire engineering, intrinsic FQAHE has been observed in twisted MoTe2 system and rhombohedral pentalayer graphene/hBN moire superlattices with anomalous Hall resistivity quantization number C <= 2/3 including the gapless composite Fermi-liquid state with C = 1/2. Here we experimentally demonstrate a new system of rhombohedral hexalayer graphene (RHG)/hBN moire superlattices showing both fractional and integer quantum anomalous Hall effects when the lowest flat Chern band is fractionally and fully filled at zero magnetic field. The zero-field Hall resistance Rho_xy = h/Ce2 is quantized to values corresponding to C = 3/5, 2/3, 5/7, 3/4, 7/9 and 1 at moire filling factors v = 3/5, 2/3, 5/7, 3/4, 7/9 and 1, respectively. Particularly, the C = 3/4 FQAHE state at v = 3/4 moire filling featuring a minimum of longitudinal resistance Rho_xx and fractionally quantized Hall resistance Rho_xy = 4h/3e2, is observed for the first time under zero magnetic field. Such a state may be similar to the C = 3/4 fractional quantum hall (FQHE) state recently observed at high magnetic fields9,10 and possibly host fractional charge excitations obeying non-Abelian statistics. By tuning the electrical and magnetic fields at 0 < v < 1, we have observed a sign reversal of the Hall resistivity for v = 2/3 state, indicating a transition from quasi-electron-like excitations to quasi-hole ones. Our experiment has established RHG/hBN moire superlattices a promising platform to explore quasi-particles with fractional charge excitations and non-Abelian anyons at zero magnetic field.","sentences":["Fractional quantum anomalous hall effect (FQAHE), a transport effect with fractionally quantized Hall plateau emerging under zero magnetic field, provides a radically new opportunity to engineer topological quantum electronics.","By construction of topological flat band with moire engineering, intrinsic FQAHE has been observed in twisted MoTe2 system and rhombohedral pentalayer graphene/hBN moire superlattices with anomalous Hall resistivity quantization number C <= 2/3 including the gapless composite Fermi-liquid state with C = 1/2.","Here we experimentally demonstrate a new system of rhombohedral hexalayer graphene (RHG)/hBN moire superlattices showing both fractional and integer quantum anomalous Hall effects when the lowest flat Chern band is fractionally and fully filled at zero magnetic field.","The zero-field Hall resistance Rho_xy = h/Ce2 is quantized to values corresponding to C = 3/5, 2/3, 5/7, 3/4, 7/9 and 1 at moire filling factors v = 3/5, 2/3, 5/7, 3/4, 7/9 and 1, respectively.","Particularly, the C = 3/4 FQAHE state at v = 3/4 moire filling featuring a minimum of longitudinal resistance Rho_xx and fractionally quantized Hall resistance Rho_xy = 4h/3e2, is observed for the first time under zero magnetic field.","Such a state may be similar to the C = 3/4 fractional quantum hall (FQHE) state recently observed at high magnetic fields9,10 and possibly host fractional charge excitations obeying non-Abelian statistics.","By tuning the electrical and magnetic fields at 0 < v < 1, we have observed a sign reversal of the Hall resistivity for v = 2/3 state, indicating a transition from quasi-electron-like excitations to quasi-hole ones.","Our experiment has established RHG/hBN moire superlattices a promising platform to explore quasi-particles with fractional charge excitations and non-Abelian anyons at zero magnetic field."],"url":"http://arxiv.org/abs/2405.16944v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-27 08:37:01","title":"A counterexample to the weak Shanks conjecture","abstract":"We give an example of a function $f$ non-vanishing in the closed bidisk and the affine polynomial minimizing the norm of $1-pf$ in the Hardy space of the bidisk among all affine polynomials $p$. We show that this polynomial vanishes inside the bidisk. This provides a counterexample to the weakest form of a conjecture due to Shanks that has been open since 1980, with applications that arose from digital filter design. This counterexample has a simple form and follows naturally from [7], where the phenomenon of zeros seeping into the unit disk was already observed for similar minimization problems in one variable.","sentences":["We give an example of a function $f$ non-vanishing in the closed bidisk and the affine polynomial minimizing the norm of $1-pf$ in the Hardy space of the bidisk among all affine polynomials $p$. We show that this polynomial vanishes inside the bidisk.","This provides a counterexample to the weakest form of a conjecture due to Shanks that has been open since 1980, with applications that arose from digital filter design.","This counterexample has a simple form and follows naturally from [7], where the phenomenon of zeros seeping into the unit disk was already observed for similar minimization problems in one variable."],"url":"http://arxiv.org/abs/2405.16943v1","category":"math.CV"}
{"created":"2024-05-27 08:30:29","title":"Adversarial Attacks on Both Face Recognition and Face Anti-spoofing Models","abstract":"Adversarial attacks on Face Recognition (FR) systems have proven highly effective in compromising pure FR models, yet adversarial examples may be ineffective to the complete FR systems as Face Anti-Spoofing (FAS) models are often incorporated and can detect a significant number of them. To address this under-explored and essential problem, we propose a novel setting of adversarially attacking both FR and FAS models simultaneously, aiming to enhance the practicability of adversarial attacks on FR systems. In particular, we introduce a new attack method, namely Style-aligned Distribution Biasing (SDB), to improve the capacity of black-box attacks on both FR and FAS models. Specifically, our SDB framework consists of three key components. Firstly, to enhance the transferability of FAS models, we design a Distribution-aware Score Biasing module to optimize adversarial face examples away from the distribution of spoof images utilizing scores. Secondly, to mitigate the substantial style differences between live images and adversarial examples initialized with spoof images, we introduce an Instance Style Alignment module that aligns the style of adversarial examples with live images. In addition, to alleviate the conflicts between the gradients of FR and FAS models, we propose a Gradient Consistency Maintenance module to minimize disparities between the gradients using Hessian approximation. Extensive experiments showcase the superiority of our proposed attack method to state-of-the-art adversarial attacks.","sentences":["Adversarial attacks on Face Recognition (FR) systems have proven highly effective in compromising pure FR models, yet adversarial examples may be ineffective to the complete FR systems as Face Anti-Spoofing (FAS) models are often incorporated and can detect a significant number of them.","To address this under-explored and essential problem, we propose a novel setting of adversarially attacking both FR and FAS models simultaneously, aiming to enhance the practicability of adversarial attacks on FR systems.","In particular, we introduce a new attack method, namely Style-aligned Distribution Biasing (SDB), to improve the capacity of black-box attacks on both FR and FAS models.","Specifically, our SDB framework consists of three key components.","Firstly, to enhance the transferability of FAS models, we design a Distribution-aware Score Biasing module to optimize adversarial face examples away from the distribution of spoof images utilizing scores.","Secondly, to mitigate the substantial style differences between live images and adversarial examples initialized with spoof images, we introduce an Instance Style Alignment module that aligns the style of adversarial examples with live images.","In addition, to alleviate the conflicts between the gradients of FR and FAS models, we propose a Gradient Consistency Maintenance module to minimize disparities between the gradients using Hessian approximation.","Extensive experiments showcase the superiority of our proposed attack method to state-of-the-art adversarial attacks."],"url":"http://arxiv.org/abs/2405.16940v1","category":"cs.CV"}
{"created":"2024-05-27 08:29:07","title":"Remote control system of a binary tree of switches -- I. constraints and inequalities","abstract":"We study a tree coloring model introduced by Guidon (2018), initially based on an analogy with a remote control system of a rail yard, seen as a switch tree. For a given rooted tree, we formalize the constraints on the coloring, in particular on the minimum number of colors, and on the distribution of the nodes among colors. We show that the sequence $(a_1,a_2,a_3,\\cdots)$, where $a_i$ denotes the number of nodes with color $i$, satisfies a set of inequalities which only involve the sequence $(n_0,n_1,n_2,\\cdots)$ where $n_i$ denotes the number of nodes with height $i$. By coloring the nodes according to their depth, we deduce that these inequalities also apply to the sequence $(d_0,d_1,d_2,\\cdots)$ where $d_i$ denotes the number of nodes with depth $i$.","sentences":["We study a tree coloring model introduced by Guidon (2018), initially based on an analogy with a remote control system of a rail yard, seen as a switch tree.","For a given rooted tree, we formalize the constraints on the coloring, in particular on the minimum number of colors, and on the distribution of the nodes among colors.","We show that the sequence $(a_1,a_2,a_3,\\cdots)$, where $a_i$ denotes the number of nodes with color $i$, satisfies a set of inequalities which only involve the sequence $(n_0,n_1,n_2,\\cdots)$ where $n_i$ denotes the number of nodes with height $i$. By coloring the nodes according to their depth, we deduce that these inequalities also apply to the sequence $(d_0,d_1,d_2,\\cdots)$ where $d_i$ denotes the number of nodes with depth $i$."],"url":"http://arxiv.org/abs/2405.16938v1","category":"cs.DM"}
{"created":"2024-05-27 08:26:19","title":"CudaSIFT-SLAM: multiple-map visual SLAM for full procedure mapping in real human endoscopy","abstract":"Monocular visual simultaneous localization and mapping (V-SLAM) is nowadays an irreplaceable tool in mobile robotics and augmented reality, where it performs robustly. However, human colonoscopies pose formidable challenges like occlusions, blur, light changes, lack of texture, deformation, water jets or tool interaction, which result in very frequent tracking losses. ORB-SLAM3, the top performing multiple-map V-SLAM, is unable to recover from them by merging sub-maps or relocalizing the camera, due to the poor performance of its place recognition algorithm based on ORB features and DBoW2 bag-of-words.   We present CudaSIFT-SLAM, the first V-SLAM system able to process complete human colonoscopies in real-time. To overcome the limitations of ORB-SLAM3, we use SIFT instead of ORB features and replace the DBoW2 direct index with the more computationally demanding brute-force matching, being able to successfully match images separated in time for relocation and map merging. Real-time performance is achieved thanks to CudaSIFT, a GPU implementation for SIFT extraction and brute-force matching.   We benchmark our system in the C3VD phantom colon dataset, and in a full real colonoscopy from the Endomapper dataset, demonstrating the capabilities to merge sub-maps and relocate in them, obtaining significantly longer sub-maps. Our system successfully maps in real-time 88 % of the frames in the C3VD dataset. In a real screening colonoscopy, despite the much higher prevalence of occluded and blurred frames, the mapping coverage is 53 % in carefully explored areas and 38 % in the full sequence, a 70 % improvement over ORB-SLAM3.","sentences":["Monocular visual simultaneous localization and mapping (V-SLAM) is nowadays an irreplaceable tool in mobile robotics and augmented reality, where it performs robustly.","However, human colonoscopies pose formidable challenges like occlusions, blur, light changes, lack of texture, deformation, water jets or tool interaction, which result in very frequent tracking losses.","ORB-SLAM3, the top performing multiple-map V-SLAM, is unable to recover from them by merging sub-maps or relocalizing the camera, due to the poor performance of its place recognition algorithm based on ORB features and DBoW2 bag-of-words.   ","We present CudaSIFT-SLAM, the first V-SLAM system able to process complete human colonoscopies in real-time.","To overcome the limitations of ORB-SLAM3, we use SIFT instead of ORB features and replace the DBoW2 direct index with the more computationally demanding brute-force matching, being able to successfully match images separated in time for relocation and map merging.","Real-time performance is achieved thanks to CudaSIFT, a GPU implementation for SIFT extraction and brute-force matching.   ","We benchmark our system in the C3VD phantom colon dataset, and in a full real colonoscopy from the Endomapper dataset, demonstrating the capabilities to merge sub-maps and relocate in them, obtaining significantly longer sub-maps.","Our system successfully maps in real-time 88 % of the frames in the C3VD dataset.","In a real screening colonoscopy, despite the much higher prevalence of occluded and blurred frames, the mapping coverage is 53 % in carefully explored areas and 38 % in the full sequence, a 70 % improvement over ORB-SLAM3."],"url":"http://arxiv.org/abs/2405.16932v1","category":"cs.RO"}
{"created":"2024-05-27 08:22:32","title":"TopoLa: a novel embedding framework for understanding complex networks","abstract":"Complex networks, which are the abstractions of many real-world systems, present a persistent challenge across disciplines for people to decipher their underlying information. Recently, hyperbolic geometry of latent spaces has gained traction in network analysis, due to its ability to preserve certain local intrinsic properties of the nodes. In this study, we explore the problem from a much broader perspective: understanding the impact of nodes' global topological structures on latent space placements. Our investigations reveal a direct correlation between the topological structure of nodes and their positioning within the latent space. Building on this deep and strong connection between node distance and network topology, we propose a novel embedding framework called Topology-encoded Latent Hyperbolic Geometry (TopoLa) for analyzing complex networks. With the encoded topological information in the latent space, TopoLa is capable of enhancing both conventional and low-rank networks, using the singular value gap to clarify the mathematical principles behind this enhancement. Meanwhile, we show that the equipped TopoLa distance can also help augment pivotal deep learning models encompassing knowledge distillation and contrastive learning.","sentences":["Complex networks, which are the abstractions of many real-world systems, present a persistent challenge across disciplines for people to decipher their underlying information.","Recently, hyperbolic geometry of latent spaces has gained traction in network analysis, due to its ability to preserve certain local intrinsic properties of the nodes.","In this study, we explore the problem from a much broader perspective: understanding the impact of nodes' global topological structures on latent space placements.","Our investigations reveal a direct correlation between the topological structure of nodes and their positioning within the latent space.","Building on this deep and strong connection between node distance and network topology, we propose a novel embedding framework called Topology-encoded Latent Hyperbolic Geometry (TopoLa) for analyzing complex networks.","With the encoded topological information in the latent space, TopoLa is capable of enhancing both conventional and low-rank networks, using the singular value gap to clarify the mathematical principles behind this enhancement.","Meanwhile, we show that the equipped TopoLa distance can also help augment pivotal deep learning models encompassing knowledge distillation and contrastive learning."],"url":"http://arxiv.org/abs/2405.16928v1","category":"cs.SI"}
{"created":"2024-05-27 08:20:53","title":"The role of spatial dimension in the emergence of localised radial patterns from a Turing instability","abstract":"The emergence of localised radial patterns from a Turing instability has been well studied in two and three dimensional settings and predicted for higher spatial dimensions. We prove the existence of localised $(n+1)$-dimensional radial patterns in general two-component reaction-diffusion systems near a Turing instability, where $n>0$ is taken to be a continuous parameter. We determine explicit dependence of each pattern's radial profile on the dimension $n$ through the introduction of $(n+1)$-dimensional Bessel functions, revealing a deep connection between the formation of localised radial patterns in different spatial dimensions.","sentences":["The emergence of localised radial patterns from a Turing instability has been well studied in two and three dimensional settings and predicted for higher spatial dimensions.","We prove the existence of localised $(n+1)$-dimensional radial patterns in general two-component reaction-diffusion systems near a Turing instability, where $n>0$ is taken to be a continuous parameter.","We determine explicit dependence of each pattern's radial profile on the dimension $n$ through the introduction of $(n+1)$-dimensional Bessel functions, revealing a deep connection between the formation of localised radial patterns in different spatial dimensions."],"url":"http://arxiv.org/abs/2405.16927v1","category":"math.DS"}
{"created":"2024-05-27 08:15:10","title":"SA-GS: Semantic-Aware Gaussian Splatting for Large Scene Reconstruction with Geometry Constrain","abstract":"With the emergence of Gaussian Splats, recent efforts have focused on large-scale scene geometric reconstruction. However, most of these efforts either concentrate on memory reduction or spatial space division, neglecting information in the semantic space. In this paper, we propose a novel method, named SA-GS, for fine-grained 3D geometry reconstruction using semantic-aware 3D Gaussian Splats. Specifically, we leverage prior information stored in large vision models such as SAM and DINO to generate semantic masks. We then introduce a geometric complexity measurement function to serve as soft regularization, guiding the shape of each Gaussian Splat within specific semantic areas. Additionally, we present a method that estimates the expected number of Gaussian Splats in different semantic areas, effectively providing a lower bound for Gaussian Splats in these areas. Subsequently, we extract the point cloud using a novel probability density-based extraction method, transforming Gaussian Splats into a point cloud crucial for downstream tasks. Our method also offers the potential for detailed semantic inquiries while maintaining high image-based reconstruction results. We provide extensive experiments on publicly available large-scale scene reconstruction datasets with highly accurate point clouds as ground truth and our novel dataset. Our results demonstrate the superiority of our method over current state-of-the-art Gaussian Splats reconstruction methods by a significant margin in terms of geometric-based measurement metrics. Code and additional results will soon be available on our project page.","sentences":["With the emergence of Gaussian Splats, recent efforts have focused on large-scale scene geometric reconstruction.","However, most of these efforts either concentrate on memory reduction or spatial space division, neglecting information in the semantic space.","In this paper, we propose a novel method, named SA-GS, for fine-grained 3D geometry reconstruction using semantic-aware 3D Gaussian Splats.","Specifically, we leverage prior information stored in large vision models such as SAM and DINO to generate semantic masks.","We then introduce a geometric complexity measurement function to serve as soft regularization, guiding the shape of each Gaussian Splat within specific semantic areas.","Additionally, we present a method that estimates the expected number of Gaussian Splats in different semantic areas, effectively providing a lower bound for Gaussian Splats in these areas.","Subsequently, we extract the point cloud using a novel probability density-based extraction method, transforming Gaussian Splats into a point cloud crucial for downstream tasks.","Our method also offers the potential for detailed semantic inquiries while maintaining high image-based reconstruction results.","We provide extensive experiments on publicly available large-scale scene reconstruction datasets with highly accurate point clouds as ground truth and our novel dataset.","Our results demonstrate the superiority of our method over current state-of-the-art Gaussian Splats reconstruction methods by a significant margin in terms of geometric-based measurement metrics.","Code and additional results will soon be available on our project page."],"url":"http://arxiv.org/abs/2405.16923v1","category":"cs.CV"}
{"created":"2024-05-27 08:08:54","title":"A numerical integration scheme for vectorised phase-space of one-dimensional collision-free, electrostatic systems","abstract":"The kinetic analyses of many-particle soft matter often employ many simulation studies of various physical phenomena which supplement the experimental limitations or compliment the theoretical findings of the study. Such simulations are generally conducted by the numerical integration techniques of the governing equations. In the typical case of collisionless electrostatic systems such as electrostatic plasmas, the Vlasov-Poisson (VP) equation system governs the dynamical evolution of the particle phase-space. The one-dimensional position-velocity (1D-1V) particle phase-space, on the other hand, is known to exhibit direct analogy with ordinary two-dimensional fluids, wherein the Vlasov equation resembles the fluid continuity equation of an in-compressible fluid. On the basis of this fluid-analogy, we present, in this work, a new numerical integration scheme which treats the 1D-1V phase-space as a two-dimensional fluid vector space. We then perform and present analyses of numerical accuracy of this scheme and compare its speed and accuracy with the well-known finite splitting scheme, which is a standardised technique for the numerical Vlasov-Poisson integration. Finally, we show some simulation results of the 1D collisionless electrostatic plasma which highlight the higher speed and accuracy of the new scheme. This work presents a fast and sufficiently accurate numerical integration technique of the VP system which can be directly employed in various simulation studies of many particle systems, including plasmas.","sentences":["The kinetic analyses of many-particle soft matter often employ many simulation studies of various physical phenomena which supplement the experimental limitations or compliment the theoretical findings of the study.","Such simulations are generally conducted by the numerical integration techniques of the governing equations.","In the typical case of collisionless electrostatic systems such as electrostatic plasmas, the Vlasov-Poisson (VP) equation system governs the dynamical evolution of the particle phase-space.","The one-dimensional position-velocity (1D-1V) particle phase-space, on the other hand, is known to exhibit direct analogy with ordinary two-dimensional fluids, wherein the Vlasov equation resembles the fluid continuity equation of an in-compressible fluid.","On the basis of this fluid-analogy, we present, in this work, a new numerical integration scheme which treats the 1D-1V phase-space as a two-dimensional fluid vector space.","We then perform and present analyses of numerical accuracy of this scheme and compare its speed and accuracy with the well-known finite splitting scheme, which is a standardised technique for the numerical Vlasov-Poisson integration.","Finally, we show some simulation results of the 1D collisionless electrostatic plasma which highlight the higher speed and accuracy of the new scheme.","This work presents a fast and sufficiently accurate numerical integration technique of the VP system which can be directly employed in various simulation studies of many particle systems, including plasmas."],"url":"http://arxiv.org/abs/2405.16916v1","category":"physics.plasm-ph"}
{"created":"2024-05-27 08:06:21","title":"Rigorous Simulation-based Testing for Autonomous Driving Systems -- Targeting the Achilles' Heel of Four Open Autopilots","abstract":"Simulation-based testing remains the main approach for validating Autonomous Driving Systems. We propose a rigorous test method based on breaking down scenarios into simple ones, taking into account the fact that autopilots make decisions according to traffic rules whose application depends on local knowledge and context. This leads us to consider the autopilot as a dynamic system receiving three different types of vistas as input, each characterizing a specific driving operation and a corresponding control policy.   The test method for the considered vista types generates test cases for critical configurations that place the vehicle under test in critical situations characterized by the transition from cautious behavior to progression in order to clear an obstacle. The test cases thus generated are realistic, i.e., they determine the initial conditions from which safe control policies are possible, based on knowledge of the vehicle's dynamic characteristics. Constraint analysis identifies the most critical test cases, whose success implies the validity of less critical ones. Test coverage can therefore be greatly simplified. Critical test cases reveal major defects in Apollo, Autoware, and the Carla and LGSVL autopilots. Defects include accidents, software failures, and traffic rule violations that would be difficult to detect by random simulation, as the test cases lead to situations characterized by finely-tuned parameters of the vehicles involved, such as their relative position and speed.   Our results corroborate real-life observations and confirm that autonomous driving systems still have a long way to go before offering acceptable safety guarantees.","sentences":["Simulation-based testing remains the main approach for validating Autonomous Driving Systems.","We propose a rigorous test method based on breaking down scenarios into simple ones, taking into account the fact that autopilots make decisions according to traffic rules whose application depends on local knowledge and context.","This leads us to consider the autopilot as a dynamic system receiving three different types of vistas as input, each characterizing a specific driving operation and a corresponding control policy.   ","The test method for the considered vista types generates test cases for critical configurations that place the vehicle under test in critical situations characterized by the transition from cautious behavior to progression in order to clear an obstacle.","The test cases thus generated are realistic, i.e., they determine the initial conditions from which safe control policies are possible, based on knowledge of the vehicle's dynamic characteristics.","Constraint analysis identifies the most critical test cases, whose success implies the validity of less critical ones.","Test coverage can therefore be greatly simplified.","Critical test cases reveal major defects in Apollo, Autoware, and the Carla and LGSVL autopilots.","Defects include accidents, software failures, and traffic rule violations that would be difficult to detect by random simulation, as the test cases lead to situations characterized by finely-tuned parameters of the vehicles involved, such as their relative position and speed.   ","Our results corroborate real-life observations and confirm that autonomous driving systems still have a long way to go before offering acceptable safety guarantees."],"url":"http://arxiv.org/abs/2405.16914v1","category":"cs.SE"}
{"created":"2024-05-27 08:05:38","title":"Chasing the eternal sun: Does a global super grid favor the deployment of solar power?","abstract":"The One Sun One World One Grid (OSOWOG) initiative advocates the development of a global Super grid for sharing renewable energy, especially solar energy. This study evaluates the economic benefits of such a Super grid, which connects six large regions spanning from Australia to the US, utilizing a detailed energy system optimization model and considering heterogeneous discount rates among countries. Integrating the six regions into a Super grid reduces the electricity system cost by 3.8% compared to isolating them. In contrast, grid expansion within each region reduces the electricity system cost by 12% on average. The economic benefits of the OSOWOG initiative's global Super grid expansion seem to be rather limited. Moreover, the allowance for a Super grid consistently results in decreased investments in solar power, indicating that it is not an effective strategy for enhancing the deployment of solar power, even when transmission grids covering 18 time zones are available.","sentences":["The One Sun One World One Grid (OSOWOG) initiative advocates the development of a global Super grid for sharing renewable energy, especially solar energy.","This study evaluates the economic benefits of such a Super grid, which connects six large regions spanning from Australia to the US, utilizing a detailed energy system optimization model and considering heterogeneous discount rates among countries.","Integrating the six regions into a Super grid reduces the electricity system cost by 3.8% compared to isolating them.","In contrast, grid expansion within each region reduces the electricity system cost by 12% on average.","The economic benefits of the OSOWOG initiative's global Super grid expansion seem to be rather limited.","Moreover, the allowance for a Super grid consistently results in decreased investments in solar power, indicating that it is not an effective strategy for enhancing the deployment of solar power, even when transmission grids covering 18 time zones are available."],"url":"http://arxiv.org/abs/2405.16913v1","category":"physics.soc-ph"}
{"created":"2024-05-27 08:01:06","title":"On the Analytical Properties of a Nonlinear Microscopic Dynamical Model for Connected and Automated Vehicles","abstract":"In this paper, we propose an integrated dynamical model of Connected and Automated Vehicles (CAVs) which incorporates CAV technologies and a microscopic car-following model to improve safety, efficiency and convenience. We rigorously investigate the analytical properties such as well-posedness, maximum principle, perturbation and stability of the proposed model in some proper functional spaces. Furthermore, we prove that the model is collision free and we derive and explicit lower bound on the distance as a safety measure.","sentences":["In this paper, we propose an integrated dynamical model of Connected and Automated Vehicles (CAVs) which incorporates CAV technologies and a microscopic car-following model to improve safety, efficiency and convenience.","We rigorously investigate the analytical properties such as well-posedness, maximum principle, perturbation and stability of the proposed model in some proper functional spaces.","Furthermore, we prove that the model is collision free and we derive and explicit lower bound on the distance as a safety measure."],"url":"http://arxiv.org/abs/2405.16912v1","category":"math.DS"}
{"created":"2024-05-27 08:00:00","title":"CycloDSP: A cyclostationary signal analysis tool for GNU Radio","abstract":"In this paper, we present a first attempt to incorporate in the GNU Radio ecosystem a tool called CycloDSP, devoted to the analysis of complex-valued cyclostationary signals. Such signals are ubiquitous in communication and signal processing, exhibiting periodic or almost periodic statistics that are characterized by a countable set of cycle frequencies, which are related to the main signal periodicities. Common cycle frequencies for modulated signals are multiple of the baudrate and/or combination of the carrier frequency and baudrate. Conventional estimation strategies for cyclostationary signal analysis typically exhibit a high computational burden. Many approaches aimed at reducing complexity exploit fast Fourier transform (FFT) algorithms, which are very efficient for batch data but are not suited for continuously streaming data, typically encountered in software-defined radio (SDR) applications. The aim of this paper is to develop an out-of-tree (OOT) GNU Radio module containing a set of building block functions, aimed at estimating functions, such as the cyclic correlation functions, typically employed for cyclostationary signal analysis. The proposed implementation must be designed so as to ensure high efficiency in processing continuous data streams of complex samples, both in terms of computational load and data storage, in order to be executed on general purpose computers. We tested our implementation by estimating in real-time the second-order cyclic statistics of a Gaussian minimum-shift keying (GMSK) modulated signal, which belongs to a class of signals commonly employed in aeronautical telemetry datalinks.","sentences":["In this paper, we present a first attempt to incorporate in the GNU Radio ecosystem a tool called CycloDSP, devoted to the analysis of complex-valued cyclostationary signals.","Such signals are ubiquitous in communication and signal processing, exhibiting periodic or almost periodic statistics that are characterized by a countable set of cycle frequencies, which are related to the main signal periodicities.","Common cycle frequencies for modulated signals are multiple of the baudrate and/or combination of the carrier frequency and baudrate.","Conventional estimation strategies for cyclostationary signal analysis typically exhibit a high computational burden.","Many approaches aimed at reducing complexity exploit fast Fourier transform (FFT) algorithms, which are very efficient for batch data but are not suited for continuously streaming data, typically encountered in software-defined radio (SDR) applications.","The aim of this paper is to develop an out-of-tree (OOT) GNU Radio module containing a set of building block functions, aimed at estimating functions, such as the cyclic correlation functions, typically employed for cyclostationary signal analysis.","The proposed implementation must be designed so as to ensure high efficiency in processing continuous data streams of complex samples, both in terms of computational load and data storage, in order to be executed on general purpose computers.","We tested our implementation by estimating in real-time the second-order cyclic statistics of a Gaussian minimum-shift keying (GMSK) modulated signal, which belongs to a class of signals commonly employed in aeronautical telemetry datalinks."],"url":"http://arxiv.org/abs/2405.16911v1","category":"eess.SP"}
{"created":"2024-05-27 07:53:51","title":"Privacy and Security Trade-off in Interconnected Systems with Known or Unknown Privacy Noise Covariance","abstract":"This paper is concerned with the security problem for interconnected systems, where each subsystem is required to detect local attacks using locally available information and the information received from its neighboring subsystems. Moreover, we consider that there exists an additional eavesdropper being able to infer the private information by eavesdropping transmitted data between subsystems. Then, a privacy-preserving method is employed by adding privacy noise to transmitted data, and the privacy level is measured by mutual information. Nevertheless, adding privacy noise to transmitted data may affect the detection performance metrics such as detection probability and false alarm probability. Thus, we theoretically analyze the trade-off between the privacy and the detection performance. An optimization problem with maximizing both the degree of privacy preservation and the detection probability is established to obtain the covariance of the privacy noise. In addition, the attack detector of each subsystem may not obtain all information about the privacy noise. We further theoretically analyze the trade-off between the privacy and the false alarm probability when the attack detector has no knowledge of the privacy noise covariance. An optimization problem with maximizing the degree of privacy preservation with guaranteeing a bound of false alarm distortion level is established to obtain {\\color{black}{the covariance of the privacy noise}}. Moreover, to analyze the effect of the privacy noise on the detection probability, we consider that each subsystem can estimate the unknown privacy noise covariance by the secondary data. Based on the estimated covariance, we construct another attack detector and analyze how the privacy noise affects its detection performance. Finally, a numerical example is provided to verify the effectiveness of theoretical results.","sentences":["This paper is concerned with the security problem for interconnected systems, where each subsystem is required to detect local attacks using locally available information and the information received from its neighboring subsystems.","Moreover, we consider that there exists an additional eavesdropper being able to infer the private information by eavesdropping transmitted data between subsystems.","Then, a privacy-preserving method is employed by adding privacy noise to transmitted data, and the privacy level is measured by mutual information.","Nevertheless, adding privacy noise to transmitted data may affect the detection performance metrics such as detection probability and false alarm probability.","Thus, we theoretically analyze the trade-off between the privacy and the detection performance.","An optimization problem with maximizing both the degree of privacy preservation and the detection probability is established to obtain the covariance of the privacy noise.","In addition, the attack detector of each subsystem may not obtain all information about the privacy noise.","We further theoretically analyze the trade-off between the privacy and the false alarm probability when the attack detector has no knowledge of the privacy noise covariance.","An optimization problem with maximizing the degree of privacy preservation with guaranteeing a bound of false alarm distortion level is established to obtain {\\color{black}{the covariance of the privacy noise}}.","Moreover, to analyze the effect of the privacy noise on the detection probability, we consider that each subsystem can estimate the unknown privacy noise covariance by the secondary data.","Based on the estimated covariance, we construct another attack detector and analyze how the privacy noise affects its detection performance.","Finally, a numerical example is provided to verify the effectiveness of theoretical results."],"url":"http://arxiv.org/abs/2405.16905v1","category":"eess.SY"}
{"created":"2024-05-27 07:53:16","title":"Kaczmarz Projection Algorithms in Moving Window: Performance Improvement via Extended Orthogonality & Forgetting","abstract":"New Kaczmarz algorithms with rank two gain update, extended orthogonality property and forgetting mechanism which includes both exponential and instantaneous forgetting (implemented via a proper choice of the forgetting factor and the window size) are introduced and associated in this report with well-known Kaczmarz algorithms with rank one update.","sentences":["New Kaczmarz algorithms with rank two gain update, extended orthogonality property and forgetting mechanism which includes both exponential and instantaneous forgetting (implemented via a proper choice of the forgetting factor and the window size) are introduced and associated in this report with well-known Kaczmarz algorithms with rank one update."],"url":"http://arxiv.org/abs/2405.16903v1","category":"math.OC"}
{"created":"2024-05-27 07:46:46","title":"Distributed Riemannian Stochastic Gradient Tracking Algorithm on the Stiefel Manifold","abstract":"This paper focus on investigating the distributed Riemannian stochastic optimization problem on the Stiefel manifold for multi-agent systems, where all the agents work collaboratively to optimize a function modeled by the average of their expectation-valued local costs. Each agent only processes its own local cost function and communicate with neighboring agents to achieve optimal results while ensuring consensus. Since the local Riemannian gradient in stochastic regimes cannot be directly calculated, we will estimate the gradient by the average of a variable number of sampled gradient, which however brings about noise to the system. We then propose a distributed Riemannian stochastic optimization algorithm on the Stiefel manifold by combining the variable sample size gradient approximation method with the gradient tracking dynamic. It is worth noticing that the suitably chosen increasing sample size plays an important role in improving the algorithm efficiency, as it reduces the noise variance. In an expectation-valued sense, the iterates of all agents are proved to converge to a stationary point (or neighborhood) with fixed step sizes. We further establish the convergence rate of the iterates for the cases when the sample size is exponentially increasing, polynomial increasing, or a constant, respectively. Finally, numerical experiments are implemented to demonstrate the theoretical results.","sentences":["This paper focus on investigating the distributed Riemannian stochastic optimization problem on the Stiefel manifold for multi-agent systems, where all the agents work collaboratively to optimize a function modeled by the average of their expectation-valued local costs.","Each agent only processes its own local cost function and communicate with neighboring agents to achieve optimal results while ensuring consensus.","Since the local Riemannian gradient in stochastic regimes cannot be directly calculated, we will estimate the gradient by the average of a variable number of sampled gradient, which however brings about noise to the system.","We then propose a distributed Riemannian stochastic optimization algorithm on the Stiefel manifold by combining the variable sample size gradient approximation method with the gradient tracking dynamic.","It is worth noticing that the suitably chosen increasing sample size plays an important role in improving the algorithm efficiency, as it reduces the noise variance.","In an expectation-valued sense, the iterates of all agents are proved to converge to a stationary point (or neighborhood) with fixed step sizes.","We further establish the convergence rate of the iterates for the cases when the sample size is exponentially increasing, polynomial increasing, or a constant, respectively.","Finally, numerical experiments are implemented to demonstrate the theoretical results."],"url":"http://arxiv.org/abs/2405.16900v1","category":"math.OC"}
{"created":"2024-05-27 07:43:54","title":"Unconventional complexity classes in unconventional computing (extended abstract)","abstract":"Many unconventional computing models, including some that appear to be quite different from traditional ones such as Turing machines, happen to characterise either the complexity class P or PSPACE when working in deterministic polynomial time (and in the maximally parallel way, where this applies). We discuss variants of cellular automata and membrane systems that escape this dichotomy and characterise intermediate complexity classes, usually defined in terms of Turing machines with oracles, as well as some possible reasons why this happens.","sentences":["Many unconventional computing models, including some that appear to be quite different from traditional ones such as Turing machines, happen to characterise either the complexity class P or PSPACE when working in deterministic polynomial time (and in the maximally parallel way, where this applies).","We discuss variants of cellular automata and membrane systems that escape this dichotomy and characterise intermediate complexity classes, usually defined in terms of Turing machines with oracles, as well as some possible reasons why this happens."],"url":"http://arxiv.org/abs/2405.16896v1","category":"cs.CC"}
{"created":"2024-05-27 17:59:35","title":"From Neurons to Neutrons: A Case Study in Interpretability","abstract":"Mechanistic Interpretability (MI) promises a path toward fully understanding how neural networks make their predictions. Prior work demonstrates that even when trained to perform simple arithmetic, models can implement a variety of algorithms (sometimes concurrently) depending on initialization and hyperparameters. Does this mean neuron-level interpretability techniques have limited applicability? We argue that high-dimensional neural networks can learn low-dimensional representations of their training data that are useful beyond simply making good predictions. Such representations can be understood through the mechanistic interpretability lens and provide insights that are surprisingly faithful to human-derived domain knowledge. This indicates that such approaches to interpretability can be useful for deriving a new understanding of a problem from models trained to solve it. As a case study, we extract nuclear physics concepts by studying models trained to reproduce nuclear data.","sentences":["Mechanistic Interpretability (MI) promises a path toward fully understanding how neural networks make their predictions.","Prior work demonstrates that even when trained to perform simple arithmetic, models can implement a variety of algorithms (sometimes concurrently) depending on initialization and hyperparameters.","Does this mean neuron-level interpretability techniques have limited applicability?","We argue that high-dimensional neural networks can learn low-dimensional representations of their training data that are useful beyond simply making good predictions.","Such representations can be understood through the mechanistic interpretability lens and provide insights that are surprisingly faithful to human-derived domain knowledge.","This indicates that such approaches to interpretability can be useful for deriving a new understanding of a problem from models trained to solve it.","As a case study, we extract nuclear physics concepts by studying models trained to reproduce nuclear data."],"url":"http://arxiv.org/abs/2405.17425v1","category":"cs.LG"}
{"created":"2024-05-27 17:59:04","title":"Survival of the Fittest Representation: A Case Study with Modular Addition","abstract":"When a neural network can learn multiple distinct algorithms to solve a task, how does it \"choose\" between them during training? To approach this question, we take inspiration from ecology: when multiple species coexist, they eventually reach an equilibrium where some survive while others die out. Analogously, we suggest that a neural network at initialization contains many solutions (representations and algorithms), which compete with each other under pressure from resource constraints, with the \"fittest\" ultimately prevailing. To investigate this Survival of the Fittest hypothesis, we conduct a case study on neural networks performing modular addition, and find that these networks' multiple circular representations at different Fourier frequencies undergo such competitive dynamics, with only a few circles surviving at the end. We find that the frequencies with high initial signals and gradients, the \"fittest,\" are more likely to survive. By increasing the embedding dimension, we also observe more surviving frequencies. Inspired by the Lotka-Volterra equations describing the dynamics between species, we find that the dynamics of the circles can be nicely characterized by a set of linear differential equations. Our results with modular addition show that it is possible to decompose complicated representations into simpler components, along with their basic interactions, to offer insight on the training dynamics of representations.","sentences":["When a neural network can learn multiple distinct algorithms to solve a task, how does it \"choose\" between them during training?","To approach this question, we take inspiration from ecology: when multiple species coexist, they eventually reach an equilibrium where some survive while others die out.","Analogously, we suggest that a neural network at initialization contains many solutions (representations and algorithms), which compete with each other under pressure from resource constraints, with the \"fittest\" ultimately prevailing.","To investigate this Survival of the Fittest hypothesis, we conduct a case study on neural networks performing modular addition, and find that these networks' multiple circular representations at different Fourier frequencies undergo such competitive dynamics, with only a few circles surviving at the end.","We find that the frequencies with high initial signals and gradients, the \"fittest,\" are more likely to survive.","By increasing the embedding dimension, we also observe more surviving frequencies.","Inspired by the Lotka-Volterra equations describing the dynamics between species, we find that the dynamics of the circles can be nicely characterized by a set of linear differential equations.","Our results with modular addition show that it is possible to decompose complicated representations into simpler components, along with their basic interactions, to offer insight on the training dynamics of representations."],"url":"http://arxiv.org/abs/2405.17420v1","category":"cs.LG"}
{"created":"2024-05-27 17:58:44","title":"Critical one-arm probability for the metric Gaussian free field in low dimensions","abstract":"We investigate the bond percolation model on transient weighted graphs ${G}$ induced by the excursion sets of the Gaussian free field on the corresponding metric graph. Under the sole assumption that its sign clusters do not percolate, we derive an extension of Lupu's formula for the two-point function at criticality. We then focus on the low-dimensional case $0< \\nu < \\frac{\\alpha}{2}$, where $\\alpha$ governs the polynomial volume growth of $G$ and $\\nu$ the decay rate of the Green's function on $G$. In particular, this includes the benchmark case ${G}=\\mathbb{Z}^3$, for which $\\alpha=3$ and $\\nu= \\alpha-2=1$. We prove under these assumptions that the critical one-arm probability decays with distance $R$ like $R^{-\\frac{\\nu}{2}}$, up to multiplicative constants.","sentences":["We investigate the bond percolation model on transient weighted graphs ${G}$ induced by the excursion sets of the Gaussian free field on the corresponding metric graph.","Under the sole assumption that its sign clusters do not percolate, we derive an extension of Lupu's formula for the two-point function at criticality.","We then focus on the low-dimensional case $0< \\nu < \\frac{\\alpha}{2}$, where $\\alpha$ governs the polynomial volume growth of $G$ and $\\nu$ the decay rate of the Green's function on $G$. In particular, this includes the benchmark case ${G}=\\mathbb{Z}^3$, for which $\\alpha=3$ and $\\nu= \\alpha-2=1$. We prove under these assumptions that the critical one-arm probability decays with distance $R$ like $R^{-\\frac{\\nu}{2}}$, up to multiplicative constants."],"url":"http://arxiv.org/abs/2405.17417v1","category":"math.PR"}
{"created":"2024-05-27 17:51:08","title":"RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control","abstract":"We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models. Existing training-free approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text descriptions, (b) unwanted content leakage from reference style images, and (c) effective composition of style and content. RB-Modulation is built on a novel stochastic optimal controller where a style descriptor encodes the desired attributes through a terminal cost. The resulting drift not only overcomes the difficulties above, but also ensures high fidelity to the reference style and adheres to the given text prompt. We also introduce a cross-attention-based feature aggregation scheme that allows RB-Modulation to decouple content and style from the reference image. With theoretical justification and empirical evidence, our framework demonstrates precise extraction and control of content and style in a training-free manner. Further, our method allows a seamless composition of content and style, which marks a departure from the dependency on external adapters or ControlNets.","sentences":["We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models.","Existing training-free approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text descriptions, (b) unwanted content leakage from reference style images, and (c) effective composition of style and content.","RB-Modulation is built on a novel stochastic optimal controller where a style descriptor encodes the desired attributes through a terminal cost.","The resulting drift not only overcomes the difficulties above, but also ensures high fidelity to the reference style and adheres to the given text prompt.","We also introduce a cross-attention-based feature aggregation scheme that allows RB-Modulation to decouple content and style from the reference image.","With theoretical justification and empirical evidence, our framework demonstrates precise extraction and control of content and style in a training-free manner.","Further, our method allows a seamless composition of content and style, which marks a departure from the dependency on external adapters or ControlNets."],"url":"http://arxiv.org/abs/2405.17401v1","category":"cs.LG"}
{"created":"2024-05-27 17:36:17","title":"Classifying 2D topological phases: mapping ground states to string-nets","abstract":"We prove the conjectured classification of topological phases in two spatial dimensions with gappable boundary, in a simplified setting. Two gapped ground states of lattice Hamiltonians are in the same quantum phase of matter, or topological phase, if they can be connected by a constant-depth quantum circuit. It is conjectured that the Levin-Wen string-net models exhaust all possible gapped phases with gappable boundary, and these phases are labeled by unitary modular tensor categories. We prove this under the assumption that every phase has a representative state with zero correlation length satisfying the entanglement bootstrap axioms, or a strict form of area law. Our main technical development is to transform these states into string-net states using constant-depth quantum circuits.","sentences":["We prove the conjectured classification of topological phases in two spatial dimensions with gappable boundary, in a simplified setting.","Two gapped ground states of lattice Hamiltonians are in the same quantum phase of matter, or topological phase, if they can be connected by a constant-depth quantum circuit.","It is conjectured that the Levin-Wen string-net models exhaust all possible gapped phases with gappable boundary, and these phases are labeled by unitary modular tensor categories.","We prove this under the assumption that every phase has a representative state with zero correlation length satisfying the entanglement bootstrap axioms, or a strict form of area law.","Our main technical development is to transform these states into string-net states using constant-depth quantum circuits."],"url":"http://arxiv.org/abs/2405.17379v1","category":"quant-ph"}
{"created":"2024-05-27 17:33:03","title":"How Does Perfect Fitting Affect Representation Learning? On the Training Dynamics of Representations in Deep Neural Networks","abstract":"In this paper, we elucidate how representations in deep neural networks (DNNs) evolve during training. We focus on overparameterized learning settings where the training continues much after the trained DNN starts to perfectly fit its training data. We examine the evolution of learned representations along the entire training process, including its perfect fitting regime, and with respect to the epoch-wise double descent phenomenon. We explore the representational similarity of DNN layers, each layer with respect to its own representations throughout the training process. For this, we use two similarity metrics: (1) The centered kernel alignment (CKA) similarity; (2) Similarity of decision regions of linear classifier probes that we train for the DNN layers. Our extensive experiments discover training dynamics patterns that can emerge in layers depending on the relative layer-depth, DNN width, and architecture. We show that representations at the deeper layers evolve much more in the training when an epoch-wise double descent occurs. For Vision Transformer, we show that the perfect fitting threshold creates a transition in the evolution of representations across all the encoder blocks.","sentences":["In this paper, we elucidate how representations in deep neural networks (DNNs) evolve during training.","We focus on overparameterized learning settings where the training continues much after the trained DNN starts to perfectly fit its training data.","We examine the evolution of learned representations along the entire training process, including its perfect fitting regime, and with respect to the epoch-wise double descent phenomenon.","We explore the representational similarity of DNN layers, each layer with respect to its own representations throughout the training process.","For this, we use two similarity metrics: (1) The centered kernel alignment (CKA) similarity; (2) Similarity of decision regions of linear classifier probes that we train for the DNN layers.","Our extensive experiments discover training dynamics patterns that can emerge in layers depending on the relative layer-depth, DNN width, and architecture.","We show that representations at the deeper layers evolve much more in the training when an epoch-wise double descent occurs.","For Vision Transformer, we show that the perfect fitting threshold creates a transition in the evolution of representations across all the encoder blocks."],"url":"http://arxiv.org/abs/2405.17377v1","category":"cs.LG"}
{"created":"2024-05-27 17:02:27","title":"DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution","abstract":"Fine-tuning large-scale pre-trained models is inherently a resource-intensive task. While it can enhance the capabilities of the model, it also incurs substantial computational costs, posing challenges to the practical application of downstream tasks. Existing parameter-efficient fine-tuning (PEFT) methods such as Low-Rank Adaptation (LoRA) rely on a bypass framework that ignores the differential parameter budget requirements across weight matrices, which may lead to suboptimal fine-tuning outcomes. To address this issue, we introduce the Dynamic Low-Rank Adaptation (DoRA) method. DoRA decomposes high-rank LoRA layers into structured single-rank components, allowing for dynamic pruning of parameter budget based on their importance to specific tasks during training, which makes the most of the limited parameter budget. Experimental results demonstrate that DoRA can achieve competitive performance compared with LoRA and full model fine-tuning, and outperform various strong baselines with the same storage parameter budget. Our code is available at https://github.com/Yulongmao1/DoRA/","sentences":["Fine-tuning large-scale pre-trained models is inherently a resource-intensive task.","While it can enhance the capabilities of the model, it also incurs substantial computational costs, posing challenges to the practical application of downstream tasks.","Existing parameter-efficient fine-tuning (PEFT) methods such as Low-Rank Adaptation (LoRA) rely on a bypass framework that ignores the differential parameter budget requirements across weight matrices, which may lead to suboptimal fine-tuning outcomes.","To address this issue, we introduce the Dynamic Low-Rank Adaptation (DoRA) method.","DoRA decomposes high-rank LoRA layers into structured single-rank components, allowing for dynamic pruning of parameter budget based on their importance to specific tasks during training, which makes the most of the limited parameter budget.","Experimental results demonstrate that DoRA can achieve competitive performance compared with LoRA and full model fine-tuning, and outperform various strong baselines with the same storage parameter budget.","Our code is available at https://github.com/Yulongmao1/DoRA/"],"url":"http://arxiv.org/abs/2405.17357v1","category":"cs.CL"}
{"created":"2024-05-27 17:02:16","title":"Physical Implementability for Reversible Magic State Manipulation","abstract":"Magic states are essential for achieving universal quantum computation. This study introduces a reversible framework for the manipulation of magic states in odd dimensions, delineating a necessary and sufficient condition for the exact transformations between magic states under maps that preserve the trace of states and positivity of discrete Wigner representation. Utilizing the stochastic formalism, we demonstrate that magic mana emerges as the unique measure for such reversible magic state transformations. We propose the concept of physical implementability for characterizing the hardness and cost of maintaining reversibility. Our findings show that, analogous to the entanglement theory, going beyond the positivity constraint enables an exact reversible theory of magic manipulation, thereby hinting at a potential incongruity between the reversibility of quantum resources and the fundamental principles of quantum mechanics. Physical implementability for reversible manipulation provides a new perspective for understanding and quantifying quantum resources, contributing to an operational framework for understanding the cost of reversible quantum resource manipulation.","sentences":["Magic states are essential for achieving universal quantum computation.","This study introduces a reversible framework for the manipulation of magic states in odd dimensions, delineating a necessary and sufficient condition for the exact transformations between magic states under maps that preserve the trace of states and positivity of discrete Wigner representation.","Utilizing the stochastic formalism, we demonstrate that magic mana emerges as the unique measure for such reversible magic state transformations.","We propose the concept of physical implementability for characterizing the hardness and cost of maintaining reversibility.","Our findings show that, analogous to the entanglement theory, going beyond the positivity constraint enables an exact reversible theory of magic manipulation, thereby hinting at a potential incongruity between the reversibility of quantum resources and the fundamental principles of quantum mechanics.","Physical implementability for reversible manipulation provides a new perspective for understanding and quantifying quantum resources, contributing to an operational framework for understanding the cost of reversible quantum resource manipulation."],"url":"http://arxiv.org/abs/2405.17356v1","category":"quant-ph"}
{"created":"2024-05-27 16:57:27","title":"Optimizing topology for quantum probing with discrete-time quantum walks","abstract":"Discrete-time quantum walk (DTQW) represents a convenient mathematical framework for describing the motion of a particle on a discrete set of positions when this motion is conditioned by the values of certain internal degrees of freedom, which are usually referred to as the {\\em coin} of the particle. As such, and owing to the inherent dependence of the position distribution on the coin degrees of freedom, DTQWs naturally emerge as promising candidates for quantum metrology. In this paper, we explore the use of DTQWs as quantum probes in scenarios where the parameter of interest is encoded in the internal degree of freedom of the walker, and investigate the role of the topology of the walker's space on the attainable precision. In particular, we start considering the encoding of the parameter by rotations for a walker on the line, and evaluate the quantum Fisher information (QFI) and the position Fisher information (FI), explicitly determining the optimal initial state in position space that maximizes the QFI across all encoding schemes. This allows us to understand the role of interference in the position space and to introduce an optimal topology, which maximizes the QFI of the coin parameter and makes the position FI equal to the QFI.","sentences":["Discrete-time quantum walk (DTQW) represents a convenient mathematical framework for describing the motion of a particle on a discrete set of positions when this motion is conditioned by the values of certain internal degrees of freedom, which are usually referred to as the {\\em coin} of the particle.","As such, and owing to the inherent dependence of the position distribution on the coin degrees of freedom, DTQWs naturally emerge as promising candidates for quantum metrology.","In this paper, we explore the use of DTQWs as quantum probes in scenarios where the parameter of interest is encoded in the internal degree of freedom of the walker, and investigate the role of the topology of the walker's space on the attainable precision.","In particular, we start considering the encoding of the parameter by rotations for a walker on the line, and evaluate the quantum Fisher information (QFI) and the position Fisher information (FI), explicitly determining the optimal initial state in position space that maximizes the QFI across all encoding schemes.","This allows us to understand the role of interference in the position space and to introduce an optimal topology, which maximizes the QFI of the coin parameter and makes the position FI equal to the QFI."],"url":"http://arxiv.org/abs/2405.17354v1","category":"quant-ph"}
{"created":"2024-05-27 16:27:47","title":"Charge Transport and Defects in Sulfur-Deficient Chalcogenide Perovskite BaZrS$_3$","abstract":"Exploring the conduction mechanism in the chalcogenide perovskite BaZrS$_3$ is of significant interest due to its potential suitability as a top absorber layer in silicon-based tandem solar cells and other optoelectronic applications. Theoretical and experimental studies anticipate native ambipolar doping in BaZrS$_3$, although experimental validation remains limited. This study reveals a transition from highly insulating behavior to n-type conductivity in BaZrS$_3$ through annealing in an S-poor environment. BaZrS$_3$ thin films are synthesized $\\textit{via}$ a two step process: co-sputtering of Ba-Zr followed by sulfurization at 600 $^{\\circ}$C, and subsequent annealing in high vacuum. UV-Vis measurement reveal a red-shift in the absorption edge concurrent with sample color darkening after annealing. The increase in defect density with vacuum annealing, coupled with low activation energy and n-type character of defects, strongly suggests that sulfur vacancies (V$_{\\mathrm{S}}$) are responsible, in agreement with theoretical predictions. The shift of the Fermi level towards conduction band minimum, quantified by Hard X-ray Photoelectron Spectroscopy (Ga K$\\alpha$, 9.25 keV), further corroborates the induced n-type of conductivity in annealed samples. Our findings indicate that vacuum annealing induces V$_{\\mathrm{S}}$ defects that dominate the charge transport, thereby making BaZrS$_3$ an n-type semiconductor under S-poor conditions. This study offers crucial insights into understanding the defect properties of BaZrS$_3$, facilitating further improvements for its use in solar cell applications.","sentences":["Exploring the conduction mechanism in the chalcogenide perovskite BaZrS$_3$ is of significant interest due to its potential suitability as a top absorber layer in silicon-based tandem solar cells and other optoelectronic applications.","Theoretical and experimental studies anticipate native ambipolar doping in BaZrS$_3$, although experimental validation remains limited.","This study reveals a transition from highly insulating behavior to n-type conductivity in BaZrS$_3$ through annealing in an S-poor environment.","BaZrS$_3$ thin films are synthesized $\\textit{via}$ a two step process: co-sputtering of Ba-Zr followed by sulfurization at 600 $^{\\circ}$C, and subsequent annealing in high vacuum.","UV-Vis measurement reveal a red-shift in the absorption edge concurrent with sample color darkening after annealing.","The increase in defect density with vacuum annealing, coupled with low activation energy and n-type character of defects, strongly suggests that sulfur vacancies (V$_{\\mathrm{S}}$) are responsible, in agreement with theoretical predictions.","The shift of the Fermi level towards conduction band minimum, quantified by Hard X-ray Photoelectron Spectroscopy (Ga K$\\alpha$, 9.25 keV), further corroborates the induced n-type of conductivity in annealed samples.","Our findings indicate that vacuum annealing induces V$_{\\mathrm{S}}$ defects that dominate the charge transport, thereby making BaZrS$_3$ an n-type semiconductor under S-poor conditions.","This study offers crucial insights into understanding the defect properties of BaZrS$_3$, facilitating further improvements for its use in solar cell applications."],"url":"http://arxiv.org/abs/2405.17327v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-27 16:21:13","title":"Tunable magnetism in Nitride MXenes:consequences of atomic layer stacking","abstract":"We have performed Density Functional Theory (DFT) based calculations to investigate the effects of stacking patterns on the electronic and magnetic properties of several Nitride MXenes. MXenes, a relatively new addition to the family of two-dimensional materials, have exhibited fascinating properties on several occasions, primarily due to their compositional flexibility. However, compared to Carbide MXenes, Nitride MXenes are much less explored. Moreover, the structural aspects of MXenes and the tunability it may offer have not been explored until recently. In this work, we have combined these two less-explored aspects to examine the structure-property relations in the field of magnetism. We find that in the family of M$_{2}$NT$_{2}$ (M=Sc, Ti, V, Cr, Mn; T=O, F) MXenes, the stacking of transition metal planes has a substantial effect on the ground state and finite temperature magnetic properties. We also find that the electronic ground states can be tuned by changing the stacking pattern in these compounds, making the materials appropriate for applications as magnetic devices. Through a detailed analysis, we have connected the unconventional stacking pattern-driven tunability of these compounds with regard to electronic and magnetic properties to the local symmetry, inhomogeneity (or lack of it) of structural parameters, and electronic structures.","sentences":["We have performed Density Functional Theory (DFT) based calculations to investigate the effects of stacking patterns on the electronic and magnetic properties of several Nitride MXenes.","MXenes, a relatively new addition to the family of two-dimensional materials, have exhibited fascinating properties on several occasions, primarily due to their compositional flexibility.","However, compared to Carbide MXenes, Nitride MXenes are much less explored.","Moreover, the structural aspects of MXenes and the tunability it may offer have not been explored until recently.","In this work, we have combined these two less-explored aspects to examine the structure-property relations in the field of magnetism.","We find that in the family of M$_{2}$NT$_{2}$ (M=Sc, Ti, V, Cr, Mn; T=O, F) MXenes, the stacking of transition metal planes has a substantial effect on the ground state and finite temperature magnetic properties.","We also find that the electronic ground states can be tuned by changing the stacking pattern in these compounds, making the materials appropriate for applications as magnetic devices.","Through a detailed analysis, we have connected the unconventional stacking pattern-driven tunability of these compounds with regard to electronic and magnetic properties to the local symmetry, inhomogeneity (or lack of it) of structural parameters, and electronic structures."],"url":"http://arxiv.org/abs/2405.17321v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-27 16:20:50","title":"An explicit formula of the parameter dependence of de partial derivatives of the Green's functions related to arbitrary two-point boundary conditions","abstract":"In this paper we obtain an explicit formula of the parameter dependence of the partial derivatives of the Green's functions related to two-point boundary conditions. Such expression follows as an integral of both kernels times the difference of the corresponding parameters of each Green's function. As a direct consequence, we deduce a simpler proof of the monotony of the constant sign of the partial derivative of a Green's function with respect to a real parameter. As a consequence, we improve the results obtained in \\cite{C1}, where the monotone dependence was proved for the constant sign Green's function (not for any ot its partial derivatives) and under weaker assumptions on the Green's function. The arguments are valid for any other types of Ordinary Differential Equations coupled to Nonlocal Conditions. Moreover, analogous ideas could be developed for Partial and Fractional Differential Equations.","sentences":["In this paper we obtain an explicit formula of the parameter dependence of the partial derivatives of the Green's functions related to two-point boundary conditions.","Such expression follows as an integral of both kernels times the difference of the corresponding parameters of each Green's function.","As a direct consequence, we deduce a simpler proof of the monotony of the constant sign of the partial derivative of a Green's function with respect to a real parameter.","As a consequence, we improve the results obtained in \\cite{C1}, where the monotone dependence was proved for the constant sign Green's function (not for any ot its partial derivatives) and under weaker assumptions on the Green's function.","The arguments are valid for any other types of Ordinary Differential Equations coupled to Nonlocal Conditions.","Moreover, analogous ideas could be developed for Partial and Fractional Differential Equations."],"url":"http://arxiv.org/abs/2405.17320v1","category":"math.CA"}
{"created":"2024-05-27 16:16:53","title":"All-day Depth Completion","abstract":"We propose a method for depth estimation under different illumination conditions, i.e., day and night time. As photometry is uninformative in regions under low-illumination, we tackle the problem through a multi-sensor fusion approach, where we take as input an additional synchronized sparse point cloud (i.e., from a LiDAR) projected onto the image plane as a sparse depth map, along with a camera image. The crux of our method lies in the use of the abundantly available synthetic data to first approximate the 3D scene structure by learning a mapping from sparse to (coarse) dense depth maps along with their predictive uncertainty - we term this, SpaDe. In poorly illuminated regions where photometric intensities do not afford the inference of local shape, the coarse approximation of scene depth serves as a prior; the uncertainty map is then used with the image to guide refinement through an uncertainty-driven residual learning (URL) scheme. The resulting depth completion network leverages complementary strengths from both modalities - depth is sparse but insensitive to illumination and in metric scale, and image is dense but sensitive with scale ambiguity. SpaDe can be used in a plug-and-play fashion, which allows for 25% improvement when augmented onto existing methods to preprocess sparse depth. We demonstrate URL on the nuScenes dataset where we improve over all baselines by an average 11.65% in all-day scenarios, 11.23% when tested specifically for daytime, and 13.12% for nighttime scenes.","sentences":["We propose a method for depth estimation under different illumination conditions, i.e., day and night time.","As photometry is uninformative in regions under low-illumination, we tackle the problem through a multi-sensor fusion approach, where we take as input an additional synchronized sparse point cloud (i.e., from a LiDAR) projected onto the image plane as a sparse depth map, along with a camera image.","The crux of our method lies in the use of the abundantly available synthetic data to first approximate the 3D scene structure by learning a mapping from sparse to (coarse) dense depth maps along with their predictive uncertainty - we term this, SpaDe.","In poorly illuminated regions where photometric intensities do not afford the inference of local shape, the coarse approximation of scene depth serves as a prior; the uncertainty map is then used with the image to guide refinement through an uncertainty-driven residual learning (URL) scheme.","The resulting depth completion network leverages complementary strengths from both modalities - depth is sparse but insensitive to illumination and in metric scale, and image is dense but sensitive with scale ambiguity.","SpaDe can be used in a plug-and-play fashion, which allows for 25% improvement when augmented onto existing methods to preprocess sparse depth.","We demonstrate URL on the nuScenes dataset where we improve over all baselines by an average 11.65% in all-day scenarios, 11.23% when tested specifically for daytime, and 13.12% for nighttime scenes."],"url":"http://arxiv.org/abs/2405.17315v1","category":"cs.CV"}
{"created":"2024-05-27 16:00:30","title":"Enhanced Automotive Radar Collaborative Sensing By Exploiting Constructive Interference","abstract":"Automotive radar emerges as a crucial sensor for autonomous vehicle perception. As more cars are equipped radars, radar interference is an unavoidable challenge. Unlike conventional approaches such as interference mitigation and interference-avoiding technologies, this paper introduces an innovative collaborative sensing scheme with multiple automotive radars that exploits constructive interference. Through collaborative sensing, our method optimally aligns cross-path interference signals from other radars with another radar's self-echo signals, thereby significantly augmenting its target detection capabilities. This approach alleviates the need for extensive raw data sharing between collaborating radars. Instead, only an optimized weighting matrix needs to be exchanged between the radars. This approach considerably decreases the data bandwidth requirements for the wireless channel, making it a more feasible and practical solution for automotive radar collaboration. Numerical results demonstrate the effectiveness of the constructive interference approach for enhanced object detection capability.","sentences":["Automotive radar emerges as a crucial sensor for autonomous vehicle perception.","As more cars are equipped radars, radar interference is an unavoidable challenge.","Unlike conventional approaches such as interference mitigation and interference-avoiding technologies, this paper introduces an innovative collaborative sensing scheme with multiple automotive radars that exploits constructive interference.","Through collaborative sensing, our method optimally aligns cross-path interference signals from other radars with another radar's self-echo signals, thereby significantly augmenting its target detection capabilities.","This approach alleviates the need for extensive raw data sharing between collaborating radars.","Instead, only an optimized weighting matrix needs to be exchanged between the radars.","This approach considerably decreases the data bandwidth requirements for the wireless channel, making it a more feasible and practical solution for automotive radar collaboration.","Numerical results demonstrate the effectiveness of the constructive interference approach for enhanced object detection capability."],"url":"http://arxiv.org/abs/2405.17297v1","category":"eess.SP"}
{"created":"2024-05-27 15:54:47","title":"Count Data Models with Heterogeneous Peer Effects under Rational Expectations","abstract":"This paper develops a micro-founded peer effect model for count responses using a game of incomplete information. The model incorporates heterogeneity in peer effects through agents' groups based on observed characteristics. Parameter identification is established using the identification condition of linear models, which relies on the presence of friends' friends who are not direct friends in the network. I show that this condition extends to a large class of nonlinear models. The model parameters are estimated using the nested pseudo-likelihood approach, controlling for network endogeneity. I present an empirical application on students' participation in extracurricular activities. I find that females are more responsive to their peers than males, whereas male peers do not influence male students. An easy-to-use R packag--named CDatanet--is available for implementing the model.","sentences":["This paper develops a micro-founded peer effect model for count responses using a game of incomplete information.","The model incorporates heterogeneity in peer effects through agents' groups based on observed characteristics.","Parameter identification is established using the identification condition of linear models, which relies on the presence of friends' friends who are not direct friends in the network.","I show that this condition extends to a large class of nonlinear models.","The model parameters are estimated using the nested pseudo-likelihood approach, controlling for network endogeneity.","I present an empirical application on students' participation in extracurricular activities.","I find that females are more responsive to their peers than males, whereas male peers do not influence male students.","An easy-to-use R packag--named CDatanet--is available for implementing the model."],"url":"http://arxiv.org/abs/2405.17290v1","category":"econ.EM"}
{"created":"2024-05-27 15:49:22","title":"Asymptotic behavior of solutions for a critical heat equation with nonlocal reaction","abstract":"In this paper, we consider the following nonlocal parabolic equation   \\begin{equation*}   u_{t}-\\Delta u=\\left( \\int_{\\Omega}\\frac{|u(y,t)|^{2^{\\ast}_{\\mu}}}{|x-y|^{\\mu}}dy\\right) |u|^{2^{\\ast}_{\\mu}-2}u,\\ \\text{in}\\ \\Omega\\times(0,\\infty),   \\end{equation*}   where $\\Omega$ is a bounded domain in $\\mathbb{R}^{N}$, $0<\\mu<N$ and $2^{\\ast}_{\\mu}=(2N-\\mu)/(N-2)$ denotes the critical exponent in the sense of the Hardy-Littlewood-Sobolev inequality. We first introduce the stable and unstable sets for the equation and prove that the problem has a potential well structure. Next, we investigate the global asymptotic behavior of the solutions.   In particular, we study the behavior of the global solutions that intersect neither with the stable set nor the unstable set.   Finally, we prove that global solutions have $L^{\\infty}$-uniform bound under some natural conditions.","sentences":["In this paper, we consider the following nonlocal parabolic equation   \\begin{equation*}   u_{t}-\\Delta u=\\left( \\int_{\\Omega}\\frac{|u(y,t)|^{2^{\\ast}_{\\mu}}}{|x-y|^{\\mu}}dy\\right) |u|^{2^{\\ast}_{\\mu}-2}u,\\ \\text{in}\\ \\Omega\\times(0,\\infty),   \\end{equation*}   where $\\Omega$ is a bounded domain in $\\mathbb{R}^{N}$, $0<\\mu<N$ and $2^{\\ast}_{\\mu}=(2N-\\mu)/(N-2)$ denotes the critical exponent in the sense of the Hardy-Littlewood-Sobolev inequality.","We first introduce the stable and unstable sets for the equation and prove that the problem has a potential well structure.","Next, we investigate the global asymptotic behavior of the solutions.   ","In particular, we study the behavior of the global solutions that intersect neither with the stable set nor the unstable set.   ","Finally, we prove that global solutions have $L^{\\infty}$-uniform bound under some natural conditions."],"url":"http://arxiv.org/abs/2405.17285v1","category":"math.AP"}
{"created":"2024-05-27 15:19:59","title":"Does Diffusion Beat GAN in Image Super Resolution?","abstract":"There is a prevalent opinion in the recent literature that Diffusion-based models outperform GAN-based counterparts on the Image Super Resolution (ISR) problem. However, in most studies, Diffusion-based ISR models were trained longer and utilized larger networks than the GAN baselines. This raises the question of whether the superiority of Diffusion models is due to the Diffusion paradigm being better suited for the ISR task or if it is a consequence of the increased scale and computational resources used in contemporary studies. In our work, we compare Diffusion-based and GAN-based Super Resolution under controlled settings, where both approaches are matched in terms of architecture, model and dataset size, and computational budget. We show that a GAN-based model can achieve results comparable to a Diffusion-based model. Additionally, we explore the impact of design choices such as text conditioning and augmentation on the performance of ISR models, showcasing their effect on several downstream tasks. We will release the inference code and weights of our scaled GAN.","sentences":["There is a prevalent opinion in the recent literature that Diffusion-based models outperform GAN-based counterparts on the Image Super Resolution (ISR) problem.","However, in most studies, Diffusion-based ISR models were trained longer and utilized larger networks than the GAN baselines.","This raises the question of whether the superiority of Diffusion models is due to the Diffusion paradigm being better suited for the ISR task or if it is a consequence of the increased scale and computational resources used in contemporary studies.","In our work, we compare Diffusion-based and GAN-based Super Resolution under controlled settings, where both approaches are matched in terms of architecture, model and dataset size, and computational budget.","We show that a GAN-based model can achieve results comparable to a Diffusion-based model.","Additionally, we explore the impact of design choices such as text conditioning and augmentation on the performance of ISR models, showcasing their effect on several downstream tasks.","We will release the inference code and weights of our scaled GAN."],"url":"http://arxiv.org/abs/2405.17261v1","category":"eess.IV"}
{"created":"2024-05-27 15:10:11","title":"Estimating treatment-effect heterogeneity across sites in multi-site randomized experiments with imperfect compliance","abstract":"We consider multi-site randomized controlled trials with a large number of small sites and imperfect compliance, conducted in non-random convenience samples in each site. We show that an Empirical-Bayes (EB) estimator can be used to estimate a lower bound of the variance of intention-to-treat (ITT) effects across sites. We also propose bounds for the coefficient from a regression of site-level ITTs on sites' control-group outcome. Turning to local average treatment effects (LATEs), the EB estimator cannot be used to estimate their variance, because site-level LATE estimators are biased. Instead, we propose two testable assumptions under which the LATEs' variance can be written as a function of sites' ITT and first-stage (FS) effects, thus allowing us to use an EB estimator leveraging only unbiased ITT and FS estimators. We revisit Behaghel et al. (2014), who study the effect of counselling programs on job seekers job-finding rate, in more than 200 job placement agencies in France. We find considerable ITT heterogeneity, and even more LATE heterogeneity: our lower bounds on ITTs' (resp. LATEs') standard deviation are more than three (resp. four) times larger than the average ITT (resp. LATE) across sites. Sites with a lower job-finding rate in the control group have larger ITT effects.","sentences":["We consider multi-site randomized controlled trials with a large number of small sites and imperfect compliance, conducted in non-random convenience samples in each site.","We show that an Empirical-Bayes (EB) estimator can be used to estimate a lower bound of the variance of intention-to-treat (ITT) effects across sites.","We also propose bounds for the coefficient from a regression of site-level ITTs on sites' control-group outcome.","Turning to local average treatment effects (LATEs), the EB estimator cannot be used to estimate their variance, because site-level LATE estimators are biased.","Instead, we propose two testable assumptions under which the LATEs' variance can be written as a function of sites' ITT and first-stage (FS) effects, thus allowing us to use an EB estimator leveraging only unbiased ITT and FS estimators.","We revisit Behaghel et al.","(2014), who study the effect of counselling programs on job seekers job-finding rate, in more than 200 job placement agencies in France.","We find considerable ITT heterogeneity, and even more LATE heterogeneity: our lower bounds on ITTs' (resp.","LATEs') standard deviation are more than three (resp.","four) times larger than the average ITT (resp.","LATE) across sites.","Sites with a lower job-finding rate in the control group have larger ITT effects."],"url":"http://arxiv.org/abs/2405.17254v1","category":"econ.EM"}
{"created":"2024-05-27 15:01:07","title":"Do Molecular Geometries Change Under Vibrational Strong Coupling?","abstract":"As pioneering experiments have shown, strong vibrational coupling between molecular vibrations and light modes in an optical cavity can significantly alter molecular properties and even affect chemical reactivity. However, the current theoretical description is limited and far from complete. To explore the origin of this exciting observation, we investigate how the molecular structure changes under strong light-matter coupling using an ab-initio method based on the cavity Born-Oppenheimer Hartree-Fock ansatz. By optimizing H$_2$O and H$_2$O$_2$ resonantly coupled to cavity modes, we study the importance of reorientation and geometric relaxation. In addition, we show that the inclusion of one or two cavity modes can change the observed results. On the basis of our findings, we derive a simple concept to estimate the effect of the cavity interaction on the molecular geometry using the molecular polarizability and the dipole moments.","sentences":["As pioneering experiments have shown, strong vibrational coupling between molecular vibrations and light modes in an optical cavity can significantly alter molecular properties and even affect chemical reactivity.","However, the current theoretical description is limited and far from complete.","To explore the origin of this exciting observation, we investigate how the molecular structure changes under strong light-matter coupling using an ab-initio method based on the cavity Born-Oppenheimer Hartree-Fock ansatz.","By optimizing H$_2$O and H$_2$O$_2$ resonantly coupled to cavity modes, we study the importance of reorientation and geometric relaxation.","In addition, we show that the inclusion of one or two cavity modes can change the observed results.","On the basis of our findings, we derive a simple concept to estimate the effect of the cavity interaction on the molecular geometry using the molecular polarizability and the dipole moments."],"url":"http://arxiv.org/abs/2405.17246v1","category":"physics.chem-ph"}
{"created":"2024-05-27 14:57:58","title":"NeurTV: Total Variation on the Neural Domain","abstract":"Recently, we have witnessed the success of total variation (TV) for many imaging applications. However, traditional TV is defined on the original pixel domain, which limits its potential. In this work, we suggest a new TV regularization defined on the neural domain. Concretely, the discrete data is continuously and implicitly represented by a deep neural network (DNN), and we use the derivatives of DNN outputs w.r.t. input coordinates to capture local correlations of data. As compared with classical TV on the original domain, the proposed TV on the neural domain (termed NeurTV) enjoys two advantages. First, NeurTV is not limited to meshgrid but is suitable for both meshgrid and non-meshgrid data. Second, NeurTV can more exactly capture local correlations across data for any direction and any order of derivatives attributed to the implicit and continuous nature of neural domain. We theoretically reinterpret NeurTV under the variational approximation framework, which allows us to build the connection between classical TV and NeurTV and inspires us to develop variants (e.g., NeurTV with arbitrary resolution and space-variant NeurTV). Extensive numerical experiments with meshgrid data (e.g., color and hyperspectral images) and non-meshgrid data (e.g., point clouds and spatial transcriptomics) showcase the effectiveness of the proposed methods.","sentences":["Recently, we have witnessed the success of total variation (TV) for many imaging applications.","However, traditional TV is defined on the original pixel domain, which limits its potential.","In this work, we suggest a new TV regularization defined on the neural domain.","Concretely, the discrete data is continuously and implicitly represented by a deep neural network (DNN), and we use the derivatives of DNN outputs w.r.t.","input coordinates to capture local correlations of data.","As compared with classical TV on the original domain, the proposed TV on the neural domain (termed NeurTV) enjoys two advantages.","First, NeurTV is not limited to meshgrid but is suitable for both meshgrid and non-meshgrid data.","Second, NeurTV can more exactly capture local correlations across data for any direction and any order of derivatives attributed to the implicit and continuous nature of neural domain.","We theoretically reinterpret NeurTV under the variational approximation framework, which allows us to build the connection between classical TV and NeurTV and inspires us to develop variants (e.g., NeurTV with arbitrary resolution and space-variant NeurTV).","Extensive numerical experiments with meshgrid data (e.g., color and hyperspectral images) and non-meshgrid data (e.g., point clouds and spatial transcriptomics) showcase the effectiveness of the proposed methods."],"url":"http://arxiv.org/abs/2405.17241v1","category":"cs.CV"}
{"created":"2024-05-27 14:42:04","title":"Quantifying the Reliance of Black-Box Decision-Makers on Variables of Interest","abstract":"This paper introduces a framework for measuring how much black-box decision-makers rely on variables of interest. The framework adapts a permutation-based measure of variable importance from the explainable machine learning literature. With an emphasis on applicability, I present some of the framework's theoretical and computational properties, explain how reliance computations have policy implications, and work through an illustrative example. In the empirical application to interruptions by Supreme Court Justices during oral argument, I find that the effect of gender is more muted compared to the existing literature's estimate; I then use this paper's framework to compare Justices' reliance on gender and alignment to their reliance on experience, which are incomparable using regression coefficients.","sentences":["This paper introduces a framework for measuring how much black-box decision-makers rely on variables of interest.","The framework adapts a permutation-based measure of variable importance from the explainable machine learning literature.","With an emphasis on applicability, I present some of the framework's theoretical and computational properties, explain how reliance computations have policy implications, and work through an illustrative example.","In the empirical application to interruptions by Supreme Court Justices during oral argument, I find that the effect of gender is more muted compared to the existing literature's estimate; I then use this paper's framework to compare Justices' reliance on gender and alignment to their reliance on experience, which are incomparable using regression coefficients."],"url":"http://arxiv.org/abs/2405.17225v1","category":"econ.EM"}
{"created":"2024-05-27 14:35:17","title":"Collage is the New Writing: Exploring the Fragmentation of Text and User Interfaces in AI Tools","abstract":"This essay proposes and explores the concept of Collage for the design of AI writing tools, transferred from avant-garde literature with four facets: 1) fragmenting text in writing interfaces, 2) juxtaposing voices (content vs command), 3) integrating material from multiple sources (e.g. text suggestions), and 4) shifting from manual writing to editorial and compositional decision-making, such as selecting and arranging snippets. The essay then employs Collage as an analytical lens to analyse the user interface design of recent AI writing tools, and as a constructive lens to inspire new design directions. Finally, a critical perspective relates the concerns that writers historically expressed through literary collage to AI writing tools. In a broad view, this essay explores how literary concepts can help advance design theory around AI writing tools. It encourages creators of future writing tools to engage not only with new technological possibilities, but also with past writing innovations.","sentences":["This essay proposes and explores the concept of Collage for the design of AI writing tools, transferred from avant-garde literature with four facets: 1) fragmenting text in writing interfaces, 2) juxtaposing voices (content vs command), 3) integrating material from multiple sources (e.g. text suggestions), and 4) shifting from manual writing to editorial and compositional decision-making, such as selecting and arranging snippets.","The essay then employs Collage as an analytical lens to analyse the user interface design of recent AI writing tools, and as a constructive lens to inspire new design directions.","Finally, a critical perspective relates the concerns that writers historically expressed through literary collage to AI writing tools.","In a broad view, this essay explores how literary concepts can help advance design theory around AI writing tools.","It encourages creators of future writing tools to engage not only with new technological possibilities, but also with past writing innovations."],"url":"http://arxiv.org/abs/2405.17217v1","category":"cs.HC"}
{"created":"2024-05-27 14:35:09","title":"$L^p$ Hodge theory for bounded subanalytic manifolds","abstract":"Given a bounded subanalytic submanifold of $\\mathbb{R}^n$, possibly admitting singularities within its closure, we study the cohomology of $L^p$ differential forms having an $L^p$ exterior differential (in the sense of currents) and satisfying Dirichlet or Neumann condition. We show an $L^p$ Hodge decomposition theorem, an $L^p$ de Rham theorem, as well as a Lefschetz duality theorem between $L^p$ and $L^{p'}$ forms (with $\\frac{1}{p}+\\frac{1}{p'}=1$) in the case where $p$ is large or close to $1$. This is achieved by proving that de Rham's pairing between complementary $L^p$ differential forms induces a pairing between cohomology classes which is nondegenerate (for such $p$). The main difficulty to carry it out is to show the density (in Sobolev spaces of differential forms) of forms that vanish near some singularities and are smooth up to the closure of the underlying manifold in the case $p$ large (Theorem 4.1). This result, which is of independent interest, also makes it possible to give a trace theorem that leads us to some explicit characterizations of Dirichlet and Neumann conditions in terms of traces and residues.","sentences":["Given a bounded subanalytic submanifold of $\\mathbb{R}^n$, possibly admitting singularities within its closure, we study the cohomology of $L^p$ differential forms having an $L^p$ exterior differential (in the sense of currents) and satisfying Dirichlet or Neumann condition.","We show an $L^p$ Hodge decomposition theorem, an $L^p$ de Rham theorem, as well as a Lefschetz duality theorem between $L^p$ and $L^{p'}$ forms (with $\\frac{1}{p}+\\frac{1}{p'}=1$) in the case where $p$ is large or close to $1$.","This is achieved by proving that de Rham's pairing between complementary $L^p$ differential forms induces a pairing between cohomology classes which is nondegenerate (for such $p$).","The main difficulty to carry it out is to show the density (in Sobolev spaces of differential forms) of forms that vanish near some singularities and are smooth up to the closure of the underlying manifold in the case $p$ large (Theorem 4.1).","This result, which is of independent interest, also makes it possible to give a trace theorem that leads us to some explicit characterizations of Dirichlet and Neumann conditions in terms of traces and residues."],"url":"http://arxiv.org/abs/2405.17215v1","category":"math.AG"}
{"created":"2024-05-27 12:59:46","title":"SDL-MVS: View Space and Depth Deformable Learning Paradigm for Multi-View Stereo Reconstruction in Remote Sensing","abstract":"Research on multi-view stereo based on remote sensing images has promoted the development of large-scale urban 3D reconstruction. However, remote sensing multi-view image data suffers from the problems of occlusion and uneven brightness between views during acquisition, which leads to the problem of blurred details in depth estimation. To solve the above problem, we re-examine the deformable learning method in the Multi-View Stereo task and propose a novel paradigm based on view Space and Depth deformable Learning (SDL-MVS), aiming to learn deformable interactions of features in different view spaces and deformably model the depth ranges and intervals to enable high accurate depth estimation. Specifically, to solve the problem of view noise caused by occlusion and uneven brightness, we propose a Progressive Space deformable Sampling (PSS) mechanism, which performs deformable learning of sampling points in the 3D frustum space and the 2D image space in a progressive manner to embed source features to the reference feature adaptively. To further optimize the depth, we introduce Depth Hypothesis deformable Discretization (DHD), which achieves precise positioning of the depth prior by adaptively adjusting the depth range hypothesis and performing deformable discretization of the depth interval hypothesis. Finally, our SDL-MVS achieves explicit modeling of occlusion and uneven brightness faced in multi-view stereo through the deformable learning paradigm of view space and depth, achieving accurate multi-view depth estimation. Extensive experiments on LuoJia-MVS and WHU datasets show that our SDL-MVS reaches state-of-the-art performance. It is worth noting that our SDL-MVS achieves an MAE error of 0.086, an accuracy of 98.9% for <0.6m, and 98.9% for <3-interval on the LuoJia-MVS dataset under the premise of three views as input.","sentences":["Research on multi-view stereo based on remote sensing images has promoted the development of large-scale urban 3D reconstruction.","However, remote sensing multi-view image data suffers from the problems of occlusion and uneven brightness between views during acquisition, which leads to the problem of blurred details in depth estimation.","To solve the above problem, we re-examine the deformable learning method in the Multi-View Stereo task and propose a novel paradigm based on view Space and Depth deformable Learning (SDL-MVS), aiming to learn deformable interactions of features in different view spaces and deformably model the depth ranges and intervals to enable high accurate depth estimation.","Specifically, to solve the problem of view noise caused by occlusion and uneven brightness, we propose a Progressive Space deformable Sampling (PSS) mechanism, which performs deformable learning of sampling points in the 3D frustum space and the 2D image space in a progressive manner to embed source features to the reference feature adaptively.","To further optimize the depth, we introduce Depth Hypothesis deformable Discretization (DHD), which achieves precise positioning of the depth prior by adaptively adjusting the depth range hypothesis and performing deformable discretization of the depth interval hypothesis.","Finally, our SDL-MVS achieves explicit modeling of occlusion and uneven brightness faced in multi-view stereo through the deformable learning paradigm of view space and depth, achieving accurate multi-view depth estimation.","Extensive experiments on LuoJia-MVS and WHU datasets show that our SDL-MVS reaches state-of-the-art performance.","It is worth noting that our SDL-MVS achieves an MAE error of 0.086, an accuracy of 98.9% for <0.6m, and 98.9% for <3-interval on the LuoJia-MVS dataset under the premise of three views as input."],"url":"http://arxiv.org/abs/2405.17140v1","category":"cs.CV"}
{"created":"2024-05-27 12:49:07","title":"Your decision path does matter in pre-training industrial recommenders with multi-source behaviors","abstract":"Online service platforms offering a wide range of services through miniapps have become crucial for users who visit these platforms with clear intentions to find services they are interested in. Aiming at effective content delivery, cross-domain recommendation are introduced to learn high-quality representations by transferring behaviors from data-rich scenarios. However, these methods overlook the impact of the decision path that users take when conduct behaviors, that is, users ultimately exhibit different behaviors based on various intents. To this end, we propose HIER, a novel Hierarchical decIsion path Enhanced Representation learning for cross-domain recommendation. With the help of graph neural networks for high-order topological information of the knowledge graph between multi-source behaviors, we further adaptively learn decision paths through well-designed exemplar-level and information bottleneck based contrastive learning. Extensive experiments in online and offline environments show the superiority of HIER.","sentences":["Online service platforms offering a wide range of services through miniapps have become crucial for users who visit these platforms with clear intentions to find services they are interested in.","Aiming at effective content delivery, cross-domain recommendation are introduced to learn high-quality representations by transferring behaviors from data-rich scenarios.","However, these methods overlook the impact of the decision path that users take when conduct behaviors, that is, users ultimately exhibit different behaviors based on various intents.","To this end, we propose HIER, a novel Hierarchical decIsion path Enhanced Representation learning for cross-domain recommendation.","With the help of graph neural networks for high-order topological information of the knowledge graph between multi-source behaviors, we further adaptively learn decision paths through well-designed exemplar-level and information bottleneck based contrastive learning.","Extensive experiments in online and offline environments show the superiority of HIER."],"url":"http://arxiv.org/abs/2405.17132v1","category":"cs.LG"}
{"created":"2024-05-27 10:39:49","title":"Iterates of composition operators on global spaces of ultradifferentiable functions","abstract":"We analyze the behavior of the iterates of composition operators defined by polynomials acting on global classes of ultradifferentiable functions of Beurling type and being invariant under Fourier transform. We characterize the polynomials $\\psi$ for which the sequence of iterates is equicontinuous between two different Gelfand-Shilov spaces. For the particular case in which the weight $\\omega$ is equivalent to a power of the logarithm, the result obtained characterizes the polynomials $\\psi$ for which the composition operator $C_\\psi$ is power bounded in ${\\mathcal S}_\\omega({\\mathbb R}).$ Unlike the composition operators in Schwartz class, the Waelbroek spectrum of an operator $C_\\psi$, being $\\psi$ a polynomial of degree greater than one lacking fixed points is never compact. We focus on the problem of convergence of Neumann series. We deduce the continuity of the resolvent operator between two different Gelfand-Shilov classes for polynomials $\\psi$ lacking fixed points. Concerning polynomials of second degree the most interesting case is the one in which the polynomial only has one fixed point: we provide some restrictions on the indices $d, d'$ that are necessary for the resolvent operator to be continuous between the Gelfand-Shilov classes $\\Sigma_d$ and $\\Sigma_{d'}.$","sentences":["We analyze the behavior of the iterates of composition operators defined by polynomials acting on global classes of ultradifferentiable functions of Beurling type and being invariant under Fourier transform.","We characterize the polynomials $\\psi$ for which the sequence of iterates is equicontinuous between two different Gelfand-Shilov spaces.","For the particular case in which the weight $\\omega$ is equivalent to a power of the logarithm, the result obtained characterizes the polynomials $\\psi$ for which the composition operator $C_\\psi$ is power bounded in ${\\mathcal S}_\\omega({\\mathbb R}).$ Unlike the composition operators in Schwartz class, the Waelbroek spectrum of an operator $C_\\psi$, being $\\psi$ a polynomial of degree greater than one lacking fixed points is never compact.","We focus on the problem of convergence of Neumann series.","We deduce the continuity of the resolvent operator between two different Gelfand-Shilov classes for polynomials $\\psi$ lacking fixed points.","Concerning polynomials of second degree the most interesting case is the one in which the polynomial only has one fixed point: we provide some restrictions on the indices $d, d'$ that are necessary for the resolvent operator to be continuous between the Gelfand-Shilov classes $\\Sigma_d$ and $\\Sigma_{d'}.$"],"url":"http://arxiv.org/abs/2405.17033v1","category":"math.FA"}
{"created":"2024-05-27 10:33:53","title":"Any-step Dynamics Model Improves Future Predictions for Online and Offline Reinforcement Learning","abstract":"Model-based methods in reinforcement learning offer a promising approach to enhance data efficiency by facilitating policy exploration within a dynamics model. However, accurately predicting sequential steps in the dynamics model remains a challenge due to the bootstrapping prediction, which attributes the next state to the prediction of the current state. This leads to accumulated errors during model roll-out. In this paper, we propose the Any-step Dynamics Model (ADM) to mitigate the compounding error by reducing bootstrapping prediction to direct prediction. ADM allows for the use of variable-length plans as inputs for predicting future states without frequent bootstrapping. We design two algorithms, ADMPO-ON and ADMPO-OFF, which apply ADM in online and offline model-based frameworks, respectively. In the online setting, ADMPO-ON demonstrates improved sample efficiency compared to previous state-of-the-art methods. In the offline setting, ADMPO-OFF not only demonstrates superior performance compared to recent state-of-the-art offline approaches but also offers better quantification of model uncertainty using only a single ADM.","sentences":["Model-based methods in reinforcement learning offer a promising approach to enhance data efficiency by facilitating policy exploration within a dynamics model.","However, accurately predicting sequential steps in the dynamics model remains a challenge due to the bootstrapping prediction, which attributes the next state to the prediction of the current state.","This leads to accumulated errors during model roll-out.","In this paper, we propose the Any-step Dynamics Model (ADM) to mitigate the compounding error by reducing bootstrapping prediction to direct prediction.","ADM allows for the use of variable-length plans as inputs for predicting future states without frequent bootstrapping.","We design two algorithms, ADMPO-ON and ADMPO-OFF, which apply ADM in online and offline model-based frameworks, respectively.","In the online setting, ADMPO-ON demonstrates improved sample efficiency compared to previous state-of-the-art methods.","In the offline setting, ADMPO-OFF not only demonstrates superior performance compared to recent state-of-the-art offline approaches but also offers better quantification of model uncertainty using only a single ADM."],"url":"http://arxiv.org/abs/2405.17031v1","category":"cs.LG"}
{"created":"2024-05-27 10:01:35","title":"A Deep-NN Beamforming Approach for Dual Function Radar-Communication THz UAV","abstract":"In this paper, we consider a scenario with one UAV equipped with a ULA, which sends combined information and sensing signals to communicate with multiple GBS and, at the same time, senses potential targets placed within an interested area on the ground. We aim to jointly design the transmit beamforming with the GBS association to optimize communication performance while ensuring high sensing accuracy. We propose a predictive beamforming framework based on a dual DNN solution to solve the formulated nonconvex optimization problem. A first DNN is trained to produce the required beamforming matrix for any point of the UAV flying area in a reduced time compared to state-of-the-art beamforming optimizers. A second DNN is trained to learn the optimal mapping from the input features, power, and EIRP constraints to the GBS association decision. Finally, we provide an extensive simulation analysis to corroborate the proposed approach and show the benefits of EIRP, SINR performance and computational speed.","sentences":["In this paper, we consider a scenario with one UAV equipped with a ULA, which sends combined information and sensing signals to communicate with multiple GBS and, at the same time, senses potential targets placed within an interested area on the ground.","We aim to jointly design the transmit beamforming with the GBS association to optimize communication performance while ensuring high sensing accuracy.","We propose a predictive beamforming framework based on a dual DNN solution to solve the formulated nonconvex optimization problem.","A first DNN is trained to produce the required beamforming matrix for any point of the UAV flying area in a reduced time compared to state-of-the-art beamforming optimizers.","A second DNN is trained to learn the optimal mapping from the input features, power, and EIRP constraints to the GBS association decision.","Finally, we provide an extensive simulation analysis to corroborate the proposed approach and show the benefits of EIRP, SINR performance and computational speed."],"url":"http://arxiv.org/abs/2405.17015v1","category":"eess.SP"}
{"created":"2024-05-27 09:59:35","title":"On the Obstacle Problem in Fractional Generalised Orlicz Spaces","abstract":"We consider the one and the two obstacles problems for the nonlocal nonlinear anisotropic $g$-Laplacian $\\mathcal{L}_g^s$, with $0<s<1$. We prove the strict T-monotonicity of $\\mathcal{L}_g^s$ and we obtain the Lewy-Stampacchia inequalities. We consider the approximation of the solutions through semilinear problems, for which we prove a global $L^\\infty$-estimate, and we extend the local H\\\"older regularity to the solutions of the obstacle problems in the case of the fractional $p(x,y)$-Laplacian operator. We make further remarks on a few elementary properties of related capacities in the fractional generalised Orlicz framework, with a special reference to the Hilbertian nonlinear case in fractional Sobolev spaces.","sentences":["We consider the one and the two obstacles problems for the nonlocal nonlinear anisotropic $g$-Laplacian $\\mathcal{L}_g^s$, with $0<s<1$. We prove the strict T-monotonicity of $\\mathcal{L}_g^s$ and we obtain the Lewy-Stampacchia inequalities.","We consider the approximation of the solutions through semilinear problems, for which we prove a global $L^\\infty$-estimate, and we extend the local H\\\"older regularity to the solutions of the obstacle problems in the case of the fractional $p(x,y)$-Laplacian operator.","We make further remarks on a few elementary properties of related capacities in the fractional generalised Orlicz framework, with a special reference to the Hilbertian nonlinear case in fractional Sobolev spaces."],"url":"http://arxiv.org/abs/2405.17014v1","category":"math.AP"}
{"created":"2024-05-27 09:41:25","title":"Searching for Long Lived Particles at LHC","abstract":"Search for BSM phenomena is one of the fundamental goals of the LHC experiments. Many BSM models foresee long-lived particles which decay far from the production vertex and a big effort has been done by the ATLAS and CMS collaborations to detect these long-lived particles. A widely studied example for such searches is made through the measurement of displaced vertices produced by a narrow resonance decaying to a muon pair. This paper will first analyze what has been done so far and will try to evaluate if there are improvements which may allow enhancing the discovery potential at LHC for such particles. The outcome of this analysis is that while long-lived particles decaying very far from the primary vertex have been carefully studied by ATLAS and CMS, the region within a few millimeters from the primary vertex may have been partially neglected. A new approach for searching such resonances, fully based on data, will then be proposed which allows minimizing systematic uncertainties while keeping at a maximum the discovery potentiality for long-lived neutral resonances which decay a few millimeters from the primary vertex.","sentences":["Search for BSM phenomena is one of the fundamental goals of the LHC experiments.","Many BSM models foresee long-lived particles which decay far from the production vertex and a big effort has been done by the ATLAS and CMS collaborations to detect these long-lived particles.","A widely studied example for such searches is made through the measurement of displaced vertices produced by a narrow resonance decaying to a muon pair.","This paper will first analyze what has been done so far and will try to evaluate if there are improvements which may allow enhancing the discovery potential at LHC for such particles.","The outcome of this analysis is that while long-lived particles decaying very far from the primary vertex have been carefully studied by ATLAS and CMS, the region within a few millimeters from the primary vertex may have been partially neglected.","A new approach for searching such resonances, fully based on data, will then be proposed which allows minimizing systematic uncertainties while keeping at a maximum the discovery potentiality for long-lived neutral resonances which decay a few millimeters from the primary vertex."],"url":"http://arxiv.org/abs/2405.16993v1","category":"hep-ex"}
{"created":"2024-05-27 09:35:22","title":"Uncertainty Learning for High-dimensional Mean-variance Portfolio","abstract":"Accounting for uncertainty in Data quality is important for accurate statistical inference. We aim to an optimal conservative allocation for a large universe of assets in mean-variance portfolio (MVP), which is the worst choice within uncertainty in data distribution. Unlike the low dimensional MVP studied in Blanchet et al. (2022, Management Science), the large number of assets raises a challenging problem in quantifying the uncertainty, due to the big deviation of the sample covariance matrix from the population version. To overcome this difficulty, we propose a data-adaptive method to quantify the uncertainty with the help of a factor structure. Monte-Carlo Simulation is conducted to show the superiority of our method in high-dimensional cases, that, avoiding the over-conservative results in Blanchet et al. (2022), our allocation is closer to the oracle version in terms of risk minimization and expected portfolio return controlling.","sentences":["Accounting for uncertainty in Data quality is important for accurate statistical inference.","We aim to an optimal conservative allocation for a large universe of assets in mean-variance portfolio (MVP), which is the worst choice within uncertainty in data distribution.","Unlike the low dimensional MVP studied in Blanchet et al. (2022, Management Science), the large number of assets raises a challenging problem in quantifying the uncertainty, due to the big deviation of the sample covariance matrix from the population version.","To overcome this difficulty, we propose a data-adaptive method to quantify the uncertainty with the help of a factor structure.","Monte-Carlo Simulation is conducted to show the superiority of our method in high-dimensional cases, that, avoiding the over-conservative results in Blanchet et al. (2022), our allocation is closer to the oracle version in terms of risk minimization and expected portfolio return controlling."],"url":"http://arxiv.org/abs/2405.16989v1","category":"stat.ME"}
{"created":"2024-05-27 09:21:40","title":"OSLO: One-Shot Label-Only Membership Inference Attacks","abstract":"We introduce One-Shot Label-Only (OSLO) membership inference attacks (MIAs), which accurately infer a given sample's membership in a target model's training set with high precision using just \\emph{a single query}, where the target model only returns the predicted hard label. This is in contrast to state-of-the-art label-only attacks which require $\\sim6000$ queries, yet get attack precisions lower than OSLO's. OSLO leverages transfer-based black-box adversarial attacks. The core idea is that a member sample exhibits more resistance to adversarial perturbations than a non-member. We compare OSLO against state-of-the-art label-only attacks and demonstrate that, despite requiring only one query, our method significantly outperforms previous attacks in terms of precision and true positive rate (TPR) under the same false positive rates (FPR). For example, compared to previous label-only MIAs, OSLO achieves a TPR that is 7$\\times$ to 28$\\times$ stronger under a 0.1\\% FPR on CIFAR10 for a ResNet model. We evaluated multiple defense mechanisms against OSLO.","sentences":["We introduce One-Shot Label-Only (OSLO) membership inference attacks (MIAs), which accurately infer a given sample's membership in a target model's training set with high precision using just \\emph{a single query}, where the target model only returns the predicted hard label.","This is in contrast to state-of-the-art label-only attacks which require $\\sim6000$ queries, yet get attack precisions lower than OSLO's.","OSLO leverages transfer-based black-box adversarial attacks.","The core idea is that a member sample exhibits more resistance to adversarial perturbations than a non-member.","We compare OSLO against state-of-the-art label-only attacks and demonstrate that, despite requiring only one query, our method significantly outperforms previous attacks in terms of precision and true positive rate (TPR) under the same false positive rates (FPR).","For example, compared to previous label-only MIAs, OSLO achieves a TPR that is 7$\\times$ to 28$\\times$ stronger under a 0.1\\% FPR on CIFAR10 for a ResNet model.","We evaluated multiple defense mechanisms against OSLO."],"url":"http://arxiv.org/abs/2405.16978v1","category":"cs.LG"}
{"created":"2024-05-27 09:08:16","title":"Chip-Scale Point-Source Sagnac Interferometer by Phase-Space Squeezing","abstract":"Matter-wave interferometry is essential to both science and technology. Phase-space squeezing has been shown to be an advantageous source of atoms, whereby the spread in momentum is decreased. Here, we show that the opposite squeezing may be just as advantageous. As a case in point, we analyze the effect of such a source on point source atom interferometry (PSI), which enables rotation sensing. We describe how a squeezed PSI (SPSI) increases the sensitivity and dynamic range while facilitating short cycle times and high repetition rates. We present regions in parameter space for which the figures of merit are improved by orders of magnitude and show that under some definition of compactness, the SPSI is superior by more than four orders of magnitude. The SPSI thus enables either enhancing the performance for standard size devices or maintaining the performance while miniaturizing to a chip-scale device, opening the door to real-life applications.","sentences":["Matter-wave interferometry is essential to both science and technology.","Phase-space squeezing has been shown to be an advantageous source of atoms, whereby the spread in momentum is decreased.","Here, we show that the opposite squeezing may be just as advantageous.","As a case in point, we analyze the effect of such a source on point source atom interferometry (PSI), which enables rotation sensing.","We describe how a squeezed PSI (SPSI) increases the sensitivity and dynamic range while facilitating short cycle times and high repetition rates.","We present regions in parameter space for which the figures of merit are improved by orders of magnitude and show that under some definition of compactness, the SPSI is superior by more than four orders of magnitude.","The SPSI thus enables either enhancing the performance for standard size devices or maintaining the performance while miniaturizing to a chip-scale device, opening the door to real-life applications."],"url":"http://arxiv.org/abs/2405.16972v1","category":"physics.atom-ph"}
{"created":"2024-05-27 09:06:26","title":"Memory-assisted measurement-device-independent quantum secret sharing","abstract":"Measurement-device-independent quantum secret sharing (MDI-QSS) can eliminate all the security loopholes associated with imperfect measurement devices and greatly enhance QS's security under practical experimental condition. MDI-QSS requires each communication user to send single photon to the measurement party for the coincident measurement. However, the unsynchronization of the transmitted photons greatly limits MDI-QSS's practical performance.In the paper, we propose a high-efficient quantum memory (QM)-assisted MDI-QSS protocol, which employs the QM-assisted synchronization of three heralded single-photon sources to efficiently generate three simultaneous single-photon states. The QM constructed with all-optical, polarization-insensitive storage loop has superior performance in terms of bandwidth, storage efficiency, and noise resistance, and is feasible under current experiment conditions. Combining with the decoy-state method, we perform the numerical simulation of the secure key rate in the symmetric model without considering the finite-size effect. The simulation results show that our QM-assisted MDI-QSS protocol exhibit largely improved secure key rate and maximal photon transmission distance compared with all existing MDI-QSS protocols without QM. Our protocol provides a promising way for implementing the high-efficient long-distance MDI-QSS in the near future.","sentences":["Measurement-device-independent quantum secret sharing (MDI-QSS) can eliminate all the security loopholes associated with imperfect measurement devices and greatly enhance QS's security under practical experimental condition.","MDI-QSS requires each communication user to send single photon to the measurement party for the coincident measurement.","However, the unsynchronization of the transmitted photons greatly limits MDI-QSS's practical performance.","In the paper, we propose a high-efficient quantum memory (QM)-assisted MDI-QSS protocol, which employs the QM-assisted synchronization of three heralded single-photon sources to efficiently generate three simultaneous single-photon states.","The QM constructed with all-optical, polarization-insensitive storage loop has superior performance in terms of bandwidth, storage efficiency, and noise resistance, and is feasible under current experiment conditions.","Combining with the decoy-state method, we perform the numerical simulation of the secure key rate in the symmetric model without considering the finite-size effect.","The simulation results show that our QM-assisted MDI-QSS protocol exhibit largely improved secure key rate and maximal photon transmission distance compared with all existing MDI-QSS protocols without QM.","Our protocol provides a promising way for implementing the high-efficient long-distance MDI-QSS in the near future."],"url":"http://arxiv.org/abs/2405.16970v1","category":"quant-ph"}
{"created":"2024-05-27 09:00:30","title":"Dual-Delayed Asynchronous SGD for Arbitrarily Heterogeneous Data","abstract":"We consider the distributed learning problem with data dispersed across multiple workers under the orchestration of a central server. Asynchronous Stochastic Gradient Descent (SGD) has been widely explored in such a setting to reduce the synchronization overhead associated with parallelization. However, the performance of asynchronous SGD algorithms often depends on a bounded dissimilarity condition among the workers' local data, a condition that can drastically affect their efficiency when the workers' data are highly heterogeneous. To overcome this limitation, we introduce the \\textit{dual-delayed asynchronous SGD (DuDe-ASGD)} algorithm designed to neutralize the adverse effects of data heterogeneity. DuDe-ASGD makes full use of stale stochastic gradients from all workers during asynchronous training, leading to two distinct time lags in the model parameters and data samples utilized in the server's iterations. Furthermore, by adopting an incremental aggregation strategy, DuDe-ASGD maintains a per-iteration computational cost that is on par with traditional asynchronous SGD algorithms. Our analysis demonstrates that DuDe-ASGD achieves a near-minimax-optimal convergence rate for smooth nonconvex problems, even when the data across workers are extremely heterogeneous. Numerical experiments indicate that DuDe-ASGD compares favorably with existing asynchronous and synchronous SGD-based algorithms.","sentences":["We consider the distributed learning problem with data dispersed across multiple workers under the orchestration of a central server.","Asynchronous Stochastic Gradient Descent (SGD) has been widely explored in such a setting to reduce the synchronization overhead associated with parallelization.","However, the performance of asynchronous SGD algorithms often depends on a bounded dissimilarity condition among the workers' local data, a condition that can drastically affect their efficiency when the workers' data are highly heterogeneous.","To overcome this limitation, we introduce the \\textit{dual-delayed asynchronous SGD (DuDe-ASGD)} algorithm designed to neutralize the adverse effects of data heterogeneity.","DuDe-ASGD makes full use of stale stochastic gradients from all workers during asynchronous training, leading to two distinct time lags in the model parameters and data samples utilized in the server's iterations.","Furthermore, by adopting an incremental aggregation strategy, DuDe-ASGD maintains a per-iteration computational cost that is on par with traditional asynchronous SGD algorithms.","Our analysis demonstrates that DuDe-ASGD achieves a near-minimax-optimal convergence rate for smooth nonconvex problems, even when the data across workers are extremely heterogeneous.","Numerical experiments indicate that DuDe-ASGD compares favorably with existing asynchronous and synchronous SGD-based algorithms."],"url":"http://arxiv.org/abs/2405.16966v1","category":"cs.LG"}
{"created":"2024-05-27 08:55:17","title":"DCPI-Depth: Explicitly Infusing Dense Correspondence Prior to Unsupervised Monocular Depth Estimation","abstract":"There has been a recent surge of interest in learning to perceive depth from monocular videos in an unsupervised fashion. A key challenge in this field is achieving robust and accurate depth estimation in challenging scenarios, particularly in regions with weak textures or where dynamic objects are present. This study makes three major contributions by delving deeply into dense correspondence priors to provide existing frameworks with explicit geometric constraints. The first novelty is a contextual-geometric depth consistency loss, which employs depth maps triangulated from dense correspondences based on estimated ego-motion to guide the learning of depth perception from contextual information, since explicitly triangulated depth maps capture accurate relative distances among pixels. The second novelty arises from the observation that there exists an explicit, deducible relationship between optical flow divergence and depth gradient. A differential property correlation loss is, therefore, designed to refine depth estimation with a specific emphasis on local variations. The third novelty is a bidirectional stream co-adjustment strategy that enhances the interaction between rigid and optical flows, encouraging the former towards more accurate correspondence and making the latter more adaptable across various scenarios under the static scene hypotheses. DCPI-Depth, a framework that incorporates all these innovative components and couples two bidirectional and collaborative streams, achieves state-of-the-art performance and generalizability across multiple public datasets, outperforming all existing prior arts. Specifically, it demonstrates accurate depth estimation in texture-less and dynamic regions, and shows more reasonable smoothness.","sentences":["There has been a recent surge of interest in learning to perceive depth from monocular videos in an unsupervised fashion.","A key challenge in this field is achieving robust and accurate depth estimation in challenging scenarios, particularly in regions with weak textures or where dynamic objects are present.","This study makes three major contributions by delving deeply into dense correspondence priors to provide existing frameworks with explicit geometric constraints.","The first novelty is a contextual-geometric depth consistency loss, which employs depth maps triangulated from dense correspondences based on estimated ego-motion to guide the learning of depth perception from contextual information, since explicitly triangulated depth maps capture accurate relative distances among pixels.","The second novelty arises from the observation that there exists an explicit, deducible relationship between optical flow divergence and depth gradient.","A differential property correlation loss is, therefore, designed to refine depth estimation with a specific emphasis on local variations.","The third novelty is a bidirectional stream co-adjustment strategy that enhances the interaction between rigid and optical flows, encouraging the former towards more accurate correspondence and making the latter more adaptable across various scenarios under the static scene hypotheses.","DCPI-Depth, a framework that incorporates all these innovative components and couples two bidirectional and collaborative streams, achieves state-of-the-art performance and generalizability across multiple public datasets, outperforming all existing prior arts.","Specifically, it demonstrates accurate depth estimation in texture-less and dynamic regions, and shows more reasonable smoothness."],"url":"http://arxiv.org/abs/2405.16960v1","category":"cs.CV"}
{"created":"2024-05-27 08:46:28","title":"Convergence of SGD with momentum in the nonconvex case: A novel time window-based analysis","abstract":"We propose a novel time window-based analysis technique to investigate the convergence behavior of the stochastic gradient descent method with momentum (SGDM) in nonconvex settings. Despite its popularity, the convergence behavior of SGDM remains less understood in nonconvex scenarios. This is primarily due to the absence of a sufficient descent property and challenges in controlling stochastic errors in an almost sure sense. To address these challenges, we study the behavior of SGDM over specific time windows, rather than examining the descent of consecutive iterates as in traditional analyses. This time window-based approach simplifies the convergence analysis and enables us to establish the first iterate convergence result for SGDM under the Kurdyka-Lojasiewicz (KL) property. Based on the underlying KL exponent and the utilized step size scheme, we further characterize local convergence rates of SGDM.","sentences":["We propose a novel time window-based analysis technique to investigate the convergence behavior of the stochastic gradient descent method with momentum (SGDM) in nonconvex settings.","Despite its popularity, the convergence behavior of SGDM remains less understood in nonconvex scenarios.","This is primarily due to the absence of a sufficient descent property and challenges in controlling stochastic errors in an almost sure sense.","To address these challenges, we study the behavior of SGDM over specific time windows, rather than examining the descent of consecutive iterates as in traditional analyses.","This time window-based approach simplifies the convergence analysis and enables us to establish the first iterate convergence result for SGDM under the Kurdyka-Lojasiewicz (KL) property.","Based on the underlying KL exponent and the utilized step size scheme, we further characterize local convergence rates of SGDM."],"url":"http://arxiv.org/abs/2405.16954v1","category":"math.OC"}
{"created":"2024-05-27 08:44:19","title":"A Variance-Preserving Interpolation Approach for Diffusion Models with Applications to Single Channel Speech Enhancement and Recognition","abstract":"In this paper, we propose a variance-preserving interpolation framework to improve diffusion models for single-channel speech enhancement (SE) and automatic speech recognition (ASR). This new variance-preserving interpolation diffusion model (VPIDM) approach requires only 25 iterative steps and obviates the need for a corrector, an essential element in the existing variance-exploding interpolation diffusion model (VEIDM). Two notable distinctions between VPIDM and VEIDM are the scaling function of the mean of state variables and the constraint imposed on the variance relative to the mean's scale. We conduct a systematic exploration of the theoretical mechanism underlying VPIDM and develop insights regarding VPIDM's applications in SE and ASR using VPIDM as a frontend. Our proposed approach, evaluated on two distinct data sets, demonstrates VPIDM's superior performances over conventional discriminative SE algorithms. Furthermore, we assess the performance of the proposed model under varying signal-to-noise ratio (SNR) levels. The investigation reveals VPIDM's improved robustness in target noise elimination when compared to VEIDM. Furthermore, utilizing the mid-outputs of both VPIDM and VEIDM results in enhanced ASR accuracies, thereby highlighting the practical efficacy of our proposed approach.","sentences":["In this paper, we propose a variance-preserving interpolation framework to improve diffusion models for single-channel speech enhancement (SE) and automatic speech recognition (ASR).","This new variance-preserving interpolation diffusion model (VPIDM) approach requires only 25 iterative steps and obviates the need for a corrector, an essential element in the existing variance-exploding interpolation diffusion model (VEIDM).","Two notable distinctions between VPIDM and VEIDM are the scaling function of the mean of state variables and the constraint imposed on the variance relative to the mean's scale.","We conduct a systematic exploration of the theoretical mechanism underlying VPIDM and develop insights regarding VPIDM's applications in SE and ASR using VPIDM as a frontend.","Our proposed approach, evaluated on two distinct data sets, demonstrates VPIDM's superior performances over conventional discriminative SE algorithms.","Furthermore, we assess the performance of the proposed model under varying signal-to-noise ratio (SNR) levels.","The investigation reveals VPIDM's improved robustness in target noise elimination when compared to VEIDM.","Furthermore, utilizing the mid-outputs of both VPIDM and VEIDM results in enhanced ASR accuracies, thereby highlighting the practical efficacy of our proposed approach."],"url":"http://arxiv.org/abs/2405.16952v1","category":"eess.AS"}
{"created":"2024-05-27 08:26:58","title":"Do Vision-Language Transformers Exhibit Visual Commonsense? An Empirical Study of VCR","abstract":"Visual Commonsense Reasoning (VCR) calls for explanatory reasoning behind question answering over visual scenes. To achieve this goal, a model is required to provide an acceptable rationale as the reason for the predicted answers. Progress on the benchmark dataset stems largely from the recent advancement of Vision-Language Transformers (VL Transformers). These models are first pre-trained on some generic large-scale vision-text datasets, and then the learned representations are transferred to the downstream VCR task. Despite their attractive performance, this paper posits that the VL Transformers do not exhibit visual commonsense, which is the key to VCR. In particular, our empirical results pinpoint several shortcomings of existing VL Transformers: small gains from pre-training, unexpected language bias, limited model architecture for the two inseparable sub-tasks, and neglect of the important object-tag correlation. With these findings, we tentatively suggest some future directions from the aspect of dataset, evaluation metric, and training tricks. We believe this work could make researchers revisit the intuition and goals of VCR, and thus help tackle the remaining challenges in visual reasoning.","sentences":["Visual Commonsense Reasoning (VCR) calls for explanatory reasoning behind question answering over visual scenes.","To achieve this goal, a model is required to provide an acceptable rationale as the reason for the predicted answers.","Progress on the benchmark dataset stems largely from the recent advancement of Vision-Language Transformers (VL Transformers).","These models are first pre-trained on some generic large-scale vision-text datasets, and then the learned representations are transferred to the downstream VCR task.","Despite their attractive performance, this paper posits that the VL Transformers do not exhibit visual commonsense, which is the key to VCR.","In particular, our empirical results pinpoint several shortcomings of existing VL Transformers: small gains from pre-training, unexpected language bias, limited model architecture for the two inseparable sub-tasks, and neglect of the important object-tag correlation.","With these findings, we tentatively suggest some future directions from the aspect of dataset, evaluation metric, and training tricks.","We believe this work could make researchers revisit the intuition and goals of VCR, and thus help tackle the remaining challenges in visual reasoning."],"url":"http://arxiv.org/abs/2405.16934v1","category":"cs.CV"}
{"created":"2024-05-27 08:17:49","title":"Demystifying amortized causal discovery with transformers","abstract":"Supervised learning approaches for causal discovery from observational data often achieve competitive performance despite seemingly avoiding explicit assumptions that traditional methods make for identifiability. In this work, we investigate CSIvA (Ke et al., 2023), a transformer-based model promising to train on synthetic data and transfer to real data. First, we bridge the gap with existing identifiability theory and show that constraints on the training data distribution implicitly define a prior on the test observations. Consistent with classical approaches, good performance is achieved when we have a good prior on the test data, and the underlying model is identifiable. At the same time, we find new trade-offs. Training on datasets generated from different classes of causal models, unambiguously identifiable in isolation, improves the test generalization. Performance is still guaranteed, as the ambiguous cases resulting from the mixture of identifiable causal models are unlikely to occur (which we formally prove). Overall, our study finds that amortized causal discovery still needs to obey identifiability theory, but it also differs from classical methods in how the assumptions are formulated, trading more reliance on assumptions on the noise type for fewer hypotheses on the mechanisms.","sentences":["Supervised learning approaches for causal discovery from observational data often achieve competitive performance despite seemingly avoiding explicit assumptions that traditional methods make for identifiability.","In this work, we investigate CSIvA (Ke et al., 2023), a transformer-based model promising to train on synthetic data and transfer to real data.","First, we bridge the gap with existing identifiability theory and show that constraints on the training data distribution implicitly define a prior on the test observations.","Consistent with classical approaches, good performance is achieved when we have a good prior on the test data, and the underlying model is identifiable.","At the same time, we find new trade-offs.","Training on datasets generated from different classes of causal models, unambiguously identifiable in isolation, improves the test generalization.","Performance is still guaranteed, as the ambiguous cases resulting from the mixture of identifiable causal models are unlikely to occur (which we formally prove).","Overall, our study finds that amortized causal discovery still needs to obey identifiability theory, but it also differs from classical methods in how the assumptions are formulated, trading more reliance on assumptions on the noise type for fewer hypotheses on the mechanisms."],"url":"http://arxiv.org/abs/2405.16924v1","category":"cs.LG"}
{"created":"2024-05-27 08:13:16","title":"Flexible strained membranes of multiferroic TbMnO3","abstract":"The multiferroic properties of TbMnO3 demonstrate high versatility under applied pressure, making the material potentially suitable for use in flexible electronics. Here, we report on the preparation of elastic freestanding TbMnO3 membranes with dominant (001) or (010) crystallographic out-of-plane orientation. Membranes with thickness of 20 nm display orthorhombic bulk-like relaxed lattice parameters with strong suppression of twinning for the (010) oriented membranes. Strain in flexible membranes was tuned by using a commercial strain cell device and characterized by Raman spectroscopy. The B1g out-of-phase oxygen-stretching mode, representative for the Mn-O bond distance, systematically shifts to lower energy with increasing strain (epsilon{max} ~ 0.5 %). The flexibility and elastic properties of the membranes allow for specific manipulation of the multiferroic state by strain, whereas the choice of the crystallographic orientation gives possibility for an in- or out-of-plane electric polarization.","sentences":["The multiferroic properties of TbMnO3 demonstrate high versatility under applied pressure, making the material potentially suitable for use in flexible electronics.","Here, we report on the preparation of elastic freestanding TbMnO3 membranes with dominant (001) or (010) crystallographic out-of-plane orientation.","Membranes with thickness of 20 nm display orthorhombic bulk-like relaxed lattice parameters with strong suppression of twinning for the (010) oriented membranes.","Strain in flexible membranes was tuned by using a commercial strain cell device and characterized by Raman spectroscopy.","The B1g out-of-phase oxygen-stretching mode, representative for the Mn-O bond distance, systematically shifts to lower energy with increasing strain (epsilon{max} ~ 0.5 %).","The flexibility and elastic properties of the membranes allow for specific manipulation of the multiferroic state by strain, whereas the choice of the crystallographic orientation gives possibility for an in- or out-of-plane electric polarization."],"url":"http://arxiv.org/abs/2405.16921v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-27 07:56:23","title":"Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?","abstract":"We posit that large language models (LLMs) should be capable of expressing their intrinsic uncertainty in natural language. For example, if the LLM is equally likely to output two contradicting answers to the same question, then its generated response should reflect this uncertainty by hedging its answer (e.g., \"I'm not sure, but I think...\"). We formalize faithful response uncertainty based on the gap between the model's intrinsic confidence in the assertions it makes and the decisiveness by which they are conveyed. This example-level metric reliably indicates whether the model reflects its uncertainty, as it penalizes both excessive and insufficient hedging. We evaluate a variety of aligned LLMs at faithfully communicating uncertainty on several knowledge-intensive question answering tasks. Our results provide strong evidence that modern LLMs are poor at faithfully conveying their uncertainty, and that better alignment is necessary to improve their trustworthiness.","sentences":["We posit that large language models (LLMs) should be capable of expressing their intrinsic uncertainty in natural language.","For example, if the LLM is equally likely to output two contradicting answers to the same question, then its generated response should reflect this uncertainty by hedging its answer (e.g., \"I'm not sure, but I think...\").","We formalize faithful response uncertainty based on the gap between the model's intrinsic confidence in the assertions it makes and the decisiveness by which they are conveyed.","This example-level metric reliably indicates whether the model reflects its uncertainty, as it penalizes both excessive and insufficient hedging.","We evaluate a variety of aligned LLMs at faithfully communicating uncertainty on several knowledge-intensive question answering tasks.","Our results provide strong evidence that modern LLMs are poor at faithfully conveying their uncertainty, and that better alignment is necessary to improve their trustworthiness."],"url":"http://arxiv.org/abs/2405.16908v1","category":"cs.CL"}
{"created":"2024-05-27 07:55:27","title":"Harnessing the Power of Vicinity-Informed Analysis for Classification under Covariate Shift","abstract":"Transfer learning enhances prediction accuracy on a target distribution by leveraging data from a source distribution, demonstrating significant benefits in various applications. This paper introduces a novel dissimilarity measure that utilizes vicinity information, i.e., the local structure of data points, to analyze the excess error in classification under covariate shift, a transfer learning setting where marginal feature distributions differ but conditional label distributions remain the same. We characterize the excess error using the proposed measure and demonstrate faster or competitive convergence rates compared to previous techniques. Notably, our approach is effective in situations where the non-absolute continuousness assumption, which often appears in real-world applications, holds. Our theoretical analysis bridges the gap between current theoretical findings and empirical observations in transfer learning, particularly in scenarios with significant differences between source and target distributions.","sentences":["Transfer learning enhances prediction accuracy on a target distribution by leveraging data from a source distribution, demonstrating significant benefits in various applications.","This paper introduces a novel dissimilarity measure that utilizes vicinity information, i.e., the local structure of data points, to analyze the excess error in classification under covariate shift, a transfer learning setting where marginal feature distributions differ but conditional label distributions remain the same.","We characterize the excess error using the proposed measure and demonstrate faster or competitive convergence rates compared to previous techniques.","Notably, our approach is effective in situations where the non-absolute continuousness assumption, which often appears in real-world applications, holds.","Our theoretical analysis bridges the gap between current theoretical findings and empirical observations in transfer learning, particularly in scenarios with significant differences between source and target distributions."],"url":"http://arxiv.org/abs/2405.16906v1","category":"stat.ML"}
{"created":"2024-05-27 07:45:23","title":"The CR geometry of the three-segment snake","abstract":"We study the geometry associated with the kinematics of a planar robot known as the \"three-segment snake,\" whose velocity distribution belongs to a class of (2,3,5) distributions. We discover that, under certain assumptions on its construction parameters, the snake may be endowed with a CR structure of CR dimension 1 and real codimension 3. We solve the associated Cartan equivalence problem and find the invariants of the snake's CR structure.","sentences":["We study the geometry associated with the kinematics of a planar robot known as the \"three-segment snake,\" whose velocity distribution belongs to a class of (2,3,5) distributions.","We discover that, under certain assumptions on its construction parameters, the snake may be endowed with a CR structure of CR dimension 1 and real codimension 3.","We solve the associated Cartan equivalence problem and find the invariants of the snake's CR structure."],"url":"http://arxiv.org/abs/2405.16898v1","category":"math.DG"}
{"created":"2024-05-27 07:13:13","title":"PivotMesh: Generic 3D Mesh Generation via Pivot Vertices Guidance","abstract":"Generating compact and sharply detailed 3D meshes poses a significant challenge for current 3D generative models. Different from extracting dense meshes from neural representation, some recent works try to model the native mesh distribution (i.e., a set of triangles), which generates more compact results as humans crafted. However, due to the complexity and variety of mesh topology, these methods are typically limited to small datasets with specific categories and are hard to extend. In this paper, we introduce a generic and scalable mesh generation framework PivotMesh, which makes an initial attempt to extend the native mesh generation to large-scale datasets. We employ a transformer-based auto-encoder to encode meshes into discrete tokens and decode them from face level to vertex level hierarchically. Subsequently, to model the complex typology, we first learn to generate pivot vertices as coarse mesh representation and then generate the complete mesh tokens with the same auto-regressive Transformer. This reduces the difficulty compared with directly modeling the mesh distribution and further improves the model controllability. PivotMesh demonstrates its versatility by effectively learning from both small datasets like Shapenet, and large-scale datasets like Objaverse and Objaverse-xl. Extensive experiments indicate that PivotMesh can generate compact and sharp 3D meshes across various categories, highlighting its great potential for native mesh modeling.","sentences":["Generating compact and sharply detailed 3D meshes poses a significant challenge for current 3D generative models.","Different from extracting dense meshes from neural representation, some recent works try to model the native mesh distribution (i.e., a set of triangles), which generates more compact results as humans crafted.","However, due to the complexity and variety of mesh topology, these methods are typically limited to small datasets with specific categories and are hard to extend.","In this paper, we introduce a generic and scalable mesh generation framework PivotMesh, which makes an initial attempt to extend the native mesh generation to large-scale datasets.","We employ a transformer-based auto-encoder to encode meshes into discrete tokens and decode them from face level to vertex level hierarchically.","Subsequently, to model the complex typology, we first learn to generate pivot vertices as coarse mesh representation and then generate the complete mesh tokens with the same auto-regressive Transformer.","This reduces the difficulty compared with directly modeling the mesh distribution and further improves the model controllability.","PivotMesh demonstrates its versatility by effectively learning from both small datasets like Shapenet, and large-scale datasets like Objaverse and Objaverse-xl.","Extensive experiments indicate that PivotMesh can generate compact and sharp 3D meshes across various categories, highlighting its great potential for native mesh modeling."],"url":"http://arxiv.org/abs/2405.16890v1","category":"cs.CV"}
{"created":"2024-05-27 07:10:21","title":"Part123: Part-aware 3D Reconstruction from a Single-view Image","abstract":"Recently, the emergence of diffusion models has opened up new opportunities for single-view reconstruction. However, all the existing methods represent the target object as a closed mesh devoid of any structural information, thus neglecting the part-based structure, which is crucial for many downstream applications, of the reconstructed shape. Moreover, the generated meshes usually suffer from large noises, unsmooth surfaces, and blurry textures, making it challenging to obtain satisfactory part segments using 3D segmentation techniques. In this paper, we present Part123, a novel framework for part-aware 3D reconstruction from a single-view image. We first use diffusion models to generate multiview-consistent images from a given image, and then leverage Segment Anything Model (SAM), which demonstrates powerful generalization ability on arbitrary objects, to generate multiview segmentation masks. To effectively incorporate 2D part-based information into 3D reconstruction and handle inconsistency, we introduce contrastive learning into a neural rendering framework to learn a part-aware feature space based on the multiview segmentation masks. A clustering-based algorithm is also developed to automatically derive 3D part segmentation results from the reconstructed models. Experiments show that our method can generate 3D models with high-quality segmented parts on various objects. Compared to existing unstructured reconstruction methods, the part-aware 3D models from our method benefit some important applications, including feature-preserving reconstruction, primitive fitting, and 3D shape editing.","sentences":["Recently, the emergence of diffusion models has opened up new opportunities for single-view reconstruction.","However, all the existing methods represent the target object as a closed mesh devoid of any structural information, thus neglecting the part-based structure, which is crucial for many downstream applications, of the reconstructed shape.","Moreover, the generated meshes usually suffer from large noises, unsmooth surfaces, and blurry textures, making it challenging to obtain satisfactory part segments using 3D segmentation techniques.","In this paper, we present Part123, a novel framework for part-aware 3D reconstruction from a single-view image.","We first use diffusion models to generate multiview-consistent images from a given image, and then leverage Segment Anything Model (SAM), which demonstrates powerful generalization ability on arbitrary objects, to generate multiview segmentation masks.","To effectively incorporate 2D part-based information into 3D reconstruction and handle inconsistency, we introduce contrastive learning into a neural rendering framework to learn a part-aware feature space based on the multiview segmentation masks.","A clustering-based algorithm is also developed to automatically derive 3D part segmentation results from the reconstructed models.","Experiments show that our method can generate 3D models with high-quality segmented parts on various objects.","Compared to existing unstructured reconstruction methods, the part-aware 3D models from our method benefit some important applications, including feature-preserving reconstruction, primitive fitting, and 3D shape editing."],"url":"http://arxiv.org/abs/2405.16888v1","category":"cs.GR"}
{"created":"2024-05-27 07:05:27","title":"Match, Compare, or Select? An Investigation of Large Language Models for Entity Matching","abstract":"Entity matching (EM) is a critical step in entity resolution. Recently, entity matching based on large language models (LLMs) has shown great promise. However, current LLM-based entity matching approaches typically follow a binary matching paradigm that ignores the global consistency between different records. In this paper, we investigate various methodologies for LLM-based entity matching that incorporate record interactions from different perspectives. Specifically, we comprehensively compare three representative strategies: matching, comparing, and selecting, and analyze their respective advantages and challenges in diverse scenarios. Based on our findings, we further design a compositional entity matching (ComEM) framework that leverages the composition of multiple strategies and LLMs. In this way, ComEM can benefit from the advantages of different sides and achieve improvements in both effectiveness and efficiency. Experimental results show that ComEM not only achieves significant performance gains on various datasets but also reduces the cost of LLM-based entity matching in real-world application.","sentences":["Entity matching (EM) is a critical step in entity resolution.","Recently, entity matching based on large language models (LLMs) has shown great promise.","However, current LLM-based entity matching approaches typically follow a binary matching paradigm that ignores the global consistency between different records.","In this paper, we investigate various methodologies for LLM-based entity matching that incorporate record interactions from different perspectives.","Specifically, we comprehensively compare three representative strategies: matching, comparing, and selecting, and analyze their respective advantages and challenges in diverse scenarios.","Based on our findings, we further design a compositional entity matching (ComEM) framework that leverages the composition of multiple strategies and LLMs.","In this way, ComEM can benefit from the advantages of different sides and achieve improvements in both effectiveness and efficiency.","Experimental results show that ComEM not only achieves significant performance gains on various datasets but also reduces the cost of LLM-based entity matching in real-world application."],"url":"http://arxiv.org/abs/2405.16884v1","category":"cs.CL"}
{"created":"2024-05-27 06:49:43","title":"Complementary Search of Fermionic Absorption Operators at Hadron Collider and Direct Detection Experiments","abstract":"Instead of the energy recoil signal at direct detection experiments, dark matter appears always as missing energy at high energy colliders. For a fermionc dark matter coupled with quarks and neutrino via absorption operators, its production is always accompanied by an invisible neutrino. We study in details the mono-X (photon, jet, and $Z$) productions at the Large Hadron Collider (LHC). To make easy comparison between the collider and DM direct detection experiments, we start from the quark-level absorption operators for the first time. In other words, we study the model-independent constraints on a generic fermionic absorption dark fermion. In addition, the interplay and comparison with the possible detection at the neutrino experiment especially Borexino is also briefly discussed. We find that for both spin-dependent and spin-independent absorption of the dark matter, the experiments with light nuclear target can provide the strongest constraints.","sentences":["Instead of the energy recoil signal at direct detection experiments, dark matter appears always as missing energy at high energy colliders.","For a fermionc dark matter coupled with quarks and neutrino via absorption operators, its production is always accompanied by an invisible neutrino.","We study in details the mono-X (photon, jet, and $Z$) productions at the Large Hadron Collider (LHC).","To make easy comparison between the collider and DM direct detection experiments, we start from the quark-level absorption operators for the first time.","In other words, we study the model-independent constraints on a generic fermionic absorption dark fermion.","In addition, the interplay and comparison with the possible detection at the neutrino experiment especially Borexino is also briefly discussed.","We find that for both spin-dependent and spin-independent absorption of the dark matter, the experiments with light nuclear target can provide the strongest constraints."],"url":"http://arxiv.org/abs/2405.16878v1","category":"hep-ph"}
{"created":"2024-05-27 06:48:21","title":"Digitalization in Infrastructure Construction Projects: A PRISMA-Based Review of Benefits and Obstacles","abstract":"The current study presents a comprehensive review of the benefits and barriers associated with the adoption of Building Information Modeling (BIM) in infrastructure projects, focusing on the period from 2013 to 2023. The research explores the manifold advantages offered by BIM, spanning the entire project life cycle, including planning, design, construction, maintenance, and sustainability. Notably, BIM enhances collaboration, facilitates real-time data-driven decision-making, and leads to substantial cost and time savings. In parallel, a systematic literature review was conducted to identify and categorize the barriers hindering BIM adoption within the infrastructure industry. Eleven studies were selected for in-depth analysis, revealing a total of 74 obstacles. Through synthetic analysis and thematic clustering, seven primary impediments to BIM adoption were identified, encompassing challenges related to education/training, resistance to change, business value clarity, perceived cost, lack of standards and guidelines, lack of mandates, and lack of initiatives. This review explores the benefits and barriers in the industry that are facing BIM adoption in infrastructure projects, giving an important perspective toward improving effective BIM adoption strategies, policies, and standards. Future directions for research and industry development are outlined, including efforts to enhance education and training, promote standardization, advocate for policy and mandates, and integrate BIM with emerging technologies.","sentences":["The current study presents a comprehensive review of the benefits and barriers associated with the adoption of Building Information Modeling (BIM) in infrastructure projects, focusing on the period from 2013 to 2023.","The research explores the manifold advantages offered by BIM, spanning the entire project life cycle, including planning, design, construction, maintenance, and sustainability.","Notably, BIM enhances collaboration, facilitates real-time data-driven decision-making, and leads to substantial cost and time savings.","In parallel, a systematic literature review was conducted to identify and categorize the barriers hindering BIM adoption within the infrastructure industry.","Eleven studies were selected for in-depth analysis, revealing a total of 74 obstacles.","Through synthetic analysis and thematic clustering, seven primary impediments to BIM adoption were identified, encompassing challenges related to education/training, resistance to change, business value clarity, perceived cost, lack of standards and guidelines, lack of mandates, and lack of initiatives.","This review explores the benefits and barriers in the industry that are facing BIM adoption in infrastructure projects, giving an important perspective toward improving effective BIM adoption strategies, policies, and standards.","Future directions for research and industry development are outlined, including efforts to enhance education and training, promote standardization, advocate for policy and mandates, and integrate BIM with emerging technologies."],"url":"http://arxiv.org/abs/2405.16875v1","category":"cs.RO"}
{"created":"2024-05-27 06:43:12","title":"ContrastAlign: Toward Robust BEV Feature Alignment via Contrastive Learning for Multi-Modal 3D Object Detection","abstract":"In the field of 3D object detection tasks, fusing heterogeneous features from LiDAR and camera sensors into a unified Bird's Eye View (BEV) representation is a widely adopted paradigm. However, existing methods are often compromised by imprecise sensor calibration, resulting in feature misalignment in LiDAR-camera BEV fusion. Moreover, such inaccuracies result in errors in depth estimation for the camera branch, ultimately causing misalignment between LiDAR and camera BEV features. In this work, we propose a novel ContrastAlign approach that utilizes contrastive learning to enhance the alignment of heterogeneous modalities, thereby improving the robustness of the fusion process. Specifically, our approach includes the L-Instance module, which directly outputs LiDAR instance features within LiDAR BEV features. Then, we introduce the C-Instance module, which predicts camera instance features through RoI (Region of Interest) pooling on the camera BEV features. We propose the InstanceFusion module, which utilizes contrastive learning to generate similar instance features across heterogeneous modalities. We then use graph matching to calculate the similarity between the neighboring camera instance features and the similarity instance features to complete the alignment of instance features. Our method achieves state-of-the-art performance, with an mAP of 70.3%, surpassing BEVFusion by 1.8% on the nuScenes validation set. Importantly, our method outperforms BEVFusion by 7.3% under conditions with misalignment noise.","sentences":["In the field of 3D object detection tasks, fusing heterogeneous features from LiDAR and camera sensors into a unified Bird's Eye View (BEV) representation is a widely adopted paradigm.","However, existing methods are often compromised by imprecise sensor calibration, resulting in feature misalignment in LiDAR-camera BEV fusion.","Moreover, such inaccuracies result in errors in depth estimation for the camera branch, ultimately causing misalignment between LiDAR and camera BEV features.","In this work, we propose a novel ContrastAlign approach that utilizes contrastive learning to enhance the alignment of heterogeneous modalities, thereby improving the robustness of the fusion process.","Specifically, our approach includes the L-Instance module, which directly outputs LiDAR instance features within LiDAR BEV features.","Then, we introduce the C-Instance module, which predicts camera instance features through RoI (Region of Interest) pooling on the camera BEV features.","We propose the InstanceFusion module, which utilizes contrastive learning to generate similar instance features across heterogeneous modalities.","We then use graph matching to calculate the similarity between the neighboring camera instance features and the similarity instance features to complete the alignment of instance features.","Our method achieves state-of-the-art performance, with an mAP of 70.3%, surpassing BEVFusion by 1.8% on the nuScenes validation set.","Importantly, our method outperforms BEVFusion by 7.3% under conditions with misalignment noise."],"url":"http://arxiv.org/abs/2405.16873v1","category":"cs.CV"}
{"created":"2024-05-27 06:35:55","title":"RCDN: Towards Robust Camera-Insensitivity Collaborative Perception via Dynamic Feature-based 3D Neural Modeling","abstract":"Collaborative perception is dedicated to tackling the constraints of single-agent perception, such as occlusions, based on the multiple agents' multi-view sensor inputs. However, most existing works assume an ideal condition that all agents' multi-view cameras are continuously available. In reality, cameras may be highly noisy, obscured or even failed during the collaboration. In this work, we introduce a new robust camera-insensitivity problem: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? To address above problems, we propose RCDN, a Robust Camera-insensitivity collaborative perception with a novel Dynamic feature-based 3D Neural modeling mechanism. The key intuition of RCDN is to construct collaborative neural rendering field representations to recover failed perceptual messages sent by multiple agents. To better model collaborative neural rendering field, RCDN first establishes a geometry BEV feature based time-invariant static field with other agents via fast hash grid modeling. Based on the static background field, the proposed time-varying dynamic field can model corresponding motion vectors for foregrounds with appropriate positions. To validate RCDN, we create OPV2V-N, a new large-scale dataset with manual labelling under different camera failed scenarios. Extensive experiments conducted on OPV2V-N show that RCDN can be ported to other baselines and improve their robustness in extreme camera-insensitivity settings. Our code and datasets will be available soon.","sentences":["Collaborative perception is dedicated to tackling the constraints of single-agent perception, such as occlusions, based on the multiple agents' multi-view sensor inputs.","However, most existing works assume an ideal condition that all agents' multi-view cameras are continuously available.","In reality, cameras may be highly noisy, obscured or even failed during the collaboration.","In this work, we introduce a new robust camera-insensitivity problem: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost?","To address above problems, we propose RCDN, a Robust Camera-insensitivity collaborative perception with a novel Dynamic feature-based 3D Neural modeling mechanism.","The key intuition of RCDN is to construct collaborative neural rendering field representations to recover failed perceptual messages sent by multiple agents.","To better model collaborative neural rendering field, RCDN first establishes a geometry BEV feature based time-invariant static field with other agents via fast hash grid modeling.","Based on the static background field, the proposed time-varying dynamic field can model corresponding motion vectors for foregrounds with appropriate positions.","To validate RCDN, we create OPV2V-N, a new large-scale dataset with manual labelling under different camera failed scenarios.","Extensive experiments conducted on OPV2V-N show that RCDN can be ported to other baselines and improve their robustness in extreme camera-insensitivity settings.","Our code and datasets will be available soon."],"url":"http://arxiv.org/abs/2405.16868v1","category":"cs.CV"}
{"created":"2024-05-27 06:28:53","title":"All-voltage control of Giant Magnetoresistance","abstract":"The aim of voltage control of magnetism is to reduce the power consumption of spintronic devices. For a spin valve, the magnetization directions of two ferromagnetic layers determine the giant magnetoresistance magnitude. However, achieving all-voltage manipulation of the magnetization directions between parallel and antiparallel states is a significant challenge. Here, we demonstrate that by utilizing two exchange-biased Co/IrMn bilayers with opposite pinning directions and with ferromagnetic coupling through the Ruderman-Kittel-Kasuya-Yosida interaction between two Co layers, the magnetization directions of the two ferromagnetic layers of a spin valve can be switched between parallel and antiparallel states through allvoltage-induced strain control. The all-voltage controlled giant magnetoresistance is repeatable and nonvolatile. The rotation of magnetizations in the two Co layers under voltages, from antiparallel to parallel states, occurs in opposite directions as revealed through simulations utilizing the Landau-Lifshitz-Gilbert equation. This work can provide valuable reference for the development of low-power all-voltage-controlled spintronic devices.","sentences":["The aim of voltage control of magnetism is to reduce the power consumption of spintronic devices.","For a spin valve, the magnetization directions of two ferromagnetic layers determine the giant magnetoresistance magnitude.","However, achieving all-voltage manipulation of the magnetization directions between parallel and antiparallel states is a significant challenge.","Here, we demonstrate that by utilizing two exchange-biased Co/IrMn bilayers with opposite pinning directions and with ferromagnetic coupling through the Ruderman-Kittel-Kasuya-Yosida interaction between two Co layers, the magnetization directions of the two ferromagnetic layers of a spin valve can be switched between parallel and antiparallel states through allvoltage-induced strain control.","The all-voltage controlled giant magnetoresistance is repeatable and nonvolatile.","The rotation of magnetizations in the two Co layers under voltages, from antiparallel to parallel states, occurs in opposite directions as revealed through simulations utilizing the Landau-Lifshitz-Gilbert equation.","This work can provide valuable reference for the development of low-power all-voltage-controlled spintronic devices."],"url":"http://arxiv.org/abs/2405.16863v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-27 05:41:06","title":"On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability","abstract":"Autoregressively trained transformers have brought a profound revolution to the world, especially with their in-context learning (ICL) ability to address downstream tasks. Recently, several studies suggest that transformers learn a mesa-optimizer during autoregressive (AR) pretraining to implement ICL. Namely, the forward pass of the trained transformer is equivalent to optimizing an inner objective function in-context. However, whether the practical non-convex training dynamics will converge to the ideal mesa-optimizer is still unclear. Towards filling this gap, we investigate the non-convex dynamics of a one-layer linear causal self-attention model autoregressively trained by gradient flow, where the sequences are generated by an AR process $x_{t+1} = W x_t$. First, under a certain condition of data distribution, we prove that an autoregressively trained transformer learns $W$ by implementing one step of gradient descent to minimize an ordinary least squares (OLS) problem in-context. It then applies the learned $\\widehat{W}$ for next-token prediction, thereby verifying the mesa-optimization hypothesis. Next, under the same data conditions, we explore the capability limitations of the obtained mesa-optimizer. We show that a stronger assumption related to the moments of data is the sufficient and necessary condition that the learned mesa-optimizer recovers the distribution. Besides, we conduct exploratory analyses beyond the first data condition and prove that generally, the trained transformer will not perform vanilla gradient descent for the OLS problem. Finally, our simulation results verify the theoretical results.","sentences":["Autoregressively trained transformers have brought a profound revolution to the world, especially with their in-context learning (ICL) ability to address downstream tasks.","Recently, several studies suggest that transformers learn a mesa-optimizer during autoregressive (AR) pretraining to implement ICL.","Namely, the forward pass of the trained transformer is equivalent to optimizing an inner objective function in-context.","However, whether the practical non-convex training dynamics will converge to the ideal mesa-optimizer is still unclear.","Towards filling this gap, we investigate the non-convex dynamics of a one-layer linear causal self-attention model autoregressively trained by gradient flow, where the sequences are generated by an AR process $x_{t+1} = W x_t$.","First, under a certain condition of data distribution, we prove that an autoregressively trained transformer learns $W$ by implementing one step of gradient descent to minimize an ordinary least squares (OLS) problem in-context.","It then applies the learned $\\widehat{W}$ for next-token prediction, thereby verifying the mesa-optimization hypothesis.","Next, under the same data conditions, we explore the capability limitations of the obtained mesa-optimizer.","We show that a stronger assumption related to the moments of data is the sufficient and necessary condition that the learned mesa-optimizer recovers the distribution.","Besides, we conduct exploratory analyses beyond the first data condition and prove that generally, the trained transformer will not perform vanilla gradient descent for the OLS problem.","Finally, our simulation results verify the theoretical results."],"url":"http://arxiv.org/abs/2405.16845v1","category":"cs.LG"}
{"created":"2024-05-27 05:23:27","title":"Delay Performance Analysis of Delay-Deterministic Wireless Networks with Infinite and Finite Blocklength Transmission","abstract":"In order to achieve stable and reliable industrial manufacturing, wireless networks must meet the stringent communication requirements of industrial automation, particularly the need for deterministic low latency communication. The limited wireless resources and time-varying fading channel contribute to the random fluctuations of transmission delay, making it challenging to realize delay-deterministic wireless networks. An open challenge in this context is to model delay determinism, also known as jitter, and analyze delay performance. In this paper, we model jitter as the variance of delay and conduct a comprehensive analysis of delay performance. Specifically, we consider two transmission regimes: infinite blocklength (IBL) and finite blocklength (FBL). In the IBL regime, the distribution of the transmission delay is analyzed, and the closed-form expressions for the average delay, jitter, and delay violation probability are derived. In the FBL regime, an upper bound on the transmission delay is first approximated at a high signalto-noise ratio. Based on this upper bound, the delay distribution, delay violation probability, average delay, and jitter are derived. Finally, simulation results are provided to validate the accuracy of the analysis and derivations. Additionally, the impact of system parameters on jitter is analyzed to gain further insights.","sentences":["In order to achieve stable and reliable industrial manufacturing, wireless networks must meet the stringent communication requirements of industrial automation, particularly the need for deterministic low latency communication.","The limited wireless resources and time-varying fading channel contribute to the random fluctuations of transmission delay, making it challenging to realize delay-deterministic wireless networks.","An open challenge in this context is to model delay determinism, also known as jitter, and analyze delay performance.","In this paper, we model jitter as the variance of delay and conduct a comprehensive analysis of delay performance.","Specifically, we consider two transmission regimes: infinite blocklength (IBL) and finite blocklength (FBL).","In the IBL regime, the distribution of the transmission delay is analyzed, and the closed-form expressions for the average delay, jitter, and delay violation probability are derived.","In the FBL regime, an upper bound on the transmission delay is first approximated at a high signalto-noise ratio.","Based on this upper bound, the delay distribution, delay violation probability, average delay, and jitter are derived.","Finally, simulation results are provided to validate the accuracy of the analysis and derivations.","Additionally, the impact of system parameters on jitter is analyzed to gain further insights."],"url":"http://arxiv.org/abs/2405.16840v1","category":"math.OC"}
{"created":"2024-05-27 05:10:49","title":"Enhancing Accuracy in Generative Models via Knowledge Transfer","abstract":"This paper investigates the accuracy of generative models and the impact of knowledge transfer on their generation precision. Specifically, we examine a generative model for a target task, fine-tuned using a pre-trained model from a source task. Building on the \"Shared Embedding\" concept, which bridges the source and target tasks, we introduce a novel framework for transfer learning under distribution metrics such as the Kullback-Leibler divergence. This framework underscores the importance of leveraging inherent similarities between diverse tasks despite their distinct data distributions. Our theory suggests that the shared structures can augment the generation accuracy for a target task, reliant on the capability of a source model to identify shared structures and effective knowledge transfer from source to target learning. To demonstrate the practical utility of this framework, we explore the theoretical implications for two specific generative models: diffusion and normalizing flows. The results show enhanced performance in both models over their non-transfer counterparts, indicating advancements for diffusion models and providing fresh insights into normalizing flows in transfer and non-transfer settings. These results highlight the significant contribution of knowledge transfer in boosting the generation capabilities of these models.","sentences":["This paper investigates the accuracy of generative models and the impact of knowledge transfer on their generation precision.","Specifically, we examine a generative model for a target task, fine-tuned using a pre-trained model from a source task.","Building on the \"Shared Embedding\" concept, which bridges the source and target tasks, we introduce a novel framework for transfer learning under distribution metrics such as the Kullback-Leibler divergence.","This framework underscores the importance of leveraging inherent similarities between diverse tasks despite their distinct data distributions.","Our theory suggests that the shared structures can augment the generation accuracy for a target task, reliant on the capability of a source model to identify shared structures and effective knowledge transfer from source to target learning.","To demonstrate the practical utility of this framework, we explore the theoretical implications for two specific generative models: diffusion and normalizing flows.","The results show enhanced performance in both models over their non-transfer counterparts, indicating advancements for diffusion models and providing fresh insights into normalizing flows in transfer and non-transfer settings.","These results highlight the significant contribution of knowledge transfer in boosting the generation capabilities of these models."],"url":"http://arxiv.org/abs/2405.16837v1","category":"stat.ML"}
{"created":"2024-05-27 04:49:41","title":"Kernel-based optimally weighted conformal prediction intervals","abstract":"Conformal prediction has been a popular distribution-free framework for uncertainty quantification. In this paper, we present a novel conformal prediction method for time-series, which we call Kernel-based Optimally Weighted Conformal Prediction Intervals (KOWCPI). Specifically, KOWCPI adapts the classic Reweighted Nadaraya-Watson (RNW) estimator for quantile regression on dependent data and learns optimal data-adaptive weights. Theoretically, we tackle the challenge of establishing a conditional coverage guarantee for non-exchangeable data under strong mixing conditions on the non-conformity scores. We demonstrate the superior performance of KOWCPI on real time-series against state-of-the-art methods, where KOWCPI achieves narrower confidence intervals without losing coverage.","sentences":["Conformal prediction has been a popular distribution-free framework for uncertainty quantification.","In this paper, we present a novel conformal prediction method for time-series, which we call Kernel-based Optimally Weighted Conformal Prediction Intervals (KOWCPI).","Specifically, KOWCPI adapts the classic Reweighted Nadaraya-Watson (RNW) estimator for quantile regression on dependent data and learns optimal data-adaptive weights.","Theoretically, we tackle the challenge of establishing a conditional coverage guarantee for non-exchangeable data under strong mixing conditions on the non-conformity scores.","We demonstrate the superior performance of KOWCPI on real time-series against state-of-the-art methods, where KOWCPI achieves narrower confidence intervals without losing coverage."],"url":"http://arxiv.org/abs/2405.16828v1","category":"cs.LG"}
{"created":"2024-05-27 04:47:28","title":"Image formation near hyperbolic umbilic in strong gravitational lensing","abstract":"Hyperbolic umbilic (HU) is a point singularity of the gravitational lens equation, giving rise to a ring-shaped image formation made of four highly magnified images, off-centred from the lens centre. Recent observations have revealed new strongly lensed image formations near HU singularities, and many more are expected in ongoing and future observations. Like fold/cusp, image formations near HU also satisfy magnification relation ($R_{\\rm hu}$), i.e., the signed magnification sum of the four images equals zero. Here, we study how $R_{\\rm hu}$ deviates from zero as a function of area ($A_{\\rm hu}$) covered by the image formation near HU and the distance ($d$) of the central maxima image (which is part of the HU image formation) from the lens centre for ideal single- and double-component cluster-scale lenses. For lens ellipticity values $\\geq0.3$, the central maxima image will form sufficiently far from the lens centre ($d\\gtrsim5''$), similar to the observed HU image formations. We also find that, in some cases, double-component and actual cluster-scale lenses can lead to large cross-sections for HU(-like) image formations for sources at $z\\gtrsim5$ effectively increasing the chances to observe HU(-like) image formation at high redshift. Finally, we study the time delay distribution in the observed HU image formation, finding that not only are these images highly magnified, but the relative time delay between various pairs of HU characteristic image formation has a typical value of $\\sim100$ days, an order of magnitude smaller than generic five-image formations in cluster lenses, making such image formations a promising target for time delay studies.","sentences":["Hyperbolic umbilic (HU) is a point singularity of the gravitational lens equation, giving rise to a ring-shaped image formation made of four highly magnified images, off-centred from the lens centre.","Recent observations have revealed new strongly lensed image formations near HU singularities, and many more are expected in ongoing and future observations.","Like fold/cusp, image formations near HU also satisfy magnification relation ($R_{\\rm hu}$), i.e., the signed magnification sum of the four images equals zero.","Here, we study how $R_{\\rm hu}$ deviates from zero as a function of area ($A_{\\rm hu}$) covered by the image formation near HU and the distance ($d$) of the central maxima image (which is part of the HU image formation) from the lens centre for ideal single- and double-component cluster-scale lenses.","For lens ellipticity values $\\geq0.3$, the central maxima image will form sufficiently far from the lens centre ($d\\gtrsim5''$), similar to the observed HU image formations.","We also find that, in some cases, double-component and actual cluster-scale lenses can lead to large cross-sections for HU(-like) image formations for sources at $z\\gtrsim5$ effectively increasing the chances to observe HU(-like) image formation at high redshift.","Finally, we study the time delay distribution in the observed HU image formation, finding that not only are these images highly magnified, but the relative time delay between various pairs of HU characteristic image formation has a typical value of $\\sim100$ days, an order of magnitude smaller than generic five-image formations in cluster lenses, making such image formations a promising target for time delay studies."],"url":"http://arxiv.org/abs/2405.16826v1","category":"astro-ph.GA"}
{"created":"2024-05-27 04:47:26","title":"From limit theorems to mixing limit theorems","abstract":"Motivated by work of Dolgopyat and N\\'andori, we establish a general method for upgrading limit theorems for Birkhoff sums and cocycles over dynamical systems to mixing limit theorems under mild ergodicity and hyperbolicity assumptions. Building on previous work of Al-Saqban and Forni, we apply this method to obtain mixing limit theorems for particular subbundles of the Kontsevich-Zorich cocycle. In forthcoming work of Arana-Herrera and Honaryar these results are applied to study the arithmetic/homological complexity of long simple closed geodesics on negatively curved surfaces.","sentences":["Motivated by work of Dolgopyat and N\\'andori, we establish a general method for upgrading limit theorems for Birkhoff sums and cocycles over dynamical systems to mixing limit theorems under mild ergodicity and hyperbolicity assumptions.","Building on previous work of Al-Saqban and Forni, we apply this method to obtain mixing limit theorems for particular subbundles of the Kontsevich-Zorich cocycle.","In forthcoming work of Arana-Herrera and Honaryar these results are applied to study the arithmetic/homological complexity of long simple closed geodesics on negatively curved surfaces."],"url":"http://arxiv.org/abs/2405.16825v1","category":"math.DS"}
{"created":"2024-05-27 04:17:10","title":"Image-level Regression for Uncertainty-aware Retinal Image Segmentation","abstract":"Accurate retinal vessel segmentation is a crucial step in the quantitative assessment of retinal vasculature, which is needed for the early detection of retinal diseases and other conditions. Numerous studies have been conducted to tackle the problem of segmenting vessels automatically using a pixel-wise classification approach. The common practice of creating ground truth labels is to categorize pixels as foreground and background. This approach is, however, biased, and it ignores the uncertainty of a human annotator when it comes to annotating e.g. thin vessels. In this work, we propose a simple and effective method that casts the retinal image segmentation task as an image-level regression. For this purpose, we first introduce a novel Segmentation Annotation Uncertainty-Aware (SAUNA) transform, which adds pixel uncertainty to the ground truth using the pixel's closeness to the annotation boundary and vessel thickness. To train our model with soft labels, we generalize the earlier proposed Jaccard metric loss to arbitrary hypercubes, which is a second contribution of this work. The proposed SAUNA transform and the new theoretical results allow us to directly train a standard U-Net-like architecture at the image level, outperforming all recently published methods. We conduct thorough experiments and compare our method to a diverse set of baselines across 5 retinal image datasets. Our implementation is available at \\url{https://github.com/Oulu-IMEDS/SAUNA}.","sentences":["Accurate retinal vessel segmentation is a crucial step in the quantitative assessment of retinal vasculature, which is needed for the early detection of retinal diseases and other conditions.","Numerous studies have been conducted to tackle the problem of segmenting vessels automatically using a pixel-wise classification approach.","The common practice of creating ground truth labels is to categorize pixels as foreground and background.","This approach is, however, biased, and it ignores the uncertainty of a human annotator when it comes to annotating e.g. thin vessels.","In this work, we propose a simple and effective method that casts the retinal image segmentation task as an image-level regression.","For this purpose, we first introduce a novel Segmentation Annotation Uncertainty-Aware (SAUNA) transform, which adds pixel uncertainty to the ground truth using the pixel's closeness to the annotation boundary and vessel thickness.","To train our model with soft labels, we generalize the earlier proposed Jaccard metric loss to arbitrary hypercubes, which is a second contribution of this work.","The proposed SAUNA transform and the new theoretical results allow us to directly train a standard U-Net-like architecture at the image level, outperforming all recently published methods.","We conduct thorough experiments and compare our method to a diverse set of baselines across 5 retinal image datasets.","Our implementation is available at \\url{https://github.com/Oulu-IMEDS/SAUNA}."],"url":"http://arxiv.org/abs/2405.16815v1","category":"cs.CV"}
{"created":"2024-05-27 04:14:20","title":"SiNGR: Brain Tumor Segmentation via Signed Normalized Geodesic Transform Regression","abstract":"One of the primary challenges in brain tumor segmentation arises from the uncertainty of voxels close to tumor boundaries. However, the conventional process of generating ground truth segmentation masks fails to treat such uncertainties properly. Those ``hard labels'' with 0s and 1s conceptually influenced the majority of prior studies on brain image segmentation. As a result, tumor segmentation is often solved through voxel classification. In this work, we instead view this problem as a voxel-level regression, where the ground truth represents a certainty mapping from any pixel based on the distance to tumor border. We propose a novel ground truth label transformation, which is based on a signed geodesic transform, to capture the uncertainty in brain tumors' vicinity, while maintaining a margin between positive and negative samples. We combine this idea with a Focal-like regression L1-loss that enables effective regression learning in high-dimensional output space by appropriately weighting voxels according to their difficulty. We thoroughly conduct an experimental evaluation to validate the components of our proposed method, compare it to a diverse array of state-of-the-art segmentation models, and show that it is architecture-agnostic. The code of our method is made publicly available (\\url{https://github.com/Oulu-IMEDS/SiNGR/}).","sentences":["One of the primary challenges in brain tumor segmentation arises from the uncertainty of voxels close to tumor boundaries.","However, the conventional process of generating ground truth segmentation masks fails to treat such uncertainties properly.","Those ``hard labels'' with 0s and 1s conceptually influenced the majority of prior studies on brain image segmentation.","As a result, tumor segmentation is often solved through voxel classification.","In this work, we instead view this problem as a voxel-level regression, where the ground truth represents a certainty mapping from any pixel based on the distance to tumor border.","We propose a novel ground truth label transformation, which is based on a signed geodesic transform, to capture the uncertainty in brain tumors' vicinity, while maintaining a margin between positive and negative samples.","We combine this idea with a Focal-like regression L1-loss that enables effective regression learning in high-dimensional output space by appropriately weighting voxels according to their difficulty.","We thoroughly conduct an experimental evaluation to validate the components of our proposed method, compare it to a diverse array of state-of-the-art segmentation models, and show that it is architecture-agnostic.","The code of our method is made publicly available (\\url{https://github.com/Oulu-IMEDS/SiNGR/})."],"url":"http://arxiv.org/abs/2405.16813v1","category":"cs.CV"}
{"created":"2024-05-27 04:02:07","title":"Gas-phase hydrogenation of large, astronomically relevant PAH cations","abstract":"To investigate the gas-phase hydrogenation processes of large, astronomically relevant cationic polycyclic aromatic hydrocarbon (PAH) molecules under the interstellar environments, the ion-molecule collision reaction between six PAH cations and H-atoms is studied. The experimental results show that the hydrogenated PAH cations are efficiently formed, and no even-odd hydrogenated mass patterns are observed in the hydrogenation processes. The structure of newly formed hydrogenated PAH cations and the bonding energy for the hydrogenation reaction pathways are investigated with quantum theoretical calculations. The exothermic energy for each reaction pathway is relatively high, and the competition between hydrogenation and dehydrogenation is confirmed. From the theoretical calculation, the bonding ability plays an important role in the gas-phase hydrogenation processes. The factors that affect the hydrogenation chemical reactivity are discussed, including the effect of carbon skeleton structure, the side-edged structure, the molecular size, the five- and six-membered C-ring structure, the bay region structure, and the neighboring hydrogenation. The IR spectra of hydrogenated PAH cations are also calculated. These results we obtain once again validate the complexity of hydrogenated PAH molecules, and provide the direction for the simulations and observations under the coevolution interstellar chemistry network. We infer that if we do not consider other chemical evolution processes (e.g., photo-evolution), then the hydrogenation states and forms of PAH compounds are intricate and complex in the interstellar medium (ISM).","sentences":["To investigate the gas-phase hydrogenation processes of large, astronomically relevant cationic polycyclic aromatic hydrocarbon (PAH) molecules under the interstellar environments, the ion-molecule collision reaction between six PAH cations and H-atoms is studied.","The experimental results show that the hydrogenated PAH cations are efficiently formed, and no even-odd hydrogenated mass patterns are observed in the hydrogenation processes.","The structure of newly formed hydrogenated PAH cations and the bonding energy for the hydrogenation reaction pathways are investigated with quantum theoretical calculations.","The exothermic energy for each reaction pathway is relatively high, and the competition between hydrogenation and dehydrogenation is confirmed.","From the theoretical calculation, the bonding ability plays an important role in the gas-phase hydrogenation processes.","The factors that affect the hydrogenation chemical reactivity are discussed, including the effect of carbon skeleton structure, the side-edged structure, the molecular size, the five- and six-membered C-ring structure, the bay region structure, and the neighboring hydrogenation.","The IR spectra of hydrogenated PAH cations are also calculated.","These results we obtain once again validate the complexity of hydrogenated PAH molecules, and provide the direction for the simulations and observations under the coevolution interstellar chemistry network.","We infer that if we do not consider other chemical evolution processes (e.g., photo-evolution), then the hydrogenation states and forms of PAH compounds are intricate and complex in the interstellar medium (ISM)."],"url":"http://arxiv.org/abs/2405.16811v1","category":"astro-ph.GA"}
{"created":"2024-05-27 03:59:13","title":"Trajectory Data Suffices for Statistically Efficient Learning in Offline RL with Linear $q^\u03c0$-Realizability and Concentrability","abstract":"We consider offline reinforcement learning (RL) in $H$-horizon Markov decision processes (MDPs) under the linear $q^\\pi$-realizability assumption, where the action-value function of every policy is linear with respect to a given $d$-dimensional feature function. The hope in this setting is that learning a good policy will be possible without requiring a sample size that scales with the number of states in the MDP. Foster et al. [2021] have shown this to be impossible even under $\\textit{concentrability}$, a data coverage assumption where a coefficient $C_\\text{conc}$ bounds the extent to which the state-action distribution of any policy can veer off the data distribution. However, the data in this previous work was in the form of a sequence of individual transitions. This leaves open the question of whether the negative result mentioned could be overcome if the data was composed of sequences of full trajectories. In this work we answer this question positively by proving that with trajectory data, a dataset of size $\\text{poly}(d,H,C_\\text{conc})/\\epsilon^2$ is sufficient for deriving an $\\epsilon$-optimal policy, regardless of the size of the state space. The main tool that makes this result possible is due to Weisz et al. [2023], who demonstrate that linear MDPs can be used to approximate linearly $q^\\pi$-realizable MDPs. The connection to trajectory data is that the linear MDP approximation relies on \"skipping\" over certain states. The associated estimation problems are thus easy when working with trajectory data, while they remain nontrivial when working with individual transitions. The question of computational efficiency under our assumptions remains open.","sentences":["We consider offline reinforcement learning (RL) in $H$-horizon Markov decision processes (MDPs) under the linear $q^\\pi$-realizability assumption, where the action-value function of every policy is linear with respect to a given $d$-dimensional feature function.","The hope in this setting is that learning a good policy will be possible without requiring a sample size that scales with the number of states in the MDP.","Foster et al.","[2021] have shown this to be impossible even under $\\textit{concentrability}$, a data coverage assumption where a coefficient $C_\\text{conc}$ bounds the extent to which the state-action distribution of any policy can veer off the data distribution.","However, the data in this previous work was in the form of a sequence of individual transitions.","This leaves open the question of whether the negative result mentioned could be overcome if the data was composed of sequences of full trajectories.","In this work we answer this question positively by proving that with trajectory data, a dataset of size $\\text{poly}(d,H,C_\\text{conc})/\\epsilon^2$ is sufficient for deriving an $\\epsilon$-optimal policy, regardless of the size of the state space.","The main tool that makes this result possible is due to Weisz et al.","[2023], who demonstrate that linear MDPs can be used to approximate linearly $q^\\pi$-realizable MDPs.","The connection to trajectory data is that the linear MDP approximation relies on \"skipping\" over certain states.","The associated estimation problems are thus easy when working with trajectory data, while they remain nontrivial when working with individual transitions.","The question of computational efficiency under our assumptions remains open."],"url":"http://arxiv.org/abs/2405.16809v1","category":"cs.LG"}
{"created":"2024-05-27 03:52:53","title":"Gradient Compressed Sensing: A Query-Efficient Gradient Estimator for High-Dimensional Zeroth-Order Optimization","abstract":"We study nonconvex zeroth-order optimization (ZOO) in a high-dimensional space $\\mathbb R^d$ for functions with approximately $s$-sparse gradients. To reduce the dependence on the dimensionality $d$ in the query complexity, high-dimensional ZOO methods seek to leverage gradient sparsity to design gradient estimators. The previous best method needs $O\\big(s\\log\\frac ds\\big)$ queries per step to achieve $O\\big(\\frac1T\\big)$ rate of convergence w.r.t. the number T of steps. In this paper, we propose *Gradient Compressed Sensing* (GraCe), a query-efficient and accurate estimator for sparse gradients that uses only $O\\big(s\\log\\log\\frac ds\\big)$ queries per step and still achieves $O\\big(\\frac1T\\big)$ rate of convergence. To our best knowledge, we are the first to achieve a *double-logarithmic* dependence on $d$ in the query complexity under weaker assumptions. Our proposed GraCe generalizes the Indyk--Price--Woodruff (IPW) algorithm in compressed sensing from linear measurements to nonlinear functions. Furthermore, since the IPW algorithm is purely theoretical due to its impractically large constant, we improve the IPW algorithm via our *dependent random partition* technique together with our corresponding novel analysis and successfully reduce the constant by a factor of nearly 4300. Our GraCe is not only theoretically query-efficient but also achieves strong empirical performance. We benchmark our GraCe against 12 existing ZOO methods with 10000-dimensional functions and demonstrate that GraCe significantly outperforms existing methods.","sentences":["We study nonconvex zeroth-order optimization (ZOO) in a high-dimensional space $\\mathbb R^d$ for functions with approximately $s$-sparse gradients.","To reduce the dependence on the dimensionality $d$ in the query complexity, high-dimensional ZOO methods seek to leverage gradient sparsity to design gradient estimators.","The previous best method needs $O\\big(s\\log\\frac ds\\big)$ queries per step to achieve $O\\big(\\frac1T\\big)$ rate of convergence w.r.t.","the number T of steps.","In this paper, we propose *Gradient Compressed Sensing* (GraCe), a query-efficient and accurate estimator for sparse gradients that uses only $O\\big(s\\log\\log\\frac ds\\big)$ queries per step and still achieves $O\\big(\\frac1T\\big)$ rate of convergence.","To our best knowledge, we are the first to achieve a *double-logarithmic* dependence on $d$ in the query complexity under weaker assumptions.","Our proposed GraCe generalizes the Indyk--Price--Woodruff (IPW) algorithm in compressed sensing from linear measurements to nonlinear functions.","Furthermore, since the IPW algorithm is purely theoretical due to its impractically large constant, we improve the IPW algorithm via our *dependent random partition* technique together with our corresponding novel analysis and successfully reduce the constant by a factor of nearly 4300.","Our GraCe is not only theoretically query-efficient but also achieves strong empirical performance.","We benchmark our GraCe against 12 existing ZOO methods with 10000-dimensional functions and demonstrate that GraCe significantly outperforms existing methods."],"url":"http://arxiv.org/abs/2405.16805v1","category":"cs.LG"}
{"created":"2024-05-27 03:52:52","title":"Gas-phase formation of fullerene/9-hydroxyfluorene cluster cations","abstract":"In interstellar environment, fullerene species readily react with large molecules (e.g., PAHs and their derivatives) in the gas phase, which may be the formation route of carbon dust grains in space. In this work, the gas-phase ion-molecule collision reaction between fullerene cations (Cn+, n=32, 34, ..., 60) and functionalized PAH molecules (9-hydroxyfluorene, C13H10O) are investigated both experimentally and theoretically. The experimental results show that fullerene/9-hydroxyfluorene cluster cations are efficiently formed, leading to a series of large fullerene/9-hydroxyfluorene cluster cations (e.g., [(C13H10O)C60]+, [(C13H10O)3C58+, and [(C26H18O)(C13H10O)2C48]+). The binding energies and optimized structures of typical fullerene/9-hydroxyfluorene cluster cations were calculated. The bonding ability plays a decisive role in the cluster formation processes. The reaction surfaces, modes and combination reaction sites can result in different binding energies, which represent the relative chemical reactivity. Therefore, the geometry and composition of fullerene/9-hydroxyfluorene cluster cations are complicated. In addition, there is an enhanced chemical reactivity for smaller fullerene cations, which is mainly attributed to the newly formed deformed carbon rings (e.g., 7 C-ring). As part of the coevolution network of interstellar fullerene chemistry, our results suggest that ion-molecule collision reactions contribute to the formation of various fullerene/9-hydroxyfluorene cluster cations in the ISM, providing insights into different chemical reactivity caused by oxygenated functional groups (e.g., hydroxyl, OH, or ether, C-O-C) on the cluster formations.","sentences":["In interstellar environment, fullerene species readily react with large molecules (e.g., PAHs and their derivatives) in the gas phase, which may be the formation route of carbon dust grains in space.","In this work, the gas-phase ion-molecule collision reaction between fullerene cations (Cn+, n=32, 34, ..., 60) and functionalized PAH molecules (9-hydroxyfluorene, C13H10O) are investigated both experimentally and theoretically.","The experimental results show that fullerene/9-hydroxyfluorene cluster cations are efficiently formed, leading to a series of large fullerene/9-hydroxyfluorene cluster cations (e.g., [(C13H10O)C60]+, [(C13H10O)3C58+, and [(C26H18O)(C13H10O)2C48]+).","The binding energies and optimized structures of typical fullerene/9-hydroxyfluorene cluster cations were calculated.","The bonding ability plays a decisive role in the cluster formation processes.","The reaction surfaces, modes and combination reaction sites can result in different binding energies, which represent the relative chemical reactivity.","Therefore, the geometry and composition of fullerene/9-hydroxyfluorene cluster cations are complicated.","In addition, there is an enhanced chemical reactivity for smaller fullerene cations, which is mainly attributed to the newly formed deformed carbon rings (e.g., 7 C-ring).","As part of the coevolution network of interstellar fullerene chemistry, our results suggest that ion-molecule collision reactions contribute to the formation of various fullerene/9-hydroxyfluorene cluster cations in the ISM, providing insights into different chemical reactivity caused by oxygenated functional groups (e.g., hydroxyl, OH, or ether, C-O-C) on the cluster formations."],"url":"http://arxiv.org/abs/2405.16804v1","category":"astro-ph.GA"}
{"created":"2024-05-27 03:24:53","title":"Joint Node Selection and Resource Allocation Optimization for Cooperative Sensing with a Shared Wireless Backhaul","abstract":"In this paper, we consider a cooperative sensing framework in the context of future multi-functional network with both communication and sensing ability, where one base station (BS) serves as a sensing transmitter and several nearby BSs serve as sensing receivers. Each receiver receives the sensing signal reflected by the target and communicates with the fusion center (FC) through a wireless multiple access channel (MAC) for cooperative target localization. To improve the localization performance, we present a hybrid information-signal domain cooperative sensing (HISDCS) design, where each sensing receiver transmits both the estimated time delay/effective reflecting coefficient and the received sensing signal sampled around the estimated time delay to the FC. Then, we propose to minimize the number of channel uses by utilizing an efficient Karhunen-Lo\\'eve transformation (KLT) encoding scheme for signal quantization and proper node selection, under the Cram\\'er-Rao lower bound (CRLB) constraint and the capacity limits of MAC. A novel matrix-inequality constrained successive convex approximation (MCSCA) algorithm is proposed to optimize the wireless backhaul resource allocation, together with a greedy strategy for node selection. Despite the high non-convexness of the considered problem, we prove that the proposed MCSCA algorithm is able to converge to the set of Karush-Kuhn-Tucker (KKT) solutions of a relaxed problem obtained by relaxing the discrete variables. Besides, a low-complexity quantization bit reallocation algorithm is designed, which does not perform explicit node selection, and is able to harvest most of the performance gain brought by HISDCS. Finally, numerical simulations are presented to show that the proposed HISDCS design is able to significantly outperform the baseline schemes.","sentences":["In this paper, we consider a cooperative sensing framework in the context of future multi-functional network with both communication and sensing ability, where one base station (BS) serves as a sensing transmitter and several nearby BSs serve as sensing receivers.","Each receiver receives the sensing signal reflected by the target and communicates with the fusion center (FC) through a wireless multiple access channel (MAC) for cooperative target localization.","To improve the localization performance, we present a hybrid information-signal domain cooperative sensing (HISDCS) design, where each sensing receiver transmits both the estimated time delay/effective reflecting coefficient and the received sensing signal sampled around the estimated time delay to the FC.","Then, we propose to minimize the number of channel uses by utilizing an efficient Karhunen-Lo\\'eve transformation (KLT) encoding scheme for signal quantization and proper node selection, under the Cram\\'er-Rao lower bound (CRLB) constraint and the capacity limits of MAC.","A novel matrix-inequality constrained successive convex approximation (MCSCA) algorithm is proposed to optimize the wireless backhaul resource allocation, together with a greedy strategy for node selection.","Despite the high non-convexness of the considered problem, we prove that the proposed MCSCA algorithm is able to converge to the set of Karush-Kuhn-Tucker (KKT) solutions of a relaxed problem obtained by relaxing the discrete variables.","Besides, a low-complexity quantization bit reallocation algorithm is designed, which does not perform explicit node selection, and is able to harvest most of the performance gain brought by HISDCS.","Finally, numerical simulations are presented to show that the proposed HISDCS design is able to significantly outperform the baseline schemes."],"url":"http://arxiv.org/abs/2405.16791v1","category":"cs.IT"}
{"created":"2024-05-27 03:23:25","title":"3D Reconstruction with Fast Dipole Sums","abstract":"We introduce a technique for the reconstruction of high-fidelity surfaces from multi-view images. Our technique uses a new point-based representation, the dipole sum, which generalizes the winding number to allow for interpolation of arbitrary per-point attributes in point clouds with noisy or outlier points. Using dipole sums allows us to represent implicit geometry and radiance fields as per-point attributes of a point cloud, which we initialize directly from structure from motion. We additionally derive Barnes-Hut fast summation schemes for accelerated forward and reverse-mode dipole sum queries. These queries facilitate the use of ray tracing to efficiently and differentiably render images with our point-based representations, and thus update their point attributes to optimize scene geometry and appearance. We evaluate this inverse rendering framework against state-of-the-art alternatives, based on ray tracing of neural representations or rasterization of Gaussian point-based representations. Our technique significantly improves reconstruction quality at equal runtimes, while also supporting more general rendering techniques such as shadow rays for direct illumination. In the supplement, we provide interactive visualizations of our results.","sentences":["We introduce a technique for the reconstruction of high-fidelity surfaces from multi-view images.","Our technique uses a new point-based representation, the dipole sum, which generalizes the winding number to allow for interpolation of arbitrary per-point attributes in point clouds with noisy or outlier points.","Using dipole sums allows us to represent implicit geometry and radiance fields as per-point attributes of a point cloud, which we initialize directly from structure from motion.","We additionally derive Barnes-Hut fast summation schemes for accelerated forward and reverse-mode dipole sum queries.","These queries facilitate the use of ray tracing to efficiently and differentiably render images with our point-based representations, and thus update their point attributes to optimize scene geometry and appearance.","We evaluate this inverse rendering framework against state-of-the-art alternatives, based on ray tracing of neural representations or rasterization of Gaussian point-based representations.","Our technique significantly improves reconstruction quality at equal runtimes, while also supporting more general rendering techniques such as shadow rays for direct illumination.","In the supplement, we provide interactive visualizations of our results."],"url":"http://arxiv.org/abs/2405.16788v1","category":"cs.CV"}
{"created":"2024-05-27 03:06:09","title":"Emergent topological magnetism in Hund's excitonic insulator","abstract":"Analogous to the charged electron-electron pair condensation in superconductors, an excitonic insulator (EI) represents Fermi surface instability due to spontaneous formation and condensation of charge-neutral electron-hole pair (exciton). Unlike in superconductors, however, the charge-neutral nature of exciton makes probing emergent EI phase via macroscopic physical properties generally difficult. Here, we propose a van der Waals coupled antiferromagnetic semiconductor GdGaI (GGI) as a new material category leading to emergent multi-q magnet intertwined with spontaneous exciton formation/condensation. Before excitonic band hybridization, a simple picture for the parent electronic state consists of electron (Gd-derived 5d) and hole (Ga-derived 4p) delocalized bands, together with Gd-derived 4f localized antiferromagnets with S = 7/2 classical nature. Through intra Gd atom 4f-5d Hund's coupling, a notable finding is the emergent minimum length scale (2a) Skyrmion-like spin texture resulting from spontaneous condensation/formation of spin-polarized exciton with BCS-BEC crossover phenomenology. This discovered platform is promising for realizing valuable quantum matter on the nanoscale; our finding will provide significant insight into designing the atomic scale topological magnetism out of itinerant systems.","sentences":["Analogous to the charged electron-electron pair condensation in superconductors, an excitonic insulator (EI) represents Fermi surface instability due to spontaneous formation and condensation of charge-neutral electron-hole pair (exciton).","Unlike in superconductors, however, the charge-neutral nature of exciton makes probing emergent EI phase via macroscopic physical properties generally difficult.","Here, we propose a van der Waals coupled antiferromagnetic semiconductor GdGaI (GGI) as a new material category leading to emergent multi-q magnet intertwined with spontaneous exciton formation/condensation.","Before excitonic band hybridization, a simple picture for the parent electronic state consists of electron (Gd-derived 5d) and hole (Ga-derived 4p) delocalized bands, together with Gd-derived 4f localized antiferromagnets with S = 7/2 classical nature.","Through intra Gd atom 4f-5d","Hund's coupling, a notable finding is the emergent minimum length scale (2a) Skyrmion-like spin texture resulting from spontaneous condensation/formation of spin-polarized exciton with BCS-BEC crossover phenomenology.","This discovered platform is promising for realizing valuable quantum matter on the nanoscale; our finding will provide significant insight into designing the atomic scale topological magnetism out of itinerant systems."],"url":"http://arxiv.org/abs/2405.16781v1","category":"cond-mat.str-el"}
{"created":"2024-05-27 03:05:29","title":"Analysis of Broken Randomized Experiments by Principal Stratification","abstract":"Although randomized controlled trials have long been regarded as the ``gold standard'' for evaluating treatment effects, there is no natural prevention from post-treatment events. For example, non-compliance makes the actual treatment different from the assigned treatment, truncation-by-death renders the outcome undefined or ill-defined, and missingness prevents the outcomes from being measured. In this paper, we develop a statistical analysis framework using principal stratification to investigate the treatment effect in broken randomized experiments. The average treatment effect in compliers and always-survivors is adopted as the target causal estimand. We establish the asymptotic property for the estimator. We apply the framework to study the effect of training on earnings in the Job Corps Study and find that the training program does not have an effect on employment but possibly have an effect on improving the earnings after employment.","sentences":["Although randomized controlled trials have long been regarded as the ``gold standard'' for evaluating treatment effects, there is no natural prevention from post-treatment events.","For example, non-compliance makes the actual treatment different from the assigned treatment, truncation-by-death renders the outcome undefined or ill-defined, and missingness prevents the outcomes from being measured.","In this paper, we develop a statistical analysis framework using principal stratification to investigate the treatment effect in broken randomized experiments.","The average treatment effect in compliers and always-survivors is adopted as the target causal estimand.","We establish the asymptotic property for the estimator.","We apply the framework to study the effect of training on earnings in the Job Corps Study and find that the training program does not have an effect on employment but possibly have an effect on improving the earnings after employment."],"url":"http://arxiv.org/abs/2405.16780v1","category":"stat.ME"}
{"created":"2024-05-27 03:05:00","title":"Simple obstructions and cone reduction","abstract":"Let $X$ be a Deligne-Mumford stack locally of finite type over an algebraically closed field $k$ of characteristic zero. We show that the intrinsic normal cone $C_X$ of $X$ is supported in the subcone $\\mathbb{V}(\\Omega_X[-1])$ ($h^1/h^0((\\Omega^1_X)^\\vee)$) of its intrinsic normal sheaf $N_X$. This leads to an alternative proof of cone reduction by cosections for $C_X$. We also discuss vanishing of simple obstructions under the Buchweitz-Flenner semiregularity map for sheaves.","sentences":["Let $X$ be a Deligne-Mumford stack locally of finite type over an algebraically closed field $k$ of characteristic zero.","We show that the intrinsic normal cone $C_X$ of $X$ is supported in the subcone $\\mathbb{V}(\\Omega_X[-1])$ ($h^1/h^0((\\Omega^1_X)^\\vee)$) of its intrinsic normal sheaf $N_X$. This leads to an alternative proof of cone reduction by cosections for $C_X$. We also discuss vanishing of simple obstructions under the Buchweitz-Flenner semiregularity map for sheaves."],"url":"http://arxiv.org/abs/2405.16779v1","category":"math.AG"}
{"created":"2024-05-27 02:45:01","title":"Balancing User Preferences by Social Networks: A Condition-Guided Social Recommendation Model for Mitigating Popularity Bias","abstract":"Social recommendation models weave social interactions into their design to provide uniquely personalized recommendation results for users. However, social networks not only amplify the popularity bias in recommendation models, resulting in more frequent recommendation of hot items and fewer long-tail items, but also include a substantial amount of redundant information that is essentially meaningless for the model's performance. Existing social recommendation models fail to address the issues of popularity bias and the redundancy of social information, as they directly characterize social influence across the entire social network without making targeted adjustments. In this paper, we propose a Condition-Guided Social Recommendation Model (named CGSoRec) to mitigate the model's popularity bias by denoising the social network and adjusting the weights of user's social preferences. More specifically, CGSoRec first includes a Condition-Guided Social Denoising Model (CSD) to remove redundant social relations in the social network for capturing users' social preferences with items more precisely. Then, CGSoRec calculates users' social preferences based on denoised social network and adjusts the weights in users' social preferences to make them can counteract the popularity bias present in the recommendation model. At last, CGSoRec includes a Condition-Guided Diffusion Recommendation Model (CGD) to introduce the adjusted social preferences as conditions to control the recommendation results for a debiased direction. Comprehensive experiments on three real-world datasets demonstrate the effectiveness of our proposed method. The code is in: https://github.com/hexin5515/CGSoRec.","sentences":["Social recommendation models weave social interactions into their design to provide uniquely personalized recommendation results for users.","However, social networks not only amplify the popularity bias in recommendation models, resulting in more frequent recommendation of hot items and fewer long-tail items, but also include a substantial amount of redundant information that is essentially meaningless for the model's performance.","Existing social recommendation models fail to address the issues of popularity bias and the redundancy of social information, as they directly characterize social influence across the entire social network without making targeted adjustments.","In this paper, we propose a Condition-Guided Social Recommendation Model (named CGSoRec) to mitigate the model's popularity bias by denoising the social network and adjusting the weights of user's social preferences.","More specifically, CGSoRec first includes a Condition-Guided Social Denoising Model (CSD) to remove redundant social relations in the social network for capturing users' social preferences with items more precisely.","Then, CGSoRec calculates users' social preferences based on denoised social network and adjusts the weights in users' social preferences to make them can counteract the popularity bias present in the recommendation model.","At last, CGSoRec includes a Condition-Guided Diffusion Recommendation Model (CGD) to introduce the adjusted social preferences as conditions to control the recommendation results for a debiased direction.","Comprehensive experiments on three real-world datasets demonstrate the effectiveness of our proposed method.","The code is in: https://github.com/hexin5515/CGSoRec."],"url":"http://arxiv.org/abs/2405.16772v1","category":"cs.SI"}
{"created":"2024-05-27 02:34:51","title":"Learning phase transitions by siamese neural network","abstract":"The wide application of machine learning (ML) techniques in statistics physics has presented new avenues for research in this field. In this paper, we introduce a semi-supervised learning method based on Siamese Neural Networks (SNN), trying to explore the potential of neural network (NN) in the study of critical behaviors beyond the approaches of supervised and unsupervised learning. By focusing on the (1+1) dimensional bond directed percolation (DP) model of nonequilibrium phase transition, we use the SNN to predict the critical values and critical exponents of the system. Different from traditional ML methods, the input of SNN is a set of configuration data pairs and the output prediction is similarity, which prompts to find an anchor point of data for pair comparison during the test. In our study, during test we set different bond probability $p$ as anchors, and discuss the impact of the configurations at this anchors on predictions. More, we use an iterative method to find the optimal training interval to make the algorithm more efficient, and the prediction results are comparable to other ML methods.","sentences":["The wide application of machine learning (ML) techniques in statistics physics has presented new avenues for research in this field.","In this paper, we introduce a semi-supervised learning method based on Siamese Neural Networks (SNN), trying to explore the potential of neural network (NN) in the study of critical behaviors beyond the approaches of supervised and unsupervised learning.","By focusing on the (1+1) dimensional bond directed percolation (DP) model of nonequilibrium phase transition, we use the SNN to predict the critical values and critical exponents of the system.","Different from traditional ML methods, the input of SNN is a set of configuration data pairs and the output prediction is similarity, which prompts to find an anchor point of data for pair comparison during the test.","In our study, during test we set different bond probability $p$ as anchors, and discuss the impact of the configurations at this anchors on predictions.","More, we use an iterative method to find the optimal training interval to make the algorithm more efficient, and the prediction results are comparable to other ML methods."],"url":"http://arxiv.org/abs/2405.16769v1","category":"physics.comp-ph"}
{"created":"2024-05-27 02:26:37","title":"Study of Robust Direction Finding Based on Joint Sparse Representation","abstract":"Standard Direction of Arrival (DOA) estimation methods are typically derived based on the Gaussian noise assumption, making them highly sensitive to outliers. Therefore, in the presence of impulsive noise, the performance of these methods may significantly deteriorate. In this paper, we model impulsive noise as Gaussian noise mixed with sparse outliers. By exploiting their statistical differences, we propose a novel DOA estimation method based on sparse signal recovery (SSR). Furthermore, to address the issue of grid mismatch, we utilize an alternating optimization approach that relies on the estimated outlier matrix and the on-grid DOA estimates to obtain the off-grid DOA estimates. Simulation results demonstrate that the proposed method exhibits robustness against large outliers.","sentences":["Standard Direction of Arrival (DOA) estimation methods are typically derived based on the Gaussian noise assumption, making them highly sensitive to outliers.","Therefore, in the presence of impulsive noise, the performance of these methods may significantly deteriorate.","In this paper, we model impulsive noise as Gaussian noise mixed with sparse outliers.","By exploiting their statistical differences, we propose a novel DOA estimation method based on sparse signal recovery (SSR).","Furthermore, to address the issue of grid mismatch, we utilize an alternating optimization approach that relies on the estimated outlier matrix and the on-grid DOA estimates to obtain the off-grid DOA estimates.","Simulation results demonstrate that the proposed method exhibits robustness against large outliers."],"url":"http://arxiv.org/abs/2405.16765v1","category":"cs.LG"}
{"created":"2024-05-27 02:26:15","title":"One-dimensional SRBM with discontinuous state-dependent drift and variance and its stationary distribution","abstract":"A semi-martingale reflecting Brownian motion is a popular process for diffusion approximations of queueing models including their networks. In this paper, we are concerned with the case that it lives on nonnegative half-line but the variance and drift of its Brownian component discontinuously change depending on its position. This reflecting diffusion process naturally arises from a state-dependent single server queue, studied by the author, and our main interest is in its stationary distribution, which is important for application. However, even the existence of such a reflecting diffusion process seems to be never considered because the discontinuous changes of the variance and drift are exceptional in stochastic analysis for a diffusion process. In this paper, we show that it exists as the weak solution of a stochastic integral equation, and derive its stationary distribution under a stability condition. Our basic tools are the generalized Ito formula for a convex function and local time. Proofs are rather elementary once it is understood how to use the Ito formula and local time.","sentences":["A semi-martingale reflecting Brownian motion is a popular process for diffusion approximations of queueing models including their networks.","In this paper, we are concerned with the case that it lives on nonnegative half-line but the variance and drift of its Brownian component discontinuously change depending on its position.","This reflecting diffusion process naturally arises from a state-dependent single server queue, studied by the author, and our main interest is in its stationary distribution, which is important for application.","However, even the existence of such a reflecting diffusion process seems to be never considered because the discontinuous changes of the variance and drift are exceptional in stochastic analysis for a diffusion process.","In this paper, we show that it exists as the weak solution of a stochastic integral equation, and derive its stationary distribution under a stability condition.","Our basic tools are the generalized Ito formula for a convex function and local time.","Proofs are rather elementary once it is understood how to use the Ito formula and local time."],"url":"http://arxiv.org/abs/2405.16764v1","category":"math.PR"}
{"created":"2024-05-27 02:22:43","title":"Addressing Discretization-Induced Bias in Demographic Prediction","abstract":"Racial and other demographic imputation is necessary for many applications, especially in auditing disparities and outreach targeting in political campaigns. The canonical approach is to construct continuous predictions -- e.g., based on name and geography -- and then to $\\textit{discretize}$ the predictions by selecting the most likely class (argmax). We study how this practice produces $\\textit{discretization bias}$. In particular, we show that argmax labeling, as used by a prominent commercial voter file vendor to impute race/ethnicity, results in a substantial under-count of African-American voters, e.g., by 28.2% points in North Carolina. This bias can have substantial implications in downstream tasks that use such labels.   We then introduce a $\\textit{joint optimization}$ approach -- and a tractable $\\textit{data-driven thresholding}$ heuristic -- that can eliminate this bias, with negligible individual-level accuracy loss. Finally, we theoretically analyze discretization bias, show that calibrated continuous models are insufficient to eliminate it, and that an approach such as ours is necessary. Broadly, we warn researchers and practitioners against discretizing continuous demographic predictions without considering downstream consequences.","sentences":["Racial and other demographic imputation is necessary for many applications, especially in auditing disparities and outreach targeting in political campaigns.","The canonical approach is to construct continuous predictions -- e.g., based on name and geography -- and then to $\\textit{discretize}$ the predictions by selecting the most likely class (argmax).","We study how this practice produces $\\textit{discretization bias}$.","In particular, we show that argmax labeling, as used by a prominent commercial voter file vendor to impute race/ethnicity, results in a substantial under-count of African-American voters, e.g., by 28.2% points in North Carolina.","This bias can have substantial implications in downstream tasks that use such labels.   ","We then introduce a $\\textit{joint optimization}$ approach -- and a tractable $\\textit{data-driven thresholding}$ heuristic -- that can eliminate this bias, with negligible individual-level accuracy loss.","Finally, we theoretically analyze discretization bias, show that calibrated continuous models are insufficient to eliminate it, and that an approach such as ours is necessary.","Broadly, we warn researchers and practitioners against discretizing continuous demographic predictions without considering downstream consequences."],"url":"http://arxiv.org/abs/2405.16762v1","category":"cs.CY"}
{"created":"2024-05-27 01:06:58","title":"CARL: A Framework for Equivariant Image Registration","abstract":"Image registration estimates spatial correspondences between a pair of images. These estimates are typically obtained via numerical optimization or regression by a deep network. A desirable property of such estimators is that a correspondence estimate (e.g., the true oracle correspondence) for an image pair is maintained under deformations of the input images. Formally, the estimator should be equivariant to a desired class of image transformations. In this work, we present careful analyses of the desired equivariance properties in the context of multi-step deep registration networks. Based on these analyses we 1) introduce the notions of $[U,U]$ equivariance (network equivariance to the same deformations of the input images) and $[W,U]$ equivariance (where input images can undergo different deformations); we 2) show that in a suitable multi-step registration setup it is sufficient for overall $[W,U]$ equivariance if the first step has $[W,U]$ equivariance and all others have $[U,U]$ equivariance; we 3) show that common displacement-predicting networks only exhibit $[U,U]$ equivariance to translations instead of the more powerful $[W,U]$ equivariance; and we 4) show how to achieve multi-step $[W,U]$ equivariance via a coordinate-attention mechanism combined with displacement-predicting refinement layers (CARL). Overall, our approach obtains excellent practical registration performance on several 3D medical image registration tasks and outperforms existing unsupervised approaches for the challenging problem of abdomen registration.","sentences":["Image registration estimates spatial correspondences between a pair of images.","These estimates are typically obtained via numerical optimization or regression by a deep network.","A desirable property of such estimators is that a correspondence estimate (e.g., the true oracle correspondence) for an image pair is maintained under deformations of the input images.","Formally, the estimator should be equivariant to a desired class of image transformations.","In this work, we present careful analyses of the desired equivariance properties in the context of multi-step deep registration networks.","Based on these analyses we 1) introduce the notions of $[U,U]$ equivariance (network equivariance to the same deformations of the input images) and $[W,U]$ equivariance (where input images can undergo different deformations); we 2) show that in a suitable multi-step registration setup it is sufficient for overall $[W,U]$ equivariance if the first step has $[W,U]$ equivariance and all others have $[U,U]$ equivariance; we 3) show that common displacement-predicting networks only exhibit $[U,U]$ equivariance to translations instead of the more powerful $[W,U]$ equivariance; and we 4) show how to achieve multi-step $[W,U]$ equivariance via a coordinate-attention mechanism combined with displacement-predicting refinement layers (CARL).","Overall, our approach obtains excellent practical registration performance on several 3D medical image registration tasks and outperforms existing unsupervised approaches for the challenging problem of abdomen registration."],"url":"http://arxiv.org/abs/2405.16738v1","category":"cs.CV"}
{"created":"2024-05-27 01:04:28","title":"A Separation in Heavy-Tailed Sampling: Gaussian vs. Stable Oracles for Proximal Samplers","abstract":"We study the complexity of heavy-tailed sampling and present a separation result in terms of obtaining high-accuracy versus low-accuracy guarantees i.e., samplers that require only $O(\\log(1/\\varepsilon))$ versus $\\Omega(\\text{poly}(1/\\varepsilon))$ iterations to output a sample which is $\\varepsilon$-close to the target in $\\chi^2$-divergence. Our results are presented for proximal samplers that are based on Gaussian versus stable oracles. We show that proximal samplers based on the Gaussian oracle have a fundamental barrier in that they necessarily achieve only low-accuracy guarantees when sampling from a class of heavy-tailed targets. In contrast, proximal samplers based on the stable oracle exhibit high-accuracy guarantees, thereby overcoming the aforementioned limitation. We also prove lower bounds for samplers under the stable oracle and show that our upper bounds cannot be fundamentally improved.","sentences":["We study the complexity of heavy-tailed sampling and present a separation result in terms of obtaining high-accuracy versus low-accuracy guarantees i.e., samplers that require only $O(\\log(1/\\varepsilon))$ versus $\\Omega(\\text{poly}(1/\\varepsilon))$ iterations to output a sample which is $\\varepsilon$-close to the target in $\\chi^2$-divergence.","Our results are presented for proximal samplers that are based on Gaussian versus stable oracles.","We show that proximal samplers based on the Gaussian oracle have a fundamental barrier in that they necessarily achieve only low-accuracy guarantees when sampling from a class of heavy-tailed targets.","In contrast, proximal samplers based on the stable oracle exhibit high-accuracy guarantees, thereby overcoming the aforementioned limitation.","We also prove lower bounds for samplers under the stable oracle and show that our upper bounds cannot be fundamentally improved."],"url":"http://arxiv.org/abs/2405.16736v1","category":"math.ST"}
{"created":"2024-05-27 00:23:42","title":"The Collusion of Memory and Nonlinearity in Stochastic Approximation With Constant Stepsize","abstract":"In this work, we investigate stochastic approximation (SA) with Markovian data and nonlinear updates under constant stepsize $\\alpha>0$. Existing work has primarily focused on either i.i.d. data or linear update rules. We take a new perspective and carefully examine the simultaneous presence of Markovian dependency of data and nonlinear update rules, delineating how the interplay between these two structures leads to complications that are not captured by prior techniques. By leveraging the smoothness and recurrence properties of the SA updates, we develop a fine-grained analysis of the correlation between the SA iterates $\\theta_k$ and Markovian data $x_k$. This enables us to overcome the obstacles in existing analysis and establish for the first time the weak convergence of the joint process $(x_k, \\theta_k)_{k\\geq0}$. Furthermore, we present a precise characterization of the asymptotic bias of the SA iterates, given by $\\mathbb{E}[\\theta_\\infty]-\\theta^\\ast=\\alpha(b_\\text{m}+b_\\text{n}+b_\\text{c})+O(\\alpha^{3/2})$. Here, $b_\\text{m}$ is associated with the Markovian noise, $b_\\text{n}$ is tied to the nonlinearity, and notably, $b_\\text{c}$ represents a multiplicative interaction between the Markovian noise and nonlinearity, which is absent in previous works. As a by-product of our analysis, we derive finite-time bounds on higher moment $\\mathbb{E}[\\|\\theta_k-\\theta^\\ast\\|^{2p}]$ and present non-asymptotic geometric convergence rates for the iterates, along with a Central Limit Theorem.","sentences":["In this work, we investigate stochastic approximation (SA) with Markovian data and nonlinear updates under constant stepsize $\\alpha>0$. Existing work has primarily focused on either i.i.d. data or linear update rules.","We take a new perspective and carefully examine the simultaneous presence of Markovian dependency of data and nonlinear update rules, delineating how the interplay between these two structures leads to complications that are not captured by prior techniques.","By leveraging the smoothness and recurrence properties of the SA updates, we develop a fine-grained analysis of the correlation between the SA iterates $\\theta_k$ and Markovian data $x_k$.","This enables us to overcome the obstacles in existing analysis and establish for the first time the weak convergence of the joint process $(x_k, \\theta_k)_{k\\geq0}$.","Furthermore, we present a precise characterization of the asymptotic bias of the SA iterates, given by $\\mathbb{E}[\\theta_\\infty]-\\theta^\\ast=\\alpha(b_\\text{m}+b_\\text{n}+b_\\text{c})+O(\\alpha^{3/2})$. Here, $b_\\text{m}$ is associated with the Markovian noise, $b_\\text{n}$ is tied to the nonlinearity, and notably, $b_\\text{c}$ represents a multiplicative interaction between the Markovian noise and nonlinearity, which is absent in previous works.","As a by-product of our analysis, we derive finite-time bounds on higher moment $\\mathbb{E}[\\|\\theta_k-\\theta^\\ast\\|^{2p}]$ and present non-asymptotic geometric convergence rates for the iterates, along with a Central Limit Theorem."],"url":"http://arxiv.org/abs/2405.16732v1","category":"stat.ML"}
{"created":"2024-05-27 00:08:36","title":"Free-Space Optical Channel Turbulence Prediction: A Machine Learning Approach","abstract":"Channel turbulence presents a formidable obstacle for free-space optical (FSO) communication. Anticipation of turbulence levels is highly important for mitigating disruptions. We study the application of machine learning (ML) to FSO data streams to rapidly predict channel turbulence levels with no additional sensing hardware. An optical bit stream was transmitted through a controlled channel in the lab under six distinct turbulence levels, and the efficacy of using ML to classify turbulence levels was examined. ML-based turbulence level classification was found to be >98% accurate with multiple ML training parameters, but highly dependent upon the timescale of changes between turbulence levels.","sentences":["Channel turbulence presents a formidable obstacle for free-space optical (FSO) communication.","Anticipation of turbulence levels is highly important for mitigating disruptions.","We study the application of machine learning (ML) to FSO data streams to rapidly predict channel turbulence levels with no additional sensing hardware.","An optical bit stream was transmitted through a controlled channel in the lab under six distinct turbulence levels, and the efficacy of using ML to classify turbulence levels was examined.","ML-based turbulence level classification was found to be >98% accurate with multiple ML training parameters, but highly dependent upon the timescale of changes between turbulence levels."],"url":"http://arxiv.org/abs/2405.16729v1","category":"eess.SY"}
{"created":"2024-05-26 23:52:51","title":"Disentangling and Integrating Relational and Sensory Information in Transformer Architectures","abstract":"The Transformer architecture processes sequences by implementing a form of neural message-passing that consists of iterative information retrieval (attention), followed by local processing (position-wise MLP). Two types of information are essential under this general computational paradigm: \"sensory\" information about individual objects, and \"relational\" information describing the relationships between objects. Standard attention naturally encodes the former, but does not explicitly encode the latter. In this paper, we present an extension of Transformers where multi-head attention is augmented with two distinct types of attention heads, each routing information of a different type. The first type is the standard attention mechanism of Transformers, which captures object-level features, while the second type is a novel attention mechanism we propose to explicitly capture relational information. The two types of attention heads each possess different inductive biases, giving the resulting architecture greater efficiency and versatility. The promise of this approach is demonstrated empirically across a range of tasks.","sentences":["The Transformer architecture processes sequences by implementing a form of neural message-passing that consists of iterative information retrieval (attention), followed by local processing (position-wise MLP).","Two types of information are essential under this general computational paradigm: \"sensory\" information about individual objects, and \"relational\" information describing the relationships between objects.","Standard attention naturally encodes the former, but does not explicitly encode the latter.","In this paper, we present an extension of Transformers where multi-head attention is augmented with two distinct types of attention heads, each routing information of a different type.","The first type is the standard attention mechanism of Transformers, which captures object-level features, while the second type is a novel attention mechanism we propose to explicitly capture relational information.","The two types of attention heads each possess different inductive biases, giving the resulting architecture greater efficiency and versatility.","The promise of this approach is demonstrated empirically across a range of tasks."],"url":"http://arxiv.org/abs/2405.16727v1","category":"cs.LG"}
{"created":"2024-05-26 23:42:31","title":"Observation of in-plane anomalous Hall effect associated with orbital magnetization","abstract":"For over a century, the Hall effect, a transverse effect under out-of-plane magnetic field or magnetization, has been a cornerstone for magnetotransport studies and applications. Modern theoretical formulation based on the Berry curvature has revealed the potential that even in-plane magnetic field can induce anomalous Hall effect, but its experimental demonstration has remained difficult due to its potentially small magnitude and strict symmetry requirements. Here we report observation of the in-plane anomalous Hall effect by measuring low-carrier density films of magnetic Weyl semimetal EuCd$_2$Sb$_2$. Anomalous Hall resistance exhibits distinct three-fold rotational symmetry for changes in the in-plane field component, and this can be understood in terms of out-of-plane Weyl points splitting or orbital magnetization induced by in-plane field, as also confirmed by model calculation. Our findings demonstrate the importance of in-plane field to control the Hall effect, accelerating materials development and further exploration of various in-plane field induced phenomena.","sentences":["For over a century, the Hall effect, a transverse effect under out-of-plane magnetic field or magnetization, has been a cornerstone for magnetotransport studies and applications.","Modern theoretical formulation based on the Berry curvature has revealed the potential that even in-plane magnetic field can induce anomalous Hall effect, but its experimental demonstration has remained difficult due to its potentially small magnitude and strict symmetry requirements.","Here we report observation of the in-plane anomalous Hall effect by measuring low-carrier density films of magnetic Weyl semimetal EuCd$_2$Sb$_2$. Anomalous Hall resistance exhibits distinct three-fold rotational symmetry for changes in the in-plane field component, and this can be understood in terms of out-of-plane Weyl points splitting or orbital magnetization induced by in-plane field, as also confirmed by model calculation.","Our findings demonstrate the importance of in-plane field to control the Hall effect, accelerating materials development and further exploration of various in-plane field induced phenomena."],"url":"http://arxiv.org/abs/2405.16722v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-26 23:27:27","title":"Alistair: Efficient On-device Budgeting for Differentially-Private Ad-Measurement Systems","abstract":"With the impending removal of third-party cookies from major browsers and the introduction of new privacy-preserving advertising APIs, the research community has a timely opportunity to assist industry in qualitatively improving the Web's privacy. This paper discusses our efforts, within a W3C community group, to enhance existing privacy-preserving advertising measurement APIs. We analyze designs from Google, Apple, Meta and Mozilla, and augment them with a more rigorous and efficient differential privacy (DP) budgeting component. Our approach, called Alistair, enforces well-defined DP guarantees and enables advertisers to conduct more private measurement queries accurately. By framing the privacy guarantee in terms of an individual form of DP, we can make DP budgeting more efficient than in current systems that use a traditional DP definition. We incorporate Alistair into Chrome and evaluate it on microbenchmarks and advertising datasets. Across all workloads, Alistair significantly outperforms baselines in enabling more advertising measurements under comparable DP protection.","sentences":["With the impending removal of third-party cookies from major browsers and the introduction of new privacy-preserving advertising APIs, the research community has a timely opportunity to assist industry in qualitatively improving the Web's privacy.","This paper discusses our efforts, within a W3C community group, to enhance existing privacy-preserving advertising measurement APIs.","We analyze designs from Google, Apple, Meta and Mozilla, and augment them with a more rigorous and efficient differential privacy (DP) budgeting component.","Our approach, called Alistair, enforces well-defined DP guarantees and enables advertisers to conduct more private measurement queries accurately.","By framing the privacy guarantee in terms of an individual form of DP, we can make DP budgeting more efficient than in current systems that use a traditional DP definition.","We incorporate Alistair into Chrome and evaluate it on microbenchmarks and advertising datasets.","Across all workloads, Alistair significantly outperforms baselines in enabling more advertising measurements under comparable DP protection."],"url":"http://arxiv.org/abs/2405.16719v1","category":"cs.CR"}
{"created":"2024-05-26 21:40:48","title":"Gravitational higher-form symmetries and the origin of hidden symmetries in Kaluza-Klein compactifications","abstract":"We show that, in presence of isometries and non-trivial topology, the Einstein--Hilbert action is invariant under certain transformations of the metric which are not diffeomorphisms. These transformations are similar to the higher-form symmetries of field theories with $p$-form fields. In the context of toroidal Kaluza--Klein compactifications, we show that these symmetries give rise to some of the ``hidden symmetries'' (dualities) of the dimensionally-reduced theories.","sentences":["We show that, in presence of isometries and non-trivial topology, the Einstein--Hilbert action is invariant under certain transformations of the metric which are not diffeomorphisms.","These transformations are similar to the higher-form symmetries of field theories with $p$-form fields.","In the context of toroidal Kaluza--Klein compactifications, we show that these symmetries give rise to some of the ``hidden symmetries'' (dualities) of the dimensionally-reduced theories."],"url":"http://arxiv.org/abs/2405.16706v1","category":"hep-th"}
{"created":"2024-05-26 20:41:36","title":"Reconstructing the Charlie Parker Omnibook using an audio-to-score automatic transcription pipeline","abstract":"The Charlie Parker Omnibook is a cornerstone of jazz music education, described by pianist Ethan Iverson as \"the most important jazz education text ever published\". In this work we propose a new transcription pipeline and explore the extent to which state of the art music technology is able to reconstruct these scores directly from the audio without human intervention. Our pipeline includes: a newly trained source separation model for saxophone, a new MIDI transcription model for solo saxophone and an adaptation of an existing MIDI-to-score method for monophonic instruments.   To assess this pipeline we also provide an enhanced dataset of Charlie Parker transcriptions as score-audio pairs with accurate MIDI alignments and downbeat annotations. This represents a challenging new benchmark for automatic audio-to-score transcription that we hope will advance research into areas beyond transcribing audio-to-MIDI alone.   Together, these form another step towards producing scores that musicians can use directly, without the need for onerous corrections or revisions. To facilitate future research, all model checkpoints and data are made available to download along with code for the transcription pipeline. Improvements in our modular pipeline could one day make the automatic transcription of complex jazz solos a routine possibility, thereby enriching the resources available for music education and preservation.","sentences":["The Charlie Parker Omnibook is a cornerstone of jazz music education, described by pianist Ethan Iverson as \"the most important jazz education text ever published\".","In this work we propose a new transcription pipeline and explore the extent to which state of the art music technology is able to reconstruct these scores directly from the audio without human intervention.","Our pipeline includes: a newly trained source separation model for saxophone, a new MIDI transcription model for solo saxophone and an adaptation of an existing MIDI-to-score method for monophonic instruments.   ","To assess this pipeline we also provide an enhanced dataset of Charlie Parker transcriptions as score-audio pairs with accurate MIDI alignments and downbeat annotations.","This represents a challenging new benchmark for automatic audio-to-score transcription that we hope will advance research into areas beyond transcribing audio-to-MIDI alone.   ","Together, these form another step towards producing scores that musicians can use directly, without the need for onerous corrections or revisions.","To facilitate future research, all model checkpoints and data are made available to download along with code for the transcription pipeline.","Improvements in our modular pipeline could one day make the automatic transcription of complex jazz solos a routine possibility, thereby enriching the resources available for music education and preservation."],"url":"http://arxiv.org/abs/2405.16687v1","category":"cs.SD"}
{"created":"2024-05-26 20:20:44","title":"A Systematic Review of Federated Generative Models","abstract":"Federated Learning (FL) has emerged as a solution for distributed systems that allow clients to train models on their data and only share models instead of local data. Generative Models are designed to learn the distribution of a dataset and generate new data samples that are similar to the original data. Many prior works have tried proposing Federated Generative Models. Using Federated Learning and Generative Models together can be susceptible to attacks, and designing the optimal architecture remains challenging.   This survey covers the growing interest in the intersection of FL and Generative Models by comprehensively reviewing research conducted from 2019 to 2024. We systematically compare nearly 100 papers, focusing on their FL and Generative Model methods and privacy considerations. To make this field more accessible to newcomers, we highlight the state-of-the-art advancements and identify unresolved challenges, offering insights for future research in this evolving field.","sentences":["Federated Learning (FL) has emerged as a solution for distributed systems that allow clients to train models on their data and only share models instead of local data.","Generative Models are designed to learn the distribution of a dataset and generate new data samples that are similar to the original data.","Many prior works have tried proposing Federated Generative Models.","Using Federated Learning and Generative Models together can be susceptible to attacks, and designing the optimal architecture remains challenging.   ","This survey covers the growing interest in the intersection of FL and Generative Models by comprehensively reviewing research conducted from 2019 to 2024.","We systematically compare nearly 100 papers, focusing on their FL and Generative Model methods and privacy considerations.","To make this field more accessible to newcomers, we highlight the state-of-the-art advancements and identify unresolved challenges, offering insights for future research in this evolving field."],"url":"http://arxiv.org/abs/2405.16682v1","category":"cs.LG"}
{"created":"2024-05-26 19:30:14","title":"Transfer Learning Under High-Dimensional Graph Convolutional Regression Model for Node Classification","abstract":"Node classification is a fundamental task, but obtaining node classification labels can be challenging and expensive in many real-world scenarios. Transfer learning has emerged as a promising solution to address this challenge by leveraging knowledge from source domains to enhance learning in a target domain. Existing transfer learning methods for node classification primarily focus on integrating Graph Convolutional Networks (GCNs) with various transfer learning techniques. While these approaches have shown promising results, they often suffer from a lack of theoretical guarantees, restrictive conditions, and high sensitivity to hyperparameter choices. To overcome these limitations, we propose a Graph Convolutional Multinomial Logistic Regression (GCR) model and a transfer learning method based on the GCR model, called Trans-GCR. We provide theoretical guarantees of the estimate obtained under GCR model in high-dimensional settings. Moreover, Trans-GCR demonstrates superior empirical performance, has a low computational cost, and requires fewer hyperparameters than existing methods.","sentences":["Node classification is a fundamental task, but obtaining node classification labels can be challenging and expensive in many real-world scenarios.","Transfer learning has emerged as a promising solution to address this challenge by leveraging knowledge from source domains to enhance learning in a target domain.","Existing transfer learning methods for node classification primarily focus on integrating Graph Convolutional Networks (GCNs) with various transfer learning techniques.","While these approaches have shown promising results, they often suffer from a lack of theoretical guarantees, restrictive conditions, and high sensitivity to hyperparameter choices.","To overcome these limitations, we propose a Graph Convolutional Multinomial Logistic Regression (GCR) model and a transfer learning method based on the GCR model, called Trans-GCR.","We provide theoretical guarantees of the estimate obtained under GCR model in high-dimensional settings.","Moreover, Trans-GCR demonstrates superior empirical performance, has a low computational cost, and requires fewer hyperparameters than existing methods."],"url":"http://arxiv.org/abs/2405.16672v1","category":"stat.ML"}
{"created":"2024-05-26 19:20:26","title":"Low-resourced Languages and Online Knowledge Repositories: A Need-Finding Study","abstract":"Online Knowledge Repositories (OKRs) like Wikipedia offer communities a way to share and preserve information about themselves and their ways of living. However, for communities with low-resourced languages -- including most African communities -- the quality and volume of content available are often inadequate. One reason for this lack of adequate content could be that many OKRs embody Western ways of knowledge preservation and sharing, requiring many low-resourced language communities to adapt to new interactions. To understand the challenges faced by low-resourced language contributors on the popular OKR Wikipedia, we conducted (1) a thematic analysis of Wikipedia forum discussions and (2) a contextual inquiry study with 14 novice contributors. We focused on three Ethiopian languages: Afan Oromo, Amharic, and Tigrinya. Our analysis revealed several recurring themes; for example, contributors struggle to find resources to corroborate their articles in low-resourced languages, and language technology support, like translation systems and spellcheck, result in several errors that waste contributors' time. We hope our study will support designers in making online knowledge repositories accessible to low-resourced language speakers.","sentences":["Online Knowledge Repositories (OKRs) like Wikipedia offer communities a way to share and preserve information about themselves and their ways of living.","However, for communities with low-resourced languages -- including most African communities -- the quality and volume of content available are often inadequate.","One reason for this lack of adequate content could be that many OKRs embody Western ways of knowledge preservation and sharing, requiring many low-resourced language communities to adapt to new interactions.","To understand the challenges faced by low-resourced language contributors on the popular OKR Wikipedia, we conducted (1) a thematic analysis of Wikipedia forum discussions and (2) a contextual inquiry study with 14 novice contributors.","We focused on three Ethiopian languages: Afan Oromo, Amharic, and Tigrinya.","Our analysis revealed several recurring themes; for example, contributors struggle to find resources to corroborate their articles in low-resourced languages, and language technology support, like translation systems and spellcheck, result in several errors that waste contributors' time.","We hope our study will support designers in making online knowledge repositories accessible to low-resourced language speakers."],"url":"http://arxiv.org/abs/2405.16669v1","category":"cs.HC"}
{"created":"2024-05-27 16:22:38","title":"Tracking Small Birds by Detection Candidate Region Filtering and Detection History-aware Association","abstract":"This paper focuses on tracking birds that appear small in a panoramic video. When the size of the tracked object is small in the image (small object tracking) and move quickly, object detection and association suffers. To address these problems, we propose Adaptive Slicing Aided Hyper Inference (Adaptive SAHI), which reduces the candidate regions to apply detection, and Detection History-aware Similarity Criterion (DHSC), which accurately associates objects in consecutive frames based on the detection history. Experiments on the NUBird2022 dataset verifies the effectiveness of the proposed method by showing improvements in both accuracy and speed.","sentences":["This paper focuses on tracking birds that appear small in a panoramic video.","When the size of the tracked object is small in the image (small object tracking) and move quickly, object detection and association suffers.","To address these problems, we propose Adaptive Slicing Aided Hyper Inference (Adaptive SAHI), which reduces the candidate regions to apply detection, and Detection History-aware Similarity Criterion (DHSC), which accurately associates objects in consecutive frames based on the detection history.","Experiments on the NUBird2022 dataset verifies the effectiveness of the proposed method by showing improvements in both accuracy and speed."],"url":"http://arxiv.org/abs/2405.17323v1","category":"cs.CV"}
{"created":"2024-05-27 15:52:53","title":"On The Implicit Large Eddy Simulation of Turbomachinery Flows Using The Flux Reconstruction Method","abstract":"A high order flux reconstruction solver has been developed and validated to perform implicit large eddy simulations of industrially representative turbomachinery flows. The T106c low-pressure turbine and VKI LS89 high-pressure turbine cases are studied. The solver uses the Rusanov Riemann solver to compute the inviscid fluxes on the wall boundaries, and HLLC or Roe to evaluate inviscid fluxes for internal faces. The impact of Riemann solvers is demonstrated in terms of accuracy and non-linear stability for turbomachinery flows. It is found that HLLC is more robust than Roe, but both Riemann solvers produce very similar results if stable solutions can be obtained. For non-linear stabilization, a local modal filter, which combines a smooth indicator and a modal filter, is used to stabilize the solution. This approach requires a tuning parameter for the smoothness criterion. Detailed analysis has been provided to guide the selection of a suitable value for different spatial orders of accuracy. This local-modal filter is also compared with the recent positivity-preserving entropy filter in terms of accuracy and stability for the LS89 turbine case. The entropy filter could stabilize the computation but is more dissipative than the local modal filter. Regarding the spanwise spacing of the grid, the case of the LS89 turbine shows that a $z^+$ of approximately $45 - 60$ is suitable for obtaining a satisfactory prediction of the heat transfer coefficient of the mean flow. This would allow for a coarse grid spacing in the spanwise direction and a cost-effective ILES aerothermal simulation for turbomachinery flows.","sentences":["A high order flux reconstruction solver has been developed and validated to perform implicit large eddy simulations of industrially representative turbomachinery flows.","The T106c low-pressure turbine and VKI LS89 high-pressure turbine cases are studied.","The solver uses the Rusanov Riemann solver to compute the inviscid fluxes on the wall boundaries, and HLLC or Roe to evaluate inviscid fluxes for internal faces.","The impact of Riemann solvers is demonstrated in terms of accuracy and non-linear stability for turbomachinery flows.","It is found that HLLC is more robust than Roe, but both Riemann solvers produce very similar results if stable solutions can be obtained.","For non-linear stabilization, a local modal filter, which combines a smooth indicator and a modal filter, is used to stabilize the solution.","This approach requires a tuning parameter for the smoothness criterion.","Detailed analysis has been provided to guide the selection of a suitable value for different spatial orders of accuracy.","This local-modal filter is also compared with the recent positivity-preserving entropy filter in terms of accuracy and stability for the LS89 turbine case.","The entropy filter could stabilize the computation but is more dissipative than the local modal filter.","Regarding the spanwise spacing of the grid, the case of the LS89 turbine shows that a $z^+$ of approximately $45 - 60$ is suitable for obtaining a satisfactory prediction of the heat transfer coefficient of the mean flow.","This would allow for a coarse grid spacing in the spanwise direction and a cost-effective ILES aerothermal simulation for turbomachinery flows."],"url":"http://arxiv.org/abs/2405.17288v1","category":"physics.flu-dyn"}
{"created":"2024-05-27 14:35:03","title":"Modelling between- and within-season trajectories in elite athletic performance data","abstract":"Athletic performance follows a typical pattern of improvement and decline during a career. This pattern is also often observed within-seasons as athlete aims for their performance to peak at key events such as the Olympic Games or World Championships. A Bayesian hierarchical model is developed to analyse the evolution of athletic sporting performance throughout an athlete's career and separate these effects whilst allowing for confounding factors such as environmental conditions. Our model works in continuous time and estimates both the average performance level of the population, $g(t)$, at age $t$ and how each $i$-th athlete differs from the average $f_i(t)$. We further decompose $f_i(t)$ into changes from season-to-season, termed the between-season performance trajectory, and within-season performance trajectories which are modelled by a constrained Bernstein polynomial. Hence, the specific focus of this project is to identify the differences in performance that exist both between and within-seasons for each athlete. For the implementation of the model an adaptive Metropolis-within-Gibbs algorithm is used. An illustration of algorithm's performance on 100 metres and 200 metres freestyle swimming in both female and male athletes is presented.","sentences":["Athletic performance follows a typical pattern of improvement and decline during a career.","This pattern is also often observed within-seasons as athlete aims for their performance to peak at key events such as the Olympic Games or World Championships.","A Bayesian hierarchical model is developed to analyse the evolution of athletic sporting performance throughout an athlete's career and separate these effects whilst allowing for confounding factors such as environmental conditions.","Our model works in continuous time and estimates both the average performance level of the population, $g(t)$, at age $t$ and how each $i$-th athlete differs from the average $f_i(t)$. We further decompose $f_i(t)$ into changes from season-to-season, termed the between-season performance trajectory, and within-season performance trajectories which are modelled by a constrained Bernstein polynomial.","Hence, the specific focus of this project is to identify the differences in performance that exist both between and within-seasons for each athlete.","For the implementation of the model an adaptive Metropolis-within-Gibbs algorithm is used.","An illustration of algorithm's performance on 100 metres and 200 metres freestyle swimming in both female and male athletes is presented."],"url":"http://arxiv.org/abs/2405.17214v1","category":"stat.AP"}
{"created":"2024-05-27 14:17:36","title":"Anisotropic Gauss Reconstruction for Unoriented Point Clouds","abstract":"Unoriented surface reconstructions based on the Gauss formula have attracted much attention due to their elegant mathematical formulation and excellent performance. However, the isotropic characteristics of the formulation limit their capacity to leverage the anisotropic information within the point cloud. In this work, we propose a novel anisotropic formulation by introducing a convection term in the original Laplace operator. By choosing different velocity vectors, the anisotropic feature can be exploited to construct more effective linear equations. Moreover, an adaptive selection strategy is introduced for the velocity vector to further enhance the orientation and reconstruction performance of thin structures. Extensive experiments demonstrate that our method achieves state-of-the-art performance and manages various challenging situations, especially for models with thin structures or small holes. The source code will be released on GitHub.","sentences":["Unoriented surface reconstructions based on the Gauss formula have attracted much attention due to their elegant mathematical formulation and excellent performance.","However, the isotropic characteristics of the formulation limit their capacity to leverage the anisotropic information within the point cloud.","In this work, we propose a novel anisotropic formulation by introducing a convection term in the original Laplace operator.","By choosing different velocity vectors, the anisotropic feature can be exploited to construct more effective linear equations.","Moreover, an adaptive selection strategy is introduced for the velocity vector to further enhance the orientation and reconstruction performance of thin structures.","Extensive experiments demonstrate that our method achieves state-of-the-art performance and manages various challenging situations, especially for models with thin structures or small holes.","The source code will be released on GitHub."],"url":"http://arxiv.org/abs/2405.17193v1","category":"cs.GR"}
{"created":"2024-05-27 11:37:20","title":"Model-Driven Engineering for Quantum Programming: A Case Study on Ground State Energy Calculation","abstract":"This study introduces a novel framework that brings together two main Quantum Programming methodologies, gate-based Quantum Computing and Quantum Annealing, by applying the Model-Driven Engineering principles. This aims to enhance the adaptability, design and scalability of quantum programs, facilitating their design and operation across diverse computing platforms. A notable achievement of this research is the development of a mapping method for programs between gate-based quantum computers and quantum annealers which can lead to the automatic transformation of these programs. Specifically, this method is applied to the Variational Quantum Eigensolver Algorithm and Quantum Anneling Ising Model, targeting ground state solutions. Finding ground-state solutions is crucial for a wide range of scientific applications, ranging from simulating chemistry lab experiments to medical applications, such as vaccine development. The success of this application demonstrates Model-Driven Engineering for Quantum Programming frameworks's practical viability and sets a clear path for quantum Computing's broader use in solving intricate problems.","sentences":["This study introduces a novel framework that brings together two main Quantum Programming methodologies, gate-based Quantum Computing and Quantum Annealing, by applying the Model-Driven Engineering principles.","This aims to enhance the adaptability, design and scalability of quantum programs, facilitating their design and operation across diverse computing platforms.","A notable achievement of this research is the development of a mapping method for programs between gate-based quantum computers and quantum annealers which can lead to the automatic transformation of these programs.","Specifically, this method is applied to the Variational Quantum Eigensolver Algorithm and Quantum Anneling Ising Model, targeting ground state solutions.","Finding ground-state solutions is crucial for a wide range of scientific applications, ranging from simulating chemistry lab experiments to medical applications, such as vaccine development.","The success of this application demonstrates Model-Driven Engineering for Quantum Programming frameworks's practical viability and sets a clear path for quantum Computing's broader use in solving intricate problems."],"url":"http://arxiv.org/abs/2405.17065v1","category":"quant-ph"}
{"created":"2024-05-27 09:47:09","title":"Graph Condensation for Open-World Graph Learning","abstract":"The burgeoning volume of graph data presents significant computational challenges in training graph neural networks (GNNs), critically impeding their efficiency in various applications. To tackle this challenge, graph condensation (GC) has emerged as a promising acceleration solution, focusing on the synthesis of a compact yet representative graph for efficiently training GNNs while retaining performance. Despite the potential to promote scalable use of GNNs, existing GC methods are limited to aligning the condensed graph with merely the observed static graph distribution. This limitation significantly restricts the generalization capacity of condensed graphs, particularly in adapting to dynamic distribution changes. In real-world scenarios, however, graphs are dynamic and constantly evolving, with new nodes and edges being continually integrated. Consequently, due to the limited generalization capacity of condensed graphs, applications that employ GC for efficient GNN training end up with sub-optimal GNNs when confronted with evolving graph structures and distributions in dynamic real-world situations. To overcome this issue, we propose open-world graph condensation (OpenGC), a robust GC framework that integrates structure-aware distribution shift to simulate evolving graph patterns and exploit the temporal environments for invariance condensation. This approach is designed to extract temporal invariant patterns from the original graph, thereby enhancing the generalization capabilities of the condensed graph and, subsequently, the GNNs trained on it. Extensive experiments on both real-world and synthetic evolving graphs demonstrate that OpenGC outperforms state-of-the-art (SOTA) GC methods in adapting to dynamic changes in open-world graph environments.","sentences":["The burgeoning volume of graph data presents significant computational challenges in training graph neural networks (GNNs), critically impeding their efficiency in various applications.","To tackle this challenge, graph condensation (GC) has emerged as a promising acceleration solution, focusing on the synthesis of a compact yet representative graph for efficiently training GNNs while retaining performance.","Despite the potential to promote scalable use of GNNs, existing GC methods are limited to aligning the condensed graph with merely the observed static graph distribution.","This limitation significantly restricts the generalization capacity of condensed graphs, particularly in adapting to dynamic distribution changes.","In real-world scenarios, however, graphs are dynamic and constantly evolving, with new nodes and edges being continually integrated.","Consequently, due to the limited generalization capacity of condensed graphs, applications that employ GC for efficient GNN training end up with sub-optimal GNNs when confronted with evolving graph structures and distributions in dynamic real-world situations.","To overcome this issue, we propose open-world graph condensation (OpenGC), a robust GC framework that integrates structure-aware distribution shift to simulate evolving graph patterns and exploit the temporal environments for invariance condensation.","This approach is designed to extract temporal invariant patterns from the original graph, thereby enhancing the generalization capabilities of the condensed graph and, subsequently, the GNNs trained on it.","Extensive experiments on both real-world and synthetic evolving graphs demonstrate that OpenGC outperforms state-of-the-art (SOTA) GC methods in adapting to dynamic changes in open-world graph environments."],"url":"http://arxiv.org/abs/2405.17003v1","category":"cs.LG"}
{"created":"2024-05-27 08:52:56","title":"Jan Veth's paintings of Jacobus Kapteyn","abstract":"Jacobus C. Kapteyn is regarded as one of the coryfees of the University of Groningen. Part of his legacy is two paintings of him by Dutch painter Jan Pieter Veth. One, showing him at his desk, decorates the Kapteyn Room in the Kapteyn Astronomical Institute, and the other one, displaying him in academic attire, is in the University's gallery of professors in the central Academy Building. The first was offered to the Kapteyns on the occasion of his 40-th anniversary as professor in 1918 and the second to the University after his retirement in 1921.   It has been suggested that there must have been a third portrait that now is lost. Former director Adriaan Blaauw has proposed that the one in the Academy Building actually was first offered in 1918, but at Mrs. Kapteyn's request replaced by the one now in the Kapteyn Room. The first version was then later adapted to the requirements of the gallery of professors by Veth himself by overpainting it with academic attire. A preliminary trial version by Veth, in the possession of Kapteyn's greatgrandson, shows what it would have looked like before the adaption.   The following reports on new evidence: the biography of Jan Veth that historian Johan Huizinga, friend of Veth, wrote, and letters Veth wrote to his wife while he was working on these paintings. This provides strong support of Blaauw's sequence of events with a few modifications. No third painting has ever been produced.","sentences":["Jacobus C. Kapteyn is regarded as one of the coryfees of the University of Groningen.","Part of his legacy is two paintings of him by Dutch painter Jan Pieter Veth.","One, showing him at his desk, decorates the Kapteyn Room in the Kapteyn Astronomical Institute, and the other one, displaying him in academic attire, is in the University's gallery of professors in the central Academy Building.","The first was offered to the Kapteyns on the occasion of his 40-th anniversary as professor in 1918 and the second to the University after his retirement in 1921.   ","It has been suggested that there must have been a third portrait that now is lost.","Former director Adriaan Blaauw has proposed that the one in the Academy Building actually was first offered in 1918, but at Mrs. Kapteyn's request replaced by the one now in the Kapteyn Room.","The first version was then later adapted to the requirements of the gallery of professors by Veth himself by overpainting it with academic attire.","A preliminary trial version by Veth, in the possession of Kapteyn's greatgrandson, shows what it would have looked like before the adaption.   ","The following reports on new evidence: the biography of Jan Veth that historian Johan Huizinga, friend of Veth, wrote, and letters Veth wrote to his wife while he was working on these paintings.","This provides strong support of Blaauw's sequence of events with a few modifications.","No third painting has ever been produced."],"url":"http://arxiv.org/abs/2405.16957v1","category":"physics.hist-ph"}
{"created":"2024-05-27 08:39:38","title":"Zero-Shot Video Semantic Segmentation based on Pre-Trained Diffusion Models","abstract":"We introduce the first zero-shot approach for Video Semantic Segmentation (VSS) based on pre-trained diffusion models. A growing research direction attempts to employ diffusion models to perform downstream vision tasks by exploiting their deep understanding of image semantics. Yet, the majority of these approaches have focused on image-related tasks like semantic correspondence and segmentation, with less emphasis on video tasks such as VSS. Ideally, diffusion-based image semantic segmentation approaches can be applied to videos in a frame-by-frame manner. However, we find their performance on videos to be subpar due to the absence of any modeling of temporal information inherent in the video data. To this end, we tackle this problem and introduce a framework tailored for VSS based on pre-trained image and video diffusion models. We propose building a scene context model based on the diffusion features, where the model is autoregressively updated to adapt to scene changes. This context model predicts per-frame coarse segmentation maps that are temporally consistent. To refine these maps further, we propose a correspondence-based refinement strategy that aggregates predictions temporally, resulting in more confident predictions. Finally, we introduce a masked modulation approach to upsample the coarse maps to the full resolution at a high quality. Experiments show that our proposed approach outperforms existing zero-shot image semantic segmentation approaches significantly on various VSS benchmarks without any training or fine-tuning. Moreover, it rivals supervised VSS approaches on the VSPW dataset despite not being explicitly trained for VSS.","sentences":["We introduce the first zero-shot approach for Video Semantic Segmentation (VSS) based on pre-trained diffusion models.","A growing research direction attempts to employ diffusion models to perform downstream vision tasks by exploiting their deep understanding of image semantics.","Yet, the majority of these approaches have focused on image-related tasks like semantic correspondence and segmentation, with less emphasis on video tasks such as VSS.","Ideally, diffusion-based image semantic segmentation approaches can be applied to videos in a frame-by-frame manner.","However, we find their performance on videos to be subpar due to the absence of any modeling of temporal information inherent in the video data.","To this end, we tackle this problem and introduce a framework tailored for VSS based on pre-trained image and video diffusion models.","We propose building a scene context model based on the diffusion features, where the model is autoregressively updated to adapt to scene changes.","This context model predicts per-frame coarse segmentation maps that are temporally consistent.","To refine these maps further, we propose a correspondence-based refinement strategy that aggregates predictions temporally, resulting in more confident predictions.","Finally, we introduce a masked modulation approach to upsample the coarse maps to the full resolution at a high quality.","Experiments show that our proposed approach outperforms existing zero-shot image semantic segmentation approaches significantly on various VSS benchmarks without any training or fine-tuning.","Moreover, it rivals supervised VSS approaches on the VSPW dataset despite not being explicitly trained for VSS."],"url":"http://arxiv.org/abs/2405.16947v1","category":"cs.CV"}
{"created":"2024-05-27 06:47:14","title":"CoCoGesture: Toward Coherent Co-speech 3D Gesture Generation in the Wild","abstract":"Deriving co-speech 3D gestures has seen tremendous progress in virtual avatar animation. Yet, the existing methods often produce stiff and unreasonable gestures with unseen human speech inputs due to the limited 3D speech-gesture data. In this paper, we propose CoCoGesture, a novel framework enabling vivid and diverse gesture synthesis from unseen human speech prompts. Our key insight is built upon the custom-designed pretrain-fintune training paradigm. At the pretraining stage, we aim to formulate a large generalizable gesture diffusion model by learning the abundant postures manifold. Therefore, to alleviate the scarcity of 3D data, we first construct a large-scale co-speech 3D gesture dataset containing more than 40M meshed posture instances across 4.3K speakers, dubbed GES-X. Then, we scale up the large unconditional diffusion model to 1B parameters and pre-train it to be our gesture experts. At the finetune stage, we present the audio ControlNet that incorporates the human voice as condition prompts to guide the gesture generation. Here, we construct the audio ControlNet through a trainable copy of our pre-trained diffusion model. Moreover, we design a novel Mixture-of-Gesture-Experts (MoGE) block to adaptively fuse the audio embedding from the human speech and the gesture features from the pre-trained gesture experts with a routing mechanism. Such an effective manner ensures audio embedding is temporal coordinated with motion features while preserving the vivid and diverse gesture generation. Extensive experiments demonstrate that our proposed CoCoGesture outperforms the state-of-the-art methods on the zero-shot speech-to-gesture generation. The dataset will be publicly available at: https://mattie-e.github.io/GES-X/","sentences":["Deriving co-speech 3D gestures has seen tremendous progress in virtual avatar animation.","Yet, the existing methods often produce stiff and unreasonable gestures with unseen human speech inputs due to the limited 3D speech-gesture data.","In this paper, we propose CoCoGesture, a novel framework enabling vivid and diverse gesture synthesis from unseen human speech prompts.","Our key insight is built upon the custom-designed pretrain-fintune training paradigm.","At the pretraining stage, we aim to formulate a large generalizable gesture diffusion model by learning the abundant postures manifold.","Therefore, to alleviate the scarcity of 3D data, we first construct a large-scale co-speech 3D gesture dataset containing more than 40M meshed posture instances across 4.3K speakers, dubbed GES-X.","Then, we scale up the large unconditional diffusion model to 1B parameters and pre-train it to be our gesture experts.","At the finetune stage, we present the audio ControlNet that incorporates the human voice as condition prompts to guide the gesture generation.","Here, we construct the audio ControlNet through a trainable copy of our pre-trained diffusion model.","Moreover, we design a novel Mixture-of-Gesture-Experts (MoGE) block to adaptively fuse the audio embedding from the human speech and the gesture features from the pre-trained gesture experts with a routing mechanism.","Such an effective manner ensures audio embedding is temporal coordinated with motion features while preserving the vivid and diverse gesture generation.","Extensive experiments demonstrate that our proposed CoCoGesture outperforms the state-of-the-art methods on the zero-shot speech-to-gesture generation.","The dataset will be publicly available at: https://mattie-e.github.io/GES-X/"],"url":"http://arxiv.org/abs/2405.16874v1","category":"cs.CV"}
{"created":"2024-05-27 06:32:36","title":"Hierarchical Rank-One Sequence Convexification for the Relaxation of Variational Problems with Microstructures","abstract":"This paper presents an efficient algorithm for the approximation of the rank-one convex hull in the context of nonlinear solid mechanics. It is based on hierarchical rank-one sequences and simultaneously provides first and second derivative information essential for the calculation of mechanical stresses and the computational minimization of discretized energies. For materials, whose microstructure can be well approximated in terms of laminates and where each laminate stage achieves energetic optimality with respect to the current stage, the approximate envelope coincides with the rank-one convex envelope. Although the proposed method provides only an upper bound for the rank-one convex hull, a careful examination of the resulting constraints shows a decent applicability in mechanical problems. Various aspects of the algorithm are discussed, including the restoration of rotational invariance, microstructure reconstruction, comparisons with other semi-convexification algorithms, and mesh independency. Overall, this paper demonstrates the efficiency of the algorithm for both, well-established mathematical benchmark problems as well as nonconvex isotropic finite-strain continuum damage models in two and three dimensions. Thereby, for the first time, a feasible concurrent numerical relaxation is established for an incremental, dissipative large-strain model with relevant applications in engineering problems.","sentences":["This paper presents an efficient algorithm for the approximation of the rank-one convex hull in the context of nonlinear solid mechanics.","It is based on hierarchical rank-one sequences and simultaneously provides first and second derivative information essential for the calculation of mechanical stresses and the computational minimization of discretized energies.","For materials, whose microstructure can be well approximated in terms of laminates and where each laminate stage achieves energetic optimality with respect to the current stage, the approximate envelope coincides with the rank-one convex envelope.","Although the proposed method provides only an upper bound for the rank-one convex hull, a careful examination of the resulting constraints shows a decent applicability in mechanical problems.","Various aspects of the algorithm are discussed, including the restoration of rotational invariance, microstructure reconstruction, comparisons with other semi-convexification algorithms, and mesh independency.","Overall, this paper demonstrates the efficiency of the algorithm for both, well-established mathematical benchmark problems as well as nonconvex isotropic finite-strain continuum damage models in two and three dimensions.","Thereby, for the first time, a feasible concurrent numerical relaxation is established for an incremental, dissipative large-strain model with relevant applications in engineering problems."],"url":"http://arxiv.org/abs/2405.16866v1","category":"cs.CE"}
{"created":"2024-05-27 04:33:53","title":"Automatic Domain Adaptation by Transformers in In-Context Learning","abstract":"Selecting or designing an appropriate domain adaptation algorithm for a given problem remains challenging. This paper presents a Transformer model that can provably approximate and opt for domain adaptation methods for a given dataset in the in-context learning framework, where a foundation model performs new tasks without updating its parameters at test time. Specifically, we prove that Transformers can approximate instance-based and feature-based unsupervised domain adaptation algorithms and automatically select an algorithm suited for a given dataset. Numerical results indicate that in-context learning demonstrates an adaptive domain adaptation surpassing existing methods.","sentences":["Selecting or designing an appropriate domain adaptation algorithm for a given problem remains challenging.","This paper presents a Transformer model that can provably approximate and opt for domain adaptation methods for a given dataset in the in-context learning framework, where a foundation model performs new tasks without updating its parameters at test time.","Specifically, we prove that Transformers can approximate instance-based and feature-based unsupervised domain adaptation algorithms and automatically select an algorithm suited for a given dataset.","Numerical results indicate that in-context learning demonstrates an adaptive domain adaptation surpassing existing methods."],"url":"http://arxiv.org/abs/2405.16819v1","category":"cs.LG"}
{"created":"2024-05-27 03:29:06","title":"Physics-informed Inverse Design of Multi-bit Programmable Metasurfaces","abstract":"Emerging reconfigurable metasurfaces offer various possibilities in programmatically manipulating electromagnetic waves across spatial, spectral, and temporal domains, showcasing great potential for enhancing terahertz applications. However, they are hindered by limited tunability, particularly evident in relatively small phase tuning over 270o, due to the design constraints with time-intensive forward design methodologies. Here, we demonstrate a multi-bit programmable metasurface capable of terahertz beam steering, facilitated by a developed physics-informed inverse design (PIID) approach. Through integrating a modified coupled mode theory (MCMT) into residual neural networks, our PIID algorithm not only significantly increases the design accuracy compared to conventional neural networks but also elucidates the intricate physical relations between the geometry and the modes. Without decreasing the reflection intensity, our method achieves the enhanced phase tuning as large as 300o. Additionally, we experimentally validate the inverse designed programmable beam steering metasurface, which is adaptable across 1-bit, 2-bit, and tri-state coding schemes, yielding a deflection angle up to 68o and broadened steering coverage. Our demonstration provides a promising pathway for rapidly exploring advanced metasurface devices, with potentially great impact on communication and imaging technologies.","sentences":["Emerging reconfigurable metasurfaces offer various possibilities in programmatically manipulating electromagnetic waves across spatial, spectral, and temporal domains, showcasing great potential for enhancing terahertz applications.","However, they are hindered by limited tunability, particularly evident in relatively small phase tuning over 270o, due to the design constraints with time-intensive forward design methodologies.","Here, we demonstrate a multi-bit programmable metasurface capable of terahertz beam steering, facilitated by a developed physics-informed inverse design (PIID) approach.","Through integrating a modified coupled mode theory (MCMT) into residual neural networks, our PIID algorithm not only significantly increases the design accuracy compared to conventional neural networks but also elucidates the intricate physical relations between the geometry and the modes.","Without decreasing the reflection intensity, our method achieves the enhanced phase tuning as large as 300o.","Additionally, we experimentally validate the inverse designed programmable beam steering metasurface, which is adaptable across 1-bit, 2-bit, and tri-state coding schemes, yielding a deflection angle up to 68o and broadened steering coverage.","Our demonstration provides a promising pathway for rapidly exploring advanced metasurface devices, with potentially great impact on communication and imaging technologies."],"url":"http://arxiv.org/abs/2405.16795v1","category":"physics.optics"}
{"created":"2024-05-27 03:13:28","title":"PromptFix: You Prompt and We Fix the Photo","abstract":"Diffusion models equipped with language models demonstrate excellent controllability in image generation tasks, allowing image processing to adhere to human instructions. However, the lack of diverse instruction-following data hampers the development of models that effectively recognize and execute user-customized instructions, particularly in low-level tasks. Moreover, the stochastic nature of the diffusion process leads to deficiencies in image generation or editing tasks that require the detailed preservation of the generated images. To address these limitations, we propose PromptFix, a comprehensive framework that enables diffusion models to follow human instructions to perform a wide variety of image-processing tasks. First, we construct a large-scale instruction-following dataset that covers comprehensive image-processing tasks, including low-level tasks, image editing, and object creation. Next, we propose a high-frequency guidance sampling method to explicitly control the denoising process and preserve high-frequency details in unprocessed areas. Finally, we design an auxiliary prompting adapter, utilizing Vision-Language Models (VLMs) to enhance text prompts and improve the model's task generalization. Experimental results show that PromptFix outperforms previous methods in various image-processing tasks. Our proposed model also achieves comparable inference efficiency with these baseline models and exhibits superior zero-shot capabilities in blind restoration and combination tasks. The dataset and code will be aviliable at https://github.com/yeates/PromptFix.","sentences":["Diffusion models equipped with language models demonstrate excellent controllability in image generation tasks, allowing image processing to adhere to human instructions.","However, the lack of diverse instruction-following data hampers the development of models that effectively recognize and execute user-customized instructions, particularly in low-level tasks.","Moreover, the stochastic nature of the diffusion process leads to deficiencies in image generation or editing tasks that require the detailed preservation of the generated images.","To address these limitations, we propose PromptFix, a comprehensive framework that enables diffusion models to follow human instructions to perform a wide variety of image-processing tasks.","First, we construct a large-scale instruction-following dataset that covers comprehensive image-processing tasks, including low-level tasks, image editing, and object creation.","Next, we propose a high-frequency guidance sampling method to explicitly control the denoising process and preserve high-frequency details in unprocessed areas.","Finally, we design an auxiliary prompting adapter, utilizing Vision-Language Models (VLMs) to enhance text prompts and improve the model's task generalization.","Experimental results show that PromptFix outperforms previous methods in various image-processing tasks.","Our proposed model also achieves comparable inference efficiency with these baseline models and exhibits superior zero-shot capabilities in blind restoration and combination tasks.","The dataset and code will be aviliable at https://github.com/yeates/PromptFix."],"url":"http://arxiv.org/abs/2405.16785v1","category":"cs.CV"}
{"created":"2024-05-27 01:54:07","title":"Adaptive VIO: Deep Visual-Inertial Odometry with Online Continual Learning","abstract":"Visual-inertial odometry (VIO) has demonstrated remarkable success due to its low-cost and complementary sensors. However, existing VIO methods lack the generalization ability to adjust to different environments and sensor attributes. In this paper, we propose Adaptive VIO, a new monocular visual-inertial odometry that combines online continual learning with traditional nonlinear optimization. Adaptive VIO comprises two networks to predict visual correspondence and IMU bias. Unlike end-to-end approaches that use networks to fuse the features from two modalities (camera and IMU) and predict poses directly, we combine neural networks with visual-inertial bundle adjustment in our VIO system. The optimized estimates will be fed back to the visual and IMU bias networks, refining the networks in a self-supervised manner. Such a learning-optimization-combined framework and feedback mechanism enable the system to perform online continual learning. Experiments demonstrate that our Adaptive VIO manifests adaptive capability on EuRoC and TUM-VI datasets. The overall performance exceeds the currently known learning-based VIO methods and is comparable to the state-of-the-art optimization-based methods.","sentences":["Visual-inertial odometry (VIO) has demonstrated remarkable success due to its low-cost and complementary sensors.","However, existing VIO methods lack the generalization ability to adjust to different environments and sensor attributes.","In this paper, we propose Adaptive VIO, a new monocular visual-inertial odometry that combines online continual learning with traditional nonlinear optimization.","Adaptive VIO comprises two networks to predict visual correspondence and IMU bias.","Unlike end-to-end approaches that use networks to fuse the features from two modalities (camera and IMU) and predict poses directly, we combine neural networks with visual-inertial bundle adjustment in our VIO system.","The optimized estimates will be fed back to the visual and IMU bias networks, refining the networks in a self-supervised manner.","Such a learning-optimization-combined framework and feedback mechanism enable the system to perform online continual learning.","Experiments demonstrate that our Adaptive VIO manifests adaptive capability on EuRoC and TUM-VI datasets.","The overall performance exceeds the currently known learning-based VIO methods and is comparable to the state-of-the-art optimization-based methods."],"url":"http://arxiv.org/abs/2405.16754v1","category":"cs.RO"}
{"created":"2024-05-27 01:31:40","title":"Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective","abstract":"The two-stage fine-tuning (FT) method, linear probing then fine-tuning (LP-FT), consistently outperforms linear probing (LP) and FT alone in terms of accuracy for both in-distribution (ID) and out-of-distribution (OOD) data. This success is largely attributed to the preservation of pre-trained features, achieved through a near-optimal linear head obtained during LP. However, despite the widespread use of large language models, the exploration of complex architectures such as Transformers remains limited. In this paper, we analyze the training dynamics of LP-FT for classification models on the basis of the neural tangent kernel (NTK) theory. Our analysis decomposes the NTK matrix into two components, highlighting the importance of the linear head norm alongside the prediction accuracy at the start of the FT stage. We also observe a significant increase in the linear head norm during LP, stemming from training with the cross-entropy (CE) loss, which effectively minimizes feature changes. Furthermore, we find that this increased norm can adversely affect model calibration, a challenge that can be addressed by temperature scaling. Additionally, we extend our analysis with the NTK to the low-rank adaptation (LoRA) method and validate its effectiveness. Our experiments with a Transformer-based model on natural language processing tasks across multiple benchmarks confirm our theoretical analysis and demonstrate the effectiveness of LP-FT in fine-tuning language models. Code is available at https://github.com/tom4649/lp-ft_ntk.","sentences":["The two-stage fine-tuning (FT) method, linear probing then fine-tuning (LP-FT), consistently outperforms linear probing (LP) and FT alone in terms of accuracy for both in-distribution (ID) and out-of-distribution (OOD) data.","This success is largely attributed to the preservation of pre-trained features, achieved through a near-optimal linear head obtained during LP.","However, despite the widespread use of large language models, the exploration of complex architectures such as Transformers remains limited.","In this paper, we analyze the training dynamics of LP-FT for classification models on the basis of the neural tangent kernel (NTK) theory.","Our analysis decomposes the NTK matrix into two components, highlighting the importance of the linear head norm alongside the prediction accuracy at the start of the FT stage.","We also observe a significant increase in the linear head norm during LP, stemming from training with the cross-entropy (CE) loss, which effectively minimizes feature changes.","Furthermore, we find that this increased norm can adversely affect model calibration, a challenge that can be addressed by temperature scaling.","Additionally, we extend our analysis with the NTK to the low-rank adaptation (LoRA) method and validate its effectiveness.","Our experiments with a Transformer-based model on natural language processing tasks across multiple benchmarks confirm our theoretical analysis and demonstrate the effectiveness of LP-FT in fine-tuning language models.","Code is available at https://github.com/tom4649/lp-ft_ntk."],"url":"http://arxiv.org/abs/2405.16747v1","category":"cs.LG"}
{"created":"2024-05-27 01:13:01","title":"PP-SAM: Perturbed Prompts for Robust Adaptation of Segment Anything Model for Polyp Segmentation","abstract":"The Segment Anything Model (SAM), originally designed for general-purpose segmentation tasks, has been used recently for polyp segmentation. Nonetheless, fine-tuning SAM with data from new imaging centers or clinics poses significant challenges. This is because this necessitates the creation of an expensive and time-intensive annotated dataset, along with the potential for variability in user prompts during inference. To address these issues, we propose a robust fine-tuning technique, PP-SAM, that allows SAM to adapt to the polyp segmentation task with limited images. To this end, we utilize variable perturbed bounding box prompts (BBP) to enrich the learning context and enhance the model's robustness to BBP perturbations during inference. Rigorous experiments on polyp segmentation benchmarks reveal that our variable BBP perturbation significantly improves model resilience. Notably, on Kvasir, 1-shot fine-tuning boosts the DICE score by 20% and 37% with 50 and 100-pixel BBP perturbations during inference, respectively. Moreover, our experiments show that 1-shot, 5-shot, and 10-shot PP-SAM with 50-pixel perturbations during inference outperform a recent state-of-the-art (SOTA) polyp segmentation method by 26%, 7%, and 5% DICE scores, respectively. Our results motivate the broader applicability of our PP-SAM for other medical imaging tasks with limited samples. Our implementation is available at https://github.com/SLDGroup/PP-SAM.","sentences":["The Segment Anything Model (SAM), originally designed for general-purpose segmentation tasks, has been used recently for polyp segmentation.","Nonetheless, fine-tuning SAM with data from new imaging centers or clinics poses significant challenges.","This is because this necessitates the creation of an expensive and time-intensive annotated dataset, along with the potential for variability in user prompts during inference.","To address these issues, we propose a robust fine-tuning technique, PP-SAM, that allows SAM to adapt to the polyp segmentation task with limited images.","To this end, we utilize variable perturbed bounding box prompts (BBP) to enrich the learning context and enhance the model's robustness to BBP perturbations during inference.","Rigorous experiments on polyp segmentation benchmarks reveal that our variable BBP perturbation significantly improves model resilience.","Notably, on Kvasir, 1-shot fine-tuning boosts the DICE score by 20% and 37% with 50 and 100-pixel BBP perturbations during inference, respectively.","Moreover, our experiments show that 1-shot, 5-shot, and 10-shot PP-SAM with 50-pixel perturbations during inference outperform a recent state-of-the-art (SOTA) polyp segmentation method by 26%, 7%, and 5% DICE scores, respectively.","Our results motivate the broader applicability of our PP-SAM for other medical imaging tasks with limited samples.","Our implementation is available at https://github.com/SLDGroup/PP-SAM."],"url":"http://arxiv.org/abs/2405.16740v1","category":"cs.CV"}
{"created":"2024-05-27 00:12:51","title":"Pretraining with Random Noise for Fast and Robust Learning without Weight Transport","abstract":"The brain prepares for learning even before interacting with the environment, by refining and optimizing its structures through spontaneous neural activity that resembles random noise. However, the mechanism of such a process has yet to be thoroughly understood, and it is unclear whether this process can benefit the algorithm of machine learning. Here, we study this issue using a neural network with a feedback alignment algorithm, demonstrating that pretraining neural networks with random noise increases the learning efficiency as well as generalization abilities without weight transport. First, we found that random noise training modifies forward weights to match backward synaptic feedback, which is necessary for teaching errors by feedback alignment. As a result, a network with pre-aligned weights learns notably faster than a network without random noise training, even reaching a convergence speed comparable to that of a backpropagation algorithm. Sequential training with both random noise and data brings weights closer to synaptic feedback than training solely with data, enabling more precise credit assignment and faster learning. We also found that each readout probability approaches the chance level and that the effective dimensionality of weights decreases in a network pretrained with random noise. This pre-regularization allows the network to learn simple solutions of a low rank, reducing the generalization loss during subsequent training. This also enables the network robustly to generalize a novel, out-of-distribution dataset. Lastly, we confirmed that random noise pretraining reduces the amount of meta-loss, enhancing the network ability to adapt to various tasks. Overall, our results suggest that random noise training with feedback alignment offers a straightforward yet effective method of pretraining that facilitates quick and reliable learning without weight transport.","sentences":["The brain prepares for learning even before interacting with the environment, by refining and optimizing its structures through spontaneous neural activity that resembles random noise.","However, the mechanism of such a process has yet to be thoroughly understood, and it is unclear whether this process can benefit the algorithm of machine learning.","Here, we study this issue using a neural network with a feedback alignment algorithm, demonstrating that pretraining neural networks with random noise increases the learning efficiency as well as generalization abilities without weight transport.","First, we found that random noise training modifies forward weights to match backward synaptic feedback, which is necessary for teaching errors by feedback alignment.","As a result, a network with pre-aligned weights learns notably faster than a network without random noise training, even reaching a convergence speed comparable to that of a backpropagation algorithm.","Sequential training with both random noise and data brings weights closer to synaptic feedback than training solely with data, enabling more precise credit assignment and faster learning.","We also found that each readout probability approaches the chance level and that the effective dimensionality of weights decreases in a network pretrained with random noise.","This pre-regularization allows the network to learn simple solutions of a low rank, reducing the generalization loss during subsequent training.","This also enables the network robustly to generalize a novel, out-of-distribution dataset.","Lastly, we confirmed that random noise pretraining reduces the amount of meta-loss, enhancing the network ability to adapt to various tasks.","Overall, our results suggest that random noise training with feedback alignment offers a straightforward yet effective method of pretraining that facilitates quick and reliable learning without weight transport."],"url":"http://arxiv.org/abs/2405.16731v1","category":"cs.LG"}
{"created":"2024-05-26 22:50:03","title":"Adaptive Incentive Design with Learning Agents","abstract":"How can the system operator learn an incentive mechanism that achieves social optimality based on limited information about the agents' behavior, who are dynamically updating their strategies? To answer this question, we propose an \\emph{adaptive} incentive mechanism. This mechanism updates the incentives of agents based on the feedback of each agent's externality, evaluated as the difference between the player's marginal cost and society's marginal cost at each time step. The proposed mechanism updates the incentives on a slower timescale compared to the agents' learning dynamics, resulting in a two-timescale coupled dynamical system. Notably, this mechanism is agnostic to the specific learning dynamics used by agents to update their strategies. We show that any fixed point of this adaptive incentive mechanism corresponds to the optimal incentive mechanism, ensuring that the Nash equilibrium coincides with the socially optimal strategy. Additionally, we provide sufficient conditions that guarantee the convergence of the adaptive incentive mechanism to a fixed point. Our results apply to both atomic and non-atomic games. To demonstrate the effectiveness of our proposed mechanism, we verify the convergence conditions in two practically relevant games: atomic networked quadratic aggregative games and non-atomic network routing games.","sentences":["How can the system operator learn an incentive mechanism that achieves social optimality based on limited information about the agents' behavior, who are dynamically updating their strategies?","To answer this question, we propose an \\emph{adaptive} incentive mechanism.","This mechanism updates the incentives of agents based on the feedback of each agent's externality, evaluated as the difference between the player's marginal cost and society's marginal cost at each time step.","The proposed mechanism updates the incentives on a slower timescale compared to the agents' learning dynamics, resulting in a two-timescale coupled dynamical system.","Notably, this mechanism is agnostic to the specific learning dynamics used by agents to update their strategies.","We show that any fixed point of this adaptive incentive mechanism corresponds to the optimal incentive mechanism, ensuring that the Nash equilibrium coincides with the socially optimal strategy.","Additionally, we provide sufficient conditions that guarantee the convergence of the adaptive incentive mechanism to a fixed point.","Our results apply to both atomic and non-atomic games.","To demonstrate the effectiveness of our proposed mechanism, we verify the convergence conditions in two practically relevant games: atomic networked quadratic aggregative games and non-atomic network routing games."],"url":"http://arxiv.org/abs/2405.16716v1","category":"cs.GT"}
{"created":"2024-05-26 22:08:58","title":"REX: Designing User-centered Repair and Explanations to Address Robot Failures","abstract":"Robots in real-world environments continuously engage with multiple users and encounter changes that lead to unexpected conflicts in fulfilling user requests. Recent technical advancements (e.g., large-language models (LLMs), program synthesis) offer various methods for automatically generating repair plans that address such conflicts. In this work, we understand how automated repair and explanations can be designed to improve user experience with robot failures through two user studies. In our first, online study ($n=162$), users expressed increased trust, satisfaction, and utility with the robot performing automated repair and explanations. However, we also identified risk factors -- safety, privacy, and complexity -- that require adaptive repair strategies. The second, in-person study ($n=24$) elucidated distinct repair and explanation strategies depending on the level of risk severity and type. Using a design-based approach, we explore automated repair with explanations as a solution for robots to handle conflicts and failures, complemented by adaptive strategies for risk factors. Finally, we discuss the implications of incorporating such strategies into robot designs to achieve seamless operation among changing user needs and environments.","sentences":["Robots in real-world environments continuously engage with multiple users and encounter changes that lead to unexpected conflicts in fulfilling user requests.","Recent technical advancements (e.g., large-language models (LLMs), program synthesis) offer various methods for automatically generating repair plans that address such conflicts.","In this work, we understand how automated repair and explanations can be designed to improve user experience with robot failures through two user studies.","In our first, online study ($n=162$), users expressed increased trust, satisfaction, and utility with the robot performing automated repair and explanations.","However, we also identified risk factors -- safety, privacy, and complexity -- that require adaptive repair strategies.","The second, in-person study ($n=24$) elucidated distinct repair and explanation strategies depending on the level of risk severity and type.","Using a design-based approach, we explore automated repair with explanations as a solution for robots to handle conflicts and failures, complemented by adaptive strategies for risk factors.","Finally, we discuss the implications of incorporating such strategies into robot designs to achieve seamless operation among changing user needs and environments."],"url":"http://arxiv.org/abs/2405.16710v1","category":"cs.RO"}
{"created":"2024-05-26 15:28:42","title":"A CMDP-within-online framework for Meta-Safe Reinforcement Learning","abstract":"Meta-reinforcement learning has widely been used as a learning-to-learn framework to solve unseen tasks with limited experience. However, the aspect of constraint violations has not been adequately addressed in the existing works, making their application restricted in real-world settings. In this paper, we study the problem of meta-safe reinforcement learning (Meta-SRL) through the CMDP-within-online framework to establish the first provable guarantees in this important setting. We obtain task-averaged regret bounds for the reward maximization (optimality gap) and constraint violations using gradient-based meta-learning and show that the task-averaged optimality gap and constraint satisfaction improve with task-similarity in a static environment or task-relatedness in a dynamic environment. Several technical challenges arise when making this framework practical. To this end, we propose a meta-algorithm that performs inexact online learning on the upper bounds of within-task optimality gap and constraint violations estimated by off-policy stationary distribution corrections. Furthermore, we enable the learning rates to be adapted for every task and extend our approach to settings with a competing dynamically changing oracle. Finally, experiments are conducted to demonstrate the effectiveness of our approach.","sentences":["Meta-reinforcement learning has widely been used as a learning-to-learn framework to solve unseen tasks with limited experience.","However, the aspect of constraint violations has not been adequately addressed in the existing works, making their application restricted in real-world settings.","In this paper, we study the problem of meta-safe reinforcement learning (Meta-SRL) through the CMDP-within-online framework to establish the first provable guarantees in this important setting.","We obtain task-averaged regret bounds for the reward maximization (optimality gap) and constraint violations using gradient-based meta-learning and show that the task-averaged optimality gap and constraint satisfaction improve with task-similarity in a static environment or task-relatedness in a dynamic environment.","Several technical challenges arise when making this framework practical.","To this end, we propose a meta-algorithm that performs inexact online learning on the upper bounds of within-task optimality gap and constraint violations estimated by off-policy stationary distribution corrections.","Furthermore, we enable the learning rates to be adapted for every task and extend our approach to settings with a competing dynamically changing oracle.","Finally, experiments are conducted to demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2405.16601v1","category":"cs.LG"}
{"created":"2024-05-26 15:14:54","title":"Protect-Your-IP: Scalable Source-Tracing and Attribution against Personalized Generation","abstract":"With the advent of personalized generation models, users can more readily create images resembling existing content, heightening the risk of violating portrait rights and intellectual property (IP). Traditional post-hoc detection and source-tracing methods for AI-generated content (AIGC) employ proactive watermark approaches; however, these are less effective against personalized generation models. Moreover, attribution techniques for AIGC rely on passive detection but often struggle to differentiate AIGC from authentic images, presenting a substantial challenge. Integrating these two processes into a cohesive framework not only meets the practical demands for protection and forensics but also improves the effectiveness of attribution tasks. Inspired by this insight, we propose a unified approach for image copyright source-tracing and attribution, introducing an innovative watermarking-attribution method that blends proactive and passive strategies. We embed copyright watermarks into protected images and train a watermark decoder to retrieve copyright information from the outputs of personalized models, using this watermark as an initial step for confirming if an image is AIGC-generated. To pinpoint specific generation techniques, we utilize powerful visual backbone networks for classification. Additionally, we implement an incremental learning strategy to adeptly attribute new personalized models without losing prior knowledge, thereby enhancing the model's adaptability to novel generation methods. We have conducted experiments using various celebrity portrait series sourced online, and the results affirm the efficacy of our method in source-tracing and attribution tasks, as well as its robustness against knowledge forgetting.","sentences":["With the advent of personalized generation models, users can more readily create images resembling existing content, heightening the risk of violating portrait rights and intellectual property (IP).","Traditional post-hoc detection and source-tracing methods for AI-generated content (AIGC) employ proactive watermark approaches; however, these are less effective against personalized generation models.","Moreover, attribution techniques for AIGC rely on passive detection but often struggle to differentiate AIGC from authentic images, presenting a substantial challenge.","Integrating these two processes into a cohesive framework not only meets the practical demands for protection and forensics but also improves the effectiveness of attribution tasks.","Inspired by this insight, we propose a unified approach for image copyright source-tracing and attribution, introducing an innovative watermarking-attribution method that blends proactive and passive strategies.","We embed copyright watermarks into protected images and train a watermark decoder to retrieve copyright information from the outputs of personalized models, using this watermark as an initial step for confirming if an image is AIGC-generated.","To pinpoint specific generation techniques, we utilize powerful visual backbone networks for classification.","Additionally, we implement an incremental learning strategy to adeptly attribute new personalized models without losing prior knowledge, thereby enhancing the model's adaptability to novel generation methods.","We have conducted experiments using various celebrity portrait series sourced online, and the results affirm the efficacy of our method in source-tracing and attribution tasks, as well as its robustness against knowledge forgetting."],"url":"http://arxiv.org/abs/2405.16596v1","category":"cs.CV"}
{"created":"2024-05-26 14:50:40","title":"CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot Classification","abstract":"Recent advances in vision-language foundational models, such as CLIP, have demonstrated significant strides in zero-shot classification. However, the extensive parameterization of models like CLIP necessitates a resource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have introduced training-free methods aimed at bolstering the efficacy of downstream tasks. While these approaches incorporate support sets to maintain data distribution consistency between knowledge cache and test sets, they often fall short in terms of generalization on the test set, particularly when faced with test data exhibiting substantial distributional variations. In this work, we present CapS-Adapter, an innovative method that employs a caption-based support set, effectively harnessing both image and caption features to exceed existing state-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly constructs support sets that closely mirror target distributions, utilizing instance-level distribution features extracted from multimodal large models. By leveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances predictive accuracy through the use of multimodal support sets. Our method achieves outstanding zero-shot classification results across 19 benchmark datasets, improving accuracy by 2.19\\% over the previous leading method. Our contributions are substantiated through extensive validation on multiple benchmark datasets, demonstrating superior performance and robust generalization capabilities. Our code is made publicly available at https://github.com/WLuLi/CapS-Adapter.","sentences":["Recent advances in vision-language foundational models, such as CLIP, have demonstrated significant strides in zero-shot classification.","However, the extensive parameterization of models like CLIP necessitates a resource-intensive fine-tuning process.","In response, TIP-Adapter and SuS-X have introduced training-free methods aimed at bolstering the efficacy of downstream tasks.","While these approaches incorporate support sets to maintain data distribution consistency between knowledge cache and test sets, they often fall short in terms of generalization on the test set, particularly when faced with test data exhibiting substantial distributional variations.","In this work, we present CapS-Adapter, an innovative method that employs a caption-based support set, effectively harnessing both image and caption features to exceed existing state-of-the-art techniques in training-free scenarios.","CapS-Adapter adeptly constructs support sets that closely mirror target distributions, utilizing instance-level distribution features extracted from multimodal large models.","By leveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances predictive accuracy through the use of multimodal support sets.","Our method achieves outstanding zero-shot classification results across 19 benchmark datasets, improving accuracy by 2.19\\% over the previous leading method.","Our contributions are substantiated through extensive validation on multiple benchmark datasets, demonstrating superior performance and robust generalization capabilities.","Our code is made publicly available at https://github.com/WLuLi/CapS-Adapter."],"url":"http://arxiv.org/abs/2405.16591v1","category":"cs.CV"}
{"created":"2024-05-26 13:56:53","title":"Local Curvature Descent: Squeezing More Curvature out of Standard and Polyak Gradient Descent","abstract":"We contribute to the growing body of knowledge on more powerful and adaptive stepsizes for convex optimization, empowered by local curvature information. We do not go the route of fully-fledged second-order methods which require the expensive computation of the Hessian. Instead, our key observation is that, for some problems (e.g., when minimizing the sum of squares of absolutely convex functions), certain local curvature information is readily available, and can be used to obtain surprisingly powerful matrix-valued stepsizes, and meaningful theory. In particular, we develop three new methods$\\unicode{x2013}$LCD1, LCD2 and LCD3$\\unicode{x2013}$where the abbreviation stands for local curvature descent. While LCD1 generalizes gradient descent with fixed stepsize, LCD2 generalizes gradient descent with Polyak stepsize. Our methods enhance these classical gradient descent baselines with local curvature information, and our theory recovers the known rates in the special case when no curvature information is used. Our last method, LCD3, is a variable metric version of LCD2; this feature leads to a closed-form expression for the iterates. Our empirical results are encouraging, and show that the local curvature descent improves upon gradient descent.","sentences":["We contribute to the growing body of knowledge on more powerful and adaptive stepsizes for convex optimization, empowered by local curvature information.","We do not go the route of fully-fledged second-order methods which require the expensive computation of the Hessian.","Instead, our key observation is that, for some problems (e.g., when minimizing the sum of squares of absolutely convex functions), certain local curvature information is readily available, and can be used to obtain surprisingly powerful matrix-valued stepsizes, and meaningful theory.","In particular, we develop three new methods$\\unicode{x2013}$LCD1, LCD2 and LCD3$\\unicode{x2013}$where the abbreviation stands for local curvature descent.","While LCD1 generalizes gradient descent with fixed stepsize, LCD2 generalizes gradient descent with Polyak stepsize.","Our methods enhance these classical gradient descent baselines with local curvature information, and our theory recovers the known rates in the special case when no curvature information is used.","Our last method, LCD3, is a variable metric version of LCD2; this feature leads to a closed-form expression for the iterates.","Our empirical results are encouraging, and show that the local curvature descent improves upon gradient descent."],"url":"http://arxiv.org/abs/2405.16574v1","category":"math.OC"}
{"created":"2024-05-26 12:26:54","title":"Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians","abstract":"3D Gaussian Splatting has emerged as a powerful representation of geometry and appearance for RGB-only dense Simultaneous Localization and Mapping (SLAM), as it provides a compact dense map representation while enabling efficient and high-quality map rendering. However, existing methods show significantly worse reconstruction quality than competing methods using other 3D representations, e.g. neural points clouds, since they either do not employ global map and pose optimization or make use of monocular depth. In response, we propose the first RGB-only SLAM system with a dense 3D Gaussian map representation that utilizes all benefits of globally optimized tracking by adapting dynamically to keyframe pose and depth updates by actively deforming the 3D Gaussian map. Moreover, we find that refining the depth updates in inaccurate areas with a monocular depth estimator further improves the accuracy of the 3D reconstruction. Our experiments on the Replica, TUM-RGBD, and ScanNet datasets indicate the effectiveness of globally optimized 3D Gaussians, as the approach achieves superior or on par performance with existing RGB-only SLAM methods methods in tracking, mapping and rendering accuracy while yielding small map sizes and fast runtimes. The source code is available at https://github.com/eriksandstroem/Splat-SLAM.","sentences":["3D Gaussian Splatting has emerged as a powerful representation of geometry and appearance for RGB-only dense Simultaneous Localization and Mapping (SLAM), as it provides a compact dense map representation while enabling efficient and high-quality map rendering.","However, existing methods show significantly worse reconstruction quality than competing methods using other 3D representations, e.g. neural points clouds, since they either do not employ global map and pose optimization or make use of monocular depth.","In response, we propose the first RGB-only SLAM system with a dense 3D Gaussian map representation that utilizes all benefits of globally optimized tracking by adapting dynamically to keyframe pose and depth updates by actively deforming the 3D Gaussian map.","Moreover, we find that refining the depth updates in inaccurate areas with a monocular depth estimator further improves the accuracy of the 3D reconstruction.","Our experiments on the Replica, TUM-RGBD, and ScanNet datasets indicate the effectiveness of globally optimized 3D Gaussians, as the approach achieves superior or on par performance with existing RGB-only SLAM methods methods in tracking, mapping and rendering accuracy while yielding small map sizes and fast runtimes.","The source code is available at https://github.com/eriksandstroem/Splat-SLAM."],"url":"http://arxiv.org/abs/2405.16544v1","category":"cs.CV"}
{"created":"2024-05-26 11:47:40","title":"I2VEdit: First-Frame-Guided Video Editing via Image-to-Video Diffusion Models","abstract":"The remarkable generative capabilities of diffusion models have motivated extensive research in both image and video editing. Compared to video editing which faces additional challenges in the time dimension, image editing has witnessed the development of more diverse, high-quality approaches and more capable software like Photoshop. In light of this gap, we introduce a novel and generic solution that extends the applicability of image editing tools to videos by propagating edits from a single frame to the entire video using a pre-trained image-to-video model. Our method, dubbed I2VEdit, adaptively preserves the visual and motion integrity of the source video depending on the extent of the edits, effectively handling global edits, local edits, and moderate shape changes, which existing methods cannot fully achieve. At the core of our method are two main processes: Coarse Motion Extraction to align basic motion patterns with the original video, and Appearance Refinement for precise adjustments using fine-grained attention matching. We also incorporate a skip-interval strategy to mitigate quality degradation from auto-regressive generation across multiple video clips. Experimental results demonstrate our framework's superior performance in fine-grained video editing, proving its capability to produce high-quality, temporally consistent outputs.","sentences":["The remarkable generative capabilities of diffusion models have motivated extensive research in both image and video editing.","Compared to video editing which faces additional challenges in the time dimension, image editing has witnessed the development of more diverse, high-quality approaches and more capable software like Photoshop.","In light of this gap, we introduce a novel and generic solution that extends the applicability of image editing tools to videos by propagating edits from a single frame to the entire video using a pre-trained image-to-video model.","Our method, dubbed I2VEdit, adaptively preserves the visual and motion integrity of the source video depending on the extent of the edits, effectively handling global edits, local edits, and moderate shape changes, which existing methods cannot fully achieve.","At the core of our method are two main processes: Coarse Motion Extraction to align basic motion patterns with the original video, and Appearance Refinement for precise adjustments using fine-grained attention matching.","We also incorporate a skip-interval strategy to mitigate quality degradation from auto-regressive generation across multiple video clips.","Experimental results demonstrate our framework's superior performance in fine-grained video editing, proving its capability to produce high-quality, temporally consistent outputs."],"url":"http://arxiv.org/abs/2405.16537v1","category":"cs.CV"}
{"created":"2024-05-26 11:29:57","title":"LoQT: Low Rank Adapters for Quantized Training","abstract":"Training of large neural networks requires significant computational resources. Despite advances using low-rank adapters and quantization, pretraining of models such as LLMs on consumer hardware has not been possible without model sharding, offloading during training, or per-layer gradient updates. To address these limitations, we propose LoQT, a method for efficiently training quantized models. LoQT uses gradient-based tensor factorization to initialize low-rank trainable weight matrices that are periodically merged into quantized full-rank weight matrices. Our approach is suitable for both pretraining and fine-tuning of models, which we demonstrate experimentally for language modeling and downstream task adaptation. We find that LoQT enables efficient training of models up to 7B parameters on a consumer-grade 24GB GPU. We also demonstrate the feasibility of training a 13B parameter model using per-layer gradient updates on the same hardware.","sentences":["Training of large neural networks requires significant computational resources.","Despite advances using low-rank adapters and quantization, pretraining of models such as LLMs on consumer hardware has not been possible without model sharding, offloading during training, or per-layer gradient updates.","To address these limitations, we propose LoQT, a method for efficiently training quantized models.","LoQT uses gradient-based tensor factorization to initialize low-rank trainable weight matrices that are periodically merged into quantized full-rank weight matrices.","Our approach is suitable for both pretraining and fine-tuning of models, which we demonstrate experimentally for language modeling and downstream task adaptation.","We find that LoQT enables efficient training of models up to 7B parameters on a consumer-grade 24GB GPU.","We also demonstrate the feasibility of training a 13B parameter model using per-layer gradient updates on the same hardware."],"url":"http://arxiv.org/abs/2405.16528v1","category":"cs.LG"}
{"created":"2024-05-26 11:28:12","title":"Adaptive estimation of the $\\mathbb{L}_2$-norm of a probability density and related topics II. Upper bounds via the oracle approach","abstract":"This is the second part of the research project initiated in Cleanthous et al (2024). We deal with the problem of the adaptive estimation of the $\\mathbb{L}_2$-norm of a probability density on $\\mathbb{R}^d$, $d\\geq 1$, from independent observations. The unknown density is assumed to be uniformly bounded by unknown constant and to belong to the union of balls in the isotropic/anisotropic Nikolskii's spaces. In Cleanthous et al (2024) we have proved that the optimally adaptive estimators do no exist in the considered problem and provided with several lower bounds for the adaptive risk. In this part we show that these bounds are tight and present the adaptive estimator which is obtained by a data-driven selection from a family of kernel-based estimators. The proposed estimation procedure as well as the computation of its risk are heavily based on new concentration inequalities for decoupled $U$-statistics of order two established in Section 4. It is also worth noting that all our results are derived from the unique oracle inequality which may be of independent interest.","sentences":["This is the second part of the research project initiated in Cleanthous et al (2024).","We deal with the problem of the adaptive estimation of the $\\mathbb{L}_2$-norm of a probability density on $\\mathbb{R}^d$, $d\\geq 1$, from independent observations.","The unknown density is assumed to be uniformly bounded by unknown constant and to belong to the union of balls in the isotropic/anisotropic Nikolskii's spaces.","In Cleanthous et al (2024) we have proved that the optimally adaptive estimators do no exist in the considered problem and provided with several lower bounds for the adaptive risk.","In this part we show that these bounds are tight and present the adaptive estimator which is obtained by a data-driven selection from a family of kernel-based estimators.","The proposed estimation procedure as well as the computation of its risk are heavily based on new concentration inequalities for decoupled $U$-statistics of order two established in Section 4.","It is also worth noting that all our results are derived from the unique oracle inequality which may be of independent interest."],"url":"http://arxiv.org/abs/2405.16527v1","category":"math.ST"}
{"created":"2024-05-26 11:01:39","title":"Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors","abstract":"We aim to tackle sparse-view reconstruction of a 360 3D scene using priors from latent diffusion models (LDM). The sparse-view setting is ill-posed and underconstrained, especially for scenes where the camera rotates 360 degrees around a point, as no visual information is available beyond some frontal views focused on the central object(s) of interest. In this work, we show that pretrained 2D diffusion models can strongly improve the reconstruction of a scene with low-cost fine-tuning. Specifically, we present SparseSplat360 (Sp2360), a method that employs a cascade of in-painting and artifact removal models to fill in missing details and clean novel views. Due to superior training and rendering speeds, we use an explicit scene representation in the form of 3D Gaussians over NeRF-based implicit representations. We propose an iterative update strategy to fuse generated pseudo novel views with existing 3D Gaussians fitted to the initial sparse inputs. As a result, we obtain a multi-view consistent scene representation with details coherent with the observed inputs. Our evaluation on the challenging Mip-NeRF360 dataset shows that our proposed 2D to 3D distillation algorithm considerably improves the performance of a regularized version of 3DGS adapted to a sparse-view setting and outperforms existing sparse-view reconstruction methods in 360 scene reconstruction. Qualitatively, our method generates entire 360 scenes from as few as 9 input views, with a high degree of foreground and background detail.","sentences":["We aim to tackle sparse-view reconstruction of a 360 3D scene using priors from latent diffusion models (LDM).","The sparse-view setting is ill-posed and underconstrained, especially for scenes where the camera rotates 360 degrees around a point, as no visual information is available beyond some frontal views focused on the central object(s) of interest.","In this work, we show that pretrained 2D diffusion models can strongly improve the reconstruction of a scene with low-cost fine-tuning.","Specifically, we present SparseSplat360 (Sp2360), a method that employs a cascade of in-painting and artifact removal models to fill in missing details and clean novel views.","Due to superior training and rendering speeds, we use an explicit scene representation in the form of 3D Gaussians over NeRF-based implicit representations.","We propose an iterative update strategy to fuse generated pseudo novel views with existing 3D Gaussians fitted to the initial sparse inputs.","As a result, we obtain a multi-view consistent scene representation with details coherent with the observed inputs.","Our evaluation on the challenging Mip-NeRF360 dataset shows that our proposed 2D to 3D distillation algorithm considerably improves the performance of a regularized version of 3DGS adapted to a sparse-view setting and outperforms existing sparse-view reconstruction methods in 360 scene reconstruction.","Qualitatively, our method generates entire 360 scenes from as few as 9 input views, with a high degree of foreground and background detail."],"url":"http://arxiv.org/abs/2405.16517v1","category":"cs.CV"}
{"created":"2024-05-26 07:54:53","title":"Multi-Level Additive Modeling for Structured Non-IID Federated Learning","abstract":"The primary challenge in Federated Learning (FL) is to model non-IID distributions across clients, whose fine-grained structure is important to improve knowledge sharing. For example, some knowledge is globally shared across all clients, some is only transferable within a subgroup of clients, and some are client-specific. To capture and exploit this structure, we train models organized in a multi-level structure, called ``Multi-level Additive Models (MAM)'', for better knowledge-sharing across heterogeneous clients and their personalization. In federated MAM (FeMAM), each client is assigned to at most one model per level and its personalized prediction sums up the outputs of models assigned to it across all levels. For the top level, FeMAM trains one global model shared by all clients as FedAvg. For every mid-level, it learns multiple models each assigned to a subgroup of clients, as clustered FL. Every bottom-level model is trained for one client only. In the training objective, each model aims to minimize the residual of the additive predictions by the other models assigned to each client. To approximate the arbitrary structure of non-IID across clients, FeMAM introduces more flexibility and adaptivity to FL by incrementally adding new models to the prediction of each client and reassigning another if necessary, automatically optimizing the knowledge-sharing structure. Extensive experiments show that FeMAM surpasses existing clustered FL and personalized FL methods in various non-IID settings. Our code is available at https://github.com/shutong043/FeMAM.","sentences":["The primary challenge in Federated Learning (FL) is to model non-IID distributions across clients, whose fine-grained structure is important to improve knowledge sharing.","For example, some knowledge is globally shared across all clients, some is only transferable within a subgroup of clients, and some are client-specific.","To capture and exploit this structure, we train models organized in a multi-level structure, called ``Multi-level Additive Models (MAM)'', for better knowledge-sharing across heterogeneous clients and their personalization.","In federated MAM (FeMAM), each client is assigned to at most one model per level and its personalized prediction sums up the outputs of models assigned to it across all levels.","For the top level, FeMAM trains one global model shared by all clients as FedAvg.","For every mid-level, it learns multiple models each assigned to a subgroup of clients, as clustered FL.","Every bottom-level model is trained for one client only.","In the training objective, each model aims to minimize the residual of the additive predictions by the other models assigned to each client.","To approximate the arbitrary structure of non-IID across clients, FeMAM introduces more flexibility and adaptivity to FL by incrementally adding new models to the prediction of each client and reassigning another if necessary, automatically optimizing the knowledge-sharing structure.","Extensive experiments show that FeMAM surpasses existing clustered FL and personalized FL methods in various non-IID settings.","Our code is available at https://github.com/shutong043/FeMAM."],"url":"http://arxiv.org/abs/2405.16472v1","category":"cs.LG"}
{"created":"2024-05-26 05:41:42","title":"Incremental Pseudo-Labeling for Black-Box Unsupervised Domain Adaptation","abstract":"Black-Box unsupervised domain adaptation (BBUDA) learns knowledge only with the prediction of target data from the source model without access to the source data and source model, which attempts to alleviate concerns about the privacy and security of data. However, incorrect pseudo-labels are prevalent in the prediction generated by the source model due to the cross-domain discrepancy, which may substantially degrade the performance of the target model. To address this problem, we propose a novel approach that incrementally selects high-confidence pseudo-labels to improve the generalization ability of the target model. Specifically, we first generate pseudo-labels using a source model and train a crude target model by a vanilla BBUDA method. Second, we iteratively select high-confidence data from the low-confidence data pool by thresholding the softmax probabilities, prototype labels, and intra-class similarity. Then, we iteratively train a stronger target network based on the crude target model to correct the wrongly labeled samples to improve the accuracy of the pseudo-label. Experimental results demonstrate that the proposed method achieves state-of-the-art black-box unsupervised domain adaptation performance on three benchmark datasets.","sentences":["Black-Box unsupervised domain adaptation (BBUDA) learns knowledge only with the prediction of target data from the source model without access to the source data and source model, which attempts to alleviate concerns about the privacy and security of data.","However, incorrect pseudo-labels are prevalent in the prediction generated by the source model due to the cross-domain discrepancy, which may substantially degrade the performance of the target model.","To address this problem, we propose a novel approach that incrementally selects high-confidence pseudo-labels to improve the generalization ability of the target model.","Specifically, we first generate pseudo-labels using a source model and train a crude target model by a vanilla BBUDA method.","Second, we iteratively select high-confidence data from the low-confidence data pool by thresholding the softmax probabilities, prototype labels, and intra-class similarity.","Then, we iteratively train a stronger target network based on the crude target model to correct the wrongly labeled samples to improve the accuracy of the pseudo-label.","Experimental results demonstrate that the proposed method achieves state-of-the-art black-box unsupervised domain adaptation performance on three benchmark datasets."],"url":"http://arxiv.org/abs/2405.16437v1","category":"cs.CV"}
{"created":"2024-05-26 01:25:02","title":"AdaFisher: Adaptive Second Order Optimization via Fisher Information","abstract":"First-order optimization methods are currently the mainstream in training deep neural networks (DNNs). Optimizers like Adam incorporate limited curvature information by employing the diagonal matrix preconditioning of the stochastic gradient during the training. Despite their widespread, second-order optimization algorithms exhibit superior convergence properties compared to their first-order counterparts e.g. Adam and SGD. However, their practicality in training DNNs are still limited due to increased per-iteration computations and suboptimal accuracy compared to the first order methods. We present AdaFisher--an adaptive second-order optimizer that leverages a block-diagonal approximation to the Fisher information matrix for adaptive gradient preconditioning. AdaFisher aims to bridge the gap between enhanced convergence capabilities and computational efficiency in second-order optimization framework for training DNNs. Despite the slow pace of second-order optimizers, we showcase that AdaFisher can be reliably adopted for image classification, language modelling and stand out for its stability and robustness in hyperparameter tuning. We demonstrate that AdaFisher outperforms the SOTA optimizers in terms of both accuracy and convergence speed. Code available from \\href{https://github.com/AtlasAnalyticsLab/AdaFisher}{https://github.com/AtlasAnalyticsLab/AdaFisher}","sentences":["First-order optimization methods are currently the mainstream in training deep neural networks (DNNs).","Optimizers like Adam incorporate limited curvature information by employing the diagonal matrix preconditioning of the stochastic gradient during the training.","Despite their widespread, second-order optimization algorithms exhibit superior convergence properties compared to their first-order counterparts e.g. Adam and SGD.","However, their practicality in training DNNs are still limited due to increased per-iteration computations and suboptimal accuracy compared to the first order methods.","We present AdaFisher--an adaptive second-order optimizer that leverages a block-diagonal approximation to the Fisher information matrix for adaptive gradient preconditioning.","AdaFisher aims to bridge the gap between enhanced convergence capabilities and computational efficiency in second-order optimization framework for training DNNs.","Despite the slow pace of second-order optimizers, we showcase that AdaFisher can be reliably adopted for image classification, language modelling and stand out for its stability and robustness in hyperparameter tuning.","We demonstrate that AdaFisher outperforms the SOTA optimizers in terms of both accuracy and convergence speed.","Code available from \\href{https://github.com/AtlasAnalyticsLab/AdaFisher}{https://github.com/AtlasAnalyticsLab/AdaFisher}"],"url":"http://arxiv.org/abs/2405.16397v1","category":"cs.LG"}
{"created":"2024-05-26 01:08:28","title":"Daily Physical Activity Monitoring -- Adaptive Learning from Multi-source Motion Sensor Data","abstract":"In healthcare applications, there is a growing need to develop machine learning models that use data from a single source, such as that from a wrist wearable device, to monitor physical activities, assess health risks, and provide immediate health recommendations or interventions. However, the limitation of using single-source data often compromises the model's accuracy, as it fails to capture the full scope of human activities. While a more comprehensive dataset can be gathered in a lab setting using multiple sensors attached to various body parts, this approach is not practical for everyday use due to the impracticality of wearing multiple sensors. To address this challenge, we introduce a transfer learning framework that optimizes machine learning models for everyday applications by leveraging multi-source data collected in a laboratory setting. We introduce a novel metric to leverage the inherent relationship between these multiple data sources, as they are all paired to capture aspects of the same physical activity. Through numerical experiments, our framework outperforms existing methods in classification accuracy and robustness to noise, offering a promising avenue for the enhancement of daily activity monitoring.","sentences":["In healthcare applications, there is a growing need to develop machine learning models that use data from a single source, such as that from a wrist wearable device, to monitor physical activities, assess health risks, and provide immediate health recommendations or interventions.","However, the limitation of using single-source data often compromises the model's accuracy, as it fails to capture the full scope of human activities.","While a more comprehensive dataset can be gathered in a lab setting using multiple sensors attached to various body parts, this approach is not practical for everyday use due to the impracticality of wearing multiple sensors.","To address this challenge, we introduce a transfer learning framework that optimizes machine learning models for everyday applications by leveraging multi-source data collected in a laboratory setting.","We introduce a novel metric to leverage the inherent relationship between these multiple data sources, as they are all paired to capture aspects of the same physical activity.","Through numerical experiments, our framework outperforms existing methods in classification accuracy and robustness to noise, offering a promising avenue for the enhancement of daily activity monitoring."],"url":"http://arxiv.org/abs/2405.16395v1","category":"cs.LG"}
{"created":"2024-05-26 00:01:29","title":"Rewarded Region Replay (R3) for Policy Learning with Discrete Action Space","abstract":"We introduce a new on-policy algorithm called Rewarded Region Replay (R3), which significantly improves on PPO in solving environments with discrete action spaces. R3 improves sample efficiency by using a replay buffer which contains past successful trajectories with reward above a certain threshold, which are used to update a PPO agent with importance sampling. Crucially, we discard the importance sampling factors which are above a certain ratio to reduce variance and stabilize training. We found that R3 significantly outperforms PPO in Minigrid environments with sparse rewards and discrete action space, such as DoorKeyEnv and CrossingEnv, and moreover we found that the improvement margin of our method versus baseline PPO increases with the complexity of the environment. We also benchmarked the performance of R3 against DDQN (Double Deep Q-Network), which is a standard baseline in off-policy methods for discrete actions, and found that R3 also outperforms DDQN agent in DoorKeyEnv. Lastly, we adapt the idea of R3 to dense reward setting to obtain the Dense R3 algorithm (or DR3) and benchmarked it against PPO on Cartpole-V1 environment. We found that DR3 outperforms PPO significantly on this dense reward environment. Our code can be found at https://github.com/chry-santhemum/R3.","sentences":["We introduce a new on-policy algorithm called Rewarded Region Replay (R3), which significantly improves on PPO in solving environments with discrete action spaces.","R3 improves sample efficiency by using a replay buffer which contains past successful trajectories with reward above a certain threshold, which are used to update a PPO agent with importance sampling.","Crucially, we discard the importance sampling factors which are above a certain ratio to reduce variance and stabilize training.","We found that R3 significantly outperforms PPO in Minigrid environments with sparse rewards and discrete action space, such as DoorKeyEnv and CrossingEnv, and moreover we found that the improvement margin of our method versus baseline PPO increases with the complexity of the environment.","We also benchmarked the performance of R3 against DDQN (Double Deep Q-Network), which is a standard baseline in off-policy methods for discrete actions, and found that R3 also outperforms DDQN agent in DoorKeyEnv.","Lastly, we adapt the idea of R3 to dense reward setting to obtain the Dense R3 algorithm (or DR3) and benchmarked it against PPO on Cartpole-V1 environment.","We found that DR3 outperforms PPO significantly on this dense reward environment.","Our code can be found at https://github.com/chry-santhemum/R3."],"url":"http://arxiv.org/abs/2405.16383v1","category":"cs.LG"}
{"created":"2024-05-25 23:55:47","title":"Video Prediction Models as General Visual Encoders","abstract":"This study explores the potential of open-source video conditional generation models as encoders for downstream tasks, focusing on instance segmentation using the BAIR Robot Pushing Dataset. The researchers propose using video prediction models as general visual encoders, leveraging their ability to capture critical spatial and temporal information which is essential for tasks such as instance segmentation. Inspired by human vision studies, particularly Gestalts principle of common fate, the approach aims to develop a latent space representative of motion from images to effectively discern foreground from background information. The researchers utilize a 3D Vector-Quantized Variational Autoencoder 3D VQVAE video generative encoder model conditioned on an input frame, coupled with downstream segmentation tasks. Experiments involve adapting pre-trained video generative models, analyzing their latent spaces, and training custom decoders for foreground-background segmentation. The findings demonstrate promising results in leveraging generative pretext learning for downstream tasks, working towards enhanced scene analysis and segmentation in computer vision applications.","sentences":["This study explores the potential of open-source video conditional generation models as encoders for downstream tasks, focusing on instance segmentation using the BAIR Robot Pushing Dataset.","The researchers propose using video prediction models as general visual encoders, leveraging their ability to capture critical spatial and temporal information which is essential for tasks such as instance segmentation.","Inspired by human vision studies, particularly Gestalts principle of common fate, the approach aims to develop a latent space representative of motion from images to effectively discern foreground from background information.","The researchers utilize a 3D Vector-Quantized Variational Autoencoder 3D VQVAE video generative encoder model conditioned on an input frame, coupled with downstream segmentation tasks.","Experiments involve adapting pre-trained video generative models, analyzing their latent spaces, and training custom decoders for foreground-background segmentation.","The findings demonstrate promising results in leveraging generative pretext learning for downstream tasks, working towards enhanced scene analysis and segmentation in computer vision applications."],"url":"http://arxiv.org/abs/2405.16382v1","category":"cs.CV"}
{"created":"2024-05-25 23:53:07","title":"Trivialized Momentum Facilitates Diffusion Generative Modeling on Lie Groups","abstract":"The generative modeling of data on manifold is an important task, for which diffusion models in flat spaces typically need nontrivial adaptations. This article demonstrates how a technique called `trivialization' can transfer the effectiveness of diffusion models in Euclidean spaces to Lie groups. In particular, an auxiliary momentum variable was algorithmically introduced to help transport the position variable between data distribution and a fixed, easy-to-sample distribution. Normally, this would incur further difficulty for manifold data because momentum lives in a space that changes with the position. However, our trivialization technique creates to a new momentum variable that stays in a simple $\\textbf{fixed vector space}$. This design, together with a manifold preserving integrator, simplifies implementation and avoids inaccuracies created by approximations such as projections to tangent space and manifold, which were typically used in prior work, hence facilitating generation with high-fidelity and efficiency. The resulting method achieves state-of-the-art performance on protein and RNA torsion angle generation and sophisticated torus datasets. We also, arguably for the first time, tackle the generation of data on high-dimensional Special Orthogonal and Unitary groups, the latter essential for quantum problems.","sentences":["The generative modeling of data on manifold is an important task, for which diffusion models in flat spaces typically need nontrivial adaptations.","This article demonstrates how a technique called `trivialization' can transfer the effectiveness of diffusion models in Euclidean spaces to Lie groups.","In particular, an auxiliary momentum variable was algorithmically introduced to help transport the position variable between data distribution and a fixed, easy-to-sample distribution.","Normally, this would incur further difficulty for manifold data because momentum lives in a space that changes with the position.","However, our trivialization technique creates to a new momentum variable that stays in a simple $\\textbf{fixed vector space}$.","This design, together with a manifold preserving integrator, simplifies implementation and avoids inaccuracies created by approximations such as projections to tangent space and manifold, which were typically used in prior work, hence facilitating generation with high-fidelity and efficiency.","The resulting method achieves state-of-the-art performance on protein and RNA torsion angle generation and sophisticated torus datasets.","We also, arguably for the first time, tackle the generation of data on high-dimensional Special Orthogonal and Unitary groups, the latter essential for quantum problems."],"url":"http://arxiv.org/abs/2405.16381v1","category":"cs.LG"}
{"created":"2024-05-25 23:32:15","title":"FPsPIN: An FPGA-based Open-Hardware Research Platform for Processing in the Network","abstract":"In the era of post-Moore computing, network offload emerges as a solution to two challenges: the imperative for low-latency communication and the push towards hardware specialisation. Various methods have been employed to offload protocol- and data-processing onto network interface cards (NICs), from firmware modification to running full Linux on NICs for application execution. The sPIN project enables users to define handlers executed upon packet arrival. While simulations show sPIN's potential across diverse workloads, a full-system evaluation is lacking. This work presents FPsPIN, a full FPGA-based implementation of sPIN. FPsPIN is showcased through offloaded MPI datatype processing, achieving a 96% overlap ratio. FPsPIN provides an adaptable open-source research platform for researchers to conduct end-to-end experiments on smart NICs.","sentences":["In the era of post-Moore computing, network offload emerges as a solution to two challenges: the imperative for low-latency communication and the push towards hardware specialisation.","Various methods have been employed to offload protocol- and data-processing onto network interface cards (NICs), from firmware modification to running full Linux on NICs for application execution.","The sPIN project enables users to define handlers executed upon packet arrival.","While simulations show sPIN's potential across diverse workloads, a full-system evaluation is lacking.","This work presents FPsPIN, a full FPGA-based implementation of sPIN.","FPsPIN is showcased through offloaded MPI datatype processing, achieving a 96% overlap ratio.","FPsPIN provides an adaptable open-source research platform for researchers to conduct end-to-end experiments on smart NICs."],"url":"http://arxiv.org/abs/2405.16378v1","category":"cs.NI"}
{"created":"2024-05-25 22:59:59","title":"Isolate and then Identify: Rethinking Adaptive Group Testing","abstract":"Group testing (GT) is the art of identifying binary signals and the marketplace for exchanging new ideas for related fields such as unique-element counting, compressed sensing, traitor tracing, and geno-typing. A GT scheme can be nonadaptive or adaptive; the latter is preferred when latency is ess of an issue. To construct adaptive GT schemes, a popular strategy is to spend the majority of tests in the first few rounds to gain as much information as possible, and uses later rounds to refine details. In this paper, we propose a transparent strategy called \"isolate and then identify\" (I@I). In the first few rounds, I@I divides the population into teams until every team contains at most one sick person. Then, in the last round, I@I identifies the sick person in each team. Performance-wise, I@I is the first GT scheme that achieves the optimal coefficient $1/$capacity$(Z)$ for the $k \\log_2 (n/k)$ term in the number of tests when $Z$ is a generic channel corrupting the test outcomes. I@I follows a modular methodology whereby the isolating part and the identification part can be optimized separately.","sentences":["Group testing (GT) is the art of identifying binary signals and the marketplace for exchanging new ideas for related fields such as unique-element counting, compressed sensing, traitor tracing, and geno-typing.","A GT scheme can be nonadaptive or adaptive; the latter is preferred when latency is ess of an issue.","To construct adaptive GT schemes, a popular strategy is to spend the majority of tests in the first few rounds to gain as much information as possible, and uses later rounds to refine details.","In this paper, we propose a transparent strategy called \"isolate and then identify\" (I@I).","In the first few rounds, I@I divides the population into teams until every team contains at most one sick person.","Then, in the last round, I@I identifies the sick person in each team.","Performance-wise, I@I is the first GT scheme that achieves the optimal coefficient $1/$capacity$(Z)$ for the $k \\log_2 (n/k)$ term in the number of tests when $Z$ is a generic channel corrupting the test outcomes.","I@I follows a modular methodology whereby the isolating part and the identification part can be optimized separately."],"url":"http://arxiv.org/abs/2405.16374v1","category":"cs.IT"}
{"created":"2024-05-25 22:04:30","title":"On a multi-dimensional transport equation with nonlocal velocity and fractional dissipation","abstract":"This paper aims to investigate a multi-dimensional transport equation with nonlocal velocity and fractional dissipation.The balance between the nonlinearity and dissipation gives rise to three different cases, namely the subcritical, critical and supercritical ranges. We study those three cases and obtain a set of results containing local well-posedness, global smoothness, eventual regularity and finite-time blowup of smooth solutions.","sentences":["This paper aims to investigate a multi-dimensional transport equation with nonlocal velocity and fractional dissipation.","The balance between the nonlinearity and dissipation gives rise to three different cases, namely the subcritical, critical and supercritical ranges.","We study those three cases and obtain a set of results containing local well-posedness, global smoothness, eventual regularity and finite-time blowup of smooth solutions."],"url":"http://arxiv.org/abs/2405.16364v1","category":"math.AP"}
{"created":"2024-05-25 21:49:08","title":"Neural L1 Adaptive Control of Vehicle Lateral Dynamics","abstract":"We address the problem of stable and robust control of vehicles with lateral error dynamics for the application of lane keeping. Lane departure is the primary reason for half of the fatalities in road accidents, making the development of stable, adaptive and robust controllers a necessity. Traditional linear feedback controllers achieve satisfactory tracking performance, however, they exhibit unstable behavior when uncertainties are induced into the system. Any disturbance or uncertainty introduced to the steering-angle input can be catastrophic for the vehicle. Therefore, controllers must be developed to actively handle such uncertainties. In this work, we introduce a Neural L1 Adaptive controller (Neural-L1) which learns the uncertainties in the lateral error dynamics of a front-steered Ackermann vehicle and guarantees stability and robustness. Our contributions are threefold: i) We extend the theoretical results for guaranteed stability and robustness of conventional L1 Adaptive controllers to Neural-L1; ii) We implement a Neural-L1 for the lane keeping application which learns uncertainties in the dynamics accurately; iii)We evaluate the performance of Neural-L1 on a physics-based simulator, PyBullet, and conduct extensive real-world experiments with the F1TENTH platform to demonstrate superior reference trajectory tracking performance of Neural-L1 compared to other state-of-the-art controllers, in the presence of uncertainties. Our project page, including supplementary material and videos, can be found at https://mukhe027.github.io/Neural-Adaptive-Control/","sentences":["We address the problem of stable and robust control of vehicles with lateral error dynamics for the application of lane keeping.","Lane departure is the primary reason for half of the fatalities in road accidents, making the development of stable, adaptive and robust controllers a necessity.","Traditional linear feedback controllers achieve satisfactory tracking performance, however, they exhibit unstable behavior when uncertainties are induced into the system.","Any disturbance or uncertainty introduced to the steering-angle input can be catastrophic for the vehicle.","Therefore, controllers must be developed to actively handle such uncertainties.","In this work, we introduce a Neural L1 Adaptive controller (Neural-L1) which learns the uncertainties in the lateral error dynamics of a front-steered Ackermann vehicle and guarantees stability and robustness.","Our contributions are threefold: i)","We extend the theoretical results for guaranteed stability and robustness of conventional L1 Adaptive controllers to Neural-L1; ii) We implement a Neural-L1 for the lane keeping application which learns uncertainties in the dynamics accurately; iii)We evaluate the performance of Neural-L1 on a physics-based simulator, PyBullet, and conduct extensive real-world experiments with the F1TENTH platform to demonstrate superior reference trajectory tracking performance of Neural-L1 compared to other state-of-the-art controllers, in the presence of uncertainties.","Our project page, including supplementary material and videos, can be found at https://mukhe027.github.io/Neural-Adaptive-Control/"],"url":"http://arxiv.org/abs/2405.16358v1","category":"cs.RO"}
{"created":"2024-05-25 19:56:01","title":"R.A.C.E.: Robust Adversarial Concept Erasure for Secure Text-to-Image Diffusion Model","abstract":"In the evolving landscape of text-to-image (T2I) diffusion models, the remarkable capability to generate high-quality images from textual descriptions faces challenges with the potential misuse of reproducing sensitive content. To address this critical issue, we introduce Robust Adversarial Concept Erase (RACE), a novel approach designed to mitigate these risks by enhancing the robustness of concept erasure method for T2I models. RACE utilizes a sophisticated adversarial training framework to identify and mitigate adversarial text embeddings, significantly reducing the Attack Success Rate (ASR). Impressively, RACE achieves a 30 percentage point reduction in ASR for the ``nudity'' concept against the leading white-box attack method. Our extensive evaluations demonstrate RACE's effectiveness in defending against both white-box and black-box attacks, marking a significant advancement in protecting T2I diffusion models from generating inappropriate or misleading imagery. This work underlines the essential need for proactive defense measures in adapting to the rapidly advancing field of adversarial challenges.","sentences":["In the evolving landscape of text-to-image (T2I) diffusion models, the remarkable capability to generate high-quality images from textual descriptions faces challenges with the potential misuse of reproducing sensitive content.","To address this critical issue, we introduce Robust Adversarial Concept Erase (RACE), a novel approach designed to mitigate these risks by enhancing the robustness of concept erasure method for T2I models.","RACE utilizes a sophisticated adversarial training framework to identify and mitigate adversarial text embeddings, significantly reducing the Attack Success Rate (ASR).","Impressively, RACE achieves a 30 percentage point reduction in ASR for the ``nudity'' concept against the leading white-box attack method.","Our extensive evaluations demonstrate RACE's effectiveness in defending against both white-box and black-box attacks, marking a significant advancement in protecting T2I diffusion models from generating inappropriate or misleading imagery.","This work underlines the essential need for proactive defense measures in adapting to the rapidly advancing field of adversarial challenges."],"url":"http://arxiv.org/abs/2405.16341v1","category":"cs.CV"}
{"created":"2024-05-25 19:40:50","title":"Learning to Reason via Program Generation, Emulation, and Search","abstract":"Program synthesis with language models (LMs) has unlocked a large set of reasoning abilities; code-tuned LMs have proven adept at generating programs that solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word concatenation). However, not all reasoning tasks are easily expressible as code, e.g. tasks involving commonsense reasoning, moral decision-making, and sarcasm understanding. Our goal is to extend an LM's program synthesis skills to such tasks and evaluate the results via pseudo-programs, namely Python programs where some leaf function calls are left undefined. To that end, we propose, Code Generation and Emulated EXecution (CoGEX). CoGEX works by (1) training LMs to generate their own pseudo-programs, (2) teaching them to emulate their generated program's execution, including those leaf functions, allowing the LM's knowledge to fill in the execution gaps; and (3) using them to search over many programs to find an optimal one. To adapt the CoGEX model to a new task, we introduce a method for performing program search to find a single program whose pseudo-execution yields optimal performance when applied to all the instances of a given dataset. We show that our approach yields large improvements compared to standard in-context learning approaches on a battery of tasks, both algorithmic and soft reasoning. This result thus demonstrates that code synthesis can be applied to a much broader class of problems than previously considered. Our released dataset, fine-tuned models, and implementation can be found at \\url{https://github.com/nweir127/CoGEX}.","sentences":["Program synthesis with language models (LMs) has unlocked a large set of reasoning abilities; code-tuned LMs have proven adept at generating programs that solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word concatenation).","However, not all reasoning tasks are easily expressible as code, e.g. tasks involving commonsense reasoning, moral decision-making, and sarcasm understanding.","Our goal is to extend an LM's program synthesis skills to such tasks and evaluate the results via pseudo-programs, namely Python programs where some leaf function calls are left undefined.","To that end, we propose, Code Generation and Emulated EXecution (CoGEX).","CoGEX works by (1) training LMs to generate their own pseudo-programs, (2) teaching them to emulate their generated program's execution, including those leaf functions, allowing the LM's knowledge to fill in the execution gaps; and (3) using them to search over many programs to find an optimal one.","To adapt the CoGEX model to a new task, we introduce a method for performing program search to find a single program whose pseudo-execution yields optimal performance when applied to all the instances of a given dataset.","We show that our approach yields large improvements compared to standard in-context learning approaches on a battery of tasks, both algorithmic and soft reasoning.","This result thus demonstrates that code synthesis can be applied to a much broader class of problems than previously considered.","Our released dataset, fine-tuned models, and implementation can be found at \\url{https://github.com/nweir127/CoGEX}."],"url":"http://arxiv.org/abs/2405.16337v1","category":"cs.CL"}
{"created":"2024-05-25 19:20:15","title":"Devil's Advocate: Anticipatory Reflection for LLM Agents","abstract":"In this work, we introduce a novel approach that equips LLM agents with introspection, enhancing consistency and adaptability in solving complex tasks. Our approach prompts LLM agents to decompose a given task into manageable subtasks (i.e., to make a plan), and to continuously introspect upon the suitability and results of their actions. We implement a three-fold introspective intervention: 1) anticipatory reflection on potential failures and alternative remedy before action execution, 2) post-action alignment with subtask objectives and backtracking with remedy to ensure utmost effort in plan execution, and 3) comprehensive review upon plan completion for future strategy refinement. By deploying and experimenting with this methodology - a zero-shot approach - within WebArena for practical tasks in web environments, our agent demonstrates superior performance over existing zero-shot methods. The experimental results suggest that our introspection-driven approach not only enhances the agent's ability to navigate unanticipated challenges through a robust mechanism of plan execution, but also improves efficiency by reducing the number of trials and plan revisions needed to achieve a task.","sentences":["In this work, we introduce a novel approach that equips LLM agents with introspection, enhancing consistency and adaptability in solving complex tasks.","Our approach prompts LLM agents to decompose a given task into manageable subtasks (i.e., to make a plan), and to continuously introspect upon the suitability and results of their actions.","We implement a three-fold introspective intervention: 1) anticipatory reflection on potential failures and alternative remedy before action execution, 2) post-action alignment with subtask objectives and backtracking with remedy to ensure utmost effort in plan execution, and 3) comprehensive review upon plan completion for future strategy refinement.","By deploying and experimenting with this methodology - a zero-shot approach - within WebArena for practical tasks in web environments, our agent demonstrates superior performance over existing zero-shot methods.","The experimental results suggest that our introspection-driven approach not only enhances the agent's ability to navigate unanticipated challenges through a robust mechanism of plan execution, but also improves efficiency by reducing the number of trials and plan revisions needed to achieve a task."],"url":"http://arxiv.org/abs/2405.16334v1","category":"cs.AI"}
{"created":"2024-05-25 18:43:05","title":"SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs","abstract":"We propose SLoPe, a Double-Pruned Sparse Plus Lazy Low-rank Adapter Pretraining method for LLMs that improves the accuracy of sparse LLMs while accelerating their pretraining and inference and reducing their memory footprint. Sparse pretraining of LLMs reduces the accuracy of the model, to overcome this, prior work uses dense models during fine-tuning. SLoPe improves the accuracy of sparsely pretrained models by adding low-rank adapters in the final 1% iterations of pretraining without adding significant overheads to the model pretraining and inference. In addition, SLoPe uses a double-pruned backward pass formulation that prunes the transposed weight matrix using N:M sparsity structures to enable an accelerated sparse backward pass. SLoPe accelerates the training and inference of models with billions of parameters up to $1.14\\times$ and $1.34\\times$ respectively (OPT-33B and OPT-66B) while reducing their memory usage by up to $0.77\\times$ and $0.51\\times$ for training and inference respectively.","sentences":["We propose SLoPe, a Double-Pruned Sparse Plus Lazy Low-rank Adapter Pretraining method for LLMs that improves the accuracy of sparse LLMs while accelerating their pretraining and inference and reducing their memory footprint.","Sparse pretraining of LLMs reduces the accuracy of the model, to overcome this, prior work uses dense models during fine-tuning.","SLoPe improves the accuracy of sparsely pretrained models by adding low-rank adapters in the final 1% iterations of pretraining without adding significant overheads to the model pretraining and inference.","In addition, SLoPe uses a double-pruned backward pass formulation that prunes the transposed weight matrix using N:M sparsity structures to enable an accelerated sparse backward pass.","SLoPe accelerates the training and inference of models with billions of parameters up to $1.14\\times$ and $1.34\\times$ respectively (OPT-33B and OPT-66B) while reducing their memory usage by up to $0.77\\times$ and $0.51\\times$ for training and inference respectively."],"url":"http://arxiv.org/abs/2405.16325v1","category":"cs.LG"}
{"created":"2024-05-25 16:03:07","title":"Intensity adaptive optics","abstract":"Adaptive optics (AO) is a powerful tool used in a wide range of research areas spanning from aerospace to microscopy. To date, AO has largely been applied to optical phase aberration correction, with recent advances extending to include the vectorial properties of light. However, intensity errors widely exist in optical systems, yet their associated correction methods are still very much in their infancy. Here, we propose a new adaptive optics method that is termed intensity adaptive optics (I-AO), which features a dual-feedback loop for intensity aberration correction that addresses both intensity uniformity and the overall intensity. We demonstrate that I-AO can operate in both sensor-based and sensorless regimes and validate its feasibility by quantitatively analysing the quality of the focus of an aberrated optical system. This technique expands the AO toolkit, broadens its scope of application, and opens a new avenue for next-generation AO innovations.","sentences":["Adaptive optics (AO) is a powerful tool used in a wide range of research areas spanning from aerospace to microscopy.","To date, AO has largely been applied to optical phase aberration correction, with recent advances extending to include the vectorial properties of light.","However, intensity errors widely exist in optical systems, yet their associated correction methods are still very much in their infancy.","Here, we propose a new adaptive optics method that is termed intensity adaptive optics (I-AO), which features a dual-feedback loop for intensity aberration correction that addresses both intensity uniformity and the overall intensity.","We demonstrate that I-AO can operate in both sensor-based and sensorless regimes and validate its feasibility by quantitatively analysing the quality of the focus of an aberrated optical system.","This technique expands the AO toolkit, broadens its scope of application, and opens a new avenue for next-generation AO innovations."],"url":"http://arxiv.org/abs/2405.16289v1","category":"physics.optics"}
{"created":"2024-05-25 14:56:30","title":"Layer-Aware Analysis of Catastrophic Overfitting: Revealing the Pseudo-Robust Shortcut Dependency","abstract":"Catastrophic overfitting (CO) presents a significant challenge in single-step adversarial training (AT), manifesting as highly distorted deep neural networks (DNNs) that are vulnerable to multi-step adversarial attacks. However, the underlying factors that lead to the distortion of decision boundaries remain unclear. In this work, we delve into the specific changes within different DNN layers and discover that during CO, the former layers are more susceptible, experiencing earlier and greater distortion, while the latter layers show relative insensitivity. Our analysis further reveals that this increased sensitivity in former layers stems from the formation of pseudo-robust shortcuts, which alone can impeccably defend against single-step adversarial attacks but bypass genuine-robust learning, resulting in distorted decision boundaries. Eliminating these shortcuts can partially restore robustness in DNNs from the CO state, thereby verifying that dependence on them triggers the occurrence of CO. This understanding motivates us to implement adaptive weight perturbations across different layers to hinder the generation of pseudo-robust shortcuts, consequently mitigating CO. Extensive experiments demonstrate that our proposed method, Layer-Aware Adversarial Weight Perturbation (LAP), can effectively prevent CO and further enhance robustness.","sentences":["Catastrophic overfitting (CO) presents a significant challenge in single-step adversarial training (AT), manifesting as highly distorted deep neural networks (DNNs) that are vulnerable to multi-step adversarial attacks.","However, the underlying factors that lead to the distortion of decision boundaries remain unclear.","In this work, we delve into the specific changes within different DNN layers and discover that during CO, the former layers are more susceptible, experiencing earlier and greater distortion, while the latter layers show relative insensitivity.","Our analysis further reveals that this increased sensitivity in former layers stems from the formation of pseudo-robust shortcuts, which alone can impeccably defend against single-step adversarial attacks but bypass genuine-robust learning, resulting in distorted decision boundaries.","Eliminating these shortcuts can partially restore robustness in DNNs from the CO state, thereby verifying that dependence on them triggers the occurrence of CO.","This understanding motivates us to implement adaptive weight perturbations across different layers to hinder the generation of pseudo-robust shortcuts, consequently mitigating CO.","Extensive experiments demonstrate that our proposed method, Layer-Aware Adversarial Weight Perturbation (LAP), can effectively prevent CO and further enhance robustness."],"url":"http://arxiv.org/abs/2405.16262v1","category":"cs.LG"}
{"created":"2024-05-25 14:36:33","title":"GeoAdaLer: Geometric Insights into Adaptive Stochastic Gradient Descent Algorithms","abstract":"The Adam optimization method has achieved remarkable success in addressing contemporary challenges in stochastic optimization. This method falls within the realm of adaptive sub-gradient techniques, yet the underlying geometric principles guiding its performance have remained shrouded in mystery, and have long confounded researchers. In this paper, we introduce GeoAdaLer (Geometric Adaptive Learner), a novel adaptive learning method for stochastic gradient descent optimization, which draws from the geometric properties of the optimization landscape. Beyond emerging as a formidable contender, the proposed method extends the concept of adaptive learning by introducing a geometrically inclined approach that enhances the interpretability and effectiveness in complex optimization scenarios","sentences":["The Adam optimization method has achieved remarkable success in addressing contemporary challenges in stochastic optimization.","This method falls within the realm of adaptive sub-gradient techniques, yet the underlying geometric principles guiding its performance have remained shrouded in mystery, and have long confounded researchers.","In this paper, we introduce GeoAdaLer (Geometric Adaptive Learner), a novel adaptive learning method for stochastic gradient descent optimization, which draws from the geometric properties of the optimization landscape.","Beyond emerging as a formidable contender, the proposed method extends the concept of adaptive learning by introducing a geometrically inclined approach that enhances the interpretability and effectiveness in complex optimization scenarios"],"url":"http://arxiv.org/abs/2405.16255v1","category":"cs.LG"}
{"created":"2024-05-25 14:11:44","title":"AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning","abstract":"Large Language Models (LLM) based agents have shown promise in autonomously completing tasks across various domains, e.g., robotics, games, and web navigation. However, these agents typically require elaborate design and expert prompts to solve tasks in specific domains, which limits their adaptability. We introduce AutoManual, a framework enabling LLM agents to autonomously build their understanding through interaction and adapt to new environments. AutoManual categorizes environmental knowledge into diverse rules and optimizes them in an online fashion by two agents: 1) The Planner codes actionable plans based on current rules for interacting with the environment. 2) The Builder updates the rules through a well-structured rule system that facilitates online rule management and essential detail retention. To mitigate hallucinations in managing rules, we introduce \\textit{case-conditioned prompting} strategy for the Builder. Finally, the Formulator agent compiles these rules into a comprehensive manual. The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable. Given only one simple demonstration, AutoManual significantly improves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with GPT-3.5-turbo on ALFWorld benchmark tasks. The source code will be available soon.","sentences":["Large Language Models (LLM) based agents have shown promise in autonomously completing tasks across various domains, e.g., robotics, games, and web navigation.","However, these agents typically require elaborate design and expert prompts to solve tasks in specific domains, which limits their adaptability.","We introduce AutoManual, a framework enabling LLM agents to autonomously build their understanding through interaction and adapt to new environments.","AutoManual categorizes environmental knowledge into diverse rules and optimizes them in an online fashion by two agents: 1) The Planner codes actionable plans based on current rules for interacting with the environment.","2) The Builder updates the rules through a well-structured rule system that facilitates online rule management and essential detail retention.","To mitigate hallucinations in managing rules, we introduce \\textit{case-conditioned prompting} strategy for the Builder.","Finally, the Formulator agent compiles these rules into a comprehensive manual.","The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable.","Given only one simple demonstration, AutoManual significantly improves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with GPT-3.5-turbo on ALFWorld benchmark tasks.","The source code will be available soon."],"url":"http://arxiv.org/abs/2405.16247v1","category":"cs.AI"}
{"created":"2024-05-25 13:54:34","title":"N-BVH: Neural ray queries with bounding volume hierarchies","abstract":"Neural representations have shown spectacular ability to compress complex signals in a fraction of the raw data size. In 3D computer graphics, the bulk of a scene's memory usage is due to polygons and textures, making them ideal candidates for neural compression. Here, the main challenge lies in finding good trade-offs between efficient compression and cheap inference while minimizing training time. In the context of rendering, we adopt a ray-centric approach to this problem and devise N-BVH, a neural compression architecture designed to answer arbitrary ray queries in 3D. Our compact model is learned from the input geometry and substituted for it whenever a ray intersection is queried by a path-tracing engine. While prior neural compression methods have focused on point queries, ours proposes neural ray queries that integrate seamlessly into standard ray-tracing pipelines. At the core of our method, we employ an adaptive BVH-driven probing scheme to optimize the parameters of a multi-resolution hash grid, focusing its neural capacity on the sparse 3D occupancy swept by the original surfaces. As a result, our N-BVH can serve accurate ray queries from a representation that is more than an order of magnitude more compact, providing faithful approximations of visibility, depth, and appearance attributes. The flexibility of our method allows us to combine and overlap neural and non-neural entities within the same 3D scene and extends to appearance level of detail.","sentences":["Neural representations have shown spectacular ability to compress complex signals in a fraction of the raw data size.","In 3D computer graphics, the bulk of a scene's memory usage is due to polygons and textures, making them ideal candidates for neural compression.","Here, the main challenge lies in finding good trade-offs between efficient compression and cheap inference while minimizing training time.","In the context of rendering, we adopt a ray-centric approach to this problem and devise N-BVH, a neural compression architecture designed to answer arbitrary ray queries in 3D.","Our compact model is learned from the input geometry and substituted for it whenever a ray intersection is queried by a path-tracing engine.","While prior neural compression methods have focused on point queries, ours proposes neural ray queries that integrate seamlessly into standard ray-tracing pipelines.","At the core of our method, we employ an adaptive BVH-driven probing scheme to optimize the parameters of a multi-resolution hash grid, focusing its neural capacity on the sparse 3D occupancy swept by the original surfaces.","As a result, our N-BVH can serve accurate ray queries from a representation that is more than an order of magnitude more compact, providing faithful approximations of visibility, depth, and appearance attributes.","The flexibility of our method allows us to combine and overlap neural and non-neural entities within the same 3D scene and extends to appearance level of detail."],"url":"http://arxiv.org/abs/2405.16237v1","category":"cs.GR"}
{"created":"2024-05-25 13:39:45","title":"Active oversight and quality control in standard Bayesian optimization for autonomous experiments","abstract":"The fusion of experimental automation and machine learning has catalyzed a new era in materials research, prominently featuring Gaussian Process Bayesian Optimization (GPBO) driven autonomous experiments navigating complex experimental conditions for accelerated scientific discovery. In traditional GPBO-driven experiments, a predefined scalarizer function is often required to preprocess the experimental data, transforming non-scalar raw data into scalar descriptors for GP training. However, such predefined scalarizer functions have limitations, which likely fail to accommodate the diversity and complexity of real-world experimental data, potentially skewing experimental outcomes. Thus, oversight and quality control are necessitated over the process to avoid GPBO from being misled by low quality scalarizers. To address the limitation, we introduce a Dual-GP approach that enhances traditional GPBO by adding a secondary surrogate model to dynamically constrain the experimental space based on real-time assessments of the raw experimental data. This Dual-GP approach enhances the optimization efficiency of traditional GPBO by isolating more promising space for BO sampling and more valuable experimental data for primary GP training. We also incorporate a flexible, human-in-the-loop intervention method in the Dual-GP workflow to adjust for unanticipated results. We demonstrate the effectiveness of the Dual-GP model with synthetic model data and implement this approach in autonomous pulsed laser deposition experimental data. This Dual-GP approach has broad applicability in diverse GPBO-driven experimental settings, providing a more adaptable and precise framework for refining autonomous experimentation for more efficient optimization.","sentences":["The fusion of experimental automation and machine learning has catalyzed a new era in materials research, prominently featuring Gaussian Process Bayesian Optimization (GPBO) driven autonomous experiments navigating complex experimental conditions for accelerated scientific discovery.","In traditional GPBO-driven experiments, a predefined scalarizer function is often required to preprocess the experimental data, transforming non-scalar raw data into scalar descriptors for GP training.","However, such predefined scalarizer functions have limitations, which likely fail to accommodate the diversity and complexity of real-world experimental data, potentially skewing experimental outcomes.","Thus, oversight and quality control are necessitated over the process to avoid GPBO from being misled by low quality scalarizers.","To address the limitation, we introduce a Dual-GP approach that enhances traditional GPBO by adding a secondary surrogate model to dynamically constrain the experimental space based on real-time assessments of the raw experimental data.","This Dual-GP approach enhances the optimization efficiency of traditional GPBO by isolating more promising space for BO sampling and more valuable experimental data for primary GP training.","We also incorporate a flexible, human-in-the-loop intervention method in the Dual-GP workflow to adjust for unanticipated results.","We demonstrate the effectiveness of the Dual-GP model with synthetic model data and implement this approach in autonomous pulsed laser deposition experimental data.","This Dual-GP approach has broad applicability in diverse GPBO-driven experimental settings, providing a more adaptable and precise framework for refining autonomous experimentation for more efficient optimization."],"url":"http://arxiv.org/abs/2405.16230v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-25 12:59:46","title":"Pin the loop taut: a one-player topologame","abstract":"We introduce a $1$-player game called \\emph{pin the loop} and analyse its complexity. A loop is a generic immersion of the circle in a surface, considered up to isotopy. A loop is taut when it is minimally intersecting in its homotopy class. A pinning set of a loop is a set of points $P$ in the surface avoiding the loop, such that the loop is taut in the surface punctured at $P$. The pinning number is the minimal cardinal of its pinning sets, and a pinning set is optimal if it has that cardinal.   We show that the decision problem associated to computing the pinning number of a plane loop is \\texttt{NP-complete}. We implement a polynomial algorithm to check if a given point-set is pinning, adapting a method of Birman--Series for computing intersection numbers of curves in surfaces. After improving a theorem of Hass--Scott characterising taut loops in surfaces, we reduce the problem to a boolean formula whose solutions correspond to the pinning sets. Finally, we reduce the vertex-cover problem for graphs to an instance of the pinning problem for plane loops.   Our results imply that given a closed geodesic in a complete Riemannian surface with punctures, it is hard to guess the location of the punctures from the sole isotopy class of the geodesic in the compactified surface.","sentences":["We introduce a $1$-player game called \\emph{pin the loop} and analyse its complexity.","A loop is a generic immersion of the circle in a surface, considered up to isotopy.","A loop is taut when it is minimally intersecting in its homotopy class.","A pinning set of a loop is a set of points $P$ in the surface avoiding the loop, such that the loop is taut in the surface punctured at $P$.","The pinning number is the minimal cardinal of its pinning sets, and a pinning set is optimal if it has that cardinal.   ","We show that the decision problem associated to computing the pinning number of a plane loop is \\texttt{NP-complete}.","We implement a polynomial algorithm to check if a given point-set is pinning, adapting a method of Birman--Series for computing intersection numbers of curves in surfaces.","After improving a theorem of Hass--Scott characterising taut loops in surfaces, we reduce the problem to a boolean formula whose solutions correspond to the pinning sets.","Finally, we reduce the vertex-cover problem for graphs to an instance of the pinning problem for plane loops.   ","Our results imply that given a closed geodesic in a complete Riemannian surface with punctures, it is hard to guess the location of the punctures from the sole isotopy class of the geodesic in the compactified surface."],"url":"http://arxiv.org/abs/2405.16216v1","category":"math.GT"}
{"created":"2024-05-25 12:57:17","title":"Individual and Contextual Variables of Cyber Security Behaviour -- An empirical analysis of national culture, industry, organisation, and individual variables of (in)secure human behaviour","abstract":"Cyber security incidents are increasing and humans play an important role in reducing their likelihood and impact. We identify a skewed focus towards technical aspects of cyber security in the literature, whereas factors influencing the secure behaviour of individuals require additional research. These factors span across both the individual level and the contextual level in which the people are situated. We analyse two datasets of a total of 37,075 records from a) self-reported security behaviours across the EU, and b) observed phishing-related behaviours from the industry security awareness training programmes. We identify that national culture, industry type, and organisational security culture play are influential Variables (antecedents) of individuals' security behaviour at contextual level. Whereas, demographics (age, gender, and level or urbanisation) and security-specific factors (security awareness, security knowledge, and prior experience with security incidents) are found to be influential variables of security behaviour at individual level. Our findings have implications for both research and practice as they fill a gap in the literature and provide concrete statistical evidence on the variables which influence security behaviour. Moreover, findings provides practical insights for organisations regarding the susceptibility of groups of people to insecure behaviour. Consequently, organisations can tailor their security training and awareness efforts (e.g., through behaviour change interventions and/or appropriate employee group profiles), adapt their communications (e.g., of information security policies), and customise their interventions according to national culture characteristics to improve security behaviour.","sentences":["Cyber security incidents are increasing and humans play an important role in reducing their likelihood and impact.","We identify a skewed focus towards technical aspects of cyber security in the literature, whereas factors influencing the secure behaviour of individuals require additional research.","These factors span across both the individual level and the contextual level in which the people are situated.","We analyse two datasets of a total of 37,075 records from a) self-reported security behaviours across the EU, and b) observed phishing-related behaviours from the industry security awareness training programmes.","We identify that national culture, industry type, and organisational security culture play are influential Variables (antecedents) of individuals' security behaviour at contextual level.","Whereas, demographics (age, gender, and level or urbanisation) and security-specific factors (security awareness, security knowledge, and prior experience with security incidents) are found to be influential variables of security behaviour at individual level.","Our findings have implications for both research and practice as they fill a gap in the literature and provide concrete statistical evidence on the variables which influence security behaviour.","Moreover, findings provides practical insights for organisations regarding the susceptibility of groups of people to insecure behaviour.","Consequently, organisations can tailor their security training and awareness efforts (e.g., through behaviour change interventions and/or appropriate employee group profiles), adapt their communications (e.g., of information security policies), and customise their interventions according to national culture characteristics to improve security behaviour."],"url":"http://arxiv.org/abs/2405.16215v1","category":"cs.CR"}
{"created":"2024-05-25 12:27:21","title":"Evolutionary Large Language Model for Automated Feature Transformation","abstract":"Feature transformation aims to reconstruct the feature space of raw features to enhance the performance of downstream models. However, the exponential growth in the combinations of features and operations poses a challenge, making it difficult for existing methods to efficiently explore a wide space. Additionally, their optimization is solely driven by the accuracy of downstream models in specific domains, neglecting the acquisition of general feature knowledge. To fill this research gap, we propose an evolutionary LLM framework for automated feature transformation. This framework consists of two parts: 1) constructing a multi-population database through an RL data collector while utilizing evolutionary algorithm strategies for database maintenance, and 2) utilizing the ability of Large Language Model (LLM) in sequence understanding, we employ few-shot prompts to guide LLM in generating superior samples based on feature transformation sequence distinction. Leveraging the multi-population database initially provides a wide search scope to discover excellent populations. Through culling and evolution, the high-quality populations are afforded greater opportunities, thereby furthering the pursuit of optimal individuals. Through the integration of LLMs with evolutionary algorithms, we achieve efficient exploration within a vast space, while harnessing feature knowledge to propel optimization, thus realizing a more adaptable search paradigm. Finally, we empirically demonstrate the effectiveness and generality of our proposed method.","sentences":["Feature transformation aims to reconstruct the feature space of raw features to enhance the performance of downstream models.","However, the exponential growth in the combinations of features and operations poses a challenge, making it difficult for existing methods to efficiently explore a wide space.","Additionally, their optimization is solely driven by the accuracy of downstream models in specific domains, neglecting the acquisition of general feature knowledge.","To fill this research gap, we propose an evolutionary LLM framework for automated feature transformation.","This framework consists of two parts: 1) constructing a multi-population database through an RL data collector while utilizing evolutionary algorithm strategies for database maintenance, and 2) utilizing the ability of Large Language Model (LLM) in sequence understanding, we employ few-shot prompts to guide LLM in generating superior samples based on feature transformation sequence distinction.","Leveraging the multi-population database initially provides a wide search scope to discover excellent populations.","Through culling and evolution, the high-quality populations are afforded greater opportunities, thereby furthering the pursuit of optimal individuals.","Through the integration of LLMs with evolutionary algorithms, we achieve efficient exploration within a vast space, while harnessing feature knowledge to propel optimization, thus realizing a more adaptable search paradigm.","Finally, we empirically demonstrate the effectiveness and generality of our proposed method."],"url":"http://arxiv.org/abs/2405.16203v1","category":"cs.LG"}
{"created":"2024-05-25 11:57:43","title":"Adaptive $Q$-Network: On-the-fly Target Selection for Deep Reinforcement Learning","abstract":"Deep Reinforcement Learning (RL) is well known for being highly sensitive to hyperparameters, requiring practitioners substantial efforts to optimize them for the problem at hand. In recent years, the field of automated Reinforcement Learning (AutoRL) has grown in popularity by trying to address this issue. However, these approaches typically hinge on additional samples to select well-performing hyperparameters, hindering sample-efficiency and practicality in RL. Furthermore, most AutoRL methods are heavily based on already existing AutoML methods, which were originally developed neglecting the additional challenges inherent to RL due to its non-stationarities. In this work, we propose a new approach for AutoRL, called Adaptive $Q$-Network (AdaQN), that is tailored to RL to take into account the non-stationarity of the optimization procedure without requiring additional samples. AdaQN learns several $Q$-functions, each one trained with different hyperparameters, which are updated online using the $Q$-function with the smallest approximation error as a shared target. Our selection scheme simultaneously handles different hyperparameters while coping with the non-stationarity induced by the RL optimization procedure and being orthogonal to any critic-based RL algorithm. We demonstrate that AdaQN is theoretically sound and empirically validate it in MuJoCo control problems, showing benefits in sample-efficiency, overall performance, training stability, and robustness to stochasticity.","sentences":["Deep Reinforcement Learning (RL) is well known for being highly sensitive to hyperparameters, requiring practitioners substantial efforts to optimize them for the problem at hand.","In recent years, the field of automated Reinforcement Learning (AutoRL) has grown in popularity by trying to address this issue.","However, these approaches typically hinge on additional samples to select well-performing hyperparameters, hindering sample-efficiency and practicality in RL.","Furthermore, most AutoRL methods are heavily based on already existing AutoML methods, which were originally developed neglecting the additional challenges inherent to RL due to its non-stationarities.","In this work, we propose a new approach for AutoRL, called Adaptive $Q$-Network (AdaQN), that is tailored to RL to take into account the non-stationarity of the optimization procedure without requiring additional samples.","AdaQN learns several $Q$-functions, each one trained with different hyperparameters, which are updated online using the $Q$-function with the smallest approximation error as a shared target.","Our selection scheme simultaneously handles different hyperparameters while coping with the non-stationarity induced by the RL optimization procedure and being orthogonal to any critic-based RL algorithm.","We demonstrate that AdaQN is theoretically sound and empirically validate it in MuJoCo control problems, showing benefits in sample-efficiency, overall performance, training stability, and robustness to stochasticity."],"url":"http://arxiv.org/abs/2405.16195v1","category":"cs.LG"}
{"created":"2024-05-25 09:51:53","title":"A hybrid approach to model reduction of Generalized Langevin Dynamics","abstract":"We consider a classical model of non-equilibrium statistical mechanics accounting for non-Markovian effects, which is referred to as the Generalized Langevin Equation in the literature. We derive reduced Markovian descriptions obtained through the neglection of inertial terms and/or heat bath variables. The adopted reduction scheme relies on the framework of the Invariant Manifold method, which allows to retain the slow degrees of freedom from a multiscale dynamical system. Our approach is also rooted on the Fluctuation-Dissipation Theorem, which helps preserve the proper dissipative structure of the reduced dynamics. We highlight the appropriate time scalings introduced within our procedure, and also prove the commutativity of selected reduction paths.","sentences":["We consider a classical model of non-equilibrium statistical mechanics accounting for non-Markovian effects, which is referred to as the Generalized Langevin Equation in the literature.","We derive reduced Markovian descriptions obtained through the neglection of inertial terms and/or heat bath variables.","The adopted reduction scheme relies on the framework of the Invariant Manifold method, which allows to retain the slow degrees of freedom from a multiscale dynamical system.","Our approach is also rooted on the Fluctuation-Dissipation Theorem, which helps preserve the proper dissipative structure of the reduced dynamics.","We highlight the appropriate time scalings introduced within our procedure, and also prove the commutativity of selected reduction paths."],"url":"http://arxiv.org/abs/2405.16157v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-25 09:43:33","title":"SuDA: Support-based Domain Adaptation for Sim2Real Motion Capture with Flexible Sensors","abstract":"Flexible sensors hold promise for human motion capture (MoCap), offering advantages such as wearability, privacy preservation, and minimal constraints on natural movement. However, existing flexible sensor-based MoCap methods rely on deep learning and necessitate large and diverse labeled datasets for training. These data typically need to be collected in MoCap studios with specialized equipment and substantial manual labor, making them difficult and expensive to obtain at scale. Thanks to the high-linearity of flexible sensors, we address this challenge by proposing a novel Sim2Real Mocap solution based on domain adaptation, eliminating the need for labeled data yet achieving comparable accuracy to supervised learning. Our solution relies on a novel Support-based Domain Adaptation method, namely SuDA, which aligns the supports of the predictive functions rather than the instance-dependent distributions between the source and target domains. Extensive experimental results demonstrate the effectiveness of our method andits superiority over state-of-the-art distribution-based domain adaptation methods in our task.","sentences":["Flexible sensors hold promise for human motion capture (MoCap), offering advantages such as wearability, privacy preservation, and minimal constraints on natural movement.","However, existing flexible sensor-based MoCap methods rely on deep learning and necessitate large and diverse labeled datasets for training.","These data typically need to be collected in MoCap studios with specialized equipment and substantial manual labor, making them difficult and expensive to obtain at scale.","Thanks to the high-linearity of flexible sensors, we address this challenge by proposing a novel Sim2Real","Mocap solution based on domain adaptation, eliminating the need for labeled data yet achieving comparable accuracy to supervised learning.","Our solution relies on a novel Support-based Domain Adaptation method, namely SuDA, which aligns the supports of the predictive functions rather than the instance-dependent distributions between the source and target domains.","Extensive experimental results demonstrate the effectiveness of our method andits superiority over state-of-the-art distribution-based domain adaptation methods in our task."],"url":"http://arxiv.org/abs/2405.16152v1","category":"cs.CV"}
{"created":"2024-05-25 09:42:58","title":"5W1H Extraction With Large Language Models","abstract":"The extraction of essential news elements through the 5W1H framework (\\textit{What}, \\textit{When}, \\textit{Where}, \\textit{Why}, \\textit{Who}, and \\textit{How}) is critical for event extraction and text summarization. The advent of Large language models (LLMs) such as ChatGPT presents an opportunity to address language-related tasks through simple prompts without fine-tuning models with much time. While ChatGPT has encountered challenges in processing longer news texts and analyzing specific attributes in context, especially answering questions about \\textit{What}, \\textit{Why}, and \\textit{How}. The effectiveness of extraction tasks is notably dependent on high-quality human-annotated datasets. However, the absence of such datasets for the 5W1H extraction increases the difficulty of fine-tuning strategies based on open-source LLMs. To address these limitations, first, we annotate a high-quality 5W1H dataset based on four typical news corpora (\\textit{CNN/DailyMail}, \\textit{XSum}, \\textit{NYT}, \\textit{RA-MDS}); second, we design several strategies from zero-shot/few-shot prompting to efficient fine-tuning to conduct 5W1H aspects extraction from the original news documents. The experimental results demonstrate that the performance of the fine-tuned models on our labelled dataset is superior to the performance of ChatGPT. Furthermore, we also explore the domain adaptation capability by testing the source-domain (e.g. NYT) models on the target domain corpus (e.g. CNN/DailyMail) for the task of 5W1H extraction.","sentences":["The extraction of essential news elements through the 5W1H framework (\\textit{What}, \\textit{When}, \\textit{Where}, \\textit{Why}, \\textit{Who}, and \\textit{How}) is critical for event extraction and text summarization.","The advent of Large language models (LLMs) such as ChatGPT presents an opportunity to address language-related tasks through simple prompts without fine-tuning models with much time.","While ChatGPT has encountered challenges in processing longer news texts and analyzing specific attributes in context, especially answering questions about \\textit{What}, \\textit{Why}, and \\textit{How}.","The effectiveness of extraction tasks is notably dependent on high-quality human-annotated datasets.","However, the absence of such datasets for the 5W1H extraction increases the difficulty of fine-tuning strategies based on open-source LLMs.","To address these limitations, first, we annotate a high-quality 5W1H dataset based on four typical news corpora (\\textit{CNN/DailyMail}, \\textit{XSum}, \\textit{NYT}, \\textit{RA-MDS}); second, we design several strategies from zero-shot/few-shot prompting to efficient fine-tuning to conduct 5W1H aspects extraction from the original news documents.","The experimental results demonstrate that the performance of the fine-tuned models on our labelled dataset is superior to the performance of ChatGPT.","Furthermore, we also explore the domain adaptation capability by testing the source-domain (e.g. NYT) models on the target domain corpus (e.g. CNN/DailyMail) for the task of 5W1H extraction."],"url":"http://arxiv.org/abs/2405.16150v1","category":"cs.CL"}
{"created":"2024-05-25 09:34:59","title":"Dual-Adapter: Training-free Dual Adaptation for Few-shot Out-of-Distribution Detection","abstract":"We study the problem of few-shot out-of-distribution (OOD) detection, which aims to detect OOD samples from unseen categories during inference time with only a few labeled in-domain (ID) samples. Existing methods mainly focus on training task-aware prompts for OOD detection. However, training on few-shot data may cause severe overfitting and textual prompts alone may not be enough for effective detection. To tackle these problems, we propose a prior-based Training-free Dual Adaptation method (Dual-Adapter) to detect OOD samples from both textual and visual perspectives. Specifically, Dual-Adapter first extracts the most significant channels as positive features and designates the remaining less relevant channels as negative features. Then, it constructs both a positive adapter and a negative adapter from a dual perspective, thereby better leveraging previously outlooked or interfering features in the training dataset. In this way, Dual-Adapter can inherit the advantages of CLIP not having to train, but also excels in distinguishing between ID and OOD samples. Extensive experimental results on four benchmark datasets demonstrate the superiority of Dual-Adapter.","sentences":["We study the problem of few-shot out-of-distribution (OOD) detection, which aims to detect OOD samples from unseen categories during inference time with only a few labeled in-domain (ID) samples.","Existing methods mainly focus on training task-aware prompts for OOD detection.","However, training on few-shot data may cause severe overfitting and textual prompts alone may not be enough for effective detection.","To tackle these problems, we propose a prior-based Training-free Dual Adaptation method (Dual-Adapter) to detect OOD samples from both textual and visual perspectives.","Specifically, Dual-Adapter first extracts the most significant channels as positive features and designates the remaining less relevant channels as negative features.","Then, it constructs both a positive adapter and a negative adapter from a dual perspective, thereby better leveraging previously outlooked or interfering features in the training dataset.","In this way, Dual-Adapter can inherit the advantages of CLIP not having to train, but also excels in distinguishing between ID and OOD samples.","Extensive experimental results on four benchmark datasets demonstrate the superiority of Dual-Adapter."],"url":"http://arxiv.org/abs/2405.16146v1","category":"cs.CV"}
{"created":"2024-05-25 09:19:47","title":"Higher Degree Inexact Model for Optimization problems","abstract":"In this paper, it was proposed a new concept of the inexact higher degree $(\\delta, L, q)$-model of a function that is a generalization of the inexact $(\\delta, L)$-model, $(\\delta, L)$-oracle and $(\\delta, L)$-oracle of degree $q \\in [0,2)$. Some examples were provided to illustrate the proposed new model. Adaptive inexact gradient and fast gradient methods for convex and strongly convex functions were constructed and analyzed using the new proposed inexact model. A universal fast gradient method that allows solving optimization problems with a weaker level of smoothness, among them non-smooth problems was proposed. For convex optimization problems it was proved that the proposed gradient and fast gradient methods could be converged with rates $O\\left(\\frac{1}{k} + \\frac{\\delta}{k^{q/2}}\\right)$ and $O\\left(\\frac{1}{k^2} + \\frac{\\delta}{k^{(3q-2)/2}}\\right)$, respectively. For the gradient method, the coefficient of $\\delta$ diminishes with $k$, and for the fast gradient method, there is no error accumulation for $q \\geq 2/3$. It proposed a definition of an inexact higher degree oracle for strongly convex functions and a projected gradient method using this inexact oracle. For variational inequalities and saddle point problems, a higher degree inexact model and an adaptive method called Generalized Mirror Prox to solve such class of problems using the proposed inexact model were proposed. Some numerical experiments were conducted to demonstrate the effectiveness of the proposed inexact model, we test the universal fast gradient method to solve some non-smooth problems with a geometrical nature.","sentences":["In this paper, it was proposed a new concept of the inexact higher degree $(\\delta, L, q)$-model of a function that is a generalization of the inexact $(\\delta, L)$-model, $(\\delta, L)$-oracle and $(\\delta, L)$-oracle of degree $q \\in [0,2)$. Some examples were provided to illustrate the proposed new model.","Adaptive inexact gradient and fast gradient methods for convex and strongly convex functions were constructed and analyzed using the new proposed inexact model.","A universal fast gradient method that allows solving optimization problems with a weaker level of smoothness, among them non-smooth problems was proposed.","For convex optimization problems it was proved that the proposed gradient and fast gradient methods could be converged with rates $O\\left(\\frac{1}{k} + \\frac{\\delta}{k^{q/2}}\\right)$ and $O\\left(\\frac{1}{k^2} + \\frac{\\delta}{k^{(3q-2)/2}}\\right)$, respectively.","For the gradient method, the coefficient of $\\delta$ diminishes with $k$, and for the fast gradient method, there is no error accumulation for $q \\geq 2/3$. It proposed a definition of an inexact higher degree oracle for strongly convex functions and a projected gradient method using this inexact oracle.","For variational inequalities and saddle point problems, a higher degree inexact model and an adaptive method called Generalized Mirror Prox to solve such class of problems using the proposed inexact model were proposed.","Some numerical experiments were conducted to demonstrate the effectiveness of the proposed inexact model, we test the universal fast gradient method to solve some non-smooth problems with a geometrical nature."],"url":"http://arxiv.org/abs/2405.16140v1","category":"math.OC"}
{"created":"2024-05-25 09:15:41","title":"Renormalized Internally-Contracted Multireference Coupled Cluster with Perturbative Triples","abstract":"In this work, we combine the many-body formulation of the internally contracted multireference coupled cluster (ic-MRCC) method with Evangelista's multireference formulation of the driven similarity renormalization group (DSRG). The DSRG method can be viewed as a unitary multireference coupled cluster theory, which renormalizes the amplitudes based on a flow equation approach to eliminate numerical instabilities. We extend this approach by demonstrating that the unitary flow equation approach can be adapted for nonunitary transformations, rationalizing the renormalization of ic-MRCC amplitudes. We denote the new approach, the renormalized ic-MRCC (ric-MRCC) method. To achieve high accuracy with a reasonable computational cost, we introduce a new approximation to the Baker-Campbell-Hausdorff expansion. We fully consider the linear commutator while approximating the quadratic commutator, for which we neglect specific contractions involving amplitudes with active indices. Moreover, we introduce approximate perturbative triples to obtain the ric-MRCCSD[T] method. We demonstrate the accuracy of our approaches in comparison to advanced multireference methods for the potential energy curves of H8, F2, H2O, N2, and Cr2. Additionally, we show that ric-MRCCSD and ric-MRCSSD[T] match the accuracy of CCSD(T) for evaluating spectroscopic constants and of full configuration interaction energies for a set of small molecules.","sentences":["In this work, we combine the many-body formulation of the internally contracted multireference coupled cluster (ic-MRCC) method with Evangelista's multireference formulation of the driven similarity renormalization group (DSRG).","The DSRG method can be viewed as a unitary multireference coupled cluster theory, which renormalizes the amplitudes based on a flow equation approach to eliminate numerical instabilities.","We extend this approach by demonstrating that the unitary flow equation approach can be adapted for nonunitary transformations, rationalizing the renormalization of ic-MRCC amplitudes.","We denote the new approach, the renormalized ic-MRCC (ric-MRCC) method.","To achieve high accuracy with a reasonable computational cost, we introduce a new approximation to the Baker-Campbell-Hausdorff expansion.","We fully consider the linear commutator while approximating the quadratic commutator, for which we neglect specific contractions involving amplitudes with active indices.","Moreover, we introduce approximate perturbative triples to obtain the ric-MRCCSD[T] method.","We demonstrate the accuracy of our approaches in comparison to advanced multireference methods for the potential energy curves of H8, F2, H2O, N2, and Cr2.","Additionally, we show that ric-MRCCSD and ric-MRCSSD[T] match the accuracy of CCSD(T) for evaluating spectroscopic constants and of full configuration interaction energies for a set of small molecules."],"url":"http://arxiv.org/abs/2405.16139v1","category":"physics.chem-ph"}
{"created":"2024-05-25 09:10:12","title":"C3LLM: Conditional Multimodal Content Generation Using Large Language Models","abstract":"We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \"acoustic vocabulary\" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.","sentences":["We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together.","C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner.","Our contributions are as follows.","First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks.","Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio.","Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \"acoustic vocabulary\" to LLM.","Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion.","Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods."],"url":"http://arxiv.org/abs/2405.16136v1","category":"cs.AI"}
{"created":"2024-05-25 08:23:05","title":"Prompt Optimization with EASE? Efficient Ordering-aware Automated Selection of Exemplars","abstract":"Large language models (LLMs) have shown impressive capabilities in real-world applications. The capability of in-context learning (ICL) allows us to adapt an LLM to downstream tasks by including input-label exemplars in the prompt without model fine-tuning. However, the quality of these exemplars in the prompt greatly impacts performance, highlighting the need for an effective automated exemplar selection method. Recent studies have explored retrieval-based approaches to select exemplars tailored to individual test queries, which can be undesirable due to extra test-time computation and an increased risk of data exposure. Moreover, existing methods fail to adequately account for the impact of exemplar ordering on the performance. On the other hand, the impact of the instruction, another essential component in the prompt given to the LLM, is often overlooked in existing exemplar selection methods. To address these challenges, we propose a novel method named EASE, which leverages the hidden embedding from a pre-trained language model to represent ordered sets of exemplars and uses a neural bandit algorithm to optimize the sets of exemplars while accounting for exemplar ordering. Our EASE can efficiently find an ordered set of exemplars that performs well for all test queries from a given task, thereby eliminating test-time computation. Importantly, EASE can be readily extended to jointly optimize both the exemplars and the instruction. Through extensive empirical evaluations (including novel tasks), we demonstrate the superiority of EASE over existing methods, and reveal practical insights about the impact of exemplar selection on ICL, which may be of independent interest. Our code is available at https://github.com/ZhaoxuanWu/EASE-Prompt-Optimization.","sentences":["Large language models (LLMs) have shown impressive capabilities in real-world applications.","The capability of in-context learning (ICL) allows us to adapt an LLM to downstream tasks by including input-label exemplars in the prompt without model fine-tuning.","However, the quality of these exemplars in the prompt greatly impacts performance, highlighting the need for an effective automated exemplar selection method.","Recent studies have explored retrieval-based approaches to select exemplars tailored to individual test queries, which can be undesirable due to extra test-time computation and an increased risk of data exposure.","Moreover, existing methods fail to adequately account for the impact of exemplar ordering on the performance.","On the other hand, the impact of the instruction, another essential component in the prompt given to the LLM, is often overlooked in existing exemplar selection methods.","To address these challenges, we propose a novel method named EASE, which leverages the hidden embedding from a pre-trained language model to represent ordered sets of exemplars and uses a neural bandit algorithm to optimize the sets of exemplars while accounting for exemplar ordering.","Our EASE can efficiently find an ordered set of exemplars that performs well for all test queries from a given task, thereby eliminating test-time computation.","Importantly, EASE can be readily extended to jointly optimize both the exemplars and the instruction.","Through extensive empirical evaluations (including novel tasks), we demonstrate the superiority of EASE over existing methods, and reveal practical insights about the impact of exemplar selection on ICL, which may be of independent interest.","Our code is available at https://github.com/ZhaoxuanWu/EASE-Prompt-Optimization."],"url":"http://arxiv.org/abs/2405.16122v1","category":"cs.AI"}
{"created":"2024-05-27 17:55:05","title":"Deep Learning Calabi-Yau four folds with hybrid and recurrent neural network architectures","abstract":"In this work, we report the results of applying deep learning based on hybrid convolutional-recurrent and purely recurrent neural network architectures to the dataset of almost one million complete intersection Calabi-Yau four-folds (CICY4) to machine-learn their four Hodge numbers $h^{1,1}, h^{2,1}, h^{3,1}, h^{2,2}$. In particular, we explored and experimented with twelve different neural network models, nine of which are convolutional-recurrent (CNN-RNN) hybrids with the RNN unit being either GRU (Gated Recurrent Unit) or Long Short Term Memory (LSTM). The remaining four models are purely recurrent neural networks based on LSTM. In terms of the $h^{1,1}, h^{2,1}, h^{3,1}, h^{2,2}$ prediction accuracies, at 72% training ratio, our best performing individual model is CNN-LSTM-400, a hybrid CNN-LSTM with the LSTM hidden size of 400, which obtained 99.74%, 98.07%, 95.19%, 81.01%, our second best performing individual model is LSTM-448, an LSTM-based model with the hidden size of 448, which obtained 99.74%, 97.51%, 94.24%, and 78.63%. These results were improved by forming ensembles of the top two, three or even four models. Our best ensemble, consisting of the top three models, achieved the accuracies of 99.80%, 98.40%, 95.80%, 83.02%. At 80% training ratio, the top two performing models LSTM-448 and LSTM-424 are both LSTM-based with the hidden sizes of 448 and 424. Compared with the 72% training ratio, there is a significant improvement of accuracies, which reached 99.85%, 98.66%, 96.26%, 84.77% for the best individual model and 99.88%, 98.91%, 96.96%, 86.78% for the best ensemble.","sentences":["In this work, we report the results of applying deep learning based on hybrid convolutional-recurrent and purely recurrent neural network architectures to the dataset of almost one million complete intersection Calabi-Yau four-folds (CICY4) to machine-learn their four Hodge numbers $h^{1,1}, h^{2,1}, h^{3,1}, h^{2,2}$. In particular, we explored and experimented with twelve different neural network models, nine of which are convolutional-recurrent (CNN-RNN) hybrids with the RNN unit being either GRU (Gated Recurrent Unit) or Long Short Term Memory (LSTM).","The remaining four models are purely recurrent neural networks based on LSTM.","In terms of the $h^{1,1}, h^{2,1}, h^{3,1}, h^{2,2}$ prediction accuracies, at 72% training ratio, our best performing individual model is CNN-LSTM-400, a hybrid CNN-LSTM with the LSTM hidden size of 400, which obtained 99.74%, 98.07%, 95.19%, 81.01%, our second best performing individual model is LSTM-448, an LSTM-based model with the hidden size of 448, which obtained 99.74%, 97.51%, 94.24%, and 78.63%.","These results were improved by forming ensembles of the top two, three or even four models.","Our best ensemble, consisting of the top three models, achieved the accuracies of 99.80%, 98.40%, 95.80%, 83.02%.","At 80% training ratio, the top two performing models LSTM-448 and LSTM-424 are both LSTM-based with the hidden sizes of 448 and 424.","Compared with the 72% training ratio, there is a significant improvement of accuracies, which reached 99.85%, 98.66%, 96.26%, 84.77% for the best individual model and 99.88%, 98.91%, 96.96%, 86.78% for the best ensemble."],"url":"http://arxiv.org/abs/2405.17406v1","category":"hep-th"}
{"created":"2024-05-27 17:10:40","title":"Controllable Deformations in Compressible Isotropic Implicit Elasticity","abstract":"For a given material, \\emph{controllable deformations} are those deformations that can be maintained in the absence of body forces and by applying only boundary tractions. For a given class of materials, \\emph{universal deformations} are those deformations that are controllable for any material within the class. In this paper, we characterize the universal deformations in compressible isotropic implicit elasticity defined by solids whose constitutive equations, in terms of the Cauchy stress $\\boldsymbol{\\sigma}$ and the left Cauchy-Green strain $\\mathbf{b}$, have the implicit form $\\boldsymbol{\\mathsf{f}}(\\boldsymbol{\\sigma},\\mathbf{b})=\\mathbf{0}$. We prove that universal deformations are homogeneous. However, an important observation is that, unlike Cauchy (and Green) elasticity, not every homogeneous deformation is permissible for a given implicit-elastic solid. In other words, the set of universal deformations is material-dependent, yet it remains a subset of homogeneous deformations.","sentences":["For a given material, \\emph{controllable deformations} are those deformations that can be maintained in the absence of body forces and by applying only boundary tractions.","For a given class of materials, \\emph{universal deformations} are those deformations that are controllable for any material within the class.","In this paper, we characterize the universal deformations in compressible isotropic implicit elasticity defined by solids whose constitutive equations, in terms of the Cauchy stress $\\boldsymbol{\\sigma}$ and the left Cauchy-Green strain $\\mathbf{b}$, have the implicit form $\\boldsymbol{\\mathsf{f}}(\\boldsymbol{\\sigma},\\mathbf{b})=\\mathbf{0}$. We prove that universal deformations are homogeneous.","However, an important observation is that, unlike Cauchy (and Green) elasticity, not every homogeneous deformation is permissible for a given implicit-elastic solid.","In other words, the set of universal deformations is material-dependent, yet it remains a subset of homogeneous deformations."],"url":"http://arxiv.org/abs/2405.17362v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-27 17:10:04","title":"A One-Layer Decoder-Only Transformer is a Two-Layer RNN: With an Application to Certified Robustness","abstract":"This paper reveals a key insight that a one-layer decoder-only Transformer is equivalent to a two-layer Recurrent Neural Network (RNN). Building on this insight, we propose ARC-Tran, a novel approach for verifying the robustness of decoder-only Transformers against arbitrary perturbation spaces. Compared to ARC-Tran, current robustness verification techniques are limited either to specific and length-preserving perturbations like word substitutions or to recursive models like LSTMs. ARC-Tran addresses these limitations by meticulously managing position encoding to prevent mismatches and by utilizing our key insight to achieve precise and scalable verification. Our evaluation shows that ARC-Tran (1) trains models more robust to arbitrary perturbation spaces than those produced by existing techniques and (2) shows high certification accuracy of the resulting models.","sentences":["This paper reveals a key insight that a one-layer decoder-only Transformer is equivalent to a two-layer Recurrent Neural Network (RNN).","Building on this insight, we propose ARC-Tran, a novel approach for verifying the robustness of decoder-only Transformers against arbitrary perturbation spaces.","Compared to ARC-Tran, current robustness verification techniques are limited either to specific and length-preserving perturbations like word substitutions or to recursive models like LSTMs.","ARC-Tran addresses these limitations by meticulously managing position encoding to prevent mismatches and by utilizing our key insight to achieve precise and scalable verification.","Our evaluation shows that ARC-Tran (1) trains models more robust to arbitrary perturbation spaces than those produced by existing techniques and (2) shows high certification accuracy of the resulting models."],"url":"http://arxiv.org/abs/2405.17361v1","category":"cs.CL"}
{"created":"2024-05-27 16:32:33","title":"Moduli spaces in positive geometry","abstract":"These are lecture notes for five lectures given at MPI Leipzig in May 2024. We study the moduli space M_{0,n} of n distinct points on P^1 as a positive geometry and a binary geometry. We develop mathematical formalism to study Cachazo-He-Yuan's scattering equations and the associated scalar and Yang-Mills amplitudes. We discuss open superstring amplitudes and relations to tropical geometry.","sentences":["These are lecture notes for five lectures given at MPI Leipzig in May 2024.","We study the moduli space M_{0,n} of n distinct points on P^1 as a positive geometry and a binary geometry.","We develop mathematical formalism to study Cachazo-He-Yuan's scattering equations and the associated scalar and Yang-Mills amplitudes.","We discuss open superstring amplitudes and relations to tropical geometry."],"url":"http://arxiv.org/abs/2405.17332v1","category":"math.AG"}
{"created":"2024-05-27 16:30:58","title":"Refraction FWI of a circular shot OBN acquisition in the Brazilian pre-salt region","abstract":"We develop a workflow based on full-waveform inversion (FWI) to estimate P-wave velocities in a deepwater Brazilian pre-salt field using the recently introduced circular shot ocean bottom node (OBN) acquisition geometry. Such a geometry comprises a source vessel sailing in large radius concentric circular trajectories and seismic signals are recorded by OBN arrays. The circular shot OBN survey provides mostly refracted waves separately from reflected waves, so the FWI process is mainly driven by diving waves. We introduce a new FWI workflow to analyze non-preprocessed OBN refraction data, which includes automated steps such as data selection solving an Eikonal equation, estimation of a source signature that accounts for ghost and bubble effects, and gradient preconditioning using a non-stationary filter and seismic illumination. We consider two objective functions based on the $L^1$ and $L^2$ norms. The FWI results demonstrated that using our proposed workflow with the $L^1$ norm objective function and the circular OBN survey can lead to an improvement in pre-salt velocity models. Furthermore, using these improved models we construct reverse-time migration (RTM) images of the conventional OBN dataset, showing significant improvements in the salt stratification, the base of salt, and the lateral resolution of the pre-salt area. The Brazilian pre-salt case study demonstrated that the circular shot OBN acquisition maximizes the illumination of deep reservoirs through the ultra-long offset and full-azimuth coverage that prioritizes the recording of diving waves.","sentences":["We develop a workflow based on full-waveform inversion (FWI) to estimate P-wave velocities in a deepwater Brazilian pre-salt field using the recently introduced circular shot ocean bottom node (OBN) acquisition geometry.","Such a geometry comprises a source vessel sailing in large radius concentric circular trajectories and seismic signals are recorded by OBN arrays.","The circular shot OBN survey provides mostly refracted waves separately from reflected waves, so the FWI process is mainly driven by diving waves.","We introduce a new FWI workflow to analyze non-preprocessed OBN refraction data, which includes automated steps such as data selection solving an Eikonal equation, estimation of a source signature that accounts for ghost and bubble effects, and gradient preconditioning using a non-stationary filter and seismic illumination.","We consider two objective functions based on the $L^1$ and $L^2$ norms.","The FWI results demonstrated that using our proposed workflow with the $L^1$ norm objective function and the circular OBN survey can lead to an improvement in pre-salt velocity models.","Furthermore, using these improved models we construct reverse-time migration (RTM) images of the conventional OBN dataset, showing significant improvements in the salt stratification, the base of salt, and the lateral resolution of the pre-salt area.","The Brazilian pre-salt case study demonstrated that the circular shot OBN acquisition maximizes the illumination of deep reservoirs through the ultra-long offset and full-azimuth coverage that prioritizes the recording of diving waves."],"url":"http://arxiv.org/abs/2405.17330v1","category":"physics.geo-ph"}
{"created":"2024-05-27 16:19:21","title":"Large deviations for the maximum and reversed order statistics of Weibull-like variables","abstract":"Motivated by metastability in the zero-range process, we consider i.i.d.\\ random variables with values in $\\N_0$ and Weibull-like (stretched exponential) law $\\mathbb P(X_i =k) = c \\exp( - k^\\alpha)$, $\\alpha \\in (0,1)$. We condition on large values of the sum $S_n= \\mu n + s n^\\gamma$ and prove large deviation principles for the rescaled maximum $M_n /n^\\gamma$ and for the reversed order statistics. The scale is $n^\\gamma$ with $\\gamma = 1/(2-\\alpha)$; on that scale, the big-jump principle for heavy-tailed variables and a naive normal approximation for moderate deviations yield bounds of the same order $n^{\\gamma \\alpha} = n^{2\\gamma-1}$, the speed of the large deviation principles. The rate function for $M_n/n^\\gamma$ is non-convex and solves a recursive equation similar to a Bellman equation.","sentences":["Motivated by metastability in the zero-range process, we consider i.i.d.\\ random variables with values in $\\N_0$ and Weibull-like (stretched exponential) law $\\mathbb P(X_i =k) = c \\exp( - k^\\alpha)$, $\\alpha \\in (0,1)$.","We condition on large values of the sum $S_n= \\mu n + s n^\\gamma$ and prove large deviation principles for the rescaled maximum $M_n /n^\\gamma$","and for the reversed order statistics.","The scale is $n^\\gamma$ with $\\gamma = 1/(2-\\alpha)$; on that scale, the big-jump principle for heavy-tailed variables and a naive normal approximation for moderate deviations yield bounds of the same order $n^{\\gamma \\alpha} = n^{2\\gamma-1}$, the speed of the large deviation principles.","The rate function for $M_n/n^\\gamma$ is non-convex and solves a recursive equation similar to a Bellman equation."],"url":"http://arxiv.org/abs/2405.17319v1","category":"math.PR"}
{"created":"2024-05-27 15:35:11","title":"Global existence for wave and beam equations with double damping and a new power nonlinearity","abstract":"We consider the Cauchy problem in $\\mathbb{R}^{n}$ for wave and beam equations with frictional, viscoelastic damping, and a new power nonlinearity. In addition to the solution and its total energy, we define the following quantity: $$Q[u](t):=\\|u_{t}(t,\\cdot)+(-\\Delta)^{\\sigma}u(t,\\cdot)\\|_{L^{2}(\\mathbb{R}^{n})}.$$   Our aim is to show that the interaction between frictional and viscoelastic damping in a linear model leads to an exponential decay of $Q[u](t)$ as $t\\to \\infty$. This decay motivates us to define a new power nonlinearity of the form $N[u]:=|u_{t}+(-\\Delta)^{\\sigma}u|^{p}$. Surprisingly, $N[u]$ can be considered a small perturbation for any $p>1$, in the sense that, the decay estimates of the unique global solution, the total energy and $Q[u](t)$ coincide with those for solutions to the corresponding linear Cauchy problem with vanishing right-hand side.","sentences":["We consider the Cauchy problem in $\\mathbb{R}^{n}$ for wave and beam equations with frictional, viscoelastic damping, and a new power nonlinearity.","In addition to the solution and its total energy, we define the following quantity: $$Q[u](t):=\\|u_{t}(t,\\cdot)+(-\\Delta)^{\\sigma}u(t,\\cdot)\\|_{L^{2}(\\mathbb{R}^{n})}.$$   Our aim is to show that the interaction between frictional and viscoelastic damping in a linear model leads to an exponential decay of $Q[u](t)$ as $t\\to \\infty$. This decay motivates us to define a new power nonlinearity of the form $N[u]:=|u_{t}+(-\\Delta)^{\\sigma}u|^{p}$. Surprisingly, $N[u]$ can be considered a small perturbation for any $p>1$, in the sense that, the decay estimates of the unique global solution, the total energy and $Q[u](t)$ coincide with those for solutions to the corresponding linear Cauchy problem with vanishing right-hand side."],"url":"http://arxiv.org/abs/2405.17274v1","category":"math.AP"}
{"created":"2024-05-27 15:20:40","title":"Deep Feature Gaussian Processes for Single-Scene Aerosol Optical Depth Reconstruction","abstract":"Remote sensing data provide a low-cost solution for large-scale monitoring of air pollution via the retrieval of aerosol optical depth (AOD), but is often limited by cloud contamination. Existing methods for AOD reconstruction rely on temporal information. However, for remote sensing data at high spatial resolution, multi-temporal observations are often unavailable. In this letter, we take advantage of deep representation learning from convolutional neural networks and propose Deep Feature Gaussian Processes (DFGP) for single-scene AOD reconstruction. By using deep learning, we transform the variables to a feature space with better explainable power. By using Gaussian processes, we explicitly consider the correlation between observed AOD and missing AOD in spatial and feature domains. Experiments on two AOD datasets with real-world cloud patterns showed that the proposed method outperformed deep CNN and random forest, achieving R$^2$ of 0.7431 on MODIS AOD and R$^2$ of 0.9211 on EMIT AOD, compared to deep CNN's R$^2$ of 0.6507 and R$^2$ of 0.8619. The proposed methods increased R$^2$ by over 0.35 compared to the popular random forest in AOD reconstruction. The data and code used in this study are available at \\url{https://skrisliu.com/dfgp}.","sentences":["Remote sensing data provide a low-cost solution for large-scale monitoring of air pollution via the retrieval of aerosol optical depth (AOD), but is often limited by cloud contamination.","Existing methods for AOD reconstruction rely on temporal information.","However, for remote sensing data at high spatial resolution, multi-temporal observations are often unavailable.","In this letter, we take advantage of deep representation learning from convolutional neural networks and propose Deep Feature Gaussian Processes (DFGP) for single-scene AOD reconstruction.","By using deep learning, we transform the variables to a feature space with better explainable power.","By using Gaussian processes, we explicitly consider the correlation between observed AOD and missing AOD in spatial and feature domains.","Experiments on two AOD datasets with real-world cloud patterns showed that the proposed method outperformed deep CNN and random forest, achieving R$^2$ of 0.7431 on MODIS AOD and R$^2$ of 0.9211 on EMIT AOD, compared to deep CNN's R$^2$ of 0.6507 and R$^2$ of 0.8619.","The proposed methods increased R$^2$ by over 0.35 compared to the popular random forest in AOD reconstruction.","The data and code used in this study are available at \\url{https://skrisliu.com/dfgp}."],"url":"http://arxiv.org/abs/2405.17262v1","category":"cs.CV"}
{"created":"2024-05-27 14:33:06","title":"Spectral-Refiner: Fine-Tuning of Accurate Spatiotemporal Neural Operator for Turbulent Flows","abstract":"Recent advancements in operator-type neural networks have shown promising results in approximating the solutions of spatiotemporal Partial Differential Equations (PDEs). However, these neural networks often entail considerable training expenses, and may not always achieve the desired accuracy required in many scientific and engineering disciplines. In this paper, we propose a new Spatiotemporal Fourier Neural Operator (SFNO) that learns maps between Bochner spaces, and a new learning framework to address these issues. This new paradigm leverages wisdom from traditional numerical PDE theory and techniques to refine the pipeline of commonly adopted end-to-end neural operator training and evaluations. Specifically, in the learning problems for the turbulent flow modeling by the Navier-Stokes Equations (NSE), the proposed architecture initiates the training with a few epochs for SFNO, concluding with the freezing of most model parameters. Then, the last linear spectral convolution layer is fine-tuned without the frequency truncation. The optimization uses a negative Sobolev norm for the first time as the loss in operator learning, defined through a reliable functional-type \\emph{a posteriori} error estimator whose evaluation is almost exact thanks to the Parseval identity. This design allows the neural operators to effectively tackle low-frequency errors while the relief of the de-aliasing filter addresses high-frequency errors. Numerical experiments on commonly used benchmarks for the 2D NSE demonstrate significant improvements in both computational efficiency and accuracy, compared to end-to-end evaluation and traditional numerical PDE solvers.","sentences":["Recent advancements in operator-type neural networks have shown promising results in approximating the solutions of spatiotemporal Partial Differential Equations (PDEs).","However, these neural networks often entail considerable training expenses, and may not always achieve the desired accuracy required in many scientific and engineering disciplines.","In this paper, we propose a new Spatiotemporal Fourier Neural Operator (SFNO) that learns maps between Bochner spaces, and a new learning framework to address these issues.","This new paradigm leverages wisdom from traditional numerical PDE theory and techniques to refine the pipeline of commonly adopted end-to-end neural operator training and evaluations.","Specifically, in the learning problems for the turbulent flow modeling by the Navier-Stokes Equations (NSE), the proposed architecture initiates the training with a few epochs for SFNO, concluding with the freezing of most model parameters.","Then, the last linear spectral convolution layer is fine-tuned without the frequency truncation.","The optimization uses a negative Sobolev norm for the first time as the loss in operator learning, defined through a reliable functional-type \\emph{a posteriori} error estimator whose evaluation is almost exact thanks to the Parseval identity.","This design allows the neural operators to effectively tackle low-frequency errors while the relief of the de-aliasing filter addresses high-frequency errors.","Numerical experiments on commonly used benchmarks for the 2D NSE demonstrate significant improvements in both computational efficiency and accuracy, compared to end-to-end evaluation and traditional numerical PDE solvers."],"url":"http://arxiv.org/abs/2405.17211v1","category":"cs.LG"}
{"created":"2024-05-27 14:18:43","title":"Renormalized stochastic pressure equation with log-correlated Gaussian coefficients","abstract":"We study periodic solutions to the following divergence-form stochastic partial differential equation with Wick-renormalized gradient on the $d$-dimensional flat torus $\\mathbb{T}^d$,   \\[   -\\nabla\\cdot\\left(e^{\\diamond (- \\beta X) }\\diamond\\nabla U\\right)=\\nabla \\cdot (e^{\\diamond (- \\beta X)} \\diamond \\mathbf{F}),   \\] where $X$ is the log-correlated Gaussian field, $\\mathbf{F}$ is a random vector and $\\diamond$ denotes the Wick product. The problem is a variant of the stochastic pressure equation, in which $U$ is modeling the pressure of a creeping water-flow in crustal rock that occurs in enhanced geothermal heating. In the original model, the Wick exponential term $e^{\\diamond(-\\beta X)}$ is modeling the random permeability of the rock. The porosity field is given by a log-correlated Gaussian random field $\\beta X$, where $\\beta<\\sqrt{d}$. We use elliptic regularity theory in order to define a notion of a solution to this (a priori very ill-posed) problem, via modifying the $S$-transform from Gaussian white noise analysis, and then establish the existence and uniqueness of solutions. Moreover, we show that the solution to the problem can be expressed in terms of the Gaussian multiplicative chaos measure.","sentences":["We study periodic solutions to the following divergence-form stochastic partial differential equation with Wick-renormalized gradient on the $d$-dimensional flat torus $\\mathbb{T}^d$,   \\[   -\\nabla\\cdot\\left(e^{\\diamond (- \\beta X) }\\diamond\\nabla U\\right)=\\nabla","\\cdot (e^{\\diamond (- \\beta X)} \\diamond \\mathbf{F}),   \\] where $X$ is the log-correlated Gaussian field, $\\mathbf{F}$ is a random vector and $\\diamond$ denotes the Wick product.","The problem is a variant of the stochastic pressure equation, in which $U$ is modeling the pressure of a creeping water-flow in crustal rock that occurs in enhanced geothermal heating.","In the original model, the Wick exponential term $e^{\\diamond(-\\beta X)}$ is modeling the random permeability of the rock.","The porosity field is given by a log-correlated Gaussian random field $\\beta X$, where $\\beta<\\sqrt{d}$. We use elliptic regularity theory in order to define a notion of a solution to this (a priori very ill-posed) problem, via modifying the $S$-transform from Gaussian white noise analysis, and then establish the existence and uniqueness of solutions.","Moreover, we show that the solution to the problem can be expressed in terms of the Gaussian multiplicative chaos measure."],"url":"http://arxiv.org/abs/2405.17195v1","category":"math.PR"}
{"created":"2024-05-27 13:44:53","title":"Partitioned Hankel-based Diffusion Models for Few-shot Low-dose CT Reconstruction","abstract":"Low-dose computed tomography (LDCT) plays a vital role in clinical applications by mitigating radiation risks. Nevertheless, reducing radiation doses significantly degrades image quality. Concurrently, common deep learning methods demand extensive data, posing concerns about privacy, cost, and time constraints. Consequently, we propose a few-shot low-dose CT reconstruction method using Partitioned Hankel-based Diffusion (PHD) models. During the prior learning stage, the projection data is first transformed into multiple partitioned Hankel matrices. Structured tensors are then extracted from these matrices to facilitate prior learning through multiple diffusion models. In the iterative reconstruction stage, an iterative stochastic differential equation solver is employed along with data consistency constraints to update the acquired projection data. Furthermore, penalized weighted least-squares and total variation techniques are introduced to enhance the resulting image quality. The results approximate those of normal-dose counterparts, validating PHD model as an effective and practical model for reducing artifacts and noise while preserving image quality.","sentences":["Low-dose computed tomography (LDCT) plays a vital role in clinical applications by mitigating radiation risks.","Nevertheless, reducing radiation doses significantly degrades image quality.","Concurrently, common deep learning methods demand extensive data, posing concerns about privacy, cost, and time constraints.","Consequently, we propose a few-shot low-dose CT reconstruction method using Partitioned Hankel-based Diffusion (PHD) models.","During the prior learning stage, the projection data is first transformed into multiple partitioned Hankel matrices.","Structured tensors are then extracted from these matrices to facilitate prior learning through multiple diffusion models.","In the iterative reconstruction stage, an iterative stochastic differential equation solver is employed along with data consistency constraints to update the acquired projection data.","Furthermore, penalized weighted least-squares and total variation techniques are introduced to enhance the resulting image quality.","The results approximate those of normal-dose counterparts, validating PHD model as an effective and practical model for reducing artifacts and noise while preserving image quality."],"url":"http://arxiv.org/abs/2405.17167v1","category":"eess.IV"}
{"created":"2024-05-27 13:38:28","title":"WeiPer: OOD Detection using Weight Perturbations of Class Projections","abstract":"Recent advances in out-of-distribution (OOD) detection on image data show that pre-trained neural network classifiers can separate in-distribution (ID) from OOD data well, leveraging the class-discriminative ability of the model itself. Methods have been proposed that either use logit information directly or that process the model's penultimate layer activations. With \"WeiPer\", we introduce perturbations of the class projections in the final fully connected layer which creates a richer representation of the input. We show that this simple trick can improve the OOD detection performance of a variety of methods and additionally propose a distance-based method that leverages the properties of the augmented WeiPer space. We achieve state-of-the-art OOD detection results across multiple benchmarks of the OpenOOD framework, especially pronounced in difficult settings in which OOD samples are positioned close to the training set distribution. We support our findings with theoretical motivations and empirical observations, and run extensive ablations to provide insights into why WeiPer works.","sentences":["Recent advances in out-of-distribution (OOD) detection on image data show that pre-trained neural network classifiers can separate in-distribution (ID) from OOD data well, leveraging the class-discriminative ability of the model itself.","Methods have been proposed that either use logit information directly or that process the model's penultimate layer activations.","With \"WeiPer\", we introduce perturbations of the class projections in the final fully connected layer which creates a richer representation of the input.","We show that this simple trick can improve the OOD detection performance of a variety of methods and additionally propose a distance-based method that leverages the properties of the augmented WeiPer space.","We achieve state-of-the-art OOD detection results across multiple benchmarks of the OpenOOD framework, especially pronounced in difficult settings in which OOD samples are positioned close to the training set distribution.","We support our findings with theoretical motivations and empirical observations, and run extensive ablations to provide insights into why WeiPer works."],"url":"http://arxiv.org/abs/2405.17164v1","category":"cs.LG"}
{"created":"2024-05-27 13:33:29","title":"Stop! In the Name of Flaws: Disentangling Personal Names and Sociodemographic Attributes in NLP","abstract":"Personal names simultaneously differentiate individuals and categorize them in ways that are important in a given society. While the natural language processing community has thus associated personal names with sociodemographic characteristics in a variety of tasks, researchers have engaged to varying degrees with the established methodological problems in doing so. To guide future work, we present an interdisciplinary background on names and naming. We then survey the issues inherent to associating names with sociodemographic attributes, covering problems of validity (e.g., systematic error, construct validity), as well as ethical concerns (e.g., harms, differential impact, cultural insensitivity). Finally, we provide guiding questions along with normative recommendations to avoid validity and ethical pitfalls when dealing with names and sociodemographic characteristics in natural language processing.","sentences":["Personal names simultaneously differentiate individuals and categorize them in ways that are important in a given society.","While the natural language processing community has thus associated personal names with sociodemographic characteristics in a variety of tasks, researchers have engaged to varying degrees with the established methodological problems in doing so.","To guide future work, we present an interdisciplinary background on names and naming.","We then survey the issues inherent to associating names with sociodemographic attributes, covering problems of validity (e.g., systematic error, construct validity), as well as ethical concerns (e.g., harms, differential impact, cultural insensitivity).","Finally, we provide guiding questions along with normative recommendations to avoid validity and ethical pitfalls when dealing with names and sociodemographic characteristics in natural language processing."],"url":"http://arxiv.org/abs/2405.17159v1","category":"cs.CL"}
{"created":"2024-05-27 12:08:59","title":"DeeperImpact: Optimizing Sparse Learned Index Structures","abstract":"A lot of recent work has focused on sparse learned indexes that use deep neural architectures to significantly improve retrieval quality while keeping the efficiency benefits of the inverted index. While such sparse learned structures achieve effectiveness far beyond those of traditional inverted index-based rankers, there is still a gap in effectiveness to the best dense retrievers, or even to sparse methods that leverage more expensive optimizations such as query expansion and query term weighting. We focus on narrowing this gap by revisiting and optimizing DeepImpact, a sparse retrieval approach that uses DocT5Query for document expansion followed by a BERT language model to learn impact scores for document terms. We first reinvestigate the expansion process and find that the recently proposed Doc2Query query filtration does not enhance retrieval quality when used with DeepImpact. Instead, substituting T5 with a fine-tuned Llama 2 model for query prediction results in a considerable improvement. Subsequently, we study training strategies that have proven effective for other models, in particular the use of hard negatives, distillation, and pre-trained CoCondenser model initialization. Our results significantly narrow the effectiveness gap with the most effective versions of SPLADE.","sentences":["A lot of recent work has focused on sparse learned indexes that use deep neural architectures to significantly improve retrieval quality while keeping the efficiency benefits of the inverted index.","While such sparse learned structures achieve effectiveness far beyond those of traditional inverted index-based rankers, there is still a gap in effectiveness to the best dense retrievers, or even to sparse methods that leverage more expensive optimizations such as query expansion and query term weighting.","We focus on narrowing this gap by revisiting and optimizing DeepImpact, a sparse retrieval approach that uses DocT5Query for document expansion followed by a BERT language model to learn impact scores for document terms.","We first reinvestigate the expansion process and find that the recently proposed Doc2Query query filtration does not enhance retrieval quality when used with DeepImpact.","Instead, substituting T5 with a fine-tuned Llama 2 model for query prediction results in a considerable improvement.","Subsequently, we study training strategies that have proven effective for other models, in particular the use of hard negatives, distillation, and pre-trained CoCondenser model initialization.","Our results significantly narrow the effectiveness gap with the most effective versions of SPLADE."],"url":"http://arxiv.org/abs/2405.17093v1","category":"cs.IR"}
{"created":"2024-05-27 12:03:54","title":"Dimension Reductions in Instanton Theory","abstract":"We study the dimension reduction of instantons over product manifolds with calibrated factors. We first prove an integrability result that relates dimension reduction with curvature conditions. Then we find a topological criterion for bundles over product manifolds to admit instantons that satisfy the aforementioned curvature conditions; in particular, pull-back bundles satisfy this criterion. Consequently, we deduce explicit descriptions for the moduli space of Hermitian Yang--Mills connections, G2-, and Spin(7)-instantons in various contexts, and establish well-behaved compactifications for these moduli spaces when one factor of the product manifold is a hyperk\\\"ahler surface.","sentences":["We study the dimension reduction of instantons over product manifolds with calibrated factors.","We first prove an integrability result that relates dimension reduction with curvature conditions.","Then we find a topological criterion for bundles over product manifolds to admit instantons that satisfy the aforementioned curvature conditions; in particular, pull-back bundles satisfy this criterion.","Consequently, we deduce explicit descriptions for the moduli space of Hermitian Yang--Mills connections, G2-, and Spin(7)-instantons in various contexts, and establish well-behaved compactifications for these moduli spaces when one factor of the product manifold is a hyperk\\\"ahler surface."],"url":"http://arxiv.org/abs/2405.17086v1","category":"math.DG"}
{"created":"2024-05-27 11:55:49","title":"F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting","abstract":"The neural radiance field (NeRF) has made significant strides in representing 3D scenes and synthesizing novel views. Despite its advancements, the high computational costs of NeRF have posed challenges for its deployment in resource-constrained environments and real-time applications. As an alternative to NeRF-like neural rendering methods, 3D Gaussian Splatting (3DGS) offers rapid rendering speeds while maintaining excellent image quality. However, as it represents objects and scenes using a myriad of Gaussians, it requires substantial storage to achieve high-quality representation. To mitigate the storage overhead, we propose Factorized 3D Gaussian Splatting (F-3DGS), a novel approach that drastically reduces storage requirements while preserving image quality. Inspired by classical matrix and tensor factorization techniques, our method represents and approximates dense clusters of Gaussians with significantly fewer Gaussians through efficient factorization. We aim to efficiently represent dense 3D Gaussians by approximating them with a limited amount of information for each axis and their combinations. This method allows us to encode a substantially large number of Gaussians along with their essential attributes -- such as color, scale, and rotation -- necessary for rendering using a relatively small number of elements. Extensive experimental results demonstrate that F-3DGS achieves a significant reduction in storage costs while maintaining comparable quality in rendered images.","sentences":["The neural radiance field (NeRF) has made significant strides in representing 3D scenes and synthesizing novel views.","Despite its advancements, the high computational costs of NeRF have posed challenges for its deployment in resource-constrained environments and real-time applications.","As an alternative to NeRF-like neural rendering methods, 3D Gaussian Splatting (3DGS) offers rapid rendering speeds while maintaining excellent image quality.","However, as it represents objects and scenes using a myriad of Gaussians, it requires substantial storage to achieve high-quality representation.","To mitigate the storage overhead, we propose Factorized 3D Gaussian Splatting (F-3DGS), a novel approach that drastically reduces storage requirements while preserving image quality.","Inspired by classical matrix and tensor factorization techniques, our method represents and approximates dense clusters of Gaussians with significantly fewer Gaussians through efficient factorization.","We aim to efficiently represent dense 3D Gaussians by approximating them with a limited amount of information for each axis and their combinations.","This method allows us to encode a substantially large number of Gaussians along with their essential attributes -- such as color, scale, and rotation -- necessary for rendering using a relatively small number of elements.","Extensive experimental results demonstrate that F-3DGS achieves a significant reduction in storage costs while maintaining comparable quality in rendered images."],"url":"http://arxiv.org/abs/2405.17083v1","category":"cs.CV"}
{"created":"2024-05-27 10:44:05","title":"BDC-Occ: Binarized Deep Convolution Unit For Binarized Occupancy Network","abstract":"Existing 3D occupancy networks demand significant hardware resources, hindering the deployment of edge devices. Binarized Neural Networks (BNN) offer substantially reduced computational and memory requirements. However, their performance decreases notably compared to full-precision networks. Moreover, it is challenging to enhance the performance of binarized models by increasing the number of binarized convolutional layers, which limits their practicability for 3D occupancy prediction. To bridge these gaps, we propose a novel binarized deep convolution (BDC) unit that effectively enhances performance while increasing the number of binarized convolutional layers. Firstly, through theoretical analysis, we demonstrate that 1 \\times 1 binarized convolutions introduce minimal binarization errors. Therefore, additional binarized convolutional layers are constrained to 1 \\times 1 in the BDC unit. Secondly, we introduce the per-channel weight branch to mitigate the impact of binarization errors from unimportant channel features on the performance of binarized models, thereby improving performance while increasing the number of binarized convolutional layers. Furthermore, we decompose the 3D occupancy network into four convolutional modules and utilize the proposed BDC unit to binarize these modules. Our BDC-Occ model is created by applying the proposed BDC unit to binarize the existing 3D occupancy networks. Comprehensive quantitative and qualitative experiments demonstrate that the proposed BDC-Occ is the state-of-the-art binarized 3D occupancy network algorithm.","sentences":["Existing 3D occupancy networks demand significant hardware resources, hindering the deployment of edge devices.","Binarized Neural Networks (BNN) offer substantially reduced computational and memory requirements.","However, their performance decreases notably compared to full-precision networks.","Moreover, it is challenging to enhance the performance of binarized models by increasing the number of binarized convolutional layers, which limits their practicability for 3D occupancy prediction.","To bridge these gaps, we propose a novel binarized deep convolution (BDC) unit that effectively enhances performance while increasing the number of binarized convolutional layers.","Firstly, through theoretical analysis, we demonstrate that 1 \\times 1 binarized convolutions introduce minimal binarization errors.","Therefore, additional binarized convolutional layers are constrained to 1 \\times 1 in the BDC unit.","Secondly, we introduce the per-channel weight branch to mitigate the impact of binarization errors from unimportant channel features on the performance of binarized models, thereby improving performance while increasing the number of binarized convolutional layers.","Furthermore, we decompose the 3D occupancy network into four convolutional modules and utilize the proposed BDC unit to binarize these modules.","Our BDC-Occ model is created by applying the proposed BDC unit to binarize the existing 3D occupancy networks.","Comprehensive quantitative and qualitative experiments demonstrate that the proposed BDC-Occ is the state-of-the-art binarized 3D occupancy network algorithm."],"url":"http://arxiv.org/abs/2405.17037v1","category":"cs.CV"}
{"created":"2024-05-27 09:56:03","title":"Standardizing the Gamma-ray burst as a standard candle and applying to the cosmological probes: constraints on the two-component dark energy model","abstract":"As one of the most energetic and brightest events, gamma-ray bursts (GRBs) have been used as a standard candle for cosmological probe. Based on the relevant features of GRBs light curves, a plateau phase followed a decay phase, we obtain X-ray samples of 31 GRBs and optical samples of 50 GRBs, which are thought to be caused by the same physical mechanism. We standardize GRBs using the two-dimension fundamental plane relation of the rest-frame luminosity of the plateau emission ($L_{b,z}$) and the end time of plateau ($T_{b,z}$) $L_{b,z}-T_{b,z}$, as well as the three-dimension fundamental plane correlation including the peak energy ($E_{p,i}$) $L_{b,z}-T_{b,z}-E_{p,i}$. For the cosmological probes, we consider the $\\omega$CDM model in which the dark energy consists of one component, and mainly focus on the $X_1X_2$CDM model in which the dark energy is made up of two independent components. We obtain the constraints on the related parameters of the cosmological models using the type Ia supernovae (SNe Ia) data and selected X-ray and optical samples. For the $X_1X_2$CDM model, we find that the values of the equations of state parameters of two dark energies, $\\omega_1$ and $\\omega_2$, are very close. We also conduct the comparison between the models using the Bayesian information criterion, and find that the $\\omega$CDM model is favoured.","sentences":["As one of the most energetic and brightest events, gamma-ray bursts (GRBs) have been used as a standard candle for cosmological probe.","Based on the relevant features of GRBs light curves, a plateau phase followed a decay phase, we obtain X-ray samples of 31 GRBs and optical samples of 50 GRBs, which are thought to be caused by the same physical mechanism.","We standardize GRBs using the two-dimension fundamental plane relation of the rest-frame luminosity of the plateau emission ($L_{b,z}$) and the end time of plateau ($T_{b,z}$) $L_{b,z}-T_{b,z}$, as well as the three-dimension fundamental plane correlation including the peak energy ($E_{p,i}$) $L_{b,z}-T_{b,z}-E_{p,i}$. For the cosmological probes, we consider the $\\omega$CDM model in which the dark energy consists of one component, and mainly focus on the $X_1X_2$CDM model in which the dark energy is made up of two independent components.","We obtain the constraints on the related parameters of the cosmological models using the type Ia supernovae (SNe Ia) data and selected X-ray and optical samples.","For the $X_1X_2$CDM model, we find that the values of the equations of state parameters of two dark energies, $\\omega_1$ and $\\omega_2$, are very close.","We also conduct the comparison between the models using the Bayesian information criterion, and find that the $\\omega$CDM model is favoured."],"url":"http://arxiv.org/abs/2405.17010v1","category":"astro-ph.HE"}
{"created":"2024-05-27 09:47:49","title":"Efficient Visual Fault Detection for Freight Train via Neural Architecture Search with Data Volume Robustness","abstract":"Deep learning-based fault detection methods have achieved significant success. In visual fault detection of freight trains, there exists a large characteristic difference between inter-class components (scale variance) but intra-class on the contrary, which entails scale-awareness for detectors. Moreover, the design of task-specific networks heavily relies on human expertise. As a consequence, neural architecture search (NAS) that automates the model design process gains considerable attention because of its promising performance. However, NAS is computationally intensive due to the large search space and huge data volume. In this work, we propose an efficient NAS-based framework for visual fault detection of freight trains to search for the task-specific detection head with capacities of multi-scale representation. First, we design a scale-aware search space for discovering an effective receptive field in the head. Second, we explore the robustness of data volume to reduce search costs based on the specifically designed search space, and a novel sharing strategy is proposed to reduce memory and further improve search efficiency. Extensive experimental results demonstrate the effectiveness of our method with data volume robustness, which achieves 46.8 and 47.9 mAP on the Bottom View and Side View datasets, respectively. Our framework outperforms the state-of-the-art approaches and linearly decreases the search costs with reduced data volumes.","sentences":["Deep learning-based fault detection methods have achieved significant success.","In visual fault detection of freight trains, there exists a large characteristic difference between inter-class components (scale variance) but","intra-class on the contrary, which entails scale-awareness for detectors.","Moreover, the design of task-specific networks heavily relies on human expertise.","As a consequence, neural architecture search (NAS) that automates the model design process gains considerable attention because of its promising performance.","However, NAS is computationally intensive due to the large search space and huge data volume.","In this work, we propose an efficient NAS-based framework for visual fault detection of freight trains to search for the task-specific detection head with capacities of multi-scale representation.","First, we design a scale-aware search space for discovering an effective receptive field in the head.","Second, we explore the robustness of data volume to reduce search costs based on the specifically designed search space, and a novel sharing strategy is proposed to reduce memory and further improve search efficiency.","Extensive experimental results demonstrate the effectiveness of our method with data volume robustness, which achieves 46.8 and 47.9 mAP on the Bottom View and Side View datasets, respectively.","Our framework outperforms the state-of-the-art approaches and linearly decreases the search costs with reduced data volumes."],"url":"http://arxiv.org/abs/2405.17004v1","category":"cs.CV"}
{"created":"2024-05-27 09:43:42","title":"Representation of wave fields in a graded multilayer medium","abstract":"In this work we present a new approach in representation of wave fields in nonuniform 1-D multilayer medium. This approach is based on the use of a modified homogeneous basis. A new form of coupled equations for describing nonuniform one-directional photonic crystals is proposed. This description can be especially useful to study radiation that originates when fast electrons move in such medium","sentences":["In this work we present a new approach in representation of wave fields in nonuniform 1-D multilayer medium.","This approach is based on the use of a modified homogeneous basis.","A new form of coupled equations for describing nonuniform one-directional photonic crystals is proposed.","This description can be especially useful to study radiation that originates when fast electrons move in such medium"],"url":"http://arxiv.org/abs/2405.16998v1","category":"physics.optics"}
{"created":"2024-05-27 09:31:25","title":"Analysis of scalar fields with series convolution","abstract":"Wave equations for some curved spacetimes may involve functions that prevent a solution in a closed form. In some cases, these functions can be eliminated by transformations and the solutions can be found analytically. In the cases where such transformations are not available, the infinite series expansions of these functions can be convoluted with the power series solution ansatz. We study such an example where the solution is based on a special function.","sentences":["Wave equations for some curved spacetimes may involve functions that prevent a solution in a closed form.","In some cases, these functions can be eliminated by transformations and the solutions can be found analytically.","In the cases where such transformations are not available, the infinite series expansions of these functions can be convoluted with the power series solution ansatz.","We study such an example where the solution is based on a special function."],"url":"http://arxiv.org/abs/2405.16986v1","category":"gr-qc"}
{"created":"2024-05-27 09:22:22","title":"DSU-Net: Dynamic Snake U-Net for 2-D Seismic First Break Picking","abstract":"In seismic exploration, identifying the first break (FB) is a critical component in establishing subsurface velocity models. Various automatic picking techniques based on deep neural networks have been developed to expedite this procedure. The most popular class is using semantic segmentation networks to pick on a shot gather called 2-dimensional (2-D) picking. Generally, 2-D segmentation-based picking methods input an image of a shot gather, and output a binary segmentation map, in which the maximum of each column is the location of FB. However, current designed segmentation networks is difficult to ensure the horizontal continuity of the segmentation. Additionally, FB jumps also exist in some areas, and it is not easy for current networks to detect such jumps. Therefore, it is important to pick as much as possible and ensure horizontal continuity. To alleviate this problem, we propose a novel semantic segmentation network for the 2-D seismic FB picking task, where we introduce the dynamic snake convolution into U-Net and call the new segmentation network dynamic-snake U-Net (DSU-Net). Specifically, we develop original dynamic-snake convolution (DSConv) in CV and propose a novel DSConv module, which can extract the horizontal continuous feature in the shallow feature of the shot gather. Many experiments have shown that DSU-Net demonstrates higher accuracy and robustness than the other 2-D segmentation-based models, achieving state-of-the-art (SOTA) performance in 2-D seismic field surveys. Particularly, it can effectively detect FB jumps and better ensure the horizontal continuity of FB. In addition, the ablation experiment and the anti-noise experiment, respectively, verify the optimal structure of the DSConv module and the robustness of the picking.","sentences":["In seismic exploration, identifying the first break (FB) is a critical component in establishing subsurface velocity models.","Various automatic picking techniques based on deep neural networks have been developed to expedite this procedure.","The most popular class is using semantic segmentation networks to pick on a shot gather called 2-dimensional (2-D) picking.","Generally, 2-D segmentation-based picking methods input an image of a shot gather, and output a binary segmentation map, in which the maximum of each column is the location of FB.","However, current designed segmentation networks is difficult to ensure the horizontal continuity of the segmentation.","Additionally, FB jumps also exist in some areas, and it is not easy for current networks to detect such jumps.","Therefore, it is important to pick as much as possible and ensure horizontal continuity.","To alleviate this problem, we propose a novel semantic segmentation network for the 2-D seismic FB picking task, where we introduce the dynamic snake convolution into U-Net and call the new segmentation network dynamic-snake U-Net (DSU-Net).","Specifically, we develop original dynamic-snake convolution (DSConv) in CV and propose a novel DSConv module, which can extract the horizontal continuous feature in the shallow feature of the shot gather.","Many experiments have shown that DSU-Net demonstrates higher accuracy and robustness than the other 2-D segmentation-based models, achieving state-of-the-art (SOTA) performance in 2-D seismic field surveys.","Particularly, it can effectively detect FB jumps and better ensure the horizontal continuity of FB.","In addition, the ablation experiment and the anti-noise experiment, respectively, verify the optimal structure of the DSConv module and the robustness of the picking."],"url":"http://arxiv.org/abs/2405.16980v1","category":"cs.CV"}
{"created":"2024-05-27 08:53:24","title":"Large Deviations of Gaussian Neural Networks with ReLU activation","abstract":"We prove a large deviation principle for deep neural networks with Gaussian weights and (at most linearly growing) activation functions. This generalises earlier work, in which bounded and continuous activation functions were considered. In practice, linearly growing activation functions such as ReLU are most commonly used. We furthermore simplify previous expressions for the rate function and a give power-series expansions for the ReLU case.","sentences":["We prove a large deviation principle for deep neural networks with Gaussian weights and (at most linearly growing) activation functions.","This generalises earlier work, in which bounded and continuous activation functions were considered.","In practice, linearly growing activation functions such as ReLU are most commonly used.","We furthermore simplify previous expressions for the rate function and a give power-series expansions for the ReLU case."],"url":"http://arxiv.org/abs/2405.16958v1","category":"stat.ML"}
{"created":"2024-05-27 08:28:09","title":"Construction of birational trilinear volumes via tensor rank criteria","abstract":"We provide effective methods to construct and manipulate trilinear birational maps $\\phi:(\\mathbb{P}^1)^3\\dashrightarrow \\mathbb{P}^3$ by establishing a novel connection between birationality and tensor rank. These yield four families of nonlinear birational transformations between 3D spaces that can be operated with enough flexibility for applications in computer-aided geometric design. More precisely, we describe the geometric constraints on the defining control points of the map that are necessary for birationality, and present constructions for such configurations. For adequately constrained control points, we prove that birationality is achieved if and only if a certain $2\\times 2\\times 2$ tensor has rank one. As a corollary, we prove that the locus of weights that ensure birationality is $\\mathbb{P}^1\\times\\mathbb{P}^1\\times\\mathbb{P}^1$. Additionally, we provide formulas for the inverse $\\phi^{-1}$ as well as the explicit defining equations of the irreducible components of the base loci. Finally, we introduce a notion of \"distance to birationality\" for trilinear rational maps, and explain how to continuously deform birational maps.","sentences":["We provide effective methods to construct and manipulate trilinear birational maps $\\phi:(\\mathbb{P}^1)^3\\dashrightarrow \\mathbb{P}^3$ by establishing a novel connection between birationality and tensor rank.","These yield four families of nonlinear birational transformations between 3D spaces that can be operated with enough flexibility for applications in computer-aided geometric design.","More precisely, we describe the geometric constraints on the defining control points of the map that are necessary for birationality, and present constructions for such configurations.","For adequately constrained control points, we prove that birationality is achieved if and only if a certain $2\\times 2\\times 2$ tensor has rank one.","As a corollary, we prove that the locus of weights that ensure birationality is $\\mathbb{P}^1\\times\\mathbb{P}^1\\times\\mathbb{P}^1$. Additionally, we provide formulas for the inverse $\\phi^{-1}$ as well as the explicit defining equations of the irreducible components of the base loci.","Finally, we introduce a notion of \"distance to birationality\" for trilinear rational maps, and explain how to continuously deform birational maps."],"url":"http://arxiv.org/abs/2405.16936v1","category":"math.AG"}
{"created":"2024-05-27 07:35:53","title":"An Unconstrained Formulation of Some Constrained Partial Differential Equations and its Application to Finite Neuron Methods","abstract":"In this paper, we present a new framework how a PDE with constraints can be formulated into a sequence of PDEs with no constraints, whose solutions are convergent to the solution of the PDE with constraints. This framework is then used to build a novel finite neuron method to solve the 2nd order elliptic equations with the Dirichlet boundary condition. Our algorithm is the first algorithm, proven to lead to shallow neural network solutions with an optimal H1 norm error. We show that a widely used penalized PDE, which imposes the Dirichlet boundary condition weakly can be interpreted as the first element of the sequence of PDEs within our framework. Furthermore, numerically, we show that it may not lead to the solution with the optimal H1 norm error bound in general. On the other hand, we theoretically demonstrate that the second and later elements of a sequence of PDEs can lead to an adequate solution with the optimal H1 norm error bound. A number of sample tests are performed to confirm the effectiveness of the proposed algorithm and the relevant theory.","sentences":["In this paper, we present a new framework how a PDE with constraints can be formulated into a sequence of PDEs with no constraints, whose solutions are convergent to the solution of the PDE with constraints.","This framework is then used to build a novel finite neuron method to solve the 2nd order elliptic equations with the Dirichlet boundary condition.","Our algorithm is the first algorithm, proven to lead to shallow neural network solutions with an optimal H1 norm error.","We show that a widely used penalized PDE, which imposes the Dirichlet boundary condition weakly can be interpreted as the first element of the sequence of PDEs within our framework.","Furthermore, numerically, we show that it may not lead to the solution with the optimal H1 norm error bound in general.","On the other hand, we theoretically demonstrate that the second and later elements of a sequence of PDEs can lead to an adequate solution with the optimal H1 norm error bound.","A number of sample tests are performed to confirm the effectiveness of the proposed algorithm and the relevant theory."],"url":"http://arxiv.org/abs/2405.16894v1","category":"math.NA"}
{"created":"2024-05-27 06:31:39","title":"An Investigation of Conformal Isometry Hypothesis for Grid Cells","abstract":"This paper investigates the conformal isometry hypothesis as a potential explanation for the emergence of hexagonal periodic patterns in the response maps of grid cells. The hypothesis posits that the activities of the population of grid cells form a high-dimensional vector in the neural space, representing the agent's self-position in 2D physical space. As the agent moves in the 2D physical space, the vector rotates in a 2D manifold in the neural space, driven by a recurrent neural network. The conformal isometry hypothesis proposes that this 2D manifold in the neural space is a conformally isometric embedding of the 2D physical space, in the sense that local displacements of the vector in neural space are proportional to local displacements of the agent in the physical space. Thus the 2D manifold forms an internal map of the 2D physical space, equipped with an internal metric. In this paper, we conduct numerical experiments to show that this hypothesis underlies the hexagon periodic patterns of grid cells. We also conduct theoretical analysis to further support this hypothesis. In addition, we propose a conformal modulation of the input velocity of the agent so that the recurrent neural network of grid cells satisfies the conformal isometry hypothesis automatically. To summarize, our work provides numerical and theoretical evidences for the conformal isometry hypothesis for grid cells and may serve as a foundation for further development of normative models of grid cells and beyond.","sentences":["This paper investigates the conformal isometry hypothesis as a potential explanation for the emergence of hexagonal periodic patterns in the response maps of grid cells.","The hypothesis posits that the activities of the population of grid cells form a high-dimensional vector in the neural space, representing the agent's self-position in 2D physical space.","As the agent moves in the 2D physical space, the vector rotates in a 2D manifold in the neural space, driven by a recurrent neural network.","The conformal isometry hypothesis proposes that this 2D manifold in the neural space is a conformally isometric embedding of the 2D physical space, in the sense that local displacements of the vector in neural space are proportional to local displacements of the agent in the physical space.","Thus the 2D manifold forms an internal map of the 2D physical space, equipped with an internal metric.","In this paper, we conduct numerical experiments to show that this hypothesis underlies the hexagon periodic patterns of grid cells.","We also conduct theoretical analysis to further support this hypothesis.","In addition, we propose a conformal modulation of the input velocity of the agent so that the recurrent neural network of grid cells satisfies the conformal isometry hypothesis automatically.","To summarize, our work provides numerical and theoretical evidences for the conformal isometry hypothesis for grid cells and may serve as a foundation for further development of normative models of grid cells and beyond."],"url":"http://arxiv.org/abs/2405.16865v1","category":"q-bio.NC"}
{"created":"2024-05-27 06:31:07","title":"Sparsity comparison of polytopal finite element methods","abstract":"In this work we compare crucial parameters for efficiency of different finite element methods for solving partial differential equations (PDEs) on polytopal meshes. We consider the Virtual Element Method (VEM) and different Discontinuous Galerkin (DG) methods, namely the Hybrid DG and Trefftz DG methods. The VEM is a conforming method, that can be seen as a generalization of the classic finite element method to arbitrary polytopal meshes. DG methods are non-conforming methods that offer high flexibility, but also come with high computational costs. Hybridization reduces these costs by introducing additional facet variables, onto which the computational costs can be transfered to. Trefftz DG methods achieve a similar reduction in complexity by selecting a special and smaller set of basis functions on each element. The association of computational costs to different geometrical entities (elements or facets) leads to differences in the performance of these methods on different grid types. This paper aims to compare the dependency of these approaches across different grid configurations.","sentences":["In this work we compare crucial parameters for efficiency of different finite element methods for solving partial differential equations (PDEs) on polytopal meshes.","We consider the Virtual Element Method (VEM) and different Discontinuous Galerkin (DG) methods, namely the Hybrid DG and Trefftz DG methods.","The VEM is a conforming method, that can be seen as a generalization of the classic finite element method to arbitrary polytopal meshes.","DG methods are non-conforming methods that offer high flexibility, but also come with high computational costs.","Hybridization reduces these costs by introducing additional facet variables, onto which the computational costs can be transfered to.","Trefftz DG methods achieve a similar reduction in complexity by selecting a special and smaller set of basis functions on each element.","The association of computational costs to different geometrical entities (elements or facets) leads to differences in the performance of these methods on different grid types.","This paper aims to compare the dependency of these approaches across different grid configurations."],"url":"http://arxiv.org/abs/2405.16864v1","category":"math.NA"}
{"created":"2024-05-27 05:52:13","title":"UniCompress: Enhancing Multi-Data Medical Image Compression with Knowledge Distillation","abstract":"In the field of medical image compression, Implicit Neural Representation (INR) networks have shown remarkable versatility due to their flexible compression ratios, yet they are constrained by a one-to-one fitting approach that results in lengthy encoding times. Our novel method, ``\\textbf{UniCompress}'', innovatively extends the compression capabilities of INR by being the first to compress multiple medical data blocks using a single INR network. By employing wavelet transforms and quantization, we introduce a codebook containing frequency domain information as a prior input to the INR network. This enhances the representational power of INR and provides distinctive conditioning for different image blocks. Furthermore, our research introduces a new technique for the knowledge distillation of implicit representations, simplifying complex model knowledge into more manageable formats to improve compression ratios. Extensive testing on CT and electron microscopy (EM) datasets has demonstrated that UniCompress outperforms traditional INR methods and commercial compression solutions like HEVC, especially in complex and high compression scenarios. Notably, compared to existing INR techniques, UniCompress achieves a 4$\\sim$5 times increase in compression speed, marking a significant advancement in the field of medical image compression. Codes will be publicly available.","sentences":["In the field of medical image compression, Implicit Neural Representation (INR) networks have shown remarkable versatility due to their flexible compression ratios, yet they are constrained by a one-to-one fitting approach that results in lengthy encoding times.","Our novel method, ``\\textbf{UniCompress}'', innovatively extends the compression capabilities of INR by being the first to compress multiple medical data blocks using a single INR network.","By employing wavelet transforms and quantization, we introduce a codebook containing frequency domain information as a prior input to the INR network.","This enhances the representational power of INR and provides distinctive conditioning for different image blocks.","Furthermore, our research introduces a new technique for the knowledge distillation of implicit representations, simplifying complex model knowledge into more manageable formats to improve compression ratios.","Extensive testing on CT and electron microscopy (EM) datasets has demonstrated that UniCompress outperforms traditional INR methods and commercial compression solutions like HEVC, especially in complex and high compression scenarios.","Notably, compared to existing INR techniques, UniCompress achieves a 4$\\sim$5 times increase in compression speed, marking a significant advancement in the field of medical image compression.","Codes will be publicly available."],"url":"http://arxiv.org/abs/2405.16850v1","category":"eess.IV"}
{"created":"2024-05-27 05:28:40","title":"Approximation of arbitrarily high-order PDEs by first-order hyperbolic relaxation","abstract":"We present a framework for constructing a first-order hyperbolic system whose solution approximates that of a desired higher-order evolution equation. Constructions of this kind have received increasing interest in recent years, and are potentially useful as either analytical or computational tools for understanding the corresponding higher-order equation. We perform a systematic analysis of a family of linear model equations and show that for each member of this family there is a stable hyperbolic approximation whose solution converges to that of the model equation in a certain limit. We then show through several examples that this approach can be applied successfully to a very wide range of nonlinear PDEs of practical interest.","sentences":["We present a framework for constructing a first-order hyperbolic system whose solution approximates that of a desired higher-order evolution equation.","Constructions of this kind have received increasing interest in recent years, and are potentially useful as either analytical or computational tools for understanding the corresponding higher-order equation.","We perform a systematic analysis of a family of linear model equations and show that for each member of this family there is a stable hyperbolic approximation whose solution converges to that of the model equation in a certain limit.","We then show through several examples that this approach can be applied successfully to a very wide range of nonlinear PDEs of practical interest."],"url":"http://arxiv.org/abs/2405.16841v1","category":"math.AP"}
{"created":"2024-05-27 04:48:09","title":"Structure-preserving finite element methods for computing dynamics of rotating Bose-Einstein condensate","abstract":"This work is concerned with the construction and analysis of structure-preserving Galerkin methods for computing the dynamics of rotating Bose-Einstein condensate (BEC) based on the Gross-Pitaevskii equation with angular momentum rotation. Due to the presence of the rotation term, constructing finite element methods (FEMs) that preserve both mass and energy remains an unresolved issue, particularly in the context of nonconforming FEMs. Furthermore, in comparison to existing works, we provide a comprehensive convergence analysis, offering a thorough demonstration of the methods' optimal and high-order convergence properties. Finally, extensive numerical results are presented to check the theoretical analysis of the structure-preserving numerical method for rotating BEC, and the quantized vortex lattice's behavior is scrutinized through a series of numerical tests.","sentences":["This work is concerned with the construction and analysis of structure-preserving Galerkin methods for computing the dynamics of rotating Bose-Einstein condensate (BEC) based on the Gross-Pitaevskii equation with angular momentum rotation.","Due to the presence of the rotation term, constructing finite element methods (FEMs) that preserve both mass and energy remains an unresolved issue, particularly in the context of nonconforming FEMs.","Furthermore, in comparison to existing works, we provide a comprehensive convergence analysis, offering a thorough demonstration of the methods' optimal and high-order convergence properties.","Finally, extensive numerical results are presented to check the theoretical analysis of the structure-preserving numerical method for rotating BEC, and the quantized vortex lattice's behavior is scrutinized through a series of numerical tests."],"url":"http://arxiv.org/abs/2405.16827v1","category":"math.NA"}
{"created":"2024-05-27 04:22:25","title":"Controlling Rate, Distortion, and Realism: Towards a Single Comprehensive Neural Image Compression Model","abstract":"In recent years, neural network-driven image compression (NIC) has gained significant attention. Some works adopt deep generative models such as GANs and diffusion models to enhance perceptual quality (realism). A critical obstacle of these generative NIC methods is that each model is optimized for a single bit rate. Consequently, multiple models are required to compress images to different bit rates, which is impractical for real-world applications. To tackle this issue, we propose a variable-rate generative NIC model. Specifically, we explore several discriminator designs tailored for the variable-rate approach and introduce a novel adversarial loss. Moreover, by incorporating the newly proposed multi-realism technique, our method allows the users to adjust the bit rate, distortion, and realism with a single model, achieving ultra-controllability. Unlike existing variable-rate generative NIC models, our method matches or surpasses the performance of state-of-the-art single-rate generative NIC models while covering a wide range of bit rates using just one model. Code will be available at https://github.com/iwa-shi/CRDR","sentences":["In recent years, neural network-driven image compression (NIC) has gained significant attention.","Some works adopt deep generative models such as GANs and diffusion models to enhance perceptual quality (realism).","A critical obstacle of these generative NIC methods is that each model is optimized for a single bit rate.","Consequently, multiple models are required to compress images to different bit rates, which is impractical for real-world applications.","To tackle this issue, we propose a variable-rate generative NIC model.","Specifically, we explore several discriminator designs tailored for the variable-rate approach and introduce a novel adversarial loss.","Moreover, by incorporating the newly proposed multi-realism technique, our method allows the users to adjust the bit rate, distortion, and realism with a single model, achieving ultra-controllability.","Unlike existing variable-rate generative NIC models, our method matches or surpasses the performance of state-of-the-art single-rate generative NIC models while covering a wide range of bit rates using just one model.","Code will be available at https://github.com/iwa-shi/CRDR"],"url":"http://arxiv.org/abs/2405.16817v1","category":"cs.CV"}
{"created":"2024-05-27 04:11:03","title":"Enhanced Geological Prediction for Tunnel Excavation Using Full Waveform Inversion Integrating Sobolev Space Regularization with a Quadratic Penalty Method","abstract":"In the process of tunnel excavation, advanced geological prediction technology has become indispensable for safe, economical, and efficient tunnel construction. Although traditional methods such as drilling and geological analysis are effective, they typically involve destructive processes, carry high risks, and incur significant costs. In contrast, non-destructive geophysical exploration offers a more convenient and economical alternative. However, the accuracy and precision of these non-destructive methods can be severely compromised by complex geological structures and environmental noise. To address these challenges effectively, a novel approach using frequency domain full waveform inversion (FWI), based on a penalty method and Sobolev space regularization, has been proposed to enhance the performance of non-destructive predictions. The proposed method constructs a soft-constrained optimization problem by restructuring the misfit function into a combination of data misfit and wave equation drive terms to enhance convexity. Additionally, it semi-extends the search space to both the wavefield and the model parameters to mitigate the strong nonlinearity of the optimization, facilitating high-resolution inversion. Furthermore, a Sobolev space regularization algorithm is introduced to flexibly adjust the regularization path, addressing issues related to noise and artefacts to improve the robustness of the inversion. We evaluated the proposed FWI with a tunnel fault model by comparing the results of the proposed method with those of traditional Tikhonov regularization and total variation regularization FWI methods. The results confirm the superior performance of the proposed algorithm as expected.","sentences":["In the process of tunnel excavation, advanced geological prediction technology has become indispensable for safe, economical, and efficient tunnel construction.","Although traditional methods such as drilling and geological analysis are effective, they typically involve destructive processes, carry high risks, and incur significant costs.","In contrast, non-destructive geophysical exploration offers a more convenient and economical alternative.","However, the accuracy and precision of these non-destructive methods can be severely compromised by complex geological structures and environmental noise.","To address these challenges effectively, a novel approach using frequency domain full waveform inversion (FWI), based on a penalty method and Sobolev space regularization, has been proposed to enhance the performance of non-destructive predictions.","The proposed method constructs a soft-constrained optimization problem by restructuring the misfit function into a combination of data misfit and wave equation drive terms to enhance convexity.","Additionally, it semi-extends the search space to both the wavefield and the model parameters to mitigate the strong nonlinearity of the optimization, facilitating high-resolution inversion.","Furthermore, a Sobolev space regularization algorithm is introduced to flexibly adjust the regularization path, addressing issues related to noise and artefacts to improve the robustness of the inversion.","We evaluated the proposed FWI with a tunnel fault model by comparing the results of the proposed method with those of traditional Tikhonov regularization and total variation regularization FWI methods.","The results confirm the superior performance of the proposed algorithm as expected."],"url":"http://arxiv.org/abs/2405.16812v1","category":"physics.geo-ph"}
{"created":"2024-05-27 03:21:53","title":"Classical and quantum thermodynamics described as a system-bath model: The dimensionless minimum work principle","abstract":"We formulate a thermodynamic theory applicable to both classical and quantum systems. These systems are depicted as thermodynamic system-bath models capable of handling isothermal, isentropic, thermostatic, and entropic processes. Our approach is based on the use of a dimensionless thermodynamic potential expressed as a function of the intensive and extensive thermodynamic variables. Using the principles of dimensionless minimum work and dimensionless maximum entropy derived from quasi-static changes of external perturbations and temperature, we obtain the Massieu-Planck potentials as entropic potentials and the Helmholtz-Gibbs potentials as free energy. These potentials can be interconverted through time-dependent Legendre transformations. Our results are verified numerically for an anharmonic Brownian system described in phase space using the low-temperature quantum Fokker-Planck equations in the quantum case and the Kramers equation in the classical case, both developed for the thermodynamic system-bath model. Thus, we clarify the conditions for thermodynamics to be valid even for small systems described by Hamiltonians and establish a basis for extending thermodynamics to non-equilibrium conditions.","sentences":["We formulate a thermodynamic theory applicable to both classical and quantum systems.","These systems are depicted as thermodynamic system-bath models capable of handling isothermal, isentropic, thermostatic, and entropic processes.","Our approach is based on the use of a dimensionless thermodynamic potential expressed as a function of the intensive and extensive thermodynamic variables.","Using the principles of dimensionless minimum work and dimensionless maximum entropy derived from quasi-static changes of external perturbations and temperature, we obtain the Massieu-Planck potentials as entropic potentials and the Helmholtz-Gibbs potentials as free energy.","These potentials can be interconverted through time-dependent Legendre transformations.","Our results are verified numerically for an anharmonic Brownian system described in phase space using the low-temperature quantum Fokker-Planck equations in the quantum case and the Kramers equation in the classical case, both developed for the thermodynamic system-bath model.","Thus, we clarify the conditions for thermodynamics to be valid even for small systems described by Hamiltonians and establish a basis for extending thermodynamics to non-equilibrium conditions."],"url":"http://arxiv.org/abs/2405.16787v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-27 03:11:57","title":"The second-order zero differential uniformity of the swapped inverse functions over finite fields","abstract":"The Feistel Boomerang Connectivity Table (FBCT) was proposed as the feistel counterpart of the Boomerang Connectivity Table. The entries of the FBCT are actually related to the second-order zero differential spectrum. Recently, several results on the second-order zero differential uniformity of some functions were introduced. However, almost all of them were focused on power functions, and there are only few results on non-power functions. In this paper, we investigate the second-order zero differential uniformity of the swapped inverse functions, which are functions obtained from swapping two points in the inverse function. We also present the second-order zero differential spectrum of the swapped inverse functions for certain cases. In particular, this paper is the first result to characterize classes of non-power functions with the second-order zero differential uniformity equal to 4, in even characteristic.","sentences":["The Feistel Boomerang Connectivity Table (FBCT) was proposed as the feistel counterpart of the Boomerang Connectivity Table.","The entries of the FBCT are actually related to the second-order zero differential spectrum.","Recently, several results on the second-order zero differential uniformity of some functions were introduced.","However, almost all of them were focused on power functions, and there are only few results on non-power functions.","In this paper, we investigate the second-order zero differential uniformity of the swapped inverse functions, which are functions obtained from swapping two points in the inverse function.","We also present the second-order zero differential spectrum of the swapped inverse functions for certain cases.","In particular, this paper is the first result to characterize classes of non-power functions with the second-order zero differential uniformity equal to 4, in even characteristic."],"url":"http://arxiv.org/abs/2405.16784v1","category":"cs.IT"}
{"created":"2024-05-27 03:03:50","title":"Unusual switch from low-temperature T-quadratic resistivity in the underdoped pseudogap phase of cuprate superconductors to low-temperature T-linear resistivity in the overdoped strange-metal phase","abstract":"The transport experiments demonstrate a dramatic switch from the low-temperature T-linear resistivity in the overdoped strange-metal phase to the T-quadratic resistivity in the underdoped pseudogap phase of cuprate superconductors, however, a consensus on the origin of this switch is still lacking. Here the low-temperature resistivity in the underdoped pseudogap phase of cuprate superconductors is investigated using the Boltzmann transport equation. The low-temperature resistivity originates from the electron umklapp scattering mediated by the spin excitation. However, the dominant contribution to the resistivity mainly comes from the antinodal umklapp scattering. In particular, a low temperature $T_{scale}$ scales with $\\Delta^{2}_{p}$ in the underdoped regime due to the opening of a momentum-dependent spin pseudogap, where $\\Delta_{p}$ is the minimal umklapp vector at the antinode. Notably, this $T_{scale}$ as a function of doping presents a similar behavior of the antinodal spin pseudogap crossover temperature, i.e., $T_{scale}$ decreases with the increase of doping in the underdoped regime, and then is reduced to a very low temperature in the overdoped regime. In the underdoped regime, the resistivity is T-quadratic in the low temperatures below $T_{scale}$ with the strength of the T-quadratic resistivity that weakens as the doping is raised. However, in the overdoped regime, the resistivity is T-linear in the low temperatures above $T_{scale}$. The result in this paper together with the recent study on the electrical transport in the overdoped regime therefore show that the electron Umklapp scattering from a spin excitation responsible for the low-temperature T-linear resistivity in the overdoped strange-metal phase naturally produces the low-temperature T-quadratic resistivity in the underdoped pseudogap phase resulting from the opening of a momentum dependent spin pseudogap.","sentences":["The transport experiments demonstrate a dramatic switch from the low-temperature T-linear resistivity in the overdoped strange-metal phase to the T-quadratic resistivity in the underdoped pseudogap phase of cuprate superconductors, however, a consensus on the origin of this switch is still lacking.","Here the low-temperature resistivity in the underdoped pseudogap phase of cuprate superconductors is investigated using the Boltzmann transport equation.","The low-temperature resistivity originates from the electron umklapp scattering mediated by the spin excitation.","However, the dominant contribution to the resistivity mainly comes from the antinodal umklapp scattering.","In particular, a low temperature $T_{scale}$ scales with $\\Delta^{2}_{p}$ in the underdoped regime due to the opening of a momentum-dependent spin pseudogap, where $\\Delta_{p}$ is the minimal umklapp vector at the antinode.","Notably, this $T_{scale}$ as a function of doping presents a similar behavior of the antinodal spin pseudogap crossover temperature, i.e., $T_{scale}$ decreases with the increase of doping in the underdoped regime, and then is reduced to a very low temperature in the overdoped regime.","In the underdoped regime, the resistivity is T-quadratic in the low temperatures below $T_{scale}$ with the strength of the T-quadratic resistivity that weakens as the doping is raised.","However, in the overdoped regime, the resistivity is T-linear in the low temperatures above $T_{scale}$. The result in this paper together with the recent study on the electrical transport in the overdoped regime therefore show that the electron Umklapp scattering from a spin excitation responsible for the low-temperature T-linear resistivity in the overdoped strange-metal phase naturally produces the low-temperature T-quadratic resistivity in the underdoped pseudogap phase resulting from the opening of a momentum dependent spin pseudogap."],"url":"http://arxiv.org/abs/2405.16778v1","category":"cond-mat.supr-con"}
{"created":"2024-05-27 03:01:57","title":"The Chern-Simons Functional Integral, Kauffman's Bracket Polynomial, and other link invariants","abstract":"We study Chern-Simons Gauge Theory in axial gauge on ${\\mathbb R}^3.$ This theory has a quadratic Lagrangian and therefore expectations can be computed nonperturbatively by explicit formulas, giving an (unbounded) linear functional on a space of polynomial functions in the gauge fields, as a mathematically well-defined avatar of the formal functional integral. We use differential-geometric methods to extend the definition of this linear functional to expectations of products of Wilson loops corresponding to oriented links in ${\\mathbb R}^3,$ and derive skein relations for them. In the case $G=SU(2)$ we show that these skein relations are closely related to those of the Kauffman bracket polynomial, which is closely related to the Jones polynomial. We also study the case of groups of higher rank. We note that in the absence of a cubic term in the action, there is no quantization condition on the coupling $\\lambda,$ which can be any complex number. This is in line with the fact that the Jones polynomial, in contrast to the manifold invariants of Witten and Reshetikhin-Turaev, is defined for any value of the coupling. The appearance of the parameter $e^{\\frac1{2\\lambda}}$ in the expectations and skein relations is also natural. Likewise, the extension of the theory to noncompact groups presents no difficulties. Finally we show how computations similar to ours, but for gauge fields in two dimensions, yield the Goldman bracket.","sentences":["We study Chern-Simons Gauge Theory in axial gauge on ${\\mathbb R}^3.$","This theory has a quadratic Lagrangian and therefore expectations can be computed nonperturbatively by explicit formulas, giving an (unbounded) linear functional on a space of polynomial functions in the gauge fields, as a mathematically well-defined avatar of the formal functional integral.","We use differential-geometric methods to extend the definition of this linear functional to expectations of products of Wilson loops corresponding to oriented links in ${\\mathbb R}^3,$ and derive skein relations for them.","In the case $G=SU(2)$ we show that these skein relations are closely related to those of the Kauffman bracket polynomial, which is closely related to the Jones polynomial.","We also study the case of groups of higher rank.","We note that in the absence of a cubic term in the action, there is no quantization condition on the coupling $\\lambda,$ which can be any complex number.","This is in line with the fact that the Jones polynomial, in contrast to the manifold invariants of Witten and Reshetikhin-Turaev, is defined for any value of the coupling.","The appearance of the parameter $e^{\\frac1{2\\lambda}}$ in the expectations and skein relations is also natural.","Likewise, the extension of the theory to noncompact groups presents no difficulties.","Finally we show how computations similar to ours, but for gauge fields in two dimensions, yield the Goldman bracket."],"url":"http://arxiv.org/abs/2405.16775v1","category":"math.DG"}
{"created":"2024-05-27 02:42:16","title":"Physics informed cell representations for variational formulation of multiscale problems","abstract":"With the rapid advancement of graphical processing units, Physics-Informed Neural Networks (PINNs) are emerging as a promising tool for solving partial differential equations (PDEs). However, PINNs are not well suited for solving PDEs with multiscale features, particularly suffering from slow convergence and poor accuracy. To address this limitation of PINNs, this article proposes physics-informed cell representations for resolving multiscale Poisson problems using a model architecture consisting of multilevel multiresolution grids coupled with a multilayer perceptron (MLP). The grid parameters (i.e., the level-dependent feature vectors) and the MLP parameters (i.e., the weights and biases) are determined using gradient-descent based optimization. The variational (weak) form based loss function accelerates computation by allowing the linear interpolation of feature vectors within grid cells. This cell-based MLP model also facilitates the use of a decoupled training scheme for Dirichlet boundary conditions and a parameter-sharing scheme for periodic boundary conditions, delivering superior accuracy compared to conventional PINNs. Furthermore, the numerical examples highlight improved speed and accuracy in solving PDEs with nonlinear or high-frequency boundary conditions and provide insights into hyperparameter selection. In essence, by cell-based MLP model along with the parallel tiny-cuda-nn library, our implementation improves convergence speed and numerical accuracy.","sentences":["With the rapid advancement of graphical processing units, Physics-Informed Neural Networks (PINNs) are emerging as a promising tool for solving partial differential equations (PDEs).","However, PINNs are not well suited for solving PDEs with multiscale features, particularly suffering from slow convergence and poor accuracy.","To address this limitation of PINNs, this article proposes physics-informed cell representations for resolving multiscale Poisson problems using a model architecture consisting of multilevel multiresolution grids coupled with a multilayer perceptron (MLP).","The grid parameters (i.e., the level-dependent feature vectors) and the MLP parameters (i.e., the weights and biases) are determined using gradient-descent based optimization.","The variational (weak) form based loss function accelerates computation by allowing the linear interpolation of feature vectors within grid cells.","This cell-based MLP model also facilitates the use of a decoupled training scheme for Dirichlet boundary conditions and a parameter-sharing scheme for periodic boundary conditions, delivering superior accuracy compared to conventional PINNs.","Furthermore, the numerical examples highlight improved speed and accuracy in solving PDEs with nonlinear or high-frequency boundary conditions and provide insights into hyperparameter selection.","In essence, by cell-based MLP model along with the parallel tiny-cuda-nn library, our implementation improves convergence speed and numerical accuracy."],"url":"http://arxiv.org/abs/2405.16770v1","category":"cs.LG"}
{"created":"2024-05-27 02:24:57","title":"Transport of Algebraic Structure to Latent Embeddings","abstract":"Machine learning often aims to produce latent embeddings of inputs which lie in a larger, abstract mathematical space. For example, in the field of 3D modeling, subsets of Euclidean space can be embedded as vectors using implicit neural representations. Such subsets also have a natural algebraic structure including operations (e.g., union) and corresponding laws (e.g., associativity). How can we learn to \"union\" two sets using only their latent embeddings while respecting associativity? We propose a general procedure for parameterizing latent space operations that are provably consistent with the laws on the input space. This is achieved by learning a bijection from the latent space to a carefully designed mirrored algebra which is constructed on Euclidean space in accordance with desired laws. We evaluate these structural transport nets for a range of mirrored algebras against baselines that operate directly on the latent space. Our experiments provide strong evidence that respecting the underlying algebraic structure of the input space is key for learning accurate and self-consistent operations.","sentences":["Machine learning often aims to produce latent embeddings of inputs which lie in a larger, abstract mathematical space.","For example, in the field of 3D modeling, subsets of Euclidean space can be embedded as vectors using implicit neural representations.","Such subsets also have a natural algebraic structure including operations (e.g., union) and corresponding laws (e.g., associativity).","How can we learn to \"union\" two sets using only their latent embeddings while respecting associativity?","We propose a general procedure for parameterizing latent space operations that are provably consistent with the laws on the input space.","This is achieved by learning a bijection from the latent space to a carefully designed mirrored algebra which is constructed on Euclidean space in accordance with desired laws.","We evaluate these structural transport nets for a range of mirrored algebras against baselines that operate directly on the latent space.","Our experiments provide strong evidence that respecting the underlying algebraic structure of the input space is key for learning accurate and self-consistent operations."],"url":"http://arxiv.org/abs/2405.16763v1","category":"cs.LG"}
{"created":"2024-05-27 01:58:23","title":"Symmetry-Informed Governing Equation Discovery","abstract":"Despite the advancements in learning governing differential equations from observations of dynamical systems, data-driven methods are often unaware of fundamental physical laws, such as frame invariance. As a result, these algorithms may search an unnecessarily large space and discover equations that are less accurate or overly complex. In this paper, we propose to leverage symmetry in automated equation discovery to compress the equation search space and improve the accuracy and simplicity of the learned equations. Specifically, we derive equivariance constraints from the time-independent symmetries of ODEs. Depending on the types of symmetries, we develop a pipeline for incorporating symmetry constraints into various equation discovery algorithms, including sparse regression and genetic programming. In experiments across a diverse range of dynamical systems, our approach demonstrates better robustness against noise and recovers governing equations with significantly higher probability than baselines without symmetry.","sentences":["Despite the advancements in learning governing differential equations from observations of dynamical systems, data-driven methods are often unaware of fundamental physical laws, such as frame invariance.","As a result, these algorithms may search an unnecessarily large space and discover equations that are less accurate or overly complex.","In this paper, we propose to leverage symmetry in automated equation discovery to compress the equation search space and improve the accuracy and simplicity of the learned equations.","Specifically, we derive equivariance constraints from the time-independent symmetries of ODEs.","Depending on the types of symmetries, we develop a pipeline for incorporating symmetry constraints into various equation discovery algorithms, including sparse regression and genetic programming.","In experiments across a diverse range of dynamical systems, our approach demonstrates better robustness against noise and recovers governing equations with significantly higher probability than baselines without symmetry."],"url":"http://arxiv.org/abs/2405.16756v1","category":"cs.LG"}
{"created":"2024-05-27 01:20:48","title":"A Study on Magnetic-sensitivity Wavelength Position of the Working Line Used by the Full-Disk Magnetograph onboard the Advanced Space based Solar Observatory (ASO-S/FMG)","abstract":"Utilizing data from the $Solar$ $Magnetism$ and $Activity$ $Telescope$ (SMAT), analytical solutions of polarized radiative transfer equations, and in-orbit test data from the Full-disk Magnetograph (FMG) onboard the Advanced Space based Solar Observatory (ASO-S), this study reveals the magnetic-sensitivity spectral positions for the Fe {\\sc i} $\\lambda$5234.19 A, working line used by FMG. From the experimental data of SMAT, it is found that the most sensitivity position is located at the line center for linear polarization (Stokes-Q/U), while it is about -0.07 A away from the line center for circular polarization (Stokes-V). Moreover, both the theoretical analysis and the in-orbit test data analysis of FMG prove again the above results. Additionally, the theoretical analysis suggests the presence of distinct spectral pockets (centered at 0.08-0.15 A) from the line, harboring intense magnetic sensitivity across all three Stokes parameters. Striking a balance between high sensitivity for both linear and circular polarization while capturing additional valuable information, a spectral position of -0.08 A emerges as the champion for routine FMG magnetic-field observations.","sentences":["Utilizing data from the $Solar$ $Magnetism$ and $Activity$ $Telescope$ (SMAT), analytical solutions of polarized radiative transfer equations, and in-orbit test data from the Full-disk Magnetograph (FMG) onboard the Advanced Space based Solar Observatory (ASO-S), this study reveals the magnetic-sensitivity spectral positions for the Fe {\\sc i} $\\lambda$5234.19 A, working line used by FMG.","From the experimental data of SMAT, it is found that the most sensitivity position is located at the line center for linear polarization (Stokes-Q/U), while it is about -0.07 A away from the line center for circular polarization (Stokes-V).","Moreover, both the theoretical analysis and the in-orbit test data analysis of FMG prove again the above results.","Additionally, the theoretical analysis suggests the presence of distinct spectral pockets (centered at 0.08-0.15 A) from the line, harboring intense magnetic sensitivity across all three Stokes parameters.","Striking a balance between high sensitivity for both linear and circular polarization while capturing additional valuable information, a spectral position of -0.08 A emerges as the champion for routine FMG magnetic-field observations."],"url":"http://arxiv.org/abs/2405.16741v1","category":"astro-ph.SR"}
{"created":"2024-05-26 23:37:56","title":"A parabolic PDE-based approach to Borell--Brascamp--Lieb inequality","abstract":"In this paper, we provide a new PDE proof for the celebrated Borell--Brascamp--Lieb inequality. Our approach reveals a deep connection between the Borell--Brascamp--Lieb inequality and properties of diffusion equations of porous medium type pertaining to the large time asymptotics and preservation of a generalized concavity of the solutions. We also recover the equality condition in the special case of the Pr\\'ekopa--Leindler inequality by further exploiting known properties of the heat equation including the eventual log-concavity and backward uniqueness of solutions.","sentences":["In this paper, we provide a new PDE proof for the celebrated Borell--Brascamp--Lieb inequality.","Our approach reveals a deep connection between the Borell--Brascamp--Lieb inequality and properties of diffusion equations of porous medium type pertaining to the large time asymptotics and preservation of a generalized concavity of the solutions.","We also recover the equality condition in the special case of the Pr\\'ekopa--Leindler inequality by further exploiting known properties of the heat equation including the eventual log-concavity and backward uniqueness of solutions."],"url":"http://arxiv.org/abs/2405.16721v1","category":"math.AP"}
{"created":"2024-05-26 21:35:38","title":"On the Phragm\u00e9n-Lindel\u00f6f and the superposition principles for the $p$-Laplacian","abstract":"We study sub and supersolutions for the $p$-Laplace type elliptic equation of the form $$-\\Delta_p u-V|u|^{p-2}u=0\\quad\\text{in $\\Omega$},$$ where $\\Omega$ is a radially symmetric domain in ${\\mathbb{R}}^N$ and $V(x)\\ge 0$ is a continuous potential such that the solutions of the equation satisfy the comparison principle on bounded subdomains of $\\Omega$. In this work we establish a superposition principle and then use it to develop a version of a Phragm\\'{e}n-Lindel\\\"{o}f comparison principle in the case $p\\ge 2$. Moreover, by applying this principle to the case of Hardy-type potentials we recover and improve a number of known lower and upper estimates for sub and supersolutions.","sentences":["We study sub and supersolutions for the $p$-Laplace type elliptic equation of the form $$-\\Delta_p u-V|u|^{p-2}u=0\\quad\\text{in $\\Omega$},$$ where $\\Omega$ is a radially symmetric domain in ${\\mathbb{R}}^N$ and $V(x)\\ge 0$ is a continuous potential such that the solutions of the equation satisfy the comparison principle on bounded subdomains of $\\Omega$. In this work we establish a superposition principle and then use it to develop a version of a Phragm\\'{e}n-Lindel\\\"{o}f comparison principle in the case $p\\ge 2$.","Moreover, by applying this principle to the case of Hardy-type potentials we recover and improve a number of known lower and upper estimates for sub and supersolutions."],"url":"http://arxiv.org/abs/2405.16705v1","category":"math.AP"}
{"created":"2024-05-26 21:33:28","title":"The three-point Gaudin model and branched coverings of the Riemann sphere","abstract":"We study the three-point $\\mathfrak{sl_2}$-Gaudin model. In this case the compactification of the parameter space is $\\overline{M_{0,4}(\\mathbb{C})}$, which is the Riemann sphere. We analyze these sphere coverings by the joint spectrum of the Gaudin Hamiltonians treating them as algebraic curves. We write equations of these curves as determinants of tridiagonal matrices and observe that their branch points form remarkable patterns.","sentences":["We study the three-point $\\mathfrak{sl_2}$-Gaudin model.","In this case the compactification of the parameter space is $\\overline{M_{0,4}(\\mathbb{C})}$, which is the Riemann sphere.","We analyze these sphere coverings by the joint spectrum of the Gaudin Hamiltonians treating them as algebraic curves.","We write equations of these curves as determinants of tridiagonal matrices and observe that their branch points form remarkable patterns."],"url":"http://arxiv.org/abs/2405.16703v1","category":"math-ph"}
{"created":"2024-05-26 21:25:50","title":"The Phase Space Distance Between Collider Events","abstract":"How can one fully harness the power of physics encoded in relativistic $N$-body phase space? Topologically, phase space is isomorphic to the product space of a simplex and a hypersphere and can be equipped with explicit coordinates and a Riemannian metric. This natural structure that scaffolds the space on which all collider physics events live opens up new directions for machine learning applications and implementation. Here we present a detailed construction of the phase space manifold and its differential line element, identifying particle ordering prescriptions that ensure that the metric satisfies necessary properties. We apply the phase space metric to several binary classification tasks, including discrimination of high-multiplicity resonance decays or boosted hadronic decays of electroweak bosons from QCD processes, and demonstrate powerful performance on simulated data. Our work demonstrates the many benefits of promoting phase space from merely a background on which calculations take place to being geometrically entwined with a theory's dynamics.","sentences":["How can one fully harness the power of physics encoded in relativistic $N$-body phase space?","Topologically, phase space is isomorphic to the product space of a simplex and a hypersphere and can be equipped with explicit coordinates and a Riemannian metric.","This natural structure that scaffolds the space on which all collider physics events live opens up new directions for machine learning applications and implementation.","Here we present a detailed construction of the phase space manifold and its differential line element, identifying particle ordering prescriptions that ensure that the metric satisfies necessary properties.","We apply the phase space metric to several binary classification tasks, including discrimination of high-multiplicity resonance decays or boosted hadronic decays of electroweak bosons from QCD processes, and demonstrate powerful performance on simulated data.","Our work demonstrates the many benefits of promoting phase space from merely a background on which calculations take place to being geometrically entwined with a theory's dynamics."],"url":"http://arxiv.org/abs/2405.16698v1","category":"hep-ph"}
{"created":"2024-05-26 21:07:21","title":"How many samples are needed to train a deep neural network?","abstract":"Neural networks have become standard tools in many areas, yet many important statistical questions remain open. This paper studies the question of how much data are needed to train a ReLU feed-forward neural network. Our theoretical and empirical results suggest that the generalization error of ReLU feed-forward neural networks scales at the rate $1/\\sqrt{n}$ in the sample size $n$ rather than the usual \"parametric rate\" $1/n$. Thus, broadly speaking, our results underpin the common belief that neural networks need \"many\" training samples.","sentences":["Neural networks have become standard tools in many areas, yet many important statistical questions remain open.","This paper studies the question of how much data are needed to train a ReLU feed-forward neural network.","Our theoretical and empirical results suggest that the generalization error of ReLU feed-forward neural networks scales at the rate $1/\\sqrt{n}$ in the sample size $n$ rather than the usual \"parametric rate\" $1/n$. Thus, broadly speaking, our results underpin the common belief that neural networks need \"many\" training samples."],"url":"http://arxiv.org/abs/2405.16696v1","category":"math.ST"}
{"created":"2024-05-26 21:00:30","title":"Oscillations in neuronal activity: a neuron-centered spatiotemporal model of the Unfolded Protein Response in prion diseases","abstract":"Many neurodegenerative diseases (NDs) are characterized by the slow spatial spread of toxic protein species in the brain. The toxic proteins can induce neuronal stress, triggering the Unfolded Protein Response (UPR), which slows or stops protein translation and can indirectly reduce the toxic load. However, the UPR may also trigger processes leading to apoptotic cell death and the UPR is implicated in the progression of several NDs. In this paper, we develop a novel mathematical model to describe the spatiotemporal dynamics of the UPR mechanism for prion diseases. Our model is centered around a single neuron, with representative proteins P (healthy) and S (toxic) interacting with heterodimer dynamics (S interacts with P to form two S's). The model takes the form of a coupled system of nonlinear reaction-diffusion equations with a delayed, nonlinear flux for P (delay from the UPR). Through the delay, we find parameter regimes that exhibit oscillations in the P- and S-protein levels. We find that oscillations are more pronounced when the S-clearance rate and S-diffusivity are small in comparison to the P-clearance rate and P-diffusivity, respectively. The oscillations become more pronounced as delays in initiating the UPR increase. We also consider quasi-realistic clinical parameters to understand how possible drug therapies can alter the course of a prion disease. We find that decreasing the production of P, decreasing the recruitment rate, increasing the diffusivity of S, increasing the UPR S-threshold, and increasing the S clearance rate appear to be the most powerful modifications to reduce the mean UPR intensity and potentially moderate the disease progression.","sentences":["Many neurodegenerative diseases (NDs) are characterized by the slow spatial spread of toxic protein species in the brain.","The toxic proteins can induce neuronal stress, triggering the Unfolded Protein Response (UPR), which slows or stops protein translation and can indirectly reduce the toxic load.","However, the UPR may also trigger processes leading to apoptotic cell death and the UPR is implicated in the progression of several NDs.","In this paper, we develop a novel mathematical model to describe the spatiotemporal dynamics of the UPR mechanism for prion diseases.","Our model is centered around a single neuron, with representative proteins P (healthy) and S (toxic) interacting with heterodimer dynamics (S interacts with P to form two S's).","The model takes the form of a coupled system of nonlinear reaction-diffusion equations with a delayed, nonlinear flux for P (delay from the UPR).","Through the delay, we find parameter regimes that exhibit oscillations in the P- and S-protein levels.","We find that oscillations are more pronounced when the S-clearance rate and S-diffusivity are small in comparison to the P-clearance rate and P-diffusivity, respectively.","The oscillations become more pronounced as delays in initiating the UPR increase.","We also consider quasi-realistic clinical parameters to understand how possible drug therapies can alter the course of a prion disease.","We find that decreasing the production of P, decreasing the recruitment rate, increasing the diffusivity of S, increasing the UPR S-threshold, and increasing the S clearance rate appear to be the most powerful modifications to reduce the mean UPR intensity and potentially moderate the disease progression."],"url":"http://arxiv.org/abs/2405.16695v1","category":"q-bio.NC"}
{"created":"2024-05-26 20:33:08","title":"gzip Predicts Data-dependent Scaling Laws","abstract":"Past work has established scaling laws that predict the performance of a neural language model (LM) as a function of its parameter count and the number of tokens it's trained on, enabling optimal allocation of a fixed compute budget. Are these scaling laws agnostic to training data as some prior work suggests? We generate training datasets of varying complexities by modulating the syntactic properties of a PCFG, finding that 1) scaling laws are sensitive to differences in data complexity and that 2) gzip, a compression algorithm, is an effective predictor of how data complexity impacts scaling properties. We propose a new data-dependent scaling law for LM's that accounts for the training data's gzip-compressibility; its compute-optimal frontier increases in dataset size preference (over parameter count preference) as training data becomes harder to compress.","sentences":["Past work has established scaling laws that predict the performance of a neural language model (LM) as a function of its parameter count and the number of tokens it's trained on, enabling optimal allocation of a fixed compute budget.","Are these scaling laws agnostic to training data as some prior work suggests?","We generate training datasets of varying complexities by modulating the syntactic properties of a PCFG, finding that 1) scaling laws are sensitive to differences in data complexity and that 2) gzip, a compression algorithm, is an effective predictor of how data complexity impacts scaling properties.","We propose a new data-dependent scaling law for LM's that accounts for the training data's gzip-compressibility; its compute-optimal frontier increases in dataset size preference (over parameter count preference) as training data becomes harder to compress."],"url":"http://arxiv.org/abs/2405.16684v1","category":"cs.CL"}
{"created":"2024-05-26 19:21:26","title":"On the regularity of axially-symmetric solutions to the incompressible Navier-Stokes equations in a cylinder","abstract":"We consider the axisymmetric Navier-Stokes equations in a finite cylinder $\\Omega\\subset\\mathbb{R}^3$. We assume that $v_r$, $v_\\varphi$, $\\omega_\\varphi$ vanish on the lateral boundary $\\partial \\Omega$ of the cylinder, and that $v_z$, $\\omega_\\varphi$, $\\partial_z v_\\varphi$ vanish on the top and bottom parts of the boundary $\\partial \\Omega$, where we used standard cylindrical coordinates, and we denoted by $\\omega =\\mathrm{curl}\\, v$ the vorticity field. We use estimates and $H^3$ Sobolev estimate on the modified stream function to derive three order-reduction estimates. These enable one to reduce the order of the nonlinear estimates of the equations, and help observe that the solutions to the equations is ``almost regular''. We use the order-reduction estimates to show that the solution to the equations remains regular as long as, for any $p\\in (6,\\infty)$, $\\| v_\\varphi \\|_{L^\\infty_t L^p_x}/\\| v_\\varphi \\|_{L^\\infty_t L^\\infty_x}$ remains bounded below by a positive number.","sentences":["We consider the axisymmetric Navier-Stokes equations in a finite cylinder $\\Omega\\subset\\mathbb{R}^3$. We assume that $v_r$, $v_\\varphi$, $\\omega_\\varphi$ vanish on the lateral boundary $\\partial \\Omega$ of the cylinder, and that $v_z$, $\\omega_\\varphi$, $\\partial_z v_\\varphi$ vanish on the top and bottom parts of the boundary $\\partial \\Omega$, where we used standard cylindrical coordinates, and we denoted by $\\omega =\\mathrm{curl}\\, v$ the vorticity field.","We use estimates and $H^3$ Sobolev estimate on the modified stream function to derive three order-reduction estimates.","These enable one to reduce the order of the nonlinear estimates of the equations, and help observe that the solutions to the equations is ``almost regular''.","We use the order-reduction estimates to show that the solution to the equations remains regular as long as, for any $p\\in (6,\\infty)$, $\\| v_\\varphi \\|_{L^\\infty_t L^p_x}/\\| v_\\varphi \\|_{L^\\infty_t L^\\infty_x}$ remains bounded below by a positive number."],"url":"http://arxiv.org/abs/2405.16670v1","category":"math.AP"}
{"created":"2024-05-26 19:13:51","title":"Comments on Friedman's Method for Class Distribution Estimation","abstract":"The purpose of class distribution estimation (also known as quantification) is to determine the values of the prior class probabilities in a test dataset without class label observations. A variety of methods to achieve this have been proposed in the literature, most of them based on the assumption that the distributions of the training and test data are related through prior probability shift (also known as label shift). Among these methods, Friedman's method has recently been found to perform relatively well both for binary and multi-class quantification. We discuss the properties of Friedman's method and another approach mentioned by Friedman (called DeBias method in the literature) in the context of a general framework for designing linear equation systems for class distribution estimation.","sentences":["The purpose of class distribution estimation (also known as quantification) is to determine the values of the prior class probabilities in a test dataset without class label observations.","A variety of methods to achieve this have been proposed in the literature, most of them based on the assumption that the distributions of the training and test data are related through prior probability shift (also known as label shift).","Among these methods, Friedman's method has recently been found to perform relatively well both for binary and multi-class quantification.","We discuss the properties of Friedman's method and another approach mentioned by Friedman (called DeBias method in the literature) in the context of a general framework for designing linear equation systems for class distribution estimation."],"url":"http://arxiv.org/abs/2405.16666v1","category":"cs.LG"}
{"created":"2024-05-26 18:59:44","title":"Private Edge Density Estimation for Random Graphs: Optimal, Efficient and Robust","abstract":"We give the first polynomial-time, differentially node-private, and robust algorithm for estimating the edge density of Erd\\H{o}s-R\\'enyi random graphs and their generalization, inhomogeneous random graphs. We further prove information-theoretical lower bounds, showing that the error rate of our algorithm is optimal up to logarithmic factors. Previous algorithms incur either exponential running time or suboptimal error rates.   Two key ingredients of our algorithm are (1) a new sum-of-squares algorithm for robust edge density estimation, and (2) the reduction from privacy to robustness based on sum-of-squares exponential mechanisms due to Hopkins et al. (STOC 2023).","sentences":["We give the first polynomial-time, differentially node-private, and robust algorithm for estimating the edge density of Erd\\H{o}s-R\\'enyi random graphs and their generalization, inhomogeneous random graphs.","We further prove information-theoretical lower bounds, showing that the error rate of our algorithm is optimal up to logarithmic factors.","Previous algorithms incur either exponential running time or suboptimal error rates.   ","Two key ingredients of our algorithm are (1) a new sum-of-squares algorithm for robust edge density estimation, and (2) the reduction from privacy to robustness based on sum-of-squares exponential mechanisms due to Hopkins et al.","(STOC 2023)."],"url":"http://arxiv.org/abs/2405.16663v1","category":"cs.DS"}
{"created":"2024-05-26 18:06:43","title":"Variational Quantum Framework for Partial Differential Equation Constrained Optimization","abstract":"We present a novel variational quantum framework for partial differential equation (PDE) constrained design optimization problems. Such problems arise in simulation based design in many scientific and engineering domains. For instance in aerodynamic design, the PDE constraints are the conservation laws such as momentum, mass and energy balance, the design variables are vehicle shape parameters and material properties, and the objective could be to minimize the effect of transient heat loads on the vehicle or to maximize the lift. The proposed framework utilizes the variational quantum linear system (VQLS) algorithm and a black box optimizer as its two main building blocks. VQLS is used to solve the linear system, arising from the discretization of the PDE constraints for given design parameters, and evaluate the design cost/objective function. The black box optimizer is used to select next set of parameter values based on this evaluated cost, leading to nested bi-level optimization structure within a hybrid classical-quantum setting. We present detailed complexity analysis to highlight the potential advantages of our proposed framework over classical techniques. We implement our framework using the PennyLane library, apply it to solve a prototypical heat transfer optimization problem, and present simulation results using Bayesian optimization as the black box","sentences":["We present a novel variational quantum framework for partial differential equation (PDE) constrained design optimization problems.","Such problems arise in simulation based design in many scientific and engineering domains.","For instance in aerodynamic design, the PDE constraints are the conservation laws such as momentum, mass and energy balance, the design variables are vehicle shape parameters and material properties, and the objective could be to minimize the effect of transient heat loads on the vehicle or to maximize the lift.","The proposed framework utilizes the variational quantum linear system (VQLS) algorithm and a black box optimizer as its two main building blocks.","VQLS is used to solve the linear system, arising from the discretization of the PDE constraints for given design parameters, and evaluate the design cost/objective function.","The black box optimizer is used to select next set of parameter values based on this evaluated cost, leading to nested bi-level optimization structure within a hybrid classical-quantum setting.","We present detailed complexity analysis to highlight the potential advantages of our proposed framework over classical techniques.","We implement our framework using the PennyLane library, apply it to solve a prototypical heat transfer optimization problem, and present simulation results using Bayesian optimization as the black box"],"url":"http://arxiv.org/abs/2405.16651v1","category":"quant-ph"}
{"created":"2024-05-26 17:42:52","title":"The Hamilton-Jacobi-Bellman Equation in Economic Dynamics with a Non-Smooth Fiscal Policy","abstract":"We consider a class of economic growth models that includes the classical Ramsey--Cass--Koopmans capital accumulation model and verify that, under several assumptions, the value function of the model is the unique viscosity solution to the Hamilton--Jacobi--Bellman equation. Moreover, we discuss a solution method for these models using differential inclusion, where the subdifferential of the value function plays an important role. Next, we present an assumption under which the value function is a classical solution to the Hamilton--Jacobi--Bellman equation, and show that many economic models satisfy this assumption. In particular, our result still holds in an economic growth model in which the government takes a non-smooth Keynesian policy rule.","sentences":["We consider a class of economic growth models that includes the classical Ramsey--Cass--Koopmans capital accumulation model and verify that, under several assumptions, the value function of the model is the unique viscosity solution to the Hamilton--Jacobi--Bellman equation.","Moreover, we discuss a solution method for these models using differential inclusion, where the subdifferential of the value function plays an important role.","Next, we present an assumption under which the value function is a classical solution to the Hamilton--Jacobi--Bellman equation, and show that many economic models satisfy this assumption.","In particular, our result still holds in an economic growth model in which the government takes a non-smooth Keynesian policy rule."],"url":"http://arxiv.org/abs/2405.16643v1","category":"econ.TH"}
{"created":"2024-05-26 17:24:09","title":"A probabilistic approach to continuous differentiability of optimal stopping boundaries","abstract":"We obtain the first probabilistic proof of continuous differentiability of time-dependent optimal boundaries in optimal stopping problems. The underlying stochastic dynamics is a one-dimensional, time-inhomogeneous diffusion. The gain function is also time-inhomogeneous and not necessarily smooth. Moreover, we include state-dependent discount rate and the time-horizon can be either finite or infinite. Our arguments of proof are of a local nature that allows us to obtain the result under more general conditions than those used in the PDE literature. As a byproduct of our main result we also obtain the first probabilistic proof of the link between the value function of an optimal stopping problem and the solution of the Stefan's problem.","sentences":["We obtain the first probabilistic proof of continuous differentiability of time-dependent optimal boundaries in optimal stopping problems.","The underlying stochastic dynamics is a one-dimensional, time-inhomogeneous diffusion.","The gain function is also time-inhomogeneous and not necessarily smooth.","Moreover, we include state-dependent discount rate and the time-horizon can be either finite or infinite.","Our arguments of proof are of a local nature that allows us to obtain the result under more general conditions than those used in the PDE literature.","As a byproduct of our main result we also obtain the first probabilistic proof of the link between the value function of an optimal stopping problem and the solution of the Stefan's problem."],"url":"http://arxiv.org/abs/2405.16636v1","category":"math.PR"}
{"created":"2024-05-26 16:43:11","title":"Magnetized compressible turbulence with a fluctuation dynamo and Reynolds numbers over a million","abstract":"Supersonic magnetohydrodynamic (MHD) turbulence is a ubiquitous state for many astrophysical plasmas. However, even the basic statistics for this type of turbulence remains uncertain. We present results from supersonic MHD turbulence simulations at unparalleled resolutions, with plasma Reynolds numbers of over a million. In the kinetic energy spectrum we find a break between the scales that are dominated by kinetic energy, with spectral index $-2$, and those that become strongly magnetized, with spectral index $-3/2$. By analyzing the Helmholtz decomposed kinetic energy spectrum, we find that the compressible modes are not passively mixed through the cascade of the incompressible modes. At high magnetic Reynolds number, above $10^5$, we find a power law in the magnetic energy spectrum with spectral index $-9/5$. On the strongly magnetized, subsonic scales the plasma tends to self-organize into locally relaxed regions, where there is strong alignment between the current density, magnetic field, velocity field and vorticity field, depleting both the nonlinearities and magnetic terms in the MHD equations, which we attribute to plasma relaxation on scales where the magnetic fluctuations evolve on shorter timescales than the velocity fluctuations. This process constrains the cascade to inhomogenous, volume-poor, fractal surfaces between relaxed regions, which has significant repercussions for understanding the nature of magnetized turbulence in astrophysical plasmas and the saturation of the fluctuation dynamo.","sentences":["Supersonic magnetohydrodynamic (MHD) turbulence is a ubiquitous state for many astrophysical plasmas.","However, even the basic statistics for this type of turbulence remains uncertain.","We present results from supersonic MHD turbulence simulations at unparalleled resolutions, with plasma Reynolds numbers of over a million.","In the kinetic energy spectrum we find a break between the scales that are dominated by kinetic energy, with spectral index $-2$, and those that become strongly magnetized, with spectral index $-3/2$.","By analyzing the Helmholtz decomposed kinetic energy spectrum, we find that the compressible modes are not passively mixed through the cascade of the incompressible modes.","At high magnetic Reynolds number, above $10^5$, we find a power law in the magnetic energy spectrum with spectral index $-9/5$.","On the strongly magnetized, subsonic scales the plasma tends to self-organize into locally relaxed regions, where there is strong alignment between the current density, magnetic field, velocity field and vorticity field, depleting both the nonlinearities and magnetic terms in the MHD equations, which we attribute to plasma relaxation on scales where the magnetic fluctuations evolve on shorter timescales than the velocity fluctuations.","This process constrains the cascade to inhomogenous, volume-poor, fractal surfaces between relaxed regions, which has significant repercussions for understanding the nature of magnetized turbulence in astrophysical plasmas and the saturation of the fluctuation dynamo."],"url":"http://arxiv.org/abs/2405.16626v1","category":"astro-ph.GA"}
{"created":"2024-05-26 16:25:55","title":"An efficient optimization model and tabu search-based global optimization approach for continuous p-dispersion problem","abstract":"Continuous p-dispersion problems with and without boundary constraints are NP-hard optimization problems with numerous real-world applications, notably in facility location and circle packing, which are widely studied in mathematics and operations research. In this work, we concentrate on general cases with a non-convex multiply-connected region that are rarely studied in the literature due to their intractability and the absence of an efficient optimization model. Using the penalty function approach, we design a unified and almost everywhere differentiable optimization model for these complex problems and propose a tabu search-based global optimization (TSGO) algorithm for solving them. Computational results over a variety of benchmark instances show that the proposed model works very well, allowing popular local optimization methods (e.g., the quasi-Newton methods and the conjugate gradient methods) to reach high-precision solutions due to the differentiability of the model. These results further demonstrate that the proposed TSGO algorithm is very efficient and significantly outperforms several popular global optimization algorithms in the literature, improving the best-known solutions for several existing instances in a short computational time. Experimental analyses are conducted to show the influence of several key ingredients of the algorithm on computational performance.","sentences":["Continuous p-dispersion problems with and without boundary constraints are NP-hard optimization problems with numerous real-world applications, notably in facility location and circle packing, which are widely studied in mathematics and operations research.","In this work, we concentrate on general cases with a non-convex multiply-connected region that are rarely studied in the literature due to their intractability and the absence of an efficient optimization model.","Using the penalty function approach, we design a unified and almost everywhere differentiable optimization model for these complex problems and propose a tabu search-based global optimization (TSGO) algorithm for solving them.","Computational results over a variety of benchmark instances show that the proposed model works very well, allowing popular local optimization methods (e.g., the quasi-Newton methods and the conjugate gradient methods) to reach high-precision solutions due to the differentiability of the model.","These results further demonstrate that the proposed TSGO algorithm is very efficient and significantly outperforms several popular global optimization algorithms in the literature, improving the best-known solutions for several existing instances in a short computational time.","Experimental analyses are conducted to show the influence of several key ingredients of the algorithm on computational performance."],"url":"http://arxiv.org/abs/2405.16618v1","category":"math.OC"}
{"created":"2024-05-26 16:08:55","title":"DPHGNN: A Dual Perspective Hypergraph Neural Networks","abstract":"Message passing on hypergraphs has been a standard framework for learning higher-order correlations between hypernodes. Recently-proposed hypergraph neural networks (HGNNs) can be categorized into spatial and spectral methods based on their design choices. In this work, we analyze the impact of change in hypergraph topology on the suboptimal performance of HGNNs and propose DPHGNN, a novel dual-perspective HGNN that introduces equivariant operator learning to capture lower-order semantics by inducing topology-aware spatial and spectral inductive biases. DPHGNN employs a unified framework to dynamically fuse lower-order explicit feature representations from the underlying graph into the super-imposed hypergraph structure. We benchmark DPHGNN over eight benchmark hypergraph datasets for the semi-supervised hypernode classification task and obtain superior performance compared to seven state-of-the-art baselines. We also provide a theoretical framework and a synthetic hypergraph isomorphism test to express the power of spatial HGNNs and quantify the expressivity of DPHGNN beyond the Generalized Weisfeiler Leman (1-GWL) test. Finally, DPHGNN was deployed by our partner e-commerce company for the Return-to-Origin (RTO) prediction task, which shows ~7% higher macro F1-Score than the best baseline.","sentences":["Message passing on hypergraphs has been a standard framework for learning higher-order correlations between hypernodes.","Recently-proposed hypergraph neural networks (HGNNs) can be categorized into spatial and spectral methods based on their design choices.","In this work, we analyze the impact of change in hypergraph topology on the suboptimal performance of HGNNs and propose DPHGNN, a novel dual-perspective HGNN that introduces equivariant operator learning to capture lower-order semantics by inducing topology-aware spatial and spectral inductive biases.","DPHGNN employs a unified framework to dynamically fuse lower-order explicit feature representations from the underlying graph into the super-imposed hypergraph structure.","We benchmark DPHGNN over eight benchmark hypergraph datasets for the semi-supervised hypernode classification task and obtain superior performance compared to seven state-of-the-art baselines.","We also provide a theoretical framework and a synthetic hypergraph isomorphism test to express the power of spatial HGNNs and quantify the expressivity of DPHGNN beyond the Generalized Weisfeiler Leman (1-GWL) test.","Finally, DPHGNN was deployed by our partner e-commerce company for the Return-to-Origin (RTO) prediction task, which shows ~7% higher macro F1-Score than the best baseline."],"url":"http://arxiv.org/abs/2405.16616v1","category":"cs.LG"}
{"created":"2024-05-26 16:08:47","title":"Rough geometric integration","abstract":"We introduce a notion of distributional $k$-forms on $d$-dimensional manifolds which can be integrated against suitably regular $k$-submanifolds. Our approach combines ideas from Whitney's geometric integration [Whi57] with those of sewing approaches to rough integration [Gub04, FdLP06].","sentences":["We introduce a notion of distributional $k$-forms on $d$-dimensional manifolds which can be integrated against suitably regular $k$-submanifolds.","Our approach combines ideas from Whitney's geometric integration","[Whi57] with those of sewing approaches to rough integration","[Gub04, FdLP06]."],"url":"http://arxiv.org/abs/2405.16615v1","category":"math.DG"}
{"created":"2024-05-26 15:37:19","title":"Efficient Probabilistic Modeling of Crystallization at Mesoscopic Scale","abstract":"Crystallization processes at the mesoscopic scale, where faceted, dendritic growth, and multigrain formation can be observed, are of particular interest within materials science and metallurgy. These processes are highly nonlinear, stochastic, and sensitive to small perturbations of system parameters and initial conditions. Methods for the simulation of these processes have been developed using discrete numerical models, but these are computationally expensive. This work aims to scale crystal growth simulation with a machine learning emulator. Specifically, autoregressive latent variable models are well suited for modeling the joint distribution over system parameters and the crystallization trajectories. However, successfully training such models is challenging due to the stochasticity and sensitivity of the system. Existing approaches consequently fail to produce diverse and faithful crystallization trajectories. In this paper, we introduce the Crystal Growth Neural Emulator (CGNE), a probabilistic model for efficient crystal growth emulation at the mesoscopic scale that overcomes these challenges. We validate CGNE results using the morphological properties of the crystals produced by numerical simulation. CGNE delivers a factor of 11 improvement in inference time and performance gains compared with recent state-of-the-art probabilistic models for dynamical systems.","sentences":["Crystallization processes at the mesoscopic scale, where faceted, dendritic growth, and multigrain formation can be observed, are of particular interest within materials science and metallurgy.","These processes are highly nonlinear, stochastic, and sensitive to small perturbations of system parameters and initial conditions.","Methods for the simulation of these processes have been developed using discrete numerical models, but these are computationally expensive.","This work aims to scale crystal growth simulation with a machine learning emulator.","Specifically, autoregressive latent variable models are well suited for modeling the joint distribution over system parameters and the crystallization trajectories.","However, successfully training such models is challenging due to the stochasticity and sensitivity of the system.","Existing approaches consequently fail to produce diverse and faithful crystallization trajectories.","In this paper, we introduce the Crystal Growth Neural Emulator (CGNE), a probabilistic model for efficient crystal growth emulation at the mesoscopic scale that overcomes these challenges.","We validate CGNE results using the morphological properties of the crystals produced by numerical simulation.","CGNE delivers a factor of 11 improvement in inference time and performance gains compared with recent state-of-the-art probabilistic models for dynamical systems."],"url":"http://arxiv.org/abs/2405.16608v1","category":"cs.LG"}
{"created":"2024-05-26 15:31:28","title":"Link Prediction on Textual Edge Graphs","abstract":"Textual-edge Graphs (TEGs), characterized by rich text annotations on edges, are increasingly significant in network science due to their ability to capture rich contextual information among entities. Existing works have proposed various edge-aware graph neural networks (GNNs) or let language models directly make predictions. However, they often fall short of fully capturing the contextualized semantics on edges and graph topology, respectively. This inadequacy is particularly evident in link prediction tasks that require a comprehensive understanding of graph topology and semantics between nodes. In this paper, we present a novel framework - Link2Doc, designed especially for link prediction on textual-edge graphs. Specifically, we propose to summarize neighborhood information between node pairs as a human-written document to preserve both semantic and topology information. A self-supervised learning model is then utilized to enhance GNN's text-understanding ability from language models. Empirical evaluations, including link prediction, edge classification, parameter analysis, runtime comparison, and ablation studies, on four real-world datasets demonstrate that Link2Doc achieves generally better performance against existing edge-aware GNNs and pre-trained language models in predicting links on TEGs.","sentences":["Textual-edge Graphs (TEGs), characterized by rich text annotations on edges, are increasingly significant in network science due to their ability to capture rich contextual information among entities.","Existing works have proposed various edge-aware graph neural networks (GNNs) or let language models directly make predictions.","However, they often fall short of fully capturing the contextualized semantics on edges and graph topology, respectively.","This inadequacy is particularly evident in link prediction tasks that require a comprehensive understanding of graph topology and semantics between nodes.","In this paper, we present a novel framework - Link2Doc, designed especially for link prediction on textual-edge graphs.","Specifically, we propose to summarize neighborhood information between node pairs as a human-written document to preserve both semantic and topology information.","A self-supervised learning model is then utilized to enhance GNN's text-understanding ability from language models.","Empirical evaluations, including link prediction, edge classification, parameter analysis, runtime comparison, and ablation studies, on four real-world datasets demonstrate that Link2Doc achieves generally better performance against existing edge-aware GNNs and pre-trained language models in predicting links on TEGs."],"url":"http://arxiv.org/abs/2405.16606v1","category":"cs.SI"}
{"created":"2024-05-26 14:44:38","title":"Anti-plane segregation and diffusion in dense, bidisperse granular shear flow","abstract":"Many dense granular systems are non-monodisperse, consisting of particles of different sizes, and will segregate based on size during flow. This phenomenon is an important aspect of many industrial and geophysical processes, necessitating predictive continuum models. This paper systematically studies a key aspect of the three-dimensional nature of segregation and diffusion in flowing, dense, bidisperse granular mixtures -- namely, segregation and diffusion acting along the direction perpendicular to the plane of shearing, which we refer to as the anti-plane modes of segregation and diffusion. To this end, we consider discrete-element method (DEM) simulations of flows of dense, bidisperse mixtures of frictional spheres in an idealized configuration that isolates anti-plane segregation and diffusion. We find that previously-developed constitutive equations, calibrated to DEM simulation results from flows in which both the segregation and diffusion processes occur within the plane of shearing, do not capture aspects of the anti-plane segregation dynamics. Accordingly, we utilize DEM simulation results to inform and calibrate constitutive equations for the segregation and diffusion fluxes in their anti-plane modes. Predictions of the resulting continuum model for the anti-plane segregation dynamics are tested against additional DEM simulation results across different cases, while parameters such as the shear strain rate and mixture composition are varied, and we find that the calibrated model predictions match well with the DEM simulation results. Finally, we suggest a strategy for generalizing the constitutive forms for the segregation and diffusion fluxes to obtain three-dimensional constitutive equations that account for both the in-plane and anti-plane modes of the segregation and diffusion processes.","sentences":["Many dense granular systems are non-monodisperse, consisting of particles of different sizes, and will segregate based on size during flow.","This phenomenon is an important aspect of many industrial and geophysical processes, necessitating predictive continuum models.","This paper systematically studies a key aspect of the three-dimensional nature of segregation and diffusion in flowing, dense, bidisperse granular mixtures -- namely, segregation and diffusion acting along the direction perpendicular to the plane of shearing, which we refer to as the anti-plane modes of segregation and diffusion.","To this end, we consider discrete-element method (DEM) simulations of flows of dense, bidisperse mixtures of frictional spheres in an idealized configuration that isolates anti-plane segregation and diffusion.","We find that previously-developed constitutive equations, calibrated to DEM simulation results from flows in which both the segregation and diffusion processes occur within the plane of shearing, do not capture aspects of the anti-plane segregation dynamics.","Accordingly, we utilize DEM simulation results to inform and calibrate constitutive equations for the segregation and diffusion fluxes in their anti-plane modes.","Predictions of the resulting continuum model for the anti-plane segregation dynamics are tested against additional DEM simulation results across different cases, while parameters such as the shear strain rate and mixture composition are varied, and we find that the calibrated model predictions match well with the DEM simulation results.","Finally, we suggest a strategy for generalizing the constitutive forms for the segregation and diffusion fluxes to obtain three-dimensional constitutive equations that account for both the in-plane and anti-plane modes of the segregation and diffusion processes."],"url":"http://arxiv.org/abs/2405.16589v1","category":"cond-mat.soft"}
{"created":"2024-05-26 14:26:52","title":"A comparison of the Coco-Russo scheme and $\\protect\\mathghost$-FEM for elliptic equations in arbitrary domains","abstract":"In this paper, a comparative study between the Coco-Russo scheme (based on finite-difference scheme) and the $\\mathghost$-FEM (based on finite-element method) is presented when solving the Poisson equation in arbitrary domains. The comparison between the two numerical methods is carried out by presenting analytical results from the literature \\cite{cocoStissi,astuto2024nodal}, together with numerical tests in various geometries and boundary conditions.","sentences":["In this paper, a comparative study between the Coco-Russo scheme (based on finite-difference scheme) and the $\\mathghost$-FEM (based on finite-element method) is presented when solving the Poisson equation in arbitrary domains.","The comparison between the two numerical methods is carried out by presenting analytical results from the literature \\cite{cocoStissi,astuto2024nodal}, together with numerical tests in various geometries and boundary conditions."],"url":"http://arxiv.org/abs/2405.16582v1","category":"math.NA"}
{"created":"2024-05-26 14:09:43","title":"Reflected Flow Matching","abstract":"Continuous normalizing flows (CNFs) learn an ordinary differential equation to transform prior samples into data. Flow matching (FM) has recently emerged as a simulation-free approach for training CNFs by regressing a velocity model towards the conditional velocity field. However, on constrained domains, the learned velocity model may lead to undesirable flows that result in highly unnatural samples, e.g., oversaturated images, due to both flow matching error and simulation error. To address this, we add a boundary constraint term to CNFs, which leads to reflected CNFs that keep trajectories within the constrained domains. We propose reflected flow matching (RFM) to train the velocity model in reflected CNFs by matching the conditional velocity fields in a simulation-free manner, similar to the vanilla FM. Moreover, the analytical form of conditional velocity fields in RFM avoids potentially biased approximations, making it superior to existing score-based generative models on constrained domains. We demonstrate that RFM achieves comparable or better results on standard image benchmarks and produces high-quality class-conditioned samples under high guidance weight.","sentences":["Continuous normalizing flows (CNFs) learn an ordinary differential equation to transform prior samples into data.","Flow matching (FM) has recently emerged as a simulation-free approach for training CNFs by regressing a velocity model towards the conditional velocity field.","However, on constrained domains, the learned velocity model may lead to undesirable flows that result in highly unnatural samples, e.g., oversaturated images, due to both flow matching error and simulation error.","To address this, we add a boundary constraint term to CNFs, which leads to reflected CNFs that keep trajectories within the constrained domains.","We propose reflected flow matching (RFM) to train the velocity model in reflected CNFs by matching the conditional velocity fields in a simulation-free manner, similar to the vanilla FM.","Moreover, the analytical form of conditional velocity fields in RFM avoids potentially biased approximations, making it superior to existing score-based generative models on constrained domains.","We demonstrate that RFM achieves comparable or better results on standard image benchmarks and produces high-quality class-conditioned samples under high guidance weight."],"url":"http://arxiv.org/abs/2405.16577v1","category":"stat.ML"}
{"created":"2024-05-26 13:40:54","title":"Exact solutions of some singular integro-differential equations related to adhesive contact problems of elasticity theory","abstract":"The problem of constructing an exact solution of singular integro-differential equations related to problems of adhesive interaction between elastic thin semi-infinite homogeneous patch and elastic plate is investigated. For the patch loaded with horizontal forces the usual model of the uniaxial stress state is valid. Using the methods of the theory of analytic functions and integral transformation the singular integro-differential equation is reduced to the Riemann boundary value problem of the theory of analytic functions. The exact solution of this problem and asymptotic estimates of tangential contact stresses are obtained.","sentences":["The problem of constructing an exact solution of singular integro-differential equations related to problems of adhesive interaction between elastic thin semi-infinite homogeneous patch and elastic plate is investigated.","For the patch loaded with horizontal forces the usual model of the uniaxial stress state is valid.","Using the methods of the theory of analytic functions and integral transformation the singular integro-differential equation is reduced to the Riemann boundary value problem of the theory of analytic functions.","The exact solution of this problem and asymptotic estimates of tangential contact stresses are obtained."],"url":"http://arxiv.org/abs/2405.16572v1","category":"math-ph"}
{"created":"2024-05-26 13:35:30","title":"On deformation quantization of the space of connections on a two manifold and Chern Simons Gauge Theory","abstract":"We use recent progress on Chern-Simons gauge theory in three dimensions [18] to give explicit, closed form formulas for the star product on some functions on the affine space ${\\mathcal A}(\\Sigma)$ of (smooth) connections on the trivialized principal $G$-bundle on a compact, oriented two manifold $\\Sigma.$ These formulas give a close relation between knot invariants, such as the Kauffman bracket polynomial, and the Jones and HOMFLY polynomials, arising in Chern Simons gauge theory, and deformation quantization of ${\\mathcal A}(\\Sigma).$ This relation echoes the relation between the manifold invariants of Witten [20] and Reshetikhin-Turaev [16] and {\\em geometric} quantization of this space (or its symplectic quotient by the action of the gauge group). In our case this relation arises from explicit algebraic formulas arising from the (mathematically well-defined) functional integrals of [18].","sentences":["We use recent progress on Chern-Simons gauge theory in three dimensions [18] to give explicit, closed form formulas for the star product on some functions on the affine space ${\\mathcal A}(\\Sigma)$ of (smooth) connections on the trivialized principal $G$-bundle on a compact, oriented two manifold $\\Sigma.$ These formulas give a close relation between knot invariants, such as the Kauffman bracket polynomial, and the Jones and HOMFLY polynomials, arising in Chern Simons gauge theory, and deformation quantization of ${\\mathcal A}(\\Sigma).$","This relation echoes the relation between the manifold invariants of Witten [20] and Reshetikhin-Turaev","[16] and {\\em geometric} quantization of this space (or its symplectic quotient by the action of the gauge group).","In our case this relation arises from explicit algebraic formulas arising from the (mathematically well-defined) functional integrals of [18]."],"url":"http://arxiv.org/abs/2405.16569v1","category":"math.DG"}
{"created":"2024-05-26 13:29:51","title":"Invertibility in nonassociative ordered rings and in weak-quasi-topological nonassociative rings","abstract":"Invertibility is important in ring theory because it enables division and facilitates solving equations. Moreover, rings can be endowed with extra ''structure'' such as order and topology that add new properties. The two main theorems of this article are contributions to invertibility in the context of ordered and weak-quasi-topological rings. Specifically, the first theorem asserts that the interval $]0,1]$ in any suitable partially ordered ring consists entirely of invertible elements. The second theorem asserts that if $f$ is a norm from a ring to a partially ordered ring endowed with interval topology, then under certain conditions, the subset of elements such that $f(1-a) < 1$ consists entirely of invertible elements. The second theorem relies on the assumption of sequential Cauchy completeness of the topology induced by the norm $f$, which as we recall, takes values in an ordered ring endowed with the interval topology (an example of a coarse topology). The fact that a ring endowed with the topology associated with a seminorm into an ordered ring endowed with the interval topology is a locally convex quasi-topological group with an additional continuity property of the product is dealt with in a separate section. A brief application to frame theory is also included.","sentences":["Invertibility is important in ring theory because it enables division and facilitates solving equations.","Moreover, rings can be endowed with extra ''structure'' such as order and topology that add new properties.","The two main theorems of this article are contributions to invertibility in the context of ordered and weak-quasi-topological rings.","Specifically, the first theorem asserts that the interval $]0,1]$ in any suitable partially ordered ring consists entirely of invertible elements.","The second theorem asserts that if $f$ is a norm from a ring to a partially ordered ring endowed with interval topology, then under certain conditions, the subset of elements such that $f(1-a) < 1$ consists entirely of invertible elements.","The second theorem relies on the assumption of sequential Cauchy completeness of the topology induced by the norm $f$, which as we recall, takes values in an ordered ring endowed with the interval topology (an example of a coarse topology).","The fact that a ring endowed with the topology associated with a seminorm into an ordered ring endowed with the interval topology is a locally convex quasi-topological group with an additional continuity property of the product is dealt with in a separate section.","A brief application to frame theory is also included."],"url":"http://arxiv.org/abs/2405.16565v1","category":"math.AC"}
{"created":"2024-05-26 13:19:32","title":"Reality Only Happens Once: Single-Path Generalization Bounds for Transformers","abstract":"One of the inherent challenges in deploying transformers on time series is that \\emph{reality only happens once}; namely, one typically only has access to a single trajectory of the data-generating process comprised of non-i.i.d. observations. We derive non-asymptotic statistical guarantees in this setting through bounds on the \\textit{generalization} of a transformer network at a future-time $t$, given that it has been trained using $N\\le t$ observations from a single perturbed trajectory of a Markov process. Under the assumption that the Markov process satisfies a log-Sobolev inequality, we obtain a generalization bound which effectively converges at the rate of ${O}(1/\\sqrt{N})$. Our bound depends explicitly on the activation function ($\\operatorname{Swish}$, $\\operatorname{GeLU}$, or $\\tanh$ are considered), the number of self-attention heads, depth, width, and norm-bounds defining the transformer architecture. Our bound consists of three components: (I) The first quantifies the gap between the stationary distribution of the data-generating Markov process and its distribution at time $t$, this term converges exponentially to $0$. (II) The next term encodes the complexity of the transformer model and, given enough time, eventually converges to $0$ at the rate ${O}(\\log(N)^r/\\sqrt{N})$ for any $r>0$. (III) The third term guarantees that the bound holds with probability at least $1$-$\\delta$, and converges at a rate of ${O}(\\sqrt{\\log(1/\\delta)}/\\sqrt{N})$.","sentences":["One of the inherent challenges in deploying transformers on time series is that \\emph{reality only happens once}; namely, one typically only has access to a single trajectory of the data-generating process comprised of non-i.i.d. observations.","We derive non-asymptotic statistical guarantees in this setting through bounds on the \\textit{generalization} of a transformer network at a future-time $t$, given that it has been trained using $N\\le t$ observations from a single perturbed trajectory of a Markov process.","Under the assumption that the Markov process satisfies a log-Sobolev inequality, we obtain a generalization bound which effectively converges at the rate of ${O}(1/\\sqrt{N})$. Our bound depends explicitly on the activation function ($\\operatorname{Swish}$, $\\operatorname{GeLU}$, or $\\tanh$ are considered), the number of self-attention heads, depth, width, and norm-bounds defining the transformer architecture.","Our bound consists of three components: (I)","The first quantifies the gap between the stationary distribution of the data-generating Markov process and its distribution at time $t$, this term converges exponentially to $0$. (II) The next term encodes the complexity of the transformer model and, given enough time, eventually converges to $0$ at the rate ${O}(\\log(N)^r/\\sqrt{N})$ for any $r>0$. (III)","The third term guarantees that the bound holds with probability at least $1$-$\\delta$, and converges at a rate of ${O}(\\sqrt{\\log(1/\\delta)}/\\sqrt{N})$."],"url":"http://arxiv.org/abs/2405.16563v1","category":"cs.LG"}
{"created":"2024-05-26 13:16:15","title":"Global existence and nonexistence analyses for a magnetic fractional pseudo-parabolic equation","abstract":"In this paper, we study the initial-boundary value problem for a pseudo-parabolic equation in magnetic fractional Orlicz-Sobolev spaces. First, by employing the imbedding theorems, the theory of potential wells and the Galerkin method, we prove the existence and uniqueness of global solutions with subcritical initial energy, critical initial energy and supercritical initial energy, respectively. Furthermore, we prove the decay estimate of global solutions with sub-sharp-critical initial energy, sharp-critical initial energy and supercritical initial energy, respectively. Specifically, we need to analyze the properties of $\\omega$-limits of solutions for supercritical initial energy. Next, we establish the finite time blowup of solutions with sub-sharp-critical initial energy and sharp-critical initial energy, respectively. Finally, we discuss the convergence relationship between the global solutions of the evolution problem and the ground state solutions of the corresponding stationary problem.","sentences":["In this paper, we study the initial-boundary value problem for a pseudo-parabolic equation in magnetic fractional Orlicz-Sobolev spaces.","First, by employing the imbedding theorems, the theory of potential wells and the Galerkin method, we prove the existence and uniqueness of global solutions with subcritical initial energy, critical initial energy and supercritical initial energy, respectively.","Furthermore, we prove the decay estimate of global solutions with sub-sharp-critical initial energy, sharp-critical initial energy and supercritical initial energy, respectively.","Specifically, we need to analyze the properties of $\\omega$-limits of solutions for supercritical initial energy.","Next, we establish the finite time blowup of solutions with sub-sharp-critical initial energy and sharp-critical initial energy, respectively.","Finally, we discuss the convergence relationship between the global solutions of the evolution problem and the ground state solutions of the corresponding stationary problem."],"url":"http://arxiv.org/abs/2405.16562v1","category":"math.AP"}
{"created":"2024-05-26 12:40:39","title":"GPU Based Differential Evolution: New Insights and Comparative Study","abstract":"Differential Evolution (DE) is a highly successful population based global optimisation algorithm, commonly used for solving numerical optimisation problems. However, as the complexity of the objective function increases, the wall-clock run-time of the algorithm suffers as many fitness function evaluations must take place to effectively explore the search space. Due to the inherently parallel nature of the DE algorithm, graphics processing units (GPU) have been used to effectively accelerate both the fitness evaluation and DE algorithm. This work reviews the main architectural choices made in the literature for GPU based DE algorithms and introduces a new GPU based numerical optimisation benchmark to evaluate and compare GPU based DE algorithms.","sentences":["Differential Evolution (DE) is a highly successful population based global optimisation algorithm, commonly used for solving numerical optimisation problems.","However, as the complexity of the objective function increases, the wall-clock run-time of the algorithm suffers as many fitness function evaluations must take place to effectively explore the search space.","Due to the inherently parallel nature of the DE algorithm, graphics processing units (GPU) have been used to effectively accelerate both the fitness evaluation and DE algorithm.","This work reviews the main architectural choices made in the literature for GPU based DE algorithms and introduces a new GPU based numerical optimisation benchmark to evaluate and compare GPU based DE algorithms."],"url":"http://arxiv.org/abs/2405.16551v1","category":"cs.NE"}
{"created":"2024-05-26 12:30:20","title":"Cocktail: A Comprehensive Information Retrieval Benchmark with LLM-Generated Documents Integration","abstract":"The proliferation of Large Language Models (LLMs) has led to an influx of AI-generated content (AIGC) on the internet, transforming the corpus of Information Retrieval (IR) systems from solely human-written to a coexistence with LLM-generated content. The impact of this surge in AIGC on IR systems remains an open question, with the primary challenge being the lack of a dedicated benchmark for researchers. In this paper, we introduce Cocktail, a comprehensive benchmark tailored for evaluating IR models in this mixed-sourced data landscape of the LLM era. Cocktail consists of 16 diverse datasets with mixed human-written and LLM-generated corpora across various text retrieval tasks and domains. Additionally, to avoid the potential bias from previously included dataset information in LLMs, we also introduce an up-to-date dataset, named NQ-UTD, with queries derived from recent events. Through conducting over 1,000 experiments to assess state-of-the-art retrieval models against the benchmarked datasets in Cocktail, we uncover a clear trade-off between ranking performance and source bias in neural retrieval models, highlighting the necessity for a balanced approach in designing future IR systems. We hope Cocktail can serve as a foundational resource for IR research in the LLM era, with all data and code publicly available at \\url{https://github.com/KID-22/Cocktail}.","sentences":["The proliferation of Large Language Models (LLMs) has led to an influx of AI-generated content (AIGC) on the internet, transforming the corpus of Information Retrieval (IR) systems from solely human-written to a coexistence with LLM-generated content.","The impact of this surge in AIGC on IR systems remains an open question, with the primary challenge being the lack of a dedicated benchmark for researchers.","In this paper, we introduce Cocktail, a comprehensive benchmark tailored for evaluating IR models in this mixed-sourced data landscape of the LLM era.","Cocktail consists of 16 diverse datasets with mixed human-written and LLM-generated corpora across various text retrieval tasks and domains.","Additionally, to avoid the potential bias from previously included dataset information in LLMs, we also introduce an up-to-date dataset, named NQ-UTD, with queries derived from recent events.","Through conducting over 1,000 experiments to assess state-of-the-art retrieval models against the benchmarked datasets in Cocktail, we uncover a clear trade-off between ranking performance and source bias in neural retrieval models, highlighting the necessity for a balanced approach in designing future IR systems.","We hope Cocktail can serve as a foundational resource for IR research in the LLM era, with all data and code publicly available at \\url{https://github.com/KID-22/Cocktail}."],"url":"http://arxiv.org/abs/2405.16546v1","category":"cs.IR"}
{"created":"2024-05-26 10:19:04","title":"AnyCBMs: How to Turn Any Black Box into a Concept Bottleneck Model","abstract":"Interpretable deep learning aims at developing neural architectures whose decision-making processes could be understood by their users. Among these techniqes, Concept Bottleneck Models enhance the interpretability of neural networks by integrating a layer of human-understandable concepts. These models, however, necessitate training a new model from the beginning, consuming significant resources and failing to utilize already trained large models. To address this issue, we introduce \"AnyCBM\", a method that transforms any existing trained model into a Concept Bottleneck Model with minimal impact on computational resources. We provide both theoretical and experimental insights showing the effectiveness of AnyCBMs in terms of classification performances and effectivenss of concept-based interventions on downstream tasks.","sentences":["Interpretable deep learning aims at developing neural architectures whose decision-making processes could be understood by their users.","Among these techniqes, Concept Bottleneck Models enhance the interpretability of neural networks by integrating a layer of human-understandable concepts.","These models, however, necessitate training a new model from the beginning, consuming significant resources and failing to utilize already trained large models.","To address this issue, we introduce \"AnyCBM\", a method that transforms any existing trained model into a Concept Bottleneck Model with minimal impact on computational resources.","We provide both theoretical and experimental insights showing the effectiveness of AnyCBMs in terms of classification performances and effectivenss of concept-based interventions on downstream tasks."],"url":"http://arxiv.org/abs/2405.16508v1","category":"cs.LG"}
{"created":"2024-05-26 09:22:15","title":"Boosting probes of CP violation in the top Yukawa coupling with Deep Learning","abstract":"The precise measurement of the top-Higgs coupling is crucial in particle physics, offering insights into potential new physics Beyond the Standard Model (BSM) carrying CP Violation (CPV) effects. In this paper, we explore the CP properties of a Higgs boson coupling with a top quark pair, focusing on events where the Higgs state decays into a pair of $b$-quarks and the top-antitop system decays leptonically. The novelty of our analysis resides in the exploitation of two conditional Deep Learning (DL) networks: a Multi-Layer Perceptron (MLP) and a Graph Convolution Network (GCN). These models are trained for selected CPV phase values and then used to interpolate all possible values ranging from $0 \\text{ to } \\pi/2$. This enables a comprehensive assessment of sensitivity across all CP phase values, thereby streamlining the process as the models are trained only once. Notably, the conditional GCN exhibits superior performance over the conditional MLP, owing to the nature of graph-based Neural Network (NN) structures. Our Machine Learning (ML) informed findings indicate that assessment of the CP properties of the Higgs coupling to the $t\\bar{t}$ pair can be within reach of the HL-LHC, quantitatively surpassing the sensitivity of more traditional approaches.","sentences":["The precise measurement of the top-Higgs coupling is crucial in particle physics, offering insights into potential new physics Beyond the Standard Model (BSM) carrying CP Violation (CPV) effects.","In this paper, we explore the CP properties of a Higgs boson coupling with a top quark pair, focusing on events where the Higgs state decays into a pair of $b$-quarks and the top-antitop system decays leptonically.","The novelty of our analysis resides in the exploitation of two conditional Deep Learning (DL) networks: a Multi-Layer Perceptron (MLP) and a Graph Convolution Network (GCN).","These models are trained for selected CPV phase values and then used to interpolate all possible values ranging from $0 \\text{ to } \\pi/2$. This enables a comprehensive assessment of sensitivity across all CP phase values, thereby streamlining the process as the models are trained only once.","Notably, the conditional GCN exhibits superior performance over the conditional MLP, owing to the nature of graph-based Neural Network (NN) structures.","Our Machine Learning (ML) informed findings indicate that assessment of the CP properties of the Higgs coupling to the $t\\bar{t}$ pair can be within reach of the HL-LHC, quantitatively surpassing the sensitivity of more traditional approaches."],"url":"http://arxiv.org/abs/2405.16499v1","category":"hep-ph"}
{"created":"2024-05-26 09:20:47","title":"On Sequential Loss Approximation for Continual Learning","abstract":"We introduce for continual learning Autodiff Quadratic Consolidation (AQC), which approximates the previous loss function with a quadratic function, and Neural Consolidation (NC), which approximates the previous loss function with a neural network. Although they are not scalable to large neural networks, they can be used with a fixed pre-trained feature extractor. We empirically study these methods in class-incremental learning, for which regularization-based methods produce unsatisfactory results, unless combined with replay. We find that for small datasets, quadratic approximation of the previous loss function leads to poor results, even with full Hessian computation, and NC could significantly improve the predictive performance, while for large datasets, when used with a fixed pre-trained feature extractor, AQC provides superior predictive performance. We also find that using tanh-output features can improve the predictive performance of AQC. In particular, in class-incremental Split MNIST, when a Convolutional Neural Network (CNN) with tanh-output features is pre-trained on EMNIST Letters and used as a fixed pre-trained feature extractor, AQC can achieve predictive performance comparable to joint training.","sentences":["We introduce for continual learning Autodiff Quadratic Consolidation (AQC), which approximates the previous loss function with a quadratic function, and Neural Consolidation (NC), which approximates the previous loss function with a neural network.","Although they are not scalable to large neural networks, they can be used with a fixed pre-trained feature extractor.","We empirically study these methods in class-incremental learning, for which regularization-based methods produce unsatisfactory results, unless combined with replay.","We find that for small datasets, quadratic approximation of the previous loss function leads to poor results, even with full Hessian computation, and NC could significantly improve the predictive performance, while for large datasets, when used with a fixed pre-trained feature extractor, AQC provides superior predictive performance.","We also find that using tanh-output features can improve the predictive performance of AQC.","In particular, in class-incremental Split MNIST, when a Convolutional Neural Network (CNN) with tanh-output features is pre-trained on EMNIST Letters and used as a fixed pre-trained feature extractor, AQC can achieve predictive performance comparable to joint training."],"url":"http://arxiv.org/abs/2405.16498v1","category":"cs.LG"}
{"created":"2024-05-27 17:46:57","title":"The Expressive Capacity of State Space Models: A Formal Language Perspective","abstract":"Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.","sentences":["Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers.","However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures.","We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs.","We find that SSMs and transformers have overlapping but distinct strengths.","In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly.","They can also model bounded hierarchical structure with optimal memory even without simulating a stack.","On the other hand, we identify a design choice in current SSMs that limits their expressive power.","We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba."],"url":"http://arxiv.org/abs/2405.17394v1","category":"cs.CL"}
{"created":"2024-05-27 16:55:48","title":"Assessing the significance of longitudinal data in Alzheimer's Disease forecasting","abstract":"In this study, we employ a transformer encoder model to characterize the significance of longitudinal patient data for forecasting the progression of Alzheimer's Disease (AD). Our model, Longitudinal Forecasting Model for Alzheimer's Disease (LongForMAD), harnesses the comprehensive temporal information embedded in sequences of patient visits that incorporate multimodal data, providing a deeper understanding of disease progression than can be drawn from single-visit data alone. We present an empirical analysis across two patient groups-Cognitively Normal (CN) and Mild Cognitive Impairment (MCI)-over a span of five follow-up years. Our findings reveal that models incorporating more extended patient histories can outperform those relying solely on present information, suggesting a deeper historical context is critical in enhancing predictive accuracy for future AD progression. Our results support the incorporation of longitudinal data in clinical settings to enhance the early detection and monitoring of AD. Our code is available at \\url{https://github.com/batuhankmkaraman/LongForMAD}.","sentences":["In this study, we employ a transformer encoder model to characterize the significance of longitudinal patient data for forecasting the progression of Alzheimer's Disease (AD).","Our model, Longitudinal Forecasting Model for Alzheimer's Disease (LongForMAD), harnesses the comprehensive temporal information embedded in sequences of patient visits that incorporate multimodal data, providing a deeper understanding of disease progression than can be drawn from single-visit data alone.","We present an empirical analysis across two patient groups-Cognitively Normal (CN) and Mild Cognitive Impairment (MCI)-over a span of five follow-up years.","Our findings reveal that models incorporating more extended patient histories can outperform those relying solely on present information, suggesting a deeper historical context is critical in enhancing predictive accuracy for future AD progression.","Our results support the incorporation of longitudinal data in clinical settings to enhance the early detection and monitoring of AD.","Our code is available at \\url{https://github.com/batuhankmkaraman/LongForMAD}."],"url":"http://arxiv.org/abs/2405.17352v1","category":"cs.LG"}
{"created":"2024-05-27 15:15:50","title":"The state learner -- a super learner for right-censored data","abstract":"In survival analysis, prediction models are needed as stand-alone tools and in applications of causal inference to estimate nuisance parameters. The super learner is a machine learning algorithm which combines a library of prediction models into a meta learner based on cross-validated loss. In right-censored data, the choice of the loss function and the estimation of the expected loss need careful consideration. We introduce the state learner, a new super learner for survival analysis, which simultaneously evaluates libraries of prediction models for the event of interest and the censoring distribution. The state learner can be applied to all types of survival models, works in the presence of competing risks, and does not require a single pre-specified estimator of the conditional censoring distribution. We establish an oracle inequality for the state learner and investigate its performance through numerical experiments. We illustrate the application of the state learner with prostate cancer data, as a stand-alone prediction tool, and, for causal inference, as a way to estimate the nuisance parameter models of a smooth statistical functional.","sentences":["In survival analysis, prediction models are needed as stand-alone tools and in applications of causal inference to estimate nuisance parameters.","The super learner is a machine learning algorithm which combines a library of prediction models into a meta learner based on cross-validated loss.","In right-censored data, the choice of the loss function and the estimation of the expected loss need careful consideration.","We introduce the state learner, a new super learner for survival analysis, which simultaneously evaluates libraries of prediction models for the event of interest and the censoring distribution.","The state learner can be applied to all types of survival models, works in the presence of competing risks, and does not require a single pre-specified estimator of the conditional censoring distribution.","We establish an oracle inequality for the state learner and investigate its performance through numerical experiments.","We illustrate the application of the state learner with prostate cancer data, as a stand-alone prediction tool, and, for causal inference, as a way to estimate the nuisance parameter models of a smooth statistical functional."],"url":"http://arxiv.org/abs/2405.17259v1","category":"stat.ME"}
{"created":"2024-05-27 15:03:21","title":"Transformer In-Context Learning for Categorical Data","abstract":"Recent research has sought to understand Transformers through the lens of in-context learning with functional data. We extend that line of work with the goal of moving closer to language models, considering categorical outcomes, nonlinear underlying models, and nonlinear attention. The contextual data are of the form $\\textsf{C}=(x_1,c_1,\\dots,x_N,c_{N})$ where each $c_i\\in\\{0,\\dots,C-1\\}$ is drawn from a categorical distribution that depends on covariates $x_i\\in\\mathbb{R}^d$. Contextual outcomes in the $m$th set of contextual data, $\\textsf{C}_m$, are modeled in terms of latent function $f_m(x)\\in\\textsf{F}$, where $\\textsf{F}$ is a functional class with $(C-1)$-dimensional vector output. The probability of observing class $c\\in\\{0,\\dots,C-1\\}$ is modeled in terms of the output components of $f_m(x)$ via the softmax. The Transformer parameters may be trained with $M$ contextual examples, $\\{\\textsf{C}_m\\}_{m=1,M}$, and the trained model is then applied to new contextual data $\\textsf{C}_{M+1}$ for new $f_{M+1}(x)\\in\\textsf{F}$. The goal is for the Transformer to constitute the probability of each category $c\\in\\{0,\\dots,C-1\\}$ for a new query $x_{N_{M+1}+1}$. We assume each component of $f_m(x)$ resides in a reproducing kernel Hilbert space (RKHS), specifying $\\textsf{F}$. Analysis and an extensive set of experiments suggest that on its forward pass the Transformer (with attention defined by the RKHS kernel) implements a form of gradient descent of the underlying function, connected to the latent vector function associated with the softmax. We present what is believed to be the first real-world demonstration of this few-shot-learning methodology, using the ImageNet dataset.","sentences":["Recent research has sought to understand Transformers through the lens of in-context learning with functional data.","We extend that line of work with the goal of moving closer to language models, considering categorical outcomes, nonlinear underlying models, and nonlinear attention.","The contextual data are of the form $\\textsf{C}=(x_1,c_1,\\dots,x_N,c_{N})$ where each $c_i\\in\\{0,\\dots,C-1\\}$ is drawn from a categorical distribution that depends on covariates $x_i\\in\\mathbb{R}^d$. Contextual outcomes in the $m$th set of contextual data, $\\textsf{C}_m$, are modeled in terms of latent function $f_m(x)\\in\\textsf{F}$, where $\\textsf{F}$ is a functional class with $(C-1)$-dimensional vector output.","The probability of observing class $c\\in\\{0,\\dots,C-1\\}$ is modeled in terms of the output components of $f_m(x)$ via the softmax.","The Transformer parameters may be trained with $M$ contextual examples, $\\{\\textsf{C}_m\\}_{m=1,M}$, and the trained model is then applied to new contextual data $\\textsf{C}_{M+1}$ for new $f_{M+1}(x)\\in\\textsf{F}$. The goal is for the Transformer to constitute the probability of each category $c\\in\\{0,\\dots,C-1\\}$ for a new query $x_{N_{M+1}+1}$. We assume each component of $f_m(x)$ resides in a reproducing kernel Hilbert space (RKHS), specifying $\\textsf{F}$. Analysis and an extensive set of experiments suggest that on its forward pass the Transformer (with attention defined by the RKHS kernel) implements a form of gradient descent of the underlying function, connected to the latent vector function associated with the softmax.","We present what is believed to be the first real-world demonstration of this few-shot-learning methodology, using the ImageNet dataset."],"url":"http://arxiv.org/abs/2405.17248v1","category":"stat.ML"}
{"created":"2024-05-27 14:37:01","title":"RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness","abstract":"Learning from feedback reduces the hallucination of multimodal large language models (MLLMs) by aligning them with human preferences. While traditional methods rely on labor-intensive and time-consuming manual labeling, recent approaches employing models as automatic labelers have shown promising results without human intervention. However, these methods heavily rely on costly proprietary models like GPT-4V, resulting in scalability issues. Moreover, this paradigm essentially distills the proprietary models to provide a temporary solution to quickly bridge the performance gap. As this gap continues to shrink, the community is soon facing the essential challenge of aligning MLLMs using labeler models of comparable capability. In this work, we introduce RLAIF-V, a novel framework that aligns MLLMs in a fully open-source paradigm for super GPT-4V trustworthiness. RLAIF-V maximally exploits the open-source feedback from two perspectives, including high-quality feedback data and online feedback learning algorithm. Extensive experiments on seven benchmarks in both automatic and human evaluation show that RLAIF-V substantially enhances the trustworthiness of models without sacrificing performance on other tasks. Using a 34B model as labeler, RLAIF-V 7B model reduces object hallucination by 82.9\\% and overall hallucination by 42.1\\%, outperforming the labeler model. Remarkably, RLAIF-V also reveals the self-alignment potential of open-source MLLMs, where a 12B model can learn from the feedback of itself to achieve less than 29.5\\% overall hallucination rate, surpassing GPT-4V (45.9\\%) by a large margin. The results shed light on a promising route to enhance the efficacy of leading-edge MLLMs.","sentences":["Learning from feedback reduces the hallucination of multimodal large language models (MLLMs) by aligning them with human preferences.","While traditional methods rely on labor-intensive and time-consuming manual labeling, recent approaches employing models as automatic labelers have shown promising results without human intervention.","However, these methods heavily rely on costly proprietary models like GPT-4V, resulting in scalability issues.","Moreover, this paradigm essentially distills the proprietary models to provide a temporary solution to quickly bridge the performance gap.","As this gap continues to shrink, the community is soon facing the essential challenge of aligning MLLMs using labeler models of comparable capability.","In this work, we introduce RLAIF-V, a novel framework that aligns MLLMs in a fully open-source paradigm for super GPT-4V trustworthiness.","RLAIF-V maximally exploits the open-source feedback from two perspectives, including high-quality feedback data and online feedback learning algorithm.","Extensive experiments on seven benchmarks in both automatic and human evaluation show that RLAIF-V substantially enhances the trustworthiness of models without sacrificing performance on other tasks.","Using a 34B model as labeler, RLAIF-V 7B model reduces object hallucination by 82.9\\% and overall hallucination by 42.1\\%, outperforming the labeler model.","Remarkably, RLAIF-V also reveals the self-alignment potential of open-source MLLMs, where a 12B model can learn from the feedback of itself to achieve less than 29.5\\% overall hallucination rate, surpassing GPT-4V (45.9\\%) by a large margin.","The results shed light on a promising route to enhance the efficacy of leading-edge MLLMs."],"url":"http://arxiv.org/abs/2405.17220v1","category":"cs.CL"}
{"created":"2024-05-27 13:01:25","title":"MVMS-RCN: A Dual-Domain Unfolding CT Reconstruction with Multi-sparse-view and Multi-scale Refinement-correction","abstract":"X-ray Computed Tomography (CT) is one of the most important diagnostic imaging techniques in clinical applications. Sparse-view CT imaging reduces the number of projection views to a lower radiation dose and alleviates the potential risk of radiation exposure. Most existing deep learning (DL) and deep unfolding sparse-view CT reconstruction methods: 1) do not fully use the projection data; 2) do not always link their architecture designs to a mathematical theory; 3) do not flexibly deal with multi-sparse-view reconstruction assignments. This paper aims to use mathematical ideas and design optimal DL imaging algorithms for sparse-view tomography reconstructions. We propose a novel dual-domain deep unfolding unified framework that offers a great deal of flexibility for multi-sparse-view CT reconstruction with different sampling views through a single model. This framework combines the theoretical advantages of model-based methods with the superior reconstruction performance of DL-based methods, resulting in the expected generalizability of DL. We propose a refinement module that utilizes unfolding projection domain to refine full-sparse-view projection errors, as well as an image domain correction module that distills multi-scale geometric error corrections to reconstruct sparse-view CT. This provides us with a new way to explore the potential of projection information and a new perspective on designing network architectures. All parameters of our proposed framework are learnable end to end, and our method possesses the potential to be applied to plug-and-play reconstruction. Extensive experiments demonstrate that our framework is superior to other existing state-of-the-art methods. Our source codes are available at https://github.com/fanxiaohong/MVMS-RCN.","sentences":["X-ray Computed Tomography (CT) is one of the most important diagnostic imaging techniques in clinical applications.","Sparse-view CT imaging reduces the number of projection views to a lower radiation dose and alleviates the potential risk of radiation exposure.","Most existing deep learning (DL) and deep unfolding sparse-view CT reconstruction methods: 1) do not fully use the projection data; 2) do not always link their architecture designs to a mathematical theory; 3) do not flexibly deal with multi-sparse-view reconstruction assignments.","This paper aims to use mathematical ideas and design optimal DL imaging algorithms for sparse-view tomography reconstructions.","We propose a novel dual-domain deep unfolding unified framework that offers a great deal of flexibility for multi-sparse-view CT reconstruction with different sampling views through a single model.","This framework combines the theoretical advantages of model-based methods with the superior reconstruction performance of DL-based methods, resulting in the expected generalizability of DL.","We propose a refinement module that utilizes unfolding projection domain to refine full-sparse-view projection errors, as well as an image domain correction module that distills multi-scale geometric error corrections to reconstruct sparse-view CT.","This provides us with a new way to explore the potential of projection information and a new perspective on designing network architectures.","All parameters of our proposed framework are learnable end to end, and our method possesses the potential to be applied to plug-and-play reconstruction.","Extensive experiments demonstrate that our framework is superior to other existing state-of-the-art methods.","Our source codes are available at https://github.com/fanxiaohong/MVMS-RCN."],"url":"http://arxiv.org/abs/2405.17141v1","category":"eess.IV"}
{"created":"2024-05-27 12:12:39","title":"Q-value Regularized Transformer for Offline Reinforcement Learning","abstract":"Recent advancements in offline reinforcement learning (RL) have underscored the capabilities of Conditional Sequence Modeling (CSM), a paradigm that learns the action distribution based on history trajectory and target returns for each state. However, these methods often struggle with stitching together optimal trajectories from sub-optimal ones due to the inconsistency between the sampled returns within individual trajectories and the optimal returns across multiple trajectories. Fortunately, Dynamic Programming (DP) methods offer a solution by leveraging a value function to approximate optimal future returns for each state, while these techniques are prone to unstable learning behaviors, particularly in long-horizon and sparse-reward scenarios. Building upon these insights, we propose the Q-value regularized Transformer (QT), which combines the trajectory modeling ability of the Transformer with the predictability of optimal future returns from DP methods. QT learns an action-value function and integrates a term maximizing action-values into the training loss of CSM, which aims to seek optimal actions that align closely with the behavior policy. Empirical evaluations on D4RL benchmark datasets demonstrate the superiority of QT over traditional DP and CSM methods, highlighting the potential of QT to enhance the state-of-the-art in offline RL.","sentences":["Recent advancements in offline reinforcement learning (RL) have underscored the capabilities of Conditional Sequence Modeling (CSM), a paradigm that learns the action distribution based on history trajectory and target returns for each state.","However, these methods often struggle with stitching together optimal trajectories from sub-optimal ones due to the inconsistency between the sampled returns within individual trajectories and the optimal returns across multiple trajectories.","Fortunately, Dynamic Programming (DP) methods offer a solution by leveraging a value function to approximate optimal future returns for each state, while these techniques are prone to unstable learning behaviors, particularly in long-horizon and sparse-reward scenarios.","Building upon these insights, we propose the Q-value regularized Transformer (QT), which combines the trajectory modeling ability of the Transformer with the predictability of optimal future returns from DP methods.","QT learns an action-value function and integrates a term maximizing action-values into the training loss of CSM, which aims to seek optimal actions that align closely with the behavior policy.","Empirical evaluations on D4RL benchmark datasets demonstrate the superiority of QT over traditional DP and CSM methods, highlighting the potential of QT to enhance the state-of-the-art in offline RL."],"url":"http://arxiv.org/abs/2405.17098v1","category":"cs.LG"}
{"created":"2024-05-27 12:06:37","title":"An improved penalty-based excited-state variational Monte Carlo approach with deep-learning ansatzes","abstract":"We introduce several improvements to the penalty-based variational quantum Monte Carlo (VMC) algorithm for computing electronic excited states, and demonstrate that the accuracy of the updated method is competitive with other available excited-state VMC approaches. A theoretical comparison of the computational aspects of these algorithms is presented, where several benefits of the penalty-based method are identified. Our main contributions include an automatic mechanism for tuning the scale of the penalty terms, an updated form of the overlap penalty with proven convergence properties, and a new term that penalizes the spin of the wave function, enabling the selective computation of states with a given spin. With these improvements, along with the use of the latest self-attention-based ansatz, the penalty-based method achieves a mean absolute error below 1 kcal/mol for the vertical excitation energies of a set of 26 atoms and molecules, without relying on variance matching schemes. Considering excited states along the dissociation of the carbon dimer, the accuracy of the penalty-based method is on par with that of natural-excited-state (NES) VMC, while also providing results for larger sections of the potential energy surface. Additionally, the accuracy of the original penalty-based method is improved for a conical intersection of ethylene, with the predicted angle of the intersection agreeing well with both NES-VMC and multi-reference configuration interaction.","sentences":["We introduce several improvements to the penalty-based variational quantum Monte Carlo (VMC) algorithm for computing electronic excited states, and demonstrate that the accuracy of the updated method is competitive with other available excited-state VMC approaches.","A theoretical comparison of the computational aspects of these algorithms is presented, where several benefits of the penalty-based method are identified.","Our main contributions include an automatic mechanism for tuning the scale of the penalty terms, an updated form of the overlap penalty with proven convergence properties, and a new term that penalizes the spin of the wave function, enabling the selective computation of states with a given spin.","With these improvements, along with the use of the latest self-attention-based ansatz, the penalty-based method achieves a mean absolute error below 1 kcal/mol for the vertical excitation energies of a set of 26 atoms and molecules, without relying on variance matching schemes.","Considering excited states along the dissociation of the carbon dimer, the accuracy of the penalty-based method is on par with that of natural-excited-state (NES) VMC, while also providing results for larger sections of the potential energy surface.","Additionally, the accuracy of the original penalty-based method is improved for a conical intersection of ethylene, with the predicted angle of the intersection agreeing well with both NES-VMC and multi-reference configuration interaction."],"url":"http://arxiv.org/abs/2405.17089v1","category":"physics.chem-ph"}
{"created":"2024-05-27 11:46:14","title":"Interaction-Force Transport Gradient Flows","abstract":"This paper presents a new type of gradient flow geometries over non-negative and probability measures motivated via a principled construction that combines the optimal transport and interaction forces modeled by reproducing kernels. Concretely, we propose the interaction-force transport (IFT) gradient flows and its spherical variant via an infimal convolution of the Wasserstein and spherical MMD Riemannian metric tensors. We then develop a particle-based optimization algorithm based on the JKO-splitting scheme of the mass-preserving spherical IFT gradient flows. Finally, we provide both theoretical global exponential convergence guarantees and empirical simulation results for applying the IFT gradient flows to the sampling task of MMD-minimization studied by Arbel et al. [2019]. Furthermore, we prove that the spherical IFT gradient flow enjoys the best of both worlds by providing the global exponential convergence guarantee for both the MMD and KL energy.","sentences":["This paper presents a new type of gradient flow geometries over non-negative and probability measures motivated via a principled construction that combines the optimal transport and interaction forces modeled by reproducing kernels.","Concretely, we propose the interaction-force transport (IFT) gradient flows and its spherical variant via an infimal convolution of the Wasserstein and spherical MMD Riemannian metric tensors.","We then develop a particle-based optimization algorithm based on the JKO-splitting scheme of the mass-preserving spherical IFT gradient flows.","Finally, we provide both theoretical global exponential convergence guarantees and empirical simulation results for applying the IFT gradient flows to the sampling task of MMD-minimization studied by Arbel et al.","[2019].","Furthermore, we prove that the spherical IFT gradient flow enjoys the best of both worlds by providing the global exponential convergence guarantee for both the MMD and KL energy."],"url":"http://arxiv.org/abs/2405.17075v1","category":"cs.LG"}
{"created":"2024-05-27 11:31:54","title":"Provably Efficient Reinforcement Learning with Multinomial Logit Function Approximation","abstract":"We study a new class of MDPs that employs multinomial logit (MNL) function approximation to ensure valid probability distributions over the state space. Despite its benefits, introducing non-linear function approximation raises significant challenges in both computational and statistical efficiency. The best-known method of Hwang and Oh [2023] has achieved an $\\widetilde{\\mathcal{O}}(\\kappa^{-1}dH^2\\sqrt{K})$ regret, where $\\kappa$ is a problem-dependent quantity, $d$ is the feature space dimension, $H$ is the episode length, and $K$ is the number of episodes. While this result attains the same rate in $K$ as the linear cases, the method requires storing all historical data and suffers from an $\\mathcal{O}(K)$ computation cost per episode. Moreover, the quantity $\\kappa$ can be exponentially small, leading to a significant gap for the regret compared to the linear cases. In this work, we first address the computational concerns by proposing an online algorithm that achieves the same regret with only $\\mathcal{O}(1)$ computation cost. Then, we design two algorithms that leverage local information to enhance statistical efficiency. They not only maintain an $\\mathcal{O}(1)$ computation cost per episode but achieve improved regrets of $\\widetilde{\\mathcal{O}}(\\kappa^{-1/2}dH^2\\sqrt{K})$ and $\\widetilde{\\mathcal{O}}(dH^2\\sqrt{K} + \\kappa^{-1}d^2H^2)$ respectively. Finally, we establish a lower bound, justifying the optimality of our results in $d$ and $K$. To the best of our knowledge, this is the first work that achieves almost the same computational and statistical efficiency as linear function approximation while employing non-linear function approximation for reinforcement learning.","sentences":["We study a new class of MDPs that employs multinomial logit (MNL) function approximation to ensure valid probability distributions over the state space.","Despite its benefits, introducing non-linear function approximation raises significant challenges in both computational and statistical efficiency.","The best-known method of Hwang and","Oh [2023] has achieved an $\\widetilde{\\mathcal{O}}(\\kappa^{-1}dH^2\\sqrt{K})$ regret, where $\\kappa$ is a problem-dependent quantity, $d$ is the feature space dimension, $H$ is the episode length, and $K$ is the number of episodes.","While this result attains the same rate in $K$ as the linear cases, the method requires storing all historical data and suffers from an $\\mathcal{O}(K)$ computation cost per episode.","Moreover, the quantity $\\kappa$ can be exponentially small, leading to a significant gap for the regret compared to the linear cases.","In this work, we first address the computational concerns by proposing an online algorithm that achieves the same regret with only $\\mathcal{O}(1)$ computation cost.","Then, we design two algorithms that leverage local information to enhance statistical efficiency.","They not only maintain an $\\mathcal{O}(1)$ computation cost per episode but achieve improved regrets of $\\widetilde{\\mathcal{O}}(\\kappa^{-1/2}dH^2\\sqrt{K})$ and $\\widetilde{\\mathcal{O}}(dH^2\\sqrt{K} + \\kappa^{-1}d^2H^2)$ respectively.","Finally, we establish a lower bound, justifying the optimality of our results in $d$ and $K$. To the best of our knowledge, this is the first work that achieves almost the same computational and statistical efficiency as linear function approximation while employing non-linear function approximation for reinforcement learning."],"url":"http://arxiv.org/abs/2405.17061v1","category":"cs.LG"}
{"created":"2024-05-27 11:21:26","title":"Improving Data-aware and Parameter-aware Robustness for Continual Learning","abstract":"The goal of Continual Learning (CL) task is to continuously learn multiple new tasks sequentially while achieving a balance between the plasticity and stability of new and old knowledge. This paper analyzes that this insufficiency arises from the ineffective handling of outliers, leading to abnormal gradients and unexpected model updates. To address this issue, we enhance the data-aware and parameter-aware robustness of CL, proposing a Robust Continual Learning (RCL) method. From the data perspective, we develop a contrastive loss based on the concepts of uniformity and alignment, forming a feature distribution that is more applicable to outliers. From the parameter perspective, we present a forward strategy for worst-case perturbation and apply robust gradient projection to the parameters. The experimental results on three benchmarks show that the proposed method effectively maintains robustness and achieves new state-of-the-art (SOTA) results. The code is available at: https://github.com/HanxiXiao/RCL","sentences":["The goal of Continual Learning (CL) task is to continuously learn multiple new tasks sequentially while achieving a balance between the plasticity and stability of new and old knowledge.","This paper analyzes that this insufficiency arises from the ineffective handling of outliers, leading to abnormal gradients and unexpected model updates.","To address this issue, we enhance the data-aware and parameter-aware robustness of CL, proposing a Robust Continual Learning (RCL) method.","From the data perspective, we develop a contrastive loss based on the concepts of uniformity and alignment, forming a feature distribution that is more applicable to outliers.","From the parameter perspective, we present a forward strategy for worst-case perturbation and apply robust gradient projection to the parameters.","The experimental results on three benchmarks show that the proposed method effectively maintains robustness and achieves new state-of-the-art (SOTA) results.","The code is available at: https://github.com/HanxiXiao/RCL"],"url":"http://arxiv.org/abs/2405.17054v1","category":"cs.LG"}
{"created":"2024-05-27 11:04:05","title":"HeNCler: Node Clustering in Heterophilous Graphs through Learned Asymmetric Similarity","abstract":"Clustering nodes in heterophilous graphs presents unique challenges due to the asymmetric relationships often overlooked by traditional methods, which moreover assume that good clustering corresponds to high intra-cluster and low inter-cluster connectivity. To address these issues, we introduce HeNCler - a novel approach for Heterophilous Node Clustering. Our method begins by defining a weighted kernel singular value decomposition to create an asymmetric similarity graph, applicable to both directed and undirected graphs. We further establish that the dual problem of this formulation aligns with asymmetric kernel spectral clustering, interpreting learned graph similarities without relying on homophily. We demonstrate the ability to solve the primal problem directly, circumventing the computational difficulties of the dual approach. Experimental evidence confirms that HeNCler significantly enhances performance in node clustering tasks within heterophilous graph contexts.","sentences":["Clustering nodes in heterophilous graphs presents unique challenges due to the asymmetric relationships often overlooked by traditional methods, which moreover assume that good clustering corresponds to high intra-cluster and low inter-cluster connectivity.","To address these issues, we introduce HeNCler - a novel approach for Heterophilous Node Clustering.","Our method begins by defining a weighted kernel singular value decomposition to create an asymmetric similarity graph, applicable to both directed and undirected graphs.","We further establish that the dual problem of this formulation aligns with asymmetric kernel spectral clustering, interpreting learned graph similarities without relying on homophily.","We demonstrate the ability to solve the primal problem directly, circumventing the computational difficulties of the dual approach.","Experimental evidence confirms that HeNCler significantly enhances performance in node clustering tasks within heterophilous graph contexts."],"url":"http://arxiv.org/abs/2405.17050v1","category":"cs.LG"}
{"created":"2024-05-27 10:54:42","title":"LabObf: A Label Protection Scheme for Vertical Federated Learning Through Label Obfuscation","abstract":"Split learning, as one of the most common architectures in vertical federated learning, has gained widespread use in industry due to its privacy-preserving characteristics. In this architecture, the party holding the labels seeks cooperation from other parties to improve model performance due to insufficient feature data. Each of these participants has a self-defined bottom model to learn hidden representations from its own feature data and uploads the embedding vectors to the top model held by the label holder for final predictions. This design allows participants to conduct joint training without directly exchanging data. However, existing research points out that malicious participants may still infer label information from the uploaded embeddings, leading to privacy leakage. In this paper, we first propose an embedding extension attack that manually modifies embeddings to undermine existing defense strategies, which rely on constraining the correlation between the embeddings uploaded by participants and the labels. Subsequently, we propose a new label obfuscation defense strategy, called `LabObf', which randomly maps each original one-hot vector label to multiple numerical soft labels with values intertwined, significantly increasing the difficulty for attackers to infer the labels. We conduct experiments on four different types of datasets, and the results show that LabObf can reduce the attacker's success rate to near random guessing while maintaining an acceptable model accuracy.","sentences":["Split learning, as one of the most common architectures in vertical federated learning, has gained widespread use in industry due to its privacy-preserving characteristics.","In this architecture, the party holding the labels seeks cooperation from other parties to improve model performance due to insufficient feature data.","Each of these participants has a self-defined bottom model to learn hidden representations from its own feature data and uploads the embedding vectors to the top model held by the label holder for final predictions.","This design allows participants to conduct joint training without directly exchanging data.","However, existing research points out that malicious participants may still infer label information from the uploaded embeddings, leading to privacy leakage.","In this paper, we first propose an embedding extension attack that manually modifies embeddings to undermine existing defense strategies, which rely on constraining the correlation between the embeddings uploaded by participants and the labels.","Subsequently, we propose a new label obfuscation defense strategy, called `LabObf', which randomly maps each original one-hot vector label to multiple numerical soft labels with values intertwined, significantly increasing the difficulty for attackers to infer the labels.","We conduct experiments on four different types of datasets, and the results show that LabObf can reduce the attacker's success rate to near random guessing while maintaining an acceptable model accuracy."],"url":"http://arxiv.org/abs/2405.17042v1","category":"cs.LG"}
{"created":"2024-05-27 10:30:59","title":"Multi-view Disparity Estimation Using a Novel Gradient Consistency Model","abstract":"Variational approaches to disparity estimation typically use a linearised brightness constancy constraint, which only applies in smooth regions and over small distances. Accordingly, current variational approaches rely on a schedule to progressively include image data. This paper proposes the use of Gradient Consistency information to assess the validity of the linearisation; this information is used to determine the weights applied to the data term as part of an analytically inspired Gradient Consistency Model. The Gradient Consistency Model penalises the data term for view pairs that have a mismatch between the spatial gradients in the source view and the spatial gradients in the target view. Instead of relying on a tuned or learned schedule, the Gradient Consistency Model is self-scheduling, since the weights evolve as the algorithm progresses. We show that the Gradient Consistency Model outperforms standard coarse-to-fine schemes and the recently proposed progressive inclusion of views approach in both rate of convergence and accuracy.","sentences":["Variational approaches to disparity estimation typically use a linearised brightness constancy constraint, which only applies in smooth regions and over small distances.","Accordingly, current variational approaches rely on a schedule to progressively include image data.","This paper proposes the use of Gradient Consistency information to assess the validity of the linearisation; this information is used to determine the weights applied to the data term as part of an analytically inspired Gradient Consistency Model.","The Gradient Consistency Model penalises the data term for view pairs that have a mismatch between the spatial gradients in the source view and the spatial gradients in the target view.","Instead of relying on a tuned or learned schedule, the Gradient Consistency Model is self-scheduling, since the weights evolve as the algorithm progresses.","We show that the Gradient Consistency Model outperforms standard coarse-to-fine schemes and the recently proposed progressive inclusion of views approach in both rate of convergence and accuracy."],"url":"http://arxiv.org/abs/2405.17029v1","category":"eess.IV"}
{"created":"2024-05-27 09:46:09","title":"UIT-DarkCow team at ImageCLEFmedical Caption 2024: Diagnostic Captioning for Radiology Images Efficiency with Transformer Models","abstract":"Purpose: This study focuses on the development of automated text generation from radiology images, termed diagnostic captioning, to assist medical professionals in reducing clinical errors and improving productivity. The aim is to provide tools that enhance report quality and efficiency, which can significantly impact both clinical practice and deep learning research in the biomedical field. Methods: In our participation in the ImageCLEFmedical2024 Caption evaluation campaign, we explored caption prediction tasks using advanced Transformer-based models. We developed methods incorporating Transformer encoder-decoder and Query Transformer architectures. These models were trained and evaluated to generate diagnostic captions from radiology images. Results: Experimental evaluations demonstrated the effectiveness of our models, with the VisionDiagnostor-BioBART model achieving the highest BERTScore of 0.6267. This performance contributed to our team, DarkCow, achieving third place on the leaderboard. Conclusion: Our diagnostic captioning models show great promise in aiding medical professionals by generating high-quality reports efficiently. This approach can facilitate better data processing and performance optimization in medical imaging departments, ultimately benefiting healthcare delivery.","sentences":["Purpose:","This study focuses on the development of automated text generation from radiology images, termed diagnostic captioning, to assist medical professionals in reducing clinical errors and improving productivity.","The aim is to provide tools that enhance report quality and efficiency, which can significantly impact both clinical practice and deep learning research in the biomedical field.","Methods: In our participation in the ImageCLEFmedical2024 Caption evaluation campaign, we explored caption prediction tasks using advanced Transformer-based models.","We developed methods incorporating Transformer encoder-decoder and Query Transformer architectures.","These models were trained and evaluated to generate diagnostic captions from radiology images.","Results: Experimental evaluations demonstrated the effectiveness of our models, with the VisionDiagnostor-BioBART model achieving the highest BERTScore of 0.6267.","This performance contributed to our team, DarkCow, achieving third place on the leaderboard.","Conclusion: Our diagnostic captioning models show great promise in aiding medical professionals by generating high-quality reports efficiently.","This approach can facilitate better data processing and performance optimization in medical imaging departments, ultimately benefiting healthcare delivery."],"url":"http://arxiv.org/abs/2405.17002v1","category":"cs.CV"}
{"created":"2024-05-27 09:42:52","title":"Mitigating Noisy Correspondence by Geometrical Structure Consistency Learning","abstract":"Noisy correspondence that refers to mismatches in cross-modal data pairs, is prevalent on human-annotated or web-crawled datasets. Prior approaches to leverage such data mainly consider the application of uni-modal noisy label learning without amending the impact on both cross-modal and intra-modal geometrical structures in multimodal learning. Actually, we find that both structures are effective to discriminate noisy correspondence through structural differences when being well-established. Inspired by this observation, we introduce a Geometrical Structure Consistency (GSC) method to infer the true correspondence. Specifically, GSC ensures the preservation of geometrical structures within and between modalities, allowing for the accurate discrimination of noisy samples based on structural differences. Utilizing these inferred true correspondence labels, GSC refines the learning of geometrical structures by filtering out the noisy samples. Experiments across four cross-modal datasets confirm that GSC effectively identifies noisy samples and significantly outperforms the current leading methods.","sentences":["Noisy correspondence that refers to mismatches in cross-modal data pairs, is prevalent on human-annotated or web-crawled datasets.","Prior approaches to leverage such data mainly consider the application of uni-modal noisy label learning without amending the impact on both cross-modal and intra-modal geometrical structures in multimodal learning.","Actually, we find that both structures are effective to discriminate noisy correspondence through structural differences when being well-established.","Inspired by this observation, we introduce a Geometrical Structure Consistency (GSC) method to infer the true correspondence.","Specifically, GSC ensures the preservation of geometrical structures within and between modalities, allowing for the accurate discrimination of noisy samples based on structural differences.","Utilizing these inferred true correspondence labels, GSC refines the learning of geometrical structures by filtering out the noisy samples.","Experiments across four cross-modal datasets confirm that GSC effectively identifies noisy samples and significantly outperforms the current leading methods."],"url":"http://arxiv.org/abs/2405.16996v1","category":"cs.CV"}
{"created":"2024-05-27 08:26:45","title":"Empowering Large Language Models to Set up a Knowledge Retrieval Indexer via Self-Learning","abstract":"Retrieval-Augmented Generation (RAG) offers a cost-effective approach to injecting real-time knowledge into large language models (LLMs). Nevertheless, constructing and validating high-quality knowledge repositories require considerable effort. We propose a pre-retrieval framework named Pseudo-Graph Retrieval-Augmented Generation (PG-RAG), which conceptualizes LLMs as students by providing them with abundant raw reading materials and encouraging them to engage in autonomous reading to record factual information in their own words. The resulting concise, well-organized mental indices are interconnected through common topics or complementary facts to form a pseudo-graph database. During the retrieval phase, PG-RAG mimics the human behavior in flipping through notes, identifying fact paths and subsequently exploring the related contexts. Adhering to the principle of the path taken by many is the best, it integrates highly corroborated fact paths to provide a structured and refined sub-graph assisting LLMs. We validated PG-RAG on three specialized question-answering datasets. In single-document tasks, PG-RAG significantly outperformed the current best baseline, KGP-LLaMA, across all key evaluation metrics, with an average overall performance improvement of 11.6%. Specifically, its BLEU score increased by approximately 14.3%, and the QE-F1 metric improved by 23.7%. In multi-document scenarios, the average metrics of PG-RAG were at least 2.35% higher than the best baseline. Notably, the BLEU score and QE-F1 metric showed stable improvements of around 7.55% and 12.75%, respectively. Our code: https://github.com/IAAR-Shanghai/PGRAG.","sentences":["Retrieval-Augmented Generation (RAG) offers a cost-effective approach to injecting real-time knowledge into large language models (LLMs).","Nevertheless, constructing and validating high-quality knowledge repositories require considerable effort.","We propose a pre-retrieval framework named Pseudo-Graph Retrieval-Augmented Generation (PG-RAG), which conceptualizes LLMs as students by providing them with abundant raw reading materials and encouraging them to engage in autonomous reading to record factual information in their own words.","The resulting concise, well-organized mental indices are interconnected through common topics or complementary facts to form a pseudo-graph database.","During the retrieval phase, PG-RAG mimics the human behavior in flipping through notes, identifying fact paths and subsequently exploring the related contexts.","Adhering to the principle of the path taken by many is the best, it integrates highly corroborated fact paths to provide a structured and refined sub-graph assisting LLMs.","We validated PG-RAG on three specialized question-answering datasets.","In single-document tasks, PG-RAG significantly outperformed the current best baseline, KGP-LLaMA, across all key evaluation metrics, with an average overall performance improvement of 11.6%.","Specifically, its BLEU score increased by approximately 14.3%, and the QE-F1 metric improved by 23.7%.","In multi-document scenarios, the average metrics of PG-RAG were at least 2.35% higher than the best baseline.","Notably, the BLEU score and QE-F1 metric showed stable improvements of around 7.55% and 12.75%, respectively.","Our code: https://github.com/IAAR-Shanghai/PGRAG."],"url":"http://arxiv.org/abs/2405.16933v1","category":"cs.CL"}
{"created":"2024-05-27 08:25:38","title":"ChiSCAT: unsupervised learning of recurrent cellular micro-motion patterns from a chaotic speckle pattern","abstract":"There is considerable evidence that action potentials are accompanied by \"intrinsic optical signals\", such as a nanometer-scale motion of the cell membrane. Here we present ChiSCAT, a technically simple imaging scheme that detects such signals with interferometric sensitivity. ChiSCAT combines illumination by a {\\bf ch}aotic speckle pattern and interferometric scattering microscopy ({\\bf iSCAT}) to sensitively detect motion in any point and any direction. The technique features reflective high-NA illumination, common-path suppression of vibrations and a large field of view. This approach maximizes sensitivity to motion, but does not produce a visually interpretable image. We show that unsupervised learning based on matched filtering and motif discovery can recover underlying motion patterns and detect action potentials. We demonstrate these claims in an experiment on blebbistatin-paralyzed cardiomyocytes. ChiSCAT promises to even work in scattering tissue, including a living brain.","sentences":["There is considerable evidence that action potentials are accompanied by \"intrinsic optical signals\", such as a nanometer-scale motion of the cell membrane.","Here we present ChiSCAT, a technically simple imaging scheme that detects such signals with interferometric sensitivity.","ChiSCAT combines illumination by a {\\bf ch}aotic speckle pattern and interferometric scattering microscopy ({\\bf iSCAT}) to sensitively detect motion in any point and any direction.","The technique features reflective high-NA illumination, common-path suppression of vibrations and a large field of view.","This approach maximizes sensitivity to motion, but does not produce a visually interpretable image.","We show that unsupervised learning based on matched filtering and motif discovery can recover underlying motion patterns and detect action potentials.","We demonstrate these claims in an experiment on blebbistatin-paralyzed cardiomyocytes.","ChiSCAT promises to even work in scattering tissue, including a living brain."],"url":"http://arxiv.org/abs/2405.16931v1","category":"physics.optics"}
{"created":"2024-05-27 08:10:46","title":"The Uncanny Valley: Exploring Adversarial Robustness from a Flatness Perspective","abstract":"Flatness of the loss surface not only correlates positively with generalization but is also related to adversarial robustness, since perturbations of inputs relate non-linearly to perturbations of weights. In this paper, we empirically analyze the relation between adversarial examples and relative flatness with respect to the parameters of one layer. We observe a peculiar property of adversarial examples: during an iterative first-order white-box attack, the flatness of the loss surface measured around the adversarial example first becomes sharper until the label is flipped, but if we keep the attack running it runs into a flat uncanny valley where the label remains flipped. We find this phenomenon across various model architectures and datasets. Our results also extend to large language models (LLMs), but due to the discrete nature of the input space and comparatively weak attacks, the adversarial examples rarely reach a truly flat region. Most importantly, this phenomenon shows that flatness alone cannot explain adversarial robustness unless we can also guarantee the behavior of the function around the examples. We theoretically connect relative flatness to adversarial robustness by bounding the third derivative of the loss surface, underlining the need for flatness in combination with a low global Lipschitz constant for a robust model.","sentences":["Flatness of the loss surface not only correlates positively with generalization but is also related to adversarial robustness, since perturbations of inputs relate non-linearly to perturbations of weights.","In this paper, we empirically analyze the relation between adversarial examples and relative flatness with respect to the parameters of one layer.","We observe a peculiar property of adversarial examples: during an iterative first-order white-box attack, the flatness of the loss surface measured around the adversarial example first becomes sharper until the label is flipped, but if we keep the attack running it runs into a flat uncanny valley where the label remains flipped.","We find this phenomenon across various model architectures and datasets.","Our results also extend to large language models (LLMs), but due to the discrete nature of the input space and comparatively weak attacks, the adversarial examples rarely reach a truly flat region.","Most importantly, this phenomenon shows that flatness alone cannot explain adversarial robustness unless we can also guarantee the behavior of the function around the examples.","We theoretically connect relative flatness to adversarial robustness by bounding the third derivative of the loss surface, underlining the need for flatness in combination with a low global Lipschitz constant for a robust model."],"url":"http://arxiv.org/abs/2405.16918v1","category":"cs.LG"}
{"created":"2024-05-27 08:08:51","title":"Multilingual Diversity Improves Vision-Language Representations","abstract":"Massive web-crawled image-text datasets lay the foundation for recent progress in multimodal learning. These datasets are designed with the goal of training a model to do well on standard computer vision benchmarks, many of which, however, have been shown to be English-centric (e.g., ImageNet). Consequently, existing data curation techniques gravitate towards using predominantly English image-text pairs and discard many potentially useful non-English samples. Our work questions this practice. Multilingual data is inherently enriching not only because it provides a gateway to learn about culturally salient concepts, but also because it depicts common concepts differently from monolingual data. We thus conduct a systematic study to explore the performance benefits of using more samples of non-English origins with respect to English vision tasks. By translating all multilingual image-text pairs from a raw web crawl to English and re-filtering them, we increase the prevalence of (translated) multilingual data in the resulting training set. Pre-training on this dataset outperforms using English-only or English-dominated datasets on ImageNet, ImageNet distribution shifts, image-English-text retrieval and on average across 38 tasks from the DataComp benchmark. On a geographically diverse task like GeoDE, we also observe improvements across all regions, with the biggest gain coming from Africa. In addition, we quantitatively show that English and non-English data are significantly different in both image and (translated) text space. We hope that our findings motivate future work to be more intentional about including multicultural and multilingual data, not just when non-English or geographically diverse tasks are involved, but to enhance model capabilities at large.","sentences":["Massive web-crawled image-text datasets lay the foundation for recent progress in multimodal learning.","These datasets are designed with the goal of training a model to do well on standard computer vision benchmarks, many of which, however, have been shown to be English-centric (e.g., ImageNet).","Consequently, existing data curation techniques gravitate towards using predominantly English image-text pairs and discard many potentially useful non-English samples.","Our work questions this practice.","Multilingual data is inherently enriching not only because it provides a gateway to learn about culturally salient concepts, but also because it depicts common concepts differently from monolingual data.","We thus conduct a systematic study to explore the performance benefits of using more samples of non-English origins with respect to English vision tasks.","By translating all multilingual image-text pairs from a raw web crawl to English and re-filtering them, we increase the prevalence of (translated) multilingual data in the resulting training set.","Pre-training on this dataset outperforms using English-only or English-dominated datasets on ImageNet, ImageNet distribution shifts, image-English-text retrieval and on average across 38 tasks from the DataComp benchmark.","On a geographically diverse task like GeoDE, we also observe improvements across all regions, with the biggest gain coming from Africa.","In addition, we quantitatively show that English and non-English data are significantly different in both image and (translated) text space.","We hope that our findings motivate future work to be more intentional about including multicultural and multilingual data, not just when non-English or geographically diverse tasks are involved, but to enhance model capabilities at large."],"url":"http://arxiv.org/abs/2405.16915v1","category":"cs.CV"}
{"created":"2024-05-27 07:50:09","title":"Predicting from a Different Perspective in Re-ranking Model for Inductive Knowledge Graph Completion","abstract":"Rule-induction models have been shown great power in the inductive setting of knowledge graph completion. In this setting, the models are tested on a knowledge graph entirely composed of unseen entities. These models learn relation patterns as rules by utilizing subgraphs. The same input but different rules cause differences in the model's predictions. In this paper, we focus on this behavior of the model. We propose a re-ranking-based model called ReDistLP (Re-ranking with a Distinct Model for Link Prediction). This model enhances the effectiveness of re-ranking by leveraging the difference in the predictions between the initial retriever and the re-ranker. ReDistLP outperforms the state-of-the-art methods in 2 out of 3 datasets.","sentences":["Rule-induction models have been shown great power in the inductive setting of knowledge graph completion.","In this setting, the models are tested on a knowledge graph entirely composed of unseen entities.","These models learn relation patterns as rules by utilizing subgraphs.","The same input but different rules cause differences in the model's predictions.","In this paper, we focus on this behavior of the model.","We propose a re-ranking-based model called ReDistLP (Re-ranking with a Distinct Model for Link Prediction).","This model enhances the effectiveness of re-ranking by leveraging the difference in the predictions between the initial retriever and the re-ranker.","ReDistLP outperforms the state-of-the-art methods in 2 out of 3 datasets."],"url":"http://arxiv.org/abs/2405.16902v1","category":"cs.LG"}
{"created":"2024-05-27 07:38:26","title":"Anonymization Prompt Learning for Facial Privacy-Preserving Text-to-Image Generation","abstract":"Text-to-image diffusion models, such as Stable Diffusion, generate highly realistic images from text descriptions. However, the generation of certain content at such high quality raises concerns. A prominent issue is the accurate depiction of identifiable facial images, which could lead to malicious deepfake generation and privacy violations. In this paper, we propose Anonymization Prompt Learning (APL) to address this problem. Specifically, we train a learnable prompt prefix for text-to-image diffusion models, which forces the model to generate anonymized facial identities, even when prompted to produce images of specific individuals. Extensive quantitative and qualitative experiments demonstrate the successful anonymization performance of APL, which anonymizes any specific individuals without compromising the quality of non-identity-specific image generation. Furthermore, we reveal the plug-and-play property of the learned prompt prefix, enabling its effective application across different pretrained text-to-image models for transferrable privacy and security protection against the risks of deepfakes.","sentences":["Text-to-image diffusion models, such as Stable Diffusion, generate highly realistic images from text descriptions.","However, the generation of certain content at such high quality raises concerns.","A prominent issue is the accurate depiction of identifiable facial images, which could lead to malicious deepfake generation and privacy violations.","In this paper, we propose Anonymization Prompt Learning (APL) to address this problem.","Specifically, we train a learnable prompt prefix for text-to-image diffusion models, which forces the model to generate anonymized facial identities, even when prompted to produce images of specific individuals.","Extensive quantitative and qualitative experiments demonstrate the successful anonymization performance of APL, which anonymizes any specific individuals without compromising the quality of non-identity-specific image generation.","Furthermore, we reveal the plug-and-play property of the learned prompt prefix, enabling its effective application across different pretrained text-to-image models for transferrable privacy and security protection against the risks of deepfakes."],"url":"http://arxiv.org/abs/2405.16895v1","category":"cs.CV"}
{"created":"2024-05-27 07:08:58","title":"Hawk: Learning to Understand Open-World Video Anomalies","abstract":"Video Anomaly Detection (VAD) systems can autonomously monitor and identify disturbances, reducing the need for manual labor and associated costs. However, current VAD systems are often limited by their superficial semantic understanding of scenes and minimal user interaction. Additionally, the prevalent data scarcity in existing datasets restricts their applicability in open-world scenarios. In this paper, we introduce Hawk, a novel framework that leverages interactive large Visual Language Models (VLM) to interpret video anomalies precisely. Recognizing the difference in motion information between abnormal and normal videos, Hawk explicitly integrates motion modality to enhance anomaly identification. To reinforce motion attention, we construct an auxiliary consistency loss within the motion and video space, guiding the video branch to focus on the motion modality. Moreover, to improve the interpretation of motion-to-language, we establish a clear supervisory relationship between motion and its linguistic representation. Furthermore, we have annotated over 8,000 anomaly videos with language descriptions, enabling effective training across diverse open-world scenarios, and also created 8,000 question-answering pairs for users' open-world questions. The final results demonstrate that Hawk achieves SOTA performance, surpassing existing baselines in both video description generation and question-answering. Our codes/dataset/demo will be released at https://github.com/jqtangust/hawk.","sentences":["Video Anomaly Detection (VAD) systems can autonomously monitor and identify disturbances, reducing the need for manual labor and associated costs.","However, current VAD systems are often limited by their superficial semantic understanding of scenes and minimal user interaction.","Additionally, the prevalent data scarcity in existing datasets restricts their applicability in open-world scenarios.","In this paper, we introduce Hawk, a novel framework that leverages interactive large Visual Language Models (VLM) to interpret video anomalies precisely.","Recognizing the difference in motion information between abnormal and normal videos, Hawk explicitly integrates motion modality to enhance anomaly identification.","To reinforce motion attention, we construct an auxiliary consistency loss within the motion and video space, guiding the video branch to focus on the motion modality.","Moreover, to improve the interpretation of motion-to-language, we establish a clear supervisory relationship between motion and its linguistic representation.","Furthermore, we have annotated over 8,000 anomaly videos with language descriptions, enabling effective training across diverse open-world scenarios, and also created 8,000 question-answering pairs for users' open-world questions.","The final results demonstrate that Hawk achieves SOTA performance, surpassing existing baselines in both video description generation and question-answering.","Our codes/dataset/demo will be released at https://github.com/jqtangust/hawk."],"url":"http://arxiv.org/abs/2405.16886v1","category":"cs.CV"}
{"created":"2024-05-27 06:39:36","title":"Multi-Behavior Generative Recommendation","abstract":"Multi-behavior sequential recommendation (MBSR) aims to incorporate behavior types of interactions for better recommendations. Existing approaches focus on the next-item prediction objective, neglecting the value of integrating the target behavior type into the learning objective. In this paper, we propose MBGen, a novel Multi-Behavior sequential Generative recommendation framework. We formulate the MBSR task into a consecutive two-step process: (1) given item sequences, MBGen first predicts the next behavior type to frame the user intention, (2) given item sequences and a target behavior type, MBGen then predicts the next items. To model such a two-step process, we tokenize both behaviors and items into tokens and construct one single token sequence with both behaviors and items placed interleaved. Furthermore, MBGen learns to autoregressively generate the next behavior and item tokens in a unified generative recommendation paradigm, naturally enabling a multi-task capability. Additionally, we exploit the heterogeneous nature of token sequences in the generative recommendation and propose a position-routed sparse architecture to efficiently and effectively scale up models. Extensive experiments on public datasets demonstrate that MBGen significantly outperforms existing MBSR models across multiple tasks.","sentences":["Multi-behavior sequential recommendation (MBSR) aims to incorporate behavior types of interactions for better recommendations.","Existing approaches focus on the next-item prediction objective, neglecting the value of integrating the target behavior type into the learning objective.","In this paper, we propose MBGen, a novel Multi-Behavior sequential Generative recommendation framework.","We formulate the MBSR task into a consecutive two-step process: (1) given item sequences, MBGen first predicts the next behavior type to frame the user intention, (2) given item sequences and a target behavior type, MBGen then predicts the next items.","To model such a two-step process, we tokenize both behaviors and items into tokens and construct one single token sequence with both behaviors and items placed interleaved.","Furthermore, MBGen learns to autoregressively generate the next behavior and item tokens in a unified generative recommendation paradigm, naturally enabling a multi-task capability.","Additionally, we exploit the heterogeneous nature of token sequences in the generative recommendation and propose a position-routed sparse architecture to efficiently and effectively scale up models.","Extensive experiments on public datasets demonstrate that MBGen significantly outperforms existing MBSR models across multiple tasks."],"url":"http://arxiv.org/abs/2405.16871v1","category":"cs.IR"}
{"created":"2024-05-27 06:26:55","title":"NCIDiff: Non-covalent Interaction-generative Diffusion Model for Improving Reliability of 3D Molecule Generation Inside Protein Pocket","abstract":"Advancements in deep generative modeling have changed the paradigm of drug discovery. Among such approaches, target-aware methods that exploit 3D structures of protein pockets were spotlighted for generating ligand molecules with their plausible binding modes. While docking scores superficially assess the quality of generated ligands, closer inspection of the binding structures reveals the inconsistency in local interactions between a pocket and generated ligands. Here, we address the issue by explicitly generating non-covalent interactions (NCIs), which are universal patterns throughout protein-ligand complexes. Our proposed model, NCIDiff, simultaneously denoises NCI types of protein-ligand edges along with a 3D graph of a ligand molecule during the sampling. With the NCI-generating strategy, our model generates ligands with more reliable NCIs, especially outperforming the baseline diffusion-based models. We further adopted inpainting techniques on NCIs to further improve the quality of the generated molecules. Finally, we showcase the applicability of NCIDiff on drug design tasks for real-world settings with specialized objectives by guiding the generation process with desired NCI patterns.","sentences":["Advancements in deep generative modeling have changed the paradigm of drug discovery.","Among such approaches, target-aware methods that exploit 3D structures of protein pockets were spotlighted for generating ligand molecules with their plausible binding modes.","While docking scores superficially assess the quality of generated ligands, closer inspection of the binding structures reveals the inconsistency in local interactions between a pocket and generated ligands.","Here, we address the issue by explicitly generating non-covalent interactions (NCIs), which are universal patterns throughout protein-ligand complexes.","Our proposed model, NCIDiff, simultaneously denoises NCI types of protein-ligand edges along with a 3D graph of a ligand molecule during the sampling.","With the NCI-generating strategy, our model generates ligands with more reliable NCIs, especially outperforming the baseline diffusion-based models.","We further adopted inpainting techniques on NCIs to further improve the quality of the generated molecules.","Finally, we showcase the applicability of NCIDiff on drug design tasks for real-world settings with specialized objectives by guiding the generation process with desired NCI patterns."],"url":"http://arxiv.org/abs/2405.16861v1","category":"q-bio.BM"}
{"created":"2024-05-27 06:11:16","title":"Estimating Depth of Monocular Panoramic Image with Teacher-Student Model Fusing Equirectangular and Spherical Representations","abstract":"Disconnectivity and distortion are the two problems which must be coped with when processing 360 degrees equirectangular images. In this paper, we propose a method of estimating the depth of monocular panoramic image with a teacher-student model fusing equirectangular and spherical representations. In contrast with the existing methods fusing an equirectangular representation with a cube map representation or tangent representation, a spherical representation is a better choice because a sampling on a sphere is more uniform and can also cope with distortion more effectively. In this processing, a novel spherical convolution kernel computing with sampling points on a sphere is developed to extract features from the spherical representation, and then, a Segmentation Feature Fusion(SFF) methodology is utilized to combine the features with ones extracted from the equirectangular representation. In contrast with the existing methods using a teacher-student model to obtain a lighter model of depth estimation, we use a teacher-student model to learn the latent features of depth images. This results in a trained model which estimates the depth map of an equirectangular image using not only the feature maps extracted from an input equirectangular image but also the distilled knowledge learnt from the ground truth of depth map of a training set. In experiments, the proposed method is tested on several well-known 360 monocular depth estimation benchmark datasets, and outperforms the existing methods for the most evaluation indexes.","sentences":["Disconnectivity and distortion are the two problems which must be coped with when processing 360 degrees equirectangular images.","In this paper, we propose a method of estimating the depth of monocular panoramic image with a teacher-student model fusing equirectangular and spherical representations.","In contrast with the existing methods fusing an equirectangular representation with a cube map representation or tangent representation, a spherical representation is a better choice because a sampling on a sphere is more uniform and can also cope with distortion more effectively.","In this processing, a novel spherical convolution kernel computing with sampling points on a sphere is developed to extract features from the spherical representation, and then, a Segmentation Feature Fusion(SFF) methodology is utilized to combine the features with ones extracted from the equirectangular representation.","In contrast with the existing methods using a teacher-student model to obtain a lighter model of depth estimation, we use a teacher-student model to learn the latent features of depth images.","This results in a trained model which estimates the depth map of an equirectangular image using not only the feature maps extracted from an input equirectangular image but also the distilled knowledge learnt from the ground truth of depth map of a training set.","In experiments, the proposed method is tested on several well-known 360 monocular depth estimation benchmark datasets, and outperforms the existing methods for the most evaluation indexes."],"url":"http://arxiv.org/abs/2405.16858v1","category":"cs.CV"}
{"created":"2024-05-27 06:00:24","title":"Knowing What Not to Do: Leverage Language Model Insights for Action Space Pruning in Multi-agent Reinforcement Learning","abstract":"Multi-agent reinforcement learning (MARL) is employed to develop autonomous agents that can learn to adopt cooperative or competitive strategies within complex environments. However, the linear increase in the number of agents leads to a combinatorial explosion of the action space, which may result in algorithmic instability, difficulty in convergence, or entrapment in local optima. While researchers have designed a variety of effective algorithms to compress the action space, these methods also introduce new challenges, such as the need for manually designed prior knowledge or reliance on the structure of the problem, which diminishes the applicability of these techniques. In this paper, we introduce Evolutionary action SPAce Reduction with Knowledge (eSpark), an exploration function generation framework driven by large language models (LLMs) to boost exploration and prune unnecessary actions in MARL. Using just a basic prompt that outlines the overall task and setting, eSpark is capable of generating exploration functions in a zero-shot manner, identifying and pruning redundant or irrelevant state-action pairs, and then achieving autonomous improvement from policy feedback. In reinforcement learning tasks involving inventory management and traffic light control encompassing a total of 15 scenarios, eSpark consistently outperforms the combined MARL algorithm in all scenarios, achieving an average performance gain of 34.4% and 9.9% in the two types of tasks respectively. Additionally, eSpark has proven to be capable of managing situations with a large number of agents, securing a 29.7% improvement in scalability challenges that featured over 500 agents. The code can be found in https://github.com/LiuZhihao2022/eSpark.git.","sentences":["Multi-agent reinforcement learning (MARL) is employed to develop autonomous agents that can learn to adopt cooperative or competitive strategies within complex environments.","However, the linear increase in the number of agents leads to a combinatorial explosion of the action space, which may result in algorithmic instability, difficulty in convergence, or entrapment in local optima.","While researchers have designed a variety of effective algorithms to compress the action space, these methods also introduce new challenges, such as the need for manually designed prior knowledge or reliance on the structure of the problem, which diminishes the applicability of these techniques.","In this paper, we introduce Evolutionary action SPAce Reduction with Knowledge (eSpark), an exploration function generation framework driven by large language models (LLMs) to boost exploration and prune unnecessary actions in MARL.","Using just a basic prompt that outlines the overall task and setting, eSpark is capable of generating exploration functions in a zero-shot manner, identifying and pruning redundant or irrelevant state-action pairs, and then achieving autonomous improvement from policy feedback.","In reinforcement learning tasks involving inventory management and traffic light control encompassing a total of 15 scenarios, eSpark consistently outperforms the combined MARL algorithm in all scenarios, achieving an average performance gain of 34.4% and 9.9% in the two types of tasks respectively.","Additionally, eSpark has proven to be capable of managing situations with a large number of agents, securing a 29.7% improvement in scalability challenges that featured over 500 agents.","The code can be found in https://github.com/LiuZhihao2022/eSpark.git."],"url":"http://arxiv.org/abs/2405.16854v1","category":"cs.MA"}
{"created":"2024-05-27 05:32:46","title":"Non-stochastic Bandits With Evolving Observations","abstract":"We introduce a novel online learning framework that unifies and generalizes pre-established models, such as delayed and corrupted feedback, to encompass adversarial environments where action feedback evolves over time. In this setting, the observed loss is arbitrary and may not correlate with the true loss incurred, with each round updating previous observations adversarially. We propose regret minimization algorithms for both the full-information and bandit settings, with regret bounds quantified by the average feedback accuracy relative to the true loss. Our algorithms match the known regret bounds across many special cases, while also introducing previously unknown bounds.","sentences":["We introduce a novel online learning framework that unifies and generalizes pre-established models, such as delayed and corrupted feedback, to encompass adversarial environments where action feedback evolves over time.","In this setting, the observed loss is arbitrary and may not correlate with the true loss incurred, with each round updating previous observations adversarially.","We propose regret minimization algorithms for both the full-information and bandit settings, with regret bounds quantified by the average feedback accuracy relative to the true loss.","Our algorithms match the known regret bounds across many special cases, while also introducing previously unknown bounds."],"url":"http://arxiv.org/abs/2405.16843v1","category":"cs.LG"}
{"created":"2024-05-27 05:04:09","title":"Speech enhancement deep-learning architecture for efficient edge processing","abstract":"Deep learning has become a de facto method of choice for speech enhancement tasks with significant improvements in speech quality. However, real-time processing with reduced size and computations for low-power edge devices drastically degrades speech quality. Recently, transformer-based architectures have greatly reduced the memory requirements and provided ways to improve the model performance through local and global contexts. However, the transformer operations remain computationally heavy. In this work, we introduce WaveUNet squeeze-excitation Res2 (WSR)-based metric generative adversarial network (WSR-MGAN) architecture that can be efficiently implemented on low-power edge devices for noise suppression tasks while maintaining speech quality. We utilize multi-scale features using Res2Net blocks that can be related to spectral content used in speech-processing tasks. In the generator, we integrate squeeze-excitation blocks (SEB) with multi-scale features for maintaining local and global contexts along with gated recurrent units (GRUs). The proposed approach is optimized through a combined loss function calculated over raw waveform, multi-resolution magnitude spectrogram, and objective metrics using a metric discriminator. Experimental results in terms of various objective metrics on VoiceBank+DEMAND and DNS-2020 challenge datasets demonstrate that the proposed speech enhancement (SE) approach outperforms the baselines and achieves state-of-the-art (SOTA) performance in the time domain.","sentences":["Deep learning has become a de facto method of choice for speech enhancement tasks with significant improvements in speech quality.","However, real-time processing with reduced size and computations for low-power edge devices drastically degrades speech quality.","Recently, transformer-based architectures have greatly reduced the memory requirements and provided ways to improve the model performance through local and global contexts.","However, the transformer operations remain computationally heavy.","In this work, we introduce WaveUNet squeeze-excitation Res2 (WSR)-based metric generative adversarial network (WSR-MGAN) architecture that can be efficiently implemented on low-power edge devices for noise suppression tasks while maintaining speech quality.","We utilize multi-scale features using Res2Net blocks that can be related to spectral content used in speech-processing tasks.","In the generator, we integrate squeeze-excitation blocks (SEB) with multi-scale features for maintaining local and global contexts along with gated recurrent units (GRUs).","The proposed approach is optimized through a combined loss function calculated over raw waveform, multi-resolution magnitude spectrogram, and objective metrics using a metric discriminator.","Experimental results in terms of various objective metrics on VoiceBank+DEMAND and DNS-2020 challenge datasets demonstrate that the proposed speech enhancement (SE) approach outperforms the baselines and achieves state-of-the-art (SOTA) performance in the time domain."],"url":"http://arxiv.org/abs/2405.16834v1","category":"eess.AS"}
{"created":"2024-05-27 05:04:05","title":"Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models","abstract":"While large language models (LLMs) such as Llama-2 or GPT-4 have shown impressive zero-shot performance, fine-tuning is still necessary to enhance their performance for customized datasets, domain-specific tasks, or other private needs. However, fine-tuning all parameters of LLMs requires significant hardware resources, which can be impractical for typical users. Therefore, parameter-efficient fine-tuning such as LoRA have emerged, allowing users to fine-tune LLMs without the need for considerable computing resources, with little performance degradation compared to fine-tuning all parameters. Unfortunately, recent studies indicate that fine-tuning can increase the risk to the safety of LLMs, even when data does not contain malicious content. To address this challenge, we propose Safe LoRA, a simple one-liner patch to the original LoRA implementation by introducing the projection of LoRA weights from selected layers to the safety-aligned subspace, effectively reducing the safety risks in LLM fine-tuning while maintaining utility. It is worth noting that Safe LoRA is a training-free and data-free approach, as it only requires the knowledge of the weights from the base and aligned LLMs. Our extensive experiments demonstrate that when fine-tuning on purely malicious data, Safe LoRA retains similar safety performance as the original aligned model. Moreover, when the fine-tuning dataset contains a mixture of both benign and malicious data, Safe LoRA mitigates the negative effect made by malicious data while preserving performance on downstream tasks.","sentences":["While large language models (LLMs) such as Llama-2 or GPT-4 have shown impressive zero-shot performance, fine-tuning is still necessary to enhance their performance for customized datasets, domain-specific tasks, or other private needs.","However, fine-tuning all parameters of LLMs requires significant hardware resources, which can be impractical for typical users.","Therefore, parameter-efficient fine-tuning such as LoRA have emerged, allowing users to fine-tune LLMs without the need for considerable computing resources, with little performance degradation compared to fine-tuning all parameters.","Unfortunately, recent studies indicate that fine-tuning can increase the risk to the safety of LLMs, even when data does not contain malicious content.","To address this challenge, we propose Safe LoRA, a simple one-liner patch to the original LoRA implementation by introducing the projection of LoRA weights from selected layers to the safety-aligned subspace, effectively reducing the safety risks in LLM fine-tuning while maintaining utility.","It is worth noting that Safe LoRA is a training-free and data-free approach, as it only requires the knowledge of the weights from the base and aligned LLMs.","Our extensive experiments demonstrate that when fine-tuning on purely malicious data, Safe LoRA retains similar safety performance as the original aligned model.","Moreover, when the fine-tuning dataset contains a mixture of both benign and malicious data, Safe LoRA mitigates the negative effect made by malicious data while preserving performance on downstream tasks."],"url":"http://arxiv.org/abs/2405.16833v1","category":"cs.LG"}
{"created":"2024-05-27 04:43:44","title":"Vidu4D: Single Generated Video to High-Fidelity 4D Reconstruction with Dynamic Gaussian Surfels","abstract":"Video generative models are receiving particular attention given their ability to generate realistic and imaginative frames. Besides, these models are also observed to exhibit strong 3D consistency, significantly enhancing their potential to act as world simulators. In this work, we present Vidu4D, a novel reconstruction model that excels in accurately reconstructing 4D (i.e., sequential 3D) representations from single generated videos, addressing challenges associated with non-rigidity and frame distortion. This capability is pivotal for creating high-fidelity virtual contents that maintain both spatial and temporal coherence. At the core of Vidu4D is our proposed Dynamic Gaussian Surfels (DGS) technique. DGS optimizes time-varying warping functions to transform Gaussian surfels (surface elements) from a static state to a dynamically warped state. This transformation enables a precise depiction of motion and deformation over time. To preserve the structural integrity of surface-aligned Gaussian surfels, we design the warped-state geometric regularization based on continuous warping fields for estimating normals. Additionally, we learn refinements on rotation and scaling parameters of Gaussian surfels, which greatly alleviates texture flickering during the warping process and enhances the capture of fine-grained appearance details. Vidu4D also contains a novel initialization state that provides a proper start for the warping fields in DGS. Equipping Vidu4D with an existing video generative model, the overall framework demonstrates high-fidelity text-to-4D generation in both appearance and geometry.","sentences":["Video generative models are receiving particular attention given their ability to generate realistic and imaginative frames.","Besides, these models are also observed to exhibit strong 3D consistency, significantly enhancing their potential to act as world simulators.","In this work, we present Vidu4D, a novel reconstruction model that excels in accurately reconstructing 4D (i.e., sequential 3D) representations from single generated videos, addressing challenges associated with non-rigidity and frame distortion.","This capability is pivotal for creating high-fidelity virtual contents that maintain both spatial and temporal coherence.","At the core of Vidu4D is our proposed Dynamic Gaussian Surfels (DGS) technique.","DGS optimizes time-varying warping functions to transform Gaussian surfels (surface elements) from a static state to a dynamically warped state.","This transformation enables a precise depiction of motion and deformation over time.","To preserve the structural integrity of surface-aligned Gaussian surfels, we design the warped-state geometric regularization based on continuous warping fields for estimating normals.","Additionally, we learn refinements on rotation and scaling parameters of Gaussian surfels, which greatly alleviates texture flickering during the warping process and enhances the capture of fine-grained appearance details.","Vidu4D also contains a novel initialization state that provides a proper start for the warping fields in DGS.","Equipping Vidu4D with an existing video generative model, the overall framework demonstrates high-fidelity text-to-4D generation in both appearance and geometry."],"url":"http://arxiv.org/abs/2405.16822v1","category":"cs.CV"}
{"created":"2024-05-27 16:00:37","title":"Equidistribution of points in the Harmonic ensemble for the Wasserstein distance","abstract":"We study the asymptotics of the expected Wasserstein distance between the empirical measure of a Determinantal Point Process and the background volume form. The main DPP studied is the harmonic ensemble, where we get the optimal rate of convergence for homogeneous manifolds of dimension $d\\geq 3$, and for two-point homogeneous manifolds. We also discuss some variations of this process on the torus.","sentences":["We study the asymptotics of the expected Wasserstein distance between the empirical measure of a Determinantal Point Process and the background volume form.","The main DPP studied is the harmonic ensemble, where we get the optimal rate of convergence for homogeneous manifolds of dimension $d\\geq 3$, and for two-point homogeneous manifolds.","We also discuss some variations of this process on the torus."],"url":"http://arxiv.org/abs/2405.17298v1","category":"math.PR"}
{"created":"2024-05-27 15:31:37","title":"Which Electronic Structure Method to Choose in Trajectory Surface Hopping Dynamics Simulations? Azomethane as a Case Study","abstract":"Non-adiabatic dynamics simulations have become a standard approach to explore photochemical reactions. Such simulations require underlying potential energy surfaces and couplings between them, calculated at a chosen level of theory, yet this aspect is rarely assessed. Here, in combination with the popular trajectory surface hopping dynamics method, we use a high-accuracy XMS-CASPT2 electronic structure level as a benchmark for assessing the performances of various post-Hartree-Fock methods (namely CIS, ADC(2), CC2 and CASSCF) and exchange-correlation functionals (PBE, PBE0, CAM-B3LYP) in a TD-DFT/TDA context, using the isomerization around a double bond as test case. Different relaxation pathways are identified, and the ability of the different methods to reproduce their relative importance and timescale is discussed. The results show that multi-reference electronic structure methods should be preferred, when studying non-adiabatic decay between excited and ground states. If not affordable, TD-DFT with TDA and hybrid functionals, and ADC(2) are efficient alternative, but overestimate the non-radiative decay yield and thus may miss deexcitation pathways.","sentences":["Non-adiabatic dynamics simulations have become a standard approach to explore photochemical reactions.","Such simulations require underlying potential energy surfaces and couplings between them, calculated at a chosen level of theory, yet this aspect is rarely assessed.","Here, in combination with the popular trajectory surface hopping dynamics method, we use a high-accuracy XMS-CASPT2 electronic structure level as a benchmark for assessing the performances of various post-Hartree-Fock methods (namely CIS, ADC(2), CC2 and CASSCF) and exchange-correlation functionals (PBE, PBE0, CAM-B3LYP) in a TD-DFT/TDA context, using the isomerization around a double bond as test case.","Different relaxation pathways are identified, and the ability of the different methods to reproduce their relative importance and timescale is discussed.","The results show that multi-reference electronic structure methods should be preferred, when studying non-adiabatic decay between excited and ground states.","If not affordable, TD-DFT with TDA and hybrid functionals, and ADC(2) are efficient alternative, but overestimate the non-radiative decay yield and thus may miss deexcitation pathways."],"url":"http://arxiv.org/abs/2405.17271v1","category":"physics.chem-ph"}
{"created":"2024-05-27 15:31:13","title":"Engineered Josephson diode effect in kinked Rashba nanochannels","abstract":"The superconducting diode effect, reminiscent of the unidirectional charge transport in semiconductor diodes, is characterized by a nonreciprocal, dissipationless flow of Cooper pairs. This remarkable phenomenon arises from the interplay between symmetry constraints and the inherent quantum behavior of superconductors. Here, we explore the geometric control of the diode effect in a kinked nanostrip Josephson junction based on a two-dimensional electron gas (2DEGs) with Rashba spin-orbit interaction. We provide a comprehensive analysis of the diode effect as a function of the kink angle and the out-of-plane magnetic field. Our analysis reveals a rich phase diagram, showcasing a geometry and field-controlled diode effect. The phase diagram also reveals the presence of an anomalous Josephson effect related to the emergence of trivial zero-energy Andreev bound states, which can evolve into Majorana bound states. Our findings indicate that the exceptional synergy between geometric control of the diode effect and topological phases can be effectively leveraged to design and optimize superconducting devices with tailored transport properties.","sentences":["The superconducting diode effect, reminiscent of the unidirectional charge transport in semiconductor diodes, is characterized by a nonreciprocal, dissipationless flow of Cooper pairs.","This remarkable phenomenon arises from the interplay between symmetry constraints and the inherent quantum behavior of superconductors.","Here, we explore the geometric control of the diode effect in a kinked nanostrip Josephson junction based on a two-dimensional electron gas (2DEGs) with Rashba spin-orbit interaction.","We provide a comprehensive analysis of the diode effect as a function of the kink angle and the out-of-plane magnetic field.","Our analysis reveals a rich phase diagram, showcasing a geometry and field-controlled diode effect.","The phase diagram also reveals the presence of an anomalous Josephson effect related to the emergence of trivial zero-energy Andreev bound states, which can evolve into Majorana bound states.","Our findings indicate that the exceptional synergy between geometric control of the diode effect and topological phases can be effectively leveraged to design and optimize superconducting devices with tailored transport properties."],"url":"http://arxiv.org/abs/2405.17269v1","category":"cond-mat.supr-con"}
{"created":"2024-05-27 14:48:05","title":"Synergistic Dynamical Decoupling and Circuit Design for Enhanced Algorithm Performance on Near-Term Quantum Devices","abstract":"Dynamical decoupling (DD) is a promising technique for mitigating errors in near term quantum devices. However, its effectiveness depends on both hardware characteristics and algorithm implementation details. This paper explores the synergistic effects of dynamical decoupling and optimized circuit design in maximizing the performance and robustness of algorithms on near term quantum devices. By utilizing eight IBM quantum devices, we analyze how hardware features and algorithm design impact the effectiveness of DD for error mitigation. Our analysis takes into account factors such as circuit fidelity, scheduling duration, and hardware native gate set. We also examine the influence of algorithmic implementation details including specific gate decompositions, DD sequences, and optimization levels. The results reveal an inverse relationship between the effectiveness of DD and the inherent performance of the algorithm. Furthermore, we emphasize the importance of gate directionality and circuit symmetry in improving performance. This study offers valuable insights for optimizing DD protocols and circuit designs, highlighting the significance of a holistic approach that leverages both hardware features and algorithm design for high quality and reliable execution of near term quantum algorithms.","sentences":["Dynamical decoupling (DD) is a promising technique for mitigating errors in near term quantum devices.","However, its effectiveness depends on both hardware characteristics and algorithm implementation details.","This paper explores the synergistic effects of dynamical decoupling and optimized circuit design in maximizing the performance and robustness of algorithms on near term quantum devices.","By utilizing eight IBM quantum devices, we analyze how hardware features and algorithm design impact the effectiveness of DD for error mitigation.","Our analysis takes into account factors such as circuit fidelity, scheduling duration, and hardware native gate set.","We also examine the influence of algorithmic implementation details including specific gate decompositions, DD sequences, and optimization levels.","The results reveal an inverse relationship between the effectiveness of DD and the inherent performance of the algorithm.","Furthermore, we emphasize the importance of gate directionality and circuit symmetry in improving performance.","This study offers valuable insights for optimizing DD protocols and circuit designs, highlighting the significance of a holistic approach that leverages both hardware features and algorithm design for high quality and reliable execution of near term quantum algorithms."],"url":"http://arxiv.org/abs/2405.17230v1","category":"quant-ph"}
{"created":"2024-05-27 14:19:08","title":"Instability and Efficiency of Non-cooperative Games","abstract":"It is well known that a non-cooperative game may have multiple equilibria. In this paper we consider the efficiency of games, measured by the ratio between the aggregate payoff over all Nash equilibria and that over all admissible controls. Such efficiency operator is typically unstable with respect to small perturbation of the game. This seemingly bad property can actually be a good news in practice: it is possible that a small change of the game mechanism may improve the efficiency of the game dramatically. We shall introduce a game mediator with limited resources and investigate the mechanism designs aiming to improve the efficiency.","sentences":["It is well known that a non-cooperative game may have multiple equilibria.","In this paper we consider the efficiency of games, measured by the ratio between the aggregate payoff over all Nash equilibria and that over all admissible controls.","Such efficiency operator is typically unstable with respect to small perturbation of the game.","This seemingly bad property can actually be a good news in practice: it is possible that a small change of the game mechanism may improve the efficiency of the game dramatically.","We shall introduce a game mediator with limited resources and investigate the mechanism designs aiming to improve the efficiency."],"url":"http://arxiv.org/abs/2405.17196v1","category":"cs.GT"}
{"created":"2024-05-27 14:12:58","title":"Rebound in epidemic control: How misaligned vaccination timing amplifies infection peaks","abstract":"In this study, we explore the dynamic interplay between the timing of vaccination campaigns and the trajectory of disease spread in a population. Through comprehensive data analysis and modeling, we have uncovered a counter-intuitive phenomenon: initiating a vaccination process at an inopportune moment can paradoxically result in a more pronounced second peak of infections. This \"rebound\" phenomenon challenges the conventional understanding of vaccination impacts on epidemic dynamics. We provide a detailed examination of how improperly timed vaccination efforts can inadvertently reduce the overall immunity level in a population, considering both natural and vaccine-induced immunity. Our findings reveal that such a decrease in population-wide immunity can lead to a delayed, yet more severe, resurgence of cases. This study not only adds a critical dimension to our understanding of vaccination strategies in controlling pandemics but also underscores the necessity for strategically timed interventions to optimize public health outcomes. Furthermore, we compute which vaccination strategies are optimal for a COVID-19 tailored mathematical model, and find that there are two types of optimal strategies. The first type prioritizes vaccinating early and rapidly to reduce the number of deaths, while the second type acts later and more slowly to reduce the number of cases; both of them target primarily the elderly population. Our results hold significant implications for the formulation of vaccination policies, particularly in the context of rapidly evolving infectious diseases.","sentences":["In this study, we explore the dynamic interplay between the timing of vaccination campaigns and the trajectory of disease spread in a population.","Through comprehensive data analysis and modeling, we have uncovered a counter-intuitive phenomenon: initiating a vaccination process at an inopportune moment can paradoxically result in a more pronounced second peak of infections.","This \"rebound\" phenomenon challenges the conventional understanding of vaccination impacts on epidemic dynamics.","We provide a detailed examination of how improperly timed vaccination efforts can inadvertently reduce the overall immunity level in a population, considering both natural and vaccine-induced immunity.","Our findings reveal that such a decrease in population-wide immunity can lead to a delayed, yet more severe, resurgence of cases.","This study not only adds a critical dimension to our understanding of vaccination strategies in controlling pandemics but also underscores the necessity for strategically timed interventions to optimize public health outcomes.","Furthermore, we compute which vaccination strategies are optimal for a COVID-19 tailored mathematical model, and find that there are two types of optimal strategies.","The first type prioritizes vaccinating early and rapidly to reduce the number of deaths, while the second type acts later and more slowly to reduce the number of cases; both of them target primarily the elderly population.","Our results hold significant implications for the formulation of vaccination policies, particularly in the context of rapidly evolving infectious diseases."],"url":"http://arxiv.org/abs/2405.17189v1","category":"physics.soc-ph"}
{"created":"2024-05-27 12:07:21","title":"Positivity preserving finite element method for the Gross-Pitaevskii ground state: discrete uniqueness and global convergence","abstract":"We propose a positivity preserving finite element discretization for the nonlinear Gross-Pitaevskii eigenvalue problem. The method employs mass lumping techniques, which allow to transfer the uniqueness up to sign and positivity properties of the continuous ground state to the discrete setting. We further prove that every non-negative discrete excited state up to sign coincides with the discrete ground state. This allows one to identify the limit of fully discretized gradient flows, which are typically used to compute the discrete ground state, and thereby establish their global convergence. Furthermore, we perform a rigorous a priori error analysis of the proposed non-standard finite element discretization, showing optimal orders of convergence for all unknowns. Numerical experiments illustrate the theoretical results of this paper.","sentences":["We propose a positivity preserving finite element discretization for the nonlinear Gross-Pitaevskii eigenvalue problem.","The method employs mass lumping techniques, which allow to transfer the uniqueness up to sign and positivity properties of the continuous ground state to the discrete setting.","We further prove that every non-negative discrete excited state up to sign coincides with the discrete ground state.","This allows one to identify the limit of fully discretized gradient flows, which are typically used to compute the discrete ground state, and thereby establish their global convergence.","Furthermore, we perform a rigorous a priori error analysis of the proposed non-standard finite element discretization, showing optimal orders of convergence for all unknowns.","Numerical experiments illustrate the theoretical results of this paper."],"url":"http://arxiv.org/abs/2405.17090v1","category":"math.NA"}
{"created":"2024-05-27 09:50:43","title":"Developments in tropical convexity","abstract":"The term \"tropical convexity\" was coined by Develin and Sturmfels who published a landmark paper with that title in 2004. However, the topic has much older roots and is deeply connected to linear and combinatorial optimization and other areas of mathematics. The purpose of this survey is to sketch how that article contributed to shaping the field of tropical geometry as we know it today.","sentences":["The term \"tropical convexity\" was coined by Develin and Sturmfels who published a landmark paper with that title in 2004.","However, the topic has much older roots and is deeply connected to linear and combinatorial optimization and other areas of mathematics.","The purpose of this survey is to sketch how that article contributed to shaping the field of tropical geometry as we know it today."],"url":"http://arxiv.org/abs/2405.17005v1","category":"math.CO"}
{"created":"2024-05-27 09:31:15","title":"Optimal error bounds for the two point flux approximation finite volume scheme","abstract":"We consider a finite volume scheme with two-point flux approximation (TPFA) to approximate a Laplace problem when the solution exhibits no more regularity than belonging to $H^1_0(\\Omega)$. We establish in this case some error bounds for both the solution and the approximation of the gradient component orthogonal to the mesh faces. This estimate is optimal, in the sense that the approximation error has the same order as that of the sum of the interpolation error and a conformity error. A numerical example illustrates the error estimate in the context of a solution with minimal regularity. This result is extended to evolution problems discretized via the implicit Euler scheme in an appendix.","sentences":["We consider a finite volume scheme with two-point flux approximation (TPFA) to approximate a Laplace problem when the solution exhibits no more regularity than belonging to $","H^1_0(\\Omega)$. We establish in this case some error bounds for both the solution and the approximation of the gradient component orthogonal to the mesh faces.","This estimate is optimal, in the sense that the approximation error has the same order as that of the sum of the interpolation error and a conformity error.","A numerical example illustrates the error estimate in the context of a solution with minimal regularity.","This result is extended to evolution problems discretized via the implicit Euler scheme in an appendix."],"url":"http://arxiv.org/abs/2405.16985v1","category":"math.NA"}
{"created":"2024-05-27 08:31:56","title":"Energy extraction via magnetic reconnection in magnetized black holes","abstract":"The Comisso-Asenjo mechanism is a novel mechanism proposed recently to extract energy from black holes through magnetic reconnection of the surrounding charged plasma, in which the magnetic field plays a crucial role. In this work, we revisit this process by taking into account the backreaction of the magnetic field on the black hole's geometry. We employ the Kerr-Melvin metric to describe the local near-horizon geometry of the magnetized black hole. By analyzing the circular orbits in the equatorial plane, the energy extraction conditions, the power and efficiency of the energy extraction, we found that while a stronger magnetic field can enhance plasma magnetization and aid energy extraction, its backreaction on the spacetime may hinder the process, with a larger magnetic field posing a greater obstacle. Balancing these effects, an optimal moderate magnetic field strength is found to be most conducive for energy extraction. Moreover, there is a maximum limit to the magnetic field strength associated with the black hole's spin, beyond which circular orbits in the equatorial plane are prohibited, thereby impeding energy extraction in the current scenario.","sentences":["The Comisso-Asenjo mechanism is a novel mechanism proposed recently to extract energy from black holes through magnetic reconnection of the surrounding charged plasma, in which the magnetic field plays a crucial role.","In this work, we revisit this process by taking into account the backreaction of the magnetic field on the black hole's geometry.","We employ the Kerr-Melvin metric to describe the local near-horizon geometry of the magnetized black hole.","By analyzing the circular orbits in the equatorial plane, the energy extraction conditions, the power and efficiency of the energy extraction, we found that while a stronger magnetic field can enhance plasma magnetization and aid energy extraction, its backreaction on the spacetime may hinder the process, with a larger magnetic field posing a greater obstacle.","Balancing these effects, an optimal moderate magnetic field strength is found to be most conducive for energy extraction.","Moreover, there is a maximum limit to the magnetic field strength associated with the black hole's spin, beyond which circular orbits in the equatorial plane are prohibited, thereby impeding energy extraction in the current scenario."],"url":"http://arxiv.org/abs/2405.16941v1","category":"gr-qc"}
{"created":"2024-05-27 08:18:41","title":"OED: Towards One-stage End-to-End Dynamic Scene Graph Generation","abstract":"Dynamic Scene Graph Generation (DSGG) focuses on identifying visual relationships within the spatial-temporal domain of videos. Conventional approaches often employ multi-stage pipelines, which typically consist of object detection, temporal association, and multi-relation classification. However, these methods exhibit inherent limitations due to the separation of multiple stages, and independent optimization of these sub-problems may yield sub-optimal solutions. To remedy these limitations, we propose a one-stage end-to-end framework, termed OED, which streamlines the DSGG pipeline. This framework reformulates the task as a set prediction problem and leverages pair-wise features to represent each subject-object pair within the scene graph. Moreover, another challenge of DSGG is capturing temporal dependencies, we introduce a Progressively Refined Module (PRM) for aggregating temporal context without the constraints of additional trackers or handcrafted trajectories, enabling end-to-end optimization of the network. Extensive experiments conducted on the Action Genome benchmark demonstrate the effectiveness of our design. The code and models are available at \\url{https://github.com/guanw-pku/OED}.","sentences":["Dynamic Scene Graph Generation (DSGG) focuses on identifying visual relationships within the spatial-temporal domain of videos.","Conventional approaches often employ multi-stage pipelines, which typically consist of object detection, temporal association, and multi-relation classification.","However, these methods exhibit inherent limitations due to the separation of multiple stages, and independent optimization of these sub-problems may yield sub-optimal solutions.","To remedy these limitations, we propose a one-stage end-to-end framework, termed OED, which streamlines the DSGG pipeline.","This framework reformulates the task as a set prediction problem and leverages pair-wise features to represent each subject-object pair within the scene graph.","Moreover, another challenge of DSGG is capturing temporal dependencies, we introduce a Progressively Refined Module (PRM) for aggregating temporal context without the constraints of additional trackers or handcrafted trajectories, enabling end-to-end optimization of the network.","Extensive experiments conducted on the Action Genome benchmark demonstrate the effectiveness of our design.","The code and models are available at \\url{https://github.com/guanw-pku/OED}."],"url":"http://arxiv.org/abs/2405.16925v1","category":"cs.CV"}
{"created":"2024-05-27 05:49:12","title":"Sync4D: Video Guided Controllable Dynamics for Physics-Based 4D Generation","abstract":"In this work, we introduce a novel approach for creating controllable dynamics in 3D-generated Gaussians using casually captured reference videos. Our method transfers the motion of objects from reference videos to a variety of generated 3D Gaussians across different categories, ensuring precise and customizable motion transfer. We achieve this by employing blend skinning-based non-parametric shape reconstruction to extract the shape and motion of reference objects. This process involves segmenting the reference objects into motion-related parts based on skinning weights and establishing shape correspondences with generated target shapes. To address shape and temporal inconsistencies prevalent in existing methods, we integrate physical simulation, driving the target shapes with matched motion. This integration is optimized through a displacement loss to ensure reliable and genuine dynamics. Our approach supports diverse reference inputs, including humans, quadrupeds, and articulated objects, and can generate dynamics of arbitrary length, providing enhanced fidelity and applicability. Unlike methods heavily reliant on diffusion video generation models, our technique offers specific and high-quality motion transfer, maintaining both shape integrity and temporal consistency.","sentences":["In this work, we introduce a novel approach for creating controllable dynamics in 3D-generated Gaussians using casually captured reference videos.","Our method transfers the motion of objects from reference videos to a variety of generated 3D Gaussians across different categories, ensuring precise and customizable motion transfer.","We achieve this by employing blend skinning-based non-parametric shape reconstruction to extract the shape and motion of reference objects.","This process involves segmenting the reference objects into motion-related parts based on skinning weights and establishing shape correspondences with generated target shapes.","To address shape and temporal inconsistencies prevalent in existing methods, we integrate physical simulation, driving the target shapes with matched motion.","This integration is optimized through a displacement loss to ensure reliable and genuine dynamics.","Our approach supports diverse reference inputs, including humans, quadrupeds, and articulated objects, and can generate dynamics of arbitrary length, providing enhanced fidelity and applicability.","Unlike methods heavily reliant on diffusion video generation models, our technique offers specific and high-quality motion transfer, maintaining both shape integrity and temporal consistency."],"url":"http://arxiv.org/abs/2405.16849v1","category":"cs.CV"}
{"created":"2024-05-27 04:20:08","title":"Characterization and Novel Application of Power Over Fiber for Electronics in a Harsh Environment","abstract":"Power-over-Fiber (PoF) technology has been used extensively in settings where high voltages require isolation from ground. In a novel application of PoF, power is provided to photon detector modules located on a surface at $\\sim$ 300 kV with respect to ground in the planned DUNE experiment. In cryogenic environments, PoF offers a reliable means of power transmission, leveraging optical fibers to transfer power with minimal system degradation. PoF technology excels in maintaining low noise levels when delivering power to sensitive electronic systems operating in extreme temperatures and high voltage environments. This paper presents the R$\\&$D effort of PoF in extreme conditions and underscores its capacity to revolutionize power delivery and management in critical applications, offering a dependable solution with low noise, optimal efficiency, and superior isolation.","sentences":["Power-over-Fiber (PoF) technology has been used extensively in settings where high voltages require isolation from ground.","In a novel application of PoF, power is provided to photon detector modules located on a surface at $\\sim$ 300 kV with respect to ground in the planned DUNE experiment.","In cryogenic environments, PoF offers a reliable means of power transmission, leveraging optical fibers to transfer power with minimal system degradation.","PoF technology excels in maintaining low noise levels when delivering power to sensitive electronic systems operating in extreme temperatures and high voltage environments.","This paper presents the R$\\&$D effort of PoF in extreme conditions and underscores its capacity to revolutionize power delivery and management in critical applications, offering a dependable solution with low noise, optimal efficiency, and superior isolation."],"url":"http://arxiv.org/abs/2405.16816v1","category":"physics.ins-det"}
{"created":"2024-05-27 03:35:50","title":"Exploring Fairness in Educational Data Mining in the Context of the Right to be Forgotten","abstract":"In education data mining (EDM) communities, machine learning has achieved remarkable success in discovering patterns and structures to tackle educational challenges. Notably, fairness and algorithmic bias have gained attention in learning analytics of EDM. With the increasing demand for the right to be forgotten, there is a growing need for machine learning models to forget sensitive data and its impact, particularly within the realm of EDM. The paradigm of selective forgetting, also known as machine unlearning, has been extensively studied to address this need by eliminating the influence of specific data from a pre-trained model without complete retraining. However, existing research assumes that interactive data removal operations are conducted in secure and reliable environments, neglecting potential malicious unlearning requests to undermine the fairness of machine learning systems. In this paper, we introduce a novel class of selective forgetting attacks designed to compromise the fairness of learning models while maintaining their predictive accuracy, thereby preventing the model owner from detecting the degradation in model performance. Additionally, we propose an innovative optimization framework for selective forgetting attacks, capable of generating malicious unlearning requests across various attack scenarios. We validate the effectiveness of our proposed selective forgetting attacks on fairness through extensive experiments using diverse EDM datasets.","sentences":["In education data mining (EDM) communities, machine learning has achieved remarkable success in discovering patterns and structures to tackle educational challenges.","Notably, fairness and algorithmic bias have gained attention in learning analytics of EDM.","With the increasing demand for the right to be forgotten, there is a growing need for machine learning models to forget sensitive data and its impact, particularly within the realm of EDM.","The paradigm of selective forgetting, also known as machine unlearning, has been extensively studied to address this need by eliminating the influence of specific data from a pre-trained model without complete retraining.","However, existing research assumes that interactive data removal operations are conducted in secure and reliable environments, neglecting potential malicious unlearning requests to undermine the fairness of machine learning systems.","In this paper, we introduce a novel class of selective forgetting attacks designed to compromise the fairness of learning models while maintaining their predictive accuracy, thereby preventing the model owner from detecting the degradation in model performance.","Additionally, we propose an innovative optimization framework for selective forgetting attacks, capable of generating malicious unlearning requests across various attack scenarios.","We validate the effectiveness of our proposed selective forgetting attacks on fairness through extensive experiments using diverse EDM datasets."],"url":"http://arxiv.org/abs/2405.16798v1","category":"cs.LG"}
{"created":"2024-05-27 00:56:21","title":"Limited-perception games","abstract":"We study rational agents with different perception capabilities in strategic games. We focus on a class of one-shot limited-perception games. These games extend simultaneous-move normal-form games by presenting each player with an individualized perception of all players' payoff functions. The accuracy of a player's perception is determined by the player's capability level. Capability levels are countable and totally ordered, with a higher level corresponding to a more accurate perception. We study the rational behavior of players in these games and formalize relevant equilibria conditions. In contrast to equilibria in conventional bimatrix games, which can be represented by a pair of mixed strategies, in our limited perception games a higher-order response function captures how the lower-capability player uses their (less accurate) perception of the payoff function to reason about the (more accurate) possible perceptions of the higher-capability opponent. This response function characterizes, for each possible perception of the higher-capability player (from the perspective of the lower-capability player), the best response of the higher capability player for that perception. Since the domain of the response function can be exponentially large or even infinite, finding one equilibrium may be computationally intractable or even undecidable. Nevertheless, we show that for any $\\epsilon$, there exists an $\\epsilon$-equilibrium with a compact, tractable representation whose size is independent of the size of the response function's domain. We further identify classes of zero-sum limited-perception games in which finding an equilibrium becomes a (typically tractable) nonsmooth convex optimization problem.","sentences":["We study rational agents with different perception capabilities in strategic games.","We focus on a class of one-shot limited-perception games.","These games extend simultaneous-move normal-form games by presenting each player with an individualized perception of all players' payoff functions.","The accuracy of a player's perception is determined by the player's capability level.","Capability levels are countable and totally ordered, with a higher level corresponding to a more accurate perception.","We study the rational behavior of players in these games and formalize relevant equilibria conditions.","In contrast to equilibria in conventional bimatrix games, which can be represented by a pair of mixed strategies, in our limited perception games a higher-order response function captures how the lower-capability player uses their (less accurate) perception of the payoff function to reason about the (more accurate) possible perceptions of the higher-capability opponent.","This response function characterizes, for each possible perception of the higher-capability player (from the perspective of the lower-capability player), the best response of the higher capability player for that perception.","Since the domain of the response function can be exponentially large or even infinite, finding one equilibrium may be computationally intractable or even undecidable.","Nevertheless, we show that for any $\\epsilon$, there exists an $\\epsilon$-equilibrium with a compact, tractable representation whose size is independent of the size of the response function's domain.","We further identify classes of zero-sum limited-perception games in which finding an equilibrium becomes a (typically tractable) nonsmooth convex optimization problem."],"url":"http://arxiv.org/abs/2405.16735v1","category":"cs.GT"}
{"created":"2024-05-27 00:53:18","title":"Faster Sampling via Stochastic Gradient Proximal Sampler","abstract":"Stochastic gradients have been widely integrated into Langevin-based methods to improve their scalability and efficiency in solving large-scale sampling problems. However, the proximal sampler, which exhibits much faster convergence than Langevin-based algorithms in the deterministic setting Lee et al. (2021), has yet to be explored in its stochastic variants. In this paper, we study the Stochastic Proximal Samplers (SPS) for sampling from non-log-concave distributions. We first establish a general framework for implementing stochastic proximal samplers and establish the convergence theory accordingly. We show that the convergence to the target distribution can be guaranteed as long as the second moment of the algorithm trajectory is bounded and restricted Gaussian oracles can be well approximated. We then provide two implementable variants based on Stochastic gradient Langevin dynamics (SGLD) and Metropolis-adjusted Langevin algorithm (MALA), giving rise to SPS-SGLD and SPS-MALA. We further show that SPS-SGLD and SPS-MALA can achieve $\\epsilon$-sampling error in total variation (TV) distance within $\\tilde{\\mathcal{O}}(d\\epsilon^{-2})$ and $\\tilde{\\mathcal{O}}(d^{1/2}\\epsilon^{-2})$ gradient complexities, which outperform the best-known result by at least an $\\tilde{\\mathcal{O}}(d^{1/3})$ factor. This enhancement in performance is corroborated by our empirical studies on synthetic data with various dimensions, demonstrating the efficiency of our proposed algorithm.","sentences":["Stochastic gradients have been widely integrated into Langevin-based methods to improve their scalability and efficiency in solving large-scale sampling problems.","However, the proximal sampler, which exhibits much faster convergence than Langevin-based algorithms in the deterministic setting Lee et al. (2021), has yet to be explored in its stochastic variants.","In this paper, we study the Stochastic Proximal Samplers (SPS) for sampling from non-log-concave distributions.","We first establish a general framework for implementing stochastic proximal samplers and establish the convergence theory accordingly.","We show that the convergence to the target distribution can be guaranteed as long as the second moment of the algorithm trajectory is bounded and restricted Gaussian oracles can be well approximated.","We then provide two implementable variants based on Stochastic gradient Langevin dynamics (SGLD) and Metropolis-adjusted Langevin algorithm (MALA), giving rise to SPS-SGLD and SPS-MALA.","We further show that SPS-SGLD and SPS-MALA can achieve $\\epsilon$-sampling error in total variation (TV) distance within $\\tilde{\\mathcal{O}}(d\\epsilon^{-2})$ and $\\tilde{\\mathcal{O}}(d^{1/2}\\epsilon^{-2})$ gradient complexities, which outperform the best-known result by at least an $\\tilde{\\mathcal{O}}(d^{1/3})$ factor.","This enhancement in performance is corroborated by our empirical studies on synthetic data with various dimensions, demonstrating the efficiency of our proposed algorithm."],"url":"http://arxiv.org/abs/2405.16734v1","category":"stat.ML"}
{"created":"2024-05-26 21:12:34","title":"CNN Autoencoder Resizer: A Power-Efficient LoS/NLoS Detector in MIMO-enabled UAV Networks","abstract":"Optimizing the design, performance, and resource efficiency of wireless networks (WNs) necessitates the ability to discern Line of Sight (LoS) and Non-Line of Sight (NLoS) scenarios across diverse applications and environments. Unmanned Aerial Vehicles (UAVs) exhibit significant potential in this regard due to their rapid mobility, aerial capabilities, and payload characteristics. Particularly, UAVs can serve as vital non-terrestrial base stations (NTBS) in the event of terrestrial base station (TBS) failures or downtime. In this paper, we propose CNN autoencoder resizer (CAR) as a framework that improves the accuracy of LoS/NLoS detection without demanding extra power consumption. Our proposed method increases the mean accuracy of detecting LoS/NLoS signals from 66% to 86%, while maintaining consistent power consumption levels. In addition, the resolution provided by CAR shows that it can be employed as a preprocessing tool in other methods to enhance the quality of signals.","sentences":["Optimizing the design, performance, and resource efficiency of wireless networks (WNs) necessitates the ability to discern Line of Sight (LoS) and Non-Line of Sight (NLoS) scenarios across diverse applications and environments.","Unmanned Aerial Vehicles (UAVs) exhibit significant potential in this regard due to their rapid mobility, aerial capabilities, and payload characteristics.","Particularly, UAVs can serve as vital non-terrestrial base stations (NTBS) in the event of terrestrial base station (TBS) failures or downtime.","In this paper, we propose CNN autoencoder resizer (CAR) as a framework that improves the accuracy of LoS/NLoS detection without demanding extra power consumption.","Our proposed method increases the mean accuracy of detecting LoS/NLoS signals from 66% to 86%, while maintaining consistent power consumption levels.","In addition, the resolution provided by CAR shows that it can be employed as a preprocessing tool in other methods to enhance the quality of signals."],"url":"http://arxiv.org/abs/2405.16697v1","category":"cs.LG"}
{"created":"2024-05-26 20:59:24","title":"Aperture Selection for CAP Arrays (CAPAs)","abstract":"The concept of aperture selection is proposed for continuous aperture array (CAPA)-based communications. The achieved performance is analyzed in an uplink scenario by considering both line-of-sight (LoS) and non-line-of-sight (NLoS) scenarios. In the LoS scenario, the optimal selection strategy is demonstrated to follow the nearest neighbor criterion, and the resulting signal-to-noise ratio (SNR) is analyzed. In the NLoS scenario, the achieved outage probability along with the diversity order is revealed. Numerical results are provided to demonstrate that aperture selection effectively maintains satisfactory performance by leveraging selection diversity while simultaneously reducing the implementation complexity of CAPAs.","sentences":["The concept of aperture selection is proposed for continuous aperture array (CAPA)-based communications.","The achieved performance is analyzed in an uplink scenario by considering both line-of-sight (LoS) and non-line-of-sight (NLoS) scenarios.","In the LoS scenario, the optimal selection strategy is demonstrated to follow the nearest neighbor criterion, and the resulting signal-to-noise ratio (SNR) is analyzed.","In the NLoS scenario, the achieved outage probability along with the diversity order is revealed.","Numerical results are provided to demonstrate that aperture selection effectively maintains satisfactory performance by leveraging selection diversity while simultaneously reducing the implementation complexity of CAPAs."],"url":"http://arxiv.org/abs/2405.16694v1","category":"eess.SP"}
{"created":"2024-05-26 20:39:06","title":"EdgeSphere: A Three-Tier Architecture for Cognitive Edge Computing","abstract":"Computing at the edge is increasingly important as Internet of Things (IoT) devices at the edge generate massive amounts of data and pose challenges in transporting all that data to the Cloud where they can be analyzed. On the other hand, harnessing the edge data is essential for offering cognitive applications, if the challenges, such as device capabilities, connectivity, and heterogeneity can be overcome. This paper proposes a novel three-tier architecture, called EdgeSphere, which harnesses resources of the edge devices, to analyze the data in situ at the edge. In contrast to the state-of-the-art cloud and mobile applications, EdgeSphere applications span across cloud, edge gateways, and edge devices. At its core, EdgeSphere builds on Apache Mesos to optimize resources usage and scheduling. EdgeSphere has been applied to practical scenarios and this paper describes the engineering challenges faced as well as innovative solutions.","sentences":["Computing at the edge is increasingly important as Internet of Things (IoT) devices at the edge generate massive amounts of data and pose challenges in transporting all that data to the Cloud where they can be analyzed.","On the other hand, harnessing the edge data is essential for offering cognitive applications, if the challenges, such as device capabilities, connectivity, and heterogeneity can be overcome.","This paper proposes a novel three-tier architecture, called EdgeSphere, which harnesses resources of the edge devices, to analyze the data in situ at the edge.","In contrast to the state-of-the-art cloud and mobile applications, EdgeSphere applications span across cloud, edge gateways, and edge devices.","At its core, EdgeSphere builds on Apache Mesos to optimize resources usage and scheduling.","EdgeSphere has been applied to practical scenarios and this paper describes the engineering challenges faced as well as innovative solutions."],"url":"http://arxiv.org/abs/2405.16685v1","category":"cs.DC"}
{"created":"2024-05-26 20:18:11","title":"Triple Preference Optimization: Achieving Better Alignment with Less Data in a Single Step Optimization","abstract":"Large Language Models (LLMs) perform well across diverse tasks, but aligning them with human demonstrations is challenging. Recently, Reinforcement Learning (RL)-free methods like Direct Preference Optimization (DPO) have emerged, offering improved stability and scalability while retaining competitive performance relative to RL-based methods. However, while RL-free methods deliver satisfactory performance, they require significant data to develop a robust Supervised Fine-Tuned (SFT) model and an additional step to fine-tune this model on a preference dataset, which constrains their utility and scalability. In this paper, we introduce Triple Preference Optimization (TPO), a new preference learning method designed to align an LLM with three preferences without requiring a separate SFT step and using considerably less data. Through a combination of practical experiments and theoretical analysis, we show the efficacy of TPO as a single-step alignment strategy. Specifically, we fine-tuned the Phi-2 (2.7B) and Mistral (7B) models using TPO directly on the UltraFeedback dataset, achieving superior results compared to models aligned through other methods such as SFT, DPO, KTO, IPO, CPO, and ORPO. Moreover, the performance of TPO without the SFT component led to notable improvements in the MT-Bench score, with increases of +1.27 and +0.63 over SFT and DPO, respectively. Additionally, TPO showed higher average accuracy, surpassing DPO and SFT by 4.2% and 4.97% on the Open LLM Leaderboard benchmarks. Our code is publicly available at https://github.com/sahsaeedi/triple-preference-optimization .","sentences":["Large Language Models (LLMs) perform well across diverse tasks, but aligning them with human demonstrations is challenging.","Recently, Reinforcement Learning (RL)-free methods like Direct Preference Optimization (DPO) have emerged, offering improved stability and scalability while retaining competitive performance relative to RL-based methods.","However, while RL-free methods deliver satisfactory performance, they require significant data to develop a robust Supervised Fine-Tuned (SFT) model and an additional step to fine-tune this model on a preference dataset, which constrains their utility and scalability.","In this paper, we introduce Triple Preference Optimization (TPO), a new preference learning method designed to align an LLM with three preferences without requiring a separate SFT step and using considerably less data.","Through a combination of practical experiments and theoretical analysis, we show the efficacy of TPO as a single-step alignment strategy.","Specifically, we fine-tuned the Phi-2 (2.7B) and Mistral (7B) models using TPO directly on the UltraFeedback dataset, achieving superior results compared to models aligned through other methods such as SFT, DPO, KTO, IPO, CPO, and ORPO.","Moreover, the performance of TPO without the SFT component led to notable improvements in the MT-Bench score, with increases of +1.27 and +0.63 over SFT and DPO, respectively.","Additionally, TPO showed higher average accuracy, surpassing DPO and SFT by 4.2% and 4.97% on the Open LLM Leaderboard benchmarks.","Our code is publicly available at https://github.com/sahsaeedi/triple-preference-optimization ."],"url":"http://arxiv.org/abs/2405.16681v1","category":"cs.CL"}
{"created":"2024-05-26 20:14:37","title":"Six-Degree-of-Freedom Aircraft Landing Trajectory Planning with Runway Alignment","abstract":"This paper presents a numerical optimization algorithm for generating approach and landing trajectories for a six-degree-of-freedom (6-DoF) aircraft. We improve on the existing research on aircraft landing trajectory generation by formulating the trajectory optimization problem with additional real-world operational constraints, including 6-DoF aircraft dynamics, runway alignment, constant wind field, and obstacle avoidance, to obtain a continuous-time nonconvex optimal control problem. Particularly, the runway alignment constraint enforces the trajectory of the aircraft to be aligned with the runway only during the final approach phase. This is a novel feature that is essential for preventing an approach that is either too steep or too shallow. The proposed method models the runway alignment constraint through a multi-phase trajectory planning scheme, imposing alignment conditions exclusively during the final approach phase. We compare this formulation with the existing state-triggered constraint formulation for runway alignment. To solve the formulated problem, we design a novel sequential convex programming algorithm called xPTR that extends the penalized trust-region (PTR) algorithm by incorporating an extrapolation step to expedite convergence. We validate the proposed method through extensive numerical simulations, including a Monte Carlo study, to evaluate the robustness of the algorithm to varying initial conditions.","sentences":["This paper presents a numerical optimization algorithm for generating approach and landing trajectories for a six-degree-of-freedom (6-DoF) aircraft.","We improve on the existing research on aircraft landing trajectory generation by formulating the trajectory optimization problem with additional real-world operational constraints, including 6-DoF aircraft dynamics, runway alignment, constant wind field, and obstacle avoidance, to obtain a continuous-time nonconvex optimal control problem.","Particularly, the runway alignment constraint enforces the trajectory of the aircraft to be aligned with the runway only during the final approach phase.","This is a novel feature that is essential for preventing an approach that is either too steep or too shallow.","The proposed method models the runway alignment constraint through a multi-phase trajectory planning scheme, imposing alignment conditions exclusively during the final approach phase.","We compare this formulation with the existing state-triggered constraint formulation for runway alignment.","To solve the formulated problem, we design a novel sequential convex programming algorithm called xPTR that extends the penalized trust-region (PTR) algorithm by incorporating an extrapolation step to expedite convergence.","We validate the proposed method through extensive numerical simulations, including a Monte Carlo study, to evaluate the robustness of the algorithm to varying initial conditions."],"url":"http://arxiv.org/abs/2405.16680v1","category":"math.OC"}
{"created":"2024-05-26 18:42:00","title":"RAPF: Efficient path planning for lunar microrovers","abstract":"Efficient path planning is key for safe autonomous navigation over complex and unknown terrains. Lunar Zebro (LZ), a project of the Delft University of Technology, aims to deploy a compact rover, no larger than an A4 sheet of paper and weighing not more than 3 kilograms. In this work, we introduce a Robust Artificial Potential Field (RAPF) algorithm, a new path-planning algorithm for reliable local navigation solution for lunar microrovers. RAPF leverages and improves state of the art Artificial Potential Field (APF)-based methods by incorporating the position of the robot in the generation of bacteria points and considering local minima as regions to avoid. We perform both simulations and on field experiments to validate the performance of RAPF, which outperforms state-of-the-art APF-based algorithms by over 15% in reachability within a similar or shorter planning time. The improvements resulted in a 200% higher success rate and 50% lower computing time compared to the conventional APF algorithm. Near-optimal paths are computed in real-time with limited available processing power. The bacterial approach of the RAPF algorithm proves faster to execute and smaller to store than path planning algorithms used in existing planetary rovers, showcasing its potential for reliable lunar exploration with computationally constrained and energy constrained robotic systems.","sentences":["Efficient path planning is key for safe autonomous navigation over complex and unknown terrains.","Lunar Zebro (LZ), a project of the Delft University of Technology, aims to deploy a compact rover, no larger than an A4 sheet of paper and weighing not more than 3 kilograms.","In this work, we introduce a Robust Artificial Potential Field (RAPF) algorithm, a new path-planning algorithm for reliable local navigation solution for lunar microrovers.","RAPF leverages and improves state of the art Artificial Potential Field (APF)-based methods by incorporating the position of the robot in the generation of bacteria points and considering local minima as regions to avoid.","We perform both simulations and on field experiments to validate the performance of RAPF, which outperforms state-of-the-art APF-based algorithms by over 15% in reachability within a similar or shorter planning time.","The improvements resulted in a 200% higher success rate and 50% lower computing time compared to the conventional APF algorithm.","Near-optimal paths are computed in real-time with limited available processing power.","The bacterial approach of the RAPF algorithm proves faster to execute and smaller to store than path planning algorithms used in existing planetary rovers, showcasing its potential for reliable lunar exploration with computationally constrained and energy constrained robotic systems."],"url":"http://arxiv.org/abs/2405.16659v1","category":"cs.RO"}
{"created":"2024-05-26 17:47:34","title":"Diffusion4D: Fast Spatial-temporal Consistent 4D Generation via Video Diffusion Models","abstract":"The availability of large-scale multimodal datasets and advancements in diffusion models have significantly accelerated progress in 4D content generation. Most prior approaches rely on multiple image or video diffusion models, utilizing score distillation sampling for optimization or generating pseudo novel views for direct supervision. However, these methods are hindered by slow optimization speeds and multi-view inconsistency issues. Spatial and temporal consistency in 4D geometry has been extensively explored respectively in 3D-aware diffusion models and traditional monocular video diffusion models. Building on this foundation, we propose a strategy to migrate the temporal consistency in video diffusion models to the spatial-temporal consistency required for 4D generation. Specifically, we present a novel framework, \\textbf{Diffusion4D}, for efficient and scalable 4D content generation. Leveraging a meticulously curated dynamic 3D dataset, we develop a 4D-aware video diffusion model capable of synthesizing orbital views of dynamic 3D assets. To control the dynamic strength of these assets, we introduce a 3D-to-4D motion magnitude metric as guidance. Additionally, we propose a novel motion magnitude reconstruction loss and 3D-aware classifier-free guidance to refine the learning and generation of motion dynamics. After obtaining orbital views of the 4D asset, we perform explicit 4D construction with Gaussian splatting in a coarse-to-fine manner. The synthesized multi-view consistent 4D image set enables us to swiftly generate high-fidelity and diverse 4D assets within just several minutes. Extensive experiments demonstrate that our method surpasses prior state-of-the-art techniques in terms of generation efficiency and 4D geometry consistency across various prompt modalities.","sentences":["The availability of large-scale multimodal datasets and advancements in diffusion models have significantly accelerated progress in 4D content generation.","Most prior approaches rely on multiple image or video diffusion models, utilizing score distillation sampling for optimization or generating pseudo novel views for direct supervision.","However, these methods are hindered by slow optimization speeds and multi-view inconsistency issues.","Spatial and temporal consistency in 4D geometry has been extensively explored respectively in 3D-aware diffusion models and traditional monocular video diffusion models.","Building on this foundation, we propose a strategy to migrate the temporal consistency in video diffusion models to the spatial-temporal consistency required for 4D generation.","Specifically, we present a novel framework, \\textbf{Diffusion4D}, for efficient and scalable 4D content generation.","Leveraging a meticulously curated dynamic 3D dataset, we develop a 4D-aware video diffusion model capable of synthesizing orbital views of dynamic 3D assets.","To control the dynamic strength of these assets, we introduce a 3D-to-4D motion magnitude metric as guidance.","Additionally, we propose a novel motion magnitude reconstruction loss and 3D-aware classifier-free guidance to refine the learning and generation of motion dynamics.","After obtaining orbital views of the 4D asset, we perform explicit 4D construction with Gaussian splatting in a coarse-to-fine manner.","The synthesized multi-view consistent 4D image set enables us to swiftly generate high-fidelity and diverse 4D assets within just several minutes.","Extensive experiments demonstrate that our method surpasses prior state-of-the-art techniques in terms of generation efficiency and 4D geometry consistency across various prompt modalities."],"url":"http://arxiv.org/abs/2405.16645v1","category":"cs.CV"}
{"created":"2024-05-26 17:43:30","title":"Gaussian Approximation and Multiplier Bootstrap for Polyak-Ruppert Averaged Linear Stochastic Approximation with Applications to TD Learning","abstract":"In this paper, we obtain the Berry-Esseen bound for multivariate normal approximation for the Polyak-Ruppert averaged iterates of the linear stochastic approximation (LSA) algorithm with decreasing step size. Our findings reveal that the fastest rate of normal approximation is achieved when setting the most aggressive step size $\\alpha_{k} \\asymp k^{-1/2}$. Moreover, we prove the non-asymptotic validity of the confidence intervals for parameter estimation with LSA based on multiplier bootstrap. This procedure updates the LSA estimate together with a set of randomly perturbed LSA estimates upon the arrival of subsequent observations. We illustrate our findings in the setting of temporal difference learning with linear function approximation.","sentences":["In this paper, we obtain the Berry-Esseen bound for multivariate normal approximation for the Polyak-Ruppert averaged iterates of the linear stochastic approximation (LSA) algorithm with decreasing step size.","Our findings reveal that the fastest rate of normal approximation is achieved when setting the most aggressive step size $\\alpha_{k} \\asymp k^{-1/2}$.","Moreover, we prove the non-asymptotic validity of the confidence intervals for parameter estimation with LSA based on multiplier bootstrap.","This procedure updates the LSA estimate together with a set of randomly perturbed LSA estimates upon the arrival of subsequent observations.","We illustrate our findings in the setting of temporal difference learning with linear function approximation."],"url":"http://arxiv.org/abs/2405.16644v1","category":"stat.ML"}
{"created":"2024-05-26 17:12:25","title":"Computational Investigation of Reactivity Parameters, UV-Vis and IR Spectra, NLO Properties, and Temperature-Dependent Thermodynamic Characteristics of Schiff-Based Interdigitated 5O.m (m=14,16) Liquid Crystalline Compounds: A DFT Analysis","abstract":"The article studies the different physical, vibrational, nonlinear optical, and thermodynamical properties of higher homologs 5O.m (m = 14,16) liquid crystalline compounds using density functional theory. The optimized structure of 5O.m (m= 14,16) liquid crystals was obtained by using density functional theory (DFT) with B3LYP functional and standard basis set 6-31G (d, p). The Infrared spectra (IR), various physical properties such as HOMO-LUMO, nonlinear optical properties (NLO), reactivity parameters, relative energy gaps, and electrostatic potential function are computed and analyzed using the optimized structure of 5O liquid crystal. The time-dependent density functional theory (TD-DFT) has been used to analyze and obtain UV-Vis spectra for both LC compounds. It is observed that the 5O.m (m=14,16) liquid crystals are showing the lower value of HOMO-LUMO energy gaps as 4.17ev which resulted in some highly fascinating optical and physical properties. Using DFT excellent agreement is observed between all spectrum patterns and the simulated UV-Vis and IR spectra. This article, however, also discussed temperature-dependent thermodynamical properties such as zero-point vibrational energy (ZPVE), total thermal energy, total specific heat capacity, rotational constants, and total entropy which enable us to understand the phase transition behavior and specific transition temperatures of different phases","sentences":["The article studies the different physical, vibrational, nonlinear optical, and thermodynamical properties of higher homologs 5O.m (m = 14,16) liquid crystalline compounds using density functional theory.","The optimized structure of 5O.m (m= 14,16) liquid crystals was obtained by using density functional theory (DFT) with B3LYP functional and standard basis set 6-31G (d, p).","The Infrared spectra (IR), various physical properties such as HOMO-LUMO, nonlinear optical properties (NLO), reactivity parameters, relative energy gaps, and electrostatic potential function are computed and analyzed using the optimized structure of 5O liquid crystal.","The time-dependent density functional theory (TD-DFT) has been used to analyze and obtain UV-Vis spectra for both LC compounds.","It is observed that the 5O.m (m=14,16) liquid crystals are showing the lower value of HOMO-LUMO energy gaps as 4.17ev which resulted in some highly fascinating optical and physical properties.","Using DFT excellent agreement is observed between all spectrum patterns and the simulated UV-Vis and IR spectra.","This article, however, also discussed temperature-dependent thermodynamical properties such as zero-point vibrational energy (ZPVE), total thermal energy, total specific heat capacity, rotational constants, and total entropy which enable us to understand the phase transition behavior and specific transition temperatures of different phases"],"url":"http://arxiv.org/abs/2405.16632v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-26 16:40:07","title":"Constraining the physical properties of large-scale jets from black hole X-ray binaries and their impact on the local environment with blast-wave dynamical models","abstract":"Relativistic discrete ejecta launched by black hole X-ray binaries (BH XRBs) can be observed to propagate up to parsec-scales from the central object. Observing the final deceleration phase of these jets is crucial to estimate their physical parameters and to reconstruct their full trajectory, with implications for the jet powering mechanism, composition and formation. In this paper we present the results of the modelling of the motion of the ejecta from three BH XRBs: MAXI J1820+070, MAXI J1535$-$571 and XTE J1752$-$223, for which high-resolution radio and X-ray observations of jets propagating up to $\\sim$15 arcsec ($\\sim$0.6 pc at 3 kpc) from the core have been published in the recent years. For each jet, we modeled its entire motion with a dynamical blast-wave model, inferring robust values for the jet Lorentz factor, inclination angle and ejection time. Under several assumptions associated to the ejection duration, the jet opening angle and the available accretion power, we are able to derive stringent constraints on the maximum jet kinetic energy for each source (between $10^{43}$ and $10^{44}$ erg, including also H1743$-$322), as well as placing interesting upper limits on the density of the ISM through which the jets are propagating (from $n_{\\rm ISM} \\lesssim 0.4$ cm$^{-3}$ down to $n_{\\rm ISM} \\lesssim 10^{-4}$ cm$^{-3}$). Overall, our results highlight the potential of applying models derived from gamma-ray bursts to the physics of jets from BH XRBs and support the emerging picture of these sources as preferentially embedded in low-density environments.","sentences":["Relativistic discrete ejecta launched by black hole X-ray binaries (BH XRBs) can be observed to propagate up to parsec-scales from the central object.","Observing the final deceleration phase of these jets is crucial to estimate their physical parameters and to reconstruct their full trajectory, with implications for the jet powering mechanism, composition and formation.","In this paper we present the results of the modelling of the motion of the ejecta from three BH XRBs: MAXI J1820+070, MAXI J1535$-$571 and XTE J1752$-$223, for which high-resolution radio and X-ray observations of jets propagating up to $\\sim$15 arcsec ($\\sim$0.6 pc at 3 kpc) from the core have been published in the recent years.","For each jet, we modeled its entire motion with a dynamical blast-wave model, inferring robust values for the jet Lorentz factor, inclination angle and ejection time.","Under several assumptions associated to the ejection duration, the jet opening angle and the available accretion power, we are able to derive stringent constraints on the maximum jet kinetic energy for each source (between $10^{43}$ and $10^{44}$ erg, including also H1743$-$322), as well as placing interesting upper limits on the density of the ISM through which the jets are propagating (from $n_{\\rm ISM} \\lesssim 0.4$ cm$^{-3}$ down to $n_{\\rm ISM} \\lesssim 10^{-4}$ cm$^{-3}$).","Overall, our results highlight the potential of applying models derived from gamma-ray bursts to the physics of jets from BH XRBs and support the emerging picture of these sources as preferentially embedded in low-density environments."],"url":"http://arxiv.org/abs/2405.16624v1","category":"astro-ph.HE"}
{"created":"2024-05-26 16:33:05","title":"Surface Code Stabilizer Measurements for Rydberg Atoms","abstract":"We consider stabilizer measurements for surface codes with neutral atoms and identify gate protocols that minimize logical error rates in the presence of a fundamental error source -- spontaneous emission from Rydberg states. We demonstrate that logical error rates are minimized by protocols that prevent the propagation of Rydberg leakage errors and not by protocols that minimize the physical two-qubit error rate. We provide laser-pulse-level gate protocols to counter these errors. These protocols significantly reduce the logical error rate for implementations of surface codes involving one or two species of atoms. Our work demonstrates the importance of optimizing quantum gates for logical errors in addition to gate fidelities and opens the way to the efficient realization of surface codes with neutral atoms.","sentences":["We consider stabilizer measurements for surface codes with neutral atoms and identify gate protocols that minimize logical error rates in the presence of a fundamental error source -- spontaneous emission from Rydberg states.","We demonstrate that logical error rates are minimized by protocols that prevent the propagation of Rydberg leakage errors and not by protocols that minimize the physical two-qubit error rate.","We provide laser-pulse-level gate protocols to counter these errors.","These protocols significantly reduce the logical error rate for implementations of surface codes involving one or two species of atoms.","Our work demonstrates the importance of optimizing quantum gates for logical errors in addition to gate fidelities and opens the way to the efficient realization of surface codes with neutral atoms."],"url":"http://arxiv.org/abs/2405.16621v1","category":"quant-ph"}
{"created":"2024-05-26 16:01:35","title":"Decision support for sustainable forest harvest planning using multi-scenario multiobjective robust optimization","abstract":"Sustainable forest management requires handling uncertainty introduced from various sources, considering different conflicting economic, environmental, and social objectives, and involving multiple decision-making periods. This study proposes an interactive and intuitive decision-support approach for sustainable, robust forest harvest scheduling in multiple periods in a short-term (6-12 months) planning horizon. The approach includes a novel multi-scenario multiobjective mixed-integer optimization problem that allows forest planners to separately study the trade-offs between demand satisfactions for multiple assortments in different planning periods. Moreover, it provides an intuitive robust analysis to support forest planners in dealing with uncertainty and investigating potential variations of the outcomes as the consequences of uncertainty in tactical forest planning problems. We validate the proposed decision-support approach in a Swedish case study with 250 forest stands, three assortments (pine, spruce, deciduous trees), and a twelve-month harvest planning horizon. We demonstrate how the proposed approach supports a forest practitioner in trade-offs and robust analyses and finding the most preferred robust solution.","sentences":["Sustainable forest management requires handling uncertainty introduced from various sources, considering different conflicting economic, environmental, and social objectives, and involving multiple decision-making periods.","This study proposes an interactive and intuitive decision-support approach for sustainable, robust forest harvest scheduling in multiple periods in a short-term (6-12 months) planning horizon.","The approach includes a novel multi-scenario multiobjective mixed-integer optimization problem that allows forest planners to separately study the trade-offs between demand satisfactions for multiple assortments in different planning periods.","Moreover, it provides an intuitive robust analysis to support forest planners in dealing with uncertainty and investigating potential variations of the outcomes as the consequences of uncertainty in tactical forest planning problems.","We validate the proposed decision-support approach in a Swedish case study with 250 forest stands, three assortments (pine, spruce, deciduous trees), and a twelve-month harvest planning horizon.","We demonstrate how the proposed approach supports a forest practitioner in trade-offs and robust analyses and finding the most preferred robust solution."],"url":"http://arxiv.org/abs/2405.16612v1","category":"math.OC"}
{"created":"2024-05-26 15:19:21","title":"MCGMapper: Light-Weight Incremental Structure from Motion and Visual Localization With Planar Markers and Camera Groups","abstract":"Structure from Motion (SfM) and visual localization in indoor texture-less scenes and industrial scenarios present prevalent yet challenging research topics. Existing SfM methods designed for natural scenes typically yield low accuracy or map-building failures due to insufficient robust feature extraction in such settings. Visual markers, with their artificially designed features, can effectively address these issues. Nonetheless, existing marker-assisted SfM methods encounter problems like slow running speed and difficulties in convergence; and also, they are governed by the strong assumption of unique marker size. In this paper, we propose a novel SfM framework that utilizes planar markers and multiple cameras with known extrinsics to capture the surrounding environment and reconstruct the marker map. In our algorithm, the initial poses of markers and cameras are calculated with Perspective-n-Points (PnP) in the front-end, while bundle adjustment methods customized for markers and camera groups are designed in the back-end to optimize the 6-DOF pose directly. Our algorithm facilitates the reconstruction of large scenes with different marker sizes, and its accuracy and speed of map building are shown to surpass existing methods. Our approach is suitable for a wide range of scenarios, including laboratories, basements, warehouses, and other industrial settings. Furthermore, we incorporate representative scenarios into simulations and also supply our datasets with pose labels to address the scarcity of quantitative ground-truth datasets in this research field. The datasets and source code are available on GitHub.","sentences":["Structure from Motion (SfM) and visual localization in indoor texture-less scenes and industrial scenarios present prevalent yet challenging research topics.","Existing SfM methods designed for natural scenes typically yield low accuracy or map-building failures due to insufficient robust feature extraction in such settings.","Visual markers, with their artificially designed features, can effectively address these issues.","Nonetheless, existing marker-assisted SfM methods encounter problems like slow running speed and difficulties in convergence; and also, they are governed by the strong assumption of unique marker size.","In this paper, we propose a novel SfM framework that utilizes planar markers and multiple cameras with known extrinsics to capture the surrounding environment and reconstruct the marker map.","In our algorithm, the initial poses of markers and cameras are calculated with Perspective-n-Points (PnP) in the front-end, while bundle adjustment methods customized for markers and camera groups are designed in the back-end to optimize the 6-DOF pose directly.","Our algorithm facilitates the reconstruction of large scenes with different marker sizes, and its accuracy and speed of map building are shown to surpass existing methods.","Our approach is suitable for a wide range of scenarios, including laboratories, basements, warehouses, and other industrial settings.","Furthermore, we incorporate representative scenarios into simulations and also supply our datasets with pose labels to address the scarcity of quantitative ground-truth datasets in this research field.","The datasets and source code are available on GitHub."],"url":"http://arxiv.org/abs/2405.16599v1","category":"cs.RO"}
{"created":"2024-05-26 14:18:38","title":"On Bits and Bandits: Quantifying the Regret-Information Trade-off","abstract":"In interactive decision-making tasks, information can be acquired by direct interactions, through receiving indirect feedback, and from external knowledgeable sources. We examine the trade-off between the information an agent accumulates and the regret it suffers. We show that information from external sources, measured in bits, can be traded off for regret, measured in reward. We invoke information-theoretic methods for obtaining regret lower bounds, that also allow us to easily re-derive several known lower bounds. We then generalize a variety of interactive decision-making tasks with external information to a new setting. Using this setting, we introduce the first Bayesian regret lower bounds that depend on the information an agent accumulates. These lower bounds also prove the near-optimality of Thompson sampling for Bayesian problems. Finally, we demonstrate the utility of these bounds in improving the performance of a question-answering task with large language models, allowing us to obtain valuable insights.","sentences":["In interactive decision-making tasks, information can be acquired by direct interactions, through receiving indirect feedback, and from external knowledgeable sources.","We examine the trade-off between the information an agent accumulates and the regret it suffers.","We show that information from external sources, measured in bits, can be traded off for regret, measured in reward.","We invoke information-theoretic methods for obtaining regret lower bounds, that also allow us to easily re-derive several known lower bounds.","We then generalize a variety of interactive decision-making tasks with external information to a new setting.","Using this setting, we introduce the first Bayesian regret lower bounds that depend on the information an agent accumulates.","These lower bounds also prove the near-optimality of Thompson sampling for Bayesian problems.","Finally, we demonstrate the utility of these bounds in improving the performance of a question-answering task with large language models, allowing us to obtain valuable insights."],"url":"http://arxiv.org/abs/2405.16581v1","category":"cs.LG"}
{"created":"2024-05-26 13:27:27","title":"Contextual Linear Optimization with Bandit Feedback","abstract":"Contextual linear optimization (CLO) uses predictive observations to reduce uncertainty in random cost coefficients and thereby improve average-cost performance. An example is a stochastic shortest path with random edge costs (e.g., traffic) and predictive features (e.g., lagged traffic, weather). Existing work on CLO assumes the data has fully observed cost coefficient vectors, but in many applications, we can only see the realized cost of a historical decision, that is, just one projection of the random cost coefficient vector, to which we refer as bandit feedback. We study a class of algorithms for CLO with bandit feedback, which we term induced empirical risk minimization (IERM), where we fit a predictive model to directly optimize the downstream performance of the policy it induces. We show a fast-rate regret bound for IERM that allows for misspecified model classes and flexible choices of the optimization estimate, and we develop computationally tractable surrogate losses. A byproduct of our theory of independent interest is fast-rate regret bound for IERM with full feedback and misspecified policy class. We compare the performance of different modeling choices numerically using a stochastic shortest path example and provide practical insights from the empirical results.","sentences":["Contextual linear optimization (CLO) uses predictive observations to reduce uncertainty in random cost coefficients and thereby improve average-cost performance.","An example is a stochastic shortest path with random edge costs (e.g., traffic) and predictive features (e.g., lagged traffic, weather).","Existing work on CLO assumes the data has fully observed cost coefficient vectors, but in many applications, we can only see the realized cost of a historical decision, that is, just one projection of the random cost coefficient vector, to which we refer as bandit feedback.","We study a class of algorithms for CLO with bandit feedback, which we term induced empirical risk minimization (IERM), where we fit a predictive model to directly optimize the downstream performance of the policy it induces.","We show a fast-rate regret bound for IERM that allows for misspecified model classes and flexible choices of the optimization estimate, and we develop computationally tractable surrogate losses.","A byproduct of our theory of independent interest is fast-rate regret bound for IERM with full feedback and misspecified policy class.","We compare the performance of different modeling choices numerically using a stochastic shortest path example and provide practical insights from the empirical results."],"url":"http://arxiv.org/abs/2405.16564v1","category":"stat.ML"}
{"created":"2024-05-26 12:25:09","title":"Variance-Reducing Couplings for Random Features: Perspectives from Optimal Transport","abstract":"Random features (RFs) are a popular technique to scale up kernel methods in machine learning, replacing exact kernel evaluations with stochastic Monte Carlo estimates. They underpin models as diverse as efficient transformers (by approximating attention) to sparse spectrum Gaussian processes (by approximating the covariance function). Efficiency can be further improved by speeding up the convergence of these estimates: a variance reduction problem. We tackle this through the unifying framework of optimal transport, using theoretical insights and numerical algorithms to develop novel, high-performing RF couplings for kernels defined on Euclidean and discrete input spaces. They enjoy concrete theoretical performance guarantees and sometimes provide strong empirical downstream gains, including for scalable approximate inference on graphs. We reach surprising conclusions about the benefits and limitations of variance reduction as a paradigm.","sentences":["Random features (RFs) are a popular technique to scale up kernel methods in machine learning, replacing exact kernel evaluations with stochastic Monte Carlo estimates.","They underpin models as diverse as efficient transformers (by approximating attention) to sparse spectrum Gaussian processes (by approximating the covariance function).","Efficiency can be further improved by speeding up the convergence of these estimates: a variance reduction problem.","We tackle this through the unifying framework of optimal transport, using theoretical insights and numerical algorithms to develop novel, high-performing RF couplings for kernels defined on Euclidean and discrete input spaces.","They enjoy concrete theoretical performance guarantees and sometimes provide strong empirical downstream gains, including for scalable approximate inference on graphs.","We reach surprising conclusions about the benefits and limitations of variance reduction as a paradigm."],"url":"http://arxiv.org/abs/2405.16541v1","category":"stat.ML"}
{"created":"2024-05-26 11:45:46","title":"A Complete Inverse Optimality Study for a Tank-Liquid System","abstract":"This paper presents a complete inverse optimality study for a linearized tank-liquid system where the liquid is described by the viscous Saint-Venant model with surface tension and possible wall friction. We define an appropriate weak solution notion for which we establish existence/uniqueness results with inputs that do not necessarily satisfy any compatibility condition as well as stabilization results with feedback laws that are constructed with the help of a Control Lyapunov Functional. We show that the proposed family of stabilizing feedback laws is optimal for a certain meaningful quadratic cost functional. Finally, we show that the optimal feedback law guarantees additional stronger stability estimates which are similar to those obtained in the case of classical solutions.","sentences":["This paper presents a complete inverse optimality study for a linearized tank-liquid system where the liquid is described by the viscous Saint-Venant model with surface tension and possible wall friction.","We define an appropriate weak solution notion for which we establish existence/uniqueness results with inputs that do not necessarily satisfy any compatibility condition as well as stabilization results with feedback laws that are constructed with the help of a Control Lyapunov Functional.","We show that the proposed family of stabilizing feedback laws is optimal for a certain meaningful quadratic cost functional.","Finally, we show that the optimal feedback law guarantees additional stronger stability estimates which are similar to those obtained in the case of classical solutions."],"url":"http://arxiv.org/abs/2405.16535v1","category":"math.OC"}
{"created":"2024-05-26 11:21:05","title":"Exploration of methods for computing sensitivities in ODE models at dynamic and steady states","abstract":"Estimating parameters of dynamic models from experimental data is a challenging, and often computationally-demanding task. It requires a large number of model simulations and objective function gradient computations, if gradient-based optimization is used. The gradient depends on derivatives of the state variables with respect to parameters, also called state sensitivities, which are expensive to compute. In many cases, steady-state computation is a part of model simulation, either due to steady-state data or an assumption that the system is at steady state at the initial time point. Various methods are available for steady-state and gradient computation. Yet, the most efficient pair of methods (one for steady states, one for gradients) for a particular model is often not clear. Moreover, depending on the model and the available data, some methods may not be applicable or sufficiently robust. In order to facilitate the selection of methods, we explore six method pairs for computing the steady state and sensitivities at steady state using six real-world problems. The method pairs involve numerical integration or Newton's method to compute the steady-state, and -- for both forward and adjoint sensitivity analysis -- numerical integration or a tailored method to compute the sensitivities at steady-state. Our evaluation shows that the two method pairs that combine numerical integration for the steady-state with a tailored method for the sensitivities at steady-state were the most robust, and amongst the most computationally-efficient. We also observed that while Newton's method for steady-state computation yields a substantial speedup compared to numerical integration, it may lead to a large number of simulation failures. Overall, our study provides a concise overview across current methods for computing sensitivities at steady state, guiding modelers to choose the right methods.","sentences":["Estimating parameters of dynamic models from experimental data is a challenging, and often computationally-demanding task.","It requires a large number of model simulations and objective function gradient computations, if gradient-based optimization is used.","The gradient depends on derivatives of the state variables with respect to parameters, also called state sensitivities, which are expensive to compute.","In many cases, steady-state computation is a part of model simulation, either due to steady-state data or an assumption that the system is at steady state at the initial time point.","Various methods are available for steady-state and gradient computation.","Yet, the most efficient pair of methods (one for steady states, one for gradients) for a particular model is often not clear.","Moreover, depending on the model and the available data, some methods may not be applicable or sufficiently robust.","In order to facilitate the selection of methods, we explore six method pairs for computing the steady state and sensitivities at steady state using six real-world problems.","The method pairs involve numerical integration or Newton's method to compute the steady-state, and -- for both forward and adjoint sensitivity analysis -- numerical integration or a tailored method to compute the sensitivities at steady-state.","Our evaluation shows that the two method pairs that combine numerical integration for the steady-state with a tailored method for the sensitivities at steady-state were the most robust, and amongst the most computationally-efficient.","We also observed that while Newton's method for steady-state computation yields a substantial speedup compared to numerical integration, it may lead to a large number of simulation failures.","Overall, our study provides a concise overview across current methods for computing sensitivities at steady state, guiding modelers to choose the right methods."],"url":"http://arxiv.org/abs/2405.16524v1","category":"q-bio.QM"}
{"created":"2024-05-26 11:02:42","title":"Experimental demonstration of 4-state reference-frame-independent quantum key distribution over 200km","abstract":"Reference frame independent quantum key distribution (RFI-QKD) has gained widespread attention due to the unique advantage for practical application, as it circumvents the need for active reference frame alignment within the system. However, in comparison to the standard BB84 protocol, the original 6-state RFI protocol requires a greater number of quantum states to be operated by Alice and Bob, which is an aspect that merits optimization. In this work, we propose a 4-state RFI protocol and illustrate that Alice and Bob each require only four quantum states to perform channel estimation that remains independent of reference frame deviation, which can proficiently reduce the system complexity. Furthermore, through numerical simulations taking the finite-size key effect into consideration, we show that 4-state RFI protocol can achieve a secure key rate and transmission distance on par with the original 6-state RFI protocol. Finally, a experiment over 200 km is inplemented to conducted the feasibility of our scheme. We believe that our protocol can streamline the implementation of RFI-QKD and thereby contribute to the practical advancement of RFI-QKD.","sentences":["Reference frame independent quantum key distribution (RFI-QKD) has gained widespread attention due to the unique advantage for practical application, as it circumvents the need for active reference frame alignment within the system.","However, in comparison to the standard BB84 protocol, the original 6-state RFI protocol requires a greater number of quantum states to be operated by Alice and Bob, which is an aspect that merits optimization.","In this work, we propose a 4-state RFI protocol and illustrate that Alice and Bob each require only four quantum states to perform channel estimation that remains independent of reference frame deviation, which can proficiently reduce the system complexity.","Furthermore, through numerical simulations taking the finite-size key effect into consideration, we show that 4-state RFI protocol can achieve a secure key rate and transmission distance on par with the original 6-state RFI protocol.","Finally, a experiment over 200 km is inplemented to conducted the feasibility of our scheme.","We believe that our protocol can streamline the implementation of RFI-QKD and thereby contribute to the practical advancement of RFI-QKD."],"url":"http://arxiv.org/abs/2405.16518v1","category":"quant-ph"}
{"created":"2024-05-27 17:59:39","title":"Benchmarking and Improving Bird's Eye View Perception Robustness in Autonomous Driving","abstract":"Recent advancements in bird's eye view (BEV) representations have shown remarkable promise for in-vehicle 3D perception. However, while these methods have achieved impressive results on standard benchmarks, their robustness in varied conditions remains insufficiently assessed. In this study, we present RoboBEV, an extensive benchmark suite designed to evaluate the resilience of BEV algorithms. This suite incorporates a diverse set of camera corruption types, each examined over three severity levels. Our benchmarks also consider the impact of complete sensor failures that occur when using multi-modal models. Through RoboBEV, we assess 33 state-of-the-art BEV-based perception models spanning tasks like detection, map segmentation, depth estimation, and occupancy prediction. Our analyses reveal a noticeable correlation between the model's performance on in-distribution datasets and its resilience to out-of-distribution challenges. Our experimental results also underline the efficacy of strategies like pre-training and depth-free BEV transformations in enhancing robustness against out-of-distribution data. Furthermore, we observe that leveraging extensive temporal information significantly improves the model's robustness. Based on our observations, we design an effective robustness enhancement strategy based on the CLIP model. The insights from this study pave the way for the development of future BEV models that seamlessly combine accuracy with real-world robustness.","sentences":["Recent advancements in bird's eye view (BEV) representations have shown remarkable promise for in-vehicle 3D perception.","However, while these methods have achieved impressive results on standard benchmarks, their robustness in varied conditions remains insufficiently assessed.","In this study, we present RoboBEV, an extensive benchmark suite designed to evaluate the resilience of BEV algorithms.","This suite incorporates a diverse set of camera corruption types, each examined over three severity levels.","Our benchmarks also consider the impact of complete sensor failures that occur when using multi-modal models.","Through RoboBEV, we assess 33 state-of-the-art BEV-based perception models spanning tasks like detection, map segmentation, depth estimation, and occupancy prediction.","Our analyses reveal a noticeable correlation between the model's performance on in-distribution datasets and its resilience to out-of-distribution challenges.","Our experimental results also underline the efficacy of strategies like pre-training and depth-free BEV transformations in enhancing robustness against out-of-distribution data.","Furthermore, we observe that leveraging extensive temporal information significantly improves the model's robustness.","Based on our observations, we design an effective robustness enhancement strategy based on the CLIP model.","The insights from this study pave the way for the development of future BEV models that seamlessly combine accuracy with real-world robustness."],"url":"http://arxiv.org/abs/2405.17426v1","category":"cs.CV"}
{"created":"2024-05-27 17:57:05","title":"The Peripatetic Hater: Predicting Movement Among Hate Subreddits","abstract":"Many online hate groups exist to disparage others based on race, gender identity, sex, or other characteristics. The accessibility of these communities allows users to join multiple types of hate groups (e.g., a racist community and misogynistic community), which calls into question whether these peripatetic users could be further radicalized compared to users that stay in one type of hate group. However, little is known about the dynamics of joining multiple types of hate groups, nor the effect of these groups on peripatetic users. In this paper, we develop a new method to classify hate subreddits, and the identities they disparage, which we use to better understand how users become peripatetic (join different types of hate subreddits). The hate classification technique utilizes human-validated LLMs to extract the protected identities attacked, if any, across 168 subreddits. We then cluster identity-attacking subreddits to discover three broad categories of hate: racist, anti-LGBTQ, and misogynistic. We show that becoming active in a user's first hate subreddit can cause them to become active in additional hate subreddits of a different category. We also find that users who join additional hate subreddits, especially of a different category, become more active in hate subreddits as a whole and develop a wider hate group lexicon. We are therefore motivated to train an AI model that we find usefully predicts the hate categories users will become active in based on post text read and written. The accuracy of this model may be partly driven by peripatetic users often using the language of hate subreddits they eventually join. Overall, these results highlight the unique risks associated with hate communities on a social media platform, as discussion of alternative targets of hate may lead users to target more protected identities.","sentences":["Many online hate groups exist to disparage others based on race, gender identity, sex, or other characteristics.","The accessibility of these communities allows users to join multiple types of hate groups (e.g., a racist community and misogynistic community), which calls into question whether these peripatetic users could be further radicalized compared to users that stay in one type of hate group.","However, little is known about the dynamics of joining multiple types of hate groups, nor the effect of these groups on peripatetic users.","In this paper, we develop a new method to classify hate subreddits, and the identities they disparage, which we use to better understand how users become peripatetic (join different types of hate subreddits).","The hate classification technique utilizes human-validated LLMs to extract the protected identities attacked, if any, across 168 subreddits.","We then cluster identity-attacking subreddits to discover three broad categories of hate: racist, anti-LGBTQ, and misogynistic.","We show that becoming active in a user's first hate subreddit can cause them to become active in additional hate subreddits of a different category.","We also find that users who join additional hate subreddits, especially of a different category, become more active in hate subreddits as a whole and develop a wider hate group lexicon.","We are therefore motivated to train an AI model that we find usefully predicts the hate categories users will become active in based on post text read and written.","The accuracy of this model may be partly driven by peripatetic users often using the language of hate subreddits they eventually join.","Overall, these results highlight the unique risks associated with hate communities on a social media platform, as discussion of alternative targets of hate may lead users to target more protected identities."],"url":"http://arxiv.org/abs/2405.17410v1","category":"cs.SI"}
{"created":"2024-05-27 17:29:51","title":"Probing the Relationship between Defects and Enhanced Mobility in MoS2 Monolayers Grown by Mo Foil","abstract":"Atomic vacancies, such as chalcogen vacancies in 2D TMDs, are important in changing the host material's electronic structure and transport properties. We present a straightforward one-step method for growing monolayer MoS2 utilizing oxidized Molybdenum (Mo) foil using CVD and delve into the transport properties of as-grown samples. Devices fabricated from these MoS2 sheets exhibit excellent electrical responses, with the standout device achieving mobility exceeding 100 cm2V-1s-1. Structural analysis and optical signatures unveiled the presence of chalcogen defects within these samples. To decipher the influence of inherent defects on the electronic transport properties, we measured low-temperature transport on two distinct sets of devices exhibiting relatively high or low mobilities. Combining the thermally activated transport model with quantum capacitance calculations, we have shown the existence of shallow states near the conduction band, likely attributed to sulfur vacancies within MoS2. These vacancies are responsible for the hopping conduction of electrons in the device channel. Furthermore, our claims were substantiated through low-temperature scanning tunnelling microscopy measurements, which revealed an abundance of isolated and lateral double sulfur vacancies in Mo foil-grown samples. We found that these vacancies increase the density of states near the conduction band, inducing intrinsic n-type doping in the MoS2 channel. Consequently, this elevated conductivity enhances the field-effect mobility of MoS2 transistors. Our study offers insights into chalcogen vacancies in CVD-grown monolayer MoS2 and highlights their beneficial impact on electronic transport properties.","sentences":["Atomic vacancies, such as chalcogen vacancies in 2D TMDs, are important in changing the host material's electronic structure and transport properties.","We present a straightforward one-step method for growing monolayer MoS2 utilizing oxidized Molybdenum (Mo) foil using CVD and delve into the transport properties of as-grown samples.","Devices fabricated from these MoS2 sheets exhibit excellent electrical responses, with the standout device achieving mobility exceeding 100 cm2V-1s-1.","Structural analysis and optical signatures unveiled the presence of chalcogen defects within these samples.","To decipher the influence of inherent defects on the electronic transport properties, we measured low-temperature transport on two distinct sets of devices exhibiting relatively high or low mobilities.","Combining the thermally activated transport model with quantum capacitance calculations, we have shown the existence of shallow states near the conduction band, likely attributed to sulfur vacancies within MoS2.","These vacancies are responsible for the hopping conduction of electrons in the device channel.","Furthermore, our claims were substantiated through low-temperature scanning tunnelling microscopy measurements, which revealed an abundance of isolated and lateral double sulfur vacancies in Mo foil-grown samples.","We found that these vacancies increase the density of states near the conduction band, inducing intrinsic n-type doping in the MoS2 channel.","Consequently, this elevated conductivity enhances the field-effect mobility of MoS2 transistors.","Our study offers insights into chalcogen vacancies in CVD-grown monolayer MoS2 and highlights their beneficial impact on electronic transport properties."],"url":"http://arxiv.org/abs/2405.17373v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-27 17:27:36","title":"Muon spin relaxation in mixed perovskite (LaAlO$_3$)$_{x}$(SrAl$_{0.5}$Ta$_{0.5}$O$_3$)$_{1-x}$ with $x\\simeq 0.3$","abstract":"We report on muon spin relaxation ($\\mu^+$SR) measurements in a mixed perovskite compound, (LaAlO$_3$)$_{x}$(SrAl$_{0.5}$Ta$_{0.5}$O$_3$)$_{1-x}$ with $x\\simeq 0.3$ (LSAT), which is widely used as a single-crystalline substrate for thin film deposition. In zero applied field (ZF), muon depolarization due to the distribution of nuclear dipole fields was observed in the temperature range from 4 K to 270 K. Interestingly, $\\mu^+$SR time spectra in ZF maintained a Gaussian-like feature over the entire range, while the depolarization rate exhibited a monotonic decrease with increasing temperature. This behavior may be attributed to the thermally activated diffusion of muons between a few adjacent sites within a confined space of the angstrom scale, where the motionally averaged local field that each muon experiences can remain non-zero and result in maintaining the Gaussian-like line shape. The spatial distribution of electrostatic potential at lattice interstices evaluated via density functional theory calculations suggests that such a restriction of muon diffusion paths can be caused by the random distribution of cations with different nominal valences in the mixed perovskite lattice.","sentences":["We report on muon spin relaxation ($\\mu^+$SR) measurements in a mixed perovskite compound, (LaAlO$_3$)$_{x}$(SrAl$_{0.5}$Ta$_{0.5}$O$_3$)$_{1-x}$ with $x\\simeq 0.3$ (LSAT), which is widely used as a single-crystalline substrate for thin film deposition.","In zero applied field (ZF), muon depolarization due to the distribution of nuclear dipole fields was observed in the temperature range from 4 K to 270 K. Interestingly, $\\mu^+$SR time spectra in ZF maintained a Gaussian-like feature over the entire range, while the depolarization rate exhibited a monotonic decrease with increasing temperature.","This behavior may be attributed to the thermally activated diffusion of muons between a few adjacent sites within a confined space of the angstrom scale, where the motionally averaged local field that each muon experiences can remain non-zero and result in maintaining the Gaussian-like line shape.","The spatial distribution of electrostatic potential at lattice interstices evaluated via density functional theory calculations suggests that such a restriction of muon diffusion paths can be caused by the random distribution of cations with different nominal valences in the mixed perovskite lattice."],"url":"http://arxiv.org/abs/2405.17371v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-27 17:04:25","title":"EPOCHS Paper X: Environmental effects on Galaxy Formation and Protocluster Galaxy candidates at $4.5<z<10$ from JWST observations","abstract":"In this paper we describe our search for galaxy protocluster candidates at $4.5< z < 10$ and explore the environmental and physical properties of their member galaxies identified through JWST wide-field surveys within the CEERS, JADES, and PEARLS NEP-TDF fields. Combining with HST data, we identify 2948 robust $z>4.5$ candidates within an area of 185.4 arcmin$^2$. We determine nearest neighbour statistics and galaxy environments. We find that high-$z$ galaxies in overdense environments exhibit higher star formation activity compared to those in underdense regions. Galaxies in dense environments have a slightly increased SFR at a given mass compared with galaxies in the lower density environments. At the high mass end we also find a gradual flattening of the $M_{\\star}$-SFR slope. We find that galaxies in high-density regions often have redder UV slopes than those in low-density regions, suggesting more dust extinction, weaker Lyman-alpha emission and / or a higher damped Lyman-alpha absorption. We also find that the mass-size relation remains consistent and statistically similar across all environments. Furthermore, we quantitatively assess the probability of a galaxy belonging to a protocluster candidate. In total, we identified 26 overdensities at $z=5-7$ and estimate their dark matter halo masses. We find that all protocluster candidates could evolve into clusters with $M_{\\rm halo} > 10^{14}M_{\\odot}$ at $z = 0$, thereby supporting the theoretical and simulation predictions of cluster formation. Notably, this marks an early search for protocluster candidates in JWST wide field based on photometric data, providing valuable candidates to study cosmic structure formation at the early stages.","sentences":["In this paper we describe our search for galaxy protocluster candidates at $4.5< z < 10$ and explore the environmental and physical properties of their member galaxies identified through JWST wide-field surveys within the CEERS, JADES, and PEARLS NEP-TDF fields.","Combining with HST data, we identify 2948 robust $z>4.5$ candidates within an area of 185.4 arcmin$^2$.","We determine nearest neighbour statistics and galaxy environments.","We find that high-$z$ galaxies in overdense environments exhibit higher star formation activity compared to those in underdense regions.","Galaxies in dense environments have a slightly increased SFR at a given mass compared with galaxies in the lower density environments.","At the high mass end we also find a gradual flattening of the $M_{\\star}$-SFR slope.","We find that galaxies in high-density regions often have redder UV slopes than those in low-density regions, suggesting more dust extinction, weaker Lyman-alpha emission and / or a higher damped Lyman-alpha absorption.","We also find that the mass-size relation remains consistent and statistically similar across all environments.","Furthermore, we quantitatively assess the probability of a galaxy belonging to a protocluster candidate.","In total, we identified 26 overdensities at $z=5-7$ and estimate their dark matter halo masses.","We find that all protocluster candidates could evolve into clusters with $M_{\\rm halo} > 10^{14}M_{\\odot}$ at $z = 0$, thereby supporting the theoretical and simulation predictions of cluster formation.","Notably, this marks an early search for protocluster candidates in JWST wide field based on photometric data, providing valuable candidates to study cosmic structure formation at the early stages."],"url":"http://arxiv.org/abs/2405.17359v1","category":"astro-ph.GA"}
{"created":"2024-05-27 16:37:17","title":"XFormParser: A Simple and Effective Multimodal Multilingual Semi-structured Form Parser","abstract":"In the domain of document AI, semi-structured form parsing plays a crucial role. This task leverages techniques from key information extraction (KIE), dealing with inputs that range from plain text to intricate modal data comprising images and structural layouts. The advent of pre-trained multimodal models has driven the extraction of key information from form documents in different formats such as PDFs and images. Nonetheless, the endeavor of form parsing is still encumbered by notable challenges like subpar capabilities in multi-lingual parsing and diminished recall in contexts rich in text and visuals. In this work, we introduce a simple but effective \\textbf{M}ultimodal and \\textbf{M}ultilingual semi-structured \\textbf{FORM} \\textbf{PARSER} (\\textbf{XFormParser}), which is anchored on a comprehensive pre-trained language model and innovatively amalgamates semantic entity recognition (SER) and relation extraction (RE) into a unified framework, enhanced by a novel staged warm-up training approach that employs soft labels to significantly refine form parsing accuracy without amplifying inference overhead. Furthermore, we have developed a groundbreaking benchmark dataset, named InDFormBench, catering specifically to the parsing requirements of multilingual forms in various industrial contexts. Through rigorous testing on established multilingual benchmarks and InDFormBench, XFormParser has demonstrated its unparalleled efficacy, notably surpassing the state-of-the-art (SOTA) models in RE tasks within language-specific setups by achieving an F1 score improvement of up to 1.79\\%. Our framework exhibits exceptionally improved performance across tasks in both multi-language and zero-shot contexts when compared to existing SOTA benchmarks. The code is publicly available at https://github.com/zhbuaa0/layoutlmft.","sentences":["In the domain of document AI, semi-structured form parsing plays a crucial role.","This task leverages techniques from key information extraction (KIE), dealing with inputs that range from plain text to intricate modal data comprising images and structural layouts.","The advent of pre-trained multimodal models has driven the extraction of key information from form documents in different formats such as PDFs and images.","Nonetheless, the endeavor of form parsing is still encumbered by notable challenges like subpar capabilities in multi-lingual parsing and diminished recall in contexts rich in text and visuals.","In this work, we introduce a simple but effective \\textbf{M}ultimodal and \\textbf{M}ultilingual semi-structured \\textbf{FORM} \\textbf{PARSER} (\\textbf{XFormParser}), which is anchored on a comprehensive pre-trained language model and innovatively amalgamates semantic entity recognition (SER) and relation extraction (RE) into a unified framework, enhanced by a novel staged warm-up training approach that employs soft labels to significantly refine form parsing accuracy without amplifying inference overhead.","Furthermore, we have developed a groundbreaking benchmark dataset, named InDFormBench, catering specifically to the parsing requirements of multilingual forms in various industrial contexts.","Through rigorous testing on established multilingual benchmarks and InDFormBench, XFormParser has demonstrated its unparalleled efficacy, notably surpassing the state-of-the-art (SOTA) models in RE tasks within language-specific setups by achieving an F1 score improvement of up to 1.79\\%.","Our framework exhibits exceptionally improved performance across tasks in both multi-language and zero-shot contexts when compared to existing SOTA benchmarks.","The code is publicly available at https://github.com/zhbuaa0/layoutlmft."],"url":"http://arxiv.org/abs/2405.17336v1","category":"cs.CL"}
{"created":"2024-05-27 16:36:29","title":"Thermodynamic Potential of the Polyakov Loop in SU(3) Quenched Lattice QCD","abstract":"Using SU(3) lattice QCD, we study the effective potential of the Polyakov loop $\\langle P \\rangle$ at finite temperature, i.e., the thermodynamic potential, in the field-theoretical manner for the first time. In the framework of the reweighting method in lattice QCD, we express the effective potential $V_{\\rm eff}(\\langle P \\rangle)$ using the expectation value with no source term. In particular, we deal with the most difficult and interesting case of vacuum coexistence at the critical temperature $T_c$. We adopt SU(3) quenched lattice QCD on $48^3 \\times 6$ at $\\beta$= 5.89379, just corresponding to the critical temperature $T_c$ of the deconfinement phase transition, and use 200,000 Monte Carlo configurations. After categorizing the gauge configurations into each of $Z_3$-symmetric and three $Z_3$-broken vacua, we perform a vacuum-associated reweighting method where gauge configurations around each vacuum are separately used. We eventually obtain the Polyakov-loop effective potential, which is well depicted around the $Z_3$-symmetric and $Z_3$-broken vacua.","sentences":["Using SU(3) lattice QCD, we study the effective potential of the Polyakov loop $\\langle P \\rangle$ at finite temperature, i.e., the thermodynamic potential, in the field-theoretical manner for the first time.","In the framework of the reweighting method in lattice QCD, we express the effective potential $V_{\\rm eff}(\\langle P \\rangle)$ using the expectation value with no source term.","In particular, we deal with the most difficult and interesting case of vacuum coexistence at the critical temperature $T_c$. We adopt SU(3) quenched lattice QCD on $48^3","\\times 6$ at $\\beta$= 5.89379, just corresponding to the critical temperature $T_c$ of the deconfinement phase transition, and use 200,000 Monte Carlo configurations.","After categorizing the gauge configurations into each of $Z_3$-symmetric and three $Z_3$-broken vacua, we perform a vacuum-associated reweighting method where gauge configurations around each vacuum are separately used.","We eventually obtain the Polyakov-loop effective potential, which is well depicted around the $Z_3$-symmetric and $Z_3$-broken vacua."],"url":"http://arxiv.org/abs/2405.17335v1","category":"hep-lat"}
{"created":"2024-05-27 16:18:29","title":"Gravity induced CP violation","abstract":"The impact of earth's gravity on neutral kaons oscillations is analyzed. The main effect of a Newtonian potential is to couple the strangeness oscillation and the strange quarks zitterbewegung. This coupling is responsible for the observed CP violations. Gravity induced CP violation is in fact a CPT violation with T conservation rather than a T violation with CPT conservation, but the finite lifetime of the short-lived kaons induces a rotation of the imaginary CPT parameter such that it becomes real and the effect is observed as a CP and T violation. Both indirect and direct violation parameters are predicted in agreement with the experimental data.","sentences":["The impact of earth's gravity on neutral kaons oscillations is analyzed.","The main effect of a Newtonian potential is to couple the strangeness oscillation and the strange quarks zitterbewegung.","This coupling is responsible for the observed CP violations.","Gravity induced CP violation is in fact a CPT violation with T conservation rather than a T violation with CPT conservation, but the finite lifetime of the short-lived kaons induces a rotation of the imaginary CPT parameter such that it becomes real and the effect is observed as a CP and T violation.","Both indirect and direct violation parameters are predicted in agreement with the experimental data."],"url":"http://arxiv.org/abs/2405.17317v1","category":"hep-ph"}
{"created":"2024-05-27 16:10:06","title":"Thermally-induced mimicry of quantum cluster excitations and implications for the magnetic transition in FePX$_3$","abstract":"In two dimensional magnets, the interplay of thermal fluctuations and spin anisotropy control the existence of long-range magnetic order. In the van der Waals antiferromagnets FePX$_3$, orbital degeneracy in the $t_{2g}$ levels of the Fe$^{2+}$ ions in octahedral coordination yields strong uniaxial anisotropy, which stabilizes magnetic order up to $T \\approx 100$ K. Recent inelastic neutron scattering measurements around the magnetic ordering transition have shown the existence of a broad spectrum of magnetic fluctuations with nontrivial momentum dependence, what has been interpreted as evidence for localized entangled cluster excitations. In this paper, we offer an alternative interpretation using classical non-linear spin dynamics simulations. We present stochastic Landau Lifshitz dynamics simulations that reproduce the neutron scattering measurements of Chen et al. [1] on FePX$_3$. These calculations faithfully explain the dynamical structure factor's momentum and energy dependence and point to a classical origin for the excitations observed in neutron spectroscopy and that the order-disorder transition can be understood in terms of thermal fluctuations overcoming the anisotropy energy.","sentences":["In two dimensional magnets, the interplay of thermal fluctuations and spin anisotropy control the existence of long-range magnetic order.","In the van der Waals antiferromagnets FePX$_3$, orbital degeneracy in the $t_{2g}$ levels of the Fe$^{2+}$ ions in octahedral coordination yields strong uniaxial anisotropy, which stabilizes magnetic order up to $T \\approx 100$ K. Recent inelastic neutron scattering measurements around the magnetic ordering transition have shown the existence of a broad spectrum of magnetic fluctuations with nontrivial momentum dependence, what has been interpreted as evidence for localized entangled cluster excitations.","In this paper, we offer an alternative interpretation using classical non-linear spin dynamics simulations.","We present stochastic","Landau Lifshitz dynamics simulations that reproduce the neutron scattering measurements of Chen et al.","[1] on FePX$_3$. These calculations faithfully explain the dynamical structure factor's momentum and energy dependence and point to a classical origin for the excitations observed in neutron spectroscopy and that the order-disorder transition can be understood in terms of thermal fluctuations overcoming the anisotropy energy."],"url":"http://arxiv.org/abs/2405.17308v1","category":"cond-mat.str-el"}
{"created":"2024-05-27 16:03:19","title":"Gribov copies in the quark propagator","abstract":"We study the impact of Gribov copies on the quark propagator in lattice 2-colour QCD. We find that the Gribov noise is comparable to the gauge noise for smaller volumes but becomes less significant for larger spatial volumes. The Gribov noise in the quark propagator is found to be comparable to, but smaller than in the gluon propagator on the same ensembles. No correlation is found between the values of either of the quark propagator form factors and the value of the gauge fixing functional, nor between the two form factors.","sentences":["We study the impact of Gribov copies on the quark propagator in lattice 2-colour QCD.","We find that the Gribov noise is comparable to the gauge noise for smaller volumes but becomes less significant for larger spatial volumes.","The Gribov noise in the quark propagator is found to be comparable to, but smaller than in the gluon propagator on the same ensembles.","No correlation is found between the values of either of the quark propagator form factors and the value of the gauge fixing functional, nor between the two form factors."],"url":"http://arxiv.org/abs/2405.17301v1","category":"hep-lat"}
{"created":"2024-05-27 15:39:13","title":"Simulating Attochemistry: Which Dynamics Method to Use?","abstract":"Attochemistry aims to exploit the properties of coherent electronic wavepackets excited via attosecond pulses, to control the formation of photoproducts. Such molecular processes can in principle be simulated with various nonadiabatic dynamics methods, yet the impact of the approximations underlying the methods is rarely assessed. The performances of widely used mixed quantum-classical approaches, the Tully surface hopping, and classical Ehrenfest methods are evaluated against the high-accuracy DD-vMCG quantum dynamics. This comparison is conducted on the valence ionization of fluorobenzene. Analyzing the nuclear motion induced in the branching space of the nearby conical intersection, the results show that the mixed quantum-classical methods reproduce quantitatively the average motion of a quantum wavepacket when initiated on a single electronic state. However, they fail to properly capture the nuclear motion induced by an electronic wavepacket along the derivative coupling, the latter originating from the quantum electronic coherence property -- key to attochemistry.","sentences":["Attochemistry aims to exploit the properties of coherent electronic wavepackets excited via attosecond pulses, to control the formation of photoproducts.","Such molecular processes can in principle be simulated with various nonadiabatic dynamics methods, yet the impact of the approximations underlying the methods is rarely assessed.","The performances of widely used mixed quantum-classical approaches, the Tully surface hopping, and classical Ehrenfest methods are evaluated against the high-accuracy DD-vMCG quantum dynamics.","This comparison is conducted on the valence ionization of fluorobenzene.","Analyzing the nuclear motion induced in the branching space of the nearby conical intersection, the results show that the mixed quantum-classical methods reproduce quantitatively the average motion of a quantum wavepacket when initiated on a single electronic state.","However, they fail to properly capture the nuclear motion induced by an electronic wavepacket along the derivative coupling, the latter originating from the quantum electronic coherence property -- key to attochemistry."],"url":"http://arxiv.org/abs/2405.17276v1","category":"physics.chem-ph"}
{"created":"2024-05-27 15:34:07","title":"A Derivation of Geometric Quantization via Feynman's Path Integral on Phase Space","abstract":"We derive the geometric quantization program of symplectic manifolds, in the sense of both Kostant-Souriau and Weinstein, from Feynman's path integral formulation on phase space. The state space we use contains states with negative norm and polarized sections determine a Hilbert space. We discuss ambiguities in the definition of path integrals arising from the distinct Riemann sum prescriptions and its consequence on the quantization of symplectomorphisms.","sentences":["We derive the geometric quantization program of symplectic manifolds, in the sense of both Kostant-Souriau and Weinstein, from Feynman's path integral formulation on phase space.","The state space we use contains states with negative norm and polarized sections determine a Hilbert space.","We discuss ambiguities in the definition of path integrals arising from the distinct Riemann sum prescriptions and its consequence on the quantization of symplectomorphisms."],"url":"http://arxiv.org/abs/2405.17273v1","category":"math.SG"}
{"created":"2024-05-27 15:24:47","title":"Dissociation and isomerization following ionization of ethylene: insights from non-adiabatic dynamics simulations","abstract":"Photoionized and electronically excited ethylene \\ce{C2H4+} can undergo \\ce{H}-loss, \\ce{H2}-loss, and ethylene-ethylidene isomerization, where the latter entails a hydrogen migration. Recent pioneering experiments with few-femtosecond extreme ultraviolet pulses and complementary theoretical studies have shed light on the photodynamics of this prototypical organic cation. However, no theoretical investigation based on dynamics simulations reported to date has described the mechanisms and time scales of dissociation and isomerization. Herein, we simulate the coupled electron-nuclear dynamics of ethylene following vertical ionization and electronic excitation to its four lowest-lying cationic states. The electronic structure is treated at the CASSCF level, with an active space large enough to describe bond breaking and formation. The simulations indicate that dissociation and isomerization take place mainly on the cationic ground state and allow the probing of previous hypotheses concerning the correlation between the photochemical outcome and the traversed conical intersections. The results, moreover, support the long-standing view that \\ce{H2}-loss may occur from the ethylidene form. However, the ethylene-ethylidene isomerization time predicted by the simulations is considerably longer than those previously inferred from indirect experimental measurements.","sentences":["Photoionized and electronically excited ethylene \\ce{C2H4+} can undergo \\ce{H}-loss, \\ce{H2}-loss, and ethylene-ethylidene isomerization, where the latter entails a hydrogen migration.","Recent pioneering experiments with few-femtosecond extreme ultraviolet pulses and complementary theoretical studies have shed light on the photodynamics of this prototypical organic cation.","However, no theoretical investigation based on dynamics simulations reported to date has described the mechanisms and time scales of dissociation and isomerization.","Herein, we simulate the coupled electron-nuclear dynamics of ethylene following vertical ionization and electronic excitation to its four lowest-lying cationic states.","The electronic structure is treated at the CASSCF level, with an active space large enough to describe bond breaking and formation.","The simulations indicate that dissociation and isomerization take place mainly on the cationic ground state and allow the probing of previous hypotheses concerning the correlation between the photochemical outcome and the traversed conical intersections.","The results, moreover, support the long-standing view that \\ce{H2}-loss may occur from the ethylidene form.","However, the ethylene-ethylidene isomerization time predicted by the simulations is considerably longer than those previously inferred from indirect experimental measurements."],"url":"http://arxiv.org/abs/2405.17266v1","category":"physics.chem-ph"}
{"created":"2024-05-27 15:07:40","title":"Next-to-Next-to-Leading Order Evolution of Polarized Parton Densities in the Larin Scheme","abstract":"In many calculations involving polarized twist-2 parton densities to higher order in the strong coupling constant one uses the Larin scheme to describe chiral effects in dimensional regularization. Upon forming observables, the scheme dependence cancels. Still one needs a corresponding regularization scheme to compute the contributing building blocks, like massless and massive Wilson coefficients, as well as the massive 3-loop operator matrix elements used in the variable flavor number scheme. These are matched to the evolved parton distribution functions in the Larin scheme. Starting with suitable input distributions we provide the solution of scale evolution of the different polarized parton distribution functions in Bjorken $x$ space for a wide range of virtualities $Q^2$ in the Larin scheme, at next-to-leading, and to next-to-next-to-leading order for the first time. We also illustrate the deviation between the parton distributions in the Larin and $\\overline{\\rm MS}$ schemes numerically.","sentences":["In many calculations involving polarized twist-2 parton densities to higher order in the strong coupling constant one uses the Larin scheme to describe chiral effects in dimensional regularization.","Upon forming observables, the scheme dependence cancels.","Still one needs a corresponding regularization scheme to compute the contributing building blocks, like massless and massive Wilson coefficients, as well as the massive 3-loop operator matrix elements used in the variable flavor number scheme.","These are matched to the evolved parton distribution functions in the Larin scheme.","Starting with suitable input distributions we provide the solution of scale evolution of the different polarized parton distribution functions in Bjorken $x$ space for a wide range of virtualities $Q^2$ in the Larin scheme, at next-to-leading, and to next-to-next-to-leading order for the first time.","We also illustrate the deviation between the parton distributions in the Larin and $\\overline{\\rm MS}$ schemes numerically."],"url":"http://arxiv.org/abs/2405.17252v1","category":"hep-ph"}
{"created":"2024-05-27 14:36:18","title":"Quasi-transitive $K_\\infty$-minor free graphs","abstract":"We prove that every locally finite quasi-transitive graph that does not contain $K_\\infty$ as a minor is quasi-isometric to some planar quasi-transitive locally finite graph. This solves a problem of Esperet and Giocanti and improves their recent result that such graphs are quasi-isometric to some planar graph of bounded degree.","sentences":["We prove that every locally finite quasi-transitive graph that does not contain $K_\\infty$ as a minor is quasi-isometric to some planar quasi-transitive locally finite graph.","This solves a problem of Esperet and Giocanti and improves their recent result that such graphs are quasi-isometric to some planar graph of bounded degree."],"url":"http://arxiv.org/abs/2405.17218v1","category":"math.CO"}
{"created":"2024-05-27 14:00:42","title":"Closing the net on transient sources of ultra-high-energy cosmic rays","abstract":"Arrival directions of ultra-high-energy cosmic rays (UHECRs) observed above $4\\times10^{19}\\,$eV provide evidence of localized excesses that are key to identifying their sources. We leverage the 3D matter distribution from optical and infrared surveys as a density model of UHECR sources, which are considered to be transient. Agreement of the sky model with UHECR data imposes constraints on both the emission rate per unit matter and the time spread induced by encountered turbulent magnetic fields. Based on radio measurements of cosmic magnetism, we identify the Local Sheet as the magnetized structure responsible for the kiloyear duration of UHECR bursts for an observer on Earth and find that the turbulence amplitude must be within $0.5-20\\,$nG for a coherence length of $10\\,$kpc. At the same time, the burst-rate density must be above $50\\,$Gpc$^{-3}\\,$yr$^{-1}$ for Local-Sheet galaxies to reproduce the UHECR excesses and below $5\\,000\\,$Gpc$^{-3}\\,$yr$^{-1}$ ($30\\,000\\,$Gpc$^{-3}\\,$yr$^{-1}$) for the Milky Way (Local-Group galaxies) not to outshine other galaxies. For the transient emissions of protons and nuclei to match the energy spectra of UHECRs, the kinetic energy of the outflows responsible for UHECR acceleration must be below $4\\times10^{54}\\,$erg and above $5\\times10^{50}\\,$erg ($2\\times10^{49}\\,$erg) if we consider the Milky Way (or not). The only stellar-sized transients that satisfy both Hillas' and our criteria are long gamma-ray bursts.","sentences":["Arrival directions of ultra-high-energy cosmic rays (UHECRs) observed above $4\\times10^{19}\\,$eV provide evidence of localized excesses that are key to identifying their sources.","We leverage the 3D matter distribution from optical and infrared surveys as a density model of UHECR sources, which are considered to be transient.","Agreement of the sky model with UHECR data imposes constraints on both the emission rate per unit matter and the time spread induced by encountered turbulent magnetic fields.","Based on radio measurements of cosmic magnetism, we identify the Local Sheet as the magnetized structure responsible for the kiloyear duration of UHECR bursts for an observer on Earth and find that the turbulence amplitude must be within $0.5-20\\,$nG for a coherence length of $10\\,$kpc.","At the same time, the burst-rate density must be above $50\\,$Gpc$^{-3}\\,$yr$^{-1}$ for Local-Sheet galaxies to reproduce the UHECR excesses and below $5\\,000\\,$Gpc$^{-3}\\,$yr$^{-1}$ ($30\\,000\\,$Gpc$^{-3}\\,$yr$^{-1}$) for the Milky Way (Local-Group galaxies) not to outshine other galaxies.","For the transient emissions of protons and nuclei to match the energy spectra of UHECRs, the kinetic energy of the outflows responsible for UHECR acceleration must be below $4\\times10^{54}\\,$erg and above $5\\times10^{50}\\,$erg ($2\\times10^{49}\\,$erg) if we consider the Milky Way (or not).","The only stellar-sized transients that satisfy both Hillas' and our criteria are long gamma-ray bursts."],"url":"http://arxiv.org/abs/2405.17179v1","category":"astro-ph.HE"}
{"created":"2024-05-27 13:42:35","title":"Cross-border cannibalization: Spillover effects of wind and solar energy on interconnected European electricity markets","abstract":"The average revenue, or market value, of wind and solar energy tends to fall with increasing market shares, as is now evident across European electricity markets. At the same time, these markets have become more interconnected. In this paper, we empirically study the multiple cross-border effects on the value of renewable energy: on one hand, interconnection is a flexibility resource that allows to export energy when it is locally abundant, benefitting renewables. On the other hand, wind and solar radiation are correlated across space, so neighboring supply adds to the local one to depress domestic prices. We estimate both effects, using spatial panel regression on electricity market data from 2015 to 2023 from 30 European bidding zones. We find that domestic wind and solar value is not only depressed by domestic, but also by neighboring renewables expansion. The better interconnected a market is, the smaller the effect of domestic but the larger the effect of neighboring renewables. While wind value is stabilized by interconnection, solar value is not. If wind market share increases both at home and in neighboring markets by one percentage point, the value factor of wind energy is reduced by just above 1 percentage points. For solar, this number is almost 4 percentage points.","sentences":["The average revenue, or market value, of wind and solar energy tends to fall with increasing market shares, as is now evident across European electricity markets.","At the same time, these markets have become more interconnected.","In this paper, we empirically study the multiple cross-border effects on the value of renewable energy: on one hand, interconnection is a flexibility resource that allows to export energy when it is locally abundant, benefitting renewables.","On the other hand, wind and solar radiation are correlated across space, so neighboring supply adds to the local one to depress domestic prices.","We estimate both effects, using spatial panel regression on electricity market data from 2015 to 2023 from 30 European bidding zones.","We find that domestic wind and solar value is not only depressed by domestic, but also by neighboring renewables expansion.","The better interconnected a market is, the smaller the effect of domestic but the larger the effect of neighboring renewables.","While wind value is stabilized by interconnection, solar value is not.","If wind market share increases both at home and in neighboring markets by one percentage point, the value factor of wind energy is reduced by just above 1 percentage points.","For solar, this number is almost 4 percentage points."],"url":"http://arxiv.org/abs/2405.17166v1","category":"econ.EM"}
{"created":"2024-05-27 12:23:18","title":"Apparent fake neutrino mixture due to seesaw mechanism responsible for oscillation","abstract":"This paper assumes that neutrino flavor conversion is induced by right-handed neutrino mixture via seesaw mechanism, which leads to apparent fake neutrino mixture with neutrino mass eigenstate consistent with flavor state of left-handed neutrino rather than mixture of flavor state. The transition probability between right-handed neutrinos due to mixture can be explained well by boson intermediating flavor flip interaction between right-handed neutrinos and neutrino oscillation can be considered macro phenomenon before all flavor flip interactions arrive at balance.","sentences":["This paper assumes that neutrino flavor conversion is induced by right-handed neutrino mixture via seesaw mechanism, which leads to apparent fake neutrino mixture with neutrino mass eigenstate consistent with flavor state of left-handed neutrino rather than mixture of flavor state.","The transition probability between right-handed neutrinos due to mixture can be explained well by boson intermediating flavor flip interaction between right-handed neutrinos and neutrino oscillation can be considered macro phenomenon before all flavor flip interactions arrive at balance."],"url":"http://arxiv.org/abs/2405.17105v1","category":"hep-ph"}
{"created":"2024-05-27 12:16:04","title":"Element-specific ultrafast lattice dynamics in monolayer WSe2","abstract":"We study monolayer WSe2 using ultrafast electron diffraction. We introduce an approach to quantitatively extract atomic-site-specific information, providing an element-specific view of incoherent atomic vibrations following femtosecond excitation. Via differences between W and Se vibrations, we identify stages in the nonthermal evolution of the lattice. Combined with a calculated phonon dispersion, this element specificity enables us to identify a long-lasting overpopulation of specific optical phonons, and to interpret the stages as energy transfer processes between specific phonon groups. These results demonstrate the appeal of resolving element-specific vibrational information in the ultrafast time domain.","sentences":["We study monolayer WSe2 using ultrafast electron diffraction.","We introduce an approach to quantitatively extract atomic-site-specific information, providing an element-specific view of incoherent atomic vibrations following femtosecond excitation.","Via differences between W and Se vibrations, we identify stages in the nonthermal evolution of the lattice.","Combined with a calculated phonon dispersion, this element specificity enables us to identify a long-lasting overpopulation of specific optical phonons, and to interpret the stages as energy transfer processes between specific phonon groups.","These results demonstrate the appeal of resolving element-specific vibrational information in the ultrafast time domain."],"url":"http://arxiv.org/abs/2405.17099v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-27 12:10:08","title":"Large-amplitude transverse MHD waves prevailing in the H$\u03b1$ chromosphere of a solar quiet region revealed by MiHI integrated field spectral observations","abstract":"The investigation of plasma motions in the solar chromosphere is crucial for understanding the transport of mechanical energy from the interior of the Sun to the outer atmosphere and into interplanetary space. We report the finding of large-amplitude oscillatory transverse motions prevailing in the non-spicular Halpha chromosphere of a small quiet region near the solar disk center. The observation was carried out on 2018 August 25 with the Microlensed Hyperspectral Imager (MiHI) installed as an extension to the spectrograph at the Swedish Solar Telescope (SST). MiHi produced high-resolution Stokes spectra of the Halpha line over a two-dimensional array of points (sampled every 0.066 arcsec on the image plane) every 1.33 s for about 17 min. We extracted the Dopple-shift-insensitive intensity data of the line core by applying a bisector fit to Stoke I line profiles. From our time-distance analysis of the intensity data, we find a variety of transverse motions with velocity amplitudes of up to 40 km/s in fan fibrils and tiny filaments. In particular, in the fan fibrils, large-amplitude transverse MHD waves were seen to occur with a mean velocity amplitude of 25 km/s and a mean period of 5.8 min, propagating at a speed of 40 km/s. These waves are nonlinear and display group behavior. We estimate the wave energy flux in the upper chromosphere at 3 x 10^6 erg cm^-2 s^-1. Our results contribute to the advancement of our understanding of the properties of transverse MHD waves in the solar chromosphere.","sentences":["The investigation of plasma motions in the solar chromosphere is crucial for understanding the transport of mechanical energy from the interior of the Sun to the outer atmosphere and into interplanetary space.","We report the finding of large-amplitude oscillatory transverse motions prevailing in the non-spicular Halpha chromosphere of a small quiet region near the solar disk center.","The observation was carried out on 2018 August 25 with the Microlensed Hyperspectral Imager (MiHI) installed as an extension to the spectrograph at the Swedish Solar Telescope (SST).","MiHi produced high-resolution Stokes spectra of the Halpha line over a two-dimensional array of points (sampled every 0.066 arcsec on the image plane) every 1.33 s for about 17 min.","We extracted the Dopple-shift-insensitive intensity data of the line core by applying a bisector fit to Stoke I line profiles.","From our time-distance analysis of the intensity data, we find a variety of transverse motions with velocity amplitudes of up to 40 km/s in fan fibrils and tiny filaments.","In particular, in the fan fibrils, large-amplitude transverse MHD waves were seen to occur with a mean velocity amplitude of 25 km/s and a mean period of 5.8 min, propagating at a speed of 40 km/s. These waves are nonlinear and display group behavior.","We estimate the wave energy flux in the upper chromosphere at 3 x 10^6 erg cm^-2 s^-1.","Our results contribute to the advancement of our understanding of the properties of transverse MHD waves in the solar chromosphere."],"url":"http://arxiv.org/abs/2405.17095v1","category":"astro-ph.SR"}
{"created":"2024-05-27 11:55:56","title":"A short note on nowhere smooth critical points of polyconvex functionals in arbitrary dimension","abstract":"For any $M, n \\geq 2$ and any open set $\\Omega \\subset \\mathbb{R}^n$ we find a smooth, strongly polyconvex function $F\\colon \\mathbb{R}^{M\\times n}\\to \\mathbb{R}$ and a Lipschitz map $u\\colon \\mathbb{R}^n \\to \\mathbb{R}^M$ that is a weak local minimizer of the energy \\[   \\int_{\\Omega} F(Du). \\] but with nowhere continuous partial derivatives. This extends celebrated results by M\\\"uller-Sver\\'ak and Sz\\'ekelyhidi to higher dimensions.","sentences":["For any $M, n \\geq 2$ and any open set $\\Omega \\subset \\mathbb{R}^n$","we find a smooth, strongly polyconvex function $F\\colon \\mathbb{R}^{M\\times n}\\to \\mathbb{R}$ and a Lipschitz map $u\\colon \\mathbb{R}^n \\to \\mathbb{R}^M$ that is a weak local minimizer of the energy \\[   \\int_{\\Omega} F(Du).","\\]","but with nowhere continuous partial derivatives.","This extends celebrated results by M\\\"uller-Sver\\'ak and Sz\\'ekelyhidi to higher dimensions."],"url":"http://arxiv.org/abs/2405.17084v1","category":"math.AP"}
{"created":"2024-05-27 11:51:53","title":"The Ca II K index-Mg II index relation: A Hilbert-Huang Transform approach","abstract":"The solar activity, which is driven by a variable magnetic field, exhibits changes along several time scales, the 11-year being the most known. In addition to the SunSpot Number, the Ca II K index and the Mg II index are indices widely employed among those proposed to quantify the solar activity, also because of their ability to trace the solar UV emission. In this work, we compare the Ca II K 0.1nm emission index to the Mg II index over the time interval 1978-2017, which covers almost four solar cycles. We show that they are strongly correlated across each solar cycle (r$\\geq$0.94), providing the corresponding linear regression fit parameters. The Hilbert-Huang Transform is then used to decompose such indices into their intrinsic mode of oscillation. By studying how their components are correlated over the different time scales, it is found that the maximum correlation is observed at the 11-year scale, while the correlation is less strong going to smaller time scales.","sentences":["The solar activity, which is driven by a variable magnetic field, exhibits changes along several time scales, the 11-year being the most known.","In addition to the SunSpot Number, the Ca II K index and the Mg II index are indices widely employed among those proposed to quantify the solar activity, also because of their ability to trace the solar UV emission.","In this work, we compare the Ca II K 0.1nm emission index to the Mg II index over the time interval 1978-2017, which covers almost four solar cycles.","We show that they are strongly correlated across each solar cycle (r$\\geq$0.94), providing the corresponding linear regression fit parameters.","The Hilbert-Huang Transform is then used to decompose such indices into their intrinsic mode of oscillation.","By studying how their components are correlated over the different time scales, it is found that the maximum correlation is observed at the 11-year scale, while the correlation is less strong going to smaller time scales."],"url":"http://arxiv.org/abs/2405.17078v1","category":"astro-ph.SR"}
{"created":"2024-05-27 11:50:58","title":"High Redshift Merger Model for Low Frequency Gravitational Wave Background","abstract":"In 2023, the Pulsar Timing Array (PTA) Collaborations announced the discovery of a gravitational wave background (GWB), predominantly attributed to supermassive black hole binary (SMBHB) mergers. However, the detected GWB is several times stronger than the default value expected from galactic observations at low and moderate redshifts. Recent findings by the James Webb Space Telescope (JWST) have unveiled a substantial number of massive, high-redshift galaxies, suggesting more massive SMBHB mergers at these early epochs. Motivated by these findings, we propose an \"early merger\" model that complements the standard merger statistics by incorporating these early, massive galaxies. We compare the early and standard \"late merger\" models, which assume peak merger rates in the local universe, and match both merger models to the current detected GWB. Our analysis shows that the early merger model has a significantly lower detection probability for single binaries and predicts a $\\sim 30 \\%$ likelihood that the first detectable single source will be highly redshifted and remarkably massive with rapid frequency evolution. In contrast, the late merger model predicts a nearly monochromatic first source at low redshift. The future confirmation of an enhanced population of massive high-redshift galaxies and the detection of fast-evolving binaries would strongly support the early merger model, offering significant insights into galaxy and SMBH redshift evolution.","sentences":["In 2023, the Pulsar Timing Array (PTA) Collaborations announced the discovery of a gravitational wave background (GWB), predominantly attributed to supermassive black hole binary (SMBHB) mergers.","However, the detected GWB is several times stronger than the default value expected from galactic observations at low and moderate redshifts.","Recent findings by the James Webb Space Telescope (JWST) have unveiled a substantial number of massive, high-redshift galaxies, suggesting more massive SMBHB mergers at these early epochs.","Motivated by these findings, we propose an \"early merger\" model that complements the standard merger statistics by incorporating these early, massive galaxies.","We compare the early and standard \"late merger\" models, which assume peak merger rates in the local universe, and match both merger models to the current detected GWB.","Our analysis shows that the early merger model has a significantly lower detection probability for single binaries and predicts a $\\sim 30 \\%$","likelihood that the first detectable single source will be highly redshifted and remarkably massive with rapid frequency evolution.","In contrast, the late merger model predicts a nearly monochromatic first source at low redshift.","The future confirmation of an enhanced population of massive high-redshift galaxies and the detection of fast-evolving binaries would strongly support the early merger model, offering significant insights into galaxy and SMBH redshift evolution."],"url":"http://arxiv.org/abs/2405.17077v1","category":"astro-ph.GA"}
{"created":"2024-05-27 10:48:47","title":"Claw-free minimal matching covered graphs","abstract":"A matching covered graph $G$ is minimal if for each edge $e$ of $G$, $G-e$ is not matching covered. An edge $e$ of a matching covered graph $G$ is removable if $G-e$ is also matching covered. Thus a matching covered graph is minimal if and only if it is free of removable edges. For bipartite graphs, Lov\\'{a}sz and Plummer gave a characterization of bipartite minimal matching covered graphs. For bricks, Lov\\'{a}sz showed that the only bricks that are minimal matching covered are $K_4$ and $\\overline{C_6}$. In this paper, we present a complete characterization of minimal matching covered graphs that are claw-free. Moreover, for cubic claw-free matching covered graphs that are not minimal matching covered, we obtain the number of their removable edges (with respect to their bricks), and then prove that they have at least 12 removable edges (the bound is sharp).","sentences":["A matching covered graph $G$ is minimal if for each edge $e$ of $G$, $G-e$ is not matching covered.","An edge $e$ of a matching covered graph $G$ is removable if $G-e$ is also matching covered.","Thus a matching covered graph is minimal if and only if it is free of removable edges.","For bipartite graphs, Lov\\'{a}sz and Plummer gave a characterization of bipartite minimal matching covered graphs.","For bricks, Lov\\'{a}sz showed that the only bricks that are minimal matching covered are $K_4$ and $\\overline{C_6}$. In this paper, we present a complete characterization of minimal matching covered graphs that are claw-free.","Moreover, for cubic claw-free matching covered graphs that are not minimal matching covered, we obtain the number of their removable edges (with respect to their bricks), and then prove that they have at least 12 removable edges (the bound is sharp)."],"url":"http://arxiv.org/abs/2405.17040v1","category":"math.CO"}
{"created":"2024-05-27 09:38:50","title":"Concentration and fluctuation phenomena in the localized phase of the pinning model","abstract":"We revisit and expand the analysis in [15] of the localized phase of disordered pinning models. The arguments are developed for i.i.d. site disorder on which we assume only that the moment generating function is bounded in a neighborhood of the origin. Quantitative $C^\\infty$ estimates on the free energy density are established, showing in particular that the regularity class is at least Gevrey-3. After explaining how a quenched concentration bound and the quenched Central Limit Theorem (CLT) on the number of the pinned sites, i.e., the contact number, can be extracted from the regularity estimates on the free energy, we establish a quenched Local CLT for the same quantity. The centering in these CLTs is random in the sense that it is disorder dependent and a concentration bound and the CLT are established also for the centering sequence, as well as a Hardy-Littlewood random walk type-estimate for its fluctuations.","sentences":["We revisit and expand the analysis in [15] of the localized phase of disordered pinning models.","The arguments are developed for i.i.d. site disorder on which we assume only that the moment generating function is bounded in a neighborhood of the origin.","Quantitative $C^\\infty$ estimates on the free energy density are established, showing in particular that the regularity class is at least Gevrey-3.","After explaining how a quenched concentration bound and the quenched Central Limit Theorem (CLT) on the number of the pinned sites, i.e., the contact number, can be extracted from the regularity estimates on the free energy, we establish a quenched Local CLT for the same quantity.","The centering in these CLTs is random in the sense that it is disorder dependent and a concentration bound and the CLT are established also for the centering sequence, as well as a Hardy-Littlewood random walk type-estimate for its fluctuations."],"url":"http://arxiv.org/abs/2405.16991v1","category":"math.PR"}
{"created":"2024-05-27 09:13:58","title":"Hydrodynamics and the eigenstate thermalization hypothesis","abstract":"The eigenstate thermalization hypothesis (ETH) describes the properties of diagonal and off-diagonal matrix elements of local operators in the eigenenergy basis. In this work, we propose a relation between (i) the singular behaviour of the off-diagonal part of ETH at small energy differences, and (ii) the smooth profile of the diagonal part of ETH as a function of the energy density. We establish this connection from the decay of the autocorrelation functions of local operators, which is constrained by the presence of local conserved quantities whose evolution is described by hydrodynamics. We corroborate our predictions with numerical simulations of two non-integrable spin-1 Ising models, one diffusive and one super-diffusive, which we perform using dynamical quantum typicality up to 18 spins.","sentences":["The eigenstate thermalization hypothesis (ETH) describes the properties of diagonal and off-diagonal matrix elements of local operators in the eigenenergy basis.","In this work, we propose a relation between (i) the singular behaviour of the off-diagonal part of ETH at small energy differences, and (ii) the smooth profile of the diagonal part of ETH as a function of the energy density.","We establish this connection from the decay of the autocorrelation functions of local operators, which is constrained by the presence of local conserved quantities whose evolution is described by hydrodynamics.","We corroborate our predictions with numerical simulations of two non-integrable spin-1","Ising models, one diffusive and one super-diffusive, which we perform using dynamical quantum typicality up to 18 spins."],"url":"http://arxiv.org/abs/2405.16975v1","category":"quant-ph"}
{"created":"2024-05-27 08:46:46","title":"Jet substructure measurements in heavy-ion collisions","abstract":"Jet substructure in heavy-ion collisions is a rapidly evolving area with lots of intriguing new measurements. This contribution presents a selection of recent jet-substructure measurements from experiments at the LHC, in particular, soft-drop groomed radii of jets and reclustered large-radius jets from ATLAS, jet axis difference and generalized jet angularities from ALICE, as well as dijet shapes and b-jet shapes from the CMS experiment.","sentences":["Jet substructure in heavy-ion collisions is a rapidly evolving area with lots of intriguing new measurements.","This contribution presents a selection of recent jet-substructure measurements from experiments at the LHC, in particular, soft-drop groomed radii of jets and reclustered large-radius jets from ATLAS, jet axis difference and generalized jet angularities from ALICE, as well as dijet shapes and b-jet shapes from the CMS experiment."],"url":"http://arxiv.org/abs/2405.16955v1","category":"nucl-ex"}
{"created":"2024-05-27 08:30:16","title":"Exploring the Dark Sector of the Extended FNSM at the LHC","abstract":"We establish the possibility of having a pseudo-Nambu-Goldstone boson (pNGB) Dark Matter (DM) candidate in the Froggatt-Nielsen Singlet Model (FNSM) wherein a direct connection exists between the DM mass and flavon symmetry-breaking scale. We find a considerable allowed region of parameter space for the ensuing pseudoscalar DM, which is dependent upon the flavon Vacuum Expectation Value (VEV) and Yukawa couplings, over which it may be possible to explain the fermion mass hierarchy. Finally, we choose a Benchmark Point (BP) and perform detailed collider analyses to probe this DM state in the context of Run 3 of the Large Hadron Collider (LHC). Specifically, in this model, one obtains large missing transverse energy ($\\slashed{E}_T$) when the DM particle is resonantly produced from the decay of a heavy Higgs field, along with multiple jets from Initial State Radiation (ISR). Thus, the ensuing $\\slashed{E}_T$ + $n\\, {\\rm jets}~(n \\geq 1)$ signature is an excellent probe of DM in this construct.","sentences":["We establish the possibility of having a pseudo-Nambu-Goldstone boson (pNGB) Dark Matter (DM) candidate in the Froggatt-Nielsen Singlet Model (FNSM) wherein a direct connection exists between the DM mass and flavon symmetry-breaking scale.","We find a considerable allowed region of parameter space for the ensuing pseudoscalar DM, which is dependent upon the flavon Vacuum Expectation Value (VEV) and Yukawa couplings, over which it may be possible to explain the fermion mass hierarchy.","Finally, we choose a Benchmark Point (BP) and perform detailed collider analyses to probe this DM state in the context of Run 3 of the Large Hadron Collider (LHC).","Specifically, in this model, one obtains large missing transverse energy ($\\slashed{E}_T$) when the DM particle is resonantly produced from the decay of a heavy Higgs field, along with multiple jets from Initial State Radiation (ISR).","Thus, the ensuing $\\slashed{E}_T$ + $n\\, {\\rm jets}~(n \\geq 1)$ signature is an excellent probe of DM in this construct."],"url":"http://arxiv.org/abs/2405.16939v1","category":"hep-ph"}
{"created":"2024-05-27 08:29:06","title":"Effect of Halogen Substituents on Charge Transport Properties of n-type Organic Semiconductors: A Theoretical Study","abstract":"Organic semiconductors (OSCs) have received much attention as promising materials for electronic devices. In this study, we investigate the impact of halogen groups on the charge transport properties of n-type OSC-6,13 bis ((triisopropylsilyl) ethynyl)-5,7,12,14-tetraazapentacene (TIPS-TAP). The computed mobilities for TAPs substituted with F and Cl exhibit excellent agreement with the experimental values, while the simulation overestimates the electron mobility for TAP. Interestingly, the mobility of TIPS-TAP-4F is significantly lower than that of TIPS-TAP-4Cl/Br, despite their similar packing structures. This discrepancy can be attributed to the strong electron-withdrawing effect of fluoride, leading to reduced electron transfer integrals and increased reorganization energy. While molecular packing is widely accepted as a dominant factor in charge transport in OSCs, our study highlights the essential role of electronic effects in OSC charge transport. This study provides new insights into the understanding of the charge transport mechanism in OSCs.","sentences":["Organic semiconductors (OSCs) have received much attention as promising materials for electronic devices.","In this study, we investigate the impact of halogen groups on the charge transport properties of n-type OSC-6,13 bis ((triisopropylsilyl) ethynyl)-5,7,12,14-tetraazapentacene (TIPS-TAP).","The computed mobilities for TAPs substituted with F and Cl exhibit excellent agreement with the experimental values, while the simulation overestimates the electron mobility for TAP.","Interestingly, the mobility of TIPS-TAP-4F is significantly lower than that of TIPS-TAP-4Cl/Br, despite their similar packing structures.","This discrepancy can be attributed to the strong electron-withdrawing effect of fluoride, leading to reduced electron transfer integrals and increased reorganization energy.","While molecular packing is widely accepted as a dominant factor in charge transport in OSCs, our study highlights the essential role of electronic effects in OSC charge transport.","This study provides new insights into the understanding of the charge transport mechanism in OSCs."],"url":"http://arxiv.org/abs/2405.16937v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-27 08:12:35","title":"Thin-wall vacuum decay in the presence of a compact dimension","abstract":"We study the problem of false vacuum decay in arbitrary dimensions, in the presence of gravity, and compute the transition probability within the thin-wall approximation, generalising the results of Coleman and de Luccia. In the particular case of one compact dimension, we present explicit formulae for the Euclidean Bounce configuration that drives the transition from a de Sitter to Minkowski or from a Minkowski to anti-de Sitter vacua.","sentences":["We study the problem of false vacuum decay in arbitrary dimensions, in the presence of gravity, and compute the transition probability within the thin-wall approximation, generalising the results of Coleman and de Luccia.","In the particular case of one compact dimension, we present explicit formulae for the Euclidean Bounce configuration that drives the transition from a de Sitter to Minkowski or from a Minkowski to anti-de Sitter vacua."],"url":"http://arxiv.org/abs/2405.16920v1","category":"hep-th"}
{"created":"2024-05-27 06:37:21","title":"Active gel model for one-dimensional cell migration coupling actin flow and adhesion dynamics","abstract":"Migration of animal cells is based on the interplay between actin polymerization at the front, adhesion along the cell-substrate interface, and actomyosin contractility at the back. Active gel theory has been used before to demonstrate that actomyosin contractility is sufficient for polarization and self-sustained cell migration in the absence of external cues, but did not consider the dynamics of adhesion. Likewise, migration models based on the mechanosensitive dynamics of adhesion receptors usually do not include the global dynamics of intracellular flow. Here we show that both aspects can be combined in a minimal active gel model for one-dimensional cell migration with dynamic adhesion. This model demonstrates that load sharing between the adhesion receptors leads to symmetry breaking, with stronger adhesion at the front, and that bistability of migration arises for intermediate adhesiveness. Local variations in adhesiveness are sufficient to switch between sessile and motile states, in qualitative agreement with experiments.","sentences":["Migration of animal cells is based on the interplay between actin polymerization at the front, adhesion along the cell-substrate interface, and actomyosin contractility at the back.","Active gel theory has been used before to demonstrate that actomyosin contractility is sufficient for polarization and self-sustained cell migration in the absence of external cues, but did not consider the dynamics of adhesion.","Likewise, migration models based on the mechanosensitive dynamics of adhesion receptors usually do not include the global dynamics of intracellular flow.","Here we show that both aspects can be combined in a minimal active gel model for one-dimensional cell migration with dynamic adhesion.","This model demonstrates that load sharing between the adhesion receptors leads to symmetry breaking, with stronger adhesion at the front, and that bistability of migration arises for intermediate adhesiveness.","Local variations in adhesiveness are sufficient to switch between sessile and motile states, in qualitative agreement with experiments."],"url":"http://arxiv.org/abs/2405.16870v1","category":"q-bio.CB"}
{"created":"2024-05-27 06:10:43","title":"KMT-2023-BLG-2669: Ninth Free-floating Planet Candidate with $\u03b8_{\\rm E}$ measurements","abstract":"We report a free-floating planet (FFP) candidate identified from the analysis of the microlensing event KMT-2023-BLG-2669. The lensing light curve is characterized by a short duration $(\\lesssim 3\\,{\\rm days})$ and a small amplitude $(\\lesssim 0.7\\,{\\rm mag})$. From the analysis, we find the Einstein timescale of $t_{\\rm E} \\backsimeq 0.33\\,{\\rm days}$ and the Einstein radius of $\\theta_{\\rm E} \\backsimeq 4.41\\,{\\mu}{\\rm as}$. These measurements enable us to infer the lens mass as $M = 8\\,M_{\\oplus} (\\pi_{\\rm rel} / 0.1\\,{\\rm mas})^{-1}$, where $\\pi_{\\rm rel}$ is the relative lens-source parallax. The inference implies that the lens is a sub-Neptune- to Saturn-mass object depending on its unknown distance. This is the ninth isolated planetary-mass microlens with $\\theta_{\\rm E} < 10\\,{\\mu}{\\rm as}$, which (as shown by \\citealt{gould22}) is a useful threshold for a FFP candidate. We conduct extensive searches for possible signals of a host star in the light curve, but find no strong evidence for the host. We discuss the possibility of using late-time high-resolution imaging to probe for possible hosts.","sentences":["We report a free-floating planet (FFP) candidate identified from the analysis of the microlensing event KMT-2023-BLG-2669.","The lensing light curve is characterized by a short duration $(\\lesssim 3\\,{\\rm days})$ and a small amplitude $(\\lesssim 0.7\\,{\\rm mag})$.","From the analysis, we find the Einstein timescale of $t_{\\rm E} \\backsimeq 0.33\\,{\\rm days}$ and the Einstein radius of $\\theta_{\\rm E} \\backsimeq 4.41\\,{\\mu}{\\rm as}$. These measurements enable us to infer the lens mass as $M = 8\\,M_{\\oplus} (\\pi_{\\rm rel} / 0.1\\,{\\rm mas})^{-1}$, where $\\pi_{\\rm rel}$ is the relative lens-source parallax.","The inference implies that the lens is a sub-Neptune- to Saturn-mass object depending on its unknown distance.","This is the ninth isolated planetary-mass microlens with $\\theta_{\\rm E} < 10\\,{\\mu}{\\rm as}$, which (as shown by \\citealt{gould22}) is a useful threshold for a FFP candidate.","We conduct extensive searches for possible signals of a host star in the light curve, but find no strong evidence for the host.","We discuss the possibility of using late-time high-resolution imaging to probe for possible hosts."],"url":"http://arxiv.org/abs/2405.16857v1","category":"astro-ph.EP"}
{"created":"2024-05-27 05:19:36","title":"On the Construction of Singular and Cospectral Hypergraphs","abstract":"In this paper, we define two operations, neighbourhood m-splitting hypergraph $NS_m(\\mathscr{G}^*)$ and non-neighbourhood splitting hypergraph $NNS(\\mathscr{G}^*)$, and obtain several properties of their adjacency spectrum. We also estimate the energies of $NS_m(\\mathscr{G}^*)$ and $NNS(\\mathscr{G}^*)$. Moreover, we introduce two new join operations on $k$-uniform hypergraphs: the neighbourhood splitting V-vertex join $\\mathscr{G}_1^*\\veebar \\mathscr{G}_2^*$ and the S-vertex join $\\mathscr{G}_1^*\\barwedge \\mathscr{G}_2^*$ of hypergraphs $\\mathscr{G}_1^*$ and $\\mathscr{G}_2^*$, and determine their adjacency spectrum. As an application, we obtain infinite families of singular hypergraphs and infinite pairs of non-regular non-isomorphic cospectral hypergraphs.","sentences":["In this paper, we define two operations, neighbourhood m-splitting hypergraph $NS_m(\\mathscr{G}^*)$ and non-neighbourhood splitting hypergraph $NNS(\\mathscr{G}^*)$, and obtain several properties of their adjacency spectrum.","We also estimate the energies of $NS_m(\\mathscr{G}^*)$ and $NNS(\\mathscr{G}^*)$.","Moreover, we introduce two new join operations on $k$-uniform hypergraphs: the neighbourhood splitting V-vertex join $\\mathscr{G}_1^*\\veebar \\mathscr{G}_2^*$ and the S-vertex join $\\mathscr{G}_1^*\\barwedge \\mathscr{G}_2^*$ of hypergraphs $\\mathscr{G}_1^*$ and $\\mathscr{G}_2^*$, and determine their adjacency spectrum.","As an application, we obtain infinite families of singular hypergraphs and infinite pairs of non-regular non-isomorphic cospectral hypergraphs."],"url":"http://arxiv.org/abs/2405.16839v1","category":"math.CO"}
{"created":"2024-05-27 05:01:58","title":"Bulk Reconstruction in De Sitter Spacetime","abstract":"The bulk reconstruction program involves expressing local bulk fields as non-local operators on the boundary. It was initiated in the context of AdS/CFT correspondence. Attempts to extend it to de Sitter have been successful for heavy(principal series) scalar fields. For other fields, the construction ran into issues. In particular, divergences were found to appear for higher spin fields. In this paper, we resolve these issues and obtain boundary representations for scalars of all masses as well as higher spin fields. We trace the origin of the previously discovered divergences and show that the smearing function becomes distributional for certain values of mass, spin and dimension. We also extend the construction from Bunch-Davies vacuum to all $\\alpha$-vacua.","sentences":["The bulk reconstruction program involves expressing local bulk fields as non-local operators on the boundary.","It was initiated in the context of AdS/CFT correspondence.","Attempts to extend it to de Sitter have been successful for heavy(principal series) scalar fields.","For other fields, the construction ran into issues.","In particular, divergences were found to appear for higher spin fields.","In this paper, we resolve these issues and obtain boundary representations for scalars of all masses as well as higher spin fields.","We trace the origin of the previously discovered divergences and show that the smearing function becomes distributional for certain values of mass, spin and dimension.","We also extend the construction from Bunch-Davies vacuum to all $\\alpha$-vacua."],"url":"http://arxiv.org/abs/2405.16832v1","category":"hep-th"}
{"created":"2024-05-27 03:58:51","title":"Geometric Phase in Kitaev Quantum Spin Liquid","abstract":"Quantum spin liquid has massive many spin entanglement in the ground state, we can evaluate it by the entanglement entropy, but the latter can not be observed directly by experiment. In this manuscript, we try to characterize its topological properties by the geometric phase. However the usual adiabatic or non-adiabatic geometric phase can not appear in the density matrix of entanglement entropy, so we extend it to the sub-geometric phase which can exist in the density matrix and have influence on the entanglement entropy, spin correlation function as well as other physical observable. We will demonstrate that the imaginary part of sub-geometric phase will deviate the resonance peak by an amount concerning with this phase and affect the energy level crossing, while the real part of sub-geometric phase will determine the stability of initial state, it may provide a complement on the selection rule of quantum transition.","sentences":["Quantum spin liquid has massive many spin entanglement in the ground state, we can evaluate it by the entanglement entropy, but the latter can not be observed directly by experiment.","In this manuscript, we try to characterize its topological properties by the geometric phase.","However the usual adiabatic or non-adiabatic geometric phase can not appear in the density matrix of entanglement entropy, so we extend it to the sub-geometric phase which can exist in the density matrix and have influence on the entanglement entropy, spin correlation function as well as other physical observable.","We will demonstrate that the imaginary part of sub-geometric phase will deviate the resonance peak by an amount concerning with this phase and affect the energy level crossing, while the real part of sub-geometric phase will determine the stability of initial state, it may provide a complement on the selection rule of quantum transition."],"url":"http://arxiv.org/abs/2405.16808v1","category":"quant-ph"}
{"created":"2024-05-27 03:44:13","title":"The neutrino force in neutrino backgrounds: Spin dependence and parity-violating effects","abstract":"The neutrino force results from the exchange of a pair of neutrinos. A neutrino background can significantly influence this force. In this work, we present a comprehensive calculation of the neutrino force in various neutrino backgrounds with spin dependence taken into account. In particular, we calculate the spin-independent and spin-dependent parity-conserving neutrino forces, in addition to the spin-dependent parity-violating neutrino forces with and without the presence of a neutrino background for both isotropic and anisotropic backgrounds. Compared with the vacuum case, the neutrino background can effectively violate Lorentz invariance and lead to additional parity-violating terms that are not suppressed by the velocity of external particles. We estimate the magnitude of the effect of atomic parity-violation experiments, and it turns out to be well below the current experimental sensitivity.","sentences":["The neutrino force results from the exchange of a pair of neutrinos.","A neutrino background can significantly influence this force.","In this work, we present a comprehensive calculation of the neutrino force in various neutrino backgrounds with spin dependence taken into account.","In particular, we calculate the spin-independent and spin-dependent parity-conserving neutrino forces, in addition to the spin-dependent parity-violating neutrino forces with and without the presence of a neutrino background for both isotropic and anisotropic backgrounds.","Compared with the vacuum case, the neutrino background can effectively violate Lorentz invariance and lead to additional parity-violating terms that are not suppressed by the velocity of external particles.","We estimate the magnitude of the effect of atomic parity-violation experiments, and it turns out to be well below the current experimental sensitivity."],"url":"http://arxiv.org/abs/2405.16801v1","category":"hep-ph"}
{"created":"2024-05-27 03:27:56","title":"X(3872) revisited: the roles of OPEP and the quark degrees of freedom","abstract":"The $X(3872)$ is investigated by employing the quark-hadron hybrid model, that consists of the $c\\bar c$ core, $D^{(*)}\\bar D{}^*$, $J/\\psi\\omega$, and $J/\\psi\\rho$ two-meson states. Due to the attraction from the $c\\bar c$-$D\\bar D{}^*$ coupling and from the OPEP tensor coupling, a very thin peak can appear at the $D^{0}\\bar D{}^{*0}$ threshold. The energy of the corresponding pole of the scattering matrix is $E=(+0.06-0.14i)$ MeV, which is on the physical sheet and above the threshold, the same as the one of the poles from the LHCb data analysis.","sentences":["The $X(3872)$ is investigated by employing the quark-hadron hybrid model, that consists of the $c\\bar c$ core, $D^{(*)}\\bar D{}^*$, $J/\\psi\\omega$, and $J/\\psi\\rho$ two-meson states.","Due to the attraction from the $c\\bar c$-$D\\bar D{}^*$ coupling and from the OPEP tensor coupling, a very thin peak can appear at the $D^{0}\\bar D{}^{*0}$ threshold.","The energy of the corresponding pole of the scattering matrix is $E=(+0.06-0.14i)$ MeV, which is on the physical sheet and above the threshold, the same as the one of the poles from the LHCb data analysis."],"url":"http://arxiv.org/abs/2405.16794v1","category":"hep-ph"}
{"created":"2024-05-27 03:27:37","title":"Multiphoton quantum sensing","abstract":"While the fundamental principles of light-matter interaction are well-understood and drive countless technologies, the world of multiphoton processes remains a fascinating puzzle, holding the potential to drastically alter our understanding of how light interacts with matter at its most basic level. This rich interplay of light and matter unveils novel phenomena that can be harnessed for sensing with exceptional precision, as exemplified by multiphoton quantum sensing. This thesis delves into the applications of multiphoton quantum protocols, particularly in imaging, communication, and plasmonic sensing, to surpass classical limitations and achieve enhanced sensitivity. We explore the potential of multiphoton quantum processes, particularly in the nanoscale regime and within subsystems of macroscopic systems, where novel and ultra-sensitive sensing methodologies emerge. Subsequent chapters of this thesis demonstrate the transformative potential of multiphoton quantum sensing, elucidating the design, implementation, and experimental results of specific sensing protocols tailored to diverse applications. Our analysis combines experimental observations and theoretical predictions to assess the sensitivity and performance of these protocols. Additionally, the thesis discusses potential future directions and advancements in the field, envisioning applications in biomolecule detection, environmental monitoring, and fundamental studies of light-matter interactions at the nanoscale. Concluding reflections highlight the implications of multiphoton quantum sensing across scientific disciplines and lay the groundwork for future research endeavors.","sentences":["While the fundamental principles of light-matter interaction are well-understood and drive countless technologies, the world of multiphoton processes remains a fascinating puzzle, holding the potential to drastically alter our understanding of how light interacts with matter at its most basic level.","This rich interplay of light and matter unveils novel phenomena that can be harnessed for sensing with exceptional precision, as exemplified by multiphoton quantum sensing.","This thesis delves into the applications of multiphoton quantum protocols, particularly in imaging, communication, and plasmonic sensing, to surpass classical limitations and achieve enhanced sensitivity.","We explore the potential of multiphoton quantum processes, particularly in the nanoscale regime and within subsystems of macroscopic systems, where novel and ultra-sensitive sensing methodologies emerge.","Subsequent chapters of this thesis demonstrate the transformative potential of multiphoton quantum sensing, elucidating the design, implementation, and experimental results of specific sensing protocols tailored to diverse applications.","Our analysis combines experimental observations and theoretical predictions to assess the sensitivity and performance of these protocols.","Additionally, the thesis discusses potential future directions and advancements in the field, envisioning applications in biomolecule detection, environmental monitoring, and fundamental studies of light-matter interactions at the nanoscale.","Concluding reflections highlight the implications of multiphoton quantum sensing across scientific disciplines and lay the groundwork for future research endeavors."],"url":"http://arxiv.org/abs/2405.16793v1","category":"physics.optics"}
{"created":"2024-05-27 03:10:43","title":"Quantum equivalence of the Freedman-Townsend model and the principal chiral $\u03c3$-model","abstract":"The Freedman-Townsend model is quantized using the Batalin-Vilkovisky approach to Lagrangian quantization of gauge theories with linearly dependent generators. Path integral arguments are then applied to demonstrate the quantum equivalence of the Freedman-Townsend model to the principal chiral $\\sigma$-model.","sentences":["The Freedman-Townsend model is quantized using the Batalin-Vilkovisky approach to Lagrangian quantization of gauge theories with linearly dependent generators.","Path integral arguments are then applied to demonstrate the quantum equivalence of the Freedman-Townsend model to the principal chiral $\\sigma$-model."],"url":"http://arxiv.org/abs/2405.16782v1","category":"hep-th"}
{"created":"2024-05-27 02:42:33","title":"ARC: A Generalist Graph Anomaly Detector with In-Context Learning","abstract":"Graph anomaly detection (GAD), which aims to identify abnormal nodes that differ from the majority within a graph, has garnered significant attention. However, current GAD methods necessitate training specific to each dataset, resulting in high training costs, substantial data requirements, and limited generalizability when being applied to new datasets and domains. To address these limitations, this paper proposes ARC, a generalist GAD approach that enables a ``one-for-all'' GAD model to detect anomalies across various graph datasets on-the-fly. Equipped with in-context learning, ARC can directly extract dataset-specific patterns from the target dataset using few-shot normal samples at the inference stage, without the need for retraining or fine-tuning on the target dataset. ARC comprises three components that are well-crafted for capturing universal graph anomaly patterns: 1) smoothness-based feature Alignment module that unifies the features of different datasets into a common and anomaly-sensitive space; 2) ego-neighbor Residual graph encoder that learns abnormality-related node embeddings; and 3) cross-attentive in-Context anomaly scoring module that predicts node abnormality by leveraging few-shot normal samples. Extensive experiments on multiple benchmark datasets from various domains demonstrate the superior anomaly detection performance, efficiency, and generalizability of ARC.","sentences":["Graph anomaly detection (GAD), which aims to identify abnormal nodes that differ from the majority within a graph, has garnered significant attention.","However, current GAD methods necessitate training specific to each dataset, resulting in high training costs, substantial data requirements, and limited generalizability when being applied to new datasets and domains.","To address these limitations, this paper proposes ARC, a generalist GAD approach that enables a ``one-for-all'' GAD model to detect anomalies across various graph datasets on-the-fly.","Equipped with in-context learning, ARC can directly extract dataset-specific patterns from the target dataset using few-shot normal samples at the inference stage, without the need for retraining or fine-tuning on the target dataset.","ARC comprises three components that are well-crafted for capturing universal graph anomaly patterns: 1) smoothness-based feature Alignment module that unifies the features of different datasets into a common and anomaly-sensitive space; 2) ego-neighbor Residual graph encoder that learns abnormality-related node embeddings; and 3) cross-attentive in-Context anomaly scoring module that predicts node abnormality by leveraging few-shot normal samples.","Extensive experiments on multiple benchmark datasets from various domains demonstrate the superior anomaly detection performance, efficiency, and generalizability of ARC."],"url":"http://arxiv.org/abs/2405.16771v1","category":"cs.LG"}
{"created":"2024-05-27 02:32:34","title":"Time-dependent complex variable solution on quasi three-dimensional shallow tunnelling in gravititational geomaterial with reasonable far-field displacement","abstract":"Three-dimensional effect of tunnel face and gravitational excavation generally occur in shallow tunnelling, which are nevertheless not adequately considered in present complex variable solutions. In this paper, a new time-dependent complex variable solution on quasi three-dimensional shallow tunnelling in gravitational geomaterial is derived, and the far-field displacement singularity is eliminated by fixed far-field ground surface in the whole excavation time span. With an equivalent coefficient of three-dimensional effect, the quasi three-dimensional shallow tunnelling is transformed into a plane strain problem with time-dependent virtual traction along tunnel periphery. The mixed boundaries of fixed far-field ground surface and nearby free segment form a homogenerous Riemann-Hilbert problem with extra constraints of the virtual traction along tunnel periphery, which is simultaneously solved using an iterative linear system with good numerical stability. The mixed boundary conditions along the ground surface in the whole excavation time span are well satisified in a numerical case, which is further examined by comparing with corresponding finite element solution. The results are in good agreements, and the proposed solution illustrates high efficiency. More discussions are made on excavation rate, viscosity, and solution convergence. A latent paradox is disclosed for objectivity.","sentences":["Three-dimensional effect of tunnel face and gravitational excavation generally occur in shallow tunnelling, which are nevertheless not adequately considered in present complex variable solutions.","In this paper, a new time-dependent complex variable solution on quasi three-dimensional shallow tunnelling in gravitational geomaterial is derived, and the far-field displacement singularity is eliminated by fixed far-field ground surface in the whole excavation time span.","With an equivalent coefficient of three-dimensional effect, the quasi three-dimensional shallow tunnelling is transformed into a plane strain problem with time-dependent virtual traction along tunnel periphery.","The mixed boundaries of fixed far-field ground surface and nearby free segment form a homogenerous Riemann-Hilbert problem with extra constraints of the virtual traction along tunnel periphery, which is simultaneously solved using an iterative linear system with good numerical stability.","The mixed boundary conditions along the ground surface in the whole excavation time span are well satisified in a numerical case, which is further examined by comparing with corresponding finite element solution.","The results are in good agreements, and the proposed solution illustrates high efficiency.","More discussions are made on excavation rate, viscosity, and solution convergence.","A latent paradox is disclosed for objectivity."],"url":"http://arxiv.org/abs/2405.16768v1","category":"math.NA"}
{"created":"2024-05-27 01:43:19","title":"Realization of 2/3-layer transition metal dichalcogenides","abstract":"Layered van der Waals transition metal dichalcogenides (TMDCs), generally composed of three atomic X-M-X planes in each layer (M = transition metal, X = chalcogen), provide versatile platforms for exploring diverse quantum phenomena. In each MX2 layer, the M-X bonds are predominantly covalent in nature, as a result, the cleavage of TMDC crystals always occurring between the layers. Here we report the controllable realization of fractional-layer WTe2 via an in-situ scanning tunnelling microscopy (STM) tip manipulation technique. By applying STM tip pulses, hundreds of the topmost Te atoms are removed to form a nanoscale monolayer Te pit in the 1T'-WTe2, thus realizing a brand-new 2/3-layer WTe2. Such a unique configuration undergoes a spontaneous atomic reconstruction, yielding an energy-dependent unidirectional charge-density-wave state with the wavevector and geometry quite distinct from that of pristine 1T'-WTe2. Our results expand the conventional understanding of the TMDCs and are expected to stimulate the research on extraordinary structures and properties based on fractional-layer TMDCs.","sentences":["Layered van der Waals transition metal dichalcogenides (TMDCs), generally composed of three atomic X-M-X planes in each layer (M = transition metal, X = chalcogen), provide versatile platforms for exploring diverse quantum phenomena.","In each MX2 layer, the M-X bonds are predominantly covalent in nature, as a result, the cleavage of TMDC crystals always occurring between the layers.","Here we report the controllable realization of fractional-layer WTe2 via an in-situ scanning tunnelling microscopy (STM) tip manipulation technique.","By applying STM tip pulses, hundreds of the topmost Te atoms are removed to form a nanoscale monolayer","Te pit in the 1T'-WTe2, thus realizing a brand-new 2/3-layer WTe2.","Such a unique configuration undergoes a spontaneous atomic reconstruction, yielding an energy-dependent unidirectional charge-density-wave state with the wavevector and geometry quite distinct from that of pristine 1T'-WTe2.","Our results expand the conventional understanding of the TMDCs and are expected to stimulate the research on extraordinary structures and properties based on fractional-layer TMDCs."],"url":"http://arxiv.org/abs/2405.16750v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-27 01:38:30","title":"DMPlug: A Plug-in Method for Solving Inverse Problems with Diffusion Models","abstract":"Pretrained diffusion models (DMs) have recently been popularly used in solving inverse problems (IPs). The existing methods mostly interleave iterative steps in the reverse diffusion process and iterative steps to bring the iterates closer to satisfying the measurement constraint. However, such interleaving methods struggle to produce final results that look like natural objects of interest (i.e., manifold feasibility) and fit the measurement (i.e., measurement feasibility), especially for nonlinear IPs. Moreover, their capabilities to deal with noisy IPs with unknown types and levels of measurement noise are unknown. In this paper, we advocate viewing the reverse process in DMs as a function and propose a novel plug-in method for solving IPs using pretrained DMs, dubbed DMPlug. DMPlug addresses the issues of manifold feasibility and measurement feasibility in a principled manner, and also shows great potential for being robust to unknown types and levels of noise. Through extensive experiments across various IP tasks, including two linear and three nonlinear IPs, we demonstrate that DMPlug consistently outperforms state-of-the-art methods, often by large margins especially for nonlinear IPs. The code is available at https://github.com/sun-umn/DMPlug.","sentences":["Pretrained diffusion models (DMs) have recently been popularly used in solving inverse problems (IPs).","The existing methods mostly interleave iterative steps in the reverse diffusion process and iterative steps to bring the iterates closer to satisfying the measurement constraint.","However, such interleaving methods struggle to produce final results that look like natural objects of interest (i.e., manifold feasibility) and fit the measurement (i.e., measurement feasibility), especially for nonlinear IPs.","Moreover, their capabilities to deal with noisy IPs with unknown types and levels of measurement noise are unknown.","In this paper, we advocate viewing the reverse process in DMs as a function and propose a novel plug-in method for solving IPs using pretrained DMs, dubbed DMPlug.","DMPlug addresses the issues of manifold feasibility and measurement feasibility in a principled manner, and also shows great potential for being robust to unknown types and levels of noise.","Through extensive experiments across various IP tasks, including two linear and three nonlinear IPs, we demonstrate that DMPlug consistently outperforms state-of-the-art methods, often by large margins especially for nonlinear IPs.","The code is available at https://github.com/sun-umn/DMPlug."],"url":"http://arxiv.org/abs/2405.16749v1","category":"cs.LG"}
{"created":"2024-05-27 01:27:06","title":"Tuning the Electronic and Optical Properties of Impurity-Engineered Two-Dimensional Graphullerene Half-Semiconductors","abstract":"A novel material consisting of a monolayer of C$_{60}$ buckyballs with hexagonal symmetry has recently been observed experimentally, named graphullerene. In this study, we present a comprehensive \\textit{ab-initio} theoretical analysis of the electronic and optical properties of both pristine and impurity-engineered monolayer graphullerene using spin-dependent density functional theory (spin-DFT). Our findings reveal that graphullerene is a direct band gap semiconductor with a band gap of approximately 1.5 eV at the $\\Gamma$ point, agreeing well with experimental data. Notably, we demonstrate that by adding impurities, in particular substitutional nitrogen, substitutional boron, or adsorbent hydrogen, to graphullerene results in the formation of spin-dependent deep donor and deep acceptor levels, thereby giving rise to a variety of half-semiconductors. All the impurities exhibit a magnetic moment of approximately $\\mu_B$ per impurity. This impurity engineering enables the tuning of spin-polarized exciton properties in graphullerene, with spin-dependent band gap energies ranging from 0.43 eV ($\\lambda \\sim$ 2.9 $\\mu$m) to 1.5 eV ($\\lambda \\sim$ 820 nm), covering the near-infrared (NIR) and short-wavelength infrared (SWIR) regimes. Our results suggest that both pristine and impurity-engineered graphullerene have significant potential for the development of carbon-based 2D semiconductor spintronic and opto-spintronic devices.","sentences":["A novel material consisting of a monolayer of C$_{60}$ buckyballs with hexagonal symmetry has recently been observed experimentally, named graphullerene.","In this study, we present a comprehensive \\textit{ab-initio} theoretical analysis of the electronic and optical properties of both pristine and impurity-engineered monolayer graphullerene using spin-dependent density functional theory (spin-DFT).","Our findings reveal that graphullerene is a direct band gap semiconductor with a band gap of approximately 1.5 eV at the $\\Gamma$ point, agreeing well with experimental data.","Notably, we demonstrate that by adding impurities, in particular substitutional nitrogen, substitutional boron, or adsorbent hydrogen, to graphullerene results in the formation of spin-dependent deep donor and deep acceptor levels, thereby giving rise to a variety of half-semiconductors.","All the impurities exhibit a magnetic moment of approximately $\\mu_B$ per impurity.","This impurity engineering enables the tuning of spin-polarized exciton properties in graphullerene, with spin-dependent band gap energies ranging from 0.43 eV ($\\lambda \\sim$ 2.9 $\\mu$m) to 1.5 eV ($\\lambda \\sim$ 820 nm), covering the near-infrared (NIR) and short-wavelength infrared (SWIR) regimes.","Our results suggest that both pristine and impurity-engineered graphullerene have significant potential for the development of carbon-based 2D semiconductor spintronic and opto-spintronic devices."],"url":"http://arxiv.org/abs/2405.16743v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-27 01:26:27","title":"Free energy formulas for confined nematic liquid crystals based on analogies with Kirchhoff-Routh theory in vortex dynamics","abstract":"Active nematics are influenced by alignment angle singularities called topological defects. The localization of these defects is of major interest for biological applications. The total distortion of alignment angles due to defects is evaluated using Frank free energy, which is one of the criteria used to determine the location and stability of these defects. Previous work used the line integrals of a complex potential associated with the alignments for the energy calculation (Miyazako and Nara, R. Soc. Open Sci., 2022), which has a high computational cost. We propose analytical formulas for the free energy in the presence of multiple topological defects in confined geometries. The formulas derived here are an analogue of Kirchhoff-Routh functions in vortex dynamics. The proposed formulas are explicit with respect to the defect locations and conformal maps, which enables the explicit calculation of the energy extrema. The formulas are applied to calculate the locations of defects in so-called doublets and triplets by solving simple polynomial formulas. A stability analysis is also conducted to detect whether defect pairs with charges $\\pm 1/2$ are stable or unstable in triplet regions. Our numerical results are shown to match the experimental results (Ienaga {\\em et al.,} Soft Matter, 2023).","sentences":["Active nematics are influenced by alignment angle singularities called topological defects.","The localization of these defects is of major interest for biological applications.","The total distortion of alignment angles due to defects is evaluated using Frank free energy, which is one of the criteria used to determine the location and stability of these defects.","Previous work used the line integrals of a complex potential associated with the alignments for the energy calculation (Miyazako and Nara, R. Soc.","Open Sci., 2022), which has a high computational cost.","We propose analytical formulas for the free energy in the presence of multiple topological defects in confined geometries.","The formulas derived here are an analogue of Kirchhoff-Routh functions in vortex dynamics.","The proposed formulas are explicit with respect to the defect locations and conformal maps, which enables the explicit calculation of the energy extrema.","The formulas are applied to calculate the locations of defects in so-called doublets and triplets by solving simple polynomial formulas.","A stability analysis is also conducted to detect whether defect pairs with charges $\\pm 1/2$ are stable or unstable in triplet regions.","Our numerical results are shown to match the experimental results (Ienaga {\\em et al.,} Soft Matter, 2023)."],"url":"http://arxiv.org/abs/2405.16742v1","category":"physics.flu-dyn"}
