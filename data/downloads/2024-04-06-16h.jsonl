{"created":"2024-04-03 17:59:59","title":"Hitting the Thermal Target for Leptophilic Dark Matter","abstract":"We study future lepton collider prospects for testing predictive models of leptophilic dark matter candidates with a thermal origin. We calculate experimental milestones for testing the parameter space compatible with freeze-out and the associated collider signals at past, present, and future facilities. This analysis places new limits on such models by leveraging the utility of lepton colliders. At $e^+e^-$ machines, we make projections using precision $Z$-pole observables from $e^+e^-\\to l^+l^- + $ missing energy signatures at LEP and future projections for FCC-ee in these channels. Additionally, a muon collider could also probe new thermal relic parameter space in this scenario via $\\mu^+\\mu^- \\to X + $ missing energy where $X$ is any easy identifiable SM object. Collectively, these processes can probe much all of the parameter space for which DM direct annihilation to $l^+l^-$ yields the observed relic density in Higgs-like models with mass-proportional couplings to charged leptons.","sentences":["We study future lepton collider prospects for testing predictive models of leptophilic dark matter candidates with a thermal origin.","We calculate experimental milestones for testing the parameter space compatible with freeze-out and the associated collider signals at past, present, and future facilities.","This analysis places new limits on such models by leveraging the utility of lepton colliders.","At $e^+e^-$ machines, we make projections using precision $Z$-pole observables from $e^+e^-\\to l^+l^- + $ missing energy signatures at LEP and future projections for FCC-ee in these channels.","Additionally, a muon collider could also probe new thermal relic parameter space in this scenario via $\\mu^+\\mu^- \\to X + $ missing energy where $X$ is any easy identifiable SM object.","Collectively, these processes can probe much all of the parameter space for which DM direct annihilation to $l^+l^-$ yields the observed relic density in Higgs-like models with mass-proportional couplings to charged leptons."],"url":"http://arxiv.org/abs/2404.02906v1","category":"hep-ph"}
{"created":"2024-04-03 17:59:53","title":"Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction","abstract":"We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine \"next-scale prediction\" or \"next-resolution prediction\", diverging from the standard raster-scan \"next-token prediction\". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.","sentences":["We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine \"next-scale prediction\" or \"next-resolution prediction\", diverging from the standard raster-scan \"next-token prediction\".","This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes AR models surpass diffusion transformers in image generation.","On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4, with around 20x faster inference speed.","It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability.","Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence.","VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing.","These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization.","We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning."],"url":"http://arxiv.org/abs/2404.02905v1","category":"cs.CV"}
{"created":"2024-04-03 17:59:36","title":"ALOHa: A New Measure for Hallucination in Captioning Models","abstract":"Despite recent advances in multimodal pre-training for visual description, state-of-the-art models still produce captions containing errors, such as hallucinating objects not present in a scene. The existing prominent metric for object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and synonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa, which leverages large language models (LLMs) to measure object hallucinations. Specifically, we use an LLM to extract groundable objects from a candidate caption, measure their semantic similarity to reference objects from captions and object detections, and use Hungarian matching to produce a final hallucination score. We show that ALOHa correctly identifies 13.6% more hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO Captions annotated for hallucinations, and 30.8% more on nocaps, where objects extend beyond MS COCO categories. Our code is available at https://davidmchan.github.io/aloha/.","sentences":["Despite recent advances in multimodal pre-training for visual description, state-of-the-art models still produce captions containing errors, such as hallucinating objects not present in a scene.","The existing prominent metric for object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and synonyms.","In this work, we propose a modernized open-vocabulary metric, ALOHa, which leverages large language models (LLMs) to measure object hallucinations.","Specifically, we use an LLM to extract groundable objects from a candidate caption, measure their semantic similarity to reference objects from captions and object detections, and use Hungarian matching to produce a final hallucination score.","We show that ALOHa correctly identifies 13.6% more hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO Captions annotated for hallucinations, and 30.8% more on nocaps, where objects extend beyond MS COCO categories.","Our code is available at https://davidmchan.github.io/aloha/."],"url":"http://arxiv.org/abs/2404.02904v1","category":"cs.CV"}
{"created":"2024-04-03 17:58:21","title":"DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets","abstract":"Vision Transformer (ViT) has emerged as a prominent architecture for various computer vision tasks. In ViT, we divide the input image into patch tokens and process them through a stack of self attention blocks. However, unlike Convolutional Neural Networks (CNN), ViTs simple architecture has no informative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a large amount of data for pre-training. Various data efficient approaches (DeiT) have been proposed to train ViT on balanced datasets effectively. However, limited literature discusses the use of ViT for datasets with long-tailed imbalances. In this work, we introduce DeiT-LT to tackle the problem of training ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an efficient and effective way of distillation from CNN via distillation DIST token by using out-of-distribution images and re-weighting the distillation loss to enhance focus on tail classes. This leads to the learning of local CNN-like features in early ViT blocks, improving generalization for tail classes. Further, to mitigate overfitting, we propose distilling from a flat CNN teacher, which leads to learning low-rank generalizable features for DIST tokens across all ViT blocks. With the proposed DeiT-LT scheme, the distillation DIST token becomes an expert on the tail classes, and the classifier CLS token becomes an expert on the head classes. The experts help to effectively learn features corresponding to both the majority and minority classes using a distinct set of tokens within the same ViT architecture. We show the effectiveness of DeiT-LT for training ViT from scratch on datasets ranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018.","sentences":["Vision Transformer (ViT) has emerged as a prominent architecture for various computer vision tasks.","In ViT, we divide the input image into patch tokens and process them through a stack of self attention blocks.","However, unlike Convolutional Neural Networks (CNN), ViTs simple architecture has no informative inductive bias (e.g., locality,etc. ).","Due to this, ViT requires a large amount of data for pre-training.","Various data efficient approaches (DeiT) have been proposed to train ViT on balanced datasets effectively.","However, limited literature discusses the use of ViT for datasets with long-tailed imbalances.","In this work, we introduce DeiT-LT to tackle the problem of training ViTs from scratch on long-tailed datasets.","In DeiT-LT, we introduce an efficient and effective way of distillation from CNN via distillation DIST token by using out-of-distribution images and re-weighting the distillation loss to enhance focus on tail classes.","This leads to the learning of local CNN-like features in early ViT blocks, improving generalization for tail classes.","Further, to mitigate overfitting, we propose distilling from a flat CNN teacher, which leads to learning low-rank generalizable features for DIST tokens across all ViT blocks.","With the proposed DeiT-LT scheme, the distillation DIST token becomes an expert on the tail classes, and the classifier CLS token becomes an expert on the head classes.","The experts help to effectively learn features corresponding to both the majority and minority classes using a distinct set of tokens within the same ViT architecture.","We show the effectiveness of DeiT-LT for training ViT from scratch on datasets ranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018."],"url":"http://arxiv.org/abs/2404.02900v1","category":"cs.CV"}
{"created":"2024-04-03 17:51:18","title":"ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline","abstract":"Large language models (LLMs) have shown excellent mastering of human language, but still struggle in real-world applications that require mathematical problem-solving. While many strategies and datasets to enhance LLMs' mathematics are developed, it remains a challenge to simultaneously maintain and improve both language and mathematical capabilities in deployed LLM systems.In this work, we tailor the Self-Critique pipeline, which addresses the challenge in the feedback learning stage of LLM alignment. We first train a general Math-Critique model from the LLM itself to provide feedback signals. Then, we sequentially employ rejective fine-tuning and direct preference optimization over the LLM's own generations for data collection. Based on ChatGLM3-32B, we conduct a series of experiments on both academic and our newly created challenging dataset, MathUserEval. Results show that our pipeline significantly enhances the LLM's mathematical problem-solving while still improving its language ability, outperforming LLMs that could be two times larger. Related techniques have been deployed to ChatGLM\\footnote{\\url{https://chatglm.cn}}, an online serving LLM. Related evaluation dataset and scripts are released at \\url{https://github.com/THUDM/ChatGLM-Math}.","sentences":["Large language models (LLMs) have shown excellent mastering of human language, but still struggle in real-world applications that require mathematical problem-solving.","While many strategies and datasets to enhance LLMs' mathematics are developed, it remains a challenge to simultaneously maintain and improve both language and mathematical capabilities in deployed LLM systems.","In this work, we tailor the Self-Critique pipeline, which addresses the challenge in the feedback learning stage of LLM alignment.","We first train a general Math-Critique model from the LLM itself to provide feedback signals.","Then, we sequentially employ rejective fine-tuning and direct preference optimization over the LLM's own generations for data collection.","Based on ChatGLM3-32B, we conduct a series of experiments on both academic and our newly created challenging dataset, MathUserEval.","Results show that our pipeline significantly enhances the LLM's mathematical problem-solving while still improving its language ability, outperforming LLMs that could be two times larger.","Related techniques have been deployed to ChatGLM\\footnote{\\url{https://chatglm.cn}}, an online serving LLM.","Related evaluation dataset and scripts are released at \\url{https://github.com/THUDM/ChatGLM-Math}."],"url":"http://arxiv.org/abs/2404.02893v1","category":"cs.CL"}
{"created":"2024-04-03 17:44:02","title":"Steganographic Passport: An Owner and User Verifiable Credential for Deep Model IP Protection Without Retraining","abstract":"Ensuring the legal usage of deep models is crucial to promoting trustable, accountable, and responsible artificial intelligence innovation. Current passport-based methods that obfuscate model functionality for license-to-use and ownership verifications suffer from capacity and quality constraints, as they require retraining the owner model for new users. They are also vulnerable to advanced Expanded Residual Block ambiguity attacks. We propose Steganographic Passport, which uses an invertible steganographic network to decouple license-to-use from ownership verification by hiding the user's identity images into the owner-side passport and recovering them from their respective user-side passports. An irreversible and collision-resistant hash function is used to avoid exposing the owner-side passport from the derived user-side passports and increase the uniqueness of the model signature. To safeguard both the passport and model's weights against advanced ambiguity attacks, an activation-level obfuscation is proposed for the verification branch of the owner's model. By jointly training the verification and deployment branches, their weights become tightly coupled. The proposed method supports agile licensing of deep models by providing a strong ownership proof and license accountability without requiring a separate model retraining for the admission of every new user. Experiment results show that our Steganographic Passport outperforms other passport-based deep model protection methods in robustness against various known attacks.","sentences":["Ensuring the legal usage of deep models is crucial to promoting trustable, accountable, and responsible artificial intelligence innovation.","Current passport-based methods that obfuscate model functionality for license-to-use and ownership verifications suffer from capacity and quality constraints, as they require retraining the owner model for new users.","They are also vulnerable to advanced Expanded Residual Block ambiguity attacks.","We propose Steganographic Passport, which uses an invertible steganographic network to decouple license-to-use from ownership verification by hiding the user's identity images into the owner-side passport and recovering them from their respective user-side passports.","An irreversible and collision-resistant hash function is used to avoid exposing the owner-side passport from the derived user-side passports and increase the uniqueness of the model signature.","To safeguard both the passport and model's weights against advanced ambiguity attacks, an activation-level obfuscation is proposed for the verification branch of the owner's model.","By jointly training the verification and deployment branches, their weights become tightly coupled.","The proposed method supports agile licensing of deep models by providing a strong ownership proof and license accountability without requiring a separate model retraining for the admission of every new user.","Experiment results show that our Steganographic Passport outperforms other passport-based deep model protection methods in robustness against various known attacks."],"url":"http://arxiv.org/abs/2404.02889v1","category":"cs.CR"}
{"created":"2024-04-03 17:34:28","title":"On the Scalability of Diffusion-based Text-to-Image Generation","abstract":"Scaling up model and data size has been quite successful for the evolution of LLMs. However, the scaling law for the diffusion based text-to-image (T2I) models is not fully explored. It is also unclear how to efficiently scale the model for better performance at reduced cost. The different training settings and expensive training cost make a fair model comparison extremely difficult. In this work, we empirically study the scaling properties of diffusion based T2I models by performing extensive and rigours ablations on scaling both denoising backbones and training set, including training scaled UNet and Transformer variants ranging from 0.4B to 4B parameters on datasets upto 600M images. For model scaling, we find the location and amount of cross attention distinguishes the performance of existing UNet designs. And increasing the transformer blocks is more parameter-efficient for improving text-image alignment than increasing channel numbers. We then identify an efficient UNet variant, which is 45% smaller and 28% faster than SDXL's UNet. On the data scaling side, we show the quality and diversity of the training set matters more than simply dataset size. Increasing caption density and diversity improves text-image alignment performance and the learning efficiency. Finally, we provide scaling functions to predict the text-image alignment performance as functions of the scale of model size, compute and dataset size.","sentences":["Scaling up model and data size has been quite successful for the evolution of LLMs.","However, the scaling law for the diffusion based text-to-image (T2I) models is not fully explored.","It is also unclear how to efficiently scale the model for better performance at reduced cost.","The different training settings and expensive training cost make a fair model comparison extremely difficult.","In this work, we empirically study the scaling properties of diffusion based T2I models by performing extensive and rigours ablations on scaling both denoising backbones and training set, including training scaled UNet and Transformer variants ranging from 0.4B to 4B parameters on datasets upto 600M images.","For model scaling, we find the location and amount of cross attention distinguishes the performance of existing UNet designs.","And increasing the transformer blocks is more parameter-efficient for improving text-image alignment than increasing channel numbers.","We then identify an efficient UNet variant, which is 45% smaller and 28% faster than SDXL's UNet.","On the data scaling side, we show the quality and diversity of the training set matters more than simply dataset size.","Increasing caption density and diversity improves text-image alignment performance and the learning efficiency.","Finally, we provide scaling functions to predict the text-image alignment performance as functions of the scale of model size, compute and dataset size."],"url":"http://arxiv.org/abs/2404.02883v1","category":"cs.CV"}
{"created":"2024-04-03 17:24:27","title":"FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery","abstract":"Object detection in remotely sensed satellite pictures is fundamental in many fields such as biophysical, and environmental monitoring. While deep learning algorithms are constantly evolving, they have been mostly implemented and tested on popular ground-based taken photos. This paper critically evaluates and compares a suite of advanced object detection algorithms customized for the task of identifying aircraft within satellite imagery. Using the large HRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset, this research encompasses an array of methodologies including YOLO versions 5 and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from scratch. This exhaustive training and validation study reveal YOLOv5 as the preeminent model for the specific case of identifying airplanes from remote sensing data, showcasing high precision and adaptability across diverse imaging conditions. This research highlight the nuanced performance landscapes of these algorithms, with YOLOv5 emerging as a robust solution for aerial object detection, underlining its importance through superior mean average precision, Recall, and Intersection over Union scores. The findings described here underscore the fundamental role of algorithm selection aligned with the specific demands of satellite imagery analysis and extend a comprehensive framework to evaluate model efficacy. The benchmark toolkit and codes, available via https://github.com/toelt-llc/FlightScope_Bench, aims to further exploration and innovation in the realm of remote sensing object detection, paving the way for improved analytical methodologies in satellite imagery applications.","sentences":["Object detection in remotely sensed satellite pictures is fundamental in many fields such as biophysical, and environmental monitoring.","While deep learning algorithms are constantly evolving, they have been mostly implemented and tested on popular ground-based taken photos.","This paper critically evaluates and compares a suite of advanced object detection algorithms customized for the task of identifying aircraft within satellite imagery.","Using the large HRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset, this research encompasses an array of methodologies including YOLO versions 5 and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from scratch.","This exhaustive training and validation study reveal YOLOv5 as the preeminent model for the specific case of identifying airplanes from remote sensing data, showcasing high precision and adaptability across diverse imaging conditions.","This research highlight the nuanced performance landscapes of these algorithms, with YOLOv5 emerging as a robust solution for aerial object detection, underlining its importance through superior mean average precision, Recall, and Intersection over Union scores.","The findings described here underscore the fundamental role of algorithm selection aligned with the specific demands of satellite imagery analysis and extend a comprehensive framework to evaluate model efficacy.","The benchmark toolkit and codes, available via https://github.com/toelt-llc/FlightScope_Bench, aims to further exploration and innovation in the realm of remote sensing object detection, paving the way for improved analytical methodologies in satellite imagery applications."],"url":"http://arxiv.org/abs/2404.02877v1","category":"cs.CV"}
{"created":"2024-04-03 17:09:00","title":"Integrating Explanations in Learning LTL Specifications from Demonstrations","abstract":"This paper investigates whether recent advances in Large Language Models (LLMs) can assist in translating human explanations into a format that can robustly support learning Linear Temporal Logic (LTL) from demonstrations. Both LLMs and optimization-based methods can extract LTL specifications from demonstrations; however, they have distinct limitations. LLMs can quickly generate solutions and incorporate human explanations, but their lack of consistency and reliability hampers their applicability in safety-critical domains. On the other hand, optimization-based methods do provide formal guarantees but cannot process natural language explanations and face scalability challenges. We present a principled approach to combining LLMs and optimization-based methods to faithfully translate human explanations and demonstrations into LTL specifications. We have implemented a tool called Janaka based on our approach. Our experiments demonstrate the effectiveness of combining explanations with demonstrations in learning LTL specifications through several case studies.","sentences":["This paper investigates whether recent advances in Large Language Models (LLMs) can assist in translating human explanations into a format that can robustly support learning Linear Temporal Logic (LTL) from demonstrations.","Both LLMs and optimization-based methods can extract LTL specifications from demonstrations; however, they have distinct limitations.","LLMs can quickly generate solutions and incorporate human explanations, but their lack of consistency and reliability hampers their applicability in safety-critical domains.","On the other hand, optimization-based methods do provide formal guarantees but cannot process natural language explanations and face scalability challenges.","We present a principled approach to combining LLMs and optimization-based methods to faithfully translate human explanations and demonstrations into LTL specifications.","We have implemented a tool called Janaka based on our approach.","Our experiments demonstrate the effectiveness of combining explanations with demonstrations in learning LTL specifications through several case studies."],"url":"http://arxiv.org/abs/2404.02872v1","category":"cs.AI"}
{"created":"2024-04-03 17:05:41","title":"Human Activity Recognition using Smartphones","abstract":"Human Activity Recognition is a subject of great research today and has its applications in remote healthcare, activity tracking of the elderly or the disables, calories burnt tracking etc. In our project, we have created an Android application that recognizes the daily human activities and calculate the calories burnt in real time. We first captured labeled triaxial acceleration readings for different daily human activities from the smartphone's embedded accelerometer. These readings were preprocessed using a median filter. 42 features were extracted using various methods. We then tested various machine learning algorithms along with dimensionality reduction. Finally, in our Android application, we used the machine learning algorithm and a subset of features that provided maximum accuracy and minimum model building time. This is used for real-time activity recognition and calculation of calories burnt using a formula based on Metabolic Equivalent.","sentences":["Human Activity Recognition is a subject of great research today and has its applications in remote healthcare, activity tracking of the elderly or the disables, calories burnt tracking etc.","In our project, we have created an Android application that recognizes the daily human activities and calculate the calories burnt in real time.","We first captured labeled triaxial acceleration readings for different daily human activities from the smartphone's embedded accelerometer.","These readings were preprocessed using a median filter.","42 features were extracted using various methods.","We then tested various machine learning algorithms along with dimensionality reduction.","Finally, in our Android application, we used the machine learning algorithm and a subset of features that provided maximum accuracy and minimum model building time.","This is used for real-time activity recognition and calculation of calories burnt using a formula based on Metabolic Equivalent."],"url":"http://arxiv.org/abs/2404.02869v1","category":"cs.LG"}
{"created":"2024-04-04 17:59:40","title":"The More You See in 2D, the More You Perceive in 3D","abstract":"Humans can infer 3D structure from 2D images of an object based on past experience and improve their 3D understanding as they see more images. Inspired by this behavior, we introduce SAP3D, a system for 3D reconstruction and novel view synthesis from an arbitrary number of unposed images. Given a few unposed images of an object, we adapt a pre-trained view-conditioned diffusion model together with the camera poses of the images via test-time fine-tuning. The adapted diffusion model and the obtained camera poses are then utilized as instance-specific priors for 3D reconstruction and novel view synthesis. We show that as the number of input images increases, the performance of our approach improves, bridging the gap between optimization-based prior-less 3D reconstruction methods and single-image-to-3D diffusion-based methods. We demonstrate our system on real images as well as standard synthetic benchmarks. Our ablation studies confirm that this adaption behavior is key for more accurate 3D understanding.","sentences":["Humans can infer 3D structure from 2D images of an object based on past experience and improve their 3D understanding as they see more images.","Inspired by this behavior, we introduce SAP3D, a system for 3D reconstruction and novel view synthesis from an arbitrary number of unposed images.","Given a few unposed images of an object, we adapt a pre-trained view-conditioned diffusion model together with the camera poses of the images via test-time fine-tuning.","The adapted diffusion model and the obtained camera poses are then utilized as instance-specific priors for 3D reconstruction and novel view synthesis.","We show that as the number of input images increases, the performance of our approach improves, bridging the gap between optimization-based prior-less 3D reconstruction methods and single-image-to-3D diffusion-based methods.","We demonstrate our system on real images as well as standard synthetic benchmarks.","Our ablation studies confirm that this adaption behavior is key for more accurate 3D understanding."],"url":"http://arxiv.org/abs/2404.03652v1","category":"cs.CV"}
{"created":"2024-04-04 17:59:22","title":"Multipartite edge modes and tensor networks","abstract":"Holographic tensor networks model AdS/CFT, but so far they have been limited by involving only systems that are very different from gravity. Unfortunately, we cannot straightforwardly discretize gravity to incorporate it, because that would break diffeomorphism invariance. In this note, we explore a resolution. In low dimensions gravity can be written as a topological gauge theory, which can be discretized without breaking gauge-invariance. However, new problems arise. Foremost, we now need a qualitatively new kind of \"area operator,\" which has no relation to the number of links along the cut and is instead topological. Secondly, the inclusion of matter becomes trickier. We successfully construct a tensor network both including matter and with this new type of area. Notably, while this area is still related to the entanglement in \"edge mode\" degrees of freedom, the edge modes are no longer bipartite entangled pairs. Instead they are highly multipartite. Along the way, we calculate the entropy of novel subalgebras in a particular topological gauge theory. We also show that the multipartite nature of the edge modes gives rise to non-commuting area operators, a property that other tensor networks do not exhibit.","sentences":["Holographic tensor networks model AdS/CFT, but so far they have been limited by involving only systems that are very different from gravity.","Unfortunately, we cannot straightforwardly discretize gravity to incorporate it, because that would break diffeomorphism invariance.","In this note, we explore a resolution.","In low dimensions gravity can be written as a topological gauge theory, which can be discretized without breaking gauge-invariance.","However, new problems arise.","Foremost, we now need a qualitatively new kind of \"area operator,\" which has no relation to the number of links along the cut and is instead topological.","Secondly, the inclusion of matter becomes trickier.","We successfully construct a tensor network both including matter and with this new type of area.","Notably, while this area is still related to the entanglement in \"edge mode\" degrees of freedom, the edge modes are no longer bipartite entangled pairs.","Instead they are highly multipartite.","Along the way, we calculate the entropy of novel subalgebras in a particular topological gauge theory.","We also show that the multipartite nature of the edge modes gives rise to non-commuting area operators, a property that other tensor networks do not exhibit."],"url":"http://arxiv.org/abs/2404.03651v1","category":"hep-th"}
{"created":"2024-04-04 17:59:08","title":"OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views","abstract":"Large visual-language models (VLMs), like CLIP, enable open-set image segmentation to segment arbitrary concepts from an image in a zero-shot manner. This goes beyond the traditional closed-set assumption, i.e., where models can only segment classes from a pre-defined training set. More recently, first works on open-set segmentation in 3D scenes have appeared in the literature. These methods are heavily influenced by closed-set 3D convolutional approaches that process point clouds or polygon meshes. However, these 3D scene representations do not align well with the image-based nature of the visual-language models. Indeed, point cloud and 3D meshes typically have a lower resolution than images and the reconstructed 3D scene geometry might not project well to the underlying 2D image sequences used to compute pixel-aligned CLIP features. To address these challenges, we propose OpenNeRF which naturally operates on posed images and directly encodes the VLM features within the NeRF. This is similar in spirit to LERF, however our work shows that using pixel-wise VLM features (instead of global CLIP features) results in an overall less complex architecture without the need for additional DINO regularization. Our OpenNeRF further leverages NeRF's ability to render novel views and extract open-set VLM features from areas that are not well observed in the initial posed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF outperforms recent open-vocabulary methods such as LERF and OpenScene by at least +4.9 mIoU.","sentences":["Large visual-language models (VLMs), like CLIP, enable open-set image segmentation to segment arbitrary concepts from an image in a zero-shot manner.","This goes beyond the traditional closed-set assumption, i.e., where models can only segment classes from a pre-defined training set.","More recently, first works on open-set segmentation in 3D scenes have appeared in the literature.","These methods are heavily influenced by closed-set 3D convolutional approaches that process point clouds or polygon meshes.","However, these 3D scene representations do not align well with the image-based nature of the visual-language models.","Indeed, point cloud and 3D meshes typically have a lower resolution than images and the reconstructed 3D scene geometry might not project well to the underlying 2D image sequences used to compute pixel-aligned CLIP features.","To address these challenges, we propose OpenNeRF which naturally operates on posed images and directly encodes the VLM features within the NeRF.","This is similar in spirit to LERF, however our work shows that using pixel-wise VLM features (instead of global CLIP features) results in an overall less complex architecture without the need for additional DINO regularization.","Our OpenNeRF further leverages NeRF's ability to render novel views and extract open-set VLM features from areas that are not well observed in the initial posed images.","For 3D point cloud segmentation on the Replica dataset, OpenNeRF outperforms recent open-vocabulary methods such as LERF and OpenScene by at least +4.9 mIoU."],"url":"http://arxiv.org/abs/2404.03650v1","category":"cs.CV"}
{"created":"2024-04-04 17:55:38","title":"Sequential Recommendation for Optimizing Both Immediate Feedback and Long-term Retention","abstract":"In the landscape of Recommender System (RS) applications, reinforcement learning (RL) has recently emerged as a powerful tool, primarily due to its proficiency in optimizing long-term rewards. Nevertheless, it suffers from instability in the learning process, stemming from the intricate interactions among bootstrapping, off-policy training, and function approximation. Moreover, in multi-reward recommendation scenarios, designing a proper reward setting that reconciles the inner dynamics of various tasks is quite intricate. In response to these challenges, we introduce DT4IER, an advanced decision transformer-based recommendation model that is engineered to not only elevate the effectiveness of recommendations but also to achieve a harmonious balance between immediate user engagement and long-term retention. The DT4IER applies an innovative multi-reward design that adeptly balances short and long-term rewards with user-specific attributes, which serve to enhance the contextual richness of the reward sequence ensuring a more informed and personalized recommendation process. To enhance its predictive capabilities, DT4IER incorporates a high-dimensional encoder, skillfully designed to identify and leverage the intricate interrelations across diverse tasks. Furthermore, we integrate a contrastive learning approach within the action embedding predictions, a strategy that significantly boosts the model's overall performance. Experiments on three real-world datasets demonstrate the effectiveness of DT4IER against state-of-the-art Sequential Recommender Systems (SRSs) and Multi-Task Learning (MTL) models in terms of both prediction accuracy and effectiveness in specific tasks. The source code is accessible online to facilitate replication","sentences":["In the landscape of Recommender System (RS) applications, reinforcement learning (RL) has recently emerged as a powerful tool, primarily due to its proficiency in optimizing long-term rewards.","Nevertheless, it suffers from instability in the learning process, stemming from the intricate interactions among bootstrapping, off-policy training, and function approximation.","Moreover, in multi-reward recommendation scenarios, designing a proper reward setting that reconciles the inner dynamics of various tasks is quite intricate.","In response to these challenges, we introduce DT4IER, an advanced decision transformer-based recommendation model that is engineered to not only elevate the effectiveness of recommendations but also to achieve a harmonious balance between immediate user engagement and long-term retention.","The DT4IER applies an innovative multi-reward design that adeptly balances short and long-term rewards with user-specific attributes, which serve to enhance the contextual richness of the reward sequence ensuring a more informed and personalized recommendation process.","To enhance its predictive capabilities, DT4IER incorporates a high-dimensional encoder, skillfully designed to identify and leverage the intricate interrelations across diverse tasks.","Furthermore, we integrate a contrastive learning approach within the action embedding predictions, a strategy that significantly boosts the model's overall performance.","Experiments on three real-world datasets demonstrate the effectiveness of DT4IER against state-of-the-art Sequential Recommender Systems (SRSs) and Multi-Task Learning (MTL) models in terms of both prediction accuracy and effectiveness in specific tasks.","The source code is accessible online to facilitate replication"],"url":"http://arxiv.org/abs/2404.03637v1","category":"cs.IR"}
{"created":"2024-04-04 17:49:38","title":"ROBUST: 221 Bugs in the Robot Operating System","abstract":"As robotic systems such as autonomous cars and delivery drones assume greater roles and responsibilities within society, the likelihood and impact of catastrophic software failure within those systems is increased.To aid researchers in the development of new methods to measure and assure the safety and quality of robotics software, we systematically curated a dataset of 221 bugs across 7 popular and diverse software systems implemented via the Robot Operating System (ROS). We produce historically accurate recreations of each of the 221 defective software versions in the form of Docker images, and use a grounded theory approach to examine and categorize their corresponding faults, failures, and fixes. Finally, we reflect on the implications of our findings and outline future research directions for the community.","sentences":["As robotic systems such as autonomous cars and delivery drones assume greater roles and responsibilities within society, the likelihood and impact of catastrophic software failure within those systems is increased.","To aid researchers in the development of new methods to measure and assure the safety and quality of robotics software, we systematically curated a dataset of 221 bugs across 7 popular and diverse software systems implemented via the Robot Operating System (ROS).","We produce historically accurate recreations of each of the 221 defective software versions in the form of Docker images, and use a grounded theory approach to examine and categorize their corresponding faults, failures, and fixes.","Finally, we reflect on the implications of our findings and outline future research directions for the community."],"url":"http://arxiv.org/abs/2404.03629v1","category":"cs.SE"}
{"created":"2024-04-04 17:39:41","title":"On the Efficiency of Convolutional Neural Networks","abstract":"Since the breakthrough performance of AlexNet in 2012, convolutional neural networks (convnets) have grown into extremely powerful vision models. Deep learning researchers have used convnets to produce accurate results that were unachievable a decade ago. Yet computer scientists make computational efficiency their primary objective. Accuracy with exorbitant cost is not acceptable; an algorithm must also minimize its computational requirements. Confronted with the daunting computation that convnets use, deep learning researchers also became interested in efficiency. Researchers applied tremendous effort to find the convnet architectures that have the greatest efficiency. However, skepticism grew among researchers and engineers alike about the relevance of arithmetic complexity. Contrary to the prevailing view that latency and arithmetic complexity are irreconcilable, a simple formula relates both through computational efficiency. This insight enabled us to co-optimize the separate factors that determine latency. We observed that the degenerate conv2d layers that produce the best accuracy-complexity trade-off also have low operational intensity. Therefore, kernels that implement these layers use significant memory resources. We solved this optimization problem with block-fusion kernels that implement all layers of a residual block, thereby creating temporal locality, avoiding communication, and reducing workspace size. Our ConvFirst model with block-fusion kernels ran approximately four times as fast as the ConvNeXt baseline with PyTorch Inductor, at equal accuracy on the ImageNet-1K classification task. Our unified approach to convnet efficiency envisions a new era of models and kernels that achieve greater accuracy at lower cost.","sentences":["Since the breakthrough performance of AlexNet in 2012, convolutional neural networks (convnets) have grown into extremely powerful vision models.","Deep learning researchers have used convnets to produce accurate results that were unachievable a decade ago.","Yet computer scientists make computational efficiency their primary objective.","Accuracy with exorbitant cost is not acceptable; an algorithm must also minimize its computational requirements.","Confronted with the daunting computation that convnets use, deep learning researchers also became interested in efficiency.","Researchers applied tremendous effort to find the convnet architectures that have the greatest efficiency.","However, skepticism grew among researchers and engineers alike about the relevance of arithmetic complexity.","Contrary to the prevailing view that latency and arithmetic complexity are irreconcilable, a simple formula relates both through computational efficiency.","This insight enabled us to co-optimize the separate factors that determine latency.","We observed that the degenerate conv2d layers that produce the best accuracy-complexity trade-off also have low operational intensity.","Therefore, kernels that implement these layers use significant memory resources.","We solved this optimization problem with block-fusion kernels that implement all layers of a residual block, thereby creating temporal locality, avoiding communication, and reducing workspace size.","Our ConvFirst model with block-fusion kernels ran approximately four times as fast as the ConvNeXt baseline with PyTorch Inductor, at equal accuracy on the ImageNet-1K classification task.","Our unified approach to convnet efficiency envisions a new era of models and kernels that achieve greater accuracy at lower cost."],"url":"http://arxiv.org/abs/2404.03617v1","category":"cs.LG"}
{"created":"2024-04-04 17:25:25","title":"A Unified Algorithmic Framework for Dynamic Assortment Optimization under MNL Choice","abstract":"We consider assortment and inventory planning problems with dynamic stockout-based substitution effects and no replenishment. We consider two settings: 1. Customers can see all available products when they arrive, which is commonly seen in physical stores. 2. The seller can choose to offer a subset of available products to each customer, which is typical on online platforms. Both settings are known to be computationally challenging, and the current approximation algorithms for the two settings are quite different. We develop a unified algorithm framework under the MNL choice model for both settings. Our algorithms improve on the state-of-the-art algorithms in terms of approximation guarantee, runtime, and the ability to manage uncertainty in the total number of customers and handle more complex constraints. In the process, we establish various novel properties of dynamic assortment planning (under the MNL choice) that may be useful more broadly.","sentences":["We consider assortment and inventory planning problems with dynamic stockout-based substitution effects and no replenishment.","We consider two settings: 1. Customers can see all available products when they arrive, which is commonly seen in physical stores.","2.","The seller can choose to offer a subset of available products to each customer, which is typical on online platforms.","Both settings are known to be computationally challenging, and the current approximation algorithms for the two settings are quite different.","We develop a unified algorithm framework under the MNL choice model for both settings.","Our algorithms improve on the state-of-the-art algorithms in terms of approximation guarantee, runtime, and the ability to manage uncertainty in the total number of customers and handle more complex constraints.","In the process, we establish various novel properties of dynamic assortment planning (under the MNL choice) that may be useful more broadly."],"url":"http://arxiv.org/abs/2404.03604v1","category":"math.OC"}
{"created":"2024-04-04 17:23:28","title":"Analysis of second-order temporal schemes for modeling flow-solute transport in unsaturated porous media","abstract":"In this study, second-order temporal discretizations are analyzed for solving the coupled system of infiltration and solute transport in unsaturated porous media. The Richards equation is used to describe unsaturated flow, while the advection-dispersion equation (ADE) is used for modeling solute transport. The standard finite element discretization in space is utilized and four time-stepping methods are studied. Three of these methods require an iterative resolution to solve the Richards equation in its mixed form. In the remaining method, a novel technique is proposed to linearize the system of equations in time, and the iterative processes are avoided. In this method, a free stabilized parameter is introduced. Numerical tests are conducted to analyze the accuracy and efficiency of methods. The developed linear scheme based on the optimal free parameter is accurate and performs better in terms of efficiency since it offers a considerable gain in computational time compared to the other methods. The reliability and effectiveness of the developed semi-implicit scheme are investigated using numerical experiments for modeling water flow and solute transport in unsaturated soils.","sentences":["In this study, second-order temporal discretizations are analyzed for solving the coupled system of infiltration and solute transport in unsaturated porous media.","The Richards equation is used to describe unsaturated flow, while the advection-dispersion equation (ADE) is used for modeling solute transport.","The standard finite element discretization in space is utilized and four time-stepping methods are studied.","Three of these methods require an iterative resolution to solve the Richards equation in its mixed form.","In the remaining method, a novel technique is proposed to linearize the system of equations in time, and the iterative processes are avoided.","In this method, a free stabilized parameter is introduced.","Numerical tests are conducted to analyze the accuracy and efficiency of methods.","The developed linear scheme based on the optimal free parameter is accurate and performs better in terms of efficiency since it offers a considerable gain in computational time compared to the other methods.","The reliability and effectiveness of the developed semi-implicit scheme are investigated using numerical experiments for modeling water flow and solute transport in unsaturated soils."],"url":"http://arxiv.org/abs/2404.03603v1","category":"math.NA"}
{"created":"2024-04-04 17:00:37","title":"Beyond the blur: using experimental point spread functions to help scanning Kelvin probe microscopy reach its full potential","abstract":"Scanning Kelvin probe microscopy (SKPM) is a powerful technique for investigating the electrostatic properties of material surfaces, enabling the imaging of variations in work function, topology, surface charge density, or combinations thereof. Regardless of the underlying signal source, SKPM results in a voltage image which is spatially distorted due to the finite size of the probe, long-range electrostatic interactions, mechanical and electrical noise, and the finite response time of the electronics. In order to recover the underlying signal, it is necessary to deconvolve the measurement with an appropriate point spread function (PSF) that accounts the aforementioned distortions, but determining this PSF is difficult. Here we describe how such PSFs can be determined experimentally, and show how they can be used to recover the underlying information of interest. We first consider the physical principles that enable SKPM, and discuss how these affect the system PSF. We then show how one can experimentally measure PSFs by looking at well defined features, and that these compare well to simulated PSFs, provided scans are performed extremely slowly and carefully. Next, we work at realistic scan speeds, and show that the idealised PSFs fail to capture temporal distortions in the scan direction. While simulating PSFs for these situations would be quite challenging, we show that measuring PSFs with similar scan parameters works well. Our approach clarifies the basic principles of and inherent challenges to SKPM measurements, and gives practical methods to improve results.","sentences":["Scanning Kelvin probe microscopy (SKPM) is a powerful technique for investigating the electrostatic properties of material surfaces, enabling the imaging of variations in work function, topology, surface charge density, or combinations thereof.","Regardless of the underlying signal source, SKPM results in a voltage image which is spatially distorted due to the finite size of the probe, long-range electrostatic interactions, mechanical and electrical noise, and the finite response time of the electronics.","In order to recover the underlying signal, it is necessary to deconvolve the measurement with an appropriate point spread function (PSF) that accounts the aforementioned distortions, but determining this PSF is difficult.","Here we describe how such PSFs can be determined experimentally, and show how they can be used to recover the underlying information of interest.","We first consider the physical principles that enable SKPM, and discuss how these affect the system PSF.","We then show how one can experimentally measure PSFs by looking at well defined features, and that these compare well to simulated PSFs, provided scans are performed extremely slowly and carefully.","Next, we work at realistic scan speeds, and show that the idealised PSFs fail to capture temporal distortions in the scan direction.","While simulating PSFs for these situations would be quite challenging, we show that measuring PSFs with similar scan parameters works well.","Our approach clarifies the basic principles of and inherent challenges to SKPM measurements, and gives practical methods to improve results."],"url":"http://arxiv.org/abs/2404.03593v1","category":"physics.app-ph"}
{"created":"2024-04-04 16:59:13","title":"Wilkins: HPC In Situ Workflows Made Easy","abstract":"In situ approaches can accelerate the pace of scientific discoveries by allowing scientists to perform data analysis at simulation time. Current in situ workflow systems, however, face challenges in handling the growing complexity and diverse computational requirements of scientific tasks. In this work, we present Wilkins, an in situ workflow system that is designed for ease-of-use while providing scalable and efficient execution of workflow tasks. Wilkins provides a flexible workflow description interface, employs a high-performance data transport layer based on HDF5, and supports tasks with disparate data rates by providing a flow control mechanism. Wilkins seamlessly couples scientific tasks that already use HDF5, without requiring task code modifications. We demonstrate the above features using both synthetic benchmarks and two science use cases in materials science and cosmology.","sentences":["In situ approaches can accelerate the pace of scientific discoveries by allowing scientists to perform data analysis at simulation time.","Current in situ workflow systems, however, face challenges in handling the growing complexity and diverse computational requirements of scientific tasks.","In this work, we present Wilkins, an in situ workflow system that is designed for ease-of-use while providing scalable and efficient execution of workflow tasks.","Wilkins provides a flexible workflow description interface, employs a high-performance data transport layer based on HDF5, and supports tasks with disparate data rates by providing a flow control mechanism.","Wilkins seamlessly couples scientific tasks that already use HDF5, without requiring task code modifications.","We demonstrate the above features using both synthetic benchmarks and two science use cases in materials science and cosmology."],"url":"http://arxiv.org/abs/2404.03591v1","category":"cs.DC"}
{"created":"2024-04-04 16:58:02","title":"Homotopy types of diagrams of chain complexes","abstract":"We study the homotopy theory of diagrams of chain complexes over a field indexed by a finite poset, and show that it can be completely described in terms of appropriate diagrams of graded vector spaces.","sentences":["We study the homotopy theory of diagrams of chain complexes over a field indexed by a finite poset, and show that it can be completely described in terms of appropriate diagrams of graded vector spaces."],"url":"http://arxiv.org/abs/2404.03589v1","category":"math.AT"}
{"created":"2024-04-04 16:42:41","title":"The CCube reconstruction algorithm for the SoLid experiment","abstract":"The SoLid experiment is a very-short-baseline experiment aimed at searching for nuclear reactor-produced active to sterile antineutrino oscillations. The detection principle is based on the pairing of two types of solid scintillators: polyvinyl toluene and $^6$LiF:ZnS(Ag), which is a new technology used in this field of Physics. In addition to good neutron-gamma discrimination, this setup allows the detector to be highly segmented (the basic detection unit is a 5~cm side cube). High segmentation provides numerous advantages, including the precise location of Inverse Beta Decay (IBD) products, the derivation of the considerate antineutrino energy estimator, and a powerful background reduction tool based on the topological signature of the signal. Finally, the system is read out by a network of wavelength-shifting fibres coupled to a photodetector (MPPC). This paper describes the design of the reconstruction algorithm that allows maximum use of the granularity of the detector. The goal of the algorithm is to convert the output of the optical-fibre readout to the list of the detection units from which it originated. This paper provides a performance comparison for three methods and concludes with a choice of the baseline approach for the experiment.","sentences":["The SoLid experiment is a very-short-baseline experiment aimed at searching for nuclear reactor-produced active to sterile antineutrino oscillations.","The detection principle is based on the pairing of two types of solid scintillators: polyvinyl toluene and $^6$LiF:ZnS(Ag), which is a new technology used in this field of Physics.","In addition to good neutron-gamma discrimination, this setup allows the detector to be highly segmented (the basic detection unit is a 5~cm side cube).","High segmentation provides numerous advantages, including the precise location of Inverse Beta Decay (IBD) products, the derivation of the considerate antineutrino energy estimator, and a powerful background reduction tool based on the topological signature of the signal.","Finally, the system is read out by a network of wavelength-shifting fibres coupled to a photodetector (MPPC).","This paper describes the design of the reconstruction algorithm that allows maximum use of the granularity of the detector.","The goal of the algorithm is to convert the output of the optical-fibre readout to the list of the detection units from which it originated.","This paper provides a performance comparison for three methods and concludes with a choice of the baseline approach for the experiment."],"url":"http://arxiv.org/abs/2404.03580v1","category":"physics.ins-det"}
{"created":"2024-04-04 16:40:11","title":"Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models","abstract":"Providing knowledge documents for large language models (LLMs) has emerged as a promising solution to update the static knowledge inherent in their parameters. However, knowledge in the document may conflict with the memory of LLMs due to outdated or incorrect knowledge in the LLMs' parameters. This leads to the necessity of examining the capability of LLMs to assimilate supplemental external knowledge that conflicts with their memory. While previous studies have explained to what extent LLMs extract conflicting knowledge from the provided text, they neglect the necessity to reason with conflicting knowledge. Furthermore, there lack a detailed analysis on strategies to enable LLMs to resolve conflicting knowledge via prompting, decoding strategy, and supervised fine-tuning. To address these limitations, we construct a new dataset, dubbed KNOT, for knowledge conflict resolution examination in the form of question answering. KNOT facilitates in-depth analysis by dividing reasoning with conflicting knowledge into three levels: (1) Direct Extraction, which directly extracts conflicting knowledge to answer questions. (2) Explicit Reasoning, which reasons with conflicting knowledge when the reasoning path is explicitly provided in the question. (3) Implicit Reasoning, where reasoning with conflicting knowledge requires LLMs to infer the reasoning path independently to answer questions. We also conduct extensive experiments on KNOT to establish empirical guidelines for LLMs to utilize conflicting knowledge in complex circumstances. Dataset and associated codes can be accessed at https://github.com/THU-KEG/KNOT .","sentences":["Providing knowledge documents for large language models (LLMs) has emerged as a promising solution to update the static knowledge inherent in their parameters.","However, knowledge in the document may conflict with the memory of LLMs due to outdated or incorrect knowledge in the LLMs' parameters.","This leads to the necessity of examining the capability of LLMs to assimilate supplemental external knowledge that conflicts with their memory.","While previous studies have explained to what extent LLMs extract conflicting knowledge from the provided text, they neglect the necessity to reason with conflicting knowledge.","Furthermore, there lack a detailed analysis on strategies to enable LLMs to resolve conflicting knowledge via prompting, decoding strategy, and supervised fine-tuning.","To address these limitations, we construct a new dataset, dubbed KNOT, for knowledge conflict resolution examination in the form of question answering.","KNOT facilitates in-depth analysis by dividing reasoning with conflicting knowledge into three levels: (1) Direct Extraction, which directly extracts conflicting knowledge to answer questions.","(2) Explicit Reasoning, which reasons with conflicting knowledge when the reasoning path is explicitly provided in the question.","(3) Implicit Reasoning, where reasoning with conflicting knowledge requires LLMs to infer the reasoning path independently to answer questions.","We also conduct extensive experiments on KNOT to establish empirical guidelines for LLMs to utilize conflicting knowledge in complex circumstances.","Dataset and associated codes can be accessed at https://github.com/THU-KEG/KNOT ."],"url":"http://arxiv.org/abs/2404.03577v1","category":"cs.CL"}
{"created":"2024-04-04 16:37:42","title":"Terrain Point Cloud Inpainting via Signal Decomposition","abstract":"The rapid development of 3D acquisition technology has made it possible to obtain point clouds of real-world terrains. However, due to limitations in sensor acquisition technology or specific requirements, point clouds often contain defects such as holes with missing data. Inpainting algorithms are widely used to patch these holes. However, existing traditional inpainting algorithms rely on precise hole boundaries, which limits their ability to handle cases where the boundaries are not well-defined. On the other hand, learning-based completion methods often prioritize reconstructing the entire point cloud instead of solely focusing on hole filling. Based on the fact that real-world terrain exhibits both global smoothness and rich local detail, we propose a novel representation for terrain point clouds. This representation can help to repair the holes without clear boundaries. Specifically, it decomposes terrains into low-frequency and high-frequency components, which are represented by B-spline surfaces and relative height maps respectively. In this way, the terrain point cloud inpainting problem is transformed into a B-spline surface fitting and 2D image inpainting problem. By solving the two problems, the highly complex and irregular holes on the terrain point clouds can be well-filled, which not only satisfies the global terrain undulation but also exhibits rich geometric details. The experimental results also demonstrate the effectiveness of our method.","sentences":["The rapid development of 3D acquisition technology has made it possible to obtain point clouds of real-world terrains.","However, due to limitations in sensor acquisition technology or specific requirements, point clouds often contain defects such as holes with missing data.","Inpainting algorithms are widely used to patch these holes.","However, existing traditional inpainting algorithms rely on precise hole boundaries, which limits their ability to handle cases where the boundaries are not well-defined.","On the other hand, learning-based completion methods often prioritize reconstructing the entire point cloud instead of solely focusing on hole filling.","Based on the fact that real-world terrain exhibits both global smoothness and rich local detail, we propose a novel representation for terrain point clouds.","This representation can help to repair the holes without clear boundaries.","Specifically, it decomposes terrains into low-frequency and high-frequency components, which are represented by B-spline surfaces and relative height maps respectively.","In this way, the terrain point cloud inpainting problem is transformed into a B-spline surface fitting and 2D image inpainting problem.","By solving the two problems, the highly complex and irregular holes on the terrain point clouds can be well-filled, which not only satisfies the global terrain undulation but also exhibits rich geometric details.","The experimental results also demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2404.03572v1","category":"cs.CV"}
{"created":"2024-04-04 16:30:20","title":"Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity","abstract":"We present an embodied AI system which receives open-ended natural language instructions from a human, and controls two arms to collaboratively accomplish potentially long-horizon tasks over a large workspace. Our system is modular: it deploys state of the art Large Language Models for task planning,Vision-Language models for semantic perception, and Point Cloud transformers for grasping. With semantic and physical safety in mind, these modules are interfaced with a real-time trajectory optimizer and a compliant tracking controller to enable human-robot proximity. We demonstrate performance for the following tasks: bi-arm sorting, bottle opening, and trash disposal tasks. These are done zero-shot where the models used have not been trained with any real world data from this bi-arm robot, scenes or workspace.Composing both learning- and non-learning-based components in a modular fashion with interpretable inputs and outputs allows the user to easily debug points of failures and fragilities. One may also in-place swap modules to improve the robustness of the overall platform, for instance with imitation-learned policies.","sentences":["We present an embodied AI system which receives open-ended natural language instructions from a human, and controls two arms to collaboratively accomplish potentially long-horizon tasks over a large workspace.","Our system is modular: it deploys state of the art Large Language Models for task planning,Vision-Language models for semantic perception, and Point Cloud transformers for grasping.","With semantic and physical safety in mind, these modules are interfaced with a real-time trajectory optimizer and a compliant tracking controller to enable human-robot proximity.","We demonstrate performance for the following tasks: bi-arm sorting, bottle opening, and trash disposal tasks.","These are done zero-shot where the models used have not been trained with any real world data from this bi-arm robot, scenes or workspace.","Composing both learning- and non-learning-based components in a modular fashion with interpretable inputs and outputs allows the user to easily debug points of failures and fragilities.","One may also in-place swap modules to improve the robustness of the overall platform, for instance with imitation-learned policies."],"url":"http://arxiv.org/abs/2404.03570v1","category":"cs.RO"}
{"created":"2024-04-04 16:15:27","title":"A characterization of zero entropy loosely Bernoulli flows via FK-pseudometric","abstract":"We introduce the Feldman-Katok pseudometric (FK-pseudometric for short) for flows. We then provide a characterization of zero entropy loosely Bernoulli measures for continuous flows via the FK-pseudometric extending the result known for discrete-time dynamical systems. We also provide a purely topological characterization of uniquely ergodic continuous flows whose unique invariant measure is zero entropy loosely Bernoulli.","sentences":["We introduce the Feldman-Katok pseudometric (FK-pseudometric for short) for flows.","We then provide a characterization of zero entropy loosely Bernoulli measures for continuous flows via the FK-pseudometric extending the result known for discrete-time dynamical systems.","We also provide a purely topological characterization of uniquely ergodic continuous flows whose unique invariant measure is zero entropy loosely Bernoulli."],"url":"http://arxiv.org/abs/2404.03559v1","category":"math.DS"}
{"created":"2024-04-04 16:10:45","title":"Signal-preserving CMB component separation with machine learning","abstract":"Analysis of microwave sky signals, such as the cosmic microwave background, often requires component separation with multi-frequency methods, where different signals are isolated by their frequency behaviors. Many so-called \"blind\" methods, such as the internal linear combination (ILC), make minimal assumptions about the spatial distribution of the signal or contaminants, and only assume knowledge of the frequency dependence of the signal. The ILC is a minimum-variance linear combination of the measured frequency maps. In the case of Gaussian, statistically isotropic fields, this is the optimal linear combination, as the variance is the only statistic of interest. However, in many cases the signal we wish to isolate, or the foregrounds we wish to remove, are non-Gaussian and/or statistically anisotropic (in particular for Galactic foregrounds). In such cases, it is possible that machine learning (ML) techniques can be used to exploit the non-Gaussian features of the foregrounds and thereby improve component separation. However, many ML techniques require the use of complex, difficult-to-interpret operations on the data. We propose a hybrid method whereby we train an ML model using only combinations of the data that $\\textit{do not contain the signal}$, and combine the resulting ML-predicted foreground estimate with the ILC solution to reduce the error from the ILC. We demonstrate our methods on simulations of extragalactic temperature and Galactic polarization foregrounds, and show that our ML model can exploit non-Gaussian features, such as point sources and spatially-varying spectral indices, to produce lower-variance maps than ILC - eg, reducing the variance of the B-mode residual by factors of up to 5 - while preserving the signal of interest in an unbiased manner. Moreover, we often find improved performance when applying our model to foreground models on which it was not trained.","sentences":["Analysis of microwave sky signals, such as the cosmic microwave background, often requires component separation with multi-frequency methods, where different signals are isolated by their frequency behaviors.","Many so-called \"blind\" methods, such as the internal linear combination (ILC), make minimal assumptions about the spatial distribution of the signal or contaminants, and only assume knowledge of the frequency dependence of the signal.","The ILC is a minimum-variance linear combination of the measured frequency maps.","In the case of Gaussian, statistically isotropic fields, this is the optimal linear combination, as the variance is the only statistic of interest.","However, in many cases the signal we wish to isolate, or the foregrounds we wish to remove, are non-Gaussian and/or statistically anisotropic (in particular for Galactic foregrounds).","In such cases, it is possible that machine learning (ML) techniques can be used to exploit the non-Gaussian features of the foregrounds and thereby improve component separation.","However, many ML techniques require the use of complex, difficult-to-interpret operations on the data.","We propose a hybrid method whereby we train an ML model using only combinations of the data that $\\textit{do not contain the signal}$, and combine the resulting ML-predicted foreground estimate with the ILC solution to reduce the error from the ILC.","We demonstrate our methods on simulations of extragalactic temperature and Galactic polarization foregrounds, and show that our ML model can exploit non-Gaussian features, such as point sources and spatially-varying spectral indices, to produce lower-variance maps than ILC - eg, reducing the variance of the B-mode residual by factors of up to 5 - while preserving the signal of interest in an unbiased manner.","Moreover, we often find improved performance when applying our model to foreground models on which it was not trained."],"url":"http://arxiv.org/abs/2404.03557v1","category":"astro-ph.CO"}
{"created":"2024-04-04 16:07:21","title":"Robot Safety Monitoring using Programmable Light Curtains","abstract":"As factories continue to evolve into collaborative spaces with multiple robots working together with human supervisors in the loop, ensuring safety for all actors involved becomes critical. Currently, laser-based light curtain sensors are widely used in factories for safety monitoring. While these conventional safety sensors meet high accuracy standards, they are difficult to reconfigure and can only monitor a fixed user-defined region of space. Furthermore, they are typically expensive. Instead, we leverage a controllable depth sensor, programmable light curtains (PLC), to develop an inexpensive and flexible real-time safety monitoring system for collaborative robot workspaces. Our system projects virtual dynamic safety envelopes that tightly envelop the moving robot at all times and detect any objects that intrude the envelope. Furthermore, we develop an instrumentation algorithm that optimally places (multiple) PLCs in a workspace to maximize the visibility coverage of robots. Our work enables fence-less human-robot collaboration, while scaling to monitor multiple robots with few sensors. We analyze our system in a real manufacturing testbed with four robot arms and demonstrate its capabilities as a fast, accurate, and inexpensive safety monitoring solution.","sentences":["As factories continue to evolve into collaborative spaces with multiple robots working together with human supervisors in the loop, ensuring safety for all actors involved becomes critical.","Currently, laser-based light curtain sensors are widely used in factories for safety monitoring.","While these conventional safety sensors meet high accuracy standards, they are difficult to reconfigure and can only monitor a fixed user-defined region of space.","Furthermore, they are typically expensive.","Instead, we leverage a controllable depth sensor, programmable light curtains (PLC), to develop an inexpensive and flexible real-time safety monitoring system for collaborative robot workspaces.","Our system projects virtual dynamic safety envelopes that tightly envelop the moving robot at all times and detect any objects that intrude the envelope.","Furthermore, we develop an instrumentation algorithm that optimally places (multiple) PLCs in a workspace to maximize the visibility coverage of robots.","Our work enables fence-less human-robot collaboration, while scaling to monitor multiple robots with few sensors.","We analyze our system in a real manufacturing testbed with four robot arms and demonstrate its capabilities as a fast, accurate, and inexpensive safety monitoring solution."],"url":"http://arxiv.org/abs/2404.03556v1","category":"cs.RO"}
{"created":"2024-04-04 15:51:43","title":"Quantum querying based on multicontrolled Toffoli gates for causal Feynman loop configurations and directed acyclic graphs","abstract":"Quantum algorithms are a promising framework for a proper treatment of Feynman loop integrals due to the existence of a manifestly causal representation scenario. Particularly, unfolding causal configurations of multiloop Feynman diagrams is understood as querying \\textit{directed acyclic graph} (DAG) configurations of undirected graphs in graph theory. In this paper we present a quantum algorithm for querying causality of multiloop Feynman diagrams using an ingenious change in the logic of the design of the oracle operator. The construction of the quantum oracle is surprisingly based exclusively on multicontrolled Toffoli gates and XNOT gates. The efficiency of the algorithm is evaluated performing a comparison with a quantum algorithm based on binary clauses. Additionally, we explicitly analise several three-, four- and five-eloop topologies, which have not been previously explored due to their higher complexity.","sentences":["Quantum algorithms are a promising framework for a proper treatment of Feynman loop integrals due to the existence of a manifestly causal representation scenario.","Particularly, unfolding causal configurations of multiloop Feynman diagrams is understood as querying \\textit{directed acyclic graph} (DAG) configurations of undirected graphs in graph theory.","In this paper we present a quantum algorithm for querying causality of multiloop Feynman diagrams using an ingenious change in the logic of the design of the oracle operator.","The construction of the quantum oracle is surprisingly based exclusively on multicontrolled Toffoli gates and XNOT gates.","The efficiency of the algorithm is evaluated performing a comparison with a quantum algorithm based on binary clauses.","Additionally, we explicitly analise several three-, four- and five-eloop topologies, which have not been previously explored due to their higher complexity."],"url":"http://arxiv.org/abs/2404.03544v1","category":"quant-ph"}
{"created":"2024-04-04 15:47:51","title":"A swimming bacterium in a two-fluid model of a polymer solution","abstract":"We analyse the motion of a flagellated bacterium in a two-fluid medium using slender body theory. The two-fluid model is useful for describing a body moving through a complex fluid with a microstructure whose length scale is comparable to the characteristic scale of the body. This is true for bacterial motion in biological fluids (entangled polymer solutions), where the entanglement results in a porous microstructure with typical pore diameters comparable to or larger than the flagellar bundle diameter but smaller than the diameter of the bacterial head. Thus the polymer and solvent satisfy different boundary conditions on the flagellar bundle and move with different velocities close to it. This gives rise to a screening length $L_B$ within which the fluids exchange momentum and the relative velocity between the two fluids decays. In this work, both the solvent and polymer of the two-fluid medium are modeled as Newtonian fluids with different viscosities $\\mu_s$ and $\\mu_p$ (viscosity ratio $\\lambda = \\mu_p/\\mu_s$), thereby capturing the effects solely introduced by the microstructure of the complex fluid. From our calculations, we observe an increased drag anisotropy for a rigid, slender flagellar bundle moving through this two-fluid medium, resulting in an enhanced swimming velocity of the organism. The results are sensitive to the interaction between the bundle and the polymer and we discuss two physical scenarios corresponding to two types of interaction. Our model provides an explanation for the experimentally observed enhancement of swimming velocity of bacteria in entangled polymer solutions and motivates further experimental investigations.","sentences":["We analyse the motion of a flagellated bacterium in a two-fluid medium using slender body theory.","The two-fluid model is useful for describing a body moving through a complex fluid with a microstructure whose length scale is comparable to the characteristic scale of the body.","This is true for bacterial motion in biological fluids (entangled polymer solutions), where the entanglement results in a porous microstructure with typical pore diameters comparable to or larger than the flagellar bundle diameter but smaller than the diameter of the bacterial head.","Thus the polymer and solvent satisfy different boundary conditions on the flagellar bundle and move with different velocities close to it.","This gives rise to a screening length $L_B$ within which the fluids exchange momentum and the relative velocity between the two fluids decays.","In this work, both the solvent and polymer of the two-fluid medium are modeled as Newtonian fluids with different viscosities $\\mu_s$ and $\\mu_p$ (viscosity ratio $\\lambda = \\mu_p/\\mu_s$), thereby capturing the effects solely introduced by the microstructure of the complex fluid.","From our calculations, we observe an increased drag anisotropy for a rigid, slender flagellar bundle moving through this two-fluid medium, resulting in an enhanced swimming velocity of the organism.","The results are sensitive to the interaction between the bundle and the polymer and we discuss two physical scenarios corresponding to two types of interaction.","Our model provides an explanation for the experimentally observed enhancement of swimming velocity of bacteria in entangled polymer solutions and motivates further experimental investigations."],"url":"http://arxiv.org/abs/2404.03540v1","category":"physics.flu-dyn"}
{"created":"2024-04-04 15:35:43","title":"COMO: Compact Mapping and Odometry","abstract":"We present COMO, a real-time monocular mapping and odometry system that encodes dense geometry via a compact set of 3D anchor points. Decoding anchor point projections into dense geometry via per-keyframe depth covariance functions guarantees that depth maps are joined together at visible anchor points. The representation enables joint optimization of camera poses and dense geometry, intrinsic 3D consistency, and efficient second-order inference. To maintain a compact yet expressive map, we introduce a frontend that leverages the covariance function for tracking and initializing potentially visually indistinct 3D points across frames. Altogether, we introduce a real-time system capable of estimating accurate poses and consistent geometry.","sentences":["We present COMO, a real-time monocular mapping and odometry system that encodes dense geometry via a compact set of 3D anchor points.","Decoding anchor point projections into dense geometry via per-keyframe depth covariance functions guarantees that depth maps are joined together at visible anchor points.","The representation enables joint optimization of camera poses and dense geometry, intrinsic 3D consistency, and efficient second-order inference.","To maintain a compact yet expressive map, we introduce a frontend that leverages the covariance function for tracking and initializing potentially visually indistinct 3D points across frames.","Altogether, we introduce a real-time system capable of estimating accurate poses and consistent geometry."],"url":"http://arxiv.org/abs/2404.03531v1","category":"cs.CV"}
{"created":"2024-04-04 15:32:34","title":"Operator growth and spread complexity in open quantum systems","abstract":"Commonly, the notion of \"quantum chaos'' refers to the fast scrambling of information throughout complex quantum systems undergoing unitary evolution. Motivated by the Krylov complexity and the operator growth hypothesis, we demonstrate that the entropy of the population distribution for an operator in time is a useful way to capture the complexity of the internal information dynamics of a system when subject to an environment and is, in principle, agnostic to the specific choice of operator basis. We demonstrate its effectiveness for the Sachdev-Ye-Kitaev (SYK) model, examining the dynamics of the system in both its Krylov basis and the basis of operator strings. We prove that the former basis minimises spread complexity while the latter is an eigenbasis for high dissipation. In both cases, we probe the long-time dynamics of the model and the phenomenological effects of decoherence on the complexity of the dynamics.","sentences":["Commonly, the notion of \"quantum chaos'' refers to the fast scrambling of information throughout complex quantum systems undergoing unitary evolution.","Motivated by the Krylov complexity and the operator growth hypothesis, we demonstrate that the entropy of the population distribution for an operator in time is a useful way to capture the complexity of the internal information dynamics of a system when subject to an environment and is, in principle, agnostic to the specific choice of operator basis.","We demonstrate its effectiveness for the Sachdev-Ye-Kitaev (SYK) model, examining the dynamics of the system in both its Krylov basis and the basis of operator strings.","We prove that the former basis minimises spread complexity while the latter is an eigenbasis for high dissipation.","In both cases, we probe the long-time dynamics of the model and the phenomenological effects of decoherence on the complexity of the dynamics."],"url":"http://arxiv.org/abs/2404.03529v1","category":"quant-ph"}
{"created":"2024-04-04 15:21:40","title":"Model Checking Recursive Probabilistic Programs with Conditioning","abstract":"We address the problem of model checking temporal logic specifications for probabilistic programs with recursive procedures, nested queries, and conditioning expressed with observe statements. We introduce probabilistic Operator Precedence Automata (pOPA), a new class of probabilistic pushdown automata suitable to model constructs and behaviors of probabilistic programs. We develop a model checking algorithm that can verify requirements expressed in a fragment of Precedence Oriented Temporal Logic (POTL$^f_\\mathcal{X}$) on a pOPA in single EXPTIME. POTL$^f_\\mathcal{X}$ is a temporal logic based on Operator Precedence Languages, which features modalities that interact with the context-free structure of program traces, matching procedure calls with returns or observe statements. We provide the first probabilistic model checking implementation of context-free language properties for probabilistic pushdown systems.","sentences":["We address the problem of model checking temporal logic specifications for probabilistic programs with recursive procedures, nested queries, and conditioning expressed with observe statements.","We introduce probabilistic Operator Precedence Automata (pOPA), a new class of probabilistic pushdown automata suitable to model constructs and behaviors of probabilistic programs.","We develop a model checking algorithm that can verify requirements expressed in a fragment of Precedence Oriented Temporal Logic (POTL$^f_\\mathcal{X}$) on a pOPA in single EXPTIME.","POTL$^f_\\mathcal{X}$ is a temporal logic based on Operator Precedence Languages, which features modalities that interact with the context-free structure of program traces, matching procedure calls with returns or observe statements.","We provide the first probabilistic model checking implementation of context-free language properties for probabilistic pushdown systems."],"url":"http://arxiv.org/abs/2404.03515v1","category":"cs.LO"}
{"created":"2024-04-04 15:14:49","title":"Materials for High Temperature Digital Electronics","abstract":"Silicon microelectronics, consisting of complementary metal oxide semiconductor (CMOS) technology, have changed nearly all aspects of human life from communication to transportation, entertainment, and healthcare. Despite the widespread and mainstream use, current silicon-based devices suffer significant reliability issues at temperatures exceeding 125 {\\deg}C. The emergent technological frontiers of space exploration, geothermal energy harvesting, nuclear energy, unmanned avionic systems, and autonomous driving will rely on control systems, sensors, and communication devices which operate at temperatures as high as 500 {\\deg}C and beyond. At these extreme temperatures, active (heat exchanger, phase change cooling) or passive (fins and thermal interface materials) cooling strategies add significant mass and complication which is often infeasible. Thus, new material solutions beyond conventional silicon CMOS devices are necessary for high temperature, resilient electronic systems. Accomplishing this will require a united effort to explore development, integration, and ultimately manufacturing of non-silicon-based logic and memory technologies, non-traditional metals for interconnects, and ceramic packaging technology.","sentences":["Silicon microelectronics, consisting of complementary metal oxide semiconductor (CMOS) technology, have changed nearly all aspects of human life from communication to transportation, entertainment, and healthcare.","Despite the widespread and mainstream use, current silicon-based devices suffer significant reliability issues at temperatures exceeding 125 {\\deg}C.","The emergent technological frontiers of space exploration, geothermal energy harvesting, nuclear energy, unmanned avionic systems, and autonomous driving will rely on control systems, sensors, and communication devices which operate at temperatures as high as 500 {\\deg}C and beyond.","At these extreme temperatures, active (heat exchanger, phase change cooling) or passive (fins and thermal interface materials) cooling strategies add significant mass and complication which is often infeasible.","Thus, new material solutions beyond conventional silicon CMOS devices are necessary for high temperature, resilient electronic systems.","Accomplishing this will require a united effort to explore development, integration, and ultimately manufacturing of non-silicon-based logic and memory technologies, non-traditional metals for interconnects, and ceramic packaging technology."],"url":"http://arxiv.org/abs/2404.03510v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 15:09:25","title":"Entanglement Degradation in the Presence of Markovian Noise: a Statistical Analysis","abstract":"Adopting a statistical approach we study the degradation of entanglement of a quantum system under the action of an ensemble of randomly distributed Markovian noise. This enables us to address scenarios where only limited information is available on the mechanisms that rule the noisy evolution of the model. As an application, we characterize the statistic of entanglement deterioration for a quantum memory formed by $n$ qudits that undergo randomly distributed local, uniform, Markovian noise evolution.","sentences":["Adopting a statistical approach we study the degradation of entanglement of a quantum system under the action of an ensemble of randomly distributed Markovian noise.","This enables us to address scenarios where only limited information is available on the mechanisms that rule the noisy evolution of the model.","As an application, we characterize the statistic of entanglement deterioration for a quantum memory formed by $n$ qudits that undergo randomly distributed local, uniform, Markovian noise evolution."],"url":"http://arxiv.org/abs/2404.03505v1","category":"quant-ph"}
{"created":"2024-04-04 14:58:16","title":"Total positivity and two inequalities by Athanasiadis and Tzanaki","abstract":"Let $\\Delta$ be a (d-1)-dimensional simplicial complex and h^\\Delta = (h_0^ ,.. , h_d) its h-vector. For a face uniform subdivision operation \\F we write \\Delta_\\F for the subdivided complex and H_\\F for the matrix such that h^{\\Delta_\\F} = H_\\F h^\\Delta.   In connection with the real rootedness of symmetric decompositions Athanasiadis and Tzanaki studied for strictly positive h-vectors the inequalities   h_0 / h_1 \\leq h_1 / h_{d-1} \\leq .... \\leq h_d / h_0 and h_1 / h_{d-1} \\geq ... \\geq h_{d-2} / h_2 \\geq h_{d-1} / h_1.   In this paper we show that if the inequalities holds for a simplicial complex $\\Delta$ and H_\\F is TP_2 (all entries and two minors are non-negative) then the inequalities hold for \\Delta_\\F.   We prove that if \\F is the barycentric subdivision then H_\\F is TP_2. If \\F is the rth-edgewise subdivision then work of Diaconis and Fulman shows H_\\F is TP_2. Indeed in this case by work of Mao and Wang H_\\F is even TP.","sentences":["Let $\\Delta$ be a (d-1)-dimensional simplicial complex and h^\\Delta = (h_0^ ,.. , h_d) its h-vector.","For a face uniform subdivision operation \\F we write \\Delta_\\F for the subdivided complex and H_\\F for the matrix such that h^{\\Delta_\\F} = H_\\F h^\\Delta.   ","In connection with the real rootedness of symmetric decompositions Athanasiadis and Tzanaki studied for strictly positive h-vectors the inequalities   h_0 / h_1 \\leq h_1 / h_{d-1} \\leq ....","\\leq h_d / h_0 and h_1 / h_{d-1} \\geq ...","\\geq h_{d-2} / h_2 \\geq h_{d-1} / h_1.   ","In this paper we show that if the inequalities holds for a simplicial complex $\\Delta$ and H_\\F is TP_2","(all entries and two minors are non-negative) then the inequalities hold for \\Delta_\\F.   We prove that if \\F is the barycentric subdivision then H_\\F is TP_2.","If \\F is the rth-edgewise subdivision then work of Diaconis and Fulman shows H_\\F is TP_2.","Indeed in this case by work of Mao and Wang H_\\F is even TP."],"url":"http://arxiv.org/abs/2404.03500v1","category":"math.CO"}
{"created":"2024-04-04 14:56:41","title":"Integrating Large Language Models with Multimodal Virtual Reality Interfaces to Support Collaborative Human-Robot Construction Work","abstract":"In the construction industry, where work environments are complex, unstructured and often dangerous, the implementation of Human-Robot Collaboration (HRC) is emerging as a promising advancement. This underlines the critical need for intuitive communication interfaces that enable construction workers to collaborate seamlessly with robotic assistants. This study introduces a conversational Virtual Reality (VR) interface integrating multimodal interaction to enhance intuitive communication between construction workers and robots. By integrating voice and controller inputs with the Robot Operating System (ROS), Building Information Modeling (BIM), and a game engine featuring a chat interface powered by a Large Language Model (LLM), the proposed system enables intuitive and precise interaction within a VR setting. Evaluated by twelve construction workers through a drywall installation case study, the proposed system demonstrated its low workload and high usability with succinct command inputs. The proposed multimodal interaction system suggests that such technological integration can substantially advance the integration of robotic assistants in the construction industry.","sentences":["In the construction industry, where work environments are complex, unstructured and often dangerous, the implementation of Human-Robot Collaboration (HRC) is emerging as a promising advancement.","This underlines the critical need for intuitive communication interfaces that enable construction workers to collaborate seamlessly with robotic assistants.","This study introduces a conversational Virtual Reality (VR) interface integrating multimodal interaction to enhance intuitive communication between construction workers and robots.","By integrating voice and controller inputs with the Robot Operating System (ROS), Building Information Modeling (BIM), and a game engine featuring a chat interface powered by a Large Language Model (LLM), the proposed system enables intuitive and precise interaction within a VR setting.","Evaluated by twelve construction workers through a drywall installation case study, the proposed system demonstrated its low workload and high usability with succinct command inputs.","The proposed multimodal interaction system suggests that such technological integration can substantially advance the integration of robotic assistants in the construction industry."],"url":"http://arxiv.org/abs/2404.03498v1","category":"cs.RO"}
{"created":"2024-04-04 14:43:43","title":"Design of Stickbug: a Six-Armed Precision Pollination Robot","abstract":"This work presents the design of Stickbug, a six-armed, multi-agent, precision pollination robot that combines the accuracy of single-agent systems with swarm parallelization in greenhouses. Precision pollination robots have often been proposed to offset the effects of a decreasing population of natural pollinators, but they frequently lack the required parallelization and scalability. Stickbug achieves this by allowing each arm and drive base to act as an individual agent, significantly reducing planning complexity. Stickbug uses a compact holonomic Kiwi drive to navigate narrow greenhouse rows, a tall mast to support multiple manipulators and reach plant heights, a detection model and classifier to identify Bramble flowers, and a felt-tipped end-effector for contact-based pollination. Initial experimental validation demonstrates that Stickbug can attempt over 1.5 pollinations per minute with a 50% success rate. Additionally, a Bramble flower perception dataset was created and is publicly available alongside Stickbug's software and design files.","sentences":["This work presents the design of Stickbug, a six-armed, multi-agent, precision pollination robot that combines the accuracy of single-agent systems with swarm parallelization in greenhouses.","Precision pollination robots have often been proposed to offset the effects of a decreasing population of natural pollinators, but they frequently lack the required parallelization and scalability.","Stickbug achieves this by allowing each arm and drive base to act as an individual agent, significantly reducing planning complexity.","Stickbug uses a compact holonomic Kiwi drive to navigate narrow greenhouse rows, a tall mast to support multiple manipulators and reach plant heights, a detection model and classifier to identify Bramble flowers, and a felt-tipped end-effector for contact-based pollination.","Initial experimental validation demonstrates that Stickbug can attempt over 1.5 pollinations per minute with a 50% success rate.","Additionally, a Bramble flower perception dataset was created and is publicly available alongside Stickbug's software and design files."],"url":"http://arxiv.org/abs/2404.03489v1","category":"cs.RO"}
{"created":"2024-04-04 14:42:04","title":"Electronic transport, metal-insulator transition, and Wigner crystallization in transition metal dichalcogenide monolayers","abstract":"Two recent electronic transport experiments from Columbia University and Harvard University have reported record high mobility and low channel densities in transition metal dichalcogenide (TMD) WSe$_2$ monolayers [J. Pack, et al., arXiv:2310.19782; A. Y. Joe, et al., Phys. Rev. Lett. 132, 056303 (2024)]. A two-dimensional (2D) metal-insulator transition (MIT) is demonstrated in the Columbia sample at low densities, a regime where the formation of a Wigner crystal (WC) is theoretically anticipated in the absence of disorder. We employ the finite-temperature Boltzmann theory to understand the low-temperature transport properties of monolayer TMDs, taking into account realistic disorder scattering. We analyze the experimental results, focusing on the 2D MIT behavior and the influence of temperature and density on mobility and resistivity in the metallic phase. We provide a discussion of the nontrivial carrier density dependence of our transport results. Our analysis elucidates the linear-in-$T$ resistivity in the metallic phase, attributing it to Friedel oscillations associated with screened charged impurities. Furthermore, we explore whether Coulomb disorder could lead to the MIT through either a quantum Anderson localization transition or a classical percolation transition. Our theoretical estimates of the disorder-induced MIT critical densities, although smaller, are within a factor of ~2 of the experimental critical density. We examine the exceptionally high melting temperature ~10 K of WCs observed experimentally in the MoSe$_2$ systems at low density, an order of magnitude larger than the pristine melting temperature. This suggests that the observed 2D low-density MIT behavior is likely a result of the complex interplay between disorder effects and interaction-driven WC physics, offering a comprehensive understanding of the low-temperature transport phenomena in TMD monolayers.","sentences":["Two recent electronic transport experiments from Columbia University and Harvard University have reported record high mobility and low channel densities in transition metal dichalcogenide (TMD) WSe$_2$ monolayers [J. Pack, et al., arXiv:2310.19782; A. Y. Joe, et al., Phys.","Rev. Lett.","132, 056303 (2024)].","A two-dimensional (2D) metal-insulator transition (MIT) is demonstrated in the Columbia sample at low densities, a regime where the formation of a Wigner crystal (WC) is theoretically anticipated in the absence of disorder.","We employ the finite-temperature Boltzmann theory to understand the low-temperature transport properties of monolayer TMDs, taking into account realistic disorder scattering.","We analyze the experimental results, focusing on the 2D MIT behavior and the influence of temperature and density on mobility and resistivity in the metallic phase.","We provide a discussion of the nontrivial carrier density dependence of our transport results.","Our analysis elucidates the linear-in-$T$ resistivity in the metallic phase, attributing it to Friedel oscillations associated with screened charged impurities.","Furthermore, we explore whether Coulomb disorder could lead to the MIT through either a quantum Anderson localization transition or a classical percolation transition.","Our theoretical estimates of the disorder-induced MIT critical densities, although smaller, are within a factor of ~2 of the experimental critical density.","We examine the exceptionally high melting temperature ~10 K of WCs observed experimentally in the MoSe$_2$ systems at low density, an order of magnitude larger than the pristine melting temperature.","This suggests that the observed 2D low-density MIT behavior is likely a result of the complex interplay between disorder effects and interaction-driven WC physics, offering a comprehensive understanding of the low-temperature transport phenomena in TMD monolayers."],"url":"http://arxiv.org/abs/2404.03488v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-04 14:40:55","title":"Explicit Witt basis over the tensor product of Clifford algebras and octonions","abstract":"In this article, we investigate how the Witt basis serves as a link between real and complex variables in higher-dimensional spaces. Our focus is on the detailed construction of the Witt basis within the tensor product space combining Clifford algebra and multiple octonionic spaces. This construction effectively introduces complex coordinates. The technique is based on a specific subgroup of octonionic automorphisms, distinguished by binary codes. This method allows us to perform a Hermitian analysis of the complex structures within the tensor product space.","sentences":["In this article, we investigate how the Witt basis serves as a link between real and complex variables in higher-dimensional spaces.","Our focus is on the detailed construction of the Witt basis within the tensor product space combining Clifford algebra and multiple octonionic spaces.","This construction effectively introduces complex coordinates.","The technique is based on a specific subgroup of octonionic automorphisms, distinguished by binary codes.","This method allows us to perform a Hermitian analysis of the complex structures within the tensor product space."],"url":"http://arxiv.org/abs/2404.03487v1","category":"math.CV"}
{"created":"2024-04-04 14:37:06","title":"Coupled harmonics due to time-modulated point scatterers","abstract":"We consider the resonance and scattering properties of a composite medium containing scatterers whose properties are modulated in time. When excited with an incident wave of a single frequency, the scattered field consists of a family of coupled harmonics at frequencies differing by the frequency of temporal modulation. Similarly, the temporal modulation induces coupling between the resonance frequencies, leading to exceptional points at certain modulation amplitudes. Moreover, the lack of energy conservation causes scattering coefficients to blow up when (complex) resonances cross the real axis. We have developed an integral operator approach to characterize the scattering problem and, for high-contrast scatterers, we present small-volume asymptotic formulas analogous to the classical results for the static (unmodulated) case. We conclude the paper with a boundary integral formulation of the time-modulated problem, which gives an efficient numerical approach and corroborates the asymptotic formulas.","sentences":["We consider the resonance and scattering properties of a composite medium containing scatterers whose properties are modulated in time.","When excited with an incident wave of a single frequency, the scattered field consists of a family of coupled harmonics at frequencies differing by the frequency of temporal modulation.","Similarly, the temporal modulation induces coupling between the resonance frequencies, leading to exceptional points at certain modulation amplitudes.","Moreover, the lack of energy conservation causes scattering coefficients to blow up when (complex) resonances cross the real axis.","We have developed an integral operator approach to characterize the scattering problem and, for high-contrast scatterers, we present small-volume asymptotic formulas analogous to the classical results for the static (unmodulated) case.","We conclude the paper with a boundary integral formulation of the time-modulated problem, which gives an efficient numerical approach and corroborates the asymptotic formulas."],"url":"http://arxiv.org/abs/2404.03483v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-04 14:33:37","title":"Evolutionary theory of convective organization","abstract":"Observed patterns of convective cloud would be extremely improbable from random walks in an abstract space of configurations. Forcing is sometimes the driver, but complexity can also develop spontaneously. Here pattern evolution is considered as a natural selection process in a strategic game among configurations, akin to ecological succession. Information (entropy) quantifies improbability, interpreted as Darwinian fitness to the extent larger-scale forcings are properly accounted. Reconciling inferred or revealed fitness with energetics could make convection a showcase of evolutionary theory, simply as reinterpretation of spectral kinetic energy (KE) budgets for dry convection. For moist convection, the flux of a total energy E, the teleological reason for convection, is conjectured to be the central resource of an evolutionary game, with water playing a constraining role like nutrients in ecology. Shannon information H is reviewed in context of this evolutionary reasoning. Analysis of a new Cloud Botany shallow convection simulation set shows growth of H over tens of hours (perhaps more pertinently, it grows with KE throughput or cumulative buoyancy flux). Anisotropy and precipitation boost H in qualitatively distinct and contingent ways, like separate species or ecological guilds. Deep convection over lowland South America also shows many-hour evolution times, beyond simple convective adjustment notions. If evolving horizontal patterns contain or imply information about vertical density profiles, they could become a new resource in data assimilation to predict larger scale flow.","sentences":["Observed patterns of convective cloud would be extremely improbable from random walks in an abstract space of configurations.","Forcing is sometimes the driver, but complexity can also develop spontaneously.","Here pattern evolution is considered as a natural selection process in a strategic game among configurations, akin to ecological succession.","Information (entropy) quantifies improbability, interpreted as Darwinian fitness to the extent larger-scale forcings are properly accounted.","Reconciling inferred or revealed fitness with energetics could make convection a showcase of evolutionary theory, simply as reinterpretation of spectral kinetic energy (KE) budgets for dry convection.","For moist convection, the flux of a total energy E, the teleological reason for convection, is conjectured to be the central resource of an evolutionary game, with water playing a constraining role like nutrients in ecology.","Shannon information H is reviewed in context of this evolutionary reasoning.","Analysis of a new Cloud Botany shallow convection simulation set shows growth of H over tens of hours (perhaps more pertinently, it grows with KE throughput or cumulative buoyancy flux).","Anisotropy and precipitation boost H in qualitatively distinct and contingent ways, like separate species or ecological guilds.","Deep convection over lowland South America also shows many-hour evolution times, beyond simple convective adjustment notions.","If evolving horizontal patterns contain or imply information about vertical density profiles, they could become a new resource in data assimilation to predict larger scale flow."],"url":"http://arxiv.org/abs/2404.03480v1","category":"nlin.AO"}
{"created":"2024-04-04 14:23:42","title":"Measurable Structure Factors of Dense Dispersions Containing Polydisperse, Optically Inhomogeneous Particles","abstract":"We exemplarily investigate how optical properties of single scatterers in interacting multi-particle systems influence measurable structure factors. Both particles with linear gradients of their scattering length density and core-shell structures evoke characteristic deviations between the weighted sum $\\langle S(Q)\\rangle$ of partial structure factors in a multicomponent system and experimentally accessible, measurable structure factors $S_{\\mathrm{M}}(Q)$. While $\\langle S(Q)\\rangle$ contains only structural information of self-organising systems, $S_{\\mathrm{M}}(Q)$ additionally is influenced by optical properties of their constituents resulting in features such as changing amplitudes, additional peaks in the low wavevector region or splitting of higher-order maxima which are not related to structural reasons. Hence, a careful data analysis regarding size-distribution and optical properties of single scatters is mandatory to avoid a misinterpretation of measurable structure factors.","sentences":["We exemplarily investigate how optical properties of single scatterers in interacting multi-particle systems influence measurable structure factors.","Both particles with linear gradients of their scattering length density and core-shell structures evoke characteristic deviations between the weighted sum $\\langle S(Q)\\rangle$ of partial structure factors in a multicomponent system and experimentally accessible, measurable structure factors $S_{\\mathrm{M}}(Q)$. While $\\langle S(Q)\\rangle$ contains only structural information of self-organising systems, $S_{\\mathrm{M}}(Q)$ additionally is influenced by optical properties of their constituents resulting in features such as changing amplitudes, additional peaks in the low wavevector region or splitting of higher-order maxima which are not related to structural reasons.","Hence, a careful data analysis regarding size-distribution and optical properties of single scatters is mandatory to avoid a misinterpretation of measurable structure factors."],"url":"http://arxiv.org/abs/2404.03470v1","category":"cond-mat.soft"}
{"created":"2024-04-04 14:23:12","title":"Class-E, Active Electrically-Small Antenna for High-Power Wideband Transmission at the High-Frequency (HF) Band","abstract":"Antennas operating at the high-frequency (HF) band (3-30 MHz) are frequently electrically small due to the large wavelength of electromagnetic waves (10-100 m). However, the bandwidth-efficiency products of passively matched electrically small antennas (ESAs) are fundamentally limited. Wideband HF waveforms using bandwidths of 24 kHz or more have recently received significant attention in military communications applications. Efficiently radiating such signals from conventional passive ESAs is very challenging due to fundamental physical limits on bandwidth-efficiency products of ESAs. However, active antennas are not subject to the same constraints. In this work, we present the design and experimental characterization of a high-power, active ESA with enhanced bandwidth-efficiency product compared to {that of} passively matched ESAs. Specifically, the proposed active ESA can radiate wideband HF signals with banwidths of 24 kHz or more, with total efficiencies up to 80$\\%$, and radiated power levels approaching 100 W. Our approach uses a highly-efficient, integrated class-E switching circuit specifically designed to drive an electrically small, high-Q HF antenna over a bandwidth exceeding 24 kHz. Using a high-Q RLC antenna model, we have successfully demonstrated wideband binary ASK, PSK, and FSK modulations with the proposed class-E switching architecture. Experimental results indicate that the bandwidth-efficiency product of this class-E active antenna is 5.4-9.8 dB higher than that of an equivalent passive design with the same data rate, and bit-error-rate (BER).","sentences":["Antennas operating at the high-frequency (HF) band (3-30 MHz) are frequently electrically small due to the large wavelength of electromagnetic waves (10-100 m).","However, the bandwidth-efficiency products of passively matched electrically small antennas (ESAs) are fundamentally limited.","Wideband HF waveforms using bandwidths of 24 kHz or more have recently received significant attention in military communications applications.","Efficiently radiating such signals from conventional passive ESAs is very challenging due to fundamental physical limits on bandwidth-efficiency products of ESAs.","However, active antennas are not subject to the same constraints.","In this work, we present the design and experimental characterization of a high-power, active ESA with enhanced bandwidth-efficiency product compared to {that of} passively matched ESAs.","Specifically, the proposed active ESA can radiate wideband HF signals with banwidths of 24 kHz or more, with total efficiencies up to 80$\\%$, and radiated power levels approaching 100 W. Our approach uses a highly-efficient, integrated class-E switching circuit specifically designed to drive an electrically small, high-Q HF antenna over a bandwidth exceeding 24 kHz.","Using a high-Q RLC antenna model, we have successfully demonstrated wideband binary ASK, PSK, and FSK modulations with the proposed class-E switching architecture.","Experimental results indicate that the bandwidth-efficiency product of this class-E active antenna is 5.4-9.8 dB higher than that of an equivalent passive design with the same data rate, and bit-error-rate (BER)."],"url":"http://arxiv.org/abs/2404.03468v1","category":"physics.app-ph"}
{"created":"2024-04-04 14:18:23","title":"Patrick Moss 25/10/1947--17/3/2024","abstract":"Patrick Moss (1947--2024) had two distinct lives as a mathematician. The first was as a ring theorist in the late 1970s, in which he worked with Ginn and Lenagan as a student. After a long career as an inspirational mathematics teacher, Patrick completed a doctorate under my supervision in 2003. This led to a second mathematical life in arithmetic dynamics almost forty years after his first period of research. This is a short obituary of his remarkable contributions, several of which have stimulated further research.","sentences":["Patrick Moss (1947--2024) had two distinct lives as a mathematician.","The first was as a ring theorist in the late 1970s, in which he worked with Ginn and Lenagan as a student.","After a long career as an inspirational mathematics teacher, Patrick completed a doctorate under my supervision in 2003.","This led to a second mathematical life in arithmetic dynamics almost forty years after his first period of research.","This is a short obituary of his remarkable contributions, several of which have stimulated further research."],"url":"http://arxiv.org/abs/2404.03464v1","category":"math.HO"}
{"created":"2024-04-04 14:01:47","title":"Charting the Complex Structure Landscape of F-theory","abstract":"We explore the landscape of F-theory compactifications on Calabi--Yau fourfolds whose complex structure moduli space is the thrice-punctured sphere. As a first part, we enumerate all such Calabi--Yau fourfolds under the additional requirement that it has a large complex structure and conifold point at two of the punctures. We find 14 monodromy tuples by demanding the monodromy around infinity to be quasi-unipotent. As second part, we study the four different types of phases arising at infinity. For each we consider a working example where we determine the leading periods and other physical couplings. We also included a notebook that sets up the period vectors for any of these models.","sentences":["We explore the landscape of F-theory compactifications on Calabi--Yau fourfolds whose complex structure moduli space is the thrice-punctured sphere.","As a first part, we enumerate all such Calabi--Yau fourfolds under the additional requirement that it has a large complex structure and conifold point at two of the punctures.","We find 14 monodromy tuples by demanding the monodromy around infinity to be quasi-unipotent.","As second part, we study the four different types of phases arising at infinity.","For each we consider a working example where we determine the leading periods and other physical couplings.","We also included a notebook that sets up the period vectors for any of these models."],"url":"http://arxiv.org/abs/2404.03456v1","category":"hep-th"}
{"created":"2024-04-04 13:44:41","title":"Simultaneous State Estimation and Contact Detection for Legged Robots by Multiple-Model Kalman Filtering","abstract":"This paper proposes an algorithm for combined contact detection and state estimation for legged robots. The proposed algorithm models the robot's movement as a switched system, in which different modes relate to different feet being in contact with the ground. The key element in the proposed algorithm is an interacting multiple-model Kalman filter, which identifies the currently-active mode defining contacts, while estimating the state. The rationale for the proposed estimation framework is that contacts (and contact forces) impact the robot's state and vice versa. This paper presents validation studies with a quadruped using (i) the high-fidelity simulator Gazebo for a comparison with ground truth values and a baseline estimator, and (ii) hardware experiments with the Unitree A1 robot. The simulation study shows that the proposed algorithm outperforms the baseline estimator, which does not simultaneous detect contacts. The hardware experiments showcase the applicability of the proposed algorithm and highlights the ability to detect contacts.","sentences":["This paper proposes an algorithm for combined contact detection and state estimation for legged robots.","The proposed algorithm models the robot's movement as a switched system, in which different modes relate to different feet being in contact with the ground.","The key element in the proposed algorithm is an interacting multiple-model Kalman filter, which identifies the currently-active mode defining contacts, while estimating the state.","The rationale for the proposed estimation framework is that contacts (and contact forces) impact the robot's state and vice versa.","This paper presents validation studies with a quadruped using (i) the high-fidelity simulator Gazebo for a comparison with ground truth values and a baseline estimator, and (ii) hardware experiments with the Unitree A1 robot.","The simulation study shows that the proposed algorithm outperforms the baseline estimator, which does not simultaneous detect contacts.","The hardware experiments showcase the applicability of the proposed algorithm and highlights the ability to detect contacts."],"url":"http://arxiv.org/abs/2404.03444v1","category":"cs.RO"}
{"created":"2024-04-04 13:39:49","title":"Privacy Engineering From Principles to Practice: A Roadmap","abstract":"Privacy engineering is gaining momentum in industry and academia alike. So far, manifold low-level primitives and higher-level methods and strategies have successfully been established. Still, fostering adoption in real-world information systems calls for additional aspects to be consciously considered in research and practice.","sentences":["Privacy engineering is gaining momentum in industry and academia alike.","So far, manifold low-level primitives and higher-level methods and strategies have successfully been established.","Still, fostering adoption in real-world information systems calls for additional aspects to be consciously considered in research and practice."],"url":"http://arxiv.org/abs/2404.03442v1","category":"cs.CR"}
{"created":"2024-04-04 13:27:22","title":"Learning From Simplicial Data Based on Random Walks and 1D Convolutions","abstract":"Triggered by limitations of graph-based deep learning methods in terms of computational expressivity and model flexibility, recent years have seen a surge of interest in computational models that operate on higher-order topological domains such as hypergraphs and simplicial complexes. While the increased expressivity of these models can indeed lead to a better classification performance and a more faithful representation of the underlying system, the computational cost of these higher-order models can increase dramatically. To this end, we here explore a simplicial complex neural network learning architecture based on random walks and fast 1D convolutions (SCRaWl), in which we can adjust the increase in computational cost by varying the length and number of random walks considered while accounting for higher-order relationships. Importantly, due to the random walk-based design, the expressivity of the proposed architecture is provably incomparable to that of existing message-passing simplicial neural networks. We empirically evaluate SCRaWl on real-world datasets and show that it outperforms other simplicial neural networks.","sentences":["Triggered by limitations of graph-based deep learning methods in terms of computational expressivity and model flexibility, recent years have seen a surge of interest in computational models that operate on higher-order topological domains such as hypergraphs and simplicial complexes.","While the increased expressivity of these models can indeed lead to a better classification performance and a more faithful representation of the underlying system, the computational cost of these higher-order models can increase dramatically.","To this end, we here explore a simplicial complex neural network learning architecture based on random walks and fast 1D convolutions (SCRaWl), in which we can adjust the increase in computational cost by varying the length and number of random walks considered while accounting for higher-order relationships.","Importantly, due to the random walk-based design, the expressivity of the proposed architecture is provably incomparable to that of existing message-passing simplicial neural networks.","We empirically evaluate SCRaWl on real-world datasets and show that it outperforms other simplicial neural networks."],"url":"http://arxiv.org/abs/2404.03434v1","category":"cs.LG"}
{"created":"2024-04-04 13:24:33","title":"MEDIATE: Mutually Endorsed Distributed Incentive Acknowledgment Token Exchange","abstract":"Recent advances in multi-agent systems (MAS) have shown that incorporating peer incentivization (PI) mechanisms vastly improves cooperation. Especially in social dilemmas, communication between the agents helps to overcome sub-optimal Nash equilibria. However, incentivization tokens need to be carefully selected. Furthermore, real-world applications might yield increased privacy requirements and limited exchange. Therefore, we extend the PI protocol for mutual acknowledgment token exchange (MATE) and provide additional analysis on the impact of the chosen tokens. Building upon those insights, we propose mutually endorsed distributed incentive acknowledgment token exchange (MEDIATE), an extended PI architecture employing automatic token derivation via decentralized consensus. Empirical results show the stable agreement on appropriate tokens yielding superior performance compared to static tokens and state-of-the-art approaches in different social dilemma environments with various reward distributions.","sentences":["Recent advances in multi-agent systems (MAS) have shown that incorporating peer incentivization (PI) mechanisms vastly improves cooperation.","Especially in social dilemmas, communication between the agents helps to overcome sub-optimal Nash equilibria.","However, incentivization tokens need to be carefully selected.","Furthermore, real-world applications might yield increased privacy requirements and limited exchange.","Therefore, we extend the PI protocol for mutual acknowledgment token exchange (MATE) and provide additional analysis on the impact of the chosen tokens.","Building upon those insights, we propose mutually endorsed distributed incentive acknowledgment token exchange (MEDIATE), an extended PI architecture employing automatic token derivation via decentralized consensus.","Empirical results show the stable agreement on appropriate tokens yielding superior performance compared to static tokens and state-of-the-art approaches in different social dilemma environments with various reward distributions."],"url":"http://arxiv.org/abs/2404.03431v1","category":"cs.MA"}
{"created":"2024-04-04 13:13:47","title":"GMMCalib: Extrinsic Calibration of LiDAR Sensors using GMM-based Joint Registration","abstract":"State-of-the-art LiDAR calibration frameworks mainly use non-probabilistic registration methods such as Iterative Closest Point (ICP) and its variants. These methods suffer from biased results due to their pair-wise registration procedure as well as their sensitivity to initialization and parameterization. This often leads to misalignments in the calibration process. Probabilistic registration methods compensate for these drawbacks by specifically modeling the probabilistic nature of the observations. This paper presents GMMCalib, an automatic target-based extrinsic calibration approach for multi-LiDAR systems. Using an implementation of a Gaussian Mixture Model (GMM)-based registration method that allows joint registration of multiple point clouds, this data-driven approach is compared to ICP algorithms. We perform simulation experiments using the digital twin of the EDGAR research vehicle and validate the results in a real-world environment. We also address the local minima problem of local registration methods for extrinsic sensor calibration and use a distance-based metric to evaluate the calibration results. Our results show that an increase in robustness against sensor miscalibrations can be achieved by using GMM-based registration algorithms. The code is open source and available on GitHub.","sentences":["State-of-the-art LiDAR calibration frameworks mainly use non-probabilistic registration methods such as Iterative Closest Point (ICP) and its variants.","These methods suffer from biased results due to their pair-wise registration procedure as well as their sensitivity to initialization and parameterization.","This often leads to misalignments in the calibration process.","Probabilistic registration methods compensate for these drawbacks by specifically modeling the probabilistic nature of the observations.","This paper presents GMMCalib, an automatic target-based extrinsic calibration approach for multi-LiDAR systems.","Using an implementation of a Gaussian Mixture Model (GMM)-based registration method that allows joint registration of multiple point clouds, this data-driven approach is compared to ICP algorithms.","We perform simulation experiments using the digital twin of the EDGAR research vehicle and validate the results in a real-world environment.","We also address the local minima problem of local registration methods for extrinsic sensor calibration and use a distance-based metric to evaluate the calibration results.","Our results show that an increase in robustness against sensor miscalibrations can be achieved by using GMM-based registration algorithms.","The code is open source and available on GitHub."],"url":"http://arxiv.org/abs/2404.03427v1","category":"cs.RO"}
{"created":"2024-04-04 12:58:46","title":"Generalizable 3D Scene Reconstruction via Divide and Conquer from a Single View","abstract":"Single-view 3D reconstruction is currently approached from two dominant perspectives: reconstruction of scenes with limited diversity using 3D data supervision or reconstruction of diverse singular objects using large image priors. However, real-world scenarios are far more complex and exceed the capabilities of these methods. We therefore propose a hybrid method following a divide-and-conquer strategy. We first process the scene holistically, extracting depth and semantic information, and then leverage a single-shot object-level method for the detailed reconstruction of individual components. By following a compositional processing approach, the overall framework achieves full reconstruction of complex 3D scenes from a single image. We purposely design our pipeline to be highly modular by carefully integrating specific procedures for each processing step, without requiring an end-to-end training of the whole system. This enables the pipeline to naturally improve as future methods can replace the individual modules. We demonstrate the reconstruction performance of our approach on both synthetic and real-world scenes, comparing favorable against prior works. Project page: https://andreeadogaru.github.io/Gen3DSR.","sentences":["Single-view 3D reconstruction is currently approached from two dominant perspectives: reconstruction of scenes with limited diversity using 3D data supervision or reconstruction of diverse singular objects using large image priors.","However, real-world scenarios are far more complex and exceed the capabilities of these methods.","We therefore propose a hybrid method following a divide-and-conquer strategy.","We first process the scene holistically, extracting depth and semantic information, and then leverage a single-shot object-level method for the detailed reconstruction of individual components.","By following a compositional processing approach, the overall framework achieves full reconstruction of complex 3D scenes from a single image.","We purposely design our pipeline to be highly modular by carefully integrating specific procedures for each processing step, without requiring an end-to-end training of the whole system.","This enables the pipeline to naturally improve as future methods can replace the individual modules.","We demonstrate the reconstruction performance of our approach on both synthetic and real-world scenes, comparing favorable against prior works.","Project page: https://andreeadogaru.github.io/Gen3DSR."],"url":"http://arxiv.org/abs/2404.03421v1","category":"cs.CV"}
{"created":"2024-04-04 12:46:01","title":"MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens","abstract":"This paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM) designed specifically for video understanding. The model is capable of processing both temporal visual and textual data, making it adept at understanding the complexities of videos. Building upon the success of MiniGPT-v2, which excelled in translating visual features into the LLM space for single images and achieved impressive results on various image-text benchmarks, this paper extends the model's capabilities to process a sequence of frames, enabling it to comprehend videos. MiniGPT4-video does not only consider visual content but also incorporates textual conversations, allowing the model to effectively answer queries involving both visual and text components. The proposed model outperforms existing state-of-the-art methods, registering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF, and TVQA benchmarks respectively. Our models and code have been made publicly available here https://vision-cair.github.io/MiniGPT4-video/","sentences":["This paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM) designed specifically for video understanding.","The model is capable of processing both temporal visual and textual data, making it adept at understanding the complexities of videos.","Building upon the success of MiniGPT-v2, which excelled in translating visual features into the LLM space for single images and achieved impressive results on various image-text benchmarks, this paper extends the model's capabilities to process a sequence of frames, enabling it to comprehend videos.","MiniGPT4-video does not only consider visual content but also incorporates textual conversations, allowing the model to effectively answer queries involving both visual and text components.","The proposed model outperforms existing state-of-the-art methods, registering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF, and TVQA benchmarks respectively.","Our models and code have been made publicly available here https://vision-cair.github.io/MiniGPT4-video/"],"url":"http://arxiv.org/abs/2404.03413v1","category":"cs.CV"}
{"created":"2024-04-04 12:45:49","title":"RADIUM: Predicting and Repairing End-to-End Robot Failures using Gradient-Accelerated Sampling","abstract":"Before autonomous systems can be deployed in safety-critical applications, we must be able to understand and verify the safety of these systems. For cases where the risk or cost of real-world testing is prohibitive, we propose a simulation-based framework for a) predicting ways in which an autonomous system is likely to fail and b) automatically adjusting the system's design and control policy to preemptively mitigate those failures. Existing tools for failure prediction struggle to search over high-dimensional environmental parameters, cannot efficiently handle end-to-end testing for systems with vision in the loop, and provide little guidance on how to mitigate failures once they are discovered. We approach this problem through the lens of approximate Bayesian inference and use differentiable simulation and rendering for efficient failure case prediction and repair. For cases where a differentiable simulator is not available, we provide a gradient-free version of our algorithm, and we include a theoretical and empirical evaluation of the trade-offs between gradient-based and gradient-free methods. We apply our approach on a range of robotics and control problems, including optimizing search patterns for robot swarms, UAV formation control, and robust network control. Compared to optimization-based falsification methods, our method predicts a more diverse, representative set of failure modes, and we find that our use of differentiable simulation yields solutions that have up to 10x lower cost and requires up to 2x fewer iterations to converge relative to gradient-free techniques. In hardware experiments, we find that repairing control policies using our method leads to a 5x robustness improvement. Accompanying code and video can be found at https://mit-realm.github.io/radium/","sentences":["Before autonomous systems can be deployed in safety-critical applications, we must be able to understand and verify the safety of these systems.","For cases where the risk or cost of real-world testing is prohibitive, we propose a simulation-based framework for a) predicting ways in which an autonomous system is likely to fail and b) automatically adjusting the system's design and control policy to preemptively mitigate those failures.","Existing tools for failure prediction struggle to search over high-dimensional environmental parameters, cannot efficiently handle end-to-end testing for systems with vision in the loop, and provide little guidance on how to mitigate failures once they are discovered.","We approach this problem through the lens of approximate Bayesian inference and use differentiable simulation and rendering for efficient failure case prediction and repair.","For cases where a differentiable simulator is not available, we provide a gradient-free version of our algorithm, and we include a theoretical and empirical evaluation of the trade-offs between gradient-based and gradient-free methods.","We apply our approach on a range of robotics and control problems, including optimizing search patterns for robot swarms, UAV formation control, and robust network control.","Compared to optimization-based falsification methods, our method predicts a more diverse, representative set of failure modes, and we find that our use of differentiable simulation yields solutions that have up to 10x lower cost and requires up to 2x fewer iterations to converge relative to gradient-free techniques.","In hardware experiments, we find that repairing control policies using our method leads to a 5x robustness improvement.","Accompanying code and video can be found at https://mit-realm.github.io/radium/"],"url":"http://arxiv.org/abs/2404.03412v1","category":"cs.RO"}
{"created":"2024-04-04 12:08:28","title":"Curves in the Fourier zeros of polytopal regions and the Pompeiu problem","abstract":"We prove that any finite union $P$ of interior-disjoint polytopes in ${\\mathbb R}^d$ has the Pompeiu property, a result first proved by Williams [Wil76]. This means that if a continuous function $f$ on $R^d$ integrates to 0 on any congruent copy of $P$ then $f$ is identically 0. By a fundamental result of Brown, Schreiber and Taylor [BST73] this is equivalent to showing that the Fourier-Laplace transform of the indicator function of $P$ does not vanish identically on any 0-centered complex sphere in ${\\mathbb C}^d$ . Our proof initially follows the recent one of Machado and Robins [MR23] who are using the Brion-Barvinok formula for the Fourier-Laplace transform of a polytope. But we simplify this method considerably by removing the use of properties of Bessel function zeros. Instead we use some elementary arguments on the growth of linear combinations of exponentials with rational functions as coefficients. Our approach allows us to prove the non-existence of complex spheres of any center in the zero-set of the Fourier-Laplace transform. The planar case is even simpler in that we do not even need the Brion-Barvinok formula. We then go further in the question of which sets can be contained in the null set of the Fourier-Laplace transform of a polytope by extending results of Engel [Eng23] who showed that rationally parametrized hypersurfaces, under some mild conditions, cannot be contained in this null-set. We show that a rationally parametrized curve which is not contained in an affine hyperplane in ${\\mathbb C}^d$ cannot be contained in this null-set. Results about curves parametrized by meromorphic functions are also given.","sentences":["We prove that any finite union $P$ of interior-disjoint polytopes in ${\\mathbb R}^d$ has the Pompeiu property, a result first proved by Williams [Wil76].","This means that if a continuous function $f$ on $R^d$ integrates to 0 on any congruent copy of $P$ then $f$ is identically 0.","By a fundamental result of Brown, Schreiber and Taylor [BST73] this is equivalent to showing that the Fourier-Laplace transform of the indicator function of $P$ does not vanish identically on any 0-centered complex sphere in ${\\mathbb C}^d$ .","Our proof initially follows the recent one of Machado and Robins","[MR23] who are using the Brion-Barvinok formula for the Fourier-Laplace transform of a polytope.","But we simplify this method considerably by removing the use of properties of Bessel function zeros.","Instead we use some elementary arguments on the growth of linear combinations of exponentials with rational functions as coefficients.","Our approach allows us to prove the non-existence of complex spheres of any center in the zero-set of the Fourier-Laplace transform.","The planar case is even simpler in that we do not even need the Brion-Barvinok formula.","We then go further in the question of which sets can be contained in the null set of the Fourier-Laplace transform of a polytope by extending results of Engel [Eng23] who showed that rationally parametrized hypersurfaces, under some mild conditions, cannot be contained in this null-set.","We show that a rationally parametrized curve which is not contained in an affine hyperplane in ${\\mathbb C}^d$ cannot be contained in this null-set.","Results about curves parametrized by meromorphic functions are also given."],"url":"http://arxiv.org/abs/2404.03405v1","category":"math.CA"}
{"created":"2024-04-04 12:03:15","title":"On steady solutions of the Hall-MHD system in Besov spaces","abstract":"In this paper, we investigate the well-posedness and ill-posedness issues for the incompressible stationary Hall-magnetohydrodynamic (Hall-MHD) system in $\\mathbb{R}^3.$ We first show the existence and uniqueness of solutions provided with the forces in $\\dot B^{3/p-3}_{p,r}(\\mathbb{R}^3)$ for $1\\leq p <3$ and $r=1$. Moreover, this result can be extended to any $1\\leq r\\leq \\infty$ whenever $p=2,$ without any additional assumption on the physical parameters. On the other hand, we establish some ill-posedness results for Hall-MHD system by using the discontinuity of the solution mapping of the three-dimensional stationary Navier-Stokes equations in \\emph{critical} function spaces $\\dot{B}^{3/p-1}_{p,r}(\\mathbb{R}^3)$ ($p\\geq 3$).","sentences":["In this paper, we investigate the well-posedness and ill-posedness issues for the incompressible stationary Hall-magnetohydrodynamic (Hall-MHD) system in $\\mathbb{R}^3.$ We first show the existence and uniqueness of solutions provided with the forces in $\\dot B^{3/p-3}_{p,r}(\\mathbb{R}^3)$ for $1\\leq p <3$ and $r=1$. Moreover, this result can be extended to any $1\\leq r\\leq \\infty$ whenever $p=2,$ without any additional assumption on the physical parameters.","On the other hand, we establish some ill-posedness results for Hall-MHD system by using the discontinuity of the solution mapping of the three-dimensional stationary Navier-Stokes equations in \\emph{critical} function spaces $\\dot{B}^{3/p-1}_{p,r}(\\mathbb{R}^3)$ ($p\\geq 3$)."],"url":"http://arxiv.org/abs/2404.03402v1","category":"math.AP"}
{"created":"2024-04-04 12:00:01","title":"An asynchronous discontinuous Galerkin method for massively parallel PDE solvers","abstract":"The discontinuous Galerkin (DG) method is widely being used to solve hyperbolic partial differential equations (PDEs) due to its ability to provide high-order accurate solutions in complex geometries, capture discontinuities, and exhibit high arithmetic intensity. However, the scalability of DG-based solvers is impeded by communication bottlenecks arising from the data movement and synchronization requirements at extreme scales. To address these challenges, recent studies have focused on the development of asynchronous computing approaches for PDE solvers. Herein, we introduce the asynchronous DG (ADG) method, which combines the benefits of the DG method with asynchronous computing to overcome communication bottlenecks. The ADG method relaxes the need for data communication and synchronization at a mathematical level, allowing processing elements to operate independently regardless of the communication status, thus potentially improving the scalability of solvers. The proposed ADG method ensures flux conservation and effectively addresses challenges arising from asynchrony. To assess its stability, Fourier-mode analysis is employed to examine the dissipation and dispersion behavior of fully-discrete equations that use the DG and ADG schemes along with the Runge-Kutta (RK) time integration scheme. Furthermore, an error analysis within a statistical framework is presented, which demonstrates that the ADG method with standard numerical fluxes achieves at most first-order accuracy. To recover accuracy, we introduce asynchrony-tolerant (AT) fluxes that utilize data from multiple time levels. Extensive numerical experiments were conducted to validate the performance of the ADG-AT scheme for both linear and nonlinear problems.","sentences":["The discontinuous Galerkin (DG) method is widely being used to solve hyperbolic partial differential equations (PDEs) due to its ability to provide high-order accurate solutions in complex geometries, capture discontinuities, and exhibit high arithmetic intensity.","However, the scalability of DG-based solvers is impeded by communication bottlenecks arising from the data movement and synchronization requirements at extreme scales.","To address these challenges, recent studies have focused on the development of asynchronous computing approaches for PDE solvers.","Herein, we introduce the asynchronous DG (ADG) method, which combines the benefits of the DG method with asynchronous computing to overcome communication bottlenecks.","The ADG method relaxes the need for data communication and synchronization at a mathematical level, allowing processing elements to operate independently regardless of the communication status, thus potentially improving the scalability of solvers.","The proposed ADG method ensures flux conservation and effectively addresses challenges arising from asynchrony.","To assess its stability, Fourier-mode analysis is employed to examine the dissipation and dispersion behavior of fully-discrete equations that use the DG and ADG schemes along with the Runge-Kutta (RK) time integration scheme.","Furthermore, an error analysis within a statistical framework is presented, which demonstrates that the ADG method with standard numerical fluxes achieves at most first-order accuracy.","To recover accuracy, we introduce asynchrony-tolerant (AT) fluxes that utilize data from multiple time levels.","Extensive numerical experiments were conducted to validate the performance of the ADG-AT scheme for both linear and nonlinear problems."],"url":"http://arxiv.org/abs/2404.03399v1","category":"physics.comp-ph"}
{"created":"2024-04-04 11:57:11","title":"Characterization of the Early Dynamics of Solar Coronal Bright Fronts","abstract":"We present a comprehensive characterization of 26 CME-driven compressive waves known as Coronal Bright Fronts (CBFs) observed in the low solar corona between 2010 and 2017. These CBFs have been found to be associated with SEP events near Earth, indicating their importance in understanding space weather phenomena. The aim of this study is to analyze and describe the early dynamics of CBFs using a physics-based heliospheric SEP forecasting system known as the SPREAdFAST framework. This framework utilizes a chain of data-driven analytic and numerical models to predict SEP fluxes at multiple locations in the inner heliosphere by considering their acceleration at CMEs near the Sun and subsequent interplanetary transport. To estimate the time-dependent plasma and compression parameters of the CBFs, we utilized sequences of base-difference images obtained from the AIA instrument on board the SDO satellite, and measurements of the height-time profiles of the CMEs obtained from the LASCO instrument on board the SOHO satellite. We employed kinematic measurements and plasma model results to derive these parameters. The SPREAdFAST framework facilitated the analysis and correlation of these observations with SEP events near Earth. Our analysis yielded statistical relations and distributions for both the shocks and plasma parameters associated with the 26 CBFs investigated. By combining the observations from the AIA and LASCO instruments, as well as the data products from the SPREAdFAST framework, we obtained a comprehensive understanding of the early dynamics of CBFs, including their temporal evolution, plasma properties, and compressional characteristics. These findings contribute to the growing body of knowledge in the field and have implications for space weather forecasting and the study of SEP events.","sentences":["We present a comprehensive characterization of 26 CME-driven compressive waves known as Coronal Bright Fronts (CBFs) observed in the low solar corona between 2010 and 2017.","These CBFs have been found to be associated with SEP events near Earth, indicating their importance in understanding space weather phenomena.","The aim of this study is to analyze and describe the early dynamics of CBFs using a physics-based heliospheric SEP forecasting system known as the SPREAdFAST framework.","This framework utilizes a chain of data-driven analytic and numerical models to predict SEP fluxes at multiple locations in the inner heliosphere by considering their acceleration at CMEs near the Sun and subsequent interplanetary transport.","To estimate the time-dependent plasma and compression parameters of the CBFs, we utilized sequences of base-difference images obtained from the AIA instrument on board the SDO satellite, and measurements of the height-time profiles of the CMEs obtained from the LASCO instrument on board the SOHO satellite.","We employed kinematic measurements and plasma model results to derive these parameters.","The SPREAdFAST framework facilitated the analysis and correlation of these observations with SEP events near Earth.","Our analysis yielded statistical relations and distributions for both the shocks and plasma parameters associated with the 26 CBFs investigated.","By combining the observations from the AIA and LASCO instruments, as well as the data products from the SPREAdFAST framework, we obtained a comprehensive understanding of the early dynamics of CBFs, including their temporal evolution, plasma properties, and compressional characteristics.","These findings contribute to the growing body of knowledge in the field and have implications for space weather forecasting and the study of SEP events."],"url":"http://arxiv.org/abs/2404.03396v1","category":"astro-ph.SR"}
{"created":"2024-04-04 11:56:51","title":"Movable Antennas-Assisted Secure Transmission Without Eavesdroppers' Instantaneous CSI","abstract":"Movable antenna (MA) technology is highly promising for improving communication performance, due to its advantage of flexibly adjusting positions of antennas to reconfigure channel conditions. In this paper, we investigate MAs-assisted secure transmission under a legitimate transmitter Alice, a legitimate receiver Bob and multiple eavesdroppers. Specifically, we consider a practical scenario where Alice has no any knowledge about the instantaneous non-line-of-sight component of the wiretap channel. Under this setup, we evaluate the secrecy performance by adopting the secrecy outage probability metric, the tight approximation of which is first derived by interpreting the Rician fading as a special case of Nakagami fading and concurrently exploiting the Laguerre series approximation. Then, we minimize the secrecy outage probability by jointly optimizing the transmit beamforming and positions of antennas at Alice. However, the problem is highly non-convex because the objective includes the complex incomplete gamma function. To tackle this challenge, we, for the first time, effectively approximate the inverse of the incomplete gamma function as a simple linear model. Based on this approximation, we arrive at a simplified problem with a clear structure, which can be solved via the developed alternating projected gradient ascent (APGA) algorithm. Considering the high complexity of the APGA, we further design another scheme where the zero-forcing based beamforming is adopted by Alice, and then we transform the problem into minimizing a simple function which is only related to positions of antennas at Alice.As demonstrated by simulations, our proposed schemes achieve significant performance gains compared to conventional schemes based on fixed-position antennas.","sentences":["Movable antenna (MA) technology is highly promising for improving communication performance, due to its advantage of flexibly adjusting positions of antennas to reconfigure channel conditions.","In this paper, we investigate MAs-assisted secure transmission under a legitimate transmitter Alice, a legitimate receiver Bob and multiple eavesdroppers.","Specifically, we consider a practical scenario where Alice has no any knowledge about the instantaneous non-line-of-sight component of the wiretap channel.","Under this setup, we evaluate the secrecy performance by adopting the secrecy outage probability metric, the tight approximation of which is first derived by interpreting the Rician fading as a special case of Nakagami fading and concurrently exploiting the Laguerre series approximation.","Then, we minimize the secrecy outage probability by jointly optimizing the transmit beamforming and positions of antennas at Alice.","However, the problem is highly non-convex because the objective includes the complex incomplete gamma function.","To tackle this challenge, we, for the first time, effectively approximate the inverse of the incomplete gamma function as a simple linear model.","Based on this approximation, we arrive at a simplified problem with a clear structure, which can be solved via the developed alternating projected gradient ascent (APGA) algorithm.","Considering the high complexity of the APGA, we further design another scheme where the zero-forcing based beamforming is adopted by Alice, and then we transform the problem into minimizing a simple function which is only related to positions of antennas at Alice.","As demonstrated by simulations, our proposed schemes achieve significant performance gains compared to conventional schemes based on fixed-position antennas."],"url":"http://arxiv.org/abs/2404.03395v1","category":"cs.IT"}
{"created":"2024-04-04 11:32:17","title":"A unified Euler--Lagrange system for analyzing continuous-time accelerated gradient methods","abstract":"This paper presents an Euler--Lagrange system for a continuous-time model of the accelerated gradient methods in smooth convex optimization and proposes an associated Lyapunov-function-based convergence analysis framework. Recently, ordinary differential equations (ODEs) with dumping terms have been developed to intuitively interpret the accelerated gradient methods, and the design of unified model describing the various individual ODE models have been examined. In existing reports, the Lagrangian, which results in the Euler-Lagrange equation, and the Lyapunov function for the convergence analysis have been separately proposed for each ODE. This paper proposes a unified Euler--Lagrange system and its Lyapunov function to cover the existing various models. In the convergence analysis using the Lyapunov function, a condition that parameters in the Lagrangian and Lyapunov function must satisfy is derived, and a parameter design for improving the convergence rate naturally results in the mysterious dumping coefficients. Especially, a symmetric Bregman divergence can lead to a relaxed condition of the parameters and a resulting improved convergence rate. As an application of this study, a slight modification in the Lyapunov function establishes the similar convergence proof for ODEs with smooth approximation in nondifferentiable objective function minimization.","sentences":["This paper presents an Euler--Lagrange system for a continuous-time model of the accelerated gradient methods in smooth convex optimization and proposes an associated Lyapunov-function-based convergence analysis framework.","Recently, ordinary differential equations (ODEs) with dumping terms have been developed to intuitively interpret the accelerated gradient methods, and the design of unified model describing the various individual ODE models have been examined.","In existing reports, the Lagrangian, which results in the Euler-Lagrange equation, and the Lyapunov function for the convergence analysis have been separately proposed for each ODE.","This paper proposes a unified Euler--Lagrange system and its Lyapunov function to cover the existing various models.","In the convergence analysis using the Lyapunov function, a condition that parameters in the Lagrangian and Lyapunov function must satisfy is derived, and a parameter design for improving the convergence rate naturally results in the mysterious dumping coefficients.","Especially, a symmetric Bregman divergence can lead to a relaxed condition of the parameters and a resulting improved convergence rate.","As an application of this study, a slight modification in the Lyapunov function establishes the similar convergence proof for ODEs with smooth approximation in nondifferentiable objective function minimization."],"url":"http://arxiv.org/abs/2404.03383v1","category":"math.OC"}
{"created":"2024-04-04 11:14:01","title":"The Nearest Graph Laplacian in Frobenius Norm","abstract":"We address the problem of finding the nearest graph Laplacian to a given matrix, with the distance measured using the Frobenius norm. Specifically, for the directed graph Laplacian, we propose two novel algorithms by reformulating the problem as convex quadratic optimization problems with a special structure: one based on the active set method and the other on direct computation of Karush-Kuhn-Tucker (KKT) points. The proposed algorithms can be applied to system identification and model reduction problems involving Laplacian dynamics. We demonstrate that these algorithms possess lower time complexities and the finite termination property, unlike the interior point method and V-FISTA, the latter of which is an accelerated projected gradient method. Our numerical experiments confirm the effectiveness of the proposed algorithms.","sentences":["We address the problem of finding the nearest graph Laplacian to a given matrix, with the distance measured using the Frobenius norm.","Specifically, for the directed graph Laplacian, we propose two novel algorithms by reformulating the problem as convex quadratic optimization problems with a special structure: one based on the active set method and the other on direct computation of Karush-Kuhn-Tucker (KKT) points.","The proposed algorithms can be applied to system identification and model reduction problems involving Laplacian dynamics.","We demonstrate that these algorithms possess lower time complexities and the finite termination property, unlike the interior point method and V-FISTA, the latter of which is an accelerated projected gradient method.","Our numerical experiments confirm the effectiveness of the proposed algorithms."],"url":"http://arxiv.org/abs/2404.03371v1","category":"math.OC"}
{"created":"2024-04-04 11:09:49","title":"Graph Neural Networks for Electric and Hydraulic Data Fusion to Enhance Short-term Forecasting of Pumped-storage Hydroelectricity","abstract":"Pumped-storage hydropower plants (PSH) actively participate in grid power-frequency control and therefore often operate under dynamic conditions, which results in rapidly varying system states. Predicting these dynamically changing states is essential for comprehending the underlying sensor and machine conditions. This understanding aids in detecting anomalies and faults, ensuring the reliable operation of the connected power grid, and in identifying faulty and miscalibrated sensors. PSH are complex, highly interconnected systems encompassing electrical and hydraulic subsystems, each characterized by their respective underlying networks that can individually be represented as graphs. To take advantage of this relational inductive bias, graph neural networks (GNNs) have been separately applied to state forecasting tasks in the individual subsystems, but without considering their interdependencies. In PSH, however, these subsystems depend on the same control input, making their operations highly interdependent and interconnected. Consequently, hydraulic and electrical sensor data should be fused across PSH subsystems to improve state forecasting accuracy. This approach has not been explored in GNN literature yet because many available PSH graphs are limited to their respective subsystem boundaries, which makes the method unsuitable to be applied directly. In this work, we introduce the application of spectral-temporal graph neural networks, which leverage self-attention mechanisms to concurrently capture and learn meaningful subsystem interdependencies and the dynamic patterns observed in electric and hydraulic sensors. Our method effectively fuses data from the PSH's subsystems by operating on a unified, system-wide graph, learned directly from the data, This approach leads to demonstrably improved state forecasting performance and enhanced generalizability.","sentences":["Pumped-storage hydropower plants (PSH) actively participate in grid power-frequency control and therefore often operate under dynamic conditions, which results in rapidly varying system states.","Predicting these dynamically changing states is essential for comprehending the underlying sensor and machine conditions.","This understanding aids in detecting anomalies and faults, ensuring the reliable operation of the connected power grid, and in identifying faulty and miscalibrated sensors.","PSH are complex, highly interconnected systems encompassing electrical and hydraulic subsystems, each characterized by their respective underlying networks that can individually be represented as graphs.","To take advantage of this relational inductive bias, graph neural networks (GNNs) have been separately applied to state forecasting tasks in the individual subsystems, but without considering their interdependencies.","In PSH, however, these subsystems depend on the same control input, making their operations highly interdependent and interconnected.","Consequently, hydraulic and electrical sensor data should be fused across PSH subsystems to improve state forecasting accuracy.","This approach has not been explored in GNN literature yet because many available PSH graphs are limited to their respective subsystem boundaries, which makes the method unsuitable to be applied directly.","In this work, we introduce the application of spectral-temporal graph neural networks, which leverage self-attention mechanisms to concurrently capture and learn meaningful subsystem interdependencies and the dynamic patterns observed in electric and hydraulic sensors.","Our method effectively fuses data from the PSH's subsystems by operating on a unified, system-wide graph, learned directly from the data, This approach leads to demonstrably improved state forecasting performance and enhanced generalizability."],"url":"http://arxiv.org/abs/2404.03368v1","category":"cs.LG"}
{"created":"2024-04-04 11:09:04","title":"Photonic Quantum Computing","abstract":"Photonic quantum computation refers to quantum computation that uses photons as the physical system for doing the quantum computation. Photons are ideal quantum systems because they operate at room temperature, and photonic technologies are relatively mature. The field is largely divided between discrete- and continuous-variable photonic quantum computation. In discrete-variable (DV) photonic quantum computation, quantum information is represented by one or more modal properties (e.g. polarization) that take on distinct values from a finite set. Quantum information is processed via operations on these modal properties and eventually measured using single photon detectors. In continuous-variable (CV) photonic quantum computation, quantum information is represented by properties of the electromagnetic field that take on any value in an interval (e.g. position). The electromagnetic field is transformed via Gaussian and non-Gaussian operations, and then detected via homodyne detection. Both CV and DV photonic quantum computation have been realized experimentally and they each have a unique set of challenges that need to be overcome to achieve scalable photonic universal quantum computation. This article is an introduction to photonic quantum computing, charting its development from the early days of linear optical quantum computing to recent developments in quantum machine learning.","sentences":["Photonic quantum computation refers to quantum computation that uses photons as the physical system for doing the quantum computation.","Photons are ideal quantum systems because they operate at room temperature, and photonic technologies are relatively mature.","The field is largely divided between discrete- and continuous-variable photonic quantum computation.","In discrete-variable (DV) photonic quantum computation, quantum information is represented by one or more modal properties (e.g. polarization) that take on distinct values from a finite set.","Quantum information is processed via operations on these modal properties and eventually measured using single photon detectors.","In continuous-variable (CV) photonic quantum computation, quantum information is represented by properties of the electromagnetic field that take on any value in an interval (e.g. position).","The electromagnetic field is transformed via Gaussian and non-Gaussian operations, and then detected via homodyne detection.","Both CV and DV photonic quantum computation have been realized experimentally and they each have a unique set of challenges that need to be overcome to achieve scalable photonic universal quantum computation.","This article is an introduction to photonic quantum computing, charting its development from the early days of linear optical quantum computing to recent developments in quantum machine learning."],"url":"http://arxiv.org/abs/2404.03367v1","category":"quant-ph"}
{"created":"2024-04-04 11:04:44","title":"Space Physiology and Technology: Musculoskeletal Adaptations, Countermeasures, and the Opportunity for Wearable Robotics","abstract":"Space poses significant challenges for human physiology, leading to physiological adaptations in response to an environment vastly different from Earth. While these adaptations can be beneficial, they may not fully counteract the adverse impact of space-related stressors. A comprehensive understanding of these physiological adaptations is needed to devise effective countermeasures to support human life in space. This review focuses on the impact of the environment in space on the musculoskeletal system. It highlights the complex interplay between bone and muscle adaptation, the underlying physiological mechanisms, and their implications on astronaut health. Furthermore, the review delves into the deployed and current advances in countermeasures and proposes, as a perspective for future developments, wearable sensing and robotic technologies, such as exoskeletons, as a fitting alternative.","sentences":["Space poses significant challenges for human physiology, leading to physiological adaptations in response to an environment vastly different from Earth.","While these adaptations can be beneficial, they may not fully counteract the adverse impact of space-related stressors.","A comprehensive understanding of these physiological adaptations is needed to devise effective countermeasures to support human life in space.","This review focuses on the impact of the environment in space on the musculoskeletal system.","It highlights the complex interplay between bone and muscle adaptation, the underlying physiological mechanisms, and their implications on astronaut health.","Furthermore, the review delves into the deployed and current advances in countermeasures and proposes, as a perspective for future developments, wearable sensing and robotic technologies, such as exoskeletons, as a fitting alternative."],"url":"http://arxiv.org/abs/2404.03363v1","category":"cs.RO"}
{"created":"2024-04-04 10:51:51","title":"Implementation of complex-valued sliding mode controllers in three-phase power converters","abstract":"This paper presents two methods for implementing complex-valued sliding mode controllers in three-phase power converters. The paper includes the description of the algorithms and a detailed analysis of the proposed implementations. The methods, that are easy to code and have a low computational burden, retain the sliding mode properties of robustness and fast response and do not require any additional processing often used to decouple the dynamics of the three-phase system. The performance of the methods is compared in numerical simulations, and the algorithms are experimentally tested in a microcontroller using a Hardware-in-the-Loop platform.","sentences":["This paper presents two methods for implementing complex-valued sliding mode controllers in three-phase power converters.","The paper includes the description of the algorithms and a detailed analysis of the proposed implementations.","The methods, that are easy to code and have a low computational burden, retain the sliding mode properties of robustness and fast response and do not require any additional processing often used to decouple the dynamics of the three-phase system.","The performance of the methods is compared in numerical simulations, and the algorithms are experimentally tested in a microcontroller using a Hardware-in-the-Loop platform."],"url":"http://arxiv.org/abs/2404.03358v1","category":"eess.SY"}
{"created":"2024-04-04 10:49:52","title":"Fast Computation of Robust Dynamic Operating Envelopes Based on Non-convex OPF for Unbalanced Distribution Networks","abstract":"Robust dynamic operating envelopes (RDOEs) solve the problem of secure allocation of latent network capacity to flexible distributed energy resources (DER) in unbalanced distribution networks. As the computational complexity of RDOEs is much higher than that of DOEs, which disregard uncertainties in network parameters and DER capacity utilisation, existing approaches to computing RDOEs have relied on linearised unbalanced three-phase optimal power flow (UTOPF) models to numerate the network feasible region approximately. The use of linearised models, however, risks producing RDOEs that undermine network integrity due to inherent errors in the approximation. This letter presents a practical sensitivity-filtering technique to simplify RDOE numerical computation based on non-convex UTOPF formulations. The accuracy and efficiency of the proposed approach are demonstrated on RDOE allocation with various fairness metrics by testing on representative Australian distribution networks.","sentences":["Robust dynamic operating envelopes (RDOEs) solve the problem of secure allocation of latent network capacity to flexible distributed energy resources (DER) in unbalanced distribution networks.","As the computational complexity of RDOEs is much higher than that of DOEs, which disregard uncertainties in network parameters and DER capacity utilisation, existing approaches to computing RDOEs have relied on linearised unbalanced three-phase optimal power flow (UTOPF) models to numerate the network feasible region approximately.","The use of linearised models, however, risks producing RDOEs that undermine network integrity due to inherent errors in the approximation.","This letter presents a practical sensitivity-filtering technique to simplify RDOE numerical computation based on non-convex UTOPF formulations.","The accuracy and efficiency of the proposed approach are demonstrated on RDOE allocation with various fairness metrics by testing on representative Australian distribution networks."],"url":"http://arxiv.org/abs/2404.03355v1","category":"math.OC"}
{"created":"2024-04-04 10:37:21","title":"Inverse scattering transform for the coupled Lakshmanan-Porsezian-Daniel equation with nonzero boundary conditions","abstract":"The challenge of solving the initial value problem for the coupled Lakshmanan Porsezian Daniel equation, while considering nonzero boundary conditions at infinity, is addressed through the development of a suitable inverse scattering transform. Analytical properties of the Jost eigenfunctions are examined, along with the analysis of scattering coefficient characteristics. This analysis leads to the derivation of additional auxiliary eigenfunctions necessary for the comprehensive investigation of the fundamental eigenfunctions. Two symmetry conditions are discussed to study the eigenfunctions and scattering coefficients. These symmetry results are utilized to rigorously define the discrete spectrum and ascertain the corresponding symmetries of scattering datas. The inverse scattering problem is formulated by the Riemann-Hilbert problem. Then we can derive the exact solutions by coupled Lakshmanan Porsezian Daniel equation, the novel soliton solutions are derived and examined in detail.","sentences":["The challenge of solving the initial value problem for the coupled Lakshmanan Porsezian Daniel equation, while considering nonzero boundary conditions at infinity, is addressed through the development of a suitable inverse scattering transform.","Analytical properties of the Jost eigenfunctions are examined, along with the analysis of scattering coefficient characteristics.","This analysis leads to the derivation of additional auxiliary eigenfunctions necessary for the comprehensive investigation of the fundamental eigenfunctions.","Two symmetry conditions are discussed to study the eigenfunctions and scattering coefficients.","These symmetry results are utilized to rigorously define the discrete spectrum and ascertain the corresponding symmetries of scattering datas.","The inverse scattering problem is formulated by the Riemann-Hilbert problem.","Then we can derive the exact solutions by coupled Lakshmanan Porsezian Daniel equation, the novel soliton solutions are derived and examined in detail."],"url":"http://arxiv.org/abs/2404.03351v1","category":"nlin.SI"}
{"created":"2024-04-04 10:23:26","title":"Out-of-equilibrium thermodynamics of autocatalytic networks","abstract":"The purpose of this work is to clarify how the stoichiometric trait of autocatalytic networks, namely their absence of a conservation law, shapes their non-equilibrium behavior. To do so, we consider an autocatalytic network coupled with external species acting as food/waste materials, necessary to fulfill mass conservation. Then, we show that the production of autocatalytic species requires a conservative influx of these external species. From this, we derive the thermodynamic potential of an autocatalytic sub-network. The latter can be obtained from the usual semigrand free-energy of an open system by taking into account the conservative work associated to the influx of external species fueling the production of autocatalytic species. In the end, we identify the cost dedicated to the production of autocatalytic species and its efficiency. It reveals that sustaining steady production of species in an autocatalytic network is possible only if they are coupled with the environment.","sentences":["The purpose of this work is to clarify how the stoichiometric trait of autocatalytic networks, namely their absence of a conservation law, shapes their non-equilibrium behavior.","To do so, we consider an autocatalytic network coupled with external species acting as food/waste materials, necessary to fulfill mass conservation.","Then, we show that the production of autocatalytic species requires a conservative influx of these external species.","From this, we derive the thermodynamic potential of an autocatalytic sub-network.","The latter can be obtained from the usual semigrand free-energy of an open system by taking into account the conservative work associated to the influx of external species fueling the production of autocatalytic species.","In the end, we identify the cost dedicated to the production of autocatalytic species and its efficiency.","It reveals that sustaining steady production of species in an autocatalytic network is possible only if they are coupled with the environment."],"url":"http://arxiv.org/abs/2404.03347v1","category":"physics.chem-ph"}
{"created":"2024-04-04 10:04:44","title":"Scaling Population-Based Reinforcement Learning with GPU Accelerated Simulation","abstract":"In recent years, deep reinforcement learning (RL) has shown its effectiveness in solving complex continuous control tasks like locomotion and dexterous manipulation. However, this comes at the cost of an enormous amount of experience required for training, exacerbated by the sensitivity of learning efficiency and the policy performance to hyperparameter selection, which often requires numerous trials of time-consuming experiments. This work introduces a Population-Based Reinforcement Learning (PBRL) approach that exploits a GPU-accelerated physics simulator to enhance the exploration capabilities of RL by concurrently training multiple policies in parallel. The PBRL framework is applied to three state-of-the-art RL algorithms -- PPO, SAC, and DDPG -- dynamically adjusting hyperparameters based on the performance of learning agents. The experiments are performed on four challenging tasks in Isaac Gym -- Anymal Terrain, Shadow Hand, Humanoid, Franka Nut Pick -- by analyzing the effect of population size and mutation mechanisms for hyperparameters. The results show that PBRL agents achieve superior performance, in terms of cumulative reward, compared to non-evolutionary baseline agents. The trained agents are finally deployed in the real world for a Franka Nut Pick} task, demonstrating successful sim-to-real transfer. Code and videos of the learned policies are available on our project website.","sentences":["In recent years, deep reinforcement learning (RL) has shown its effectiveness in solving complex continuous control tasks like locomotion and dexterous manipulation.","However, this comes at the cost of an enormous amount of experience required for training, exacerbated by the sensitivity of learning efficiency and the policy performance to hyperparameter selection, which often requires numerous trials of time-consuming experiments.","This work introduces a Population-Based Reinforcement Learning (PBRL) approach that exploits a GPU-accelerated physics simulator to enhance the exploration capabilities of RL by concurrently training multiple policies in parallel.","The PBRL framework is applied to three state-of-the-art RL algorithms -- PPO, SAC, and DDPG -- dynamically adjusting hyperparameters based on the performance of learning agents.","The experiments are performed on four challenging tasks in Isaac Gym -- Anymal Terrain, Shadow Hand, Humanoid, Franka Nut Pick -- by analyzing the effect of population size and mutation mechanisms for hyperparameters.","The results show that PBRL agents achieve superior performance, in terms of cumulative reward, compared to non-evolutionary baseline agents.","The trained agents are finally deployed in the real world for a Franka Nut Pick} task, demonstrating successful sim-to-real transfer.","Code and videos of the learned policies are available on our project website."],"url":"http://arxiv.org/abs/2404.03336v1","category":"cs.RO"}
{"created":"2024-04-04 10:03:42","title":"Control and homogenization of a system of coupled parabolic equations with an oscillating coefficient","abstract":"In this article, we study the uniform null controllability problem for a system of coupled parabolic equations with an oscillating coefficient. This is done in three steps -- first, we study the spectral properties of an elliptic operator; second, we allow the system to evolve freely and obtain the required decay; third, we use a Carleman estimate to prove a suitable observability result. This uniform null controllability property is then used to homogenize the associated coupled parabolic system.","sentences":["In this article, we study the uniform null controllability problem for a system of coupled parabolic equations with an oscillating coefficient.","This is done in three steps -- first, we study the spectral properties of an elliptic operator; second, we allow the system to evolve freely and obtain the required decay; third, we use a Carleman estimate to prove a suitable observability result.","This uniform null controllability property is then used to homogenize the associated coupled parabolic system."],"url":"http://arxiv.org/abs/2404.03335v1","category":"math.AP"}
{"created":"2024-04-04 10:02:18","title":"First detection in space of the high-energy isomer of cyanomethanimine: H2CNCN","abstract":"We report the first detection in the interstellar medium of $N$-cyanomethanimine (H$_2$CNCN), the stable dimer of HCN of highest energy, and the most complex organic molecule identified in space containing the prebiotically relevant NCN backbone. We have identified a plethora of $a$-type rotational transitions with 3 $\\leq J_\\text{up} \\leq$ 11 and $K_\\text{a} \\leq$ 2 that belong to this species towards the Galactic Center G+0.693-0.027 molecular cloud, the only interstellar source showing the three cyanomethanimine isomers (including the $Z$- and $E$- isomers of $C$-cyanomethanimine, HNCHCN). We have derived a total column density for H$_2$CNCN of (2.9$\\, \\pm \\,$0.1)$\\times$10$^{12}$ cm$^{-2}$, which translates into a total molecular abundance with respect to H$_2$ of (2.1$\\, \\pm \\,$0.3)$\\times$10$^{-11}$. We have also revisited the previous detection of $E$- and $Z$-HNCHCN, and found a total $C/N$-cyanomethanimine abundance ratio of 31.8$\\, \\pm \\,$1.8 and a $Z/E$-HNCHCN ratio of 4.5$\\, \\pm \\,$0.2. While the latter can be explained on the basis of thermodynamic equilibrium, chemical kinetics are more likely responsible for the observed $C/N$-cyanomethanimine abundance ratio, where the gas-phase reaction between methanimine (CH$_2$NH) and the cyanogen radical (CN) arises as the primary formation route.","sentences":["We report the first detection in the interstellar medium of $N$-cyanomethanimine (H$_2$CNCN), the stable dimer of HCN of highest energy, and the most complex organic molecule identified in space containing the prebiotically relevant NCN backbone.","We have identified a plethora of $a$-type rotational transitions with 3 $\\leq J_\\text{up} \\leq$ 11 and $K_\\text{a} \\leq$ 2 that belong to this species towards the Galactic Center G+0.693-0.027 molecular cloud, the only interstellar source showing the three cyanomethanimine isomers (including the $Z$- and $E$- isomers of $C$-cyanomethanimine, HNCHCN).","We have derived a total column density for H$_2$CNCN of (2.9$\\, \\pm \\,$0.1)$\\times$10$^{12}$ cm$^{-2}$, which translates into a total molecular abundance with respect to H$_2$ of (2.1$\\, \\pm \\,$0.3)$\\times$10$^{-11}$. We have also revisited the previous detection of $E$- and $Z$-HNCHCN, and found a total $C/N$-cyanomethanimine abundance ratio of 31.8$\\, \\pm \\,$1.8 and a $Z/E$-HNCHCN ratio of 4.5$\\, \\pm \\,$0.2.","While the latter can be explained on the basis of thermodynamic equilibrium, chemical kinetics are more likely responsible for the observed $C/N$-cyanomethanimine abundance ratio, where the gas-phase reaction between methanimine (CH$_2$NH) and the cyanogen radical (CN) arises as the primary formation route."],"url":"http://arxiv.org/abs/2404.03334v1","category":"astro-ph.GA"}
{"created":"2024-04-04 09:57:29","title":"LancBiO: dynamic Lanczos-aided bilevel optimization via Krylov subspace","abstract":"Bilevel optimization, with broad applications in machine learning, has an intricate hierarchical structure. Gradient-based methods have emerged as a common approach to large-scale bilevel problems. However, the computation of the hyper-gradient, which involves a Hessian inverse vector product, confines the efficiency and is regarded as a bottleneck. To circumvent the inverse, we construct a sequence of low-dimensional approximate Krylov subspaces with the aid of the Lanczos process. As a result, the constructed subspace is able to dynamically and incrementally approximate the Hessian inverse vector product with less effort and thus leads to a favorable estimate of the hyper-gradient. Moreover, we propose a~provable subspace-based framework for bilevel problems where one central step is to solve a small-size tridiagonal linear system. To the best of our knowledge, this is the first time that subspace techniques are incorporated into bilevel optimization. This successful trial not only enjoys $\\mathcal{O}(\\epsilon^{-1})$ convergence rate but also demonstrates efficiency in a synthetic problem and two deep learning tasks.","sentences":["Bilevel optimization, with broad applications in machine learning, has an intricate hierarchical structure.","Gradient-based methods have emerged as a common approach to large-scale bilevel problems.","However, the computation of the hyper-gradient, which involves a Hessian inverse vector product, confines the efficiency and is regarded as a bottleneck.","To circumvent the inverse, we construct a sequence of low-dimensional approximate Krylov subspaces with the aid of the Lanczos process.","As a result, the constructed subspace is able to dynamically and incrementally approximate the Hessian inverse vector product with less effort and thus leads to a favorable estimate of the hyper-gradient.","Moreover, we propose a~provable subspace-based framework for bilevel problems where one central step is to solve a small-size tridiagonal linear system.","To the best of our knowledge, this is the first time that subspace techniques are incorporated into bilevel optimization.","This successful trial not only enjoys $\\mathcal{O}(\\epsilon^{-1})$ convergence rate but also demonstrates efficiency in a synthetic problem and two deep learning tasks."],"url":"http://arxiv.org/abs/2404.03331v1","category":"math.OC"}
{"created":"2024-04-04 09:35:48","title":"Exploring Lightweight Federated Learning for Distributed Load Forecasting","abstract":"Federated Learning (FL) is a distributed learning scheme that enables deep learning to be applied to sensitive data streams and applications in a privacy-preserving manner. This paper focuses on the use of FL for analyzing smart energy meter data with the aim to achieve comparable accuracy to state-of-the-art methods for load forecasting while ensuring the privacy of individual meter data. We show that with a lightweight fully connected deep neural network, we are able to achieve forecasting accuracy comparable to existing schemes, both at each meter source and at the aggregator, by utilising the FL framework. The use of lightweight models further reduces the energy and resource consumption caused by complex deep-learning models, making this approach ideally suited for deployment across resource-constrained smart meter systems. With our proposed lightweight model, we are able to achieve an overall average load forecasting RMSE of 0.17, with the model having a negligible energy overhead of 50 mWh when performing training and inference on an Arduino Uno platform.","sentences":["Federated Learning (FL) is a distributed learning scheme that enables deep learning to be applied to sensitive data streams and applications in a privacy-preserving manner.","This paper focuses on the use of FL for analyzing smart energy meter data with the aim to achieve comparable accuracy to state-of-the-art methods for load forecasting while ensuring the privacy of individual meter data.","We show that with a lightweight fully connected deep neural network, we are able to achieve forecasting accuracy comparable to existing schemes, both at each meter source and at the aggregator, by utilising the FL framework.","The use of lightweight models further reduces the energy and resource consumption caused by complex deep-learning models, making this approach ideally suited for deployment across resource-constrained smart meter systems.","With our proposed lightweight model, we are able to achieve an overall average load forecasting RMSE of 0.17, with the model having a negligible energy overhead of 50 mWh when performing training and inference on an Arduino Uno platform."],"url":"http://arxiv.org/abs/2404.03320v1","category":"cs.LG"}
{"created":"2024-04-04 09:35:42","title":"Early warning systems for financial markets of emerging economies","abstract":"We develop and apply a new online early warning system (EWS) for what is known in machine learning as concept drift, in economics as a regime shift and in statistics as a change point. The system goes beyond linearity assumed in many conventional methods, and is robust to heavy tails and tail-dependence in the data, making it particularly suitable for emerging markets. The key component is an effective change-point detection mechanism for conditional entropy of the data, rather than for a particular indicator of interest. Combined with recent advances in machine learning methods for high-dimensional random forests, the mechanism is capable of finding significant shifts in information transfer between interdependent time series when traditional methods fail. We explore when this happens using simulations and we provide illustrations by applying the method to Uzbekistan's commodity and equity markets as well as to Russia's equity market in 2021-2023.","sentences":["We develop and apply a new online early warning system (EWS) for what is known in machine learning as concept drift, in economics as a regime shift and in statistics as a change point.","The system goes beyond linearity assumed in many conventional methods, and is robust to heavy tails and tail-dependence in the data, making it particularly suitable for emerging markets.","The key component is an effective change-point detection mechanism for conditional entropy of the data, rather than for a particular indicator of interest.","Combined with recent advances in machine learning methods for high-dimensional random forests, the mechanism is capable of finding significant shifts in information transfer between interdependent time series when traditional methods fail.","We explore when this happens using simulations and we provide illustrations by applying the method to Uzbekistan's commodity and equity markets as well as to Russia's equity market in 2021-2023."],"url":"http://arxiv.org/abs/2404.03319v1","category":"econ.EM"}
{"created":"2024-04-04 09:23:51","title":"The complexity of non-stationary ideals","abstract":"We present an overview of results on the question of whether the non-stationary ideal of an uncountable regular cardinal $\\kappa$ can be defined by a $\\Pi_1$-formula using parameters of hereditary cardinality at most $\\kappa$. These results show that this question is deeply connected to several central topics of current research in set theory.","sentences":["We present an overview of results on the question of whether the non-stationary ideal of an uncountable regular cardinal $\\kappa$ can be defined by a $\\Pi_1$-formula using parameters of hereditary cardinality at most $\\kappa$. These results show that this question is deeply connected to several central topics of current research in set theory."],"url":"http://arxiv.org/abs/2404.03315v1","category":"math.LO"}
{"created":"2024-04-04 09:17:22","title":"M3TCM: Multi-modal Multi-task Context Model for Utterance Classification in Motivational Interviews","abstract":"Accurate utterance classification in motivational interviews is crucial to automatically understand the quality and dynamics of client-therapist interaction, and it can serve as a key input for systems mediating such interactions. Motivational interviews exhibit three important characteristics. First, there are two distinct roles, namely client and therapist. Second, they are often highly emotionally charged, which can be expressed both in text and in prosody. Finally, context is of central importance to classify any given utterance. Previous works did not adequately incorporate all of these characteristics into utterance classification approaches for mental health dialogues. In contrast, we present M3TCM, a Multi-modal, Multi-task Context Model for utterance classification. Our approach for the first time employs multi-task learning to effectively model both joint and individual components of therapist and client behaviour. Furthermore, M3TCM integrates information from the text and speech modality as well as the conversation context. With our novel approach, we outperform the state of the art for utterance classification on the recently introduced AnnoMI dataset with a relative improvement of 20% for the client- and by 15% for therapist utterance classification. In extensive ablation studies, we quantify the improvement resulting from each contribution.","sentences":["Accurate utterance classification in motivational interviews is crucial to automatically understand the quality and dynamics of client-therapist interaction, and it can serve as a key input for systems mediating such interactions.","Motivational interviews exhibit three important characteristics.","First, there are two distinct roles, namely client and therapist.","Second, they are often highly emotionally charged, which can be expressed both in text and in prosody.","Finally, context is of central importance to classify any given utterance.","Previous works did not adequately incorporate all of these characteristics into utterance classification approaches for mental health dialogues.","In contrast, we present M3TCM, a Multi-modal, Multi-task Context Model for utterance classification.","Our approach for the first time employs multi-task learning to effectively model both joint and individual components of therapist and client behaviour.","Furthermore, M3TCM integrates information from the text and speech modality as well as the conversation context.","With our novel approach, we outperform the state of the art for utterance classification on the recently introduced AnnoMI dataset with a relative improvement of 20% for the client- and by 15% for therapist utterance classification.","In extensive ablation studies, we quantify the improvement resulting from each contribution."],"url":"http://arxiv.org/abs/2404.03312v1","category":"cs.CL"}
{"created":"2024-04-04 09:13:06","title":"Non-wellfounded parsimonious proofs and non-uniform complexity","abstract":"In this paper we investigate the complexity-theoretical aspects of cyclic and non-wellfounded proofs in the context of parsimonious logic, a variant of linear logic where the exponential modality ! is interpreted as a constructor for streams over finite data. We present non-wellfounded parsimonious proof systems capturing the classes $\\mathbf{FPTIME}$ and $\\mathbf{FP}/\\mathsf{poly}$. Soundness is established via a polynomial modulus of continuity for continuous cut-elimination. Completeness relies on an encoding of polynomial Turing machines with advice.   As a byproduct of our proof methods, we establish a series of characterisation results for various finitary proof systems.","sentences":["In this paper we investigate the complexity-theoretical aspects of cyclic and non-wellfounded proofs in the context of parsimonious logic, a variant of linear logic where the exponential modality !","is interpreted as a constructor for streams over finite data.","We present non-wellfounded parsimonious proof systems capturing the classes $\\mathbf{FPTIME}$ and $\\mathbf{FP}/\\mathsf{poly}$. Soundness is established via a polynomial modulus of continuity for continuous cut-elimination.","Completeness relies on an encoding of polynomial Turing machines with advice.   ","As a byproduct of our proof methods, we establish a series of characterisation results for various finitary proof systems."],"url":"http://arxiv.org/abs/2404.03311v1","category":"cs.LO"}
{"created":"2024-04-04 09:08:04","title":"Optimistic Online Non-stochastic Control via FTRL","abstract":"This paper brings the concept of \"optimism\" to the new and promising framework of online Non-stochastic Control (NSC). Namely, we study how can NSC benefit from a prediction oracle of unknown quality responsible for forecasting future costs. The posed problem is first reduced to an optimistic learning with delayed feedback problem, which is handled through the Optimistic Follow the Regularized Leader (OFTRL) algorithmic family. This reduction enables the design of OptFTRL-C, the first Disturbance Action Controller (DAC) with optimistic policy regret bounds. These new bounds are commensurate with the oracle's accuracy, ranging from $\\mathcal{O}(1)$ for perfect predictions to the order-optimal $\\mathcal{O}(\\sqrt{T})$ even when all predictions fail. By addressing the challenge of incorporating untrusted predictions into control systems, our work contributes to the advancement of the NSC framework and paves the way towards effective and robust learning-based controllers.","sentences":["This paper brings the concept of \"optimism\" to the new and promising framework of online Non-stochastic Control (NSC).","Namely, we study how can NSC benefit from a prediction oracle of unknown quality responsible for forecasting future costs.","The posed problem is first reduced to an optimistic learning with delayed feedback problem, which is handled through the Optimistic Follow the Regularized Leader (OFTRL) algorithmic family.","This reduction enables the design of OptFTRL-C, the first Disturbance Action Controller (DAC) with optimistic policy regret bounds.","These new bounds are commensurate with the oracle's accuracy, ranging from $\\mathcal{O}(1)$ for perfect predictions to the order-optimal $\\mathcal{O}(\\sqrt{T})$ even when all predictions fail.","By addressing the challenge of incorporating untrusted predictions into control systems, our work contributes to the advancement of the NSC framework and paves the way towards effective and robust learning-based controllers."],"url":"http://arxiv.org/abs/2404.03309v1","category":"cs.LG"}
{"created":"2024-04-04 08:57:52","title":"Evolutionary game on any hypergraph","abstract":"Cooperation plays a fundamental role in societal and biological domains, and the population structure profoundly shapes the dynamics of evolution. Practically, individuals behave either altruistically or egoistically in multiple groups, such as relatives, friends and colleagues, and feedbacks from these groupwise interactions will contribute to one's cognition and behavior. Due to the intricacy within and between groups, exploration of evolutionary dynamics over hypergraphs is relatively limited to date. To uncover this conundrum, we develop a higher-order random walk framework for five distinct updating rules, thus establishing explicit conditions for cooperation emergence on hypergraphs, and finding the overlaps between groups tend to foster cooperative behaviors. Our systematic analysis quantifies how the order and hyperdegree govern evolutionary outcomes. We also discover that whenever following a group wisdom update protocol, choosing a high-fitness group to interact equally within its members, cooperators will significantly prevail throughout the community. These findings underscore a crucial role of higher-order interaction and interdisciplinary collaboration throughout a broad range of living systems, favoring social prosperity.","sentences":["Cooperation plays a fundamental role in societal and biological domains, and the population structure profoundly shapes the dynamics of evolution.","Practically, individuals behave either altruistically or egoistically in multiple groups, such as relatives, friends and colleagues, and feedbacks from these groupwise interactions will contribute to one's cognition and behavior.","Due to the intricacy within and between groups, exploration of evolutionary dynamics over hypergraphs is relatively limited to date.","To uncover this conundrum, we develop a higher-order random walk framework for five distinct updating rules, thus establishing explicit conditions for cooperation emergence on hypergraphs, and finding the overlaps between groups tend to foster cooperative behaviors.","Our systematic analysis quantifies how the order and hyperdegree govern evolutionary outcomes.","We also discover that whenever following a group wisdom update protocol, choosing a high-fitness group to interact equally within its members, cooperators will significantly prevail throughout the community.","These findings underscore a crucial role of higher-order interaction and interdisciplinary collaboration throughout a broad range of living systems, favoring social prosperity."],"url":"http://arxiv.org/abs/2404.03305v1","category":"nlin.AO"}
{"created":"2024-04-04 08:52:30","title":"How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?","abstract":"By leveraging the retrieval of information from external knowledge databases, Large Language Models (LLMs) exhibit enhanced capabilities for accomplishing many knowledge-intensive tasks. However, due to the inherent flaws of current retrieval systems, there might exist irrelevant information within those retrieving top-ranked passages. In this work, we present a comprehensive investigation into the robustness of LLMs to different types of irrelevant information under various conditions. We initially introduce a framework to construct high-quality irrelevant information that ranges from semantically unrelated, partially related, and related to questions. Furthermore, our analysis demonstrates that the constructed irrelevant information not only scores highly on similarity metrics, being highly retrieved by existing systems, but also bears semantic connections to the context. Our investigation reveals that current LLMs still face challenges in discriminating highly semantically related information and can be easily distracted by these irrelevant yet misleading contents. Besides, we also find that current solutions for handling irrelevant information have limitations in improving the robustness of LLMs to such distractions. Resources are available at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.","sentences":["By leveraging the retrieval of information from external knowledge databases, Large Language Models (LLMs) exhibit enhanced capabilities for accomplishing many knowledge-intensive tasks.","However, due to the inherent flaws of current retrieval systems, there might exist irrelevant information within those retrieving top-ranked passages.","In this work, we present a comprehensive investigation into the robustness of LLMs to different types of irrelevant information under various conditions.","We initially introduce a framework to construct high-quality irrelevant information that ranges from semantically unrelated, partially related, and related to questions.","Furthermore, our analysis demonstrates that the constructed irrelevant information not only scores highly on similarity metrics, being highly retrieved by existing systems, but also bears semantic connections to the context.","Our investigation reveals that current LLMs still face challenges in discriminating highly semantically related information and can be easily distracted by these irrelevant yet misleading contents.","Besides, we also find that current solutions for handling irrelevant information have limitations in improving the robustness of LLMs to such distractions.","Resources are available at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information."],"url":"http://arxiv.org/abs/2404.03302v1","category":"cs.CL"}
{"created":"2024-04-04 08:52:25","title":"Probing Large Language Models for Scalar Adjective Lexical Semantics and Scalar Diversity Pragmatics","abstract":"Scalar adjectives pertain to various domain scales and vary in intensity within each scale (e.g. certain is more intense than likely on the likelihood scale). Scalar implicatures arise from the consideration of alternative statements which could have been made. They can be triggered by scalar adjectives and require listeners to reason pragmatically about them. Some scalar adjectives are more likely to trigger scalar implicatures than others. This phenomenon is referred to as scalar diversity. In this study, we probe different families of Large Language Models such as GPT-4 for their knowledge of the lexical semantics of scalar adjectives and one specific aspect of their pragmatics, namely scalar diversity. We find that they encode rich lexical-semantic information about scalar adjectives. However, the rich lexical-semantic knowledge does not entail a good understanding of scalar diversity. We also compare current models of different sizes and complexities and find that larger models are not always better. Finally, we explain our probing results by leveraging linguistic intuitions and model training objectives.","sentences":["Scalar adjectives pertain to various domain scales and vary in intensity within each scale (e.g. certain is more intense than likely on the likelihood scale).","Scalar implicatures arise from the consideration of alternative statements which could have been made.","They can be triggered by scalar adjectives and require listeners to reason pragmatically about them.","Some scalar adjectives are more likely to trigger scalar implicatures than others.","This phenomenon is referred to as scalar diversity.","In this study, we probe different families of Large Language Models such as GPT-4 for their knowledge of the lexical semantics of scalar adjectives and one specific aspect of their pragmatics, namely scalar diversity.","We find that they encode rich lexical-semantic information about scalar adjectives.","However, the rich lexical-semantic knowledge does not entail a good understanding of scalar diversity.","We also compare current models of different sizes and complexities and find that larger models are not always better.","Finally, we explain our probing results by leveraging linguistic intuitions and model training objectives."],"url":"http://arxiv.org/abs/2404.03301v1","category":"cs.CL"}
{"created":"2024-04-04 08:41:10","title":"Use Cases for High Performance Research Desktops","abstract":"High Performance Research Desktops are used by HPC centers and research computing organizations to lower the barrier of entry to HPC systems. These Linux desktops are deployed alongside HPC systems, leveraging the investments in HPC compute and storage infrastructure. By serving as a gateway to HPC systems they provide users with an environment to perform setup and infrastructure tasks related to the actual HPC work. Such tasks can take significant amounts of time, are vital to the successful use of HPC systems, and can benefit from a graphical desktop environment. In addition to serving as a gateway to HPC systems, High Performance Research Desktops are also used to run interactive graphical applications like MATLAB, RStudio or VMD. This paper defines the concept of High Performance Research Desktops and summarizes use cases from Indiana University, Lund University and Technical University of Denmark, which have implemented and operated such a system for more than 10 years. Based on these use cases, possible future directions are presented.","sentences":["High Performance Research Desktops are used by HPC centers and research computing organizations to lower the barrier of entry to HPC systems.","These Linux desktops are deployed alongside HPC systems, leveraging the investments in HPC compute and storage infrastructure.","By serving as a gateway to HPC systems they provide users with an environment to perform setup and infrastructure tasks related to the actual HPC work.","Such tasks can take significant amounts of time, are vital to the successful use of HPC systems, and can benefit from a graphical desktop environment.","In addition to serving as a gateway to HPC systems, High Performance Research Desktops are also used to run interactive graphical applications like MATLAB, RStudio or VMD.","This paper defines the concept of High Performance Research Desktops and summarizes use cases from Indiana University, Lund University and Technical University of Denmark, which have implemented and operated such a system for more than 10 years.","Based on these use cases, possible future directions are presented."],"url":"http://arxiv.org/abs/2404.03298v1","category":"cs.DC"}
{"created":"2024-04-04 08:38:40","title":"Gibbs measures for hardcore-SOS models on Cayley trees","abstract":"We investigate the finite-state $p$-solid-on-solid model, for $p=\\infty$, on Cayley trees of order $k\\geq 2$ and establish a system of functional equations where each solution corresponds to a (splitting) Gibbs measure of the model. Our main result is that, for three states, $k=2,3$ and increasing coupling strength, the number of translation-invariant Gibbs measures behaves as $1\\to3\\to5\\to6\\to7$. This phase diagram is qualitatively similar to the one observed for three-state $p$-SOS models with $p>0$ and, in the case of $k=2$, we demonstrate that, on the level of the functional equations, the transition $p\\to\\infty$ is continuous.","sentences":["We investigate the finite-state $p$-solid-on-solid model, for $p=\\infty$, on Cayley trees of order $k\\geq 2$ and establish a system of functional equations where each solution corresponds to a (splitting) Gibbs measure of the model.","Our main result is that, for three states, $k=2,3$ and increasing coupling strength, the number of translation-invariant Gibbs measures behaves as $1\\to3\\to5\\to6\\to7$. This phase diagram is qualitatively similar to the one observed for three-state $p$-SOS models with $p>0$ and, in the case of $k=2$, we demonstrate that, on the level of the functional equations, the transition $p\\to\\infty$ is continuous."],"url":"http://arxiv.org/abs/2404.03297v1","category":"math-ph"}
{"created":"2024-04-04 08:18:52","title":"A complex node of the cosmic web associated with the massive galaxy cluster MACS J0600.1-2008","abstract":"MACS J0600.1-2008 (MACS0600) is an X-ray luminous, massive galaxy cluster at $z_{\\mathrm{d}}=0.43$, studied previously as part of the REionization LensIng Cluster Survey (RELICS) and ALMA Lensing Cluster Survey (ALCS) projects which revealed a complex, bimodal mass distribution and an intriguing high-redshift object behind it. Here, we report on the results of an extended strong-lensing (SL) analysis of this system. Using new JWST and ground-based Gemini-N and Keck data, we obtain 13 new spectroscopic redshifts of multiply imaged galaxies and identify 12 new photometric multiple-image systems and candidates, including two multiply imaged $z\\sim7$ objects. Taking advantage of the larger areal coverage, our analysis reveals a new bimodal, massive SL structure adjacent to the cluster which we measure spectroscopically to lie at the same redshift and whose existence was implied by previous SL-modeling analyses. While based in part on photometric systems identified in ground-based imaging requiring further verification, our extended SL model suggests that the cluster may have the second-largest critical area and effective Einstein radius observed to date, $A_{\\mathrm{crit}}\\simeq2.16\\,\\mathrm{arcmin}^2$ and $\\theta_{\\mathrm{E}}=49.7''\\pm5.0''$ for a source at $z_{\\mathrm{s}}=2$, enclosing a total mass of $M(<\\theta_{\\mathrm{E}})=(4.7\\pm0.7)\\times10^{14}\\,\\mathrm{M}_{\\odot}$. Yet another, probably related massive cluster structure, discovered in X-rays $5'$ (1.7 Mpc) further north, suggests that MACS0600 is in fact part of an even larger filamentary structure. This discovery adds to several recent detections of massive structures around SL galaxy clusters and establishes MACS0600 as a prime target for future high-redshift surveys with JWST.","sentences":["MACS J0600.1-2008 (MACS0600) is an X-ray luminous, massive galaxy cluster at $z_{\\mathrm{d}}=0.43$, studied previously as part of the REionization LensIng Cluster Survey (RELICS) and ALMA Lensing Cluster Survey (ALCS) projects which revealed a complex, bimodal mass distribution and an intriguing high-redshift object behind it.","Here, we report on the results of an extended strong-lensing (SL) analysis of this system.","Using new JWST and ground-based Gemini-N and Keck data, we obtain 13 new spectroscopic redshifts of multiply imaged galaxies and identify 12 new photometric multiple-image systems and candidates, including two multiply imaged $z\\sim7$ objects.","Taking advantage of the larger areal coverage, our analysis reveals a new bimodal, massive SL structure adjacent to the cluster which we measure spectroscopically to lie at the same redshift and whose existence was implied by previous SL-modeling analyses.","While based in part on photometric systems identified in ground-based imaging requiring further verification, our extended SL model suggests that the cluster may have the second-largest critical area and effective Einstein radius observed to date, $A_{\\mathrm{crit}}\\simeq2.16\\,\\mathrm{arcmin}^2$ and $\\theta_{\\mathrm{E}}=49.7''\\pm5.0''$ for a source at $z_{\\mathrm{s}}=2$, enclosing a total mass of $M(<\\theta_{\\mathrm{E}})=(4.7\\pm0.7)\\times10^{14}\\,\\mathrm{M}_{\\odot}$. Yet another, probably related massive cluster structure, discovered in X-rays $5'$ (1.7 Mpc) further north, suggests that MACS0600 is in fact part of an even larger filamentary structure.","This discovery adds to several recent detections of massive structures around SL galaxy clusters and establishes MACS0600 as a prime target for future high-redshift surveys with JWST."],"url":"http://arxiv.org/abs/2404.03286v1","category":"astro-ph.GA"}
{"created":"2024-04-04 08:15:42","title":"Combined DL-UL Distributed Beamforming Design for Cell-Free Massive MIMO","abstract":"We consider a cell-free massive multiple-input multiple-output system with multi-antenna access points (APs) and user equipments (UEs), where the UEs can be served in both the downlink (DL) and uplink (UL) within a resource block. We tackle the combined optimization of the DL precoders and combiners at the APs and DL UEs, respectively, together with the UL combiners and precoders at the APs and UL UEs, respectively. To this end, we propose distributed beamforming designs enabled by iterative bi-directional training (IBT) and based on the minimum mean squared error criterion. To reduce the IBT overhead and thus enhance the effective DL and UL rates, we carry out the distributed beamforming design by assuming that all the UEs are served solely in the DL and then utilize the obtained beamformers for the DL and UL data transmissions after proper scaling. Numerical results show the superiority of the proposed combined DL-UL distributed beamforming design over separate DL and UL designs, especially with short resource blocks.","sentences":["We consider a cell-free massive multiple-input multiple-output system with multi-antenna access points (APs) and user equipments (UEs), where the UEs can be served in both the downlink (DL) and uplink (UL) within a resource block.","We tackle the combined optimization of the DL precoders and combiners at the APs and DL UEs, respectively, together with the UL combiners and precoders at the APs and UL UEs, respectively.","To this end, we propose distributed beamforming designs enabled by iterative bi-directional training (IBT) and based on the minimum mean squared error criterion.","To reduce the IBT overhead and thus enhance the effective DL and UL rates, we carry out the distributed beamforming design by assuming that all the UEs are served solely in the DL and then utilize the obtained beamformers for the DL and UL data transmissions after proper scaling.","Numerical results show the superiority of the proposed combined DL-UL distributed beamforming design over separate DL and UL designs, especially with short resource blocks."],"url":"http://arxiv.org/abs/2404.03285v1","category":"cs.IT"}
{"created":"2024-04-04 08:04:28","title":"MMSE Channel Estimation in Large-Scale MIMO: Improved Robustness with Reduced Complexity","abstract":"Large-scale MIMO systems with a massive number N of individually controlled antennas pose significant challenges for minimum mean square error (MMSE) channel estimation, based on uplink pilots. The major ones arise from the computational complexity, which scales with $N^3$, and from the need for accurate knowledge of the channel statistics. This paper aims to address both challenges by introducing reduced-complexity channel estimation methods that achieve the performance of MMSE in terms of estimation accuracy and uplink spectral efficiency while demonstrating improved robustness in practical scenarios where channel statistics must be estimated. This is achieved by exploiting the inherent structure of the spatial correlation matrix induced by the array geometry. Specifically, we use a Kronecker decomposition for uniform planar arrays and a well-suited circulant approximation for uniform linear arrays. By doing so, a significantly lower computational complexity is achieved, scaling as $N\\sqrt{N}$ and $N\\log N$ for squared planar arrays and linear arrays, respectively.","sentences":["Large-scale MIMO systems with a massive number N of individually controlled antennas pose significant challenges for minimum mean square error (MMSE) channel estimation, based on uplink pilots.","The major ones arise from the computational complexity, which scales with $N^3$, and from the need for accurate knowledge of the channel statistics.","This paper aims to address both challenges by introducing reduced-complexity channel estimation methods that achieve the performance of MMSE in terms of estimation accuracy and uplink spectral efficiency while demonstrating improved robustness in practical scenarios where channel statistics must be estimated.","This is achieved by exploiting the inherent structure of the spatial correlation matrix induced by the array geometry.","Specifically, we use a Kronecker decomposition for uniform planar arrays and a well-suited circulant approximation for uniform linear arrays.","By doing so, a significantly lower computational complexity is achieved, scaling as $N\\sqrt{N}$ and $N\\log N$ for squared planar arrays and linear arrays, respectively."],"url":"http://arxiv.org/abs/2404.03279v1","category":"cs.IT"}
{"created":"2024-04-04 07:59:18","title":"Traversability-aware Adaptive Optimization for Path Planning and Control in Mountainous Terrain","abstract":"Autonomous navigation in extreme mountainous terrains poses challenges due to the presence of mobility-stressing elements and undulating surfaces, making it particularly difficult compared to conventional off-road driving scenarios. In such environments, estimating traversability solely based on exteroceptive sensors often leads to the inability to reach the goal due to a high prevalence of non-traversable areas. In this paper, we consider traversability as a relative value that integrates the robot's internal state, such as speed and torque to exhibit resilient behavior to reach its goal successfully. We separate traversability into apparent traversability and relative traversability, then incorporate these distinctions in the optimization process of sampling-based planning and motion predictive control. Our method enables the robots to execute the desired behaviors more accurately while avoiding hazardous regions and getting stuck. Experiments conducted on simulation with 27 diverse types of mountainous terrain and real-world demonstrate the robustness of the proposed framework, with increasingly better performance observed in more complex environments.","sentences":["Autonomous navigation in extreme mountainous terrains poses challenges due to the presence of mobility-stressing elements and undulating surfaces, making it particularly difficult compared to conventional off-road driving scenarios.","In such environments, estimating traversability solely based on exteroceptive sensors often leads to the inability to reach the goal due to a high prevalence of non-traversable areas.","In this paper, we consider traversability as a relative value that integrates the robot's internal state, such as speed and torque to exhibit resilient behavior to reach its goal successfully.","We separate traversability into apparent traversability and relative traversability, then incorporate these distinctions in the optimization process of sampling-based planning and motion predictive control.","Our method enables the robots to execute the desired behaviors more accurately while avoiding hazardous regions and getting stuck.","Experiments conducted on simulation with 27 diverse types of mountainous terrain and real-world demonstrate the robustness of the proposed framework, with increasingly better performance observed in more complex environments."],"url":"http://arxiv.org/abs/2404.03274v1","category":"cs.RO"}
{"created":"2024-04-04 07:48:22","title":"Run your HPC jobs in Eco-Mode: revealing the potential of user-assisted power capping in supercomputing systems","abstract":"The energy consumption of an exascale High-Performance Computing (HPC) supercomputer rivals that of tens of thousands of people in terms of electricity demand. Given the substantial energy footprint of exascale HPC systems and the increasing strain on power grids due to climate-related events, electricity providers are starting to impose power caps during critical periods to their users. In this context, it becomes crucial to implement strategies that manage the power consumption of supercomputers while simultaneously ensuring their uninterrupted operation.This paper investigates the proposition that HPC users can willingly sacrifice some processing performance to contribute to a global energy-saving initiative. With the objective of offering an efficient energy-saving strategy by involving users, we introduce a user-assisted supercomputer power-capping methodology. In this approach, users have the option to voluntarily permit their applications to operate in a power-capped mode, denoted as 'Eco-Mode', as necessary. Leveraging HPC simulations, along with energy traces and application metadata derived from a recent Top500 HPC supercomputer, we conducted an experimental campaign to quantify the effects of Eco-Mode on energy conservation and on user experience. Specifically, our study aimed to demonstrate that, with a sufficient number of users choosing Eco-Mode, the supercomputer maintains good performances within the specified power cap. Furthermore, we sought to determine the optimal conditions regarding the number of users embracing Eco-Mode and the magnitude of power capping required for applications (i.e., the intensity of Eco-Mode). Our findings indicate that decreasing the speed of jobs can decrease significantly the number of jobs that must be killed. Moreover, as the adoption of Eco-Mode increases among users, the likelihood of every job to be killed also decreases.","sentences":["The energy consumption of an exascale High-Performance Computing (HPC) supercomputer rivals that of tens of thousands of people in terms of electricity demand.","Given the substantial energy footprint of exascale HPC systems and the increasing strain on power grids due to climate-related events, electricity providers are starting to impose power caps during critical periods to their users.","In this context, it becomes crucial to implement strategies that manage the power consumption of supercomputers while simultaneously ensuring their uninterrupted operation.","This paper investigates the proposition that HPC users can willingly sacrifice some processing performance to contribute to a global energy-saving initiative.","With the objective of offering an efficient energy-saving strategy by involving users, we introduce a user-assisted supercomputer power-capping methodology.","In this approach, users have the option to voluntarily permit their applications to operate in a power-capped mode, denoted as 'Eco-Mode', as necessary.","Leveraging HPC simulations, along with energy traces and application metadata derived from a recent Top500 HPC supercomputer, we conducted an experimental campaign to quantify the effects of Eco-Mode on energy conservation and on user experience.","Specifically, our study aimed to demonstrate that, with a sufficient number of users choosing Eco-Mode, the supercomputer maintains good performances within the specified power cap.","Furthermore, we sought to determine the optimal conditions regarding the number of users embracing Eco-Mode and the magnitude of power capping required for applications (i.e., the intensity of Eco-Mode).","Our findings indicate that decreasing the speed of jobs can decrease significantly the number of jobs that must be killed.","Moreover, as the adoption of Eco-Mode increases among users, the likelihood of every job to be killed also decreases."],"url":"http://arxiv.org/abs/2404.03271v1","category":"math.OC"}
{"created":"2024-04-04 07:35:39","title":"Quantum aggregation with temporal delay","abstract":"Advanced quantum networking systems rely on efficient quantum error correction codes for their optimal realization. The rate at which the encoded information is transmitted is a fundamental limit that affects the performance of such systems. Quantum aggregation allows one to increase the transmission rate by adding multiple paths connecting two distant users. Aggregating channels of different paths allows more users to simultaneously exchange the encoded information. Recent work has shown that quantum aggregation can also reduce the number of physical resources of an error correction code when it is combined with the quantum multiplexing technique. However, the different channel lengths across the various paths means some of the encoded quantum information will arrive earlier than others and it must be stored in quantum memories. The information stored will then deteriorate due to decoherence processes leading to detrimental effects for the fidelity of the final quantum state. Here, we explore the effects of a depolarization channel that occurs for the quantum Reed-Solomon code when quantum aggregation involving different channel lengths is used. We determine the best distribution of resources among the various channels connecting two remote users. Further we estimate the coherence time required to achieve a certain fidelity. Our results will have a significant impact on the ways physical resources are distributed across a quantum network.","sentences":["Advanced quantum networking systems rely on efficient quantum error correction codes for their optimal realization.","The rate at which the encoded information is transmitted is a fundamental limit that affects the performance of such systems.","Quantum aggregation allows one to increase the transmission rate by adding multiple paths connecting two distant users.","Aggregating channels of different paths allows more users to simultaneously exchange the encoded information.","Recent work has shown that quantum aggregation can also reduce the number of physical resources of an error correction code when it is combined with the quantum multiplexing technique.","However, the different channel lengths across the various paths means some of the encoded quantum information will arrive earlier than others and it must be stored in quantum memories.","The information stored will then deteriorate due to decoherence processes leading to detrimental effects for the fidelity of the final quantum state.","Here, we explore the effects of a depolarization channel that occurs for the quantum Reed-Solomon code when quantum aggregation involving different channel lengths is used.","We determine the best distribution of resources among the various channels connecting two remote users.","Further we estimate the coherence time required to achieve a certain fidelity.","Our results will have a significant impact on the ways physical resources are distributed across a quantum network."],"url":"http://arxiv.org/abs/2404.03262v1","category":"quant-ph"}
{"created":"2024-04-04 07:34:02","title":"Restricted Phase Space Thermodynamics of NED-AdS Black Holes","abstract":"We study the Restricted Phase Space Thermodynamics (RPST) of magnetically charged by nonlinear electrodynamics (NED)-AdS black holes. The first law and the corresponding Euler relation is examined using the scaling properties. The Euler relation is found to hold automatically. We observe that the first order homogeneity of mass and the zeroth order homogeneity of the intensive variables is intact. We use numerical and graphical techniques to find the critical points of the thermodynamic quantities $S, Q, T, F$. By utilizing the re-scaling properties of the equation of states, we study the thermodynamic processes using different pairs of variables. We also analyze the phase transition behavior of free energy and other thermodynamic variables. It is observed that the thermodynamics in the RPS formalism of our concerned black hole system is similar to RN-AdS, Kerr AdS and Kerr-Sen AdS black hole. This suggests that there should be some underlying universality in the RPST formalism. However, one particular $\\mu-C$ process is found to differ from the earlier studied black holes, calling for further studies on the $\\mu-C$ processes to comment on their universality.","sentences":["We study the Restricted Phase Space Thermodynamics (RPST) of magnetically charged by nonlinear electrodynamics (NED)-AdS black holes.","The first law and the corresponding Euler relation is examined using the scaling properties.","The Euler relation is found to hold automatically.","We observe that the first order homogeneity of mass and the zeroth order homogeneity of the intensive variables is intact.","We use numerical and graphical techniques to find the critical points of the thermodynamic quantities $S, Q, T, F$.","By utilizing the re-scaling properties of the equation of states, we study the thermodynamic processes using different pairs of variables.","We also analyze the phase transition behavior of free energy and other thermodynamic variables.","It is observed that the thermodynamics in the RPS formalism of our concerned black hole system is similar to RN-AdS, Kerr AdS and Kerr-Sen AdS black hole.","This suggests that there should be some underlying universality in the RPST formalism.","However, one particular $\\mu-C$ process is found to differ from the earlier studied black holes, calling for further studies on the $\\mu-C$ processes to comment on their universality."],"url":"http://arxiv.org/abs/2404.03261v1","category":"hep-th"}
{"created":"2024-04-04 07:25:25","title":"Geometry-induced friction at a soft interface","abstract":"Soft and biological matter come in a variety of shapes and geometries. When soft surfaces that do not fit into each other due to a mismatch in Gaussian curvatures form an interface, beautiful geometry-induced patterns emerge. In this paper, we study the effect of geometry on the dynamical response of soft surfaces moving relative to each other. Using a novel experimental scheme, we measure friction between a highly bendable thin polymer sheet and a hydrogel substrate. At this soft and low-friction interface, we find a strong dependence of friction on the relative geometry of the two surfaces - a flat sheet experiences significantly larger friction on a spherical substrate than on planar or cylindrical substrate. We show that the stress developed in the sheet due to its geometrically incompatible confinement is responsible for the enhanced friction. This mechanism also leads to a transition in the nature of friction as the sheet radius is increased beyond a critical value. Our finding reveals a hitherto unnoticed non-specific mechanism of purely geometrical origin that may influence friction significantly in soft, biological, and nano-scale systems. In particular, it provokes us to re-examine our understanding of phenomena such as the curvature dependence of biological cell mobility.","sentences":["Soft and biological matter come in a variety of shapes and geometries.","When soft surfaces that do not fit into each other due to a mismatch in Gaussian curvatures form an interface, beautiful geometry-induced patterns emerge.","In this paper, we study the effect of geometry on the dynamical response of soft surfaces moving relative to each other.","Using a novel experimental scheme, we measure friction between a highly bendable thin polymer sheet and a hydrogel substrate.","At this soft and low-friction interface, we find a strong dependence of friction on the relative geometry of the two surfaces - a flat sheet experiences significantly larger friction on a spherical substrate than on planar or cylindrical substrate.","We show that the stress developed in the sheet due to its geometrically incompatible confinement is responsible for the enhanced friction.","This mechanism also leads to a transition in the nature of friction as the sheet radius is increased beyond a critical value.","Our finding reveals a hitherto unnoticed non-specific mechanism of purely geometrical origin that may influence friction significantly in soft, biological, and nano-scale systems.","In particular, it provokes us to re-examine our understanding of phenomena such as the curvature dependence of biological cell mobility."],"url":"http://arxiv.org/abs/2404.03255v1","category":"cond-mat.soft"}
{"created":"2024-04-04 07:03:13","title":"On the Range of a class of Complex Monge-Amp\u00e8re operators on compact Hermitian manifolds","abstract":"Let $(X,\\omega)$ be a compact Hermitian manifold of complex dimension $n$. Let $\\beta$ be a smooth real closed $(1,1)$ form such that there exists a function $\\rho \\in \\mbox{PSH}(X,\\beta)\\cap L^{\\infty}(X)$. We study the range of the complex non-pluripolar Monge-Amp\\`ere operator $\\langle(\\beta+dd^c\\cdot)^n\\rangle$ on weighted Monge-Amp\\`ere energy classes on $X$. In particular, when $\\rho$ is assumed to be continuous, we give a complete characterization of the range of the complex Monge-Amp\\`ere operator on the class $\\mathcal E(X,\\beta)$, which is the class of all $\\varphi \\in \\mbox{PSH}(X,\\beta)$ with full Monge-Amp\\`ere mass, i.e. $\\int_X\\langle (\\beta+dd^c\\varphi)^n\\rangle=\\int_X\\beta^n$.","sentences":["Let $(X,\\omega)$ be a compact Hermitian manifold of complex dimension $n$. Let $\\beta$ be a smooth real closed $(1,1)$ form such that there exists a function $\\rho \\in \\mbox{PSH}(X,\\beta)\\cap L^{\\infty}(X)$. We study the range of the complex non-pluripolar Monge-Amp\\`ere operator $\\langle(\\beta+dd^c\\cdot)^n\\rangle$ on weighted Monge-Amp\\`ere energy classes on $X$. In particular, when $\\rho$ is assumed to be continuous, we give a complete characterization of the range of the complex Monge-Amp\\`ere operator on the class $\\mathcal E(X,\\beta)$, which is the class of all $\\varphi \\in \\mbox{PSH}(X,\\beta)$ with full Monge-Amp\\`ere mass, i.e. $\\int_X\\langle (\\beta+dd^c\\varphi)^n\\rangle=\\int_X\\beta^n$."],"url":"http://arxiv.org/abs/2404.03246v1","category":"math.CV"}
{"created":"2024-04-04 06:56:32","title":"Knowledge-Based Convolutional Neural Network for the Simulation and Prediction of Two-Phase Darcy Flows","abstract":"Physics-informed neural networks (PINNs) have gained significant prominence as a powerful tool in the field of scientific computing and simulations. Their ability to seamlessly integrate physical principles into deep learning architectures has revolutionized the approaches to solving complex problems in physics and engineering. However, a persistent challenge faced by mainstream PINNs lies in their handling of discontinuous input data, leading to inaccuracies in predictions. This study addresses these challenges by incorporating the discretized forms of the governing equations into the PINN framework. We propose to combine the power of neural networks with the dynamics imposed by the discretized differential equations. By discretizing the governing equations, the PINN learns to account for the discontinuities and accurately capture the underlying relationships between inputs and outputs, improving the accuracy compared to traditional interpolation techniques. Moreover, by leveraging the power of neural networks, the computational cost associated with numerical simulations is substantially reduced. We evaluate our model on a large-scale dataset for the prediction of pressure and saturation fields demonstrating high accuracies compared to non-physically aware models.","sentences":["Physics-informed neural networks (PINNs) have gained significant prominence as a powerful tool in the field of scientific computing and simulations.","Their ability to seamlessly integrate physical principles into deep learning architectures has revolutionized the approaches to solving complex problems in physics and engineering.","However, a persistent challenge faced by mainstream PINNs lies in their handling of discontinuous input data, leading to inaccuracies in predictions.","This study addresses these challenges by incorporating the discretized forms of the governing equations into the PINN framework.","We propose to combine the power of neural networks with the dynamics imposed by the discretized differential equations.","By discretizing the governing equations, the PINN learns to account for the discontinuities and accurately capture the underlying relationships between inputs and outputs, improving the accuracy compared to traditional interpolation techniques.","Moreover, by leveraging the power of neural networks, the computational cost associated with numerical simulations is substantially reduced.","We evaluate our model on a large-scale dataset for the prediction of pressure and saturation fields demonstrating high accuracies compared to non-physically aware models."],"url":"http://arxiv.org/abs/2404.03240v1","category":"cs.LG"}
{"created":"2024-04-04 06:24:11","title":"Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks","abstract":"We address the challenge of sampling and remote estimation for autoregressive Markovian processes in a multi-hop wireless network with statistically-identical agents. Agents cache the most recent samples from others and communicate over wireless collision channels governed by an underlying graph topology. Our goal is to minimize time-average estimation error and/or age of information with decentralized scalable sampling and transmission policies, considering both oblivious (where decision-making is independent of the physical processes) and non-oblivious policies (where decision-making depends on physical processes). We prove that in oblivious policies, minimizing estimation error is equivalent to minimizing the age of information. The complexity of the problem, especially the multi-dimensional action spaces and arbitrary network topologies, makes theoretical methods for finding optimal transmission policies intractable. We optimize the policies using a graphical multi-agent reinforcement learning framework, where each agent employs a permutation-equivariant graph neural network architecture. Theoretically, we prove that our proposed framework exhibits desirable transferability properties, allowing transmission policies trained on small- or moderate-size networks to be executed effectively on large-scale topologies. Numerical experiments demonstrate that (i) Our proposed framework outperforms state-of-the-art baselines; (ii) The trained policies are transferable to larger networks, and their performance gains increase with the number of agents; (iii) The training procedure withstands non-stationarity even if we utilize independent learning techniques; and, (iv) Recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity in independent learning.","sentences":["We address the challenge of sampling and remote estimation for autoregressive Markovian processes in a multi-hop wireless network with statistically-identical agents.","Agents cache the most recent samples from others and communicate over wireless collision channels governed by an underlying graph topology.","Our goal is to minimize time-average estimation error and/or age of information with decentralized scalable sampling and transmission policies, considering both oblivious (where decision-making is independent of the physical processes) and non-oblivious policies (where decision-making depends on physical processes).","We prove that in oblivious policies, minimizing estimation error is equivalent to minimizing the age of information.","The complexity of the problem, especially the multi-dimensional action spaces and arbitrary network topologies, makes theoretical methods for finding optimal transmission policies intractable.","We optimize the policies using a graphical multi-agent reinforcement learning framework, where each agent employs a permutation-equivariant graph neural network architecture.","Theoretically, we prove that our proposed framework exhibits desirable transferability properties, allowing transmission policies trained on small- or moderate-size networks to be executed effectively on large-scale topologies.","Numerical experiments demonstrate that (i) Our proposed framework outperforms state-of-the-art baselines; (ii) The trained policies are transferable to larger networks, and their performance gains increase with the number of agents; (iii) The training procedure withstands non-stationarity even if we utilize independent learning techniques; and, (iv) Recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity in independent learning."],"url":"http://arxiv.org/abs/2404.03227v1","category":"eess.SP"}
{"created":"2024-04-04 06:23:21","title":"INSPIRIT: Optimizing Heterogeneous Task Scheduling through Adaptive Priority in Task-based Runtime Systems","abstract":"As modern HPC computing platforms become increasingly heterogeneous, it is challenging for programmers to fully leverage the computation power of massive parallelism offered by such heterogeneity. Consequently, task-based runtime systems have been proposed as an intermediate layer to hide the complex heterogeneity from the application programmers. The core functionality of these systems is to realize efficient task-to-resource mapping in the form of Directed Acyclic Graph (DAG) scheduling. However, existing scheduling schemes face several drawbacks to determine task priorities due to the heavy reliance on domain knowledge or failure to efficiently exploit the interaction of application and hardware characteristics. In this paper, we propose INSPIRIT, an efficient and lightweight scheduling framework with adaptive priority designed for task-based runtime systems. INSPIRIT introduces two novel task attributes \\textit{inspiring ability} and \\textit{inspiring efficiency} for dictating scheduling, eliminating the need for application domain knowledge. In addition, INSPIRIT jointly considers runtime information such as ready tasks in worker queues to guide task scheduling. This approach exposes more performance opportunities in heterogeneous hardware at runtime while effectively reducing the overhead for adjusting task priorities. Our evaluation results demonstrate that INSPIRIT achieves superior performance compared to cutting edge scheduling schemes on both synthesized and real-world task DAGs.","sentences":["As modern HPC computing platforms become increasingly heterogeneous, it is challenging for programmers to fully leverage the computation power of massive parallelism offered by such heterogeneity.","Consequently, task-based runtime systems have been proposed as an intermediate layer to hide the complex heterogeneity from the application programmers.","The core functionality of these systems is to realize efficient task-to-resource mapping in the form of Directed Acyclic Graph (DAG) scheduling.","However, existing scheduling schemes face several drawbacks to determine task priorities due to the heavy reliance on domain knowledge or failure to efficiently exploit the interaction of application and hardware characteristics.","In this paper, we propose INSPIRIT, an efficient and lightweight scheduling framework with adaptive priority designed for task-based runtime systems.","INSPIRIT introduces two novel task attributes \\textit{inspiring ability} and \\textit{inspiring efficiency} for dictating scheduling, eliminating the need for application domain knowledge.","In addition, INSPIRIT jointly considers runtime information such as ready tasks in worker queues to guide task scheduling.","This approach exposes more performance opportunities in heterogeneous hardware at runtime while effectively reducing the overhead for adjusting task priorities.","Our evaluation results demonstrate that INSPIRIT achieves superior performance compared to cutting edge scheduling schemes on both synthesized and real-world task DAGs."],"url":"http://arxiv.org/abs/2404.03226v1","category":"cs.DC"}
{"created":"2024-04-04 06:19:29","title":"Diagrammatic Negative Information","abstract":"The flow of information through a complex system can be readily understood with category theory. However, negative information (e.g., what is not possible) does not have an immediately evident categorical representation. The formalization of nategories using unconventional composition addresses this issue, and lets imposed limitations on categories be considered. However, traditional nategories abandon core categorical constructs and rely on extensive mathematical development. This creates a divide between the consideration of positive and negative information composition. In this work, we show that negative information can be considered in a natural categorical manner. This is aided by functor string diagrams, a novel flexible diagrammatic approach that can intuitively show the operation of hom-functors and natural transformations in expressions. This insight reveals how to consider the composition of negative information with foundational categorical constructs without relying on enrichment. We present diagrammatic means to consider not only nategories, but preorders more broadly. This paper introduces diagrammatic methods for the consideration of triangle inequalities and co-designs $\\mathbf{DP/Feas_{Bool}}$, showing how important cases of negative information composition can be categorically and diagrammatically approached. In particular, we develop systematic tools to rigorously consider imposed limitations on systems, advancing our mathematical understanding, and present intuitive diagrams which motivate widespread adoption and usage for various applications.","sentences":["The flow of information through a complex system can be readily understood with category theory.","However, negative information (e.g., what is not possible) does not have an immediately evident categorical representation.","The formalization of nategories using unconventional composition addresses this issue, and lets imposed limitations on categories be considered.","However, traditional nategories abandon core categorical constructs and rely on extensive mathematical development.","This creates a divide between the consideration of positive and negative information composition.","In this work, we show that negative information can be considered in a natural categorical manner.","This is aided by functor string diagrams, a novel flexible diagrammatic approach that can intuitively show the operation of hom-functors and natural transformations in expressions.","This insight reveals how to consider the composition of negative information with foundational categorical constructs without relying on enrichment.","We present diagrammatic means to consider not only nategories, but preorders more broadly.","This paper introduces diagrammatic methods for the consideration of triangle inequalities and co-designs $\\mathbf{DP/Feas_{Bool}}$, showing how important cases of negative information composition can be categorically and diagrammatically approached.","In particular, we develop systematic tools to rigorously consider imposed limitations on systems, advancing our mathematical understanding, and present intuitive diagrams which motivate widespread adoption and usage for various applications."],"url":"http://arxiv.org/abs/2404.03224v1","category":"math.CT"}
{"created":"2024-04-04 06:10:57","title":"Enabling Clean Energy Resilience with Machine Learning-Empowered Underground Hydrogen Storage","abstract":"To address the urgent challenge of climate change, there is a critical need to transition away from fossil fuels towards sustainable energy systems, with renewable energy sources playing a pivotal role. However, the inherent variability of renewable energy, without effective storage solutions, often leads to imbalances between energy supply and demand. Underground Hydrogen Storage (UHS) emerges as a promising long-term storage solution to bridge this gap, yet its widespread implementation is impeded by the high computational costs associated with high fidelity UHS simulations. This paper introduces UHS from a data-driven perspective and outlines a roadmap for integrating machine learning into UHS, thereby facilitating the large-scale deployment of UHS.","sentences":["To address the urgent challenge of climate change, there is a critical need to transition away from fossil fuels towards sustainable energy systems, with renewable energy sources playing a pivotal role.","However, the inherent variability of renewable energy, without effective storage solutions, often leads to imbalances between energy supply and demand.","Underground Hydrogen Storage (UHS) emerges as a promising long-term storage solution to bridge this gap, yet its widespread implementation is impeded by the high computational costs associated with high fidelity UHS simulations.","This paper introduces UHS from a data-driven perspective and outlines a roadmap for integrating machine learning into UHS, thereby facilitating the large-scale deployment of UHS."],"url":"http://arxiv.org/abs/2404.03222v1","category":"cs.LG"}
{"created":"2024-04-04 05:54:19","title":"iSeg: Interactive 3D Segmentation via Interactive Attention","abstract":"We present iSeg, a new interactive technique for segmenting 3D shapes. Previous works have focused mainly on leveraging pre-trained 2D foundation models for 3D segmentation based on text. However, text may be insufficient for accurately describing fine-grained spatial segmentations. Moreover, achieving a consistent 3D segmentation using a 2D model is challenging since occluded areas of the same semantic region may not be visible together from any 2D view. Thus, we design a segmentation method conditioned on fine user clicks, which operates entirely in 3D. Our system accepts user clicks directly on the shape's surface, indicating the inclusion or exclusion of regions from the desired shape partition. To accommodate various click settings, we propose a novel interactive attention module capable of processing different numbers and types of clicks, enabling the training of a single unified interactive segmentation model. We apply iSeg to a myriad of shapes from different domains, demonstrating its versatility and faithfulness to the user's specifications. Our project page is at https://threedle.github.io/iSeg/.","sentences":["We present iSeg, a new interactive technique for segmenting 3D shapes.","Previous works have focused mainly on leveraging pre-trained 2D foundation models for 3D segmentation based on text.","However, text may be insufficient for accurately describing fine-grained spatial segmentations.","Moreover, achieving a consistent 3D segmentation using a 2D model is challenging since occluded areas of the same semantic region may not be visible together from any 2D view.","Thus, we design a segmentation method conditioned on fine user clicks, which operates entirely in 3D.","Our system accepts user clicks directly on the shape's surface, indicating the inclusion or exclusion of regions from the desired shape partition.","To accommodate various click settings, we propose a novel interactive attention module capable of processing different numbers and types of clicks, enabling the training of a single unified interactive segmentation model.","We apply iSeg to a myriad of shapes from different domains, demonstrating its versatility and faithfulness to the user's specifications.","Our project page is at https://threedle.github.io/iSeg/."],"url":"http://arxiv.org/abs/2404.03219v1","category":"cs.CV"}
{"created":"2024-04-04 05:38:16","title":"Holographic Global Vortices with Novel Boundary Conditions","abstract":"The AdS/CFT correspondence has significantly impacted the study of strongly coupled systems, providing insights into various condensed matter phenomena through its holographic duality. This paper introduces an alternative approach to the breaking of the global $U(1)$ symmetry in the bulk in two asymptotically AdS spacetimes: AdS plus hard wall and AdS Blackbrane. We explore a $(3+1)$-dimensional bulk $U(1)$ symmetry-breaking phase vacuum within a global $U(1)$ $\\phi^4$ field theory, without the gauge field present in prior models. We find the symmetry-breaking vacuum requires that the mass squared is proportional to the quartic coupling. We also investigate numerical solutions of topologically stable vortex strings extending into the bulk. We find evidence that the full UV expansion is dual to a point-like boundary excitation.","sentences":["The AdS/CFT correspondence has significantly impacted the study of strongly coupled systems, providing insights into various condensed matter phenomena through its holographic duality.","This paper introduces an alternative approach to the breaking of the global $U(1)$ symmetry in the bulk in two asymptotically AdS spacetimes: AdS plus hard wall and AdS Blackbrane.","We explore a $(3+1)$-dimensional bulk $U(1)$ symmetry-breaking phase vacuum within a global $U(1)$ $\\phi^4$ field theory, without the gauge field present in prior models.","We find the symmetry-breaking vacuum requires that the mass squared is proportional to the quartic coupling.","We also investigate numerical solutions of topologically stable vortex strings extending into the bulk.","We find evidence that the full UV expansion is dual to a point-like boundary excitation."],"url":"http://arxiv.org/abs/2404.03212v1","category":"hep-th"}
{"created":"2024-04-04 05:35:59","title":"Convergence Conditions of Online Regularized Statistical Learning in Reproducing Kernel Hilbert Space With Non-Stationary Data","abstract":"We study the convergence of recursive regularized learning algorithms in the reproducing kernel Hilbert space (RKHS) with dependent and non-stationary online data streams. Firstly, we study the mean square asymptotic stability of a class of random difference equations in RKHS, whose non-homogeneous terms are martingale difference sequences dependent on the homogeneous ones. Secondly, we introduce the concept of random Tikhonov regularization path, and show that if the regularization path is slowly time-varying in some sense, then the output of the algorithm is consistent with the regularization path in mean square. Furthermore, if the data streams also satisfy the RKHS persistence of excitation condition, i.e. there exists a fixed length of time period, such that each eigenvalue of the conditional expectation of the operators induced by the input data accumulated over every time period has a uniformly positive lower bound with respect to time, then the output of the algorithm is consistent with the unknown function in mean square. Finally, for the case with independent and non-identically distributed data streams, the algorithm achieves the mean square consistency provided the marginal probability measures induced by the input data are slowly time-varying and the average measure over each fixed-length time period has a uniformly strictly positive lower bound.","sentences":["We study the convergence of recursive regularized learning algorithms in the reproducing kernel Hilbert space (RKHS) with dependent and non-stationary online data streams.","Firstly, we study the mean square asymptotic stability of a class of random difference equations in RKHS, whose non-homogeneous terms are martingale difference sequences dependent on the homogeneous ones.","Secondly, we introduce the concept of random Tikhonov regularization path, and show that if the regularization path is slowly time-varying in some sense, then the output of the algorithm is consistent with the regularization path in mean square.","Furthermore, if the data streams also satisfy the RKHS persistence of excitation condition, i.e. there exists a fixed length of time period, such that each eigenvalue of the conditional expectation of the operators induced by the input data accumulated over every time period has a uniformly positive lower bound with respect to time, then the output of the algorithm is consistent with the unknown function in mean square.","Finally, for the case with independent and non-identically distributed data streams, the algorithm achieves the mean square consistency provided the marginal probability measures induced by the input data are slowly time-varying and the average measure over each fixed-length time period has a uniformly strictly positive lower bound."],"url":"http://arxiv.org/abs/2404.03211v1","category":"cs.LG"}
{"created":"2024-04-04 05:16:18","title":"Optimal Dynamical Gauge in the Quantum Rabi Model","abstract":"In this paper, we investigate the gauge dependence of various physical observables in the quantum Rabi model (QRM) under different potential fields, arising from the Hilbert-space truncation of the atomic degree of freedom. We discover that in both the square-well potential and oscillator potential,the optimal gauges for the ground-state energy of the QRM vary with respect to the cavity frequency, with the dipole gauge being optimal in the low-frequency limit and the Coulomb gauge in the high-frequency limit of the cavity frequency. Additionally, for higher energy levels, the optimal gauge asymptotically approaches the dipole gauge. However, for the dynamical quantity out-time-order correlator (OTOC), we find the necessity to introduce an optimal dynamical gauge. We determine the optimal dynamical gauge by minimizing the mean error between the two-level OTOC and the full Hamiltonian one. We expect that this study will contribute to a more profound understanding of the subtle relation between gauge choice and the dynamics of QED systems.","sentences":["In this paper, we investigate the gauge dependence of various physical observables in the quantum Rabi model (QRM) under different potential fields, arising from the Hilbert-space truncation of the atomic degree of freedom.","We discover that in both the square-well potential and oscillator potential,the optimal gauges for the ground-state energy of the QRM vary with respect to the cavity frequency, with the dipole gauge being optimal in the low-frequency limit and the Coulomb gauge in the high-frequency limit of the cavity frequency.","Additionally, for higher energy levels, the optimal gauge asymptotically approaches the dipole gauge.","However, for the dynamical quantity out-time-order correlator (OTOC), we find the necessity to introduce an optimal dynamical gauge.","We determine the optimal dynamical gauge by minimizing the mean error between the two-level OTOC and the full Hamiltonian one.","We expect that this study will contribute to a more profound understanding of the subtle relation between gauge choice and the dynamics of QED systems."],"url":"http://arxiv.org/abs/2404.03205v1","category":"quant-ph"}
{"created":"2024-04-04 05:10:26","title":"OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field Reconstruction using Omnidirectional Images","abstract":"Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in robotics. However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. As a result, we realize differentiable optimization of the radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. To benefit the research community, the code will be made publicly available once the paper is published.","sentences":["Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in robotics.","However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images.","In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction.","Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting.","According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering.","As a result, we realize differentiable optimization of the radiance field without the requirement of cube-map rectification or tangent-plane approximation.","Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images.","To benefit the research community, the code will be made publicly available once the paper is published."],"url":"http://arxiv.org/abs/2404.03202v1","category":"cs.CV"}
{"created":"2024-04-04 05:09:38","title":"Groundhog: Linearly-Scalable Smart Contracting via Commutative Transaction Semantics","abstract":"Groundhog is a novel design for a smart contract execution engine based around concurrent execution of blocks of transactions. Unlike prior work, transactions within a block in Groundhog are not ordered relative to one another. Instead, our key design insights are first, to design a set of commutative semantics that lets the Groundhog runtime deterministically resolve concurrent accesses to shared data. Second, some storage accesses (such as withdrawing money from an account) conflict irresolvably; Groundhog therefore enforces validity constraints on persistent storage accesses via a reserve-commit process. These two ideas give Groundhog a set of semantics that, while not as powerful as traditional sequential semantics, are flexible enough to implement a wide variety of important applications, and are strictly more powerful than the semantics used in some production blockchains today. Unlike prior smart contract systems, transactions throughput never suffers from contention between transactions. Using 96 CPU cores, Groundhog can process more than half a million payment transactions per second, whether between 10M accounts or just 2.","sentences":["Groundhog is a novel design for a smart contract execution engine based around concurrent execution of blocks of transactions.","Unlike prior work, transactions within a block in Groundhog are not ordered relative to one another.","Instead, our key design insights are first, to design a set of commutative semantics that lets the Groundhog runtime deterministically resolve concurrent accesses to shared data.","Second, some storage accesses (such as withdrawing money from an account) conflict irresolvably; Groundhog therefore enforces validity constraints on persistent storage accesses via a reserve-commit process.","These two ideas give Groundhog a set of semantics that, while not as powerful as traditional sequential semantics, are flexible enough to implement a wide variety of important applications, and are strictly more powerful than the semantics used in some production blockchains today.","Unlike prior smart contract systems, transactions throughput never suffers from contention between transactions.","Using 96 CPU cores, Groundhog can process more than half a million payment transactions per second, whether between 10M accounts or just 2."],"url":"http://arxiv.org/abs/2404.03201v1","category":"cs.DC"}
{"created":"2024-04-04 04:57:55","title":"A Rolling Horizon Restoration Framework for Post-disaster Restoration of Electrical Distribution Networks","abstract":"Severe weather events such as floods, hurricanes, earthquakes, and large wind or ice storms can cause extensive damage to electrical distribution networks, requiring a multi-day restoration effort. Complicating the recovery process is the lack of complete and accurate information regarding the extent and locations of damages, at least during the initial part of the recovery process. These factors make workforce planning challenging. In this paper, we adopt a rolling horizon restoration framework whereby repairs are planned for adjustable finite length restoration windows. Considering both repair times as well as travel times, we show that the optimal scheduling problem with multiple crews, each with their own time budget, can be recast in terms of a cost constrained reward maximizing mTSP (traveling salesman problem) on doubly weighted graphs, where the objective is to maximize the aggregate reward earned during the upcoming restoration window, provided no crew violates its time budget and certain electrical continuity constraints are met. We propose a mixed integer linear programming (MILP) model for solving the above problem which is validated on standard IEEE PES test feeder networks.","sentences":["Severe weather events such as floods, hurricanes, earthquakes, and large wind or ice storms can cause extensive damage to electrical distribution networks, requiring a multi-day restoration effort.","Complicating the recovery process is the lack of complete and accurate information regarding the extent and locations of damages, at least during the initial part of the recovery process.","These factors make workforce planning challenging.","In this paper, we adopt a rolling horizon restoration framework whereby repairs are planned for adjustable finite length restoration windows.","Considering both repair times as well as travel times, we show that the optimal scheduling problem with multiple crews, each with their own time budget, can be recast in terms of a cost constrained reward maximizing mTSP (traveling salesman problem) on doubly weighted graphs, where the objective is to maximize the aggregate reward earned during the upcoming restoration window, provided no crew violates its time budget and certain electrical continuity constraints are met.","We propose a mixed integer linear programming (MILP) model for solving the above problem which is validated on standard IEEE PES test feeder networks."],"url":"http://arxiv.org/abs/2404.03197v1","category":"eess.SY"}
{"created":"2024-04-04 04:30:38","title":"Reservoir Sampling over Joins","abstract":"Sampling over joins is a fundamental task in large-scale data analytics. Instead of computing the full join results, which could be massive, a uniform sample of the join results would suffice for many purposes, such as answering analytical queries or training machine learning models. In this paper, we study the problem of how to maintain a random sample over joins while the tuples are streaming in. Without the join, this problem can be solved by some simple and classical reservoir sampling algorithms. However, the join operator makes the problem significantly harder, as the join size can be polynomially larger than the input. We present a new algorithm for this problem that achieves a near-linear complexity. The key technical components are a generalized reservoir sampling algorithm that supports a predicate, and a dynamic index for sampling over joins. We also conduct extensive experiments on both graph and relational data over various join queries, and the experimental results demonstrate significant performance improvement over the state of the art.","sentences":["Sampling over joins is a fundamental task in large-scale data analytics.","Instead of computing the full join results, which could be massive, a uniform sample of the join results would suffice for many purposes, such as answering analytical queries or training machine learning models.","In this paper, we study the problem of how to maintain a random sample over joins while the tuples are streaming in.","Without the join, this problem can be solved by some simple and classical reservoir sampling algorithms.","However, the join operator makes the problem significantly harder, as the join size can be polynomially larger than the input.","We present a new algorithm for this problem that achieves a near-linear complexity.","The key technical components are a generalized reservoir sampling algorithm that supports a predicate, and a dynamic index for sampling over joins.","We also conduct extensive experiments on both graph and relational data over various join queries, and the experimental results demonstrate significant performance improvement over the state of the art."],"url":"http://arxiv.org/abs/2404.03194v1","category":"cs.DB"}
{"created":"2024-04-04 04:25:01","title":"Foundation of Floer homotopy theory I: Flow categories","abstract":"We construct a stable infinity category with objects flow categories and morphisms flow bimodules; our construction has many flavors, related to a choice of bordism theory, and we discuss in particular framed bordism and the bordism theory of complex oriented derived orbifolds. In this setup, the construction of homotopy types associated to Floer-theoretic data is immediate: the moduli spaces of solutions to Floer's equation assemble into a flow category with respect to the appropriate bordism theory, and the associated Floer homotopy types arise as suitable mapping spectra in this category. The definition of these mapping spectra is sufficiently explicit to allow a direct interpretation of the Floer homotopy groups as Floer bordism groups. In the setting of framed bordism, we show that the category we construct is a model for the category of spectra. We implement the construction of Floer homotopy types in this new formalism for the case of Hamiltonian Floer theory.","sentences":["We construct a stable infinity category with objects flow categories and morphisms flow bimodules; our construction has many flavors, related to a choice of bordism theory, and we discuss in particular framed bordism and the bordism theory of complex oriented derived orbifolds.","In this setup, the construction of homotopy types associated to Floer-theoretic data is immediate: the moduli spaces of solutions to Floer's equation assemble into a flow category with respect to the appropriate bordism theory, and the associated Floer homotopy types arise as suitable mapping spectra in this category.","The definition of these mapping spectra is sufficiently explicit to allow a direct interpretation of the Floer homotopy groups as Floer bordism groups.","In the setting of framed bordism, we show that the category we construct is a model for the category of spectra.","We implement the construction of Floer homotopy types in this new formalism for the case of Hamiltonian Floer theory."],"url":"http://arxiv.org/abs/2404.03193v1","category":"math.SG"}
{"created":"2024-04-04 04:02:05","title":"RAnGE: Reachability Analysis for Guaranteed Ergodicity","abstract":"This paper investigates performance guarantees on coverage-based ergodic exploration methods in environments containing disturbances. Ergodic exploration methods generate trajectories for autonomous robots such that time spent in an area is proportional to the utility of exploring in the area. However, providing formal performance guarantees for ergodic exploration methods is still an open challenge due to the complexities in the problem formulation. In this work, we propose to formulate ergodic search as a differential game, in which a controller and external disturbance force seek to minimize and maximize the ergodic metric, respectively. Through an extended-state Bolza-form transform of the ergodic problem, we demonstrate it is possible to use techniques from reachability analysis to solve for optimal controllers that guarantee coverage and are robust against disturbances. Our approach leverages neural-network based methods to obtain approximate value function solutions for reachability problems that mitigate the increased computational scaling due to the extended state. As a result, we are able to compute continuous value functions for the ergodic exploration problem and provide performance guarantees for coverage under disturbances. Simulated and experimental results demonstrate the efficacy of our approach to generate robust ergodic trajectories for search and exploration with external disturbance force.","sentences":["This paper investigates performance guarantees on coverage-based ergodic exploration methods in environments containing disturbances.","Ergodic exploration methods generate trajectories for autonomous robots such that time spent in an area is proportional to the utility of exploring in the area.","However, providing formal performance guarantees for ergodic exploration methods is still an open challenge due to the complexities in the problem formulation.","In this work, we propose to formulate ergodic search as a differential game, in which a controller and external disturbance force seek to minimize and maximize the ergodic metric, respectively.","Through an extended-state Bolza-form transform of the ergodic problem, we demonstrate it is possible to use techniques from reachability analysis to solve for optimal controllers that guarantee coverage and are robust against disturbances.","Our approach leverages neural-network based methods to obtain approximate value function solutions for reachability problems that mitigate the increased computational scaling due to the extended state.","As a result, we are able to compute continuous value functions for the ergodic exploration problem and provide performance guarantees for coverage under disturbances.","Simulated and experimental results demonstrate the efficacy of our approach to generate robust ergodic trajectories for search and exploration with external disturbance force."],"url":"http://arxiv.org/abs/2404.03186v1","category":"cs.RO"}
{"created":"2024-04-04 03:57:08","title":"$\\frac{5}{2}$ fractional quantum Hall state in GaAs with Landau level mixing","abstract":"The Landau level mixing is the key in understanding the mysterious 5/2 fractional quantum Hall effect in GaAs quantum well. Theoretical calculations with and without Landau level mixing show striking differences. However, the way to deal with the considerable strong Landau level mixing in GaAs is still unsatisfactory. We develop a method combining the screening and the perturbation theories to study the nature of the 5/2 fractional quantum Hall effect in GaAs efficiently. The screening which has been succeed in explaining ZnO systems integrates out the low-energy Landau levels close to the related Landau level, while the other high-energy Landau levels are integrated out by the perturbation theory. We find that the ground states still hold the quasi-triplet degeneracy which implies the Pfaffian nature of the system. Furthermore, the particle-hole symmetry is only weakly violated since the particle-hole parity is close to unity. We propose that the ground state can be described by the two-component trial state cluster constructed by the superposition of the Pfaffian and anti-Pfaffian states with varied weights depending on the external conditions. In the experimental environment the two weights are close, corresponding that its thermal conductance slightly biased from 2.5 quanta can be understood straightforwardly.","sentences":["The Landau level mixing is the key in understanding the mysterious 5/2 fractional quantum Hall effect in GaAs quantum well.","Theoretical calculations with and without Landau level mixing show striking differences.","However, the way to deal with the considerable strong Landau level mixing in GaAs is still unsatisfactory.","We develop a method combining the screening and the perturbation theories to study the nature of the 5/2 fractional quantum Hall effect in GaAs efficiently.","The screening which has been succeed in explaining ZnO systems integrates out the low-energy Landau levels close to the related Landau level, while the other high-energy Landau levels are integrated out by the perturbation theory.","We find that the ground states still hold the quasi-triplet degeneracy which implies the Pfaffian nature of the system.","Furthermore, the particle-hole symmetry is only weakly violated since the particle-hole parity is close to unity.","We propose that the ground state can be described by the two-component trial state cluster constructed by the superposition of the Pfaffian and anti-Pfaffian states with varied weights depending on the external conditions.","In the experimental environment the two weights are close, corresponding that its thermal conductance slightly biased from 2.5 quanta can be understood straightforwardly."],"url":"http://arxiv.org/abs/2404.03185v1","category":"cond-mat.str-el"}
{"created":"2024-04-04 03:28:44","title":"Berezinskii-Kosterlitz-Thouless transitions in a ferromagnetic superfluid: effects of axial magnetization","abstract":"An easy-plane ferromagnetic spin-1 Bose gas undergoes two Berezinskii-Kosterlitz-Thouless (BKT) transitions, associated with mass and spin superfluidity respectively. We study the effect of axial magnetization on the superfluid properties of this system. We find that nonzero axial magnetization couples mass and spin superflow, via a mechanism analogous to the Andreev-Bashkin effect present in two-component superfluids. With sufficiently large axial magnetization mass and spin superfluidity arise simultaneously. The cross-over to this phase provides a finite-temperature generalization of the zero-temperature broken-axisymmetric to easy-axis transition. We present analytic relations connecting mass and spin superfluidity with experimentally observable coherence of the three spinor components and local magnetization.","sentences":["An easy-plane ferromagnetic spin-1","Bose gas undergoes two Berezinskii-Kosterlitz-Thouless (BKT) transitions, associated with mass and spin superfluidity respectively.","We study the effect of axial magnetization on the superfluid properties of this system.","We find that nonzero axial magnetization couples mass and spin superflow, via a mechanism analogous to the Andreev-Bashkin effect present in two-component superfluids.","With sufficiently large axial magnetization mass and spin superfluidity arise simultaneously.","The cross-over to this phase provides a finite-temperature generalization of the zero-temperature broken-axisymmetric to easy-axis transition.","We present analytic relations connecting mass and spin superfluidity with experimentally observable coherence of the three spinor components and local magnetization."],"url":"http://arxiv.org/abs/2404.03178v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-04 03:14:52","title":"Centimeter-Scale Achromatic Hybrid Metalens Design: A New Paradigm Based on Differentiable Ray Tracing in the Visible Spectrum","abstract":"Single metalenses are limited by their physical constraints, precluding themselves from achieving high numerical aperture across a wide visible spectral band in large-aperture applications. A hybrid system that integrates a metalens with a refractive lens can address this issue, yet previous designs lacked sufficient flexibility. Here, by reanalyzing the generalized Snell's law, we introduce a new paradigm for the hybrid metalens design based on differentiable ray tracing. Through joint optimization of the phase distribution of the metalens and refractive lens parameters, our system achieves achromatic performance within the broad spectral range of 440-700 nm, with an aperture of 1 cm and an f-number of 1.4. Owing to the differentiable nature of the proposed system, it can be seamlessly integrated as the optical front-end into any differentiable computational imaging system. Our system offers unprecedented opportunities for the advancement of metalenses in innovative optical design and computational imaging domains.","sentences":["Single metalenses are limited by their physical constraints, precluding themselves from achieving high numerical aperture across a wide visible spectral band in large-aperture applications.","A hybrid system that integrates a metalens with a refractive lens can address this issue, yet previous designs lacked sufficient flexibility.","Here, by reanalyzing the generalized Snell's law, we introduce a new paradigm for the hybrid metalens design based on differentiable ray tracing.","Through joint optimization of the phase distribution of the metalens and refractive lens parameters, our system achieves achromatic performance within the broad spectral range of 440-700 nm, with an aperture of 1 cm and an f-number of 1.4.","Owing to the differentiable nature of the proposed system, it can be seamlessly integrated as the optical front-end into any differentiable computational imaging system.","Our system offers unprecedented opportunities for the advancement of metalenses in innovative optical design and computational imaging domains."],"url":"http://arxiv.org/abs/2404.03173v1","category":"physics.optics"}
{"created":"2024-04-04 03:11:24","title":"SEPE-SQED: Symbolic Quick Error Detection by Semantically Equivalent Program Execution","abstract":"Symbolic quick error detection (SQED) has greatly improved efficiency in formal chip verification. However, it has a limitation in detecting single-instruction bugs due to its reliance on the self-consistency property. To address this, we propose a new variant called symbolic quick error detection by semantically equivalent program execution (SEPE-SQED), which utilizes program synthesis techniques to find sequences with equivalent meanings to original instructions. SEPE-SQED effectively detects single-instruction bugs by differentiating their impact on the original instruction and its semantically equivalent program (instruction sequence). To manage the search space associated with program synthesis, we introduce the CEGIS based on the highest priority first algorithm. The experimental results show that our proposed CEGIS approach improves the speed of generating the desired set of equivalent programs by 50% in time compared to previous methods. Compared to SQED, SEPE-SQED offers a wider variety of instruction combinations and can provide a shorter trace for triggering bugs in certain scenarios.","sentences":["Symbolic quick error detection (SQED) has greatly improved efficiency in formal chip verification.","However, it has a limitation in detecting single-instruction bugs due to its reliance on the self-consistency property.","To address this, we propose a new variant called symbolic quick error detection by semantically equivalent program execution (SEPE-SQED), which utilizes program synthesis techniques to find sequences with equivalent meanings to original instructions.","SEPE-SQED effectively detects single-instruction bugs by differentiating their impact on the original instruction and its semantically equivalent program (instruction sequence).","To manage the search space associated with program synthesis, we introduce the CEGIS based on the highest priority first algorithm.","The experimental results show that our proposed CEGIS approach improves the speed of generating the desired set of equivalent programs by 50% in time compared to previous methods.","Compared to SQED, SEPE-SQED offers a wider variety of instruction combinations and can provide a shorter trace for triggering bugs in certain scenarios."],"url":"http://arxiv.org/abs/2404.03172v1","category":"cs.SE"}
{"created":"2024-04-04 02:55:31","title":"Maximal Entropy Measures for Non-Accessible Topological Skew Products","abstract":"In this paper we establish a dichotomy for the ergodic measures of maximal entropy for partially hyperbolic diffeomorphisms with one-dimensional compact center leaves which are virtually skew products over (transitive) Anosov homeomorphism. We prove that if the whole manifold is the unique minimal invariant set saturated by unstable foliation, then either there exists a unique measure of maximal entropy which is non-hyperbolic or there are exactly two hyperbolic ergodic measures of maximal entropy.","sentences":["In this paper we establish a dichotomy for the ergodic measures of maximal entropy for partially hyperbolic diffeomorphisms with one-dimensional compact center leaves which are virtually skew products over (transitive) Anosov homeomorphism.","We prove that if the whole manifold is the unique minimal invariant set saturated by unstable foliation, then either there exists a unique measure of maximal entropy which is non-hyperbolic or there are exactly two hyperbolic ergodic measures of maximal entropy."],"url":"http://arxiv.org/abs/2404.03169v1","category":"math.DS"}
{"created":"2024-04-04 02:30:51","title":"LTRDetector: Exploring Long-Term Relationship for Advanced Persistent Threats Detection","abstract":"Advanced Persistent Threat (APT) is challenging to detect due to prolonged duration, infrequent occurrence, and adept concealment techniques. Existing approaches primarily concentrate on the observable traits of attack behaviors, neglecting the intricate relationships formed throughout the persistent attack lifecycle. Thus, we present an innovative APT detection framework named LTRDetector, implementing an end-to-end holistic operation. LTRDetector employs an innovative graph embedding technique to retain comprehensive contextual information, then derives long-term features from these embedded provenance graphs. During the process, we compress the data of the system provenance graph for effective feature learning. Furthermore, in order to detect attacks conducted by using zero-day exploits, we captured the system's regular behavior and detects abnormal activities without relying on predefined attack signatures. We also conducted extensive evaluations using five prominent datasets, the efficacy evaluation of which underscores the superiority of LTRDetector compared to existing state-of-the-art techniques.","sentences":["Advanced Persistent Threat (APT) is challenging to detect due to prolonged duration, infrequent occurrence, and adept concealment techniques.","Existing approaches primarily concentrate on the observable traits of attack behaviors, neglecting the intricate relationships formed throughout the persistent attack lifecycle.","Thus, we present an innovative APT detection framework named LTRDetector, implementing an end-to-end holistic operation.","LTRDetector employs an innovative graph embedding technique to retain comprehensive contextual information, then derives long-term features from these embedded provenance graphs.","During the process, we compress the data of the system provenance graph for effective feature learning.","Furthermore, in order to detect attacks conducted by using zero-day exploits, we captured the system's regular behavior and detects abnormal activities without relying on predefined attack signatures.","We also conducted extensive evaluations using five prominent datasets, the efficacy evaluation of which underscores the superiority of LTRDetector compared to existing state-of-the-art techniques."],"url":"http://arxiv.org/abs/2404.03162v1","category":"cs.CR"}
{"created":"2024-04-04 02:15:16","title":"HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud","abstract":"Extracting keypoint locations from input hand frames, known as 3D hand pose estimation, is a critical task in various human-computer interaction applications. Essentially, the 3D hand pose estimation can be regarded as a 3D point subset generative problem conditioned on input frames. Thanks to the recent significant progress on diffusion-based generative models, hand pose estimation can also benefit from the diffusion model to estimate keypoint locations with high quality. However, directly deploying the existing diffusion models to solve hand pose estimation is non-trivial, since they cannot achieve the complex permutation mapping and precise localization. Based on this motivation, this paper proposes HandDiff, a diffusion-based hand pose estimation model that iteratively denoises accurate hand pose conditioned on hand-shaped image-point clouds. In order to recover keypoint permutation and accurate location, we further introduce joint-wise condition and local detail condition. Experimental results demonstrate that the proposed HandDiff significantly outperforms the existing approaches on four challenging hand pose benchmark datasets. Codes and pre-trained models are publicly available at https://github.com/cwc1260/HandDiff.","sentences":["Extracting keypoint locations from input hand frames, known as 3D hand pose estimation, is a critical task in various human-computer interaction applications.","Essentially, the 3D hand pose estimation can be regarded as a 3D point subset generative problem conditioned on input frames.","Thanks to the recent significant progress on diffusion-based generative models, hand pose estimation can also benefit from the diffusion model to estimate keypoint locations with high quality.","However, directly deploying the existing diffusion models to solve hand pose estimation is non-trivial, since they cannot achieve the complex permutation mapping and precise localization.","Based on this motivation, this paper proposes HandDiff, a diffusion-based hand pose estimation model that iteratively denoises accurate hand pose conditioned on hand-shaped image-point clouds.","In order to recover keypoint permutation and accurate location, we further introduce joint-wise condition and local detail condition.","Experimental results demonstrate that the proposed HandDiff significantly outperforms the existing approaches on four challenging hand pose benchmark datasets.","Codes and pre-trained models are publicly available at https://github.com/cwc1260/HandDiff."],"url":"http://arxiv.org/abs/2404.03159v1","category":"cs.CV"}
{"created":"2024-04-04 02:12:36","title":"Stabilization in two-species chemotaxis systems with singular sensitivity and Lotka-Volterra competitive kinetics","abstract":"The current paper is concerned with the stabilization in the following parabolic-parabolic-elliptic chemotaxis system with singular sensitivity and Lotka-Volterra competitive kinetics, \\begin{equation} \\begin{cases} u_t=\\Delta u-\\chi_1 \\nabla\\cdot (\\frac{u}{w} \\nabla w)+u(a_1-b_1u-c_1v) ,\\quad &x\\in \\Omega\\cr v_t=\\Delta v-\\chi_2 \\nabla\\cdot (\\frac{v}{w} \\nabla w)+v(a_2-b_2v-c_2u),\\quad &x\\in \\Omega\\cr 0=\\Delta w-\\mu w +\\nu u+ \\lambda v,\\quad &x\\in \\Omega \\cr \\frac{\\partial u}{\\partial n}=\\frac{\\partial v}{\\partial n}=\\frac{\\partial w}{\\partial n}=0,\\quad &x\\in\\partial\\Omega, \\end{cases} \\end{equation} where $\\Omega \\subset \\mathbb{R}^N$ is a bounded smooth domain, and $\\chi_i,a_i, b_i, c_i$ ($i=1,2$) and $\\mu,\\, \\nu, \\, \\lambda$ are positive constants. In [25], we proved that for any given nonnegative initial data $u_0,v_0\\in C^0(\\bar\\Omega)$ with $u_0+v_0\\not \\equiv 0$, (0.1) has a unique globally defined classical solution provided that $\\min\\{a_1,a_2\\}$ is large relative to $\\chi_1,\\chi_2$, and $u_0+v_0$ is not small in the case that $(\\chi_1-\\chi_2)^2\\le \\max\\{4\\chi_1,4\\chi_2\\}$ and $u_0+v_0$ is neither small nor big in the case that $(\\chi_1-\\chi_2)^2>\\max\\{4\\chi_1,4\\chi_2\\}$. In this paper, we proved that (0.1) has a unique positive constant solution $(u^*,v^*,w^*)$, where $$ u^*=\\frac{a_1b_2-c_1a_2}{b_1b_2-c_1c_2},\\quad v^*=\\frac{b_1a_2-a_1c_2}{b_1b_2-c_1c_2}, \\quad w^*=\\frac{\\nu}{\\mu}u^*+\\frac{\\lambda}{\\mu} v^*. $$ We obtain some explicit conditions on $\\chi_1,\\chi_2$ which ensure that the positive constant solution $(u^*,v^*,w^*)$ is globally stable in the sense that for any given nonnegative initial data $u_0,v_0\\in C^0(\\bar\\Omega)$ with $u_0\\not \\equiv 0$ and $v_0\\not \\equiv 0$, $$ \\lim_{t\\to\\infty}\\Big(\\|u(t,\\cdot;u_0,v_0)-u^*\\|_\\infty +\\|v(t,\\cdot;u_0,v_0)-v^*\\|_\\infty+\\|w(t,\\cdot;u_0,v_0)-w^*\\|_\\infty\\Big)=0. $$","sentences":["The current paper is concerned with the stabilization in the following parabolic-parabolic-elliptic chemotaxis system with singular sensitivity and Lotka-Volterra competitive kinetics, \\begin{equation} \\begin{cases} u_t=\\Delta u-\\chi_1 \\nabla\\cdot (\\frac{u}{w} \\nabla w)+u(a_1-b_1u-c_1v) ,\\quad &x\\in \\Omega\\cr v_t=\\Delta v-\\chi_2 \\nabla\\cdot (\\frac{v}{w} \\nabla w)+v(a_2-b_2v-c_2u),\\quad &x\\in \\Omega\\cr 0=\\Delta w-\\mu w +\\nu u+","\\lambda v,\\quad &x\\in \\Omega","\\cr \\frac{\\partial u}{\\partial n}=\\frac{\\partial v}{\\partial n}=\\frac{\\partial w}{\\partial n}=0,\\quad &x\\in\\partial\\Omega, \\end{cases} \\end{equation} where $\\Omega \\subset \\mathbb{R}^N$ is a bounded smooth domain, and $\\chi_i,a_i, b_i, c_i$ ($i=1,2$) and $\\mu,\\, \\nu, \\, \\lambda$ are positive constants.","In [25], we proved that for any given nonnegative initial data $u_0,v_0\\in C^0(\\bar\\Omega)$ with $u_0+v_0\\not \\equiv 0$, (0.1) has a unique globally defined classical solution provided that $\\min\\{a_1,a_2\\}$ is large relative to $\\chi_1,\\chi_2$, and $u_0+v_0$ is not small in the case that $(\\chi_1-\\chi_2)^2\\le \\max\\{4\\chi_1,4\\chi_2\\}$ and $u_0+v_0$ is neither small nor big in the case that $(\\chi_1-\\chi_2)^2>\\max\\{4\\chi_1,4\\chi_2\\}$. In this paper, we proved that (0.1) has a unique positive constant solution $(u^*,v^*,w^*)$, where $$ u^*=\\frac{a_1b_2-c_1a_2}{b_1b_2-c_1c_2},\\quad v^*=\\frac{b_1a_2-a_1c_2}{b_1b_2-c_1c_2}, \\quad w^*=\\frac{\\nu}{\\mu}u^*+\\frac{\\lambda}{\\mu} v^*.","$$ We obtain some explicit conditions on $\\chi_1,\\chi_2$ which ensure that the positive constant solution $(u^*,v^*,w^*)$ is globally stable in the sense that for any given nonnegative initial data $u_0,v_0\\in C^0(\\bar\\Omega)$ with $u_0\\not \\equiv 0$ and $v_0\\not \\equiv 0$, $$ \\lim_{t\\to\\infty}\\Big(\\|u(t,\\cdot;u_0,v_0)-u^*\\|_\\infty +\\|v(t,\\cdot;u_0,v_0)-v^*\\|_\\infty+\\|w(t,\\cdot;u_0,v_0)-w^*\\|_\\infty\\Big)=0.","$$"],"url":"http://arxiv.org/abs/2404.03158v1","category":"math.AP"}
{"created":"2024-04-04 02:09:15","title":"Irreducible symplectic varieties via relative Prym varieties","abstract":"Generalizing work of Markushevich--Tikhomirov and Arbarello--Sacc\\`a--Ferretti, we use relative Prym varieties to construct Lagrangian fibered symplectic varieties in infinitely many dimensions. We then give criteria for when the construction yields primitive symplectic varieties, respectively, irreducible symplectic varieties. The starting point of the construction is a K3 surface endowed with an anti-symplectic involution and an effective linear system on the quotient surface. We give sufficient conditions on the linear system to ensure that the relative Prym varieties satisfy the criteria above. As a consequence, we produce infinite series of irreducible symplectic varieties.","sentences":["Generalizing work of Markushevich--Tikhomirov and Arbarello--Sacc\\`a--Ferretti, we use relative Prym varieties to construct Lagrangian fibered symplectic varieties in infinitely many dimensions.","We then give criteria for when the construction yields primitive symplectic varieties, respectively, irreducible symplectic varieties.","The starting point of the construction is a K3 surface endowed with an anti-symplectic involution and an effective linear system on the quotient surface.","We give sufficient conditions on the linear system to ensure that the relative Prym varieties satisfy the criteria above.","As a consequence, we produce infinite series of irreducible symplectic varieties."],"url":"http://arxiv.org/abs/2404.03157v1","category":"math.AG"}
{"created":"2024-04-04 02:09:12","title":"The Dynamics of Debris Disk Creation in Neutron Star Mergers","abstract":"The detection of GW170817/AT2017gfo inaugurated an era of multimessenger astrophysics, in which gravitational wave and multiwavelength photon observations complement one another to provide unique insight on astrophysical systems. A broad theoretical consensus exists in which the photon phenomenology of neutron star mergers largely rests upon the evolution of the small amount of matter left on bound orbits around the black hole or massive neutron star remaining after the merger. Because this accretion disk is far from inflow equilibrium, its subsequent evolution depends very strongly on its initial state, yet very little is known about how this state is determined. Using both snapshot and tracer particle data from a numerical relativity/MHD simulation of an equal-mass neutron star merger that collapses to a black hole, we show how gravitational forces arising in a non-axisymmetric, dynamical spacetime supplement hydrodynamical effects in shaping the initial structure of the bound debris disk. The work done by hydrodynamical forces is ${\\sim}10$ times greater than that due to time-dependent gravity. Although gravitational torques prior to remnant relaxation are an order of magnitude larger than hydrodynamical torques, their intrinsic sign symmetry leads to strong cancellation; as a result, hydrodynamical and gravitational torques have comparable effect. We also show that the debris disk's initial specific angular momentum distribution is sharply peaked at roughly the specific angular momentum of the merged neutron star's outer layers, a few $r_g c$, and identify the regulating mechanism.","sentences":["The detection of GW170817/AT2017gfo inaugurated an era of multimessenger astrophysics, in which gravitational wave and multiwavelength photon observations complement one another to provide unique insight on astrophysical systems.","A broad theoretical consensus exists in which the photon phenomenology of neutron star mergers largely rests upon the evolution of the small amount of matter left on bound orbits around the black hole or massive neutron star remaining after the merger.","Because this accretion disk is far from inflow equilibrium, its subsequent evolution depends very strongly on its initial state, yet very little is known about how this state is determined.","Using both snapshot and tracer particle data from a numerical relativity/MHD simulation of an equal-mass neutron star merger that collapses to a black hole, we show how gravitational forces arising in a non-axisymmetric, dynamical spacetime supplement hydrodynamical effects in shaping the initial structure of the bound debris disk.","The work done by hydrodynamical forces is ${\\sim}10$ times greater than that due to time-dependent gravity.","Although gravitational torques prior to remnant relaxation are an order of magnitude larger than hydrodynamical torques, their intrinsic sign symmetry leads to strong cancellation; as a result, hydrodynamical and gravitational torques have comparable effect.","We also show that the debris disk's initial specific angular momentum distribution is sharply peaked at roughly the specific angular momentum of the merged neutron star's outer layers, a few $r_g c$, and identify the regulating mechanism."],"url":"http://arxiv.org/abs/2404.03156v1","category":"astro-ph.HE"}
{"created":"2024-04-04 02:07:15","title":"TEGRA -- Scaling Up Terascale Graph Processing with Disaggregated Computing","abstract":"Graphs are essential for representing relationships in various domains, driving modern AI applications such as graph analytics and neural networks across science, engineering, cybersecurity, transportation, and economics. However, the size of modern graphs are rapidly expanding, posing challenges for traditional CPUs and GPUs in meeting real-time processing demands. As a result, hardware accelerators for graph processing have been proposed. However, the largest graphs that can be handled by these systems is still modest often targeting Twitter graph(1.4B edges approximately). This paper aims to address this limitation by developing a graph accelerator capable of terascale graph processing. Scale out architectures, architectures where nodes are replicated to expand to larger datasets, are natural for handling larger graphs. We argue that this approach is not appropriate for very large-scale graphs because it leads to under utilization of both memory resources and compute resources. Additionally, vertex and edge processing have different access patterns. Communication overheads also pose further challenges in designing scalable architectures. To overcome these issues, this paper proposes TEGRA, a scale-up architecture for terascale graph processing. TEGRA leverages a composable computing system with disaggregated resources and a communication architecture inspired by Active Messages. By employing direct communication between cores and optimizing memory interconnect utilization, TEGRA effectively reduces communication overhead and improves resource utilization, therefore enabling efficient processing of terascale graphs.","sentences":["Graphs are essential for representing relationships in various domains, driving modern AI applications such as graph analytics and neural networks across science, engineering, cybersecurity, transportation, and economics.","However, the size of modern graphs are rapidly expanding, posing challenges for traditional CPUs and GPUs in meeting real-time processing demands.","As a result, hardware accelerators for graph processing have been proposed.","However, the largest graphs that can be handled by these systems is still modest often targeting Twitter graph(1.4B edges approximately).","This paper aims to address this limitation by developing a graph accelerator capable of terascale graph processing.","Scale out architectures, architectures where nodes are replicated to expand to larger datasets, are natural for handling larger graphs.","We argue that this approach is not appropriate for very large-scale graphs because it leads to under utilization of both memory resources and compute resources.","Additionally, vertex and edge processing have different access patterns.","Communication overheads also pose further challenges in designing scalable architectures.","To overcome these issues, this paper proposes TEGRA, a scale-up architecture for terascale graph processing.","TEGRA leverages a composable computing system with disaggregated resources and a communication architecture inspired by Active Messages.","By employing direct communication between cores and optimizing memory interconnect utilization, TEGRA effectively reduces communication overhead and improves resource utilization, therefore enabling efficient processing of terascale graphs."],"url":"http://arxiv.org/abs/2404.03155v1","category":"cs.ET"}
{"created":"2024-04-04 01:46:31","title":"Design and Evaluation of a Compact 3D End-effector Assistive Robot for Adaptive Arm Support","abstract":"We developed a 3D end-effector type of upper limb assistive robot, named as Assistive Robotic Arm Extender (ARAE), that provides transparency movement and adaptive arm support control to achieve home-based therapy and training in the real environment. The proposed system composes five degrees of freedom, including three active motors and two passive joints at the end-effector module. The core structure of the system is based on a parallel mechanism. The kinematic and dynamic modeling are illustrated in detail. The proposed adaptive arm support control framework calculates the compensated force based on the estimated human arm posture in 3D space. It firstly estimates human arm joint angles using two proposed methods: fixed torso and sagittal plane models without using external sensors such as IMUs, magnetic sensors, or depth cameras. The experiments were carried out to evaluate the performance of the two proposed angle estimation methods. Then, the estimated human joint angles were input into the human upper limb dynamics model to derive the required support force generated by the robot. The muscular activities were measured to evaluate the effects of the proposed framework. The obvious reduction of muscular activities was exhibited when participants were tested with the ARAE under an adaptive arm gravity compensation control framework. The overall results suggest that the ARAE system, when combined with the proposed control framework, has the potential to offer adaptive arm support. This integration could enable effective training with Activities of Daily Living (ADLs) and interaction with real environments.","sentences":["We developed a 3D end-effector type of upper limb assistive robot, named as Assistive Robotic Arm Extender (ARAE), that provides transparency movement and adaptive arm support control to achieve home-based therapy and training in the real environment.","The proposed system composes five degrees of freedom, including three active motors and two passive joints at the end-effector module.","The core structure of the system is based on a parallel mechanism.","The kinematic and dynamic modeling are illustrated in detail.","The proposed adaptive arm support control framework calculates the compensated force based on the estimated human arm posture in 3D space.","It firstly estimates human arm joint angles using two proposed methods: fixed torso and sagittal plane models without using external sensors such as IMUs, magnetic sensors, or depth cameras.","The experiments were carried out to evaluate the performance of the two proposed angle estimation methods.","Then, the estimated human joint angles were input into the human upper limb dynamics model to derive the required support force generated by the robot.","The muscular activities were measured to evaluate the effects of the proposed framework.","The obvious reduction of muscular activities was exhibited when participants were tested with the ARAE under an adaptive arm gravity compensation control framework.","The overall results suggest that the ARAE system, when combined with the proposed control framework, has the potential to offer adaptive arm support.","This integration could enable effective training with Activities of Daily Living (ADLs) and interaction with real environments."],"url":"http://arxiv.org/abs/2404.03149v1","category":"cs.RO"}
{"created":"2024-04-04 01:18:11","title":"Robust Partitioning and Operation for Maximal Uncertain-Load Delivery in Distribution Grids","abstract":"To mitigate the vulnerability of distribution grids to severe weather events, some electric utilities use preemptive de-energization as the primary line of defense, causing significant power outages. In such instances, networked microgrids could improve resiliency and maximize load delivery, though the modeling of three-phase unbalanced network physics and computational complexity pose challenges. These challenges are further exacerbated by an increased penetration of uncertain loads. In this paper, we present a two-stage mixed-integer robust optimization problem that configures and operates networked microgrids, and is guaranteed to be robust and feasible to all realizations of loads within a specified uncertainty set, while maximizing load delivery. To solve this problem, we propose a cutting-plane algorithm, with convergence guarantees, which approximates a convex recourse function with sub-gradient cuts. Finally, we provide a detailed case study on the IEEE 37-bus test system to demonstrate the economic benefits of networking microgrids to maximize uncertain-load delivery.","sentences":["To mitigate the vulnerability of distribution grids to severe weather events, some electric utilities use preemptive de-energization as the primary line of defense, causing significant power outages.","In such instances, networked microgrids could improve resiliency and maximize load delivery, though the modeling of three-phase unbalanced network physics and computational complexity pose challenges.","These challenges are further exacerbated by an increased penetration of uncertain loads.","In this paper, we present a two-stage mixed-integer robust optimization problem that configures and operates networked microgrids, and is guaranteed to be robust and feasible to all realizations of loads within a specified uncertainty set, while maximizing load delivery.","To solve this problem, we propose a cutting-plane algorithm, with convergence guarantees, which approximates a convex recourse function with sub-gradient cuts.","Finally, we provide a detailed case study on the IEEE 37-bus test system to demonstrate the economic benefits of networking microgrids to maximize uncertain-load delivery."],"url":"http://arxiv.org/abs/2404.03137v1","category":"math.OC"}
{"created":"2024-04-04 01:16:49","title":"Promatch: Extending the Reach of Real-Time Quantum Error Correction with Adaptive Predecoding","abstract":"Fault-tolerant quantum computing relies on Quantum Error Correction, which encodes logical qubits into data and parity qubits. Error decoding is the process of translating the measured parity bits into types and locations of errors. To prevent a backlog of errors, error decoding must be performed in real-time. Minimum Weight Perfect Matching (MWPM) is an accurate decoding algorithm for surface code, and recent research has demonstrated real-time implementations of MWPM (RT-MWPM) for a distance of up to 9. Unfortunately, beyond d=9, the number of flipped parity bits in the syndrome, referred to as the Hamming weight of the syndrome, exceeds the capabilities of existing RT-MWPM decoders. In this work, our goal is to enable larger distance RT-MWPM decoders by using adaptive predecoding that converts high Hamming weight syndromes into low Hamming weight syndromes, which are accurately decoded by the RT-MWPM decoder. An effective predecoder must balance both accuracy and coverage. In this paper, we propose Promatch, a real-time adaptive predecoder that predecodes both simple and complex patterns using a locality-aware, greedy approach. Our approach ensures two crucial factors: 1) high accuracy in prematching flipped bits, ensuring that the decoding accuracy is not hampered by the predecoder, and 2) enough coverage adjusted based on the main decoder's capability given the time constraints. Promatch represents the first real-time decoding framework capable of decoding surface codes of distances 11 and 13, achieving an LER of $2.6\\times 10^{-14}$ for distance 13. Moreover, we demonstrate that running Promatch concurrently with the recently proposed Astrea-G achieves LER equivalent to MWPM LER, $3.4\\times10^{-15}$, for distance 13, representing the first real-time accurate decoder for up-to a distance of 13.","sentences":["Fault-tolerant quantum computing relies on Quantum Error Correction, which encodes logical qubits into data and parity qubits.","Error decoding is the process of translating the measured parity bits into types and locations of errors.","To prevent a backlog of errors, error decoding must be performed in real-time.","Minimum Weight Perfect Matching (MWPM) is an accurate decoding algorithm for surface code, and recent research has demonstrated real-time implementations of MWPM (RT-MWPM) for a distance of up to 9.","Unfortunately, beyond d=9, the number of flipped parity bits in the syndrome, referred to as the Hamming weight of the syndrome, exceeds the capabilities of existing RT-MWPM decoders.","In this work, our goal is to enable larger distance RT-MWPM decoders by using adaptive predecoding that converts high Hamming weight syndromes into low Hamming weight syndromes, which are accurately decoded by the RT-MWPM decoder.","An effective predecoder must balance both accuracy and coverage.","In this paper, we propose Promatch, a real-time adaptive predecoder that predecodes both simple and complex patterns using a locality-aware, greedy approach.","Our approach ensures two crucial factors: 1) high accuracy in prematching flipped bits, ensuring that the decoding accuracy is not hampered by the predecoder, and 2) enough coverage adjusted based on the main decoder's capability given the time constraints.","Promatch represents the first real-time decoding framework capable of decoding surface codes of distances 11 and 13, achieving an LER of $2.6\\times 10^{-14}$ for distance 13.","Moreover, we demonstrate that running Promatch concurrently with the recently proposed Astrea-G achieves LER equivalent to MWPM LER, $3.4\\times10^{-15}$, for distance 13, representing the first real-time accurate decoder for up-to a distance of 13."],"url":"http://arxiv.org/abs/2404.03136v1","category":"quant-ph"}
{"created":"2024-04-04 00:47:13","title":"Biodegradable Interactive Materials","abstract":"The sense of touch is fundamental to how we interact with the physical and digital world. Conventional interactive surfaces and tactile interfaces use electronic sensors embedded into objects, however this approach poses serious challenges both for environmental sustainability and a future of truly ubiquitous interaction systems where information is encoded into everyday objects. In this work, we present Biodegradable Interactive Materials: backyard-compostable interactive interfaces that leverage information encoded in material properties. Inspired by natural systems, we propose an architecture that programmatically encodes multidimensional information into materials themselves and combines them with wearable devices that extend human senses to perceive the embedded data. We combine unrefined biological matter from plants and algae like chlorella with natural minerals like graphite and magnetite to produce materials with varying electrical, magnetic, and surface properties. We perform in-depth analysis using physics models, computational simulations, and real-world experiments to characterize their information density and develop decoding methods. Our passive, chip-less materials can robustly encode 12 bits of information, equivalent to 4096 unique classes. We further develop wearable device prototypes that can decode this information during touch interactions using off-the-shelf sensors. We demonstrate sample applications such as customized buttons, tactile maps, and interactive surfaces. We further demonstrate the natural degradation of these interactive materials in degrade outdoors within 21 days and perform a comparative environmental analysis of the benefits of this approach.","sentences":["The sense of touch is fundamental to how we interact with the physical and digital world.","Conventional interactive surfaces and tactile interfaces use electronic sensors embedded into objects, however this approach poses serious challenges both for environmental sustainability and a future of truly ubiquitous interaction systems where information is encoded into everyday objects.","In this work, we present Biodegradable Interactive Materials: backyard-compostable interactive interfaces that leverage information encoded in material properties.","Inspired by natural systems, we propose an architecture that programmatically encodes multidimensional information into materials themselves and combines them with wearable devices that extend human senses to perceive the embedded data.","We combine unrefined biological matter from plants and algae like chlorella with natural minerals like graphite and magnetite to produce materials with varying electrical, magnetic, and surface properties.","We perform in-depth analysis using physics models, computational simulations, and real-world experiments to characterize their information density and develop decoding methods.","Our passive, chip-less materials can robustly encode 12 bits of information, equivalent to 4096 unique classes.","We further develop wearable device prototypes that can decode this information during touch interactions using off-the-shelf sensors.","We demonstrate sample applications such as customized buttons, tactile maps, and interactive surfaces.","We further demonstrate the natural degradation of these interactive materials in degrade outdoors within 21 days and perform a comparative environmental analysis of the benefits of this approach."],"url":"http://arxiv.org/abs/2404.03130v1","category":"cs.HC"}
{"created":"2024-04-04 00:44:58","title":"Performant Automatic Differentiation of Local Coupled Cluster Theories: Response Properties and Ab Initio Molecular Dynamics","abstract":"In this work, we introduce a differentiable implementation of the local natural orbital coupled cluster (LNOCC) method within the automatic differentiation framework of the PySCFAD package. The implementation is comprehensively tuned for enhanced performance, which enables the calculation of first-order static response properties on medium-sized molecular systems using coupled cluster theory with single, double, and perturbative triple excitations [CCSD(T)]. We evaluate the accuracy of our method by benchmarking it against the canonical CCSD(T) reference for nuclear gradients, dipole moments, and geometry optimizations. In addition, we demonstrate the possibility of property calculations for chemically interesting systems through the computation of bond orders and M\\\"ossbauer spectroscopy parameters for a [NiFe]-hydrogenase active site model, along with the simulation of infrared (IR) spectra via ab initio LNO-CC molecular dynamics for a protonated water hexamer.","sentences":["In this work, we introduce a differentiable implementation of the local natural orbital coupled cluster (LNOCC) method within the automatic differentiation framework of the PySCFAD package.","The implementation is comprehensively tuned for enhanced performance, which enables the calculation of first-order static response properties on medium-sized molecular systems using coupled cluster theory with single, double, and perturbative triple excitations [CCSD(T)].","We evaluate the accuracy of our method by benchmarking it against the canonical CCSD(T) reference for nuclear gradients, dipole moments, and geometry optimizations.","In addition, we demonstrate the possibility of property calculations for chemically interesting systems through the computation of bond orders and M\\\"ossbauer spectroscopy parameters for a [NiFe]-hydrogenase active site model, along with the simulation of infrared (IR) spectra via ab initio LNO-CC molecular dynamics for a protonated water hexamer."],"url":"http://arxiv.org/abs/2404.03129v1","category":"physics.chem-ph"}
{"created":"2024-04-03 23:59:59","title":"Utilizing Computer Vision for Continuous Monitoring of Vaccine Side Effects in Experimental Mice","abstract":"The demand for improved efficiency and accuracy in vaccine safety assessments is increasing. Here, we explore the application of computer vision technologies to automate the monitoring of experimental mice for potential side effects after vaccine administration. Traditional observation methods are labor-intensive and lack the capability for continuous monitoring. By deploying a computer vision system, our research aims to improve the efficiency and accuracy of vaccine safety assessments. The methodology involves training machine learning models on annotated video data of mice behaviors pre- and post-vaccination. Preliminary results indicate that computer vision effectively identify subtle changes, signaling possible side effects. Therefore, our approach has the potential to significantly enhance the monitoring process in vaccine trials in animals, providing a practical solution to the limitations of human observation.","sentences":["The demand for improved efficiency and accuracy in vaccine safety assessments is increasing.","Here, we explore the application of computer vision technologies to automate the monitoring of experimental mice for potential side effects after vaccine administration.","Traditional observation methods are labor-intensive and lack the capability for continuous monitoring.","By deploying a computer vision system, our research aims to improve the efficiency and accuracy of vaccine safety assessments.","The methodology involves training machine learning models on annotated video data of mice behaviors pre- and post-vaccination.","Preliminary results indicate that computer vision effectively identify subtle changes, signaling possible side effects.","Therefore, our approach has the potential to significantly enhance the monitoring process in vaccine trials in animals, providing a practical solution to the limitations of human observation."],"url":"http://arxiv.org/abs/2404.03121v1","category":"cs.CV"}
{"created":"2024-04-04 17:58:21","title":"Decoupling Static and Hierarchical Motion Perception for Referring Video Segmentation","abstract":"Referring video segmentation relies on natural language expressions to identify and segment objects, often emphasizing motion clues. Previous works treat a sentence as a whole and directly perform identification at the video-level, mixing up static image-level cues with temporal motion cues. However, image-level features cannot well comprehend motion cues in sentences, and static cues are not crucial for temporal perception. In fact, static cues can sometimes interfere with temporal perception by overshadowing motion cues. In this work, we propose to decouple video-level referring expression understanding into static and motion perception, with a specific emphasis on enhancing temporal comprehension. Firstly, we introduce an expression-decoupling module to make static cues and motion cues perform their distinct role, alleviating the issue of sentence embeddings overlooking motion cues. Secondly, we propose a hierarchical motion perception module to capture temporal information effectively across varying timescales. Furthermore, we employ contrastive learning to distinguish the motions of visually similar objects. These contributions yield state-of-the-art performance across five datasets, including a remarkable $\\textbf{9.2%}$ $\\mathcal{J\\&F}$ improvement on the challenging $\\textbf{MeViS}$ dataset. Code is available at https://github.com/heshuting555/DsHmp.","sentences":["Referring video segmentation relies on natural language expressions to identify and segment objects, often emphasizing motion clues.","Previous works treat a sentence as a whole and directly perform identification at the video-level, mixing up static image-level cues with temporal motion cues.","However, image-level features cannot well comprehend motion cues in sentences, and static cues are not crucial for temporal perception.","In fact, static cues can sometimes interfere with temporal perception by overshadowing motion cues.","In this work, we propose to decouple video-level referring expression understanding into static and motion perception, with a specific emphasis on enhancing temporal comprehension.","Firstly, we introduce an expression-decoupling module to make static cues and motion cues perform their distinct role, alleviating the issue of sentence embeddings overlooking motion cues.","Secondly, we propose a hierarchical motion perception module to capture temporal information effectively across varying timescales.","Furthermore, we employ contrastive learning to distinguish the motions of visually similar objects.","These contributions yield state-of-the-art performance across five datasets, including a remarkable $\\textbf{9.2%}$ $\\mathcal{J\\&F}$ improvement on the challenging $\\textbf{MeViS}$ dataset.","Code is available at https://github.com/heshuting555/DsHmp."],"url":"http://arxiv.org/abs/2404.03645v1","category":"cs.CV"}
{"created":"2024-04-04 17:55:41","title":"Cooperation between electron-phonon coupling and electronic interaction in bilayer nickelates La$_3$Ni$_2$O$_7$","abstract":"The recent observation of high-T$_c$ superconductivity in the bilayer nickelate La$_3$Ni$_2$O$_7$ under pressure has garnered significant interests. While researches have predominantly focused on the role of electron-electron interactions in the superconducting mechanism, the impact of electron-phonon coupling (EPC) has remained elusive. In this work, we perform first-principles calculations to study the phonon spectrum and electron-phonon coupling within La$_3$Ni$_2$O$_7$ under pressure and explore of the interplay between EPC and electronic interactions on the superconductivity by employing functional renormalization group approach. Our calculations reveal that EPC alone is insufficient to trigger superconductivity in La$_3$Ni$_2$O$_7$ under pressure. We identify unique out-of-plane and in-plane breathing phonon modes which selectively couple with the Ni $d_{z^2}$ and $d_{x^2-y^2}$ orbitals, showcasing an orbital-selective EPC. Within the bilayer two-orbital model, it is revealed that solely electronic interactions foster $s_{\\pm}$-wave pairing characterized by notable frustration in the band space, leading to a low transition temperature. Remarkably, we find that this out-of-plane EPC can act in concert with electronic interactions to promote the onsite and interlayer pairing in the $d_{z^2}$ orbital, partially releasing the pairing frustration and thus elevating T$_c$. In contrast, the inclusion of in-plane EPC only marginally affects the superconductivity, distinct from the cuprates. Potential experimental implications in La$_3$Ni$_2$O$_7$ are also discussed.","sentences":["The recent observation of high-T$_c$ superconductivity in the bilayer nickelate La$_3$Ni$_2$O$_7$ under pressure has garnered significant interests.","While researches have predominantly focused on the role of electron-electron interactions in the superconducting mechanism, the impact of electron-phonon coupling (EPC) has remained elusive.","In this work, we perform first-principles calculations to study the phonon spectrum and electron-phonon coupling within La$_3$Ni$_2$O$_7$ under pressure and explore of the interplay between EPC and electronic interactions on the superconductivity by employing functional renormalization group approach.","Our calculations reveal that EPC alone is insufficient to trigger superconductivity in La$_3$Ni$_2$O$_7$ under pressure.","We identify unique out-of-plane and in-plane breathing phonon modes which selectively couple with the Ni $d_{z^2}$ and $d_{x^2-y^2}$ orbitals, showcasing an orbital-selective EPC.","Within the bilayer two-orbital model, it is revealed that solely electronic interactions foster $s_{\\pm}$-wave pairing characterized by notable frustration in the band space, leading to a low transition temperature.","Remarkably, we find that this out-of-plane EPC can act in concert with electronic interactions to promote the onsite and interlayer pairing in the $d_{z^2}$ orbital, partially releasing the pairing frustration and thus elevating T$_c$.","In contrast, the inclusion of in-plane EPC only marginally affects the superconductivity, distinct from the cuprates.","Potential experimental implications in La$_3$Ni$_2$O$_7$ are also discussed."],"url":"http://arxiv.org/abs/2404.03638v1","category":"cond-mat.supr-con"}
{"created":"2024-04-04 17:32:37","title":"Fundamental inequalities for the iterated Fourier-cosine convolution with Gaussian weight and its application","abstract":"Derived from the results in (Math. Nachr., 283(12):1758-1770, 2010), in this paper, we devoted to studying the boundedness properties for Fourier-cosine convolution weighted by Gaussian functions via Young's type theorem and Saitoh's type inequality. New norm estimations in the weighted space are obtained and application of the corresponding class of convolutions in Fredholm's second kind of integral equation is discussed. The conditions for the solvability of this equation in $L_1$ space are also found, along with the analysis of an illustrative example, which exemplifies that the present object and method solve cases that are not under the conditions of previously known techniques.","sentences":["Derived from the results in (Math. Nachr.",", 283(12):1758-1770, 2010), in this paper, we devoted to studying the boundedness properties for Fourier-cosine convolution weighted by Gaussian functions via Young's type theorem and Saitoh's type inequality.","New norm estimations in the weighted space are obtained and application of the corresponding class of convolutions in Fredholm's second kind of integral equation is discussed.","The conditions for the solvability of this equation in $L_1$ space are also found, along with the analysis of an illustrative example, which exemplifies that the present object and method solve cases that are not under the conditions of previously known techniques."],"url":"http://arxiv.org/abs/2404.03609v1","category":"math.CA"}
{"created":"2024-04-04 17:25:30","title":"Mitigating the Impact of Outlier Channels for Language Model Quantization with Activation Regularization","abstract":"We consider the problem of accurate quantization for language models, where both the weights and activations are uniformly quantized to 4 bits per parameter, the lowest bitwidth format natively supported by GPU hardware. In this context, the key challenge is activation quantization: it is known that language models contain outlier channels whose values on average are orders of magnitude higher than than other channels, which prevents accurate low-bitwidth quantization with known techniques. We systematically study this phenomena and find that these outlier channels emerge early in training, and that they occur more frequently in layers with residual streams. We then propose a simple strategy which regularizes a layer's inputs via quantization-aware training (QAT) and its outputs via activation kurtosis regularization. We show that regularizing both the inputs and outputs is crucial for preventing a model's \"migrating\" the difficulty in input quantization to the weights, which makes post-training quantization (PTQ) of weights more difficult. When combined with weight PTQ, we show that our approach can obtain a W4A4 model that performs competitively to the standard-precision W16A16 baseline.","sentences":["We consider the problem of accurate quantization for language models, where both the weights and activations are uniformly quantized to 4 bits per parameter, the lowest bitwidth format natively supported by GPU hardware.","In this context, the key challenge is activation quantization: it is known that language models contain outlier channels whose values on average are orders of magnitude higher than than other channels, which prevents accurate low-bitwidth quantization with known techniques.","We systematically study this phenomena and find that these outlier channels emerge early in training, and that they occur more frequently in layers with residual streams.","We then propose a simple strategy which regularizes a layer's inputs via quantization-aware training (QAT) and its outputs via activation kurtosis regularization.","We show that regularizing both the inputs and outputs is crucial for preventing a model's \"migrating\" the difficulty in input quantization to the weights, which makes post-training quantization (PTQ) of weights more difficult.","When combined with weight PTQ, we show that our approach can obtain a W4A4 model that performs competitively to the standard-precision W16A16 baseline."],"url":"http://arxiv.org/abs/2404.03605v1","category":"cs.LG"}
{"created":"2024-04-04 16:47:02","title":"Nuclear Matter Equation of State in the Brueckner-Hartree-Fock Approach and Standard Skyrme Energy-Density Functionals","abstract":"The equation of state of asymmetric nuclear matter as well as the neutron and proton effective masses and their partial-wave and spin-isospin decomposition are analyzed within the Brueckner--Hartree--Fock approach. Theoretical uncertainties for all these quantities are estimated by using several phase-shift-equivalent nucleon-nucleon forces together with two types of three-nucleon forces, phenomenological and microscopic. It is shown that the choice of the three-nucleon force plays an important role above saturation density, leading to different density dependencies of the energy per particle. These results are compared to the standard form of the Skyrme energy-density functional and we find that it is not possible to reproduce the BHF predictions in the $(S,T)$ channels in symmetric and neutron matter above saturation density, already at the level of the two-body interaction, and even more including the three-body interaction.","sentences":["The equation of state of asymmetric nuclear matter as well as the neutron and proton effective masses and their partial-wave and spin-isospin decomposition are analyzed within the Brueckner--Hartree--Fock approach.","Theoretical uncertainties for all these quantities are estimated by using several phase-shift-equivalent nucleon-nucleon forces together with two types of three-nucleon forces, phenomenological and microscopic.","It is shown that the choice of the three-nucleon force plays an important role above saturation density, leading to different density dependencies of the energy per particle.","These results are compared to the standard form of the Skyrme energy-density functional and we find that it is not possible to reproduce the BHF predictions in the $(S,T)$ channels in symmetric and neutron matter above saturation density, already at the level of the two-body interaction, and even more including the three-body interaction."],"url":"http://arxiv.org/abs/2404.03583v1","category":"nucl-th"}
{"created":"2024-04-04 16:38:40","title":"Enhanced mobility of quantum droplets in periodic lattices","abstract":"We predict that one- and two-dimensional self-bound quantum droplets, forming in Bose-Einstein condensates in the presence of Lee-Huang-Yang (LHY) quantum corrections to the mean-field energy, may demonstrate exceptional mobility in periodic optical lattices and that they may exhibit considerable displacements across the lattice, remaining dynamically stable, even under weak initial phase kicks imparted to them. Mobility properties of quantum droplets are determined by their internal structure and strongly depend on the number of particles in them. We find that due to the peculiar effect of the LHY quantum corrections, odd (i.e., on-site centered) and even (i.e., intersite-centered) one-dimensional quantum droplets feature alternating mobility and immobility bands closely corresponding to the regions, where translational perturbation mode is unstable and stable, respectively. This picture becomes even richer in two-dimensional case, where odd-odd, even-odd or even-even quantum droplets also feature alternating mobility and immobility domains, and where, surprisingly, the droplet may be mobile in one direction, but immobile in the orthogonal direction. We link changes in mobility properties with multiple intersections of energy $E(\\mu)$ and norm $N(\\mu)$ dependencies for droplets with different internal structure.","sentences":["We predict that one-","and two-dimensional self-bound quantum droplets, forming in Bose-Einstein condensates in the presence of Lee-Huang-Yang (LHY) quantum corrections to the mean-field energy, may demonstrate exceptional mobility in periodic optical lattices and that they may exhibit considerable displacements across the lattice, remaining dynamically stable, even under weak initial phase kicks imparted to them.","Mobility properties of quantum droplets are determined by their internal structure and strongly depend on the number of particles in them.","We find that due to the peculiar effect of the LHY quantum corrections, odd (i.e., on-site centered) and even (i.e., intersite-centered) one-dimensional quantum droplets feature alternating mobility and immobility bands closely corresponding to the regions, where translational perturbation mode is unstable and stable, respectively.","This picture becomes even richer in two-dimensional case, where odd-odd, even-odd or even-even quantum droplets also feature alternating mobility and immobility domains, and where, surprisingly, the droplet may be mobile in one direction, but immobile in the orthogonal direction.","We link changes in mobility properties with multiple intersections of energy $E(\\mu)$ and norm $N(\\mu)$ dependencies for droplets with different internal structure."],"url":"http://arxiv.org/abs/2404.03573v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-04 16:29:08","title":"The Cauchy problem for the nonlinear Schr\u00f6dinger equation with a convolution potential","abstract":"This paper investigates the nonlinear Schr\\\"{o}dinger equation with a singular convolution potential. It demonstrates the local well-posedness of this equation in a modified Sobolev space linked to the energy. Additionally, we derive conditions under which the solutions are uniformly bounded in the energy space. This finding is closely linked to the existence of standing waves for this equation.","sentences":["This paper investigates the nonlinear Schr\\\"{o}dinger equation with a singular convolution potential.","It demonstrates the local well-posedness of this equation in a modified Sobolev space linked to the energy.","Additionally, we derive conditions under which the solutions are uniformly bounded in the energy space.","This finding is closely linked to the existence of standing waves for this equation."],"url":"http://arxiv.org/abs/2404.03568v1","category":"math.AP"}
{"created":"2024-04-04 16:18:37","title":"EASSE-DE: Easier Automatic Sentence Simplification Evaluation for German","abstract":"In this work, we propose EASSE-multi, a framework for easier automatic sentence evaluation for languages other than English. Compared to the original EASSE framework, EASSE-multi does not focus only on English. It contains tokenizers and versions of text simplification evaluation metrics which are suitable for multiple languages. In this paper, we exemplify the usage of EASSE-multi for German TS, resulting in EASSE-DE. Further, we compare text simplification results when evaluating with different language or tokenization settings of the metrics. Based on this, we formulate recommendations on how to make the evaluation of (German) TS models more transparent and better comparable. The code of EASSE-multi and its German specialisation (EASSE-DE) can be found at https://github.com/rstodden/easse-de.","sentences":["In this work, we propose EASSE-multi, a framework for easier automatic sentence evaluation for languages other than English.","Compared to the original EASSE framework, EASSE-multi does not focus only on English.","It contains tokenizers and versions of text simplification evaluation metrics which are suitable for multiple languages.","In this paper, we exemplify the usage of EASSE-multi for German TS, resulting in EASSE-DE.","Further, we compare text simplification results when evaluating with different language or tokenization settings of the metrics.","Based on this, we formulate recommendations on how to make the evaluation of (German) TS models more transparent and better comparable.","The code of EASSE-multi and its German specialisation (EASSE-DE) can be found at https://github.com/rstodden/easse-de."],"url":"http://arxiv.org/abs/2404.03563v1","category":"cs.CL"}
{"created":"2024-04-04 15:24:25","title":"Hub Network Design Problem with Capacity, Congestion and Heterogeneous Economies of Scale","abstract":"We propose a joint model that links the strategic level location and capacity decisions with the operational level routing and hub assignment decisions to solve hub network design problem with congestion and heterogeneous economics of scale. We also develop a novel flow-based mixed-integer second-order cone programming (MISOCP) formulation. We perform numerical experiments on a real-world data set to validate the efficiency of solving the MISOCP reformulation. The numerical studies yield observations can be used as guidelines in the design of transportation network for a logistics company.","sentences":["We propose a joint model that links the strategic level location and capacity decisions with the operational level routing and hub assignment decisions to solve hub network design problem with congestion and heterogeneous economics of scale.","We also develop a novel flow-based mixed-integer second-order cone programming (MISOCP) formulation.","We perform numerical experiments on a real-world data set to validate the efficiency of solving the MISOCP reformulation.","The numerical studies yield observations can be used as guidelines in the design of transportation network for a logistics company."],"url":"http://arxiv.org/abs/2404.03521v1","category":"math.OC"}
{"created":"2024-04-04 15:24:00","title":"Formal deformations of modular forms and multiple L-values","abstract":"We relate analytically defined deformations of modular curves and modular forms from the literature to motivic periods via cohomological descriptions of deformation theory. Leveraging cohomological vanishing results, we prove the existence and essential uniqueness of deformations, which we make constructive via established Lie algebraic arguments and a notion of formal logarithmic deformations. Further, we construct a canonical and a totally holomorphic canonical universal family of deformations of modular forms of all weights, which we obtain from the canonical cocycle associated with periods on the moduli space $\\mathcal{M}_{1,1}$. Our uniqueness statement shows that non-critical multiple $\\mathrm{L}$-values, which appear in our deformations but are a priori non-geometric, are genuinely linked to deformations. Our work thus suggests a new geometric perspective on them.","sentences":["We relate analytically defined deformations of modular curves and modular forms from the literature to motivic periods via cohomological descriptions of deformation theory.","Leveraging cohomological vanishing results, we prove the existence and essential uniqueness of deformations, which we make constructive via established Lie algebraic arguments and a notion of formal logarithmic deformations.","Further, we construct a canonical and a totally holomorphic canonical universal family of deformations of modular forms of all weights, which we obtain from the canonical cocycle associated with periods on the moduli space $\\mathcal{M}_{1,1}$. Our uniqueness statement shows that non-critical multiple $\\mathrm{L}$-values, which appear in our deformations but are a priori non-geometric, are genuinely linked to deformations.","Our work thus suggests a new geometric perspective on them."],"url":"http://arxiv.org/abs/2404.03519v1","category":"math.NT"}
{"created":"2024-04-04 15:23:14","title":"SDPose: Tokenized Pose Estimation via Circulation-Guide Self-Distillation","abstract":"Recently, transformer-based methods have achieved state-of-the-art prediction quality on human pose estimation(HPE). Nonetheless, most of these top-performing transformer-based models are too computation-consuming and storage-demanding to deploy on edge computing platforms. Those transformer-based models that require fewer resources are prone to under-fitting due to their smaller scale and thus perform notably worse than their larger counterparts. Given this conundrum, we introduce SDPose, a new self-distillation method for improving the performance of small transformer-based models. To mitigate the problem of under-fitting, we design a transformer module named Multi-Cycled Transformer(MCT) based on multiple-cycled forwards to more fully exploit the potential of small model parameters. Further, in order to prevent the additional inference compute-consuming brought by MCT, we introduce a self-distillation scheme, extracting the knowledge from the MCT module to a naive forward model. Specifically, on the MSCOCO validation dataset, SDPose-T obtains 69.7% mAP with 4.4M parameters and 1.8 GFLOPs. Furthermore, SDPose-S-V2 obtains 73.5% mAP on the MSCOCO validation dataset with 6.2M parameters and 4.7 GFLOPs, achieving a new state-of-the-art among predominant tiny neural network methods. Our code is available at https://github.com/MartyrPenink/SDPose.","sentences":["Recently, transformer-based methods have achieved state-of-the-art prediction quality on human pose estimation(HPE).","Nonetheless, most of these top-performing transformer-based models are too computation-consuming and storage-demanding to deploy on edge computing platforms.","Those transformer-based models that require fewer resources are prone to under-fitting due to their smaller scale and thus perform notably worse than their larger counterparts.","Given this conundrum, we introduce SDPose, a new self-distillation method for improving the performance of small transformer-based models.","To mitigate the problem of under-fitting, we design a transformer module named Multi-Cycled Transformer(MCT) based on multiple-cycled forwards to more fully exploit the potential of small model parameters.","Further, in order to prevent the additional inference compute-consuming brought by MCT, we introduce a self-distillation scheme, extracting the knowledge from the MCT module to a naive forward model.","Specifically, on the MSCOCO validation dataset, SDPose-T obtains 69.7% mAP with 4.4M parameters and 1.8 GFLOPs.","Furthermore, SDPose-S-V2 obtains 73.5% mAP on the MSCOCO validation dataset with 6.2M parameters and 4.7 GFLOPs, achieving a new state-of-the-art among predominant tiny neural network methods.","Our code is available at https://github.com/MartyrPenink/SDPose."],"url":"http://arxiv.org/abs/2404.03518v1","category":"cs.CV"}
{"created":"2024-04-04 15:08:59","title":"Magnetic signals from oceanic tides: new satellite observations and applications","abstract":"Tidal flow of seawater across the Earth's magnetic field induces electric currents and magnetic fields within the ocean and solid Earth. The amplitude and phase of the induced fields depends on electrical properties of both the seawater and the solid Earth, thus can be used as a proxy to study seabed properties or potentially for monitoring long-term trends in the global ocean climatology. This paper presents new global oceanic tidal magnetic field models and their uncertainties for four tidal constituents, including $M_2, N_2, O_1$ and for the first time $Q_1$. Models are obtained through a robust least-squares analysis of magnetic field observations from the Swarm and CHAMP satellites using a specially designed data selection scheme. We compare the retrieved magnetic signals with several alternative models reported in the literature. Additionally, we validate them using a series of high-resolution global 3-D electromagnetic simulations and place constraints on the conductivity of sub-oceanic mantle for all tidal constituents, revealing an excellent agreement between all tidal constituents and the oceanic upper mantle structure.","sentences":["Tidal flow of seawater across the Earth's magnetic field induces electric currents and magnetic fields within the ocean and solid Earth.","The amplitude and phase of the induced fields depends on electrical properties of both the seawater and the solid Earth, thus can be used as a proxy to study seabed properties or potentially for monitoring long-term trends in the global ocean climatology.","This paper presents new global oceanic tidal magnetic field models and their uncertainties for four tidal constituents, including $M_2, N_2, O_1$ and for the first time $Q_1$. Models are obtained through a robust least-squares analysis of magnetic field observations from the Swarm and CHAMP satellites using a specially designed data selection scheme.","We compare the retrieved magnetic signals with several alternative models reported in the literature.","Additionally, we validate them using a series of high-resolution global 3-D electromagnetic simulations and place constraints on the conductivity of sub-oceanic mantle for all tidal constituents, revealing an excellent agreement between all tidal constituents and the oceanic upper mantle structure."],"url":"http://arxiv.org/abs/2404.03504v1","category":"physics.geo-ph"}
{"created":"2024-04-04 14:50:50","title":"About Test-time training for outlier detection","abstract":"In this paper, we introduce DOUST, our method applying test-time training for outlier detection, significantly improving the detection performance. After thoroughly evaluating our algorithm on common benchmark datasets, we discuss a common problem and show that it disappears with a large enough test set. Thus, we conclude that under reasonable conditions, our algorithm can reach almost supervised performance even when no labeled outliers are given.","sentences":["In this paper, we introduce DOUST, our method applying test-time training for outlier detection, significantly improving the detection performance.","After thoroughly evaluating our algorithm on common benchmark datasets, we discuss a common problem and show that it disappears with a large enough test set.","Thus, we conclude that under reasonable conditions, our algorithm can reach almost supervised performance even when no labeled outliers are given."],"url":"http://arxiv.org/abs/2404.03495v1","category":"cs.LG"}
{"created":"2024-04-04 14:39:07","title":"Combining exchangeable p-values","abstract":"Significant recent progress has been made on deriving combination rules that can take as input a set of arbitrarily dependent p-values, and produce as output a single valid p-value. Here, we show that under the assumption of exchangeability of the p-values, many of those rules can be improved (made more powerful). While this observation by itself has practical implications (for example, under repeated tests involving data splitting), it also has implications for combining arbitrarily dependent p-values, since the latter can be made exchangeable by applying a uniformly random permutation. In particular, we derive several simple randomized combination rules for arbitrarily dependent p-values that are more powerful than their deterministic counterparts. For example, we derive randomized and exchangeable improvements of well known p-value combination rules like \"twice the median\" and \"twice the average\", as well as geometric and harmonic means. The main technical advance is to show that all these combination rules can be obtained by calibrating the p-values to e-values (using an $\\alpha$-dependent calibrator), averaging those e-values, converting to a level $\\alpha$ test using Markov's inequality, and finally obtaining p-values by combining this family of tests. The improvements are delivered via recent randomized and exchangeable variants of Markov's inequality.","sentences":["Significant recent progress has been made on deriving combination rules that can take as input a set of arbitrarily dependent p-values, and produce as output a single valid p-value.","Here, we show that under the assumption of exchangeability of the p-values, many of those rules can be improved (made more powerful).","While this observation by itself has practical implications (for example, under repeated tests involving data splitting), it also has implications for combining arbitrarily dependent p-values, since the latter can be made exchangeable by applying a uniformly random permutation.","In particular, we derive several simple randomized combination rules for arbitrarily dependent p-values that are more powerful than their deterministic counterparts.","For example, we derive randomized and exchangeable improvements of well known p-value combination rules like \"twice the median\" and \"twice the average\", as well as geometric and harmonic means.","The main technical advance is to show that all these combination rules can be obtained by calibrating the p-values to e-values (using an $\\alpha$-dependent calibrator), averaging those e-values, converting to a level $\\alpha$ test using Markov's inequality, and finally obtaining p-values by combining this family of tests.","The improvements are delivered via recent randomized and exchangeable variants of Markov's inequality."],"url":"http://arxiv.org/abs/2404.03484v1","category":"math.ST"}
{"created":"2024-04-04 14:08:11","title":"On the sum of fifth powers in arithmetic progression","abstract":"In this paper we study equation $$(x-dr)^5+\\cdots+x^5+\\cdots+(x+dr)^5=y^p$$ under the condition $\\gcd(x,r)=1$. We present a recipe for proving the non-existence of non-trivial integer solutions of the above equation, and as an application we obtain explicit results for the cases $d=2,3$ (the case $d=1$ was already solved). We also prove an asymptotic result for $d\\equiv 1, 7\\pmod9$. Our main tools include the modular method, employing Frey curves and their associated modular forms, as well as the symplectic argument.","sentences":["In this paper we study equation $$(x-dr)^5+\\cdots+x^5+\\cdots+(x+dr)^5=y^p$$ under the condition $\\gcd(x,r)=1$. We present a recipe for proving the non-existence of non-trivial integer solutions of the above equation, and as an application we obtain explicit results for the cases $d=2,3$ (the case $d=1$ was already solved).","We also prove an asymptotic result for $d\\equiv 1, 7\\pmod9$.","Our main tools include the modular method, employing Frey curves and their associated modular forms, as well as the symplectic argument."],"url":"http://arxiv.org/abs/2404.03457v1","category":"math.NT"}
{"created":"2024-04-04 13:36:27","title":"Fractional-charge hadrons and leptons to tell the Standard Model group apart","abstract":"The gauge group of strong and electroweak interactions in Nature could be any of the four that share the same Lie algebra, $SU(3)_c\\times SU(2)_L\\times U(1)_Y/Z_p\\equiv G_p$ with $Z_p=\\left\\{Z_6,Z_3,Z_2,Z_1\\right\\}$. Each of these cases allows in its spectrum for the matter fields of the SM but also for new distinctive representations, e.g. under the assumption that $q_L$ possesses the minimum possible hypercharge in Nature, $G_p$ allows for particles with a multiple of $p\\,e/6$ for electric charge. This letter discusses how these new possibilities in the spectrum could be used to tell the SM group apart.","sentences":["The gauge group of strong and electroweak interactions in Nature could be any of the four that share the same Lie algebra, $SU(3)_c\\times SU(2)_L\\times U(1)_Y/Z_p\\equiv G_p$ with $Z_p=\\left\\{Z_6,Z_3,Z_2,Z_1\\right\\}$. Each of these cases allows in its spectrum for the matter fields of the SM but also for new distinctive representations, e.g. under the assumption that $q_L$ possesses the minimum possible hypercharge in Nature, $G_p$ allows for particles with a multiple of $p\\,e/6$ for electric charge.","This letter discusses how these new possibilities in the spectrum could be used to tell the SM group apart."],"url":"http://arxiv.org/abs/2404.03438v1","category":"hep-ph"}
{"created":"2024-04-04 13:09:26","title":"Accurate estimation of feature importance faithfulness for tree models","abstract":"In this paper, we consider a perturbation-based metric of predictive faithfulness of feature rankings (or attributions) that we call PGI squared. When applied to decision tree-based regression models, the metric can be computed accurately and efficiently for arbitrary independent feature perturbation distributions. In particular, the computation does not involve Monte Carlo sampling that has been typically used for computing similar metrics and which is inherently prone to inaccuracies. Moreover, we propose a method of ranking features by their importance for the tree model's predictions based on PGI squared. Our experiments indicate that in some respects, the method may identify the globally important features better than the state-of-the-art SHAP explainer","sentences":["In this paper, we consider a perturbation-based metric of predictive faithfulness of feature rankings (or attributions) that we call PGI squared.","When applied to decision tree-based regression models, the metric can be computed accurately and efficiently for arbitrary independent feature perturbation distributions.","In particular, the computation does not involve Monte Carlo sampling that has been typically used for computing similar metrics and which is inherently prone to inaccuracies.","Moreover, we propose a method of ranking features by their importance for the tree model's predictions based on PGI squared.","Our experiments indicate that in some respects, the method may identify the globally important features better than the state-of-the-art SHAP explainer"],"url":"http://arxiv.org/abs/2404.03426v1","category":"cs.LG"}
{"created":"2024-04-04 13:04:39","title":"Empirical Bayes for the Reluctant Frequentist","abstract":"Empirical Bayes methods offer valuable tools for a large class of compound decision problems. In this tutorial we describe some basic principles of the empirical Bayes paradigm stressing their frequentist interpretation. Emphasis is placed on recent developments of nonparametric maximum likelihood methods for estimating mixture models. A more extensive introductory treatment will eventually be available in \\citet{kg24}. The methods are illustrated with an extended application to models of heterogeneous income dynamics based on PSID data.","sentences":["Empirical Bayes methods offer valuable tools for a large class of compound decision problems.","In this tutorial we describe some basic principles of the empirical Bayes paradigm stressing their frequentist interpretation.","Emphasis is placed on recent developments of nonparametric maximum likelihood methods for estimating mixture models.","A more extensive introductory treatment will eventually be available in \\citet{kg24}.","The methods are illustrated with an extended application to models of heterogeneous income dynamics based on PSID data."],"url":"http://arxiv.org/abs/2404.03422v1","category":"stat.ME"}
{"created":"2024-04-04 12:02:35","title":"Riemannian Covariance Fitting for Direction-of-Arrival Estimation","abstract":"Covariance fitting (CF) is a comprehensive approach for direction of arrival (DoA) estimation, consolidating many common solutions. Standard practice is to use Euclidean criteria for CF, disregarding the intrinsic Hermitian positive-definite (HPD) geometry of the spatial covariance matrices. We assert that this oversight leads to inherent limitations. In this paper, as a remedy, we present a comprehensive study of the use of various Riemannian metrics of HPD matrices in CF. We focus on the advantages of the Affine-Invariant (AI) and the Log-Euclidean (LE) Riemannian metrics. Consequently, we propose a new practical beamformer based on the LE metric and derive analytically its spatial characteristics, such as the beamwidth and sidelobe attenuation, under noisy conditions. Comparing these features to classical beamformers shows significant advantage. In addition, we demonstrate, both theoretically and experimentally, the LE beamformer's robustness in scenarios with small sample sizes and in the presence of noise, interference, and multipath channels.","sentences":["Covariance fitting (CF) is a comprehensive approach for direction of arrival (DoA) estimation, consolidating many common solutions.","Standard practice is to use Euclidean criteria for CF, disregarding the intrinsic Hermitian positive-definite (HPD) geometry of the spatial covariance matrices.","We assert that this oversight leads to inherent limitations.","In this paper, as a remedy, we present a comprehensive study of the use of various Riemannian metrics of HPD matrices in CF.","We focus on the advantages of the Affine-Invariant (AI) and the Log-Euclidean (LE) Riemannian metrics.","Consequently, we propose a new practical beamformer based on the LE metric and derive analytically its spatial characteristics, such as the beamwidth and sidelobe attenuation, under noisy conditions.","Comparing these features to classical beamformers shows significant advantage.","In addition, we demonstrate, both theoretically and experimentally, the LE beamformer's robustness in scenarios with small sample sizes and in the presence of noise, interference, and multipath channels."],"url":"http://arxiv.org/abs/2404.03401v1","category":"eess.SP"}
{"created":"2024-04-04 11:44:17","title":"Material design optimization for large-m 11B4C-based Ni/Ti supermirror neutron optics","abstract":"State-of-the-art Ni/Ti supermirror neutron optics have limited reflected intensity and a restricted neutron energy range due to the interface width. Incorporating low-neutron-absorbing 11B4C enhances reflectivity and allows for thinner layers to be deposited, with which more efficient supermirrors with higher m-values can be realized. However, incorporating 11B4C reduces the optical contrast, limiting the attainable reflectivity at low scattering vectors, making this approach infeasible. This study explores various approaches to optimize the material design of 11B4C-containing Ni/Ti supermirrors to maintain high reflectivity at low scattering vectors and achieve low interface widths at large scattering vectors. The scattering length density contrast versus interface width is investigated for multilayer periods of 30 {\\AA}, 48 {\\AA}, and 84 {\\AA}, for designs involving pure Ni/Ti multilayers, multilayers with 11B4C co-deposited in Ni and Ti layers, multilayers with 11B4C co-deposited only in Ni layers, and multilayers with 11B4C as thin interlayers between Ni and Ti layers. Our results suggest that a depth-graded hybrid material design by incorporating 11B4C inside the Ni and Ti layers, below approximately 26 {\\AA}, and introducing 1.5 {\\AA} 11B4C interlayers between the thicker Ni and Ti layers can achieve a higher reflectivity than state-of-the-art Ni/Ti multilayers over the entire scattering vector range.","sentences":["State-of-the-art Ni/Ti supermirror neutron optics have limited reflected intensity and a restricted neutron energy range due to the interface width.","Incorporating low-neutron-absorbing 11B4C enhances reflectivity and allows for thinner layers to be deposited, with which more efficient supermirrors with higher m-values can be realized.","However, incorporating 11B4C reduces the optical contrast, limiting the attainable reflectivity at low scattering vectors, making this approach infeasible.","This study explores various approaches to optimize the material design of 11B4C-containing Ni/Ti supermirrors to maintain high reflectivity at low scattering vectors and achieve low interface widths at large scattering vectors.","The scattering length density contrast versus interface width is investigated for multilayer periods of 30 {\\AA}, 48 {\\AA}, and 84 {\\AA}, for designs involving pure Ni/Ti multilayers, multilayers with 11B4C co-deposited in Ni and Ti layers, multilayers with 11B4C co-deposited only in Ni layers, and multilayers with 11B4C as thin interlayers between Ni and Ti layers.","Our results suggest that a depth-graded hybrid material design by incorporating 11B4C inside the Ni and Ti layers, below approximately 26 {\\AA}, and introducing 1.5 {\\AA} 11B4C interlayers between the thicker Ni and Ti layers can achieve a higher reflectivity than state-of-the-art Ni/Ti multilayers over the entire scattering vector range."],"url":"http://arxiv.org/abs/2404.03390v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 11:24:33","title":"3D scaling laws and projection effects in The300-NIKA2 Sunyaev-Zeldovich Large Program Twin Samples","abstract":"The abundance of galaxy clusters with mass and redshift is a well-known cosmological probe. The cluster mass is a key parameter for studies that aim to constrain cosmological parameters using galaxy clusters, making it critical to understand and properly account for the errors in its estimates. Subsequently, it becomes important to correctly calibrate scaling relations between observables like the integrated Compton parameter and the mass of the cluster.   The NIKA2 Sunyaev-Zeldovich Large program (LPSZ) enables one to map the intracluster medium profiles in the mm-wavelength band with great details (resolution of $11 \\ \\mathrm{\\&}\\ 17^{\\prime \\prime}$ at $1.2 \\ \\mathrm{\\&}\\ 2 $ mm, respectively) and hence, to estimate the cluster hydrostatic mass more precisely than previous SZ observations. However, there are certain systematic effects which can only be accounted for with the use of simulations. For this purpose, we employ THE THREE HUNDRED simulations which have been modelled with a range of physics modules to simulate galaxy clusters. The so-called twin samples are constructed by picking synthetic clusters of galaxies with properties close to the observational targets of the LPSZ. In particular, we use the Compton parameter maps and projected total mass maps of these twin samples along 29 different lines of sight. We investigate the scatter that projection induces on the total masses. Eventually, we consider the statistical values along different lines of sight to construct a kind of 3D scaling law between the integrated Compton parameter, total mass, and overdensity of the galaxy clusters to determine the overdensity that is least impacted by the projection effect.","sentences":["The abundance of galaxy clusters with mass and redshift is a well-known cosmological probe.","The cluster mass is a key parameter for studies that aim to constrain cosmological parameters using galaxy clusters, making it critical to understand and properly account for the errors in its estimates.","Subsequently, it becomes important to correctly calibrate scaling relations between observables like the integrated Compton parameter and the mass of the cluster.   ","The NIKA2 Sunyaev-Zeldovich Large program (LPSZ) enables one to map the intracluster medium profiles in the mm-wavelength band with great details (resolution of $11 \\ \\mathrm{\\&}\\ 17^{\\prime \\prime}$ at $1.2 \\ \\mathrm{\\&}\\ 2 $ mm, respectively) and hence, to estimate the cluster hydrostatic mass more precisely than previous SZ observations.","However, there are certain systematic effects which can only be accounted for with the use of simulations.","For this purpose, we employ THE THREE HUNDRED simulations which have been modelled with a range of physics modules to simulate galaxy clusters.","The so-called twin samples are constructed by picking synthetic clusters of galaxies with properties close to the observational targets of the LPSZ.","In particular, we use the Compton parameter maps and projected total mass maps of these twin samples along 29 different lines of sight.","We investigate the scatter that projection induces on the total masses.","Eventually, we consider the statistical values along different lines of sight to construct a kind of 3D scaling law between the integrated Compton parameter, total mass, and overdensity of the galaxy clusters to determine the overdensity that is least impacted by the projection effect."],"url":"http://arxiv.org/abs/2404.03376v1","category":"astro-ph.CO"}
{"created":"2024-04-04 11:16:16","title":"Elementary Analysis of Policy Gradient Methods","abstract":"Projected policy gradient under the simplex parameterization, policy gradient and natural policy gradient under the softmax parameterization, are fundamental algorithms in reinforcement learning. There have been a flurry of recent activities in studying these algorithms from the theoretical aspect. Despite this, their convergence behavior is still not fully understood, even given the access to exact policy evaluations. In this paper, we focus on the discounted MDP setting and conduct a systematic study of the aforementioned policy optimization methods. Several novel results are presented, including 1) global linear convergence of projected policy gradient for any constant step size, 2) sublinear convergence of softmax policy gradient for any constant step size, 3) global linear convergence of softmax natural policy gradient for any constant step size, 4) global linear convergence of entropy regularized softmax policy gradient for a wider range of constant step sizes than existing result, 5) tight local linear convergence rate of entropy regularized natural policy gradient, and 6) a new and concise local quadratic convergence rate of soft policy iteration without the assumption on the stationary distribution under the optimal policy. New and elementary analysis techniques have been developed to establish these results.","sentences":["Projected policy gradient under the simplex parameterization, policy gradient and natural policy gradient under the softmax parameterization, are fundamental algorithms in reinforcement learning.","There have been a flurry of recent activities in studying these algorithms from the theoretical aspect.","Despite this, their convergence behavior is still not fully understood, even given the access to exact policy evaluations.","In this paper, we focus on the discounted MDP setting and conduct a systematic study of the aforementioned policy optimization methods.","Several novel results are presented, including 1) global linear convergence of projected policy gradient for any constant step size, 2) sublinear convergence of softmax policy gradient for any constant step size, 3) global linear convergence of softmax natural policy gradient for any constant step size, 4) global linear convergence of entropy regularized softmax policy gradient for a wider range of constant step sizes than existing result, 5) tight local linear convergence rate of entropy regularized natural policy gradient, and 6) a new and concise local quadratic convergence rate of soft policy iteration without the assumption on the stationary distribution under the optimal policy.","New and elementary analysis techniques have been developed to establish these results."],"url":"http://arxiv.org/abs/2404.03372v1","category":"math.OC"}
{"created":"2024-04-04 10:50:41","title":"Agora Elevator Bodily Sensation Study -- a report","abstract":"This study set out to examine the relationship between expressed social emotions (i.e. that what people say they are feeling) and physical sensations, the connection between emotion and bodily experience. It additionally provided the opportunity to investigate how the neurological findings of gender differences can be observed in practice, what difference does it make in behaviour and judgment that we have varying levels of mirror neuron activity? The following report documents the study, procedure, results and findings.","sentences":["This study set out to examine the relationship between expressed social emotions (i.e. that what people say they are feeling) and physical sensations, the connection between emotion and bodily experience.","It additionally provided the opportunity to investigate how the neurological findings of gender differences can be observed in practice, what difference does it make in behaviour and judgment that we have varying levels of mirror neuron activity?","The following report documents the study, procedure, results and findings."],"url":"http://arxiv.org/abs/2404.03356v1","category":"cs.HC"}
{"created":"2024-04-04 10:16:01","title":"The Classification of all weak solutions to $-\u0394u={u^{-\u03b3}}$ in the half-space","abstract":"We provide the classification of all the positive solutions to $-\\Delta u=\\frac{1}{u^\\gamma}$ in the half space, under minimal assumption.","sentences":["We provide the classification of all the positive solutions to $-\\Delta u=\\frac{1}{u^\\gamma}$ in the half space, under minimal assumption."],"url":"http://arxiv.org/abs/2404.03343v1","category":"math.AP"}
{"created":"2024-04-04 10:10:38","title":"Meta Invariance Defense Towards Generalizable Robustness to Unknown Adversarial Attacks","abstract":"Despite providing high-performance solutions for computer vision tasks, the deep neural network (DNN) model has been proved to be extremely vulnerable to adversarial attacks. Current defense mainly focuses on the known attacks, but the adversarial robustness to the unknown attacks is seriously overlooked. Besides, commonly used adaptive learning and fine-tuning technique is unsuitable for adversarial defense since it is essentially a zero-shot problem when deployed. Thus, to tackle this challenge, we propose an attack-agnostic defense method named Meta Invariance Defense (MID). Specifically, various combinations of adversarial attacks are randomly sampled from a manually constructed Attacker Pool to constitute different defense tasks against unknown attacks, in which a student encoder is supervised by multi-consistency distillation to learn the attack-invariant features via a meta principle. The proposed MID has two merits: 1) Full distillation from pixel-, feature- and prediction-level between benign and adversarial samples facilitates the discovery of attack-invariance. 2) The model simultaneously achieves robustness to the imperceptible adversarial perturbations in high-level image classification and attack-suppression in low-level robust image regeneration. Theoretical and empirical studies on numerous benchmarks such as ImageNet verify the generalizable robustness and superiority of MID under various attacks.","sentences":["Despite providing high-performance solutions for computer vision tasks, the deep neural network (DNN) model has been proved to be extremely vulnerable to adversarial attacks.","Current defense mainly focuses on the known attacks, but the adversarial robustness to the unknown attacks is seriously overlooked.","Besides, commonly used adaptive learning and fine-tuning technique is unsuitable for adversarial defense since it is essentially a zero-shot problem when deployed.","Thus, to tackle this challenge, we propose an attack-agnostic defense method named Meta Invariance Defense (MID).","Specifically, various combinations of adversarial attacks are randomly sampled from a manually constructed Attacker Pool to constitute different defense tasks against unknown attacks, in which a student encoder is supervised by multi-consistency distillation to learn the attack-invariant features via a meta principle.","The proposed MID has two merits: 1) Full distillation from pixel-, feature- and prediction-level between benign and adversarial samples facilitates the discovery of attack-invariance.","2)","The model simultaneously achieves robustness to the imperceptible adversarial perturbations in high-level image classification and attack-suppression in low-level robust image regeneration.","Theoretical and empirical studies on numerous benchmarks such as ImageNet verify the generalizable robustness and superiority of MID under various attacks."],"url":"http://arxiv.org/abs/2404.03340v1","category":"cs.CV"}
{"created":"2024-04-04 09:57:08","title":"3D Growth and Remodeling Theory Supports the Hypothesis of Staphyloma Formation from Local Scleral Weakening under Normal Intraocular Pressure","abstract":"$\\bf{Purpose}$: To assess whether Growth & Remodeling (G&R) theory could explain staphyloma formation from a local scleral weakening.   $\\bf{Methods}$: A finite element model of a healthy eye was reconstructed, including the following connective tissues: the lamina cribrosa, the peripapillary sclera, and the peripheral sclera. The scleral shell was modelled as a constrained mixture, consisting of an isotropic ground matrix and two collagen fiber families (circumferential and meridional). The homogenized constrained mixture model was employed to simulate the adaptation of the sclera to alterations in its biomechanical environment over a duration of 13.7 years. G&R processes were triggered by reducing the shear stiffness of the ground matrix in the peripapillary sclera and lamina cribrosa by 85%. Three distinct G&R scenarios were investigated: (1) low mass turnover rate in combination with transmural volumetric growth; (2) high mass turnover rate in combination with transmural volumetric growth; and (3) high mass turnover rate in combination with mass density growth.   $\\bf{Results}$: In scenario 1, we observed a significant outpouching of the posterior pole, closely resembling the shape of a Type-III staphyloma. Additionally, we found a notable change in scleral curvature and a thinning of the peripapillary sclera by 84%. In contrast, scenarios 2 and 3 exhibited less drastic deformations, with stable posterior staphylomas after approximately 7 years.   $\\bf{Conclusions}$: Our framework suggests that local scleral weakening is sufficient to trigger staphyloma formation under normal intraocular pressure. With patient-specific scleral geometries (obtainable via wide-field optical coherence tomography), our framework could aid in identifying individuals at risk of developing posterior staphylomas.","sentences":["$\\bf{Purpose}$: To assess whether Growth & Remodeling (G&R) theory could explain staphyloma formation from a local scleral weakening.   ","$\\bf{Methods}$: A finite element model of a healthy eye was reconstructed, including the following connective tissues: the lamina cribrosa, the peripapillary sclera, and the peripheral sclera.","The scleral shell was modelled as a constrained mixture, consisting of an isotropic ground matrix and two collagen fiber families (circumferential and meridional).","The homogenized constrained mixture model was employed to simulate the adaptation of the sclera to alterations in its biomechanical environment over a duration of 13.7 years.","G&R processes were triggered by reducing the shear stiffness of the ground matrix in the peripapillary sclera and lamina cribrosa by 85%.","Three distinct G&R scenarios were investigated: (1) low mass turnover rate in combination with transmural volumetric growth; (2) high mass turnover rate in combination with transmural volumetric growth; and (3) high mass turnover rate in combination with mass density growth.   ","$\\bf{Results}$: In scenario 1, we observed a significant outpouching of the posterior pole, closely resembling the shape of a Type-III staphyloma.","Additionally, we found a notable change in scleral curvature and a thinning of the peripapillary sclera by 84%.","In contrast, scenarios 2 and 3 exhibited less drastic deformations, with stable posterior staphylomas after approximately 7 years.   ","$\\bf{Conclusions}$: Our framework suggests that local scleral weakening is sufficient to trigger staphyloma formation under normal intraocular pressure.","With patient-specific scleral geometries (obtainable via wide-field optical coherence tomography), our framework could aid in identifying individuals at risk of developing posterior staphylomas."],"url":"http://arxiv.org/abs/2404.03330v1","category":"cs.CE"}
{"created":"2024-04-04 08:50:32","title":"MusE GAs FLOw and Wind (MEGAFLOW) XI. Scaling relations between outflows and host galaxy properties","abstract":"Absorption line spectroscopy using background quasars can provide strong constraints on galactic outflows. In this paper, we investigate possible scaling relations between outflow properties, namely outflow velocity \\Vout, the mass ejection rate $\\dot M_{\\rm out}$, and the mass loading factor $\\eta$ and the host galaxy properties, such as star formation rate (SFR), SFR surface density, redshift, and stellar mass using galactic outflows probed by background quasars from MEGAFLOW and other surveys. We find that $V_{\\rm out}$ ($\\eta$) is (anti-)correlated with SFR and SFR surface density. We extend the formalism of momentum-driven outflows of Heckman et al. to show that it applies not only to down the barrel studies but also to winds probed by background quasars, suggesting a possible universal wind formalism. Under this formalism, we find a clear distinction between ``strong'' and ``weak'' outflows where ``strong'' outflows seem to have tighter correlations with galaxy properties (SFR or galaxy stellar mass) than ``weak'' outflows.","sentences":["Absorption line spectroscopy using background quasars can provide strong constraints on galactic outflows.","In this paper, we investigate possible scaling relations between outflow properties, namely outflow velocity \\Vout, the mass ejection rate $\\dot M_{\\rm out}$, and the mass loading factor $\\eta$ and the host galaxy properties, such as star formation rate (SFR), SFR surface density, redshift, and stellar mass using galactic outflows probed by background quasars from MEGAFLOW and other surveys.","We find that $V_{\\rm out}$ ($\\eta$) is (anti-)correlated with SFR and SFR surface density.","We extend the formalism of momentum-driven outflows of Heckman et al. to show that it applies not only to down the barrel studies but also to winds probed by background quasars, suggesting a possible universal wind formalism.","Under this formalism, we find a clear distinction between ``strong'' and ``weak'' outflows where ``strong'' outflows seem to have tighter correlations with galaxy properties (SFR or galaxy stellar mass) than ``weak'' outflows."],"url":"http://arxiv.org/abs/2404.03300v1","category":"astro-ph.GA"}
{"created":"2024-04-04 08:09:33","title":"Improving Patient Transport in Hospitals: A Literature Review of Operations Research Methods","abstract":"Most activities in hospitals require the presence of the patient. Delays in patient transport can therefore cause disruptions and costly downtime in many different areas and departments, which makes patient transport planning a central operational problem in hospitals. This paper provides the first literature review of Operations Research approaches for improving non-emergency patient transport in hospitals. We structure the different patient transport problems considered in the literature according to several main characteristics and introduce a four-field notation for patient transport problems that allows for a concise representation of different problem variants. We then analyze the relevant literature with respect to different aspects related to the considered problem variant, the employed modeling and solution techniques, as well as the data used and the level of practical implementation achieved. Based on our literature analysis and semi-structured interviews with hospital practitioners, we provide a comparison of current hospital practice and the existing literature on patient transport, and we identify research gaps and formulate an agenda for relevant future research in this area.","sentences":["Most activities in hospitals require the presence of the patient.","Delays in patient transport can therefore cause disruptions and costly downtime in many different areas and departments, which makes patient transport planning a central operational problem in hospitals.","This paper provides the first literature review of Operations Research approaches for improving non-emergency patient transport in hospitals.","We structure the different patient transport problems considered in the literature according to several main characteristics and introduce a four-field notation for patient transport problems that allows for a concise representation of different problem variants.","We then analyze the relevant literature with respect to different aspects related to the considered problem variant, the employed modeling and solution techniques, as well as the data used and the level of practical implementation achieved.","Based on our literature analysis and semi-structured interviews with hospital practitioners, we provide a comparison of current hospital practice and the existing literature on patient transport, and we identify research gaps and formulate an agenda for relevant future research in this area."],"url":"http://arxiv.org/abs/2404.03282v1","category":"math.OC"}
{"created":"2024-04-04 08:04:24","title":"Evaluating Document Simplification: On the Importance of Separately Assessing Simplicity and Meaning Preservation","abstract":"Text simplification intends to make a text easier to read while preserving its core meaning. Intuitively and as shown in previous works, these two dimensions (simplification and meaning preservation) are often-times inversely correlated. An overly conservative text will fail to simplify sufficiently, whereas extreme simplification will degrade meaning preservation. Yet, popular evaluation metrics either aggregate meaning preservation and simplification into a single score (SARI, LENS), or target meaning preservation alone (BERTScore, QuestEval). Moreover, these metrics usually require a set of references and most previous work has only focused on sentence-level simplification. In this paper, we focus on the evaluation of document-level text simplification and compare existing models using distinct metrics for meaning preservation and simplification. We leverage existing metrics from similar tasks and introduce a reference-less metric variant for simplicity, showing that models are mostly biased towards either simplification or meaning preservation, seldom performing well on both dimensions. Making use of the fact that the metrics we use are all reference-less, we also investigate the performance of existing models when applied to unseen data (where reference simplifications are unavailable).","sentences":["Text simplification intends to make a text easier to read while preserving its core meaning.","Intuitively and as shown in previous works, these two dimensions (simplification and meaning preservation) are often-times inversely correlated.","An overly conservative text will fail to simplify sufficiently, whereas extreme simplification will degrade meaning preservation.","Yet, popular evaluation metrics either aggregate meaning preservation and simplification into a single score (SARI, LENS), or target meaning preservation alone (BERTScore, QuestEval).","Moreover, these metrics usually require a set of references and most previous work has only focused on sentence-level simplification.","In this paper, we focus on the evaluation of document-level text simplification and compare existing models using distinct metrics for meaning preservation and simplification.","We leverage existing metrics from similar tasks and introduce a reference-less metric variant for simplicity, showing that models are mostly biased towards either simplification or meaning preservation, seldom performing well on both dimensions.","Making use of the fact that the metrics we use are all reference-less, we also investigate the performance of existing models when applied to unseen data (where reference simplifications are unavailable)."],"url":"http://arxiv.org/abs/2404.03278v1","category":"cs.CL"}
{"created":"2024-04-04 06:49:48","title":"Marginal Treatment Effects and Monotonicity","abstract":"How robust are analyses based on marginal treatment effects (MTE) to violations of Imbens and Angrist (1994) monotonicity? In this note, I present weaker forms of monotonicity under which popular MTE-based estimands still identify the parameters of interest.","sentences":["How robust are analyses based on marginal treatment effects (MTE) to violations of Imbens and Angrist (1994) monotonicity?","In this note, I present weaker forms of monotonicity under which popular MTE-based estimands still identify the parameters of interest."],"url":"http://arxiv.org/abs/2404.03235v1","category":"econ.EM"}
{"created":"2024-04-04 06:37:46","title":"Learn What You Want to Unlearn: Unlearning Inversion Attacks against Machine Unlearning","abstract":"Machine unlearning has become a promising solution for fulfilling the \"right to be forgotten\", under which individuals can request the deletion of their data from machine learning models. However, existing studies of machine unlearning mainly focus on the efficacy and efficiency of unlearning methods, while neglecting the investigation of the privacy vulnerability during the unlearning process. With two versions of a model available to an adversary, that is, the original model and the unlearned model, machine unlearning opens up a new attack surface. In this paper, we conduct the first investigation to understand the extent to which machine unlearning can leak the confidential content of the unlearned data. Specifically, under the Machine Learning as a Service setting, we propose unlearning inversion attacks that can reveal the feature and label information of an unlearned sample by only accessing the original and unlearned model. The effectiveness of the proposed unlearning inversion attacks is evaluated through extensive experiments on benchmark datasets across various model architectures and on both exact and approximate representative unlearning approaches. The experimental results indicate that the proposed attack can reveal the sensitive information of the unlearned data. As such, we identify three possible defenses that help to mitigate the proposed attacks, while at the cost of reducing the utility of the unlearned model. The study in this paper uncovers an underexplored gap between machine unlearning and the privacy of unlearned data, highlighting the need for the careful design of mechanisms for implementing unlearning without leaking the information of the unlearned data.","sentences":["Machine unlearning has become a promising solution for fulfilling the \"right to be forgotten\", under which individuals can request the deletion of their data from machine learning models.","However, existing studies of machine unlearning mainly focus on the efficacy and efficiency of unlearning methods, while neglecting the investigation of the privacy vulnerability during the unlearning process.","With two versions of a model available to an adversary, that is, the original model and the unlearned model, machine unlearning opens up a new attack surface.","In this paper, we conduct the first investigation to understand the extent to which machine unlearning can leak the confidential content of the unlearned data.","Specifically, under the Machine Learning as a Service setting, we propose unlearning inversion attacks that can reveal the feature and label information of an unlearned sample by only accessing the original and unlearned model.","The effectiveness of the proposed unlearning inversion attacks is evaluated through extensive experiments on benchmark datasets across various model architectures and on both exact and approximate representative unlearning approaches.","The experimental results indicate that the proposed attack can reveal the sensitive information of the unlearned data.","As such, we identify three possible defenses that help to mitigate the proposed attacks, while at the cost of reducing the utility of the unlearned model.","The study in this paper uncovers an underexplored gap between machine unlearning and the privacy of unlearned data, highlighting the need for the careful design of mechanisms for implementing unlearning without leaking the information of the unlearned data."],"url":"http://arxiv.org/abs/2404.03233v1","category":"cs.CR"}
{"created":"2024-04-04 06:31:56","title":"Quantum Phases of a Dipolar Fermi Gas with Laser-assisted Interwire Tunneling","abstract":"We systematically investigate unconventional superfluid phases of fermionic dipolar particles lying in a double-wire setup with laser-assisted interwire tunneling. Our numerical simulations, based on the nonlocal Kohn-Sham Bogoliubov-de Gennes equation, reveal the existence of a large Fulde-Ferrell-Larkin-Ovchinnikov (FFLO) region with a stripe phase under an imbalance of particle densities between two wires. When the laser-assisted interwire tunneling is present, it induces a transition from the FFLO phase to the topological superfluid phase and the associated Majorana zero modes exhibit an oscillation structure, which is significantly enhanced by the long-range nature of the interwire dipolar interaction. This distinguishes itself from the results obtained with usual contact interaction and offers new opportunities for manipulating and reshaping Majorana zero modes by adjusting the degree of the nonlocality and the interwire separation.","sentences":["We systematically investigate unconventional superfluid phases of fermionic dipolar particles lying in a double-wire setup with laser-assisted interwire tunneling.","Our numerical simulations, based on the nonlocal Kohn-Sham Bogoliubov-de Gennes equation, reveal the existence of a large Fulde-Ferrell-Larkin-Ovchinnikov (FFLO) region with a stripe phase under an imbalance of particle densities between two wires.","When the laser-assisted interwire tunneling is present, it induces a transition from the FFLO phase to the topological superfluid phase and the associated Majorana zero modes exhibit an oscillation structure, which is significantly enhanced by the long-range nature of the interwire dipolar interaction.","This distinguishes itself from the results obtained with usual contact interaction and offers new opportunities for manipulating and reshaping Majorana zero modes by adjusting the degree of the nonlocality and the interwire separation."],"url":"http://arxiv.org/abs/2404.03230v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-04 05:45:52","title":"Accurate Low-Degree Polynomial Approximation of Non-polynomial Operators for Fast Private Inference in Homomorphic Encryption","abstract":"As machine learning (ML) permeates fields like healthcare, facial recognition, and blockchain, the need to protect sensitive data intensifies. Fully Homomorphic Encryption (FHE) allows inference on encrypted data, preserving the privacy of both data and the ML model. However, it slows down non-secure inference by up to five magnitudes, with a root cause of replacing non-polynomial operators (ReLU and MaxPooling) with high-degree Polynomial Approximated Function (PAF). We propose SmartPAF, a framework to replace non-polynomial operators with low-degree PAF and then recover the accuracy of PAF-approximated model through four techniques: (1) Coefficient Tuning (CT) -- adjust PAF coefficients based on the input distributions before training, (2) Progressive Approximation (PA) -- progressively replace one non-polynomial operator at a time followed by a fine-tuning, (3) Alternate Training (AT) -- alternate the training between PAFs and other linear operators in the decoupled manner, and (4) Dynamic Scale (DS) / Static Scale (SS) -- dynamically scale PAF input value within (-1, 1) in training, and fix the scale as the running max value in FHE deployment. The synergistic effect of CT, PA, AT, and DS/SS enables SmartPAF to enhance the accuracy of the various models approximated by PAFs with various low degrees under multiple datasets. For ResNet-18 under ImageNet-1k, the Pareto-frontier spotted by SmartPAF in latency-accuracy tradeoff space achieves 1.42x ~ 13.64x accuracy improvement and 6.79x ~ 14.9x speedup than prior works. Further, SmartPAF enables a 14-degree PAF (f1^2 g_1^2) to achieve 7.81x speedup compared to the 27-degree PAF obtained by minimax approximation with the same 69.4% post-replacement accuracy. Our code is available at https://github.com/TorchFHE/SmartPAF.","sentences":["As machine learning (ML) permeates fields like healthcare, facial recognition, and blockchain, the need to protect sensitive data intensifies.","Fully Homomorphic Encryption (FHE) allows inference on encrypted data, preserving the privacy of both data and the ML model.","However, it slows down non-secure inference by up to five magnitudes, with a root cause of replacing non-polynomial operators (ReLU and MaxPooling) with high-degree Polynomial Approximated Function (PAF).","We propose SmartPAF, a framework to replace non-polynomial operators with low-degree PAF and then recover the accuracy of PAF-approximated model through four techniques: (1) Coefficient Tuning (CT) -- adjust PAF coefficients based on the input distributions before training, (2) Progressive Approximation (PA) -- progressively replace one non-polynomial operator at a time followed by a fine-tuning, (3) Alternate Training (AT) -- alternate the training between PAFs and other linear operators in the decoupled manner, and (4) Dynamic Scale (DS) / Static Scale (SS) -- dynamically scale PAF input value within (-1, 1) in training, and fix the scale as the running max value in FHE deployment.","The synergistic effect of CT, PA, AT, and DS/SS enables SmartPAF to enhance the accuracy of the various models approximated by PAFs with various low degrees under multiple datasets.","For ResNet-18 under ImageNet-1k, the Pareto-frontier spotted by SmartPAF in latency-accuracy tradeoff space achieves 1.42x ~","13.64x accuracy improvement and 6.79x ~ 14.9x speedup than prior works.","Further, SmartPAF enables a 14-degree PAF (f1^2 g_1^2) to achieve 7.81x speedup compared to the 27-degree PAF obtained by minimax approximation with the same 69.4% post-replacement accuracy.","Our code is available at https://github.com/TorchFHE/SmartPAF."],"url":"http://arxiv.org/abs/2404.03216v1","category":"cs.CR"}
{"created":"2024-04-04 05:39:09","title":"LeGrad: An Explainability Method for Vision Transformers via Feature Formation Sensitivity","abstract":"Vision Transformers (ViTs), with their ability to model long-range dependencies through self-attention mechanisms, have become a standard architecture in computer vision. However, the interpretability of these models remains a challenge. To address this, we propose LeGrad, an explainability method specifically designed for ViTs. LeGrad computes the gradient with respect to the attention maps of ViT layers, considering the gradient itself as the explainability signal. We aggregate the signal over all layers, combining the activations of the last as well as intermediate tokens to produce the merged explainability map. This makes LeGrad a conceptually simple and an easy-to-implement tool for enhancing the transparency of ViTs. We evaluate LeGrad in challenging segmentation, perturbation, and open-vocabulary settings, showcasing its versatility compared to other SotA explainability methods demonstrating its superior spatial fidelity and robustness to perturbations. A demo and the code is available at https://github.com/WalBouss/LeGrad.","sentences":["Vision Transformers (ViTs), with their ability to model long-range dependencies through self-attention mechanisms, have become a standard architecture in computer vision.","However, the interpretability of these models remains a challenge.","To address this, we propose LeGrad, an explainability method specifically designed for ViTs.","LeGrad computes the gradient with respect to the attention maps of ViT layers, considering the gradient itself as the explainability signal.","We aggregate the signal over all layers, combining the activations of the last as well as intermediate tokens to produce the merged explainability map.","This makes LeGrad a conceptually simple and an easy-to-implement tool for enhancing the transparency of ViTs.","We evaluate LeGrad in challenging segmentation, perturbation, and open-vocabulary settings, showcasing its versatility compared to other SotA explainability methods demonstrating its superior spatial fidelity and robustness to perturbations.","A demo and the code is available at https://github.com/WalBouss/LeGrad."],"url":"http://arxiv.org/abs/2404.03214v1","category":"cs.CV"}
{"created":"2024-04-04 03:21:38","title":"Direct visualization of local magnetic domain dynamics in a 2D Van der Walls material/ferromagnet interface","abstract":"Exploring new strategies for controlling the magnetic domain propagation is the key to realize ultrafast, high-density domain wall-based memory and logic devices for next generation computing. These strategies include strain modulation in multiferroic devices, geometric confinement and area-selective pinning of domain wall. 2D Van der Waals materials introduce localized modifications to the interfacial magnetic order, enabling control over the propagation of magnetic domains. Here, using Lorentz-Transmission Electron Microscopy (L-TEM) along with the Modified Transport of Intensity equations (MTIE), we demonstrate controlled domain expansion with in-situ magnetic field in a ferromagnet (Permalloy, NiFe) interfacing with a 2D Van der Waals material Graphene (Gr). The Gr/NiFe interface exhibits distinctive domain expansion rate with magnetic field selectively near the interface which is further analyzed using micromagnetic simulations. Our findings are crucial for comprehending direct visualization of interface controlled magnetic domain expansion, offering insights for developing future domain wall-based technology.","sentences":["Exploring new strategies for controlling the magnetic domain propagation is the key to realize ultrafast, high-density domain wall-based memory and logic devices for next generation computing.","These strategies include strain modulation in multiferroic devices, geometric confinement and area-selective pinning of domain wall.","2D Van der Waals materials introduce localized modifications to the interfacial magnetic order, enabling control over the propagation of magnetic domains.","Here, using Lorentz-Transmission Electron Microscopy (L-TEM) along with the Modified Transport of Intensity equations (MTIE), we demonstrate controlled domain expansion with in-situ magnetic field in a ferromagnet (Permalloy, NiFe) interfacing with a 2D Van der Waals material Graphene (Gr).","The Gr/NiFe interface exhibits distinctive domain expansion rate with magnetic field selectively near the interface which is further analyzed using micromagnetic simulations.","Our findings are crucial for comprehending direct visualization of interface controlled magnetic domain expansion, offering insights for developing future domain wall-based technology."],"url":"http://arxiv.org/abs/2404.03177v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 03:20:35","title":"Information-Theoretic Generalization Bounds for Deep Neural Networks","abstract":"Deep neural networks (DNNs) exhibit an exceptional capacity for generalization in practical applications. This work aims to capture the effect and benefits of depth for supervised learning via information-theoretic generalization bounds. We first derive two hierarchical bounds on the generalization error in terms of the Kullback-Leibler (KL) divergence or the 1-Wasserstein distance between the train and test distributions of the network internal representations. The KL divergence bound shrinks as the layer index increases, while the Wasserstein bound implies the existence of a layer that serves as a generalization funnel, which attains a minimal 1-Wasserstein distance. Analytic expressions for both bounds are derived under the setting of binary Gaussian classification with linear DNNs. To quantify the contraction of the relevant information measures when moving deeper into the network, we analyze the strong data processing inequality (SDPI) coefficient between consecutive layers of three regularized DNN models: Dropout, DropConnect, and Gaussian noise injection. This enables refining our generalization bounds to capture the contraction as a function of the network architecture parameters. Specializing our results to DNNs with a finite parameter space and the Gibbs algorithm reveals that deeper yet narrower network architectures generalize better in those examples, although how broadly this statement applies remains a question.","sentences":["Deep neural networks (DNNs) exhibit an exceptional capacity for generalization in practical applications.","This work aims to capture the effect and benefits of depth for supervised learning via information-theoretic generalization bounds.","We first derive two hierarchical bounds on the generalization error in terms of the Kullback-Leibler (KL) divergence or the 1-Wasserstein distance between the train and test distributions of the network internal representations.","The KL divergence bound shrinks as the layer index increases, while the Wasserstein bound implies the existence of a layer that serves as a generalization funnel, which attains a minimal 1-Wasserstein distance.","Analytic expressions for both bounds are derived under the setting of binary Gaussian classification with linear DNNs.","To quantify the contraction of the relevant information measures when moving deeper into the network, we analyze the strong data processing inequality (SDPI) coefficient between consecutive layers of three regularized DNN models: Dropout, DropConnect, and Gaussian noise injection.","This enables refining our generalization bounds to capture the contraction as a function of the network architecture parameters.","Specializing our results to DNNs with a finite parameter space and the Gibbs algorithm reveals that deeper yet narrower network architectures generalize better in those examples, although how broadly this statement applies remains a question."],"url":"http://arxiv.org/abs/2404.03176v1","category":"cs.LG"}
{"created":"2024-04-04 03:17:27","title":"Matching-star size Ramsey numbers under connectivity constraint","abstract":"Recently, Caro, Patk\\'os, and Tuza (2022) introduced the concept of connected Tur\\'an number. We study a similar parameter in Ramsey theory. Given two graphs $G_1$ and $G_2$, the size Ramsey number $\\hat{r}(G_1,G_2)$ refers to the smallest number of edges in a graph $G$ such that for any red-blue edge-coloring of $G$, either a red subgraph $G_1$ or a blue subgraph $G_2$ is present in $G$. If we further restrict the host graph $G$ to be connected, we obtain the connected size Ramsey number, denoted as $\\hat{r}_c(G_1,G_2)$. Erd\\H{o}s and Faudree (1984) proved that $\\hat r(nK_2,K_{1,m})=mn$ for all positive integers $m,n$. In this paper, we concentrate on the connected analog of this result. Rahadjeng, Baskoro, and Assiyatun (2016) provided the exact values of $\\hat r_c(nK_2,K_{1,m})$ for $n=2,3$. We establish a more general result: for all positive integers $m$ and $n$ with $m\\ge (n^2+2pn+n-3)/2$, we have $\\hat r_c(nK_{1,p},K_{1,m})=n(m+p)-1$. As a corollary, $\\hat r_c(nK_2,K_{1,m})=nm+n-1$ for $m\\ge (n^2+3n-3)/2$. We also propose a conjecture for the interested reader.","sentences":["Recently, Caro, Patk\\'os, and Tuza (2022) introduced the concept of connected Tur\\'an number.","We study a similar parameter in Ramsey theory.","Given two graphs $G_1$ and $G_2$, the size Ramsey number $\\hat{r}(G_1,G_2)$ refers to the smallest number of edges in a graph $G$ such that for any red-blue edge-coloring of $G$, either a red subgraph $G_1$ or a blue subgraph $G_2$ is present in $G$. If we further restrict the host graph $G$ to be connected, we obtain the connected size Ramsey number, denoted as $\\hat{r}_c(G_1,G_2)$. Erd\\H{o}s and Faudree (1984) proved that $\\hat r(nK_2,K_{1,m})=mn$ for all positive integers $m,n$.","In this paper, we concentrate on the connected analog of this result.","Rahadjeng, Baskoro, and Assiyatun (2016) provided the exact values of $\\hat r_c(nK_2,K_{1,m})$ for $n=2,3$. We establish a more general result: for all positive integers $m$ and $n$ with $m\\ge (n^2+2pn+n-3)/2$, we have $\\hat r_c(nK_{1,p},K_{1,m})=n(m+p)-1$. As a corollary, $\\hat r_c(nK_2,K_{1,m})=nm+n-1$ for $m\\ge (n^2+3n-3)/2$. We also propose a conjecture for the interested reader."],"url":"http://arxiv.org/abs/2404.03175v1","category":"math.CO"}
{"created":"2024-04-04 02:55:55","title":"A Dynamic Droplet Breakup Model for Eulerian-Lagrangian Simulation of Liquid-fueled Detonation","abstract":"This study proposes a dynamic model to reflect the physical image of the droplet breakup process in two-phase detonation flows. This breakup model is implemented in a two-phase detonation solver developed based on an open-source computational fluid dynamic platform, OpenFOAM, and compared with three prevalent models (TAB, PilchErdman, and ReitzKH-RT model) under different droplet diameters in one- and two-dimensional detonation problems. The simulating results show that the present breakup model well predicts experimentally determined detonation parameters such as detonation velocities and post-wave temperature. In addition, the present model has the advantage of being free of the KH breakup time parameter, which is needed by the ReitzKH-RT model to fit the experimental data. The one-dimensional detonation simulations indicate that different breakup models have a slight impact on the detonation wave velocity because the droplet breakup process does not significantly affect the total heat release as long as it is sufficiently fast to sustain the detonation. However, the two-dimensional detonation simulations show that both the breakup model and the droplet initial diameter significantly affect the detonation cell size due to the different droplet distributions predicted by different models. The breakup length, which is the distance from the shock wave to the location at which sufficiently small child droplets appear, affects the chemical reaction zone thickness, which in turn affects the detonation cell size. A longer breakup length will result in a larger detonation cell size.","sentences":["This study proposes a dynamic model to reflect the physical image of the droplet breakup process in two-phase detonation flows.","This breakup model is implemented in a two-phase detonation solver developed based on an open-source computational fluid dynamic platform, OpenFOAM, and compared with three prevalent models (TAB, PilchErdman, and ReitzKH-RT model) under different droplet diameters in one-","and two-dimensional detonation problems.","The simulating results show that the present breakup model well predicts experimentally determined detonation parameters such as detonation velocities and post-wave temperature.","In addition, the present model has the advantage of being free of the KH breakup time parameter, which is needed by the ReitzKH-RT model to fit the experimental data.","The one-dimensional detonation simulations indicate that different breakup models have a slight impact on the detonation wave velocity because the droplet breakup process does not significantly affect the total heat release as long as it is sufficiently fast to sustain the detonation.","However, the two-dimensional detonation simulations show that both the breakup model and the droplet initial diameter significantly affect the detonation cell size due to the different droplet distributions predicted by different models.","The breakup length, which is the distance from the shock wave to the location at which sufficiently small child droplets appear, affects the chemical reaction zone thickness, which in turn affects the detonation cell size.","A longer breakup length will result in a larger detonation cell size."],"url":"http://arxiv.org/abs/2404.03170v1","category":"physics.flu-dyn"}
{"created":"2024-04-04 02:34:46","title":"Towards Collaborative Family-Centered Design for Online Safety, Privacy and Security","abstract":"Traditional online safety technologies often overly restrict teens and invade their privacy, while parents often lack knowledge regarding their digital privacy. As such, prior researchers have called for more collaborative approaches on adolescent online safety and networked privacy. In this paper, we propose family-centered approaches to foster parent-teen collaboration in ensuring their mobile privacy and online safety while respecting individual privacy, to enhance open discussion and teens' self-regulation. However, challenges such as power imbalances and conflicts with family values arise when implementing such approaches, making parent-teen collaboration difficult. Therefore, attending the family-centered design workshop will provide an invaluable opportunity for us to discuss these challenges and identify best research practices for the future of collaborative online safety and privacy within families.","sentences":["Traditional online safety technologies often overly restrict teens and invade their privacy, while parents often lack knowledge regarding their digital privacy.","As such, prior researchers have called for more collaborative approaches on adolescent online safety and networked privacy.","In this paper, we propose family-centered approaches to foster parent-teen collaboration in ensuring their mobile privacy and online safety while respecting individual privacy, to enhance open discussion and teens' self-regulation.","However, challenges such as power imbalances and conflicts with family values arise when implementing such approaches, making parent-teen collaboration difficult.","Therefore, attending the family-centered design workshop will provide an invaluable opportunity for us to discuss these challenges and identify best research practices for the future of collaborative online safety and privacy within families."],"url":"http://arxiv.org/abs/2404.03165v1","category":"cs.HC"}
{"created":"2024-04-04 01:59:59","title":"Orthogonal calibration via posterior projections with applications to the Schwarzschild model","abstract":"The orbital superposition method originally developed by Schwarzschild (1979) is used to study the dynamics of growth of a black hole and its host galaxy, and has uncovered new relationships between the galaxy's global characteristics. Scientists are specifically interested in finding optimal parameter choices for this model that best match physical measurements along with quantifying the uncertainty of such procedures. This renders a statistical calibration problem with multivariate outcomes. In this article, we develop a Bayesian method for calibration with multivariate outcomes using orthogonal bias functions thus ensuring parameter identifiability. Our approach is based on projecting the posterior to an appropriate space which allows the user to choose any nonparametric prior on the bias function(s) instead of having to model it (them) with Gaussian processes. We develop a functional projection approach using the theory of Hilbert spaces. A finite-dimensional analogue of the projection problem is also considered. We illustrate the proposed approach using a BART prior and apply it to calibrate the Schwarzschild model illustrating how a multivariate approach may resolve discrepancies resulting from a univariate calibration.","sentences":["The orbital superposition method originally developed by Schwarzschild (1979) is used to study the dynamics of growth of a black hole and its host galaxy, and has uncovered new relationships between the galaxy's global characteristics.","Scientists are specifically interested in finding optimal parameter choices for this model that best match physical measurements along with quantifying the uncertainty of such procedures.","This renders a statistical calibration problem with multivariate outcomes.","In this article, we develop a Bayesian method for calibration with multivariate outcomes using orthogonal bias functions thus ensuring parameter identifiability.","Our approach is based on projecting the posterior to an appropriate space which allows the user to choose any nonparametric prior on the bias function(s) instead of having to model it (them) with Gaussian processes.","We develop a functional projection approach using the theory of Hilbert spaces.","A finite-dimensional analogue of the projection problem is also considered.","We illustrate the proposed approach using a BART prior and apply it to calibrate the Schwarzschild model illustrating how a multivariate approach may resolve discrepancies resulting from a univariate calibration."],"url":"http://arxiv.org/abs/2404.03152v1","category":"stat.ME"}
{"created":"2024-04-04 01:40:59","title":"Computational Study Based Prediction of New Photocatalysts for water splitting by systematic manipulation of MXene surfaces","abstract":"The compositional and structural flexibility of functionalised two-dimensional metal carbonitrides or MXenes has been exploited through a combinatorial search for new materials that can act as catalysts for photo-assisted water splitting by absorbing sunlight with energy in the infra-red region. Detailed calculations on 49 Janus MXenes where two surfaces are of asymmetric nature are carried out by first-principles Density Functional Theory. A screening procedure is adopted to arrive at potential candidates. Our calculations predict four new materials whose surfaces can activate both hydrogen and oxygen evolution reactions upon splitting water, two out of which are infra-red active, and the rest are visible light-active. We have performed a detailed microscopic analysis to find out the interrelations of the structural model of surface functionalisation, the chemistry of the surfaces, the electronic structure, and the alignment of bands with respect to the reaction potentials that explain our results. Apart from these four compounds, we find thirteen other compounds that are suitable for either hydrogen evolution or oxygen reduction reactions. This study lays out a guideline for the systematic discovery of potential new catalysts for water splitting under sunlight irradiation.","sentences":["The compositional and structural flexibility of functionalised two-dimensional metal carbonitrides or MXenes has been exploited through a combinatorial search for new materials that can act as catalysts for photo-assisted water splitting by absorbing sunlight with energy in the infra-red region.","Detailed calculations on 49 Janus MXenes where two surfaces are of asymmetric nature are carried out by first-principles Density Functional Theory.","A screening procedure is adopted to arrive at potential candidates.","Our calculations predict four new materials whose surfaces can activate both hydrogen and oxygen evolution reactions upon splitting water, two out of which are infra-red active, and the rest are visible light-active.","We have performed a detailed microscopic analysis to find out the interrelations of the structural model of surface functionalisation, the chemistry of the surfaces, the electronic structure, and the alignment of bands with respect to the reaction potentials that explain our results.","Apart from these four compounds, we find thirteen other compounds that are suitable for either hydrogen evolution or oxygen reduction reactions.","This study lays out a guideline for the systematic discovery of potential new catalysts for water splitting under sunlight irradiation."],"url":"http://arxiv.org/abs/2404.03146v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 00:38:51","title":"Results of existence and uniqueness for the Cauchy problem of semilinear heat equations on stratified Lie groups","abstract":"The aim of this paper is to give existence and uniqueness results for solutions of the Cauchy problem for semilinear heat equations on stratified Lie groups $\\mathbb{G}$ with the homogeneous dimension $N$. We consider the nonlinear function behaves like $|u|^{\\alpha}$ or $|u|^{\\alpha-1}u$ $(\\alpha>1)$ and the initial data $u_0$ belongs to the Sobolev spaces $L^p_s(\\mathbb{G})$ for $1<p<\\infty$ and $0<s<N/p$. Since stratified Lie groups $\\mathbb{G}$ include the Euclidean space ${\\mathbb R}^n$ as an example, our results are an extension of the existence and uniqueness results obtained by F. Ribaud on ${\\mathbb R}^n$ to $\\mathbb{G}$. It should be noted that our proof is very different from it given by Ribaud on ${\\mathbb R}^n$. We adopt the generalized fractional chain rule on $\\mathbb{G}$ to obtain the estimate for the nonlinear term, which is very different from the paracomposition technique adopted by Ribaud on ${\\mathbb R}^n$. By using the generalized fractional chain rule on $\\mathbb{G}$, we can avoid the discussion of Fourier analysis on $\\mathbb{G}$ and make the proof more simple.","sentences":["The aim of this paper is to give existence and uniqueness results for solutions of the Cauchy problem for semilinear heat equations on stratified Lie groups $\\mathbb{G}$ with the homogeneous dimension $N$. We consider the nonlinear function behaves like $|u|^{\\alpha}$ or $|u|^{\\alpha-1}u$ $(\\alpha>1)$ and the initial data $u_0$ belongs to the Sobolev spaces $L^p_s(\\mathbb{G})$ for $1<p<\\infty$ and $0<s<N/p$. Since stratified Lie groups $\\mathbb{G}$ include the Euclidean space ${\\mathbb R}^n$ as an example, our results are an extension of the existence and uniqueness results obtained by F. Ribaud on ${\\mathbb R}^n$ to $\\mathbb{G}$. It should be noted that our proof is very different from it given by Ribaud on ${\\mathbb R}^n$. We adopt the generalized fractional chain rule on $\\mathbb{G}$ to obtain the estimate for the nonlinear term, which is very different from the paracomposition technique adopted by Ribaud on ${\\mathbb R}^n$. By using the generalized fractional chain rule on $\\mathbb{G}$, we can avoid the discussion of Fourier analysis on $\\mathbb{G}$ and make the proof more simple."],"url":"http://arxiv.org/abs/2404.03128v1","category":"math.AP"}
{"created":"2024-04-04 00:28:50","title":"GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis","abstract":"We present GaSpCT, a novel view synthesis and 3D scene representation method used to generate novel projection views for Computer Tomography (CT) scans. We adapt the Gaussian Splatting framework to enable novel view synthesis in CT based on limited sets of 2D image projections and without the need for Structure from Motion (SfM) methodologies. Therefore, we reduce the total scanning duration and the amount of radiation dose the patient receives during the scan. We adapted the loss function to our use-case by encouraging a stronger background and foreground distinction using two sparsity promoting regularizers: a beta loss and a total variation (TV) loss. Finally, we initialize the Gaussian locations across the 3D space using a uniform prior distribution of where the brain's positioning would be expected to be within the field of view. We evaluate the performance of our model using brain CT scans from the Parkinson's Progression Markers Initiative (PPMI) dataset and demonstrate that the rendered novel views closely match the original projection views of the simulated scan, and have better performance than other implicit 3D scene representations methodologies. Furthermore, we empirically observe reduced training time compared to neural network based image synthesis for sparse-view CT image reconstruction. Finally, the memory requirements of the Gaussian Splatting representations are reduced by 17% compared to the equivalent voxel grid image representations.","sentences":["We present GaSpCT, a novel view synthesis and 3D scene representation method used to generate novel projection views for Computer Tomography (CT) scans.","We adapt the Gaussian Splatting framework to enable novel view synthesis in CT based on limited sets of 2D image projections and without the need for Structure from Motion (SfM) methodologies.","Therefore, we reduce the total scanning duration and the amount of radiation dose the patient receives during the scan.","We adapted the loss function to our use-case by encouraging a stronger background and foreground distinction using two sparsity promoting regularizers: a beta loss and a total variation (TV) loss.","Finally, we initialize the Gaussian locations across the 3D space using a uniform prior distribution of where the brain's positioning would be expected to be within the field of view.","We evaluate the performance of our model using brain CT scans from the Parkinson's Progression Markers Initiative (PPMI) dataset and demonstrate that the rendered novel views closely match the original projection views of the simulated scan, and have better performance than other implicit 3D scene representations methodologies.","Furthermore, we empirically observe reduced training time compared to neural network based image synthesis for sparse-view CT image reconstruction.","Finally, the memory requirements of the Gaussian Splatting representations are reduced by 17% compared to the equivalent voxel grid image representations."],"url":"http://arxiv.org/abs/2404.03126v1","category":"eess.IV"}
{"created":"2024-04-04 00:21:57","title":"The Diffusive Ultrasound Modulated Bioluminescence Tomography with Partial Data and Uncertain Optical Parameters","abstract":"The paper studies an imaging problem in the diffusive ultrasound-modulated bioluminescence tomography with partial boundary measurement in an anisotropic medium. Assuming plane-wave modulation, we transform the imaging problem to an inverse problem with internal data, and derive a reconstruction procedure to recover the bioluminescent source. Subsequently, an uncertainty quantification estimate is established to assess the robustness of the reconstruction. To facilitate practical implementation, we discretize the diffusive model using the staggered grid scheme, resulting in a discrete formulation of the UMBLT inverse problem. A discrete reconstruction procedure is then presented along with a discrete uncertainty quantification estimate. Finally, the reconstruction procedure is quantitatively validated through numerical examples to demonstrate the efficacy and reliability of the proposed approach and estimates.","sentences":["The paper studies an imaging problem in the diffusive ultrasound-modulated bioluminescence tomography with partial boundary measurement in an anisotropic medium.","Assuming plane-wave modulation, we transform the imaging problem to an inverse problem with internal data, and derive a reconstruction procedure to recover the bioluminescent source.","Subsequently, an uncertainty quantification estimate is established to assess the robustness of the reconstruction.","To facilitate practical implementation, we discretize the diffusive model using the staggered grid scheme, resulting in a discrete formulation of the UMBLT inverse problem.","A discrete reconstruction procedure is then presented along with a discrete uncertainty quantification estimate.","Finally, the reconstruction procedure is quantitatively validated through numerical examples to demonstrate the efficacy and reliability of the proposed approach and estimates."],"url":"http://arxiv.org/abs/2404.03124v1","category":"math.AP"}
{"created":"2024-04-04 00:14:15","title":"Constraints on the spacetime variation of the fine-structure constant using DESI emission-line galaxies","abstract":"We present strong constraints on the spacetime variation of the fine-structure constant $\\alpha$ using the Dark Energy Spectroscopic Instrument (DESI). In this pilot work, we utilize $\\sim110,000$ galaxies with strong and narrow O III $\\lambda\\lambda$4959,5007 emission lines to measure the relative variation $\\Delta\\alpha/\\alpha$ in space and time. The O III doublet is arguably the best choice for this purpose owing to its wide wavelength separation between the two lines and its strong emission in many galaxies. Our galaxy sample spans a redshift range of $0<z<0.95$, covering half of all cosmic time. We divide the sample into subsamples in 10 redshift bins ($\\Delta z=0.1$), and calculate $\\Delta\\alpha/\\alpha$ for the individual subsamples. The uncertainties of the measured $\\Delta\\alpha/\\alpha$ are roughly between $2\\times10^{-6}$ and $2\\times10^{-5}$. We find an apparent $\\alpha$ variation with redshift at a level of $\\Delta\\alpha/\\alpha=(2\\sim3)\\times10^{-5}$. This is highly likely to be caused by systematics associated with wavelength calibration, since such small systematics can be caused by a wavelength distortion of $0.002-0.003$ \\AA, which is beyond the accuracy that the current DESI data can achieve. We refine the wavelength calibration using sky lines for a small fraction of the galaxies, but it does not change our main results. We further probe the spatial variation of $\\alpha$ in small redshift ranges, and do not find obvious, large-scale structures in the spatial distribution of $\\Delta\\alpha/\\alpha$. As DESI is ongoing, we will include more galaxies, and by improving the wavelength calibration, we expect to obtain a better constraint that is comparable to the strongest current constraint.","sentences":["We present strong constraints on the spacetime variation of the fine-structure constant $\\alpha$ using the Dark Energy Spectroscopic Instrument (DESI).","In this pilot work, we utilize $\\sim110,000$ galaxies with strong and narrow","O III $\\lambda\\lambda$4959,5007 emission lines to measure the relative variation $\\Delta\\alpha/\\alpha$ in space and time.","The O III doublet is arguably the best choice for this purpose owing to its wide wavelength separation between the two lines and its strong emission in many galaxies.","Our galaxy sample spans a redshift range of $0<z<0.95$, covering half of all cosmic time.","We divide the sample into subsamples in 10 redshift bins ($\\Delta z=0.1$), and calculate $\\Delta\\alpha/\\alpha$ for the individual subsamples.","The uncertainties of the measured $\\Delta\\alpha/\\alpha$ are roughly between $2\\times10^{-6}$ and $2\\times10^{-5}$. We find an apparent $\\alpha$ variation with redshift at a level of $\\Delta\\alpha/\\alpha=(2\\sim3)\\times10^{-5}$. This is highly likely to be caused by systematics associated with wavelength calibration, since such small systematics can be caused by a wavelength distortion of $0.002-0.003$ \\AA, which is beyond the accuracy that the current DESI data can achieve.","We refine the wavelength calibration using sky lines for a small fraction of the galaxies, but it does not change our main results.","We further probe the spatial variation of $\\alpha$ in small redshift ranges, and do not find obvious, large-scale structures in the spatial distribution of $\\Delta\\alpha/\\alpha$. As DESI is ongoing, we will include more galaxies, and by improving the wavelength calibration, we expect to obtain a better constraint that is comparable to the strongest current constraint."],"url":"http://arxiv.org/abs/2404.03123v1","category":"astro-ph.CO"}
{"created":"2024-04-03 23:59:35","title":"Enhancing Student Engagement in Large-Scale Capstone Courses: An Experience Report","abstract":"Computer science (CS) capstone courses offer students a valuable opportunity to gain hands-on experience in software development, practice essential soft skills, and enhance their employability prospects. They are a core component in many CS undergraduate degrees and address the ACM curricula requirements of inculcating professional dispositions in students and making them aware of the broader societal implications of computing. However, coordinating a capstone course, especially for a large student cohort, can be a daunting task for academic staff. It demands considerable time and energy for planning and coordinating activities between students, academic staff, and any external stakeholders. In this experience report, we outline the iterative development and refinement of our capstone course as it grew substantially in size over a span of six consecutive sessions. We outline the pedagogies that helped us to enhance student engagement and motivation in the course as assessed by end-of-course surveys and students' written reflections. We share the lessons that we have learnt and provide recommendations to educators who are designing new capstone courses or looking to scale existing ones.","sentences":["Computer science (CS) capstone courses offer students a valuable opportunity to gain hands-on experience in software development, practice essential soft skills, and enhance their employability prospects.","They are a core component in many CS undergraduate degrees and address the ACM curricula requirements of inculcating professional dispositions in students and making them aware of the broader societal implications of computing.","However, coordinating a capstone course, especially for a large student cohort, can be a daunting task for academic staff.","It demands considerable time and energy for planning and coordinating activities between students, academic staff, and any external stakeholders.","In this experience report, we outline the iterative development and refinement of our capstone course as it grew substantially in size over a span of six consecutive sessions.","We outline the pedagogies that helped us to enhance student engagement and motivation in the course as assessed by end-of-course surveys and students' written reflections.","We share the lessons that we have learnt and provide recommendations to educators who are designing new capstone courses or looking to scale existing ones."],"url":"http://arxiv.org/abs/2404.03120v1","category":"cs.CY"}
{"created":"2024-04-03 23:51:01","title":"Suppressing the sample variance of DESI-like galaxy clustering with fast simulations","abstract":"Ongoing and upcoming galaxy redshift surveys, such as the Dark Energy Spectroscopic Instrument (DESI) survey, will observe vast regions of sky and a wide range of redshifts. In order to model the observations and address various systematic uncertainties, $N$-body simulations are routinely adopted, however, the number of large simulations with sufficiently high mass resolution is usually limited by available computing time. Therefore, achieving a simulation volume with the effective statistical errors significantly smaller than those of the observations becomes prohibitively expensive. In this study, we apply the Convergence Acceleration by Regression and Pooling (CARPool) method to mitigate the sample variance of the DESI-like galaxy clustering in the AbacusSummit simulations, with the assistance of the quasi-$N$-body simulations FastPM. Based on the halo occupation distribution (HOD) models, we construct different FastPM galaxy catalogs, including the luminous red galaxies (LRGs), emission line galaxies (ELGs), and quasars, with their number densities and two-point clustering statistics well matched to those of AbacusSummit. We also employ the same initial conditions between AbacusSummit and FastPM to achieve high cross-correlation, as it is useful in effectively suppressing the variance. Our method of reducing noise in clustering is equivalent to performing a simulation with volume larger by a factor of 5 and 4 for LRGs and ELGs, respectively. We also mitigate the standard deviation of the LRG bispectrum with the triangular configurations $k_2=2k_1=0.2\\hMpc$ by a factor of 1.6. With smaller sample variance on galaxy clustering, we are able to constrain the BAO scale parameters to higher precision. The CARPool method will be beneficial to better constrain the theoretical systematics of BAO, redshift space distortions (RSD) and primordial non-Gaussianity.","sentences":["Ongoing and upcoming galaxy redshift surveys, such as the Dark Energy Spectroscopic Instrument (DESI) survey, will observe vast regions of sky and a wide range of redshifts.","In order to model the observations and address various systematic uncertainties, $N$-body simulations are routinely adopted, however, the number of large simulations with sufficiently high mass resolution is usually limited by available computing time.","Therefore, achieving a simulation volume with the effective statistical errors significantly smaller than those of the observations becomes prohibitively expensive.","In this study, we apply the Convergence Acceleration by Regression and Pooling (CARPool) method to mitigate the sample variance of the DESI-like galaxy clustering in the AbacusSummit simulations, with the assistance of the quasi-$N$-body simulations FastPM.","Based on the halo occupation distribution (HOD) models, we construct different FastPM galaxy catalogs, including the luminous red galaxies (LRGs), emission line galaxies (ELGs), and quasars, with their number densities and two-point clustering statistics well matched to those of AbacusSummit.","We also employ the same initial conditions between AbacusSummit and FastPM to achieve high cross-correlation, as it is useful in effectively suppressing the variance.","Our method of reducing noise in clustering is equivalent to performing a simulation with volume larger by a factor of 5 and 4 for LRGs and ELGs, respectively.","We also mitigate the standard deviation of the LRG bispectrum with the triangular configurations $k_2=2k_1=0.2\\hMpc$ by a factor of 1.6.","With smaller sample variance on galaxy clustering, we are able to constrain the BAO scale parameters to higher precision.","The CARPool method will be beneficial to better constrain the theoretical systematics of BAO, redshift space distortions (RSD) and primordial non-Gaussianity."],"url":"http://arxiv.org/abs/2404.03117v1","category":"astro-ph.CO"}
{"created":"2024-04-03 23:32:53","title":"QED: Scalable Verification of Hardware Memory Consistency","abstract":"Memory consistency model (MCM) issues in out-of-order-issue microprocessor-based shared-memory systems are notoriously non-intuitive and a source of hardware design bugs. Prior hardware verification work is limited to in-order-issue processors, to proving the correctness only of some test cases, or to bounded verification that does not scale in practice beyond 7 instructions across all threads. Because cache coherence (i.e., write serialization and atomicity) and pipeline front-end verification and testing are well-studied, we focus on the memory ordering in an out-of-order-issue processor's load-store queue and the coherence interface between the core and global coherence. We propose QED based on the key notion of observability that any hardware reordering matters only if a forbidden value is produced. We argue that one needs to consider (1) only directly-ordered instruction pairs -- transitively non-redundant pairs connected by an edge in the MCM-imposed partial order -- and not all in-flight instructions, and (2) only the ordering of external events from other cores (e.g.,invalidations) but not the events' originating cores, achieving verification scalability in both the numbers of in-flight memory instructions and of cores. Exhaustively considering all pairs of instruction types and all types of external events intervening between each pair, QED attempts to restore any reordered instructions to an MCM-complaint order without changing the execution values, where failure indicates an MCM violation. Each instruction pair's exploration results in a decision tree of simple, narrowly-defined predicates to be evaluated against the RTL. In our experiments, we automatically generate the decision trees for SC, TSO, and RISC-V WMO, and illustrate automatable verification by evaluating a substantial predicate against BOOMv3 implementation of RISC-V WMO, leaving full automation to future work.","sentences":["Memory consistency model (MCM) issues in out-of-order-issue microprocessor-based shared-memory systems are notoriously non-intuitive and a source of hardware design bugs.","Prior hardware verification work is limited to in-order-issue processors, to proving the correctness only of some test cases, or to bounded verification that does not scale in practice beyond 7 instructions across all threads.","Because cache coherence (i.e., write serialization and atomicity) and pipeline front-end verification and testing are well-studied, we focus on the memory ordering in an out-of-order-issue processor's load-store queue and the coherence interface between the core and global coherence.","We propose QED based on the key notion of observability that any hardware reordering matters only if a forbidden value is produced.","We argue that one needs to consider (1) only directly-ordered instruction pairs -- transitively non-redundant pairs connected by an edge in the MCM-imposed partial order -- and not all in-flight instructions, and (2) only the ordering of external events from other cores (e.g.,invalidations) but not the events' originating cores, achieving verification scalability in both the numbers of in-flight memory instructions and of cores.","Exhaustively considering all pairs of instruction types and all types of external events intervening between each pair, QED attempts to restore any reordered instructions to an MCM-complaint order without changing the execution values, where failure indicates an MCM violation.","Each instruction pair's exploration results in a decision tree of simple, narrowly-defined predicates to be evaluated against the RTL.","In our experiments, we automatically generate the decision trees for SC, TSO, and RISC-V WMO, and illustrate automatable verification by evaluating a substantial predicate against BOOMv3 implementation of RISC-V WMO, leaving full automation to future work."],"url":"http://arxiv.org/abs/2404.03113v1","category":"cs.AR"}
{"created":"2024-04-03 23:24:52","title":"PDRs4All VIII: Mid-IR emission line inventory of the Orion Bar","abstract":"Mid-infrared emission features probe the properties of ionized gas, and hot or warm molecular gas. The Orion Bar is a frequently studied photodissociation region (PDR) containing large amounts of gas under these conditions, and was observed with the MIRI IFU aboard JWST as part of the \"PDRs4All\" program. The resulting IR spectroscopic images of high angular resolution (0.2\") reveal a rich observational inventory of mid-IR emission lines, and spatially resolve the substructure of the PDR, with a mosaic cutting perpendicularly across the ionization front and three dissociation fronts. We extracted five spectra that represent the ionized, atomic, and molecular gas layers, and measured the most prominent gas emission lines. An initial analysis summarizes the physical conditions of the gas and the potential of these data. We identified around 100 lines, report an additional 18 lines that remain unidentified, and measured the line intensities and central wavelengths. The H I recombination lines originating from the ionized gas layer bordering the PDR, have intensity ratios that are well matched by emissivity coefficients from H recombination theory, but deviate up to 10% due contamination by He I lines. We report the observed emission lines of various ionization stages of Ne, P, S, Cl, Ar, Fe, and Ni, and show how certain line ratios vary between the five regions. We observe the pure-rotational H$_2$ lines in the vibrational ground state from 0-0 S(1) to 0-0 S(8), and in the first vibrationally excited state from 1-1 S(5) to 1-1 S(9). We derive H$_2$ excitation diagrams, and approximate the excitation with one thermal (~700 K) component representative of an average gas temperature, and one non-thermal component (~2700 K) probing the effect of UV pumping. We compare these results to an existing model for the Orion Bar PDR and highlight the differences with the observations.","sentences":["Mid-infrared emission features probe the properties of ionized gas, and hot or warm molecular gas.","The Orion Bar is a frequently studied photodissociation region (PDR) containing large amounts of gas under these conditions, and was observed with the MIRI IFU aboard JWST as part of the \"PDRs4All\" program.","The resulting IR spectroscopic images of high angular resolution (0.2\") reveal a rich observational inventory of mid-IR emission lines, and spatially resolve the substructure of the PDR, with a mosaic cutting perpendicularly across the ionization front and three dissociation fronts.","We extracted five spectra that represent the ionized, atomic, and molecular gas layers, and measured the most prominent gas emission lines.","An initial analysis summarizes the physical conditions of the gas and the potential of these data.","We identified around 100 lines, report an additional 18 lines that remain unidentified, and measured the line intensities and central wavelengths.","The H I recombination lines originating from the ionized gas layer bordering the PDR, have intensity ratios that are well matched by emissivity coefficients from H recombination theory, but deviate up to 10% due contamination by He I lines.","We report the observed emission lines of various ionization stages of Ne, P, S, Cl, Ar, Fe, and Ni, and show how certain line ratios vary between the five regions.","We observe the pure-rotational H$_2$ lines in the vibrational ground state from 0-0 S(1) to 0-0 S(8), and in the first vibrationally excited state from 1-1 S(5) to 1-1 S(9).","We derive H$_2$ excitation diagrams, and approximate the excitation with one thermal (~700 K) component representative of an average gas temperature, and one non-thermal component (~2700 K) probing the effect of UV pumping.","We compare these results to an existing model for the Orion Bar PDR and highlight the differences with the observations."],"url":"http://arxiv.org/abs/2404.03111v1","category":"astro-ph.GA"}
{"created":"2024-04-03 23:15:26","title":"Reducing the Impact of I/O Contention in Numerical Weather Prediction Workflows at Scale Using DAOS","abstract":"Operational Numerical Weather Prediction (NWP) workflows are highly data-intensive. Data volumes have increased by many orders of magnitude over the last 40 years, and are expected to continue to do so, especially given the upcoming adoption of Machine Learning in forecast processes. Parallel POSIX-compliant file systems have been the dominant paradigm in data storage and exchange in HPC workflows for many years. This paper presents ECMWF's move beyond the POSIX paradigm, implementing a backend for their storage library to support DAOS -- a novel high-performance object store designed for massively distributed Non-Volatile Memory. This system is demonstrated to be able to outperform the highly mature and optimised POSIX backend when used under high load and contention, as per typical forecast workflow I/O patterns. This work constitutes a significant step forward, beyond the performance constraints imposed by POSIX semantics.","sentences":["Operational Numerical Weather Prediction (NWP) workflows are highly data-intensive.","Data volumes have increased by many orders of magnitude over the last 40 years, and are expected to continue to do so, especially given the upcoming adoption of Machine Learning in forecast processes.","Parallel POSIX-compliant file systems have been the dominant paradigm in data storage and exchange in HPC workflows for many years.","This paper presents ECMWF's move beyond the POSIX paradigm, implementing a backend for their storage library to support DAOS -- a novel high-performance object store designed for massively distributed Non-Volatile Memory.","This system is demonstrated to be able to outperform the highly mature and optimised POSIX backend when used under high load and contention, as per typical forecast workflow I/O patterns.","This work constitutes a significant step forward, beyond the performance constraints imposed by POSIX semantics."],"url":"http://arxiv.org/abs/2404.03107v1","category":"cs.DC"}
{"created":"2024-04-03 23:07:24","title":"Methodology for Interpretable Reinforcement Learning for Optimizing Mechanical Ventilation","abstract":"Mechanical ventilation is a critical life-support intervention that uses a machine to deliver controlled air and oxygen to a patient's lungs, assisting or replacing spontaneous breathing. While several data-driven approaches have been proposed to optimize ventilator control strategies, they often lack interpretability and agreement with general domain knowledge. This paper proposes a methodology for interpretable reinforcement learning (RL) using decision trees for mechanical ventilation control. Using a causal, nonparametric model-based off-policy evaluation, we evaluate the policies in their ability to gain increases in SpO2 while avoiding aggressive ventilator settings which are known to cause ventilator induced lung injuries and other complications. Numerical experiments using MIMIC-III data on the stays of real patients' intensive care unit stays demonstrate that the decision tree policy outperforms the behavior cloning policy and is comparable to state-of-the-art RL policy. Future work concerns better aligning the cost function with medical objectives to generate deeper clinical insights.","sentences":["Mechanical ventilation is a critical life-support intervention that uses a machine to deliver controlled air and oxygen to a patient's lungs, assisting or replacing spontaneous breathing.","While several data-driven approaches have been proposed to optimize ventilator control strategies, they often lack interpretability and agreement with general domain knowledge.","This paper proposes a methodology for interpretable reinforcement learning (RL) using decision trees for mechanical ventilation control.","Using a causal, nonparametric model-based off-policy evaluation, we evaluate the policies in their ability to gain increases in SpO2 while avoiding aggressive ventilator settings which are known to cause ventilator induced lung injuries and other complications.","Numerical experiments using MIMIC-III data on the stays of real patients' intensive care unit stays demonstrate that the decision tree policy outperforms the behavior cloning policy and is comparable to state-of-the-art RL policy.","Future work concerns better aligning the cost function with medical objectives to generate deeper clinical insights."],"url":"http://arxiv.org/abs/2404.03105v1","category":"cs.LG"}
{"created":"2024-04-03 22:06:14","title":"Direct detection of light dark matter charged under a $L_\u03bc - L_\u03c4$ symmetry","abstract":"A possible extension of the Standard Model able to explain the recent measurement of the anomalous magnetic moment of the muon consists in adding a gauged $U(1)_{L_{\\mu}-L_{\\tau}}$ symmetry. If the dark matter particle is charged under this symmetry, the kinetic mixing between the new gauge boson and the photon induces dark matter-electron interactions. We derive direct detection constraints on light dark matter charged under a $U(1)_{L_{\\mu}-L_{\\tau}}$ symmetry with electron recoil experiments, and explore prospects with XLZD and OSCURA to close in the parameter space able to explain simultaneously the recent measurement on the anomalous magnetic moment of the muon and the observed relic density of dark matter. We further discuss the spin-dependent scattering contribution arising in this model, which was ignored previously in the literature.","sentences":["A possible extension of the Standard Model able to explain the recent measurement of the anomalous magnetic moment of the muon consists in adding a gauged $U(1)_{L_{\\mu}-L_{\\tau}}$ symmetry.","If the dark matter particle is charged under this symmetry, the kinetic mixing between the new gauge boson and the photon induces dark matter-electron interactions.","We derive direct detection constraints on light dark matter charged under a $U(1)_{L_{\\mu}-L_{\\tau}}$ symmetry with electron recoil experiments, and explore prospects with XLZD and OSCURA to close in the parameter space able to explain simultaneously the recent measurement on the anomalous magnetic moment of the muon and the observed relic density of dark matter.","We further discuss the spin-dependent scattering contribution arising in this model, which was ignored previously in the literature."],"url":"http://arxiv.org/abs/2404.03090v1","category":"hep-ph"}
{"created":"2024-04-03 22:01:26","title":"Auditing the Use of Language Models to Guide Hiring Decisions","abstract":"Regulatory efforts to protect against algorithmic bias have taken on increased urgency with rapid advances in large language models (LLMs), which are machine learning models that can achieve performance rivaling human experts on a wide array of tasks. A key theme of these initiatives is algorithmic \"auditing,\" but current regulations -- as well as the scientific literature -- provide little guidance on how to conduct these assessments. Here we propose and investigate one approach for auditing algorithms: correspondence experiments, a widely applied tool for detecting bias in human judgements. In the employment context, correspondence experiments aim to measure the extent to which race and gender impact decisions by experimentally manipulating elements of submitted application materials that suggest an applicant's demographic traits, such as their listed name. We apply this method to audit candidate assessments produced by several state-of-the-art LLMs, using a novel corpus of applications to K-12 teaching positions in a large public school district. We find evidence of moderate race and gender disparities, a pattern largely robust to varying the types of application material input to the models, as well as the framing of the task to the LLMs. We conclude by discussing some important limitations of correspondence experiments for auditing algorithms.","sentences":["Regulatory efforts to protect against algorithmic bias have taken on increased urgency with rapid advances in large language models (LLMs), which are machine learning models that can achieve performance rivaling human experts on a wide array of tasks.","A key theme of these initiatives is algorithmic \"auditing,\" but current regulations -- as well as the scientific literature -- provide little guidance on how to conduct these assessments.","Here we propose and investigate one approach for auditing algorithms: correspondence experiments, a widely applied tool for detecting bias in human judgements.","In the employment context, correspondence experiments aim to measure the extent to which race and gender impact decisions by experimentally manipulating elements of submitted application materials that suggest an applicant's demographic traits, such as their listed name.","We apply this method to audit candidate assessments produced by several state-of-the-art LLMs, using a novel corpus of applications to K-12 teaching positions in a large public school district.","We find evidence of moderate race and gender disparities, a pattern largely robust to varying the types of application material input to the models, as well as the framing of the task to the LLMs.","We conclude by discussing some important limitations of correspondence experiments for auditing algorithms."],"url":"http://arxiv.org/abs/2404.03086v1","category":"stat.AP"}
{"created":"2024-04-03 21:45:27","title":"vPALs: Towards Verified Performance-aware Learning System For Resource Management","abstract":"Accurately predicting task performance at runtime in a cluster is advantageous for a resource management system to determine whether a task should be migrated due to performance degradation caused by interference. This is beneficial for both cluster operators and service owners. However, deploying performance prediction systems with learning methods requires sophisticated safeguard mechanisms due to the inherent stochastic and black-box natures of these models, such as Deep Neural Networks (DNNs). Vanilla Neural Networks (NNs) can be vulnerable to out-of-distribution data samples that can lead to sub-optimal decisions. To take a step towards a safe learning system in performance prediction, We propose vPALs that leverage well-correlated system metrics, and verification to produce safe performance prediction at runtime, providing an extra layer of safety to integrate learning techniques to cluster resource management systems. Our experiments show that vPALs can outperform vanilla NNs across our benchmark workload.","sentences":["Accurately predicting task performance at runtime in a cluster is advantageous for a resource management system to determine whether a task should be migrated due to performance degradation caused by interference.","This is beneficial for both cluster operators and service owners.","However, deploying performance prediction systems with learning methods requires sophisticated safeguard mechanisms due to the inherent stochastic and black-box natures of these models, such as Deep Neural Networks (DNNs).","Vanilla Neural Networks (NNs) can be vulnerable to out-of-distribution data samples that can lead to sub-optimal decisions.","To take a step towards a safe learning system in performance prediction, We propose vPALs that leverage well-correlated system metrics, and verification to produce safe performance prediction at runtime, providing an extra layer of safety to integrate learning techniques to cluster resource management systems.","Our experiments show that vPALs can outperform vanilla NNs across our benchmark workload."],"url":"http://arxiv.org/abs/2404.03079v1","category":"cs.DC"}
{"created":"2024-04-03 21:37:49","title":"Quadrilateral Particle Arrangement within Shocks in a Two-Dimensional Dusty Plasma","abstract":"The microscopic structure within a two-dimensional shock was studied using data from a dusty plasma experiment. A single layer of charged microparticles, levitated in a glow-discharge plasma, was perturbed by an electrically floating wire that was moved at a steady supersonic speed to excite a compressional shock. A rearrangement of particles was observed, from a hexagonal lattice in the preshock into a quadrilateral microstructure within the shock. This quadrilateral structure would not be stable in a monolayer of identical repulsive particles, under equilibrium conditions. Glaser-Clark polygon analysis of the microstructure helped in identifying quadrilaterals. Voronoi analysis was used to characterize the defect fraction behind the shock, as an indication of shock-induced melting.","sentences":["The microscopic structure within a two-dimensional shock was studied using data from a dusty plasma experiment.","A single layer of charged microparticles, levitated in a glow-discharge plasma, was perturbed by an electrically floating wire that was moved at a steady supersonic speed to excite a compressional shock.","A rearrangement of particles was observed, from a hexagonal lattice in the preshock into a quadrilateral microstructure within the shock.","This quadrilateral structure would not be stable in a monolayer of identical repulsive particles, under equilibrium conditions.","Glaser-Clark polygon analysis of the microstructure helped in identifying quadrilaterals.","Voronoi analysis was used to characterize the defect fraction behind the shock, as an indication of shock-induced melting."],"url":"http://arxiv.org/abs/2404.03078v1","category":"physics.plasm-ph"}
{"created":"2024-04-03 21:30:46","title":"PowerSimulations.jl -- A Power Systems operations simulation Library","abstract":"PowerSimulations.jl is a Julia-based BSD-licensed power system operations simulation tool developed as a flexible and open source software for quasi-static power systems simulations including Production Cost Models. PowerSimulations.jl tackles the issues of developing a simulation model in a modular way providing tools for the formulation of decision models and emulation models that can be solved independently or in an interconnected fashion. This paper discusses the software implementation of PowerSimulations.jl as a template for the development and implementation of operation simulators, providing solutions to commonly encountered issues like time series read/write and results sharing between models. The paper includes a publicly-available validation of classical operations simulations as well as examples of the advanced features of the software.","sentences":["PowerSimulations.jl is a Julia-based BSD-licensed power system operations simulation tool developed as a flexible and open source software for quasi-static power systems simulations including Production Cost Models.","PowerSimulations.jl tackles the issues of developing a simulation model in a modular way providing tools for the formulation of decision models and emulation models that can be solved independently or in an interconnected fashion.","This paper discusses the software implementation of PowerSimulations.jl as a template for the development and implementation of operation simulators, providing solutions to commonly encountered issues like time series read/write and results sharing between models.","The paper includes a publicly-available validation of classical operations simulations as well as examples of the advanced features of the software."],"url":"http://arxiv.org/abs/2404.03074v1","category":"eess.SY"}
{"created":"2024-04-03 20:56:33","title":"Asymptotically-exact selective inference for quantile regression","abstract":"When analyzing large datasets, it is common to select a model prior to making inferences. For reliable inferences, it is important to make adjustments that account for the model selection process, resulting in selective inferences. Our paper introduces an asymptotic pivot to infer about the effects of selected variables on conditional quantile functions. Utilizing estimators from smoothed quantile regression, our proposed pivot is easy to compute and ensures asymptotically-exact selective inferences without making strict distributional assumptions about the response variable. At the core of the pivot is the use of external randomization, which enables us to utilize the full sample for both selection and inference without the need to partition the data into independent data subsets or discard data at either step. On simulated data, we find that: (i) the asymptotic confidence intervals based on our pivot achieve the desired coverage rates, even in cases where sample splitting fails due to insufficient sample size for inference; (ii) our intervals are consistently shorter than those produced by sample splitting across various models and signal settings. We report similar findings when we apply our approach to study risk factors for low birth weights in a publicly accessible dataset of US birth records from 2022.","sentences":["When analyzing large datasets, it is common to select a model prior to making inferences.","For reliable inferences, it is important to make adjustments that account for the model selection process, resulting in selective inferences.","Our paper introduces an asymptotic pivot to infer about the effects of selected variables on conditional quantile functions.","Utilizing estimators from smoothed quantile regression, our proposed pivot is easy to compute and ensures asymptotically-exact selective inferences without making strict distributional assumptions about the response variable.","At the core of the pivot is the use of external randomization, which enables us to utilize the full sample for both selection and inference without the need to partition the data into independent data subsets or discard data at either step.","On simulated data, we find that: (i) the asymptotic confidence intervals based on our pivot achieve the desired coverage rates, even in cases where sample splitting fails due to insufficient sample size for inference; (ii) our intervals are consistently shorter than those produced by sample splitting across various models and signal settings.","We report similar findings when we apply our approach to study risk factors for low birth weights in a publicly accessible dataset of US birth records from 2022."],"url":"http://arxiv.org/abs/2404.03059v1","category":"stat.ME"}
{"created":"2024-04-03 20:38:47","title":"Emission Line Predictions for Mock Galaxy Catalogues: a New Differentiable and Empirical Mapping from DESI","abstract":"We present a simple, differentiable method for predicting emission line strengths from rest-frame optical continua using an empirically-determined mapping. Extensive work has been done to develop mock galaxy catalogues that include robust predictions for galaxy photometry, but reliably predicting the strengths of emission lines has remained challenging. Our new mapping is a simple neural network implemented using the JAX Python automatic differentiation library. It is trained on Dark Energy Spectroscopic Instrument Early Release data to predict the equivalent widths (EWs) of the eight brightest optical emission lines (including H$\\alpha$, H$\\beta$, [O II], and [O III]) from a galaxy's rest-frame optical continuum. The predicted EW distributions are consistent with the observed ones when noise is accounted for, and we find Spearman's rank correlation coefficient $\\rho_s > 0.87$ between predictions and observations for most lines. Using a non-linear dimensionality reduction technique (UMAP), we show that this is true for galaxies across the full range of observed spectral energy distributions. In addition, we find that adding measurement uncertainties to the predicted line strengths is essential for reproducing the distribution of observed line-ratios in the BPT diagram. Our trained network can easily be incorporated into a differentiable stellar population synthesis pipeline without hindering differentiability or scalability with GPUs. A synthetic catalogue generated with such a pipeline can be used to characterise and account for biases in the spectroscopic training sets used for training and calibration of photo-$z$'s, improving the modelling of systematic incompleteness for the Rubin Observatory LSST and other surveys.","sentences":["We present a simple, differentiable method for predicting emission line strengths from rest-frame optical continua using an empirically-determined mapping.","Extensive work has been done to develop mock galaxy catalogues that include robust predictions for galaxy photometry, but reliably predicting the strengths of emission lines has remained challenging.","Our new mapping is a simple neural network implemented using the JAX Python automatic differentiation library.","It is trained on Dark Energy Spectroscopic Instrument Early Release data to predict the equivalent widths (EWs) of the eight brightest optical emission lines (including H$\\alpha$, H$\\beta$, [O II], and [O III]) from a galaxy's rest-frame optical continuum.","The predicted EW distributions are consistent with the observed ones when noise is accounted for, and we find Spearman's rank correlation coefficient $\\rho_s > 0.87$ between predictions and observations for most lines.","Using a non-linear dimensionality reduction technique (UMAP), we show that this is true for galaxies across the full range of observed spectral energy distributions.","In addition, we find that adding measurement uncertainties to the predicted line strengths is essential for reproducing the distribution of observed line-ratios in the BPT diagram.","Our trained network can easily be incorporated into a differentiable stellar population synthesis pipeline without hindering differentiability or scalability with GPUs.","A synthetic catalogue generated with such a pipeline can be used to characterise and account for biases in the spectroscopic training sets used for training and calibration of photo-$z$'s, improving the modelling of systematic incompleteness for the Rubin Observatory LSST and other surveys."],"url":"http://arxiv.org/abs/2404.03055v1","category":"astro-ph.GA"}
{"created":"2024-04-03 20:15:29","title":"Have any LISA verification binaries been found?","abstract":"Some electromagnetically observed ultra-compact binaries will be strong gravitational wave sources for space-based detectors like the Laser Interferometer Space Antenna (LISA). These sources have historically been referred to as \"verification binaries\" under the assumption that they will be exploited to assess mission performance. This paper quantitatively interrogates that scenario by considering targeted analyses of known galactic sources in the context of a full simulation of the galactic gravitational wave foreground. We find that the analysis of the best currently known LISA binaries, even making maximal use of the available information about the sources, is susceptible to ambiguity or biases when not simultaneously fitting to the rest of the galactic population. While galactic binaries discovered electromagnetically in advance of, or during, the LISA survey are highly valuable multimessenger systems, the need for a global treatment of the galactic gravitational wave foreground calls into question whether or not they are the best sources for data characterization.","sentences":["Some electromagnetically observed ultra-compact binaries will be strong gravitational wave sources for space-based detectors like the Laser Interferometer Space Antenna (LISA).","These sources have historically been referred to as \"verification binaries\" under the assumption that they will be exploited to assess mission performance.","This paper quantitatively interrogates that scenario by considering targeted analyses of known galactic sources in the context of a full simulation of the galactic gravitational wave foreground.","We find that the analysis of the best currently known LISA binaries, even making maximal use of the available information about the sources, is susceptible to ambiguity or biases when not simultaneously fitting to the rest of the galactic population.","While galactic binaries discovered electromagnetically in advance of, or during, the LISA survey are highly valuable multimessenger systems, the need for a global treatment of the galactic gravitational wave foreground calls into question whether or not they are the best sources for data characterization."],"url":"http://arxiv.org/abs/2404.03046v1","category":"astro-ph.HE"}
{"created":"2024-04-03 20:15:16","title":"Analysis of a VEM-fully discrete polytopal scheme with bubble stabilisation for contact mechanics with Tresca friction","abstract":"This work performs the convergence analysis of the polytopal nodal discretisation of contact-mechanics (with Tresca friction) recently introduced in [18] in the framework of poro-elastic models in fractured porous media. The scheme is based on a mixed formulation, using face-wise constant approximations of the Lagrange multipliers along the fracture network and a fully discrete first order nodal approximation of the displacement field. The displacement field is enriched with additional bubble degrees of freedom along the fractures to ensure the inf-sup stability with the Lagrange multiplier space. It is presented in a fully discrete formulation, which makes its study more straightforward, but also has a Virtual Element interpretation. The analysis establishes an abstract error estimate accounting for the fully discrete framework and the non-conformity of the discretisation. A first order error estimate is deduced for sufficiently smooth solutions both for the gradient of the displacement field and the Lagrange multiplier. A key difficulty of the numerical analysis is the proof of a discrete inf-sup condition, which is based on a non-standard $H^{-1/2}$-norm (to deal with fracture networks) and involves the jump of the displacements, not their traces. The analysis also requires the proof of a discrete Korn inequality for the discrete displacement field which takes into account fracture networks. Numerical experiments based on analytical solutions confirm our theoretical findings","sentences":["This work performs the convergence analysis of the polytopal nodal discretisation of contact-mechanics (with Tresca friction) recently introduced in [18] in the framework of poro-elastic models in fractured porous media.","The scheme is based on a mixed formulation, using face-wise constant approximations of the Lagrange multipliers along the fracture network and a fully discrete first order nodal approximation of the displacement field.","The displacement field is enriched with additional bubble degrees of freedom along the fractures to ensure the inf-sup stability with the Lagrange multiplier space.","It is presented in a fully discrete formulation, which makes its study more straightforward, but also has a Virtual Element interpretation.","The analysis establishes an abstract error estimate accounting for the fully discrete framework and the non-conformity of the discretisation.","A first order error estimate is deduced for sufficiently smooth solutions both for the gradient of the displacement field and the Lagrange multiplier.","A key difficulty of the numerical analysis is the proof of a discrete inf-sup condition, which is based on a non-standard $H^{-1/2}$-norm (to deal with fracture networks) and involves the jump of the displacements, not their traces.","The analysis also requires the proof of a discrete Korn inequality for the discrete displacement field which takes into account fracture networks.","Numerical experiments based on analytical solutions confirm our theoretical findings"],"url":"http://arxiv.org/abs/2404.03045v1","category":"math.NA"}
{"created":"2024-04-03 20:05:00","title":"Linear Anchored Gaussian Mixture Model for Location and Width Computation of Objects in Thick Line Shape","abstract":"An accurate detection of the centerlines of linear objects is a challenging topic in many sensitive real-world applications such X-ray imaging, remote sensing and lane marking detection in road traffic. Model-based approaches using Hough and Radon transforms are often used but, are not recommended for thick line detection, whereas approaches based on image derivatives need further step-by-step processing, making their efficiency dependent on each step outcomes. In this paper, we aim to detect linear structures found in images by considering the 3D representation of the image gray levels as a finite mixture model of statistical distribution. The latter, which we named linear anchored Gaussian distribution could be parametrized by a scale value {\\sigma} describing the linear structure thickness and a line equation, parametrized, in turn, by a radius \\r{ho} and an orientation angle {\\theta}, describing the linear structure centerline location. Expectation-Maximization (EM) algorithm is used for the mixture model parameter estimation, where a new paradigm, using the background subtraction for the likelihood function computation, is proposed. For the EM algorithm, two {\\theta} parameter initialization schemes are used: the first one is based on a random choice of the first component of {\\theta} vector, whereas the second is based on the image Hessian with a simultaneous computation of the mixture model components number. Experiments on real world images and synthetic images corrupted by blur and additive noise show the good performance of the proposed methods, where the algorithm using background subtraction and Hessian-based {\\theta} initialization provides an outstanding accuracy of the linear structure detection despite irregular image background and presence of blur and noise.","sentences":["An accurate detection of the centerlines of linear objects is a challenging topic in many sensitive real-world applications such X-ray imaging, remote sensing and lane marking detection in road traffic.","Model-based approaches using Hough and Radon transforms are often used but, are not recommended for thick line detection, whereas approaches based on image derivatives need further step-by-step processing, making their efficiency dependent on each step outcomes.","In this paper, we aim to detect linear structures found in images by considering the 3D representation of the image gray levels as a finite mixture model of statistical distribution.","The latter, which we named linear anchored Gaussian distribution could be parametrized by a scale value {\\sigma} describing the linear structure thickness and a line equation, parametrized, in turn, by a radius \\r{ho} and an orientation angle {\\theta}, describing the linear structure centerline location.","Expectation-Maximization (EM) algorithm is used for the mixture model parameter estimation, where a new paradigm, using the background subtraction for the likelihood function computation, is proposed.","For the EM algorithm, two {\\theta} parameter initialization schemes are used: the first one is based on a random choice of the first component of {\\theta} vector, whereas the second is based on the image Hessian with a simultaneous computation of the mixture model components number.","Experiments on real world images and synthetic images corrupted by blur and additive noise show the good performance of the proposed methods, where the algorithm using background subtraction and Hessian-based {\\theta} initialization provides an outstanding accuracy of the linear structure detection despite irregular image background and presence of blur and noise."],"url":"http://arxiv.org/abs/2404.03043v1","category":"cs.CV"}
{"created":"2024-04-03 19:34:06","title":"The Impact of Extended H$_{2}$O Cross-Sections on Temperate Anoxic Planet Atmospheres: Implications for Spectral Characterization of Habitable Worlds","abstract":"JWST has created a new era of terrestrial exoplanet atmospheric characterization, and with it the possibility to detect potential biosignature gases like CH$_{4}$. Our interpretation of exoplanet atmospheric spectra, and the veracity of these interpretations, will be limited by our understanding of atmospheric processes and the accuracy of input modeling data. Molecular cross-sections are essential inputs to these models. The photochemistry of temperate planets depends on photolysis reactions whose rates are governed by the dissociation cross-sections of key molecules. H$_{2}$O is one such molecule; the photolysis of H$_{2}$O produces OH, a highly reactive and efficient sink for atmospheric trace gases. We investigate the photochemical effects of improved H$_{2}$O cross-sections on anoxic terrestrial planets as a function of host star spectral type (FGKM) and CH$_{4}$ surface flux. Our results show that updated H$_{2}$O cross-sections, extended to wavelengths $>$200 nm, substantially impact the predicted abundances of trace gases destroyed by OH. The differences for anoxic terrestrial planets orbiting Sun-like host stars are greatest, showing changes of up to three orders of magnitude in surface CO levels, and over an order of magnitude in surface CH$_{4}$ levels. These differences lead to observable changes in simulated planetary spectra, especially important in the context of future direct-imaging missions. In contrast, the atmospheres of planets orbiting M-dwarf stars are substantially less affected. Our results demonstrate a pressing need for refined dissociation cross-section data for H$_{2}$O, where uncertainties remain, and other key molecules, especially at mid-UV wavelengths $>$200 nm.","sentences":["JWST has created a new era of terrestrial exoplanet atmospheric characterization, and with it the possibility to detect potential biosignature gases like CH$_{4}$. Our interpretation of exoplanet atmospheric spectra, and the veracity of these interpretations, will be limited by our understanding of atmospheric processes and the accuracy of input modeling data.","Molecular cross-sections are essential inputs to these models.","The photochemistry of temperate planets depends on photolysis reactions whose rates are governed by the dissociation cross-sections of key molecules.","H$_{2}$O is one such molecule; the photolysis of H$_{2}$O produces OH, a highly reactive and efficient sink for atmospheric trace gases.","We investigate the photochemical effects of improved H$_{2}$O cross-sections on anoxic terrestrial planets as a function of host star spectral type (FGKM) and CH$_{4}$ surface flux.","Our results show that updated H$_{2}$O cross-sections, extended to wavelengths $>$200 nm, substantially impact the predicted abundances of trace gases destroyed by OH.","The differences for anoxic terrestrial planets orbiting Sun-like host stars are greatest, showing changes of up to three orders of magnitude in surface CO levels, and over an order of magnitude in surface CH$_{4}$ levels.","These differences lead to observable changes in simulated planetary spectra, especially important in the context of future direct-imaging missions.","In contrast, the atmospheres of planets orbiting M-dwarf stars are substantially less affected.","Our results demonstrate a pressing need for refined dissociation cross-section data for H$_{2}$O, where uncertainties remain, and other key molecules, especially at mid-UV wavelengths $>$200 nm."],"url":"http://arxiv.org/abs/2404.03031v1","category":"astro-ph.EP"}
{"created":"2024-04-03 19:32:57","title":"Leveraging Apache Arrow for Zero-copy, Zero-serialization Cluster Shared Memory","abstract":"This paper describes a distributed implementation of Apache Arrow that can leverage cluster-shared load-store addressable memory that is hardware-coherent only within each node. The implementation is built on the ThymesisFlow prototype that leverages the OpenCAPI interface to create a shared address space across a cluster. While Apache Arrow structures are immutable, simplifying their use in a cluster shared memory, this paper creates distributed Apache Arrow tables and makes them accessible in each node.","sentences":["This paper describes a distributed implementation of Apache Arrow that can leverage cluster-shared load-store addressable memory that is hardware-coherent only within each node.","The implementation is built on the ThymesisFlow prototype that leverages the OpenCAPI interface to create a shared address space across a cluster.","While Apache Arrow structures are immutable, simplifying their use in a cluster shared memory, this paper creates distributed Apache Arrow tables and makes them accessible in each node."],"url":"http://arxiv.org/abs/2404.03030v1","category":"cs.ET"}
{"created":"2024-04-03 19:31:56","title":"An Incomplete Loop: Deductive, Inductive, and Abductive Learning in Large Language Models","abstract":"Modern language models (LMs) can learn to perform new tasks in different ways: in instruction following, the target task is described explicitly in natural language; in few-shot prompting, the task is specified implicitly with a small number of examples; in instruction inference, LMs are presented with in-context examples and are then prompted to generate a natural language task description before making predictions. Each of these procedures may be thought of as invoking a different form of reasoning: instruction following involves deductive reasoning, few-shot prompting involves inductive reasoning, and instruction inference involves abductive reasoning. How do these different capabilities relate? Across four LMs (from the gpt and llama families) and two learning problems (involving arithmetic functions and machine translation) we find a strong dissociation between the different types of reasoning: LMs can sometimes learn effectively from few-shot prompts even when they are unable to explain their own prediction rules; conversely, they sometimes infer useful task descriptions while completely failing to learn from human-generated descriptions of the same task. Our results highlight the non-systematic nature of reasoning even in some of today's largest LMs, and underscore the fact that very different learning mechanisms may be invoked by seemingly similar prompting procedures.","sentences":["Modern language models (LMs) can learn to perform new tasks in different ways: in instruction following, the target task is described explicitly in natural language; in few-shot prompting, the task is specified implicitly with a small number of examples; in instruction inference, LMs are presented with in-context examples and are then prompted to generate a natural language task description before making predictions.","Each of these procedures may be thought of as invoking a different form of reasoning: instruction following involves deductive reasoning, few-shot prompting involves inductive reasoning, and instruction inference involves abductive reasoning.","How do these different capabilities relate?","Across four LMs (from the gpt and llama families) and two learning problems (involving arithmetic functions and machine translation) we find a strong dissociation between the different types of reasoning: LMs can sometimes learn effectively from few-shot prompts even when they are unable to explain their own prediction rules; conversely, they sometimes infer useful task descriptions while completely failing to learn from human-generated descriptions of the same task.","Our results highlight the non-systematic nature of reasoning even in some of today's largest LMs, and underscore the fact that very different learning mechanisms may be invoked by seemingly similar prompting procedures."],"url":"http://arxiv.org/abs/2404.03028v1","category":"cs.CL"}
{"created":"2024-04-03 19:07:39","title":"Probing electron quadrupling order through ultrasound","abstract":"Recent experiments have pointed to the formation of a new state of matter, the electron quadrupling condensate in Ba$_{1-x}$K$_x$Fe$_2$As$_2$ . The state spontaneously breaks time-reversal symmetry and is sandwiched between two critical points, separating it from the superconducting and normal metal states. The adjacent two critical points make acoustic effects a promising tool to study such states because of their sensitivity to symmetry-breaking transitions. We report a theory of the acoustic effects of systems with an electron quadrupling phase and new ultrasound velocity measurements of Ba$_{1-x}$K$_x$Fe$_2$As$_2$ single crystals. The presented theory for the electron quadrupling state gives the same type of singularities that are observed in experiment.","sentences":["Recent experiments have pointed to the formation of a new state of matter, the electron quadrupling condensate in Ba$_{1-x}$K$_x$Fe$_2$As$_2$ .","The state spontaneously breaks time-reversal symmetry and is sandwiched between two critical points, separating it from the superconducting and normal metal states.","The adjacent two critical points make acoustic effects a promising tool to study such states because of their sensitivity to symmetry-breaking transitions.","We report a theory of the acoustic effects of systems with an electron quadrupling phase and new ultrasound velocity measurements of Ba$_{1-x}$K$_x$Fe$_2$As$_2$ single crystals.","The presented theory for the electron quadrupling state gives the same type of singularities that are observed in experiment."],"url":"http://arxiv.org/abs/2404.03020v1","category":"cond-mat.supr-con"}
{"created":"2024-04-03 19:00:19","title":"Incorporating non-linear effects in fast semi-analytical thermal modelling of powder bed fusion","abstract":"The usefulness of semi-analytical thermal models for predicting the connection between process, microstructure and properties in powder bed fusion has been well illustrated in recent years. Such an approach provides the promise of accuracy comparable to tools that are orders of magnitude more computationally expensive. The opportunity to make predictions that span several orders of magnitude in space and time comes at the cost of significant simplifications, limiting fully quantitative predictions without empirical calibration. This approach relies on solving a linear problem meaning that first order non-linear effects induced by e.g. the temperature dependence of material properties and surface boundary conditions, are not incorporated. Here, we revisit these limitations and highlight ways that temperature varying material properties and radiative heat loss from the melt pool can be systematically accounted for. These corrections, made with an eye to minimizing additional computational overhead, bring the technique's predictive capability much closer to that of high fidelity thermal simulations. Quantitative comparisons to experiments are used to illustrate the important impact of including such corrections.","sentences":["The usefulness of semi-analytical thermal models for predicting the connection between process, microstructure and properties in powder bed fusion has been well illustrated in recent years.","Such an approach provides the promise of accuracy comparable to tools that are orders of magnitude more computationally expensive.","The opportunity to make predictions that span several orders of magnitude in space and time comes at the cost of significant simplifications, limiting fully quantitative predictions without empirical calibration.","This approach relies on solving a linear problem meaning that first order non-linear effects induced by e.g. the temperature dependence of material properties and surface boundary conditions, are not incorporated.","Here, we revisit these limitations and highlight ways that temperature varying material properties and radiative heat loss from the melt pool can be systematically accounted for.","These corrections, made with an eye to minimizing additional computational overhead, bring the technique's predictive capability much closer to that of high fidelity thermal simulations.","Quantitative comparisons to experiments are used to illustrate the important impact of including such corrections."],"url":"http://arxiv.org/abs/2404.03018v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-03 18:57:54","title":"Distributionally Robust Policy and Lyapunov-Certificate Learning","abstract":"This article presents novel methods for synthesizing distributionally robust stabilizing neural controllers and certificates for control systems under model uncertainty. A key challenge in designing controllers with stability guarantees for uncertain systems is the accurate determination of and adaptation to shifts in model parametric uncertainty during online deployment. We tackle this with a novel distributionally robust formulation of the Lyapunov derivative chance constraint ensuring a monotonic decrease of the Lyapunov certificate. To avoid the computational complexity involved in dealing with the space of probability measures, we identify a sufficient condition in the form of deterministic convex constraints that ensures the Lyapunov derivative constraint is satisfied. We integrate this condition into a loss function for training a neural network-based controller and show that, for the resulting closed-loop system, the global asymptotic stability of its equilibrium can be certified with high confidence, even with Out-of-Distribution (OoD) model uncertainties. To demonstrate the efficacy and efficiency of the proposed methodology, we compare it with an uncertainty-agnostic baseline approach and several reinforcement learning approaches in two control problems in simulation.","sentences":["This article presents novel methods for synthesizing distributionally robust stabilizing neural controllers and certificates for control systems under model uncertainty.","A key challenge in designing controllers with stability guarantees for uncertain systems is the accurate determination of and adaptation to shifts in model parametric uncertainty during online deployment.","We tackle this with a novel distributionally robust formulation of the Lyapunov derivative chance constraint ensuring a monotonic decrease of the Lyapunov certificate.","To avoid the computational complexity involved in dealing with the space of probability measures, we identify a sufficient condition in the form of deterministic convex constraints that ensures the Lyapunov derivative constraint is satisfied.","We integrate this condition into a loss function for training a neural network-based controller and show that, for the resulting closed-loop system, the global asymptotic stability of its equilibrium can be certified with high confidence, even with Out-of-Distribution (OoD) model uncertainties.","To demonstrate the efficacy and efficiency of the proposed methodology, we compare it with an uncertainty-agnostic baseline approach and several reinforcement learning approaches in two control problems in simulation."],"url":"http://arxiv.org/abs/2404.03017v1","category":"eess.SY"}
{"created":"2024-04-03 18:54:27","title":"DPFT: Dual Perspective Fusion Transformer for Camera-Radar-based Object Detection","abstract":"The perception of autonomous vehicles has to be efficient, robust, and cost-effective. However, cameras are not robust against severe weather conditions, lidar sensors are expensive, and the performance of radar-based perception is still inferior to the others. Camera-radar fusion methods have been proposed to address this issue, but these are constrained by the typical sparsity of radar point clouds and often designed for radars without elevation information. We propose a novel camera-radar fusion approach called Dual Perspective Fusion Transformer (DPFT), designed to overcome these limitations. Our method leverages lower-level radar data (the radar cube) instead of the processed point clouds to preserve as much information as possible and employs projections in both the camera and ground planes to effectively use radars with elevation information and simplify the fusion with camera data. As a result, DPFT has demonstrated state-of-the-art performance on the K-Radar dataset while showing remarkable robustness against adverse weather conditions and maintaining a low inference time. The code is made available as open-source software under https://github.com/TUMFTM/DPFT.","sentences":["The perception of autonomous vehicles has to be efficient, robust, and cost-effective.","However, cameras are not robust against severe weather conditions, lidar sensors are expensive, and the performance of radar-based perception is still inferior to the others.","Camera-radar fusion methods have been proposed to address this issue, but these are constrained by the typical sparsity of radar point clouds and often designed for radars without elevation information.","We propose a novel camera-radar fusion approach called Dual Perspective Fusion Transformer (DPFT), designed to overcome these limitations.","Our method leverages lower-level radar data (the radar cube) instead of the processed point clouds to preserve as much information as possible and employs projections in both the camera and ground planes to effectively use radars with elevation information and simplify the fusion with camera data.","As a result, DPFT has demonstrated state-of-the-art performance on the K-Radar dataset while showing remarkable robustness against adverse weather conditions and maintaining a low inference time.","The code is made available as open-source software under https://github.com/TUMFTM/DPFT."],"url":"http://arxiv.org/abs/2404.03015v1","category":"cs.CV"}
{"created":"2024-04-03 18:41:49","title":"DESI 2024 III: Baryon Acoustic Oscillations from Galaxies and Quasars","abstract":"We present the DESI 2024 galaxy and quasar baryon acoustic oscillations (BAO) measurements using over 5.7 million unique galaxy and quasar redshifts in the range 0.1<z<2.1. Divided by tracer type, we utilize 300,017 galaxies from the magnitude-limited Bright Galaxy Survey with 0.1<z<0.4, 2,138,600 Luminous Red Galaxies with 0.4<z<1.1, 2,432,022 Emission Line Galaxies with 0.8<z<1.6, and 856,652 quasars with 0.8<z<2.1, over a ~7,500 square degree footprint. The analysis was blinded at the catalog-level to avoid confirmation bias. All fiducial choices of the BAO fitting and reconstruction methodology, as well as the size of the systematic errors, were determined on the basis of the tests with mock catalogs and the blinded data catalogs. We present several improvements to the BAO analysis pipeline, including enhancing the BAO fitting and reconstruction methods in a more physically-motivated direction, and also present results using combinations of tracers. We present a re-analysis of SDSS BOSS and eBOSS results applying the improved DESI methodology and find scatter consistent with the level of the quoted SDSS theoretical systematic uncertainties. With the total effective survey volume of ~ 18 Gpc$^3$, the combined precision of the BAO measurements across the six different redshift bins is ~0.52%, marking a 1.2-fold improvement over the previous state-of-the-art results using only first-year data. We detect the BAO in all of these six redshift bins. The highest significance of BAO detection is $9.1\\sigma$ at the effective redshift of 0.93, with a constraint of 0.86% placed on the BAO scale. We find our measurements are systematically larger than the prediction of Planck-2018 LCDM model at z<0.8. We translate the results into transverse comoving distance and radial Hubble distance measurements, which are used to constrain cosmological models in our companion paper [abridged].","sentences":["We present the DESI 2024 galaxy and quasar baryon acoustic oscillations (BAO) measurements using over 5.7 million unique galaxy and quasar redshifts in the range 0.1<z<2.1.","Divided by tracer type, we utilize 300,017 galaxies from the magnitude-limited Bright Galaxy Survey with 0.1<z<0.4, 2,138,600 Luminous Red Galaxies with 0.4<z<1.1, 2,432,022 Emission Line Galaxies with 0.8<z<1.6, and 856,652 quasars with 0.8<z<2.1, over a ~7,500 square degree footprint.","The analysis was blinded at the catalog-level to avoid confirmation bias.","All fiducial choices of the BAO fitting and reconstruction methodology, as well as the size of the systematic errors, were determined on the basis of the tests with mock catalogs and the blinded data catalogs.","We present several improvements to the BAO analysis pipeline, including enhancing the BAO fitting and reconstruction methods in a more physically-motivated direction, and also present results using combinations of tracers.","We present a re-analysis of SDSS BOSS and eBOSS results applying the improved DESI methodology and find scatter consistent with the level of the quoted SDSS theoretical systematic uncertainties.","With the total effective survey volume of ~ 18","Gpc$^3$, the combined precision of the BAO measurements across the six different redshift bins is ~0.52%, marking a 1.2-fold improvement over the previous state-of-the-art results using only first-year data.","We detect the BAO in all of these six redshift bins.","The highest significance of BAO detection is $9.1\\sigma$ at the effective redshift of 0.93, with a constraint of 0.86% placed on the BAO scale.","We find our measurements are systematically larger than the prediction of Planck-2018 LCDM model at z<0.8.","We translate the results into transverse comoving distance and radial Hubble distance measurements, which are used to constrain cosmological models in our companion paper [abridged]."],"url":"http://arxiv.org/abs/2404.03000v1","category":"astro-ph.CO"}
{"created":"2024-04-03 18:38:39","title":"The off-shell expansion relation of the Yang-Mills scalar theory","abstract":"In this work, we investigate the off-shell expansion relation of the Yang-Mills scalar theory. We have showed explicitly that the single-trace Berends-Giele current in YMS theory can be decomposed into a term expressed by a linear combination of bi-adjoint scalar Berends-Giele currents and one that vanishes under the on-shell limit. We proved that the bi-adjoint scalar currents, as well as the corresponding coefficients, can be characterized by a graphic approach that was originally studied in Einstein-Yang-Mills expansion. Furthermore, we generalized the decomposition to the multi-trace case through unifying relations and established the connection both in single-trace and multi-trace graphic descriptions. Finally, we established the relations between the Yang-Mills currents and the single-trace Yang-Mills scalar currents choosing special reference orders of the Yang-Mills graphs.","sentences":["In this work, we investigate the off-shell expansion relation of the Yang-Mills scalar theory.","We have showed explicitly that the single-trace Berends-Giele current in YMS theory can be decomposed into a term expressed by a linear combination of bi-adjoint scalar Berends-Giele currents and one that vanishes under the on-shell limit.","We proved that the bi-adjoint scalar currents, as well as the corresponding coefficients, can be characterized by a graphic approach that was originally studied in Einstein-Yang-Mills expansion.","Furthermore, we generalized the decomposition to the multi-trace case through unifying relations and established the connection both in single-trace and multi-trace graphic descriptions.","Finally, we established the relations between the Yang-Mills currents and the single-trace Yang-Mills scalar currents choosing special reference orders of the Yang-Mills graphs."],"url":"http://arxiv.org/abs/2404.02997v1","category":"hep-th"}
{"created":"2024-04-03 18:37:51","title":"Tricks from the Trade for Large-Scale Markdown Pricing: Heuristic Cut Generation for Lagrangian Decomposition","abstract":"In automated decision making processes in the online fashion industry, the 'predict-then-optimize' paradigm is frequently applied, particularly for markdown pricing strategies. This typically involves a mixed-integer optimization step, which is crucial for maximizing profit and merchandise volume. In practice, the size and complexity of the optimization problem is prohibitive for using off-the-shelf solvers for mixed integer programs and specifically tailored approaches are a necessity. Our paper introduces specific heuristics designed to work alongside decomposition methods, leading to almost-optimal solutions. These heuristics, which include both primal heuristic methods and a cutting plane generation technique within a Lagrangian decomposition framework, are the core focus of the present paper. We provide empirical evidence for their effectiveness, drawing on real-world applications at Zalando SE, one of Europe's leading online fashion retailers, highlighting the practical value of our work. The contributions of this paper are deeply ingrained into Zalando's production environment to its large-scale catalog ranging in the millions of products and improving weekly profits by millions of Euros.","sentences":["In automated decision making processes in the online fashion industry, the 'predict-then-optimize' paradigm is frequently applied, particularly for markdown pricing strategies.","This typically involves a mixed-integer optimization step, which is crucial for maximizing profit and merchandise volume.","In practice, the size and complexity of the optimization problem is prohibitive for using off-the-shelf solvers for mixed integer programs and specifically tailored approaches are a necessity.","Our paper introduces specific heuristics designed to work alongside decomposition methods, leading to almost-optimal solutions.","These heuristics, which include both primal heuristic methods and a cutting plane generation technique within a Lagrangian decomposition framework, are the core focus of the present paper.","We provide empirical evidence for their effectiveness, drawing on real-world applications at Zalando SE, one of Europe's leading online fashion retailers, highlighting the practical value of our work.","The contributions of this paper are deeply ingrained into Zalando's production environment to its large-scale catalog ranging in the millions of products and improving weekly profits by millions of Euros."],"url":"http://arxiv.org/abs/2404.02996v1","category":"math.OC"}
{"created":"2024-04-03 18:23:41","title":"Topological Data Analysis for Particulate Gels","abstract":"Soft gels, formed via the self-assembly of particulate organic materials, exhibit intricate multi-scale structures that provides them with flexibility and resilience when subjected to external stresses. This work combines molecular simulations and topological data analysis (TDA) to characterize the complex multi-scale structure of soft gels. Our TDA analysis focuses on the use of the Euler characteristic, which is an interpretable and computationally-scalable topological descriptor that is combined with filtration operations to obtain information on the geometric (local) and topological (global) structure of soft gels. We reduce the topological information obtained with TDA using principal component analysis (PCA) and show that this provides an informative low-dimensional representation of gel structure. We use the proposed computational framework to investigate the influence of gel preparation (e.g., quench rate, volume fraction) on soft gel structure and to explore dynamic deformations that emerge under oscillatory shear in various response regimes (linear, nonlinear, and flow). Our analysis identifies specific scales and extents at which hierarchical structures in soft gels are affected; moreover, correlations between structural deformations and mechanical phenomena (such as shear stiffening) are explored. In summary, we show that TDA facilitates the mathematical representation, quantification, and analysis of soft gel structures, extending traditional network analysis methods to capture both local and global organization.","sentences":["Soft gels, formed via the self-assembly of particulate organic materials, exhibit intricate multi-scale structures that provides them with flexibility and resilience when subjected to external stresses.","This work combines molecular simulations and topological data analysis (TDA) to characterize the complex multi-scale structure of soft gels.","Our TDA analysis focuses on the use of the Euler characteristic, which is an interpretable and computationally-scalable topological descriptor that is combined with filtration operations to obtain information on the geometric (local) and topological (global) structure of soft gels.","We reduce the topological information obtained with TDA using principal component analysis (PCA) and show that this provides an informative low-dimensional representation of gel structure.","We use the proposed computational framework to investigate the influence of gel preparation (e.g., quench rate, volume fraction) on soft gel structure and to explore dynamic deformations that emerge under oscillatory shear in various response regimes (linear, nonlinear, and flow).","Our analysis identifies specific scales and extents at which hierarchical structures in soft gels are affected; moreover, correlations between structural deformations and mechanical phenomena (such as shear stiffening) are explored.","In summary, we show that TDA facilitates the mathematical representation, quantification, and analysis of soft gel structures, extending traditional network analysis methods to capture both local and global organization."],"url":"http://arxiv.org/abs/2404.02991v1","category":"cond-mat.soft"}
{"created":"2024-04-03 18:16:47","title":"Risk-averse Learning with Non-Stationary Distributions","abstract":"Considering non-stationary environments in online optimization enables decision-maker to effectively adapt to changes and improve its performance over time. In such cases, it is favorable to adopt a strategy that minimizes the negative impact of change to avoid potentially risky situations. In this paper, we investigate risk-averse online optimization where the distribution of the random cost changes over time. We minimize risk-averse objective function using the Conditional Value at Risk (CVaR) as risk measure. Due to the difficulty in obtaining the exact CVaR gradient, we employ a zeroth-order optimization approach that queries the cost function values multiple times at each iteration and estimates the CVaR gradient using the sampled values. To facilitate the regret analysis, we use a variation metric based on Wasserstein distance to capture time-varying distributions. Given that the distribution variation is sub-linear in the total number of episodes, we show that our designed learning algorithm achieves sub-linear dynamic regret with high probability for both convex and strongly convex functions. Moreover, theoretical results suggest that increasing the number of samples leads to a reduction in the dynamic regret bounds until the sampling number reaches a specific limit. Finally, we provide numerical experiments of dynamic pricing in a parking lot to illustrate the efficacy of the designed algorithm.","sentences":["Considering non-stationary environments in online optimization enables decision-maker to effectively adapt to changes and improve its performance over time.","In such cases, it is favorable to adopt a strategy that minimizes the negative impact of change to avoid potentially risky situations.","In this paper, we investigate risk-averse online optimization where the distribution of the random cost changes over time.","We minimize risk-averse objective function using the Conditional Value at Risk (CVaR) as risk measure.","Due to the difficulty in obtaining the exact CVaR gradient, we employ a zeroth-order optimization approach that queries the cost function values multiple times at each iteration and estimates the CVaR gradient using the sampled values.","To facilitate the regret analysis, we use a variation metric based on Wasserstein distance to capture time-varying distributions.","Given that the distribution variation is sub-linear in the total number of episodes, we show that our designed learning algorithm achieves sub-linear dynamic regret with high probability for both convex and strongly convex functions.","Moreover, theoretical results suggest that increasing the number of samples leads to a reduction in the dynamic regret bounds until the sampling number reaches a specific limit.","Finally, we provide numerical experiments of dynamic pricing in a parking lot to illustrate the efficacy of the designed algorithm."],"url":"http://arxiv.org/abs/2404.02988v1","category":"eess.SY"}
{"created":"2024-04-03 18:14:23","title":"Universal Functional Regression with Neural Operator Flows","abstract":"Regression on function spaces is typically limited to models with Gaussian process priors. We introduce the notion of universal functional regression, in which we aim to learn a prior distribution over non-Gaussian function spaces that remains mathematically tractable for functional regression. To do this, we develop Neural Operator Flows (OpFlow), an infinite-dimensional extension of normalizing flows. OpFlow is an invertible operator that maps the (potentially unknown) data function space into a Gaussian process, allowing for exact likelihood estimation of functional point evaluations. OpFlow enables robust and accurate uncertainty quantification via drawing posterior samples of the Gaussian process and subsequently mapping them into the data function space. We empirically study the performance of OpFlow on regression and generation tasks with data generated from Gaussian processes with known posterior forms and non-Gaussian processes, as well as real-world earthquake seismograms with an unknown closed-form distribution.","sentences":["Regression on function spaces is typically limited to models with Gaussian process priors.","We introduce the notion of universal functional regression, in which we aim to learn a prior distribution over non-Gaussian function spaces that remains mathematically tractable for functional regression.","To do this, we develop Neural Operator Flows (OpFlow), an infinite-dimensional extension of normalizing flows.","OpFlow is an invertible operator that maps the (potentially unknown) data function space into a Gaussian process, allowing for exact likelihood estimation of functional point evaluations.","OpFlow enables robust and accurate uncertainty quantification via drawing posterior samples of the Gaussian process and subsequently mapping them into the data function space.","We empirically study the performance of OpFlow on regression and generation tasks with data generated from Gaussian processes with known posterior forms and non-Gaussian processes, as well as real-world earthquake seismograms with an unknown closed-form distribution."],"url":"http://arxiv.org/abs/2404.02986v1","category":"cs.LG"}
{"created":"2024-04-03 18:14:11","title":"R-matrix with time-dependence calculations for three-sideband RABBITT in helium","abstract":"Following up on a recent paper [Bharti et al., Phys. Rev. A 109 (2024) 023110], we compare the predictions from severalR-matrix with time-dependence calculations for a modified three-sideband version of the \"reconstruction of attosecond beating by interference of two-photon transitions\" (RABBITT) configuration applied to helium. Except for the special case of the threshold sideband, which appears to be very sensitive to the details of coupling to the bound Rydberg states, increasing the number of coupled states in the close-coupling expansion used to describe the ejected-electron--residual-ion interaction hardly changes the results. Consequently, the remaining discrepancies between the experimental data and the theoretical predictions are likely due to uncertainties in the experimental parameters, particularly the detailed knowledge of the laser pulse.","sentences":["Following up on a recent paper [Bharti et al., Phys.","Rev.","A 109 (2024) 023110], we compare the predictions from severalR-matrix with time-dependence calculations for a modified three-sideband version of the \"reconstruction of attosecond beating by interference of two-photon transitions\" (RABBITT) configuration applied to helium.","Except for the special case of the threshold sideband, which appears to be very sensitive to the details of coupling to the bound Rydberg states, increasing the number of coupled states in the close-coupling expansion used to describe the ejected-electron--residual-ion interaction hardly changes the results.","Consequently, the remaining discrepancies between the experimental data and the theoretical predictions are likely due to uncertainties in the experimental parameters, particularly the detailed knowledge of the laser pulse."],"url":"http://arxiv.org/abs/2404.02985v1","category":"physics.atom-ph"}
{"created":"2024-04-03 18:07:02","title":"Spatio-temporal Modeling of Count Data","abstract":"We introduce parsimonious parameterisations for multivariate autoregressive count time series models for spatio-temporal data, including possible regressions on covariates. The number of parameters is reduced by specifying spatial neighbourhood structures for possibly huge matrices that take into account spatio-temporal dependencies. Consistency and asymptotic normality of the parameter estimators are obtained under mild assumptions by employing quasi-maximum likelihood methodology. This is used to obtain an asymptotic Wald test for testing the significance of individual or group effects. Several simulations and two data examples support and illustrate the methods proposed in this paper.","sentences":["We introduce parsimonious parameterisations for multivariate autoregressive count time series models for spatio-temporal data, including possible regressions on covariates.","The number of parameters is reduced by specifying spatial neighbourhood structures for possibly huge matrices that take into account spatio-temporal dependencies.","Consistency and asymptotic normality of the parameter estimators are obtained under mild assumptions by employing quasi-maximum likelihood methodology.","This is used to obtain an asymptotic Wald test for testing the significance of individual or group effects.","Several simulations and two data examples support and illustrate the methods proposed in this paper."],"url":"http://arxiv.org/abs/2404.02982v1","category":"stat.ME"}
{"created":"2024-04-03 18:03:59","title":"Quasi-Random Frequency Sampling for Optical Turbulence Simulations","abstract":"Optical turbulence modelling and simulation are crucial for developing astronomical ground-based instruments, laser communication, laser metrology, or any application where light propagates through a turbulent medium. In the context of spectrum-based optical turbulence Monte-Carlo simulations, we present an alternative approach to the methods based on the Fast Fourier Transform (FFT) using a quasi-random frequency sampling heuristic. This approach provides complete control over the spectral information expressed in the simulated measurable, without the drawbacks encountered with FFT-based methods such as high-frequency aliasing, low-frequency under-sampling, and static sampling statistics. The method's heuristics, implementation, and an application example from the study of differential piston fluctuations are discussed.","sentences":["Optical turbulence modelling and simulation are crucial for developing astronomical ground-based instruments, laser communication, laser metrology, or any application where light propagates through a turbulent medium.","In the context of spectrum-based optical turbulence Monte-Carlo simulations, we present an alternative approach to the methods based on the Fast Fourier Transform (FFT) using a quasi-random frequency sampling heuristic.","This approach provides complete control over the spectral information expressed in the simulated measurable, without the drawbacks encountered with FFT-based methods such as high-frequency aliasing, low-frequency under-sampling, and static sampling statistics.","The method's heuristics, implementation, and an application example from the study of differential piston fluctuations are discussed."],"url":"http://arxiv.org/abs/2404.02978v1","category":"astro-ph.IM"}
{"created":"2024-04-03 18:00:48","title":"NGTS-30 b/TOI-4862 b: An 1 Gyr old 98-day transiting warm Jupiter","abstract":"Long-period transiting exoplanets bridge the gap between the bulk of transit- and Doppler-based exoplanet discoveries, providing key insights into the formation and evolution of planetary systems. The wider separation between these planets and their host stars results in the exoplanets typically experiencing less radiation from their host stars; hence, they should maintain more of their original atmospheres, which can be probed during transit via transmission spectroscopy. Although the known population of long-period transiting exoplanets is relatively sparse, surveys performed by the Transiting Exoplanet Survey Satellite (TESS) and the Next Generation Transit Survey (NGTS) are now discovering new exoplanets to fill in this crucial region of the exoplanetary parameter space. This study presents the detection and characterisation of NGTS-30 b/TOI-4862 b, a new long-period transiting exoplanet detected by following up on a single-transit candidate found in the TESS mission. Through monitoring using a combination of photometric instruments (TESS, NGTS, and EulerCam) and spectroscopic instruments (CORALIE, FEROS, HARPS, and PFS), NGTS-30 b/TOI-4862 b was found to be a long-period (P = 98.29838 day) Jupiter-sized (0.928 RJ; 0.960 MJ) planet transiting a 1.1 Gyr old G-type star. With a moderate eccentricity of 0.294, its equilibrium temperature could be expected to vary from 274 K to 500 K over the course of its orbit. Through interior modelling, NGTS-30 b/TOI-4862 b was found to have a heavy element mass fraction of 0.23 and a heavy element enrichment (Zp/Z_star) of 20, making it metal-enriched compared to its host star. NGTS-30 b/TOI-4862 b is one of the youngest well-characterised long-period exoplanets found to date and will therefore be important in the quest to understanding the formation and evolution of exoplanets across the full range of orbital separations and ages.","sentences":["Long-period transiting exoplanets bridge the gap between the bulk of transit- and Doppler-based exoplanet discoveries, providing key insights into the formation and evolution of planetary systems.","The wider separation between these planets and their host stars results in the exoplanets typically experiencing less radiation from their host stars; hence, they should maintain more of their original atmospheres, which can be probed during transit via transmission spectroscopy.","Although the known population of long-period transiting exoplanets is relatively sparse, surveys performed by the Transiting Exoplanet Survey Satellite (TESS) and the Next Generation Transit Survey (NGTS) are now discovering new exoplanets to fill in this crucial region of the exoplanetary parameter space.","This study presents the detection and characterisation of NGTS-30 b/TOI-4862 b, a new long-period transiting exoplanet detected by following up on a single-transit candidate found in the TESS mission.","Through monitoring using a combination of photometric instruments (TESS, NGTS, and EulerCam) and spectroscopic instruments (CORALIE, FEROS, HARPS, and PFS), NGTS-30 b/TOI-4862 b was found to be a long-period (P = 98.29838 day)","Jupiter-sized (0.928 RJ; 0.960 MJ) planet transiting a 1.1 Gyr old G-type star.","With a moderate eccentricity of 0.294, its equilibrium temperature could be expected to vary from 274 K to 500 K over the course of its orbit.","Through interior modelling, NGTS-30 b/TOI-4862 b was found to have a heavy element mass fraction of 0.23 and a heavy element enrichment (Zp/Z_star) of 20, making it metal-enriched compared to its host star.","NGTS-30 b/TOI-4862 b is one of the youngest well-characterised long-period exoplanets found to date and will therefore be important in the quest to understanding the formation and evolution of exoplanets across the full range of orbital separations and ages."],"url":"http://arxiv.org/abs/2404.02974v1","category":"astro-ph.EP"}
{"created":"2024-04-03 18:00:09","title":"A novel strategy to prove chiral symmetry breaking in QCD-like theories","abstract":"We demonstrate that chiral symmetry breaking occurs in the confining phase of QCD-like theories with $N_c$ colors and $N_f$ flavors. Our proof is based on a novel strategy, called `downlifting', by which solutions of the 't Hooft anomaly matching and persistent mass conditions for a theory with $N_f-1$ flavors are constructed from those of a theory with $N_f$ flavors, while $N_c$ is fixed. By induction, chiral symmetry breaking is proven for any $N_f\\geq p_{min}$, where $p_{min}$ is the smallest prime factor of $N_c$. The proof can be extended to $N_f <p_{min}$ under the additional assumption on the absence of phase transitions when quark masses are sent to infinity. Our results do not rely on ad-hoc assumptions on the spectrum of massless bound states.","sentences":["We demonstrate that chiral symmetry breaking occurs in the confining phase of QCD-like theories with $N_c$ colors and $N_f$ flavors.","Our proof is based on a novel strategy, called `downlifting', by which solutions of the 't Hooft anomaly matching and persistent mass conditions for a theory with $N_f-1$ flavors are constructed from those of a theory with $N_f$ flavors, while $N_c$ is fixed.","By induction, chiral symmetry breaking is proven for any $N_f\\geq p_{min}$, where $p_{min}$ is the smallest prime factor of $N_c$. The proof can be extended to $N_f <p_{min}$ under the additional assumption on the absence of phase transitions when quark masses are sent to infinity.","Our results do not rely on ad-hoc assumptions on the spectrum of massless bound states."],"url":"http://arxiv.org/abs/2404.02967v1","category":"hep-th"}
{"created":"2024-04-03 18:00:04","title":"Hamiltonian Simulation in the Interaction Picture Using the Magnus Expansion","abstract":"We propose an algorithm for simulating the dynamics of a geometrically local Hamiltonian $A$ under a small geometrically local perturbation $\\alpha B$. In certain regimes, the algorithm achieves the optimal scaling and outperforms the state-of-the-art algorithms. By moving into the interaction frame of $A$ and classically computing the Magnus expansion of the interaction-picture Hamiltonian, our algorithm bypasses the need for ancillary qubits. In analyzing its performance, we develop a framework to capture the quasi-locality of the Magnus operators, leading to a tightened bound for the error of the Magnus truncation. The Lieb-Robinson bound also guarantees the efficiency of computing the Magnus operators and of their subsequent decomposition into elementary quantum gates. These features make our algorithm appealing for near-term and early-fault-tolerant simulations.","sentences":["We propose an algorithm for simulating the dynamics of a geometrically local Hamiltonian $A$ under a small geometrically local perturbation $\\alpha B$. In certain regimes, the algorithm achieves the optimal scaling and outperforms the state-of-the-art algorithms.","By moving into the interaction frame of $A$ and classically computing the Magnus expansion of the interaction-picture Hamiltonian, our algorithm bypasses the need for ancillary qubits.","In analyzing its performance, we develop a framework to capture the quasi-locality of the Magnus operators, leading to a tightened bound for the error of the Magnus truncation.","The Lieb-Robinson bound also guarantees the efficiency of computing the Magnus operators and of their subsequent decomposition into elementary quantum gates.","These features make our algorithm appealing for near-term and early-fault-tolerant simulations."],"url":"http://arxiv.org/abs/2404.02966v1","category":"quant-ph"}
{"created":"2024-04-03 18:00:02","title":"A first determination of the strong coupling $\u03b1_S$ at approximate N$^{3}$LO order in a global PDF fit","abstract":"We present the first determination of the value of the strong coupling via a simultaneous global fit of the proton parton distribution functions (PDFs) at approximate N$^{3}$LO (aN$^{3}$LO) order in QCD. This makes use of the MSHT global PDF fitting framework, and in particular the recent theoretical advances that allow a PDF fit to now be performed at this order. The value of the strong coupling is found to be $\\alpha_S(M_Z^2)$(aN$^{3}$LO)$ = 0.1170 \\pm 0.0016$. This is in excellent agreement with the NNLO value of $\\alpha_S(M_Z^2)$(NNLO) $= 0.1171 \\pm 0.0014$, indicating that good perturbative convergence has been found. The resulting uncertainties, calculated using the MSHT dynamic tolerance procedure, are somewhat larger, but more accurate, at aN$^{3}$LO, due to the missing higher order theoretical uncertainties that are included at this order, but not at NNLO. We in addition present a detailed breakdown of the individual dataset sensitivity to the value of the strong coupling, with special focus on the impact of fitting dijet rather than inclusive jet data. This choice is found to have a non-negligible impact, but with overall good consistency found, especially at aN$^{3}$LO.","sentences":["We present the first determination of the value of the strong coupling via a simultaneous global fit of the proton parton distribution functions (PDFs) at approximate N$^{3}$LO (aN$^{3}$LO) order in QCD.","This makes use of the MSHT global PDF fitting framework, and in particular the recent theoretical advances that allow a PDF fit to now be performed at this order.","The value of the strong coupling is found to be $\\alpha_S(M_Z^2)$(aN$^{3}$LO)$ = 0.1170 \\pm 0.0016$.","This is in excellent agreement with the NNLO value of $\\alpha_S(M_Z^2)$(NNLO) $= 0.1171 \\pm 0.0014$, indicating that good perturbative convergence has been found.","The resulting uncertainties, calculated using the MSHT dynamic tolerance procedure, are somewhat larger, but more accurate, at aN$^{3}$LO, due to the missing higher order theoretical uncertainties that are included at this order, but not at NNLO.","We in addition present a detailed breakdown of the individual dataset sensitivity to the value of the strong coupling, with special focus on the impact of fitting dijet rather than inclusive jet data.","This choice is found to have a non-negligible impact, but with overall good consistency found, especially at aN$^{3}$LO."],"url":"http://arxiv.org/abs/2404.02964v1","category":"hep-ph"}
{"created":"2024-04-03 18:00:01","title":"Probing the eccentricity in protostellar discs -- Modeling kinematics and morphologies","abstract":"Protostellar discs are mostly modelled as circular structures of gas and dust orbiting a protostar. However, a number of physical mechanisms, e.g. the presence of a (sub)stellar companion or initial axial asymmetry, can cause the gas and dust orbital motion to become eccentric. Theoretical studies have revealed that, when present, disc eccentricity is expected to occur with predictable profiles that can be long-lasting and potentially observable in protostellar systems. We construct an analytical model predicting the typical features of the kinematics and morphology of eccentric protostellar discs, with the final goal of characterising the observational appearance of eccentricity in discs. We validate the model using a numerical simulation of a circumbinary disc (where the binary makes the disc eccentric). We finally post-process the simulation with Monte Carlo Radiative Transfer to study how eccentric features would appear through the \"eyes\" of ALMA. Besides the motion of the material on eccentric Keplerian orbits in the disc orbital plane, the most characteristic eccentric feature emerging from the analytical model is strong vertical motion with a typical anti-symmetric pattern (with respect to the disc line of pericentres). A circumbinary disc with a $\\approx 40$ au eccentric cavity ($e_{\\rm cav}=0.2$), carved by an $a_{\\rm bin}=15$ au binary, placed at a distance $d=130$ pc, is expected to host in its upper emission surface vertical oscillations up to $v_{z}\\sim 400\\, {\\rm ms}^{-1}$ close to the cavity edge, i.e. well within ALMA spectral and spatial resolution capabilities. A residual spiral pattern in the vertical velocity $\\Delta v_{z}\\sim 150\\, {\\rm ms}^{-1}$ of the simulation cannot be captured by the theoretical model, we speculate it to be possibly linked to the presence of a companion in the system.","sentences":["Protostellar discs are mostly modelled as circular structures of gas and dust orbiting a protostar.","However, a number of physical mechanisms, e.g. the presence of a (sub)stellar companion or initial axial asymmetry, can cause the gas and dust orbital motion to become eccentric.","Theoretical studies have revealed that, when present, disc eccentricity is expected to occur with predictable profiles that can be long-lasting and potentially observable in protostellar systems.","We construct an analytical model predicting the typical features of the kinematics and morphology of eccentric protostellar discs, with the final goal of characterising the observational appearance of eccentricity in discs.","We validate the model using a numerical simulation of a circumbinary disc (where the binary makes the disc eccentric).","We finally post-process the simulation with Monte Carlo Radiative Transfer to study how eccentric features would appear through the \"eyes\" of ALMA.","Besides the motion of the material on eccentric Keplerian orbits in the disc orbital plane, the most characteristic eccentric feature emerging from the analytical model is strong vertical motion with a typical anti-symmetric pattern (with respect to the disc line of pericentres).","A circumbinary disc with a $\\approx 40$ au eccentric cavity ($e_{\\rm cav}=0.2$), carved by an $a_{\\rm bin}=15$ au binary, placed at a distance $d=130$ pc, is expected to host in its upper emission surface vertical oscillations up to $v_{z}\\sim 400\\, {\\rm ms}^{-1}$ close to the cavity edge, i.e. well within ALMA spectral and spatial resolution capabilities.","A residual spiral pattern in the vertical velocity $\\Delta v_{z}\\sim 150\\, {\\rm ms}^{-1}$ of the simulation cannot be captured by the theoretical model, we speculate it to be possibly linked to the presence of a companion in the system."],"url":"http://arxiv.org/abs/2404.02958v1","category":"astro-ph.EP"}
{"created":"2024-04-03 18:00:00","title":"Sledgehamr: Simulating Scalar Fields with Adaptive Mesh Refinement","abstract":"Understanding the non-linear dynamics of coupled scalar fields often necessitates simulations on a three-dimensional mesh. These simulations can be computationally expensive if a large scale separation is involved. A common solution is adaptive mesh refinement which, however, greatly increases a simulation's complexity. In this work, we present sledgehamr, an AMReX-based code package to make the simulation of coupled scalar fields on an adaptive mesh more accessible. Compatible with both GPU and CPU clusters, sledgehamr offers a flexible and customizable framework. While the code had been primarily developed to evolve axion string networks, this framework enables various other applications, such as the study of gravitational waves sourced by the dynamics of scalar fields.","sentences":["Understanding the non-linear dynamics of coupled scalar fields often necessitates simulations on a three-dimensional mesh.","These simulations can be computationally expensive if a large scale separation is involved.","A common solution is adaptive mesh refinement which, however, greatly increases a simulation's complexity.","In this work, we present sledgehamr, an AMReX-based code package to make the simulation of coupled scalar fields on an adaptive mesh more accessible.","Compatible with both GPU and CPU clusters, sledgehamr offers a flexible and customizable framework.","While the code had been primarily developed to evolve axion string networks, this framework enables various other applications, such as the study of gravitational waves sourced by the dynamics of scalar fields."],"url":"http://arxiv.org/abs/2404.02950v1","category":"hep-ph"}
{"created":"2024-04-03 18:00:00","title":"Inferring dark matter subhalo properties from simulated subhalo-stream encounters","abstract":"In the cold dark matter paradigm, our Galaxy is predicted to contain >10000 dark matter subhaloes in the $10^5-10^8M_\\odot$ range which should be completely devoid of stars. Stellar streams are sensitive to the presence of these subhaloes, which can create small-scale features in streams if they pass closely enough. Modelling these encounters can therefore, potentially recover the subhalo's properties. In this work, we demonstrate this for streams generated in numerical simulations, modelled on eccentric orbits in a realistic Milky Way potential, which includes the Large Magellanic Cloud and the subhalo itself. We focus on a mock model of the ATLAS-Aliqa Uma stream and inject a $10^7 M_\\odot$ subhalo, creating a similar discontinuous morphology to current observations. We then explore how well subhalo properties are recovered using mock stream observations, consisting of no observational errors, as well as assuming realistic observational setups. These setups include present day style observations, and what will be possible with 4MOST and Gaia DR5 in the future. We show that we can recover all parameters describing the impact even with uncertainties matching existing data, including subhalo positions, velocities, mass and scale radius. Modelling the subhalo on an orbit instead of assuming an impulse approximation, we greatly reduce the degeneracy between subhalo mass and velocity seen in previous works. However, we find a slight bias in the subhalo mass (~0.1 dex). This demonstrates that we should be able to reliably extract the properties of subhaloes with stellar streams in the near future.","sentences":["In the cold dark matter paradigm, our Galaxy is predicted to contain >10000 dark matter subhaloes in the $10^5-10^8M_\\odot$ range which should be completely devoid of stars.","Stellar streams are sensitive to the presence of these subhaloes, which can create small-scale features in streams if they pass closely enough.","Modelling these encounters can therefore, potentially recover the subhalo's properties.","In this work, we demonstrate this for streams generated in numerical simulations, modelled on eccentric orbits in a realistic Milky Way potential, which includes the Large Magellanic Cloud and the subhalo itself.","We focus on a mock model of the ATLAS-Aliqa Uma stream and inject a $10^7 M_\\odot$ subhalo, creating a similar discontinuous morphology to current observations.","We then explore how well subhalo properties are recovered using mock stream observations, consisting of no observational errors, as well as assuming realistic observational setups.","These setups include present day style observations, and what will be possible with 4MOST and Gaia DR5 in the future.","We show that we can recover all parameters describing the impact even with uncertainties matching existing data, including subhalo positions, velocities, mass and scale radius.","Modelling the subhalo on an orbit instead of assuming an impulse approximation, we greatly reduce the degeneracy between subhalo mass and velocity seen in previous works.","However, we find a slight bias in the subhalo mass (~0.1 dex).","This demonstrates that we should be able to reliably extract the properties of subhaloes with stellar streams in the near future."],"url":"http://arxiv.org/abs/2404.02953v1","category":"astro-ph.GA"}
{"created":"2024-04-03 17:58:29","title":"The Lavrentiev phenomenon","abstract":"The basic problem of the calculus of variations consists of finding a function that minimizes an energy, like finding the fastest trajectory between two points for a point mass in a gravity field moving without friction under the influence of gravity or finding the best shape of a wing. The existence of a solution may be established in quite abstract spaces, much larger than the space of smooth functions. An important practical problem is that of being able to approach the value of the infimum of the energy. However, numerical methods work with very concrete functions and sometimes they are unable to approximate the infimum: this is the surprising Lavrentiev phenomenon. The papers that ensure the non-occurrence of the phenomenon form a recent saga, and the most general result formulated in the early '90s was actually fully proved just recently, more than 30 years later. Our aim here is to introduce the reader to the calculus of variations, to illustrate the Lavrentiev phenomenon with the simplest known example, and to give an elementary proof of the non-occurrence of the phenomenon.","sentences":["The basic problem of the calculus of variations consists of finding a function that minimizes an energy, like finding the fastest trajectory between two points for a point mass in a gravity field moving without friction under the influence of gravity or finding the best shape of a wing.","The existence of a solution may be established in quite abstract spaces, much larger than the space of smooth functions.","An important practical problem is that of being able to approach the value of the infimum of the energy.","However, numerical methods work with very concrete functions and sometimes they are unable to approximate the infimum: this is the surprising Lavrentiev phenomenon.","The papers that ensure the non-occurrence of the phenomenon form a recent saga, and the most general result formulated in the early '90s was actually fully proved just recently, more than 30 years later.","Our aim here is to introduce the reader to the calculus of variations, to illustrate the Lavrentiev phenomenon with the simplest known example, and to give an elementary proof of the non-occurrence of the phenomenon."],"url":"http://arxiv.org/abs/2404.02901v1","category":"math.OC"}
{"created":"2024-04-03 17:51:20","title":"Automated Transparency: A Legal and Empirical Analysis of the Digital Services Act Transparency Database","abstract":"The Digital Services Act (DSA) is a much awaited platforms liability reform in the European Union that was adopted on 1 November 2022 with the ambition to set a global example in terms of accountability and transparency. Among other obligations, the DSA emphasizes the need for online platforms to report on their content moderation decisions (`statements of reasons' - SoRs), which is a novel transparency mechanism we refer to as automated transparency in this study. SoRs are currently made available in the DSA Transparency Database, launched by the European Commission in September 2023. The DSA Transparency Database marks a historical achievement in platform governance, and allows investigations about the actual transparency gains, both at structure level as well as at the level of platform compliance. This study aims to understand whether the Transparency Database helps the DSA to live up to its transparency promises. We use legal and empirical arguments to show that while there are some transparency gains, compliance remains problematic, as the current database structure allows for a lot of discretion from platforms in terms of transparency practices. In our empirical study, we analyze a representative sample of the Transparency Database (131m SoRs) submitted in November 2023, to characterise and evaluate platform content moderation practices.","sentences":["The Digital Services Act (DSA) is a much awaited platforms liability reform in the European Union that was adopted on 1 November 2022 with the ambition to set a global example in terms of accountability and transparency.","Among other obligations, the DSA emphasizes the need for online platforms to report on their content moderation decisions (`statements of reasons' - SoRs), which is a novel transparency mechanism we refer to as automated transparency in this study.","SoRs are currently made available in the DSA Transparency Database, launched by the European Commission in September 2023.","The DSA Transparency Database marks a historical achievement in platform governance, and allows investigations about the actual transparency gains, both at structure level as well as at the level of platform compliance.","This study aims to understand whether the Transparency Database helps the DSA to live up to its transparency promises.","We use legal and empirical arguments to show that while there are some transparency gains, compliance remains problematic, as the current database structure allows for a lot of discretion from platforms in terms of transparency practices.","In our empirical study, we analyze a representative sample of the Transparency Database (131m SoRs) submitted in November 2023, to characterise and evaluate platform content moderation practices."],"url":"http://arxiv.org/abs/2404.02894v1","category":"cs.CY"}
{"created":"2024-04-03 17:33:21","title":"Linear Attention Sequence Parallelism","abstract":"Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with long sequences and large batches. We conduct extensive experiments on two linear attention-based models with varying sequence lengths and GPU cluster sizes. LASP scales sequence length up to 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than existing SP methods while being significantly faster. The code is available at https://github.com/OpenNLPLab/LASP.","sentences":["Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU.","However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models.","In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models.","Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP.","We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters.","Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with long sequences and large batches.","We conduct extensive experiments on two linear attention-based models with varying sequence lengths and GPU cluster sizes.","LASP scales sequence length up to 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than existing SP methods while being significantly faster.","The code is available at https://github.com/OpenNLPLab/LASP."],"url":"http://arxiv.org/abs/2404.02882v1","category":"cs.LG"}
{"created":"2024-04-03 17:32:52","title":"On computing approximate Lewis weights","abstract":"In this note we provide and analyze a simple method that given an $n \\times d$ matrix, outputs approximate $\\ell_p$-Lewis weights, a natural measure of the importance of the rows with respect to the $\\ell_p$ norm, for $p \\geq 2$. More precisely, we provide a simple post-processing procedure that turns natural one-sided approximate $\\ell_p$-Lewis weights into two-sided approximations. When combined with a simple one-sided approximation algorithm presented by Lee (PhD thesis, `16) this yields an algorithm for computing two-sided approximations of the $\\ell_p$-Lewis weights of an $n \\times d$-matrix using $\\mathrm{poly}(d,p)$ approximate leverage score computations. While efficient high-accuracy algorithms for approximating $\\ell_p$-Lewis had been established previously by Fazel, Lee, Padmanabhan and Sidford (SODA `22), the simple structure and approximation tolerance of our algorithm may make it of use for different applications.","sentences":["In this note we provide and analyze a simple method that given an $n \\times d$ matrix, outputs approximate $\\ell_p$-Lewis weights, a natural measure of the importance of the rows with respect to the $\\ell_p$ norm, for $p \\geq 2$.","More precisely, we provide a simple post-processing procedure that turns natural one-sided approximate $\\ell_p$-Lewis weights into two-sided approximations.","When combined with a simple one-sided approximation algorithm presented by Lee (PhD thesis, `16) this yields an algorithm for computing two-sided approximations of the $\\ell_p$-Lewis weights of an $n \\times d$-matrix using $\\mathrm{poly}(d,p)$ approximate leverage score computations.","While efficient high-accuracy algorithms for approximating $\\ell_p$-Lewis had been established previously by Fazel, Lee, Padmanabhan and Sidford (SODA `22), the simple structure and approximation tolerance of our algorithm may make it of use for different applications."],"url":"http://arxiv.org/abs/2404.02881v1","category":"cs.DS"}
{"created":"2024-04-03 17:32:50","title":"Fragmented Moments, Balanced Choices: How Do People Make Use of Their Waiting Time?","abstract":"Everyone spends some time waiting every day. HCI research has developed tools for boosting productivity while waiting. However, little is known about how people naturally spend their waiting time. We conducted an experience sampling study with 21 working adults who used a mobile app to report their daily waiting time activities over two weeks. The aim of this study is to understand the activities people do while waiting and the effect of situational factors. We found that participants spent about 60% of their waiting time on leisure activities, 20% on productive activities, and 20% on maintenance activities. These choices are sensitive to situational factors, including accessible device, location, and certain routines of the day. Our study complements previous ones by demonstrating that people purpose waiting time for various goals beyond productivity and to maintain work-life balance. Our findings shed light on future empirical research and system design for time management.","sentences":["Everyone spends some time waiting every day.","HCI research has developed tools for boosting productivity while waiting.","However, little is known about how people naturally spend their waiting time.","We conducted an experience sampling study with 21 working adults who used a mobile app to report their daily waiting time activities over two weeks.","The aim of this study is to understand the activities people do while waiting and the effect of situational factors.","We found that participants spent about 60% of their waiting time on leisure activities, 20% on productive activities, and 20% on maintenance activities.","These choices are sensitive to situational factors, including accessible device, location, and certain routines of the day.","Our study complements previous ones by demonstrating that people purpose waiting time for various goals beyond productivity and to maintain work-life balance.","Our findings shed light on future empirical research and system design for time management."],"url":"http://arxiv.org/abs/2404.02880v1","category":"cs.HC"}
{"created":"2024-04-03 17:21:59","title":"Sensing Resource Allocation Against Data-Poisoning Attacks in Traffic Routing","abstract":"Data-poisoning attacks can disrupt the efficient operations of transportation systems by misdirecting traffic flows via falsified data. One challenge in countering these attacks is to reduce the uncertainties on the types of attacks, such as the distribution of their targets and intensities. We introduce a resource allocation method in transportation networks to detect and distinguish different types of attacks and facilitate efficient traffic routing. The idea is to first cluster different types of attacks based on the corresponding optimal routing strategies, then allocate sensing resources to a subset of network links to distinguish attacks from different clusters via lexicographical mixed-integer programming. We illustrate the application of the proposed method using the Anaheim network, a benchmark model in traffic routing that contains more than 400 nodes and 900 links.","sentences":["Data-poisoning attacks can disrupt the efficient operations of transportation systems by misdirecting traffic flows via falsified data.","One challenge in countering these attacks is to reduce the uncertainties on the types of attacks, such as the distribution of their targets and intensities.","We introduce a resource allocation method in transportation networks to detect and distinguish different types of attacks and facilitate efficient traffic routing.","The idea is to first cluster different types of attacks based on the corresponding optimal routing strategies, then allocate sensing resources to a subset of network links to distinguish attacks from different clusters via lexicographical mixed-integer programming.","We illustrate the application of the proposed method using the Anaheim network, a benchmark model in traffic routing that contains more than 400 nodes and 900 links."],"url":"http://arxiv.org/abs/2404.02876v1","category":"math.OC"}
{"created":"2024-04-03 17:08:23","title":"A mean-field model of optimal investment","abstract":"We establish the existence and uniqueness of the equilibrium for a stochastic mean-field game of optimal investment. The analysis covers both finite and infinite time horizons, and the mean-field interaction of the representative company with a mass of identical and indistinguishable firms is modeled through the time-dependent price at which the produced good is sold. At equilibrium, this price is given in terms of a nonlinear function of the expected (optimally controlled) production capacity of the representative company at each time. The proof of the existence and uniqueness of the mean-field equilibrium relies on a priori estimates and the study of nonlinear integral equations, but employs different techniques for the finite and infinite horizon cases. Additionally, we investigate the deterministic counterpart of the mean-field game under study.","sentences":["We establish the existence and uniqueness of the equilibrium for a stochastic mean-field game of optimal investment.","The analysis covers both finite and infinite time horizons, and the mean-field interaction of the representative company with a mass of identical and indistinguishable firms is modeled through the time-dependent price at which the produced good is sold.","At equilibrium, this price is given in terms of a nonlinear function of the expected (optimally controlled) production capacity of the representative company at each time.","The proof of the existence and uniqueness of the mean-field equilibrium relies on a priori estimates and the study of nonlinear integral equations, but employs different techniques for the finite and infinite horizon cases.","Additionally, we investigate the deterministic counterpart of the mean-field game under study."],"url":"http://arxiv.org/abs/2404.02871v1","category":"math.OC"}
{"created":"2024-04-03 17:03:18","title":"UDON: A case for offloading to general purpose compute on CXL memory","abstract":"Upcoming CXL-based disaggregated memory devices feature special purpose units to offload compute to near-memory. In this paper, we explore opportunities for offloading compute to general purpose cores on CXL memory devices, thereby enabling a greater utility and diversity of offload.   We study two classes of popular memory intensive applications: ML inference and vector database as candidates for computational offload. The study uses Arm AArch64-based dual-socket NUMA systems to emulate CXL type-2 devices.   Our study shows promising results. With our ML inference model partitioning strategy for compute offload, we can place up to 90% data in remote memory with just 20% performance trade-off. Offloading Hierarchical Navigable Small World (HNSW) kernels in vector databases can provide upto 6.87$\\times$ performance improvement with under 10% offload overhead.","sentences":["Upcoming CXL-based disaggregated memory devices feature special purpose units to offload compute to near-memory.","In this paper, we explore opportunities for offloading compute to general purpose cores on CXL memory devices, thereby enabling a greater utility and diversity of offload.   ","We study two classes of popular memory intensive applications: ML inference and vector database as candidates for computational offload.","The study uses Arm AArch64-based dual-socket NUMA systems to emulate CXL type-2 devices.   ","Our study shows promising results.","With our ML inference model partitioning strategy for compute offload, we can place up to 90% data in remote memory with just 20% performance trade-off.","Offloading Hierarchical Navigable Small World (HNSW) kernels in vector databases can provide upto 6.87$\\times$ performance improvement with under 10% offload overhead."],"url":"http://arxiv.org/abs/2404.02868v1","category":"cs.ET"}
{"created":"2024-04-03 17:01:21","title":"Dark energy as a geometrical effect in geodetic brane gravity","abstract":"Within the framework of the modified geodetic brane gravity, conformed by the Regge-Teitelboim model and enhanced with a linear term in the extrinsic curvature of the brane, the possibility that under an FRW geometry this theory emulates the so-called dark energy is discussed. The cosmological behavior of this model displays a self-(non-self)-accelerated expansion of this universe which is caused by a combination of usual matter and gravitational geometric effects controlled by a $\\beta$ parameter that accompanies the correction $K$ term. Indeed, the self-accelerated branch, provided by the trace $K$ model raises the question of whether the extrinsic curvature correction terms might be suitable for dark energy candidates. We discuss the analytical expression obtained for $\\rd$ in addition to the main cosmological parameters such as the state parameter $\\omega_{\\text{\\tiny eff}}$ and the deceleration parameter $q$. Moreover, when we call for the contribution of dark radiation-like energy to be switched off, $\\Odr \\to 0$, we find the same acceleration behavior, as well as the same dark energy content provided by the DGP theory.","sentences":["Within the framework of the modified geodetic brane gravity, conformed by the Regge-Teitelboim model and enhanced with a linear term in the extrinsic curvature of the brane, the possibility that under an FRW geometry this theory emulates the so-called dark energy is discussed.","The cosmological behavior of this model displays a self-(non-self)-accelerated expansion of this universe which is caused by a combination of usual matter and gravitational geometric effects controlled by a $\\beta$ parameter that accompanies the correction $K$ term.","Indeed, the self-accelerated branch, provided by the trace $K$ model raises the question of whether the extrinsic curvature correction terms might be suitable for dark energy candidates.","We discuss the analytical expression obtained for $\\rd$ in addition to the main cosmological parameters such as the state parameter $\\omega_{\\text{\\tiny eff}}$ and the deceleration parameter $q$. Moreover, when we call for the contribution of dark radiation-like energy to be switched off, $\\Odr \\to 0$, we find the same acceleration behavior, as well as the same dark energy content provided by the DGP theory."],"url":"http://arxiv.org/abs/2404.02867v1","category":"gr-qc"}
{"created":"2024-04-03 16:57:16","title":"Discovery of universal phonon thermal Hall effect in crystals","abstract":"Thermal Hall effect (THE) in insulator is a remarkable phenomenon that arises from the motion of chargeless quasi-particles under a magnetic field. While magnons or exotic spin excitations were considered as the origin of THE in some magnetic materials, there are more and more evidences suggest that phonons play a significant role. However, the mechanism behind phonon THE is still unknown. Here we report the observation of THE, including planar THE, in a broad range of non-magnetic insulators and semiconductor: SrTiO$_3$, SiO$_2$ (quartz), MgO and Si. While the presence of antiferrodistortive domains in SrTiO$_3$ and chiral phonons in SiO$_2$ may complicate the interpretation of THE, the striking observations of THE in the trivial insulator MgO and high-purity intrinsic semiconductor Si demonstrate that phonon THE is a universal property of crystals. Without other effects on phonons such as from magnons, this universal phonon THE is characterized by a scaling law of |$\\kappa_{xy}$| $\\sim$ $\\kappa_{xx}^2$. Our results experimentally discover a fundamental physics of phonons in magnetic field, which must come from the direct coupling between atom vibrations and the field. Starting from this universal phonon THE in crystals, all previous interpretations of THE in magnetic or non-magnetic materials need to be reconsidered.","sentences":["Thermal Hall effect (THE) in insulator is a remarkable phenomenon that arises from the motion of chargeless quasi-particles under a magnetic field.","While magnons or exotic spin excitations were considered as the origin of THE in some magnetic materials, there are more and more evidences suggest that phonons play a significant role.","However, the mechanism behind phonon THE is still unknown.","Here we report the observation of THE, including planar THE, in a broad range of non-magnetic insulators and semiconductor:","SrTiO$_3$, SiO$_2$ (quartz), MgO and Si.","While the presence of antiferrodistortive domains in SrTiO$_3$ and chiral phonons in SiO$_2$ may complicate the interpretation of THE, the striking observations of THE in the trivial insulator MgO and high-purity intrinsic semiconductor Si demonstrate that phonon THE is a universal property of crystals.","Without other effects on phonons such as from magnons, this universal phonon THE is characterized by a scaling law of |$\\kappa_{xy}$| $\\sim$ $\\kappa_{xx}^2$. Our results experimentally discover a fundamental physics of phonons in magnetic field, which must come from the direct coupling between atom vibrations and the field.","Starting from this universal phonon THE in crystals, all previous interpretations of THE in magnetic or non-magnetic materials need to be reconsidered."],"url":"http://arxiv.org/abs/2404.02863v1","category":"cond-mat.str-el"}
{"created":"2024-04-03 16:53:32","title":"Pre-equilibrium Photon and Dilepton Production","abstract":"We use QCD kinetic theory to compute photon and dilepton production in the chemically equilibrating out-of-equilibrium quark-gluon plasma created in the early stages of high-energy heavy-ion collisions. We derive universal scaling functions for the pre-equilibrium spectra of photons and dileptons. These scaling functions can be used to make realistic predictions for the pre-equilibrium emission and consequently establish the significance of the pre-equilibrium phase for the production of electromagnetic probes in heavy-ion collisions.","sentences":["We use QCD kinetic theory to compute photon and dilepton production in the chemically equilibrating out-of-equilibrium quark-gluon plasma created in the early stages of high-energy heavy-ion collisions.","We derive universal scaling functions for the pre-equilibrium spectra of photons and dileptons.","These scaling functions can be used to make realistic predictions for the pre-equilibrium emission and consequently establish the significance of the pre-equilibrium phase for the production of electromagnetic probes in heavy-ion collisions."],"url":"http://arxiv.org/abs/2404.02861v1","category":"hep-ph"}
{"created":"2024-04-03 16:53:24","title":"Spin alignment of $K^\\ast$ induced by strange-baryon density inhomogeneity","abstract":"The difference between the spin alignments of $K^\\ast$ and those of $\\phi$ at the low collision energies is a puzzle raised by the recent experiments. Unlike $\\phi$ meson, $K^\\ast$, carrying a unit strange charge, should react to strange chemical potential $\\mu_S$. In this paper, we shall first convince you that $\\mu_S$ is not small in a brayon-rich medium for keeping strange neutrality, and then derive the spin alignment induced by the gradient of $\\mu_S$, and hence of baryon chemical potential $\\mu_B$, using linear response theory, with the transport coefficients expressed, without any approximation, in terms of the $K^\\ast$'s in-medium spectral properties by employing Ward-Takahashi identity. It turns out that such an effect applies mainly to the particles whose longitudinal and transverse modes diverge, and induces only the local spin alignment in a static medium. The magnitudes of these coefficients will be further estimated under the quasi-particle approximation.","sentences":["The difference between the spin alignments of $K^\\ast$ and those of $\\phi$ at the low collision energies is a puzzle raised by the recent experiments.","Unlike $\\phi$ meson, $K^\\ast$, carrying a unit strange charge, should react to strange chemical potential $\\mu_S$. In this paper, we shall first convince you that $\\mu_S$ is not small in a brayon-rich medium for keeping strange neutrality, and then derive the spin alignment induced by the gradient of $\\mu_S$, and hence of baryon chemical potential $\\mu_B$, using linear response theory, with the transport coefficients expressed, without any approximation, in terms of the $K^\\ast$'s in-medium spectral properties by employing Ward-Takahashi identity.","It turns out that such an effect applies mainly to the particles whose longitudinal and transverse modes diverge, and induces only the local spin alignment in a static medium.","The magnitudes of these coefficients will be further estimated under the quasi-particle approximation."],"url":"http://arxiv.org/abs/2404.02860v1","category":"nucl-th"}
{"created":"2024-04-03 16:37:16","title":"Tight stability bounds for entropic Brenier maps","abstract":"Entropic Brenier maps are regularized analogues of Brenier maps (optimal transport maps) which converge to Brenier maps as the regularization parameter shrinks. In this work, we prove quantitative stability bounds between entropic Brenier maps under variations of the target measure. In particular, when all measures have bounded support, we establish the optimal Lipschitz constant for the mapping from probability measures to entropic Brenier maps. This provides an exponential improvement to a result of Carlier, Chizat, and Laborde (2024). As an application, we prove near-optimal bounds for the stability of semi-discrete \\emph{unregularized} Brenier maps for a family of discrete target measures.","sentences":["Entropic Brenier maps are regularized analogues of Brenier maps (optimal transport maps) which converge to Brenier maps as the regularization parameter shrinks.","In this work, we prove quantitative stability bounds between entropic Brenier maps under variations of the target measure.","In particular, when all measures have bounded support, we establish the optimal Lipschitz constant for the mapping from probability measures to entropic Brenier maps.","This provides an exponential improvement to a result of Carlier, Chizat, and Laborde (2024).","As an application, we prove near-optimal bounds for the stability of semi-discrete \\emph{unregularized} Brenier maps for a family of discrete target measures."],"url":"http://arxiv.org/abs/2404.02855v1","category":"math.PR"}
{"created":"2024-04-03 16:36:21","title":"Axisymmetric steady Navier-Stokes flows under suction","abstract":"We prove the existence of solutions for the axisymmetric steady Navier-Stokes system around an infinite cylinder under external forces. The solutions are constructed to be decaying at the horizontal infinity, despite an analogue of the Stokes paradox for the linearized system, and having neither periodicity nor decay in the vertical direction. The proof is based on perturbation of the nonlinear system around a suction flow. The class of functions in this paper, which is a subspace of the space of Fourier transformed vector finite Radon measures, is inspired by Giga-Saal (2013) treating rotating boundary layers.","sentences":["We prove the existence of solutions for the axisymmetric steady Navier-Stokes system around an infinite cylinder under external forces.","The solutions are constructed to be decaying at the horizontal infinity, despite an analogue of the Stokes paradox for the linearized system, and having neither periodicity nor decay in the vertical direction.","The proof is based on perturbation of the nonlinear system around a suction flow.","The class of functions in this paper, which is a subspace of the space of Fourier transformed vector finite Radon measures, is inspired by Giga-Saal (2013) treating rotating boundary layers."],"url":"http://arxiv.org/abs/2404.02854v1","category":"math.AP"}
{"created":"2024-04-03 16:33:48","title":"Domination number of modular product graphs","abstract":"The modular product $G\\diamond H$ of graphs $G$ and $H$ is a graph on vertex set $V(G)\\times V(H)$. Two vertices $(g,h)$ and $(g^{\\prime},h^{\\prime})$ of $G\\diamond H$ are adjacent if $g=g^{\\prime}$ and $hh^{\\prime}\\in E(H)$, or $gg^{\\prime}\\in E(G)$ and $h=h^{\\prime}$, or $gg^{\\prime}\\in E(G)$ and $hh^{\\prime}\\in E(H)$, or (for $g\\neq g^{\\prime}$ and $h\\neq h^{\\prime}$) $gg^{\\prime}\\notin E(G)$ and $hh^{\\prime}\\notin E(H)$. A set $D\\subseteq V(G)$ is a dominating set of $G$ if every vertex outside of $D$ contains a neighbor in $D$. A set $D\\subseteq V(G)$ is a total dominating set of $G$ if every vertex of $G$ contains a neighbor in $D$. The domination number $\\gamma(G)$ (resp. total domination number $\\gamma_{t}(G)$) of $G$ is the minimum cardinality of a dominating set (resp. total dominating set) of $G$. In this work we give several upper and lower bounds for $\\gamma(G\\diamond H)$ in terms of $\\gamma(G),$ $\\gamma(H)$, $\\gamma_{t}(\\overline{G})$ and $\\gamma _{t}(\\overline{H})$, where $\\overline{G}$ is the complement graph of $G$. Further, we fully describe graphs where $\\gamma(G\\diamond H)=k$ for $k\\in\\{1,2,3\\}$. Several conditions on $G$ and $H$ under which $\\gamma (G\\diamond H)$ is at most $4$ and $5$ are also given. A new type of simultaneous domination $\\bar{\\gamma}(G)$, defined as the smallest number of vertices that dominates $G$ and totally dominates the complement of $G,$ emerged as useful and we believe it could be of independent interest. We conclude the paper by proposing few directions for possible further research.","sentences":["The modular product $G\\diamond H$ of graphs $G$ and $H$ is a graph on vertex set $V(G)\\times V(H)$. Two vertices $(g,h)$ and $(g^{\\prime},h^{\\prime})$ of $G\\diamond H$ are adjacent if $g=g^{\\prime}$ and $hh^{\\prime}\\in E(H)$, or $gg^{\\prime}\\in E(G)$ and $h=h^{\\prime}$, or $gg^{\\prime}\\in E(G)$ and $hh^{\\prime}\\in E(H)$, or (for $g\\neq g^{\\prime}$ and $h\\neq h^{\\prime}$) $gg^{\\prime}\\notin E(G)$ and $hh^{\\prime}\\notin E(H)$. A set $D\\subseteq V(G)$ is a dominating set of $G$ if every vertex outside of $D$ contains a neighbor in $D$. A set $D\\subseteq V(G)$ is a total dominating set of $G$ if every vertex of $G$ contains a neighbor in $D$. The domination number $\\gamma(G)$ (resp.","total domination number $\\gamma_{t}(G)$) of $G$ is the minimum cardinality of a dominating set (resp.","total dominating set) of $G$.","In this work we give several upper and lower bounds for $\\gamma(G\\diamond H)$ in terms of $\\gamma(G),$ $\\gamma(H)$, $\\gamma_{t}(\\overline{G})$ and $\\gamma _{t}(\\overline{H})$, where $\\overline{G}$ is the complement graph of $G$. Further, we fully describe graphs where $\\gamma(G\\diamond H)=k$ for $k\\in\\{1,2,3\\}$. Several conditions on $G$ and $H$ under which $\\gamma (G\\diamond H)$ is at most $4$ and $5$ are also given.","A new type of simultaneous domination $\\bar{\\gamma}(G)$, defined as the smallest number of vertices that dominates $G$ and totally dominates the complement of $G,$ emerged as useful and we believe it could be of independent interest.","We conclude the paper by proposing few directions for possible further research."],"url":"http://arxiv.org/abs/2404.02853v1","category":"math.CO"}
{"created":"2024-04-03 16:33:42","title":"Toward Inference-optimal Mixture-of-Expert Large Language Models","abstract":"Mixture-of-Expert (MoE) based large language models (LLMs), such as the recent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size without suffering from the quadratic growth of training cost of dense transformers. Like dense models, training MoEs requires answering the same question: given a training budget, what is the optimal allocation on the model size and number of tokens? We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric besides the validation loss. We find that MoEs with a few (4/8) experts are the most serving efficient solution under the same performance, but costs 2.5-3.5x more in training. On the other hand, training a (16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, but with a larger training dataset is a promising setup under a training budget.","sentences":["Mixture-of-Expert (MoE) based large language models (LLMs), such as the recent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size without suffering from the quadratic growth of training cost of dense transformers.","Like dense models, training MoEs requires answering the same question: given a training budget, what is the optimal allocation on the model size and number of tokens?","We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree.","Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time.","We propose to amend the scaling law of MoE by introducing inference efficiency as another metric besides the validation loss.","We find that MoEs with a few (4/8) experts are the most serving efficient solution under the same performance, but costs 2.5-3.5x more in training.","On the other hand, training a (16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, but with a larger training dataset is a promising setup under a training budget."],"url":"http://arxiv.org/abs/2404.02852v1","category":"cs.LG"}
{"created":"2024-04-03 16:30:48","title":"Efficient Structure-Informed Featurization and Property Prediction of Ordered, Dilute, and Random Atomic Structures","abstract":"Structure-informed materials informatics is a rapidly evolving discipline of materials science relying on the featurization of atomic structures or configurations to construct vector, voxel, graph, graphlet, and other representations useful for machine learning prediction of properties, fingerprinting, and generative design. This work discusses how current featurizers typically perform redundant calculations and how their efficiency could be improved by considering (1) fundamentals of crystallographic (orbits) equivalency to optimize ordered cases and (2) representation-dependent equivalency to optimize cases of dilute, doped, and defect structures with broken symmetry. It also discusses and contrasts ways of (3) approximating random solid solutions occupying arbitrary lattices under such representations.   Efficiency improvements discussed in this work were implemented within pySIPFENN or python toolset for Structure-Informed Property and Feature Engineering with Neural Networks developed by authors since 2019 and shown to increase performance from 2 to 10 times for typical inputs. Throughout this work, the authors explicitly discuss how these advances can be applied to different kinds of similar tools in the community.","sentences":["Structure-informed materials informatics is a rapidly evolving discipline of materials science relying on the featurization of atomic structures or configurations to construct vector, voxel, graph, graphlet, and other representations useful for machine learning prediction of properties, fingerprinting, and generative design.","This work discusses how current featurizers typically perform redundant calculations and how their efficiency could be improved by considering (1) fundamentals of crystallographic (orbits) equivalency to optimize ordered cases and (2) representation-dependent equivalency to optimize cases of dilute, doped, and defect structures with broken symmetry.","It also discusses and contrasts ways of (3) approximating random solid solutions occupying arbitrary lattices under such representations.   ","Efficiency improvements discussed in this work were implemented within pySIPFENN or python toolset for Structure-Informed Property and Feature Engineering with Neural Networks developed by authors since 2019 and shown to increase performance from 2 to 10 times for typical inputs.","Throughout this work, the authors explicitly discuss how these advances can be applied to different kinds of similar tools in the community."],"url":"http://arxiv.org/abs/2404.02849v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-03 16:21:01","title":"Mixed volumes of zonoids and the absolute value of the Grassmannian (Extended Abstract)","abstract":"Zonoids are Hausdorff limits of zonotopes, while zonotopes are convex polytopes defined as the Minkowski sums of finitely many segments. We present a combinatorial framework that links the study of mixed volumes of zonoids (a topic that has applications in algebraic combinatorics) with the study of the absolute value of the Grassmannian, defined as the image of the Grassmannian under the coordinate-wise absolute value map. We use polyhedral computations to derive new families of inequalities for n zonoids in dimension d, when (n,d)=(6,2) and (6,3). Unlike the classical geometric inequalities, originating from the Brunn-Minkowski and Aleksandrov-Fenchel inequalities, the inequalities we produce have the special feature of being Minkowski linear in each of the n zonoids they involve.","sentences":["Zonoids are Hausdorff limits of zonotopes, while zonotopes are convex polytopes defined as the Minkowski sums of finitely many segments.","We present a combinatorial framework that links the study of mixed volumes of zonoids (a topic that has applications in algebraic combinatorics) with the study of the absolute value of the Grassmannian, defined as the image of the Grassmannian under the coordinate-wise absolute value map.","We use polyhedral computations to derive new families of inequalities for n zonoids in dimension d, when (n,d)=(6,2) and (6,3).","Unlike the classical geometric inequalities, originating from the Brunn-Minkowski and Aleksandrov-Fenchel inequalities, the inequalities we produce have the special feature of being Minkowski linear in each of the n zonoids they involve."],"url":"http://arxiv.org/abs/2404.02842v1","category":"math.CO"}
{"created":"2024-04-04 17:58:31","title":"Locating and Editing Factual Associations in Mamba","abstract":"We investigate the mechanisms of factual recall in the Mamba state space model. Our work is inspired by previous findings in autoregressive transformer language models suggesting that their knowledge recall is localized to particular modules at specific token locations; we therefore ask whether factual recall in Mamba can be similarly localized. To investigate this, we conduct four lines of experiments on Mamba. First, we apply causal tracing or interchange interventions to localize key components inside Mamba that are responsible for recalling facts, revealing that specific components within middle layers show strong causal effects at the last token of the subject, while the causal effect of intervening on later layers is most pronounced at the last token of the prompt, matching previous findings on autoregressive transformers. Second, we show that rank-one model editing methods can successfully insert facts at specific locations, again resembling findings on transformer models. Third, we examine the linearity of Mamba's representations of factual relations. Finally we adapt attention-knockout techniques to Mamba to dissect information flow during factual recall. We compare Mamba directly to a similar-sized transformer and conclude that despite significant differences in architectural approach, when it comes to factual recall, the two architectures share many similarities.","sentences":["We investigate the mechanisms of factual recall in the Mamba state space model.","Our work is inspired by previous findings in autoregressive transformer language models suggesting that their knowledge recall is localized to particular modules at specific token locations; we therefore ask whether factual recall in Mamba can be similarly localized.","To investigate this, we conduct four lines of experiments on Mamba.","First, we apply causal tracing or interchange interventions to localize key components inside Mamba that are responsible for recalling facts, revealing that specific components within middle layers show strong causal effects at the last token of the subject, while the causal effect of intervening on later layers is most pronounced at the last token of the prompt, matching previous findings on autoregressive transformers.","Second, we show that rank-one model editing methods can successfully insert facts at specific locations, again resembling findings on transformer models.","Third, we examine the linearity of Mamba's representations of factual relations.","Finally we adapt attention-knockout techniques to Mamba to dissect information flow during factual recall.","We compare Mamba directly to a similar-sized transformer and conclude that despite significant differences in architectural approach, when it comes to factual recall, the two architectures share many similarities."],"url":"http://arxiv.org/abs/2404.03646v1","category":"cs.CL"}
{"created":"2024-04-04 17:54:12","title":"PreAfford: Universal Affordance-Based Pre-Grasping for Diverse Objects and Environments","abstract":"Robotic manipulation of ungraspable objects with two-finger grippers presents significant challenges due to the paucity of graspable features, while traditional pre-grasping techniques, which rely on repositioning objects and leveraging external aids like table edges, lack the adaptability across object categories and scenes. Addressing this, we introduce PreAfford, a novel pre-grasping planning framework that utilizes a point-level affordance representation and a relay training approach to enhance adaptability across a broad range of environments and object types, including those previously unseen. Demonstrated on the ShapeNet-v2 dataset, PreAfford significantly improves grasping success rates by 69% and validates its practicality through real-world experiments. This work offers a robust and adaptable solution for manipulating ungraspable objects.","sentences":["Robotic manipulation of ungraspable objects with two-finger grippers presents significant challenges due to the paucity of graspable features, while traditional pre-grasping techniques, which rely on repositioning objects and leveraging external aids like table edges, lack the adaptability across object categories and scenes.","Addressing this, we introduce PreAfford, a novel pre-grasping planning framework that utilizes a point-level affordance representation and a relay training approach to enhance adaptability across a broad range of environments and object types, including those previously unseen.","Demonstrated on the ShapeNet-v2 dataset, PreAfford significantly improves grasping success rates by 69% and validates its practicality through real-world experiments.","This work offers a robust and adaptable solution for manipulating ungraspable objects."],"url":"http://arxiv.org/abs/2404.03634v1","category":"cs.RO"}
{"created":"2024-04-04 15:49:25","title":"Physics Informed Neural Networks for Free Shear Flows","abstract":"The transformative impact of machine learning, particularly Deep Learning (DL), on scientific and engineering domains is evident. In the context of computational fluid dynamics (CFD), Physics-Informed Neural Networks (PINNs) represent a significant innovation, enabling data-driven fluid simulations while incorporating physics-based laws described by partial differential equations (PDEs). While PINNs have demonstrated efficacy in various fluid flow scenarios, a noticeable gap exists in their application to simulate jet flows - an essential category in engineering. Jets, crucial for downburst outflow, ventilation, and heat transfer, lack comprehensive exploration through PINNs in existing literature. This study addresses this gap by focusing on the application of PINNs to simulate steady jet flows, specifically 2D planar turbulent jet flow scenarios. The novelty lies not only in adapting PINNs for simulating jet flows but also in overcoming challenges such as poor convergence during training, attributed to imbalances in loss terms. We propose a novel PINN architecture for Reynolds-Averaged Navier-Stokes (RANS) simulation of steady turbulent jet flows, without the need for turbulence models and simulation data. Additionally, an extended dynamic weighting strategy is introduced to enhance the balance of loss term contributions in the PINN, resulting in improved convergence and more accurate predictions.","sentences":["The transformative impact of machine learning, particularly Deep Learning (DL), on scientific and engineering domains is evident.","In the context of computational fluid dynamics (CFD), Physics-Informed Neural Networks (PINNs) represent a significant innovation, enabling data-driven fluid simulations while incorporating physics-based laws described by partial differential equations (PDEs).","While PINNs have demonstrated efficacy in various fluid flow scenarios, a noticeable gap exists in their application to simulate jet flows - an essential category in engineering.","Jets, crucial for downburst outflow, ventilation, and heat transfer, lack comprehensive exploration through PINNs in existing literature.","This study addresses this gap by focusing on the application of PINNs to simulate steady jet flows, specifically 2D planar turbulent jet flow scenarios.","The novelty lies not only in adapting PINNs for simulating jet flows but also in overcoming challenges such as poor convergence during training, attributed to imbalances in loss terms.","We propose a novel PINN architecture for Reynolds-Averaged Navier-Stokes (RANS) simulation of steady turbulent jet flows, without the need for turbulence models and simulation data.","Additionally, an extended dynamic weighting strategy is introduced to enhance the balance of loss term contributions in the PINN, resulting in improved convergence and more accurate predictions."],"url":"http://arxiv.org/abs/2404.03542v1","category":"physics.flu-dyn"}
{"created":"2024-04-04 15:29:50","title":"Approximate Gradient Coding for Privacy-Flexible Federated Learning with Non-IID Data","abstract":"This work focuses on the challenges of non-IID data and stragglers/dropouts in federated learning. We introduce and explore a privacy-flexible paradigm that models parts of the clients' local data as non-private, offering a more versatile and business-oriented perspective on privacy. Within this framework, we propose a data-driven strategy for mitigating the effects of label heterogeneity and client straggling on federated learning. Our solution combines both offline data sharing and approximate gradient coding techniques. Through numerical simulations using the MNIST dataset, we demonstrate that our approach enables achieving a deliberate trade-off between privacy and utility, leading to improved model convergence and accuracy while using an adaptable portion of non-private data.","sentences":["This work focuses on the challenges of non-IID data and stragglers/dropouts in federated learning.","We introduce and explore a privacy-flexible paradigm that models parts of the clients' local data as non-private, offering a more versatile and business-oriented perspective on privacy.","Within this framework, we propose a data-driven strategy for mitigating the effects of label heterogeneity and client straggling on federated learning.","Our solution combines both offline data sharing and approximate gradient coding techniques.","Through numerical simulations using the MNIST dataset, we demonstrate that our approach enables achieving a deliberate trade-off between privacy and utility, leading to improved model convergence and accuracy while using an adaptable portion of non-private data."],"url":"http://arxiv.org/abs/2404.03524v1","category":"cs.LG"}
{"created":"2024-04-04 14:25:21","title":"Lower bounds for graph reconstruction with maximal independent set queries","abstract":"We investigate the number of maximal independent set queries required to reconstruct the edges of a hidden graph. We show that randomised adaptive algorithms need at least $\\Omega(\\Delta^2 \\log(n / \\Delta) / \\log \\Delta)$ queries to reconstruct $n$-vertex graphs of maximum degree $\\Delta$ with success probability at least $1/2$, and we further improve this lower bound to $\\Omega(\\Delta^2 \\log(n / \\Delta))$ for randomised non-adaptive algorithms. We also prove that deterministic non-adaptive algorithms require at least $\\Omega(\\Delta^3 \\log n / \\log \\Delta)$ queries.   This improves bounds of Konrad, O'Sullivan, and Traistaru, and answers one of their questions. The proof of the lower bound for deterministic non-adaptive algorithms relies on a connection to cover-free families, for which we also improve known bounds.","sentences":["We investigate the number of maximal independent set queries required to reconstruct the edges of a hidden graph.","We show that randomised adaptive algorithms need at least $\\Omega(\\Delta^2 \\log(n / \\Delta) / \\log \\Delta)$ queries to reconstruct $n$-vertex graphs of maximum degree $\\Delta$ with success probability at least $1/2$, and we further improve this lower bound to $\\Omega(\\Delta^2 \\log(n / \\Delta))$ for randomised non-adaptive algorithms.","We also prove that deterministic non-adaptive algorithms require at least $\\Omega(\\Delta^3 \\log n / \\log \\Delta)$ queries.   ","This improves bounds of Konrad, O'Sullivan, and Traistaru, and answers one of their questions.","The proof of the lower bound for deterministic non-adaptive algorithms relies on a connection to cover-free families, for which we also improve known bounds."],"url":"http://arxiv.org/abs/2404.03472v1","category":"cs.DS"}
{"created":"2024-04-04 11:13:54","title":"Weighted Energy-Dissipation approach to semilinear gradient flows with state-dependent dissipation","abstract":"We investigate the Weighted Energy-Dissipation variational approach to semilinear gradient flows with state-dependent dissipation. A family of parameter-dependent functionals defined over entire trajectories is introduced and proved to admit global minimizers. These global minimizers correspond to solutions of elliptic-in-time regularizations of the limiting causal problem. By passing to the limit in the parameter we prove that such global minimizers converge, up to subsequences, to a solution of the gradient flow.","sentences":["We investigate the Weighted Energy-Dissipation variational approach to semilinear gradient flows with state-dependent dissipation.","A family of parameter-dependent functionals defined over entire trajectories is introduced and proved to admit global minimizers.","These global minimizers correspond to solutions of elliptic-in-time regularizations of the limiting causal problem.","By passing to the limit in the parameter we prove that such global minimizers converge, up to subsequences, to a solution of the gradient flow."],"url":"http://arxiv.org/abs/2404.03370v1","category":"math.AP"}
{"created":"2024-04-04 09:53:00","title":"DI-Retinex: Digital-Imaging Retinex Theory for Low-Light Image Enhancement","abstract":"Many existing methods for low-light image enhancement (LLIE) based on Retinex theory ignore important factors that affect the validity of this theory in digital imaging, such as noise, quantization error, non-linearity, and dynamic range overflow. In this paper, we propose a new expression called Digital-Imaging Retinex theory (DI-Retinex) through theoretical and experimental analysis of Retinex theory in digital imaging. Our new expression includes an offset term in the enhancement model, which allows for pixel-wise brightness contrast adjustment with a non-linear mapping function. In addition, to solve the lowlight enhancement problem in an unsupervised manner, we propose an image-adaptive masked reverse degradation loss in Gamma space. We also design a variance suppression loss for regulating the additional offset term. Extensive experiments show that our proposed method outperforms all existing unsupervised methods in terms of visual quality, model size, and speed. Our algorithm can also assist downstream face detectors in low-light, as it shows the most performance gain after the low-light enhancement compared to other methods.","sentences":["Many existing methods for low-light image enhancement (LLIE) based on Retinex theory ignore important factors that affect the validity of this theory in digital imaging, such as noise, quantization error, non-linearity, and dynamic range overflow.","In this paper, we propose a new expression called Digital-Imaging Retinex theory (DI-Retinex) through theoretical and experimental analysis of Retinex theory in digital imaging.","Our new expression includes an offset term in the enhancement model, which allows for pixel-wise brightness contrast adjustment with a non-linear mapping function.","In addition, to solve the lowlight enhancement problem in an unsupervised manner, we propose an image-adaptive masked reverse degradation loss in Gamma space.","We also design a variance suppression loss for regulating the additional offset term.","Extensive experiments show that our proposed method outperforms all existing unsupervised methods in terms of visual quality, model size, and speed.","Our algorithm can also assist downstream face detectors in low-light, as it shows the most performance gain after the low-light enhancement compared to other methods."],"url":"http://arxiv.org/abs/2404.03327v1","category":"cs.CV"}
{"created":"2024-04-04 09:48:14","title":"A Comparative Analysis of Word-Level Metric Differential Privacy: Benchmarking The Privacy-Utility Trade-off","abstract":"The application of Differential Privacy to Natural Language Processing techniques has emerged in relevance in recent years, with an increasing number of studies published in established NLP outlets. In particular, the adaptation of Differential Privacy for use in NLP tasks has first focused on the $\\textit{word-level}$, where calibrated noise is added to word embedding vectors to achieve \"noisy\" representations. To this end, several implementations have appeared in the literature, each presenting an alternative method of achieving word-level Differential Privacy. Although each of these includes its own evaluation, no comparative analysis has been performed to investigate the performance of such methods relative to each other. In this work, we conduct such an analysis, comparing seven different algorithms on two NLP tasks with varying hyperparameters, including the $\\textit{epsilon ($\\varepsilon$)}$ parameter, or privacy budget. In addition, we provide an in-depth analysis of the results with a focus on the privacy-utility trade-off, as well as open-source our implementation code for further reproduction. As a result of our analysis, we give insight into the benefits and challenges of word-level Differential Privacy, and accordingly, we suggest concrete steps forward for the research field.","sentences":["The application of Differential Privacy to Natural Language Processing techniques has emerged in relevance in recent years, with an increasing number of studies published in established NLP outlets.","In particular, the adaptation of Differential Privacy for use in NLP tasks has first focused on the $\\textit{word-level}$, where calibrated noise is added to word embedding vectors to achieve \"noisy\" representations.","To this end, several implementations have appeared in the literature, each presenting an alternative method of achieving word-level Differential Privacy.","Although each of these includes its own evaluation, no comparative analysis has been performed to investigate the performance of such methods relative to each other.","In this work, we conduct such an analysis, comparing seven different algorithms on two NLP tasks with varying hyperparameters, including the $\\textit{epsilon ($\\varepsilon$)}$ parameter, or privacy budget.","In addition, we provide an in-depth analysis of the results with a focus on the privacy-utility trade-off, as well as open-source our implementation code for further reproduction.","As a result of our analysis, we give insight into the benefits and challenges of word-level Differential Privacy, and accordingly, we suggest concrete steps forward for the research field."],"url":"http://arxiv.org/abs/2404.03324v1","category":"cs.CL"}
{"created":"2024-04-04 08:37:27","title":"AdaBM: On-the-Fly Adaptive Bit Mapping for Image Super-Resolution","abstract":"Although image super-resolution (SR) problem has experienced unprecedented restoration accuracy with deep neural networks, it has yet limited versatile applications due to the substantial computational costs. Since different input images for SR face different restoration difficulties, adapting computational costs based on the input image, referred to as adaptive inference, has emerged as a promising solution to compress SR networks. Specifically, adapting the quantization bit-widths has successfully reduced the inference and memory cost without sacrificing the accuracy. However, despite the benefits of the resultant adaptive network, existing works rely on time-intensive quantization-aware training with full access to the original training pairs to learn the appropriate bit allocation policies, which limits its ubiquitous usage. To this end, we introduce the first on-the-fly adaptive quantization framework that accelerates the processing time from hours to seconds. We formulate the bit allocation problem with only two bit mapping modules: one to map the input image to the image-wise bit adaptation factor and one to obtain the layer-wise adaptation factors. These bit mappings are calibrated and fine-tuned using only a small number of calibration images. We achieve competitive performance with the previous adaptive quantization methods, while the processing time is accelerated by x2000. Codes are available at https://github.com/Cheeun/AdaBM.","sentences":["Although image super-resolution (SR) problem has experienced unprecedented restoration accuracy with deep neural networks, it has yet limited versatile applications due to the substantial computational costs.","Since different input images for SR face different restoration difficulties, adapting computational costs based on the input image, referred to as adaptive inference, has emerged as a promising solution to compress SR networks.","Specifically, adapting the quantization bit-widths has successfully reduced the inference and memory cost without sacrificing the accuracy.","However, despite the benefits of the resultant adaptive network, existing works rely on time-intensive quantization-aware training with full access to the original training pairs to learn the appropriate bit allocation policies, which limits its ubiquitous usage.","To this end, we introduce the first on-the-fly adaptive quantization framework that accelerates the processing time from hours to seconds.","We formulate the bit allocation problem with only two bit mapping modules: one to map the input image to the image-wise bit adaptation factor and one to obtain the layer-wise adaptation factors.","These bit mappings are calibrated and fine-tuned using only a small number of calibration images.","We achieve competitive performance with the previous adaptive quantization methods, while the processing time is accelerated by x2000.","Codes are available at https://github.com/Cheeun/AdaBM."],"url":"http://arxiv.org/abs/2404.03296v1","category":"cs.CV"}
{"created":"2024-04-04 05:53:33","title":"An adaptive heavy ball method for ill-posed inverse problems","abstract":"In this paper we consider ill-posed inverse problems, both linear and nonlinear, by a heavy ball method in which a strongly convex regularization function is incorporated to detect the feature of the sought solution. We develop ideas on how to adaptively choose the step-sizes and the momentum coefficients to achieve acceleration over the Landweber-type method. We then analyze the method and establish its regularization property when it is terminated by the discrepancy principle. Various numerical results are reported which demonstrate the superior performance of our method over the Landweber-type method by reducing substantially the required number of iterations and the computational time.","sentences":["In this paper we consider ill-posed inverse problems, both linear and nonlinear, by a heavy ball method in which a strongly convex regularization function is incorporated to detect the feature of the sought solution.","We develop ideas on how to adaptively choose the step-sizes and the momentum coefficients to achieve acceleration over the Landweber-type method.","We then analyze the method and establish its regularization property when it is terminated by the discrepancy principle.","Various numerical results are reported which demonstrate the superior performance of our method over the Landweber-type method by reducing substantially the required number of iterations and the computational time."],"url":"http://arxiv.org/abs/2404.03218v1","category":"math.NA"}
{"created":"2024-04-04 03:29:41","title":"Goldfish: An Efficient Federated Unlearning Framework","abstract":"With recent legislation on the right to be forgotten, machine unlearning has emerged as a crucial research area. It facilitates the removal of a user's data from federated trained machine learning models without the necessity for retraining from scratch. However, current machine unlearning algorithms are confronted with challenges of efficiency and validity.To address the above issues, we propose a new framework, named Goldfish. It comprises four modules: basic model, loss function, optimization, and extension. To address the challenge of low validity in existing machine unlearning algorithms, we propose a novel loss function. It takes into account the loss arising from the discrepancy between predictions and actual labels in the remaining dataset. Simultaneously, it takes into consideration the bias of predicted results on the removed dataset. Moreover, it accounts for the confidence level of predicted results. Additionally, to enhance efficiency, we adopt knowledge distillation technique in basic model and introduce an optimization module that encompasses the early termination mechanism guided by empirical risk and the data partition mechanism. Furthermore, to bolster the robustness of the aggregated model, we propose an extension module that incorporates a mechanism using adaptive distillation temperature to address the heterogeneity of user local data and a mechanism using adaptive weight to handle the variety in the quality of uploaded models. Finally, we conduct comprehensive experiments to illustrate the effectiveness of proposed approach.","sentences":["With recent legislation on the right to be forgotten, machine unlearning has emerged as a crucial research area.","It facilitates the removal of a user's data from federated trained machine learning models without the necessity for retraining from scratch.","However, current machine unlearning algorithms are confronted with challenges of efficiency and validity.","To address the above issues, we propose a new framework, named Goldfish.","It comprises four modules: basic model, loss function, optimization, and extension.","To address the challenge of low validity in existing machine unlearning algorithms, we propose a novel loss function.","It takes into account the loss arising from the discrepancy between predictions and actual labels in the remaining dataset.","Simultaneously, it takes into consideration the bias of predicted results on the removed dataset.","Moreover, it accounts for the confidence level of predicted results.","Additionally, to enhance efficiency, we adopt knowledge distillation technique in basic model and introduce an optimization module that encompasses the early termination mechanism guided by empirical risk and the data partition mechanism.","Furthermore, to bolster the robustness of the aggregated model, we propose an extension module that incorporates a mechanism using adaptive distillation temperature to address the heterogeneity of user local data and a mechanism using adaptive weight to handle the variety in the quality of uploaded models.","Finally, we conduct comprehensive experiments to illustrate the effectiveness of proposed approach."],"url":"http://arxiv.org/abs/2404.03180v1","category":"cs.LG"}
{"created":"2024-04-04 01:08:45","title":"Spin caloric resistance of Dirac plasma in graphene Corbino device","abstract":"The thermal resistance of a spin-polarized hydrodynamic Dirac plasma in graphene is considered. A mechanism for the coupling of heat and spin flows is discussed, demonstrating that spin diffusion and spin thermocurrent modify viscous dissipation, leading to a significant enhancement of thermal resistance. Practical calculations are then presented for graphene devices in the Corbino geometry.","sentences":["The thermal resistance of a spin-polarized hydrodynamic Dirac plasma in graphene is considered.","A mechanism for the coupling of heat and spin flows is discussed, demonstrating that spin diffusion and spin thermocurrent modify viscous dissipation, leading to a significant enhancement of thermal resistance.","Practical calculations are then presented for graphene devices in the Corbino geometry."],"url":"http://arxiv.org/abs/2404.03135v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-04 00:24:49","title":"A primal-dual adaptive finite element method for total variation based motion estimation","abstract":"Based on previous work we extend a primal-dual semi-smooth Newton method for minimizing a general $L^1$-$L^2$-$TV$ functional over the space of functions of bounded variations by adaptivity in a finite element setting. For automatically generating an adaptive grid we introduce indicators based on a-posteriori error estimates. Further we discuss data interpolation methods on unstructured grids in the context of image processing and present a pixel-based interpolation method. For the computation of optical flow we derive an adaptive finite element coarse-to-fine scheme for resolving large displacements. The coarse-to-fine scheme speeds-up the computing time tremendously.","sentences":["Based on previous work we extend a primal-dual semi-smooth Newton method for minimizing a general $L^1$-$L^2$-$TV$ functional over the space of functions of bounded variations by adaptivity in a finite element setting.","For automatically generating an adaptive grid we introduce indicators based on a-posteriori error estimates.","Further we discuss data interpolation methods on unstructured grids in the context of image processing and present a pixel-based interpolation method.","For the computation of optical flow we derive an adaptive finite element coarse-to-fine scheme for resolving large displacements.","The coarse-to-fine scheme speeds-up the computing time tremendously."],"url":"http://arxiv.org/abs/2404.03125v1","category":"math.NA"}
{"created":"2024-04-03 23:59:20","title":"Krylov-based Adaptive-Rank Implicit Time Integrators for Stiff Problems with Application to Nonlinear Fokker-Planck Kinetic Models","abstract":"We propose a high order adaptive-rank implicit integrators for stiff time-dependent PDEs, leveraging extended Krylov subspaces to efficiently and adaptively populate low-rank solution bases. This allows for the accurate representation of solutions with significantly reduced computational costs. We further introduce an efficient mechanism for residual evaluation and an adaptive rank-seeking strategy that optimizes low-rank settings based on a comparison between the residual size and the local truncation errors of the time-stepping discretization. We demonstrate our approach with the challenging Lenard-Bernstein Fokker-Planck (LBFP) nonlinear equation, which describes collisional processes in a fully ionized plasma. The preservation of {the equilibrium state} is achieved through the Chang-Cooper discretization, and strict conservation of mass, momentum and energy via a Locally Macroscopic Conservative (LoMaC) procedure. The development of implicit adaptive-rank integrators, demonstrated here up to third-order temporal accuracy via diagonally implicit Runge-Kutta schemes, showcases superior performance in terms of accuracy, computational efficiency, equilibrium preservation, and conservation of macroscopic moments. This study offers a starting point for developing scalable, efficient, and accurate methods for high-dimensional time-dependent problems.","sentences":["We propose a high order adaptive-rank implicit integrators for stiff time-dependent PDEs, leveraging extended Krylov subspaces to efficiently and adaptively populate low-rank solution bases.","This allows for the accurate representation of solutions with significantly reduced computational costs.","We further introduce an efficient mechanism for residual evaluation and an adaptive rank-seeking strategy that optimizes low-rank settings based on a comparison between the residual size and the local truncation errors of the time-stepping discretization.","We demonstrate our approach with the challenging Lenard-Bernstein Fokker-Planck (LBFP) nonlinear equation, which describes collisional processes in a fully ionized plasma.","The preservation of {the equilibrium state} is achieved through the Chang-Cooper discretization, and strict conservation of mass, momentum and energy via a Locally Macroscopic Conservative (LoMaC) procedure.","The development of implicit adaptive-rank integrators, demonstrated here up to third-order temporal accuracy via diagonally implicit Runge-Kutta schemes, showcases superior performance in terms of accuracy, computational efficiency, equilibrium preservation, and conservation of macroscopic moments.","This study offers a starting point for developing scalable, efficient, and accurate methods for high-dimensional time-dependent problems."],"url":"http://arxiv.org/abs/2404.03119v1","category":"math.NA"}
{"created":"2024-04-03 23:20:40","title":"Many-to-many Image Generation with Auto-regressive Diffusion Models","abstract":"Recent advancements in image generation have made significant progress, yet existing models present limitations in perceiving and generating an arbitrary number of interrelated images within a broad context. This limitation becomes increasingly critical as the demand for multi-image scenarios, such as multi-view images and visual narratives, grows with the expansion of multimedia platforms. This paper introduces a domain-general framework for many-to-many image generation, capable of producing interrelated image series from a given set of images, offering a scalable solution that obviates the need for task-specific solutions across different multi-image scenarios. To facilitate this, we present MIS, a novel large-scale multi-image dataset, containing 12M synthetic multi-image samples, each with 25 interconnected images. Utilizing Stable Diffusion with varied latent noises, our method produces a set of interconnected images from a single caption. Leveraging MIS, we learn M2M, an autoregressive model for many-to-many generation, where each image is modeled within a diffusion framework. Throughout training on the synthetic MIS, the model excels in capturing style and content from preceding images - synthetic or real - and generates novel images following the captured patterns. Furthermore, through task-specific fine-tuning, our model demonstrates its adaptability to various multi-image generation tasks, including Novel View Synthesis and Visual Procedure Generation.","sentences":["Recent advancements in image generation have made significant progress, yet existing models present limitations in perceiving and generating an arbitrary number of interrelated images within a broad context.","This limitation becomes increasingly critical as the demand for multi-image scenarios, such as multi-view images and visual narratives, grows with the expansion of multimedia platforms.","This paper introduces a domain-general framework for many-to-many image generation, capable of producing interrelated image series from a given set of images, offering a scalable solution that obviates the need for task-specific solutions across different multi-image scenarios.","To facilitate this, we present MIS, a novel large-scale multi-image dataset, containing 12M synthetic multi-image samples, each with 25 interconnected images.","Utilizing Stable Diffusion with varied latent noises, our method produces a set of interconnected images from a single caption.","Leveraging MIS, we learn M2M, an autoregressive model for many-to-many generation, where each image is modeled within a diffusion framework.","Throughout training on the synthetic MIS, the model excels in capturing style and content from preceding images - synthetic or real - and generates novel images following the captured patterns.","Furthermore, through task-specific fine-tuning, our model demonstrates its adaptability to various multi-image generation tasks, including Novel View Synthesis and Visual Procedure Generation."],"url":"http://arxiv.org/abs/2404.03109v1","category":"cs.CV"}
{"created":"2024-04-03 23:10:09","title":"Ab initio leading order effective potential for elastic proton scattering based on the symmetry-adapted no-core shell model","abstract":"Based on the Watson expansion of the multiple scattering series, we employ a nonlocal translationally invariant nuclear density derived within the symmetry-adapted no-core shell model (SA-NCSM) framework from a chiral next-to-next-to-leading order (NNLO) nucleon-nucleon interaction and the very same interaction for a consistent full-folding calculation of the effective (optical) potential for nucleon-nucleus scattering for medium-heavy nuclei. The leading order effective (optical) folding potential is computed by integrating over a translationally invariant SA-NCSM one-body scalar density, spin-projected momentum distribution, and the Wolfenstein amplitudes $A$, $C$, and $M$. The resulting nonlocal potentials serve as input for a momentum space Lippmann-Schwinger equation, whose solutions are summed up to obtain nucleon-nucleus scattering observables. In the SA-NCSM, the model space is systematically up-selected using $\\SpR{3}$ symmetry considerations. For the light nucleus of $^6$He, we establish a systematic selection scheme in the SA-NCSM for scattering observables. Then, we apply this scheme to calculations of scattering observables, such as differential cross sections, analyzing powers, and spin rotation functions for elastic proton scattering from $^{20}$Ne and $^{40}$Ca in the energy regime between 65 and 200 MeV, and compare to available data. Our calculations show that the leading order effective nucleon-nucleus potential in the Watson expansion of multiple scattering theory obtained from an up-selected SA-NCSM model space describes $^{40}$Ca elastic scattering observables reasonably well to about 60 degrees in the center-of-mass frame, which coincides roughly with the validity of the NNLO chiral interaction used to calculate both the nucleon-nucleon amplitudes and the one-body scalar and spin nuclear densities.","sentences":["Based on the Watson expansion of the multiple scattering series, we employ a nonlocal translationally invariant nuclear density derived within the symmetry-adapted no-core shell model (SA-NCSM) framework from a chiral next-to-next-to-leading order (NNLO) nucleon-nucleon interaction and the very same interaction for a consistent full-folding calculation of the effective (optical) potential for nucleon-nucleus scattering for medium-heavy nuclei.","The leading order effective (optical) folding potential is computed by integrating over a translationally invariant SA-NCSM one-body scalar density, spin-projected momentum distribution, and the Wolfenstein amplitudes $A$, $C$, and $M$. The resulting nonlocal potentials serve as input for a momentum space Lippmann-Schwinger equation, whose solutions are summed up to obtain nucleon-nucleus scattering observables.","In the SA-NCSM, the model space is systematically up-selected using $\\SpR{3}$ symmetry considerations.","For the light nucleus of $^6$He, we establish a systematic selection scheme in the SA-NCSM for scattering observables.","Then, we apply this scheme to calculations of scattering observables, such as differential cross sections, analyzing powers, and spin rotation functions for elastic proton scattering from $^{20}$Ne and $^{40}$Ca in the energy regime between 65 and 200 MeV, and compare to available data.","Our calculations show that the leading order effective nucleon-nucleus potential in the Watson expansion of multiple scattering theory obtained from an up-selected SA-NCSM model space describes $^{40}$Ca elastic scattering observables reasonably well to about 60 degrees in the center-of-mass frame, which coincides roughly with the validity of the NNLO chiral interaction used to calculate both the nucleon-nucleon amplitudes and the one-body scalar and spin nuclear densities."],"url":"http://arxiv.org/abs/2404.03106v1","category":"nucl-th"}
{"created":"2024-04-03 23:03:53","title":"Multi-Robot Planning for Filming Groups of Moving Actors Leveraging Submodularity and Pixel Density","abstract":"Observing and filming a group of moving actors with a team of aerial robots is a challenging problem that combines elements of multi-robot coordination, coverage, and view planning. A single camera may observe multiple actors at once, and the robot team may observe individual actors from multiple views. As actors move about, groups may split, merge, and reform, and robots filming these actors should be able to adapt smoothly to such changes in actor formations. Rather than adopt an approach based on explicit formations or assignments, we propose an approach based on optimizing views directly. We model actors as moving polyhedra and compute approximate pixel densities for each face and camera view. Then, we propose an objective that exhibits diminishing returns as pixel densities increase from repeated observation. This gives rise to a multi-robot perception planning problem which we solve via a combination of value iteration and greedy submodular maximization. %using a combination of value iteration to optimize views for individual robots and sequential submodular maximization methods to coordinate the team. We evaluate our approach on challenging scenarios modeled after various kinds of social behaviors and featuring different numbers of robots and actors and observe that robot assignments and formations arise implicitly based on the movements of groups of actors. Simulation results demonstrate that our approach consistently outperforms baselines, and in addition to performing well with the planner's approximation of pixel densities our approach also performs comparably for evaluation based on rendered views. Overall, the multi-round variant of the sequential planner we propose meets (within 1%) or exceeds the formation and assignment baselines in all scenarios we consider.","sentences":["Observing and filming a group of moving actors with a team of aerial robots is a challenging problem that combines elements of multi-robot coordination, coverage, and view planning.","A single camera may observe multiple actors at once, and the robot team may observe individual actors from multiple views.","As actors move about, groups may split, merge, and reform, and robots filming these actors should be able to adapt smoothly to such changes in actor formations.","Rather than adopt an approach based on explicit formations or assignments, we propose an approach based on optimizing views directly.","We model actors as moving polyhedra and compute approximate pixel densities for each face and camera view.","Then, we propose an objective that exhibits diminishing returns as pixel densities increase from repeated observation.","This gives rise to a multi-robot perception planning problem which we solve via a combination of value iteration and greedy submodular maximization.","%using a combination of value iteration to optimize views for individual robots and sequential submodular maximization methods to coordinate the team.","We evaluate our approach on challenging scenarios modeled after various kinds of social behaviors and featuring different numbers of robots and actors and observe that robot assignments and formations arise implicitly based on the movements of groups of actors.","Simulation results demonstrate that our approach consistently outperforms baselines, and in addition to performing well with the planner's approximation of pixel densities our approach also performs comparably for evaluation based on rendered views.","Overall, the multi-round variant of the sequential planner we propose meets (within 1%) or exceeds the formation and assignment baselines in all scenarios we consider."],"url":"http://arxiv.org/abs/2404.03103v1","category":"cs.RO"}
{"created":"2024-04-03 22:51:54","title":"MARL-LNS: Cooperative Multi-agent Reinforcement Learning via Large Neighborhoods Search","abstract":"Cooperative multi-agent reinforcement learning (MARL) has been an increasingly important research topic in the last half-decade because of its great potential for real-world applications. Because of the curse of dimensionality, the popular \"centralized training decentralized execution\" framework requires a long time in training, yet still cannot converge efficiently. In this paper, we propose a general training framework, MARL-LNS, to algorithmically address these issues by training on alternating subsets of agents using existing deep MARL algorithms as low-level trainers, while not involving any additional parameters to be trained. Based on this framework, we provide three algorithm variants based on the framework: random large neighborhood search (RLNS), batch large neighborhood search (BLNS), and adaptive large neighborhood search (ALNS), which alternate the subsets of agents differently. We test our algorithms on both the StarCraft Multi-Agent Challenge and Google Research Football, showing that our algorithms can automatically reduce at least 10% of training time while reaching the same final skill level as the original algorithm.","sentences":["Cooperative multi-agent reinforcement learning (MARL) has been an increasingly important research topic in the last half-decade because of its great potential for real-world applications.","Because of the curse of dimensionality, the popular \"centralized training decentralized execution\" framework requires a long time in training, yet still cannot converge efficiently.","In this paper, we propose a general training framework, MARL-LNS, to algorithmically address these issues by training on alternating subsets of agents using existing deep MARL algorithms as low-level trainers, while not involving any additional parameters to be trained.","Based on this framework, we provide three algorithm variants based on the framework: random large neighborhood search (RLNS), batch large neighborhood search (BLNS), and adaptive large neighborhood search (ALNS), which alternate the subsets of agents differently.","We test our algorithms on both the StarCraft Multi-Agent Challenge and Google Research Football, showing that our algorithms can automatically reduce at least 10% of training time while reaching the same final skill level as the original algorithm."],"url":"http://arxiv.org/abs/2404.03101v1","category":"cs.MA"}
{"created":"2024-04-03 22:38:54","title":"SalFoM: Dynamic Saliency Prediction with Video Foundation Models","abstract":"Recent advancements in video saliency prediction (VSP) have shown promising performance compared to the human visual system, whose emulation is the primary goal of VSP. However, current state-of-the-art models employ spatio-temporal transformers trained on limited amounts of data, hindering generalizability adaptation to downstream tasks. The benefits of vision foundation models present a potential solution to improve the VSP process. However, adapting image foundation models to the video domain presents significant challenges in modeling scene dynamics and capturing temporal information. To address these challenges, and as the first initiative to design a VSP model based on video foundation models, we introduce SalFoM, a novel encoder-decoder video transformer architecture. Our model employs UnMasked Teacher (UMT) as feature extractor and presents a heterogeneous decoder which features a locality-aware spatio-temporal transformer and integrates local and global spatio-temporal information from various perspectives to produce the final saliency map. Our qualitative and quantitative experiments on the challenging VSP benchmark datasets of DHF1K, Hollywood-2 and UCF-Sports demonstrate the superiority of our proposed model in comparison with the state-of-the-art methods.","sentences":["Recent advancements in video saliency prediction (VSP) have shown promising performance compared to the human visual system, whose emulation is the primary goal of VSP.","However, current state-of-the-art models employ spatio-temporal transformers trained on limited amounts of data, hindering generalizability adaptation to downstream tasks.","The benefits of vision foundation models present a potential solution to improve the VSP process.","However, adapting image foundation models to the video domain presents significant challenges in modeling scene dynamics and capturing temporal information.","To address these challenges, and as the first initiative to design a VSP model based on video foundation models, we introduce SalFoM, a novel encoder-decoder video transformer architecture.","Our model employs UnMasked Teacher (UMT) as feature extractor and presents a heterogeneous decoder which features a locality-aware spatio-temporal transformer and integrates local and global spatio-temporal information from various perspectives to produce the final saliency map.","Our qualitative and quantitative experiments on the challenging VSP benchmark datasets of DHF1K, Hollywood-2 and UCF-Sports demonstrate the superiority of our proposed model in comparison with the state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.03097v1","category":"cs.CV"}
{"created":"2024-04-03 21:47:02","title":"First-order PDES for Graph Neural Networks: Advection And Burgers Equation Models","abstract":"Graph Neural Networks (GNNs) have established themselves as the preferred methodology in a multitude of domains, ranging from computer vision to computational biology, especially in contexts where data inherently conform to graph structures. While many existing methods have endeavored to model GNNs using various techniques, a prevalent challenge they grapple with is the issue of over-smoothing. This paper presents new Graph Neural Network models that incorporate two first-order Partial Differential Equations (PDEs). These models do not increase complexity but effectively mitigate the over-smoothing problem. Our experimental findings highlight the capacity of our new PDE model to achieve comparable results with higher-order PDE models and fix the over-smoothing problem up to 64 layers. These results underscore the adaptability and versatility of GNNs, indicating that unconventional approaches can yield outcomes on par with established techniques.","sentences":["Graph Neural Networks (GNNs) have established themselves as the preferred methodology in a multitude of domains, ranging from computer vision to computational biology, especially in contexts where data inherently conform to graph structures.","While many existing methods have endeavored to model GNNs using various techniques, a prevalent challenge they grapple with is the issue of over-smoothing.","This paper presents new Graph Neural Network models that incorporate two first-order Partial Differential Equations (PDEs).","These models do not increase complexity but effectively mitigate the over-smoothing problem.","Our experimental findings highlight the capacity of our new PDE model to achieve comparable results with higher-order PDE models and fix the over-smoothing problem up to 64 layers.","These results underscore the adaptability and versatility of GNNs, indicating that unconventional approaches can yield outcomes on par with established techniques."],"url":"http://arxiv.org/abs/2404.03081v1","category":"cs.LG"}
{"created":"2024-04-03 20:02:49","title":"Unveiling Energy Pathways in AGN Accretion Flows with the Warm Corona Model for the Soft Excess","abstract":"The soft excess in active galactic nuclei (AGNs) may arise through a combination of relativistic reflection and the effects of a warm corona at the surface of the accretion disc. Detailed examination of the soft excess can therefore constrain models of the transport and dissipation of accretion energy. Here, we analyze 34 XMM-Newton observations from 14 Type I AGNs with the reXcor spectral model which self-consistently combines emission from a warm corona with relativistic reflection assuming a lamppost corona. The model divides accretion energy between the disc, the warm corona, and the lamppost. The XMM-Newton observations span a factor of 188 in Eddington ratio ($\\lambda_{\\mathrm{obs}}$) and 350 in black hole mass, and we find that a warm corona is a significant contributor to the soft excess for 13 of the 14 AGNs with a mean warm corona heating fraction of $0.51$. The reXcor fits reveal that the fraction of accretion energy dissipated in the lamppost is anti-correlated with $\\lambda_{\\mathrm{obs}}$. In contrast, the relationship between $\\lambda_{\\mathrm{obs}}$ and both the optical depth and heating fraction of the warm corona appears to transition from an anti-correlation to a correlation at $\\lambda_{\\mathrm{obs,t}} \\approx 0.15$. Therefore, at least one other physical process in addition to the accretion rate is needed to explain the evolution of the warm corona. Overall, we find that a warm corona appears to be a crucial depository of accretion energy in AGNs across a broad range of $\\lambda_{\\mathrm{obs}}$ and black hole mass.","sentences":["The soft excess in active galactic nuclei (AGNs) may arise through a combination of relativistic reflection and the effects of a warm corona at the surface of the accretion disc.","Detailed examination of the soft excess can therefore constrain models of the transport and dissipation of accretion energy.","Here, we analyze 34 XMM-Newton observations from 14 Type I AGNs with the reXcor spectral model which self-consistently combines emission from a warm corona with relativistic reflection assuming a lamppost corona.","The model divides accretion energy between the disc, the warm corona, and the lamppost.","The XMM-Newton observations span a factor of 188 in Eddington ratio ($\\lambda_{\\mathrm{obs}}$) and 350 in black hole mass, and we find that a warm corona is a significant contributor to the soft excess for 13 of the 14 AGNs with a mean warm corona heating fraction of $0.51$. The reXcor fits reveal that the fraction of accretion energy dissipated in the lamppost is anti-correlated with $\\lambda_{\\mathrm{obs}}$. In contrast, the relationship between $\\lambda_{\\mathrm{obs}}$ and both the optical depth and heating fraction of the warm corona appears to transition from an anti-correlation to a correlation at $\\lambda_{\\mathrm{obs,t}} \\approx 0.15$.","Therefore, at least one other physical process in addition to the accretion rate is needed to explain the evolution of the warm corona.","Overall, we find that a warm corona appears to be a crucial depository of accretion energy in AGNs across a broad range of $\\lambda_{\\mathrm{obs}}$ and black hole mass."],"url":"http://arxiv.org/abs/2404.03040v1","category":"astro-ph.HE"}
{"created":"2024-04-03 19:43:17","title":"Global Convergence of High-Order Regularization Methods with Sums-of-Squares Taylor Models","abstract":"High-order tensor methods that employ Taylor-based local models (of degree $p\\ge 3$) within adaptive regularization frameworks have been recently proposed for both convex and nonconvex optimization problems. They have been shown to have superior, and even optimal, worst-case global convergence rates and local rates compared to Newton's method. Finding rigorous and efficient techniques for minimizing the Taylor polynomial sub-problems remains a challenging aspect for these algorithms. Ahmadi et al. recently introduced a tensor method based on sum-of-squares (SoS) reformulations, so that each Taylor polynomial sub-problem in their approach can be tractably minimized using semidefinite programming (SDP); however, the global convergence and complexity of their method have not been addressed for general nonconvex problems. This paper introduces an algorithmic framework that combines the Sum of Squares (SoS) Taylor model with adaptive regularization techniques for nonconvex smooth optimization problems. Each iteration minimizes an SoS Taylor model, offering a polynomial cost per iteration. For general nonconvex functions, the worst-case evaluation complexity bound is $\\mathcal{O}(\\epsilon^{-2})$, while for strongly convex functions, an improved evaluation complexity bound of $\\mathcal{O}(\\epsilon^{-\\frac{1}{p}})$ is established. To the best of our knowledge, this is the first global rate analysis for an adaptive regularization algorithm with a tractable high-order sub-problem in nonconvex smooth optimization, opening the way for further improvements.","sentences":["High-order tensor methods that employ Taylor-based local models (of degree $p\\ge 3$) within adaptive regularization frameworks have been recently proposed for both convex and nonconvex optimization problems.","They have been shown to have superior, and even optimal, worst-case global convergence rates and local rates compared to Newton's method.","Finding rigorous and efficient techniques for minimizing the Taylor polynomial sub-problems remains a challenging aspect for these algorithms.","Ahmadi et al. recently introduced a tensor method based on sum-of-squares (SoS) reformulations, so that each Taylor polynomial sub-problem in their approach can be tractably minimized using semidefinite programming (SDP); however, the global convergence and complexity of their method have not been addressed for general nonconvex problems.","This paper introduces an algorithmic framework that combines the Sum of Squares (SoS) Taylor model with adaptive regularization techniques for nonconvex smooth optimization problems.","Each iteration minimizes an SoS Taylor model, offering a polynomial cost per iteration.","For general nonconvex functions, the worst-case evaluation complexity bound is $\\mathcal{O}(\\epsilon^{-2})$, while for strongly convex functions, an improved evaluation complexity bound of $\\mathcal{O}(\\epsilon^{-\\frac{1}{p}})$ is established.","To the best of our knowledge, this is the first global rate analysis for an adaptive regularization algorithm with a tractable high-order sub-problem in nonconvex smooth optimization, opening the way for further improvements."],"url":"http://arxiv.org/abs/2404.03035v1","category":"math.OC"}
{"created":"2024-04-03 18:04:17","title":"Damping Reveals Hidden Dimensions in Elastic Metastructures Through Induced Transparency","abstract":"Damping typically results in attenuation of vibrations and elastic wave propagation in mechanical systems. Contrary to this conventional understanding, we demonstrate experimentally and explain theoretically the revival of an elastic wave transmitted through a periodic metastructure when a weak non-Hermitian defect (damping mechanism) induces violation of time-reversal symmetry. Damping alters the nature of the system's resonant modes, instigating interference in the scattering field. This leads to transmission revival, revealing the presence of hidden modes which are otherwise masked by the symmetry. Our findings offer an innovative approach for designing dissipation-driven switches and controllers and non-destructive structural health monitoring systems.","sentences":["Damping typically results in attenuation of vibrations and elastic wave propagation in mechanical systems.","Contrary to this conventional understanding, we demonstrate experimentally and explain theoretically the revival of an elastic wave transmitted through a periodic metastructure when a weak non-Hermitian defect (damping mechanism) induces violation of time-reversal symmetry.","Damping alters the nature of the system's resonant modes, instigating interference in the scattering field.","This leads to transmission revival, revealing the presence of hidden modes which are otherwise masked by the symmetry.","Our findings offer an innovative approach for designing dissipation-driven switches and controllers and non-destructive structural health monitoring systems."],"url":"http://arxiv.org/abs/2404.02979v1","category":"physics.app-ph"}
{"created":"2024-04-03 18:00:36","title":"Scaling Laws for Galaxy Images","abstract":"We present the first systematic investigation of supervised scaling laws outside of an ImageNet-like context - on images of galaxies. We use 840k galaxy images and over 100M annotations by Galaxy Zoo volunteers, comparable in scale to Imagenet-1K. We find that adding annotated galaxy images provides a power law improvement in performance across all architectures and all tasks, while adding trainable parameters is effective only for some (typically more subjectively challenging) tasks. We then compare the downstream performance of finetuned models pretrained on either ImageNet-12k alone vs. additionally pretrained on our galaxy images. We achieve an average relative error rate reduction of 31% across 5 downstream tasks of scientific interest. Our finetuned models are more label-efficient and, unlike their ImageNet-12k-pretrained equivalents, often achieve linear transfer performance equal to that of end-to-end finetuning. We find relatively modest additional downstream benefits from scaling model size, implying that scaling alone is not sufficient to address our domain gap, and suggest that practitioners with qualitatively different images might benefit more from in-domain adaption followed by targeted downstream labelling.","sentences":["We present the first systematic investigation of supervised scaling laws outside of an ImageNet-like context - on images of galaxies.","We use 840k galaxy images and over 100M annotations by Galaxy Zoo volunteers, comparable in scale to Imagenet-1K.","We find that adding annotated galaxy images provides a power law improvement in performance across all architectures and all tasks, while adding trainable parameters is effective only for some (typically more subjectively challenging) tasks.","We then compare the downstream performance of finetuned models pretrained on either ImageNet-12k alone vs. additionally pretrained on our galaxy images.","We achieve an average relative error rate reduction of 31% across 5 downstream tasks of scientific interest.","Our finetuned models are more label-efficient and, unlike their ImageNet-12k-pretrained equivalents, often achieve linear transfer performance equal to that of end-to-end finetuning.","We find relatively modest additional downstream benefits from scaling model size, implying that scaling alone is not sufficient to address our domain gap, and suggest that practitioners with qualitatively different images might benefit more from in-domain adaption followed by targeted downstream labelling."],"url":"http://arxiv.org/abs/2404.02973v1","category":"cs.CV"}
{"created":"2024-04-03 16:23:37","title":"Cross-Modal Conditioned Reconstruction for Language-guided Medical Image Segmentation","abstract":"Recent developments underscore the potential of textual information in enhancing learning models for a deeper understanding of medical visual semantics. However, language-guided medical image segmentation still faces a challenging issue. Previous works employ implicit and ambiguous architectures to embed textual information. This leads to segmentation results that are inconsistent with the semantics represented by the language, sometimes even diverging significantly. To this end, we propose a novel cross-modal conditioned Reconstruction for Language-guided Medical Image Segmentation (RecLMIS) to explicitly capture cross-modal interactions, which assumes that well-aligned medical visual features and medical notes can effectively reconstruct each other. We introduce conditioned interaction to adaptively predict patches and words of interest. Subsequently, they are utilized as conditioning factors for mutual reconstruction to align with regions described in the medical notes. Extensive experiments demonstrate the superiority of our RecLMIS, surpassing LViT by 3.74% mIoU on the publicly available MosMedData+ dataset and achieving an average increase of 1.89% mIoU for cross-domain tests on our QATA-CoV19 dataset. Simultaneously, we achieve a relative reduction of 20.2% in parameter count and a 55.5% decrease in computational load. The code will be available at https://github.com/ShashankHuang/RecLMIS.","sentences":["Recent developments underscore the potential of textual information in enhancing learning models for a deeper understanding of medical visual semantics.","However, language-guided medical image segmentation still faces a challenging issue.","Previous works employ implicit and ambiguous architectures to embed textual information.","This leads to segmentation results that are inconsistent with the semantics represented by the language, sometimes even diverging significantly.","To this end, we propose a novel cross-modal conditioned Reconstruction for Language-guided Medical Image Segmentation (RecLMIS) to explicitly capture cross-modal interactions, which assumes that well-aligned medical visual features and medical notes can effectively reconstruct each other.","We introduce conditioned interaction to adaptively predict patches and words of interest.","Subsequently, they are utilized as conditioning factors for mutual reconstruction to align with regions described in the medical notes.","Extensive experiments demonstrate the superiority of our RecLMIS, surpassing LViT by 3.74% mIoU on the publicly available MosMedData+ dataset and achieving an average increase of 1.89% mIoU for cross-domain tests on our QATA-CoV19 dataset.","Simultaneously, we achieve a relative reduction of 20.2% in parameter count and a 55.5% decrease in computational load.","The code will be available at https://github.com/ShashankHuang/RecLMIS."],"url":"http://arxiv.org/abs/2404.02845v1","category":"cs.CV"}
{"created":"2024-04-04 17:57:53","title":"Exploring scalar contributions with $K^+ \\to \u03c0^+ \\ell^+ \\ell^-$","abstract":"The rare kaon decay $K^+ \\to \\pi^+ \\ell^+ \\ell^-$ is an interesting process that offers insights into both Standard Model physics and potential New Physics contributions. While primarily driven by vector interactions in the Standard Model, it also provides a window to explore non-standard contributions. In this letter, we analyse the potential of $K^+ \\to \\pi^+ \\ell^+ \\ell^-$ decays to test the limits on scalar contributions. A simple yet effective analysis using differential decay width and the Forward-Backward Asymmetry is proposed to achieve the state-of-the-art limits on the extent of these scalar contributions. The bounds are obtained for the first time since the E865 experiment through a reinterpretation of the NA48/2 and NA62 experimental results.","sentences":["The rare kaon decay $K^+ \\to \\pi^+ \\ell^+ \\ell^-$ is an interesting process that offers insights into both Standard Model physics and potential New Physics contributions.","While primarily driven by vector interactions in the Standard Model, it also provides a window to explore non-standard contributions.","In this letter, we analyse the potential of $K^+ \\to \\pi^+ \\ell^+ \\ell^-$ decays to test the limits on scalar contributions.","A simple yet effective analysis using differential decay width and the Forward-Backward Asymmetry is proposed to achieve the state-of-the-art limits on the extent of these scalar contributions.","The bounds are obtained for the first time since the E865 experiment through a reinterpretation of the NA48/2 and NA62 experimental results."],"url":"http://arxiv.org/abs/2404.03643v1","category":"hep-ph"}
{"created":"2024-04-04 17:55:51","title":"Representation theory of the reflection equation algebra I: A quantization of Sylvester's law of inertia","abstract":"We prove a version of Sylvester's law of inertia for the Reflection Equation Algebra (=REA). We will only be concerned with the REA constructed from the $R$-matrix associated to the standard $q$-deformation of $GL(N,\\mathbb{C})$. For $q$ positive, this particular REA comes equipped with a natural $*$-structure, by which it can be viewed as a $q$-deformation of the $*$-algebra of polynomial functions on the space of self-adjoint $N$-by-$N$-matrices. We will show that this REA satisfies a type $I$-condition, so that its irreducible representations can in principle be classified. Moreover, we will show that, up to the adjoint action of quantum $GL(N,\\mathbb{C})$, any irreducible representation of the REA is determined by its \\emph{extended signature}, which is a classical signature vector extended by a parameter in $\\mathbb{R}/\\mathbb{Z}$. It is this latter result that we see as a quantized version of Sylvester's law of inertia.","sentences":["We prove a version of Sylvester's law of inertia for the Reflection Equation Algebra (=REA).","We will only be concerned with the REA constructed from the $R$-matrix associated to the standard $q$-deformation of $GL(N,\\mathbb{C})$. For $q$ positive, this particular REA comes equipped with a natural $*$-structure, by which it can be viewed as a $q$-deformation of the $*$-algebra of polynomial functions on the space of self-adjoint $N$-by-$N$-matrices.","We will show that this REA satisfies a type $I$-condition, so that its irreducible representations can in principle be classified.","Moreover, we will show that, up to the adjoint action of quantum $GL(N,\\mathbb{C})$, any irreducible representation of the REA is determined by its \\emph{extended signature}, which is a classical signature vector extended by a parameter in $\\mathbb{R}/\\mathbb{Z}$. It is this latter result that we see as a quantized version of Sylvester's law of inertia."],"url":"http://arxiv.org/abs/2404.03640v1","category":"math.RT"}
{"created":"2024-04-04 17:53:56","title":"Existence and asymptotic behaviour of solutions for a multi-dimensional fractional thin-film equation","abstract":"In this paper, we discuss existence and finite speed of propagation for the solutions to an initial-boundary value problem for a family of fractional thin-film equations in a bounded domain in $\\mathbb{R}^d$. The nonlocal operator we consider is the spectral fractional Laplacian with Neumann boundary conditions. In the case of a \"strong slippage\" regime with \"complete wetting\" interfacial conditions, we prove local entropy estimates that entail finite speed of propagation of the support and a lower bound for the waiting time phenomenon.","sentences":["In this paper, we discuss existence and finite speed of propagation for the solutions to an initial-boundary value problem for a family of fractional thin-film equations in a bounded domain in $\\mathbb{R}^d$. The nonlocal operator we consider is the spectral fractional Laplacian with Neumann boundary conditions.","In the case of a \"strong slippage\" regime with \"complete wetting\" interfacial conditions, we prove local entropy estimates that entail finite speed of propagation of the support and a lower bound for the waiting time phenomenon."],"url":"http://arxiv.org/abs/2404.03633v1","category":"math.AP"}
{"created":"2024-04-04 16:18:26","title":"Multi-mode masers of thermally polarized nuclear spins in solution NMR","abstract":"We present experimental single and multimode sustained 1H NMR masers in solution on thermally polarized spins at room temperature and 9.4 T achieved through the electronic control of radiation feedback (radiation damping). Our observations illustrate the breakdown of the usual three-dimensional Maxwell-Bloch equations for radiation feedback and a simple toy model of few coupled classical moments is used to interpret these experiments.","sentences":["We present experimental single and multimode sustained 1H NMR masers in solution on thermally polarized spins at room temperature and 9.4 T achieved through the electronic control of radiation feedback (radiation damping).","Our observations illustrate the breakdown of the usual three-dimensional Maxwell-Bloch equations for radiation feedback and a simple toy model of few coupled classical moments is used to interpret these experiments."],"url":"http://arxiv.org/abs/2404.03562v1","category":"physics.chem-ph"}
{"created":"2024-04-04 15:31:21","title":"BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with Semantic Neural Graph Filtering","abstract":"Knowledge Graphs (KGs) have proven essential in information processing and reasoning applications because they link related entities and give context-rich information, supporting efficient information retrieval and knowledge discovery; presenting information flow in a very effective manner. Despite being widely used globally, Bangla is relatively underrepresented in KGs due to a lack of comprehensive datasets, encoders, NER (named entity recognition) models, POS (part-of-speech) taggers, and lemmatizers, hindering efficient information processing and reasoning applications in the language. Addressing the KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework that is able to automatically construct Bengali KGs from any Bangla text. We utilize multilingual LLMs to understand various languages and correlate entities and relations universally. By employing a translation dictionary to identify English equivalents and extracting word features from pre-trained BERT models, we construct the foundational KG. To reduce noise and align word embeddings with our goal, we employ graph-based polynomial filters. Lastly, we implement a GNN-based semantic filter, which elevates contextual understanding and trims unnecessary edges, culminating in the formation of the definitive KG. Empirical findings and case studies demonstrate the universal effectiveness of our model, capable of autonomously constructing semantically enriched KGs from any text.","sentences":["Knowledge Graphs (KGs) have proven essential in information processing and reasoning applications because they link related entities and give context-rich information, supporting efficient information retrieval and knowledge discovery; presenting information flow in a very effective manner.","Despite being widely used globally, Bangla is relatively underrepresented in KGs due to a lack of comprehensive datasets, encoders, NER (named entity recognition) models, POS (part-of-speech) taggers, and lemmatizers, hindering efficient information processing and reasoning applications in the language.","Addressing the KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework that is able to automatically construct Bengali KGs from any Bangla text.","We utilize multilingual LLMs to understand various languages and correlate entities and relations universally.","By employing a translation dictionary to identify English equivalents and extracting word features from pre-trained BERT models, we construct the foundational KG.","To reduce noise and align word embeddings with our goal, we employ graph-based polynomial filters.","Lastly, we implement a GNN-based semantic filter, which elevates contextual understanding and trims unnecessary edges, culminating in the formation of the definitive KG.","Empirical findings and case studies demonstrate the universal effectiveness of our model, capable of autonomously constructing semantically enriched KGs from any text."],"url":"http://arxiv.org/abs/2404.03528v1","category":"cs.CL"}
{"created":"2024-04-04 13:56:38","title":"Schr\u00f6dinger equation in dimension two with competing logarithmic self-interaction","abstract":"In this paper we study the equation \\[ -\\Delta u +(\\log |\\cdot|*|u|^2)u=(\\log|\\cdot|*|u|^q)|u|^{q-2}u, \\qquad \\hbox{ in }\\mathbb{R}^2, \\] where $8/3 < q < 4$. By means of variational arguments, we find infinitely many radially symmetric classical solutions. The main difficulties rely on the competition between the two nonlocal terms and on the presence of logarithmic kernels, which have not a prescribed sign. In addition, in order to find finite energy solutions, a suitable functional setting analysis is required.","sentences":["In this paper we study the equation \\[ -\\Delta u +(\\log |\\cdot|*|u|^2)u=(\\log|\\cdot|*|u|^q)|u|^{q-2}u, \\qquad \\hbox{ in }\\mathbb{R}^2, \\] where $8/3 < q < 4$. By means of variational arguments, we find infinitely many radially symmetric classical solutions.","The main difficulties rely on the competition between the two nonlocal terms and on the presence of logarithmic kernels, which have not a prescribed sign.","In addition, in order to find finite energy solutions, a suitable functional setting analysis is required."],"url":"http://arxiv.org/abs/2404.03452v1","category":"math.AP"}
{"created":"2024-04-04 13:06:19","title":"Information entropy for central $^{197}$Au+$^{197}$Au collisions in the UrQMD model","abstract":"Multiplicity information entropy in central $^{197}$Au + $^{197}$Au collisions at impact parameters of 0$-$3 fm are calculated at various center of mass energies ($\\sqrt{s_{\\rm NN}}$) of 5.0, 7.7, 11.5, 14.5, 19.6, 27.0, 32.0, 35.0, 39.0, and 54.4 GeV using the Ultra-relativistic Quantum Molecular Dynamics model (UrQMD). The simulations in UrQMD model are compared with hydro modes with three different equations of state (EoS) and also with a default mode without hydrodynamics. The study reveals that the information entropies of baryons, net-baryons and net-protons with different equations of state in the hydro modes exhibit first decreases and then slowly increases with the increase of $\\sqrt{s_{\\rm NN}}$, while those of hadrons and anti-hadrons, antibaryons show a monotonous increase with $\\sqrt{s_{\\rm NN}}$. An enhancement is found around $\\sqrt{s_{\\rm NN}}$ $\\sim$ 30 GeV potentially corresponding to the critical endpoint with chiral hadron gas EoS and Bag model EoS.","sentences":["Multiplicity information entropy in central $^{197}$Au + $^{197}$Au collisions at impact parameters of 0$-$3 fm are calculated at various center of mass energies ($\\sqrt{s_{\\rm NN}}$) of 5.0, 7.7, 11.5, 14.5, 19.6, 27.0, 32.0, 35.0, 39.0, and 54.4 GeV using the Ultra-relativistic Quantum Molecular Dynamics model (UrQMD).","The simulations in UrQMD model are compared with hydro modes with three different equations of state (EoS) and also with a default mode without hydrodynamics.","The study reveals that the information entropies of baryons, net-baryons and net-protons with different equations of state in the hydro modes exhibit first decreases and then slowly increases with the increase of $\\sqrt{s_{\\rm NN}}$, while those of hadrons and anti-hadrons, antibaryons show a monotonous increase with $\\sqrt{s_{\\rm NN}}$. An enhancement is found around $\\sqrt{s_{\\rm NN}}$ $\\sim$ 30 GeV potentially corresponding to the critical endpoint with chiral hadron gas EoS and Bag model EoS."],"url":"http://arxiv.org/abs/2404.03424v1","category":"nucl-th"}
{"created":"2024-04-04 11:04:21","title":"Exploring Universe acceleration through observational constraints via Hubble parameter reconstruction","abstract":"In this article, we introduce an innovative parametric representation of the Hubble parameter, providing a model-independent means to explore the dynamics of an accelerating cosmos. The model's parameters are rigorously constrained through a Markov Chain Monte Carlo (MCMC) approach, leveraging a comprehensive dataset consisting of 31 data points from cosmic chronometers (CC), 1701 updated observations of Pantheon supernovae type Ia (SNeIa), and 6 data points from baryonic acoustic oscillations (BAO). Our analysis delves into the behavior of various cosmological parameters within the model, including the transition from a decelerating phase to an accelerating one, as well as the density parameters and the equation of state (EoS) parameter. The outcomes of our investigation reveal that the equation of state parameter aligns with characteristics reminiscent of the phantom model, supporting the prevailing understanding of our universe's current state of acceleration. This research contributes valuable insights into the ongoing cosmic expansion and underscores the utility of our novel parametric approach.","sentences":["In this article, we introduce an innovative parametric representation of the Hubble parameter, providing a model-independent means to explore the dynamics of an accelerating cosmos.","The model's parameters are rigorously constrained through a Markov Chain Monte Carlo (MCMC) approach, leveraging a comprehensive dataset consisting of 31 data points from cosmic chronometers (CC), 1701 updated observations of Pantheon supernovae type Ia (SNeIa), and 6 data points from baryonic acoustic oscillations (BAO).","Our analysis delves into the behavior of various cosmological parameters within the model, including the transition from a decelerating phase to an accelerating one, as well as the density parameters and the equation of state (EoS) parameter.","The outcomes of our investigation reveal that the equation of state parameter aligns with characteristics reminiscent of the phantom model, supporting the prevailing understanding of our universe's current state of acceleration.","This research contributes valuable insights into the ongoing cosmic expansion and underscores the utility of our novel parametric approach."],"url":"http://arxiv.org/abs/2404.03362v1","category":"astro-ph.CO"}
{"created":"2024-04-04 10:57:42","title":"Geometrisation of Fermions in Flat Spacetimes","abstract":"Requiring physical consistency in a classical flat spacetime geometrisation of fermions is shown to suggest the introduction of torsion. A resulting simple model for that torsion produces a localised quantum-like particle as a solution of a spinor identity that closely resembles the Dirac equation of quantum electrodynamics. All relevant integrals involving solutions of the spinor identity converge and require that the spin, which is no longer intrinsic but arises from circulating currents, be 1/2 whilst the magnetic moment takes the quantum value of 1 magneton, characteristic of an isolated Dirac particle. The underlying torsion associates the otherwise strictly localised particle with an extended spacetime structure that may be relevant to wider quantum phenomena.","sentences":["Requiring physical consistency in a classical flat spacetime geometrisation of fermions is shown to suggest the introduction of torsion.","A resulting simple model for that torsion produces a localised quantum-like particle as a solution of a spinor identity that closely resembles the Dirac equation of quantum electrodynamics.","All relevant integrals involving solutions of the spinor identity converge and require that the spin, which is no longer intrinsic but arises from circulating currents, be 1/2 whilst the magnetic moment takes the quantum value of 1 magneton, characteristic of an isolated Dirac particle.","The underlying torsion associates the otherwise strictly localised particle with an extended spacetime structure that may be relevant to wider quantum phenomena."],"url":"http://arxiv.org/abs/2404.03360v1","category":"hep-th"}
{"created":"2024-04-04 10:30:28","title":"VF-NeRF: Viewshed Fields for Rigid NeRF Registration","abstract":"3D scene registration is a fundamental problem in computer vision that seeks the best 6-DoF alignment between two scenes. This problem was extensively investigated in the case of point clouds and meshes, but there has been relatively limited work regarding Neural Radiance Fields (NeRF). In this paper, we consider the problem of rigid registration between two NeRFs when the position of the original cameras is not given. Our key novelty is the introduction of Viewshed Fields (VF), an implicit function that determines, for each 3D point, how likely it is to be viewed by the original cameras. We demonstrate how VF can help in the various stages of NeRF registration, with an extensive evaluation showing that VF-NeRF achieves SOTA results on various datasets with different capturing approaches such as LLFF and Objaverese.","sentences":["3D scene registration is a fundamental problem in computer vision that seeks the best 6-DoF alignment between two scenes.","This problem was extensively investigated in the case of point clouds and meshes, but there has been relatively limited work regarding Neural Radiance Fields (NeRF).","In this paper, we consider the problem of rigid registration between two NeRFs when the position of the original cameras is not given.","Our key novelty is the introduction of Viewshed Fields (VF), an implicit function that determines, for each 3D point, how likely it is to be viewed by the original cameras.","We demonstrate how VF can help in the various stages of NeRF registration, with an extensive evaluation showing that VF-NeRF achieves SOTA results on various datasets with different capturing approaches such as LLFF and Objaverese."],"url":"http://arxiv.org/abs/2404.03349v1","category":"cs.CV"}
{"created":"2024-04-04 10:12:34","title":"Defect of irreducible plane curves with simple singularities","abstract":"In this note we focus on the defect of singular plane curve that was recently introduced by Dimca. Roughly speaking, the defect of a reduced plane curve measures the discrepancy from the property of being a free curve. We find some lower-bound on the defect for certain classes of irreducible plane curves admitting nodes, ordinary cusps and ordinary triple points. The main result of the note tells us that reduced simply singular plane curves with sufficiently high Arnold exponents are never free.","sentences":["In this note we focus on the defect of singular plane curve that was recently introduced by Dimca.","Roughly speaking, the defect of a reduced plane curve measures the discrepancy from the property of being a free curve.","We find some lower-bound on the defect for certain classes of irreducible plane curves admitting nodes, ordinary cusps and ordinary triple points.","The main result of the note tells us that reduced simply singular plane curves with sufficiently high Arnold exponents are never free."],"url":"http://arxiv.org/abs/2404.03341v1","category":"math.AG"}
{"created":"2024-04-04 10:08:52","title":"Approximation of Some Nonlinear Fractional Order BVPs by Weighted Residual Methods","abstract":"To extract the approximate solutions in the case of nonlinear fractional order differential equations with the homogeneous and nonhomogeneous boundary conditions, the weighted residual method is embedded here. We exploit three methods such as Galerkin, Least Square, and Collocation for the efficient numerical solution of nonlinear two-point boundary value problems. Some nonlinear cases are examined for observing the maximum absolute errors by the considered methods, demonstrating the accuracy and reliability of the present technique using the modified Legendre and modified Bernoulli polynomials as weight functions. The mathematical formulations and computational algorithms are more straightforward and uncomplicated to understand. Absolute errors and the graphical representation reflect that our method is more accurate and reliable.","sentences":["To extract the approximate solutions in the case of nonlinear fractional order differential equations with the homogeneous and nonhomogeneous boundary conditions, the weighted residual method is embedded here.","We exploit three methods such as Galerkin, Least Square, and Collocation for the efficient numerical solution of nonlinear two-point boundary value problems.","Some nonlinear cases are examined for observing the maximum absolute errors by the considered methods, demonstrating the accuracy and reliability of the present technique using the modified Legendre and modified Bernoulli polynomials as weight functions.","The mathematical formulations and computational algorithms are more straightforward and uncomplicated to understand.","Absolute errors and the graphical representation reflect that our method is more accurate and reliable."],"url":"http://arxiv.org/abs/2404.03338v1","category":"math.NA"}
{"created":"2024-04-04 09:34:41","title":"Cartan Flat Non-degenerate CR Lie Groups","abstract":"In this paper we determine all the simply connected non-degenerate CR Lie groups, which are flat with respect to the Cartan connection: in terms of associated Lie algebras, we assert that the only Cartan flat non-degenerate CR Lie algebras are $\\mathfrak{su}(2)$, $\\mathfrak{sl}(2,\\mathbb{R})$, $\\mathfrak{aff}(\\mathbb{R}) \\oplus \\mathbb{R}$, and $\\mathfrak{h}_{2m+1}$ with its modifications, where $\\mathfrak{aff}(\\mathbb{R})$ is the affine Lie algebra of dimension 2 and $\\mathfrak{h}_{2m+1}$ is the Heisenberg Lie algebra of dimension $2m+1$. Furthermore, we determine all the (flat and non-flat) non-degenerate CR structures on each of these Lie groups.","sentences":["In this paper we determine all the simply connected non-degenerate CR Lie groups, which are flat with respect to the Cartan connection: in terms of associated Lie algebras, we assert that the only Cartan flat non-degenerate CR Lie algebras are $\\mathfrak{su}(2)$, $\\mathfrak{sl}(2,\\mathbb{R})$, $\\mathfrak{aff}(\\mathbb{R})","\\oplus \\mathbb{R}$, and $\\mathfrak{h}_{2m+1}$ with its modifications, where $\\mathfrak{aff}(\\mathbb{R})$ is the affine Lie algebra of dimension 2 and $\\mathfrak{h}_{2m+1}$ is the Heisenberg Lie algebra of dimension $2m+1$. Furthermore, we determine all the (flat and non-flat) non-degenerate CR structures on each of these Lie groups."],"url":"http://arxiv.org/abs/2404.03318v1","category":"math.DG"}
{"created":"2024-04-04 07:45:18","title":"Riemannian L-systems: Modeling growing forms in curved spaces","abstract":"In the past 50 years, the formalism of L-systems has been successfully used and developed to model the growth of filamentous and branching biological forms. These simulations take place in classical 2-D or 3-D Euclidean spaces. However, various biological forms actually grow in curved, non-Euclidean, spaces. This is for example the case of vein networks growing within curved leaf blades, of unicellular filaments, such as pollen tubes, growing on curved surfaces to fertilize distant ovules, of teeth patterns growing on folded epithelia of animals, of diffusion of chemical or mechanical signals at the surface of plant or animal tissues, etc. To model these forms growing in curved spaces, we thus extended the formalism of L-systems to non-Euclidean spaces. In a first step we show that this extension can be carried out by integrating concepts of differential geometry in the notion of turtle geometry. We then illustrate how this extension can be applied to model and program the development of both mathematical and biological forms on curved surfaces embedded in our Euclidean space. We provide various examples applied to plant development. We finally show that this approach can be extended to more abstract spaces, called abstract Riemannian spaces, that are not embedded into any higher space, while being intrinsically curved. We suggest that this abstract extension can be used to provide a new approach for effective modeling of tropism phenomena and illustrate this idea on a few conceptual examples.","sentences":["In the past 50 years, the formalism of L-systems has been successfully used and developed to model the growth of filamentous and branching biological forms.","These simulations take place in classical 2-D or 3-D Euclidean spaces.","However, various biological forms actually grow in curved, non-Euclidean, spaces.","This is for example the case of vein networks growing within curved leaf blades, of unicellular filaments, such as pollen tubes, growing on curved surfaces to fertilize distant ovules, of teeth patterns growing on folded epithelia of animals, of diffusion of chemical or mechanical signals at the surface of plant or animal tissues, etc.","To model these forms growing in curved spaces, we thus extended the formalism of L-systems to non-Euclidean spaces.","In a first step we show that this extension can be carried out by integrating concepts of differential geometry in the notion of turtle geometry.","We then illustrate how this extension can be applied to model and program the development of both mathematical and biological forms on curved surfaces embedded in our Euclidean space.","We provide various examples applied to plant development.","We finally show that this approach can be extended to more abstract spaces, called abstract Riemannian spaces, that are not embedded into any higher space, while being intrinsically curved.","We suggest that this abstract extension can be used to provide a new approach for effective modeling of tropism phenomena and illustrate this idea on a few conceptual examples."],"url":"http://arxiv.org/abs/2404.03270v1","category":"q-bio.QM"}
{"created":"2024-04-04 07:42:47","title":"To Search or to Recommend: Predicting Open-App Motivation with Neural Hawkes Process","abstract":"Incorporating Search and Recommendation (S&R) services within a singular application is prevalent in online platforms, leading to a new task termed open-app motivation prediction, which aims to predict whether users initiate the application with the specific intent of information searching, or to explore recommended content for entertainment. Studies have shown that predicting users' motivation to open an app can help to improve user engagement and enhance performance in various downstream tasks. However, accurately predicting open-app motivation is not trivial, as it is influenced by user-specific factors, search queries, clicked items, as well as their temporal occurrences. Furthermore, these activities occur sequentially and exhibit intricate temporal dependencies. Inspired by the success of the Neural Hawkes Process (NHP) in modeling temporal dependencies in sequences, this paper proposes a novel neural Hawkes process model to capture the temporal dependencies between historical user browsing and querying actions. The model, referred to as Neural Hawkes Process-based Open-App Motivation prediction model (NHP-OAM), employs a hierarchical transformer and a novel intensity function to encode multiple factors, and open-app motivation prediction layer to integrate time and user-specific information for predicting users' open-app motivations. To demonstrate the superiority of our NHP-OAM model and construct a benchmark for the Open-App Motivation Prediction task, we not only extend the public S&R dataset ZhihuRec but also construct a new real-world Open-App Motivation Dataset (OAMD). Experiments on these two datasets validate NHP-OAM's superiority over baseline models. Further downstream application experiments demonstrate NHP-OAM's effectiveness in predicting users' Open-App Motivation, highlighting the immense application value of NHP-OAM.","sentences":["Incorporating Search and Recommendation (S&R) services within a singular application is prevalent in online platforms, leading to a new task termed open-app motivation prediction, which aims to predict whether users initiate the application with the specific intent of information searching, or to explore recommended content for entertainment.","Studies have shown that predicting users' motivation to open an app can help to improve user engagement and enhance performance in various downstream tasks.","However, accurately predicting open-app motivation is not trivial, as it is influenced by user-specific factors, search queries, clicked items, as well as their temporal occurrences.","Furthermore, these activities occur sequentially and exhibit intricate temporal dependencies.","Inspired by the success of the Neural Hawkes Process (NHP) in modeling temporal dependencies in sequences, this paper proposes a novel neural Hawkes process model to capture the temporal dependencies between historical user browsing and querying actions.","The model, referred to as Neural Hawkes Process-based Open-App Motivation prediction model (NHP-OAM), employs a hierarchical transformer and a novel intensity function to encode multiple factors, and open-app motivation prediction layer to integrate time and user-specific information for predicting users' open-app motivations.","To demonstrate the superiority of our NHP-OAM model and construct a benchmark for the Open-App Motivation Prediction task, we not only extend the public S&R dataset ZhihuRec but also construct a new real-world Open-App Motivation Dataset (OAMD).","Experiments on these two datasets validate NHP-OAM's superiority over baseline models.","Further downstream application experiments demonstrate NHP-OAM's effectiveness in predicting users' Open-App Motivation, highlighting the immense application value of NHP-OAM."],"url":"http://arxiv.org/abs/2404.03267v1","category":"cs.IR"}
{"created":"2024-04-04 07:42:02","title":"On the derivation of the linear Boltzmann equation from the nonideal Rayleigh gas","abstract":"This paper's objective is to improve the existing proof of the derivation of the Rayleigh--Boltzmann equation from the nonideal Rayleigh gas [6], yielding a far faster convergence rate. This equation is a linear version of the Boltzmann equation, describing the behavior of a small fraction of tagged particles having been perturbed from thermodynamic equilibrium. This linear equation, derived from the microscopic Newton laws as suggested by the Hilbert's sixth problem, is much better understood than the quadratic Boltzmann equation, and even enable results on long time scales for the kinetic description of gas dynamics.The present paper improves the physically poor convergence rate that had been previously proved, into a much more satisfactory rate which is more than exponentially better.","sentences":["This paper's objective is to improve the existing proof of the derivation of the Rayleigh--Boltzmann equation from the nonideal Rayleigh gas [6], yielding a far faster convergence rate.","This equation is a linear version of the Boltzmann equation, describing the behavior of a small fraction of tagged particles having been perturbed from thermodynamic equilibrium.","This linear equation, derived from the microscopic Newton laws as suggested by the Hilbert's sixth problem, is much better understood than the quadratic Boltzmann equation, and even enable results on long time scales for the kinetic description of gas dynamics.","The present paper improves the physically poor convergence rate that had been previously proved, into a much more satisfactory rate which is more than exponentially better."],"url":"http://arxiv.org/abs/2404.03266v1","category":"math.AP"}
{"created":"2024-04-04 07:32:22","title":"Interaction-induced nonlinear magnon transport in noncentrosymmetric ferromagnets","abstract":"We study the effect of the magnon-magnon interaction on the nonlinear magnon transport. The magnon-magnon interaction induces nonreciprocal magnon decay when the time-reversal symmetry is broken, and leads to nonlinear thermal responses of magnons. We construct a theoretical framework to study the nonlinear thermal responses due to the nonreciprocal magnon decay by using the imaginary Dyson equation and quantum kinetic theory, which is then applied to a model of honeycomb ferromagnets with Dzyaloshinskii-Moriya interactions. An order estimate shows that the nonlinear thermal response from the present mechanism is feasible for experimental measurement.","sentences":["We study the effect of the magnon-magnon interaction on the nonlinear magnon transport.","The magnon-magnon interaction induces nonreciprocal magnon decay when the time-reversal symmetry is broken, and leads to nonlinear thermal responses of magnons.","We construct a theoretical framework to study the nonlinear thermal responses due to the nonreciprocal magnon decay by using the imaginary Dyson equation and quantum kinetic theory, which is then applied to a model of honeycomb ferromagnets with Dzyaloshinskii-Moriya interactions.","An order estimate shows that the nonlinear thermal response from the present mechanism is feasible for experimental measurement."],"url":"http://arxiv.org/abs/2404.03260v1","category":"cond-mat.str-el"}
{"created":"2024-04-04 06:17:03","title":"Blow up analysis for a parabolic MEMS problem, I: H\u00f6lder estimate","abstract":"This is the first in a series of papers devoted to the blow up analysis for the quenching phenomena in a parabolic MEMS equation. In this paper, we first give an optimal H\\\"{o}lder estimate for solutions to this equation by using the blow up method and some Liouville theorems on stationary two-valued caloric functions, and then establish a convergence theory for sequences of uniformly H\\\"{o}lder continuous solutions. These results are also used to prove a stratification theorem on the rupture set $\\{u=0\\}$.","sentences":["This is the first in a series of papers devoted to the blow up analysis for the quenching phenomena in a parabolic MEMS equation.","In this paper, we first give an optimal","H\\\"{o}lder estimate for solutions to this equation by using the blow up method and some Liouville theorems on stationary two-valued caloric functions, and then establish a convergence theory for sequences of uniformly H\\\"{o}lder continuous solutions.","These results are also used to prove a stratification theorem on the rupture set $\\{u=0\\}$."],"url":"http://arxiv.org/abs/2404.03223v1","category":"math.AP"}
{"created":"2024-04-04 05:38:30","title":"Neutrino mean free path in neutron stars in the presence of hyperons","abstract":"We investigate the neutrino elastic differential cross-section (NDCS) and corresponding mean free path for neutral current scattering in the dense matter of a neutron star. A wide range of observed neutron star (NS) masses is considered, including the presence of $\\Lambda$, $\\Xi^{-}$, and $\\Xi^{0}$ hyperons in the heaviest stars. Their presence significantly decreases the total neutrino mean free path in the heavier stars.","sentences":["We investigate the neutrino elastic differential cross-section (NDCS) and corresponding mean free path for neutral current scattering in the dense matter of a neutron star.","A wide range of observed neutron star (NS) masses is considered, including the presence of $\\Lambda$, $\\Xi^{-}$, and $\\Xi^{0}$ hyperons in the heaviest stars.","Their presence significantly decreases the total neutrino mean free path in the heavier stars."],"url":"http://arxiv.org/abs/2404.03213v1","category":"nucl-th"}
{"created":"2024-04-04 04:12:30","title":"AGL-NET: Aerial-Ground Cross-Modal Global Localization with Varying Scales","abstract":"We present AGL-NET, a novel learning-based method for global localization using LiDAR point clouds and satellite maps. AGL-NET tackles two critical challenges: bridging the representation gap between image and points modalities for robust feature matching, and handling inherent scale discrepancies between global view and local view. To address these challenges, AGL-NET leverages a unified network architecture with a novel two-stage matching design. The first stage extracts informative neural features directly from raw sensor data and performs initial feature matching. The second stage refines this matching process by extracting informative skeleton features and incorporating a novel scale alignment step to rectify scale variations between LiDAR and map data. Furthermore, a novel scale and skeleton loss function guides the network toward learning scale-invariant feature representations, eliminating the need for pre-processing satellite maps. This significantly improves real-world applicability in scenarios with unknown map scales. To facilitate rigorous performance evaluation, we introduce a meticulously designed dataset within the CARLA simulator specifically tailored for metric localization training and assessment. The code and dataset will be made publicly available.","sentences":["We present AGL-NET, a novel learning-based method for global localization using LiDAR point clouds and satellite maps.","AGL-NET tackles two critical challenges: bridging the representation gap between image and points modalities for robust feature matching, and handling inherent scale discrepancies between global view and local view.","To address these challenges, AGL-NET leverages a unified network architecture with a novel two-stage matching design.","The first stage extracts informative neural features directly from raw sensor data and performs initial feature matching.","The second stage refines this matching process by extracting informative skeleton features and incorporating a novel scale alignment step to rectify scale variations between LiDAR and map data.","Furthermore, a novel scale and skeleton loss function guides the network toward learning scale-invariant feature representations, eliminating the need for pre-processing satellite maps.","This significantly improves real-world applicability in scenarios with unknown map scales.","To facilitate rigorous performance evaluation, we introduce a meticulously designed dataset within the CARLA simulator specifically tailored for metric localization training and assessment.","The code and dataset will be made publicly available."],"url":"http://arxiv.org/abs/2404.03187v1","category":"cs.CV"}
{"created":"2024-04-04 03:45:17","title":"BodyMAP -- Jointly Predicting Body Mesh and 3D Applied Pressure Map for People in Bed","abstract":"Accurately predicting the 3D human posture and the pressure exerted on the body for people resting in bed, visualized as a body mesh (3D pose & shape) with a 3D pressure map, holds significant promise for healthcare applications, particularly, in the prevention of pressure ulcers. Current methods focus on singular facets of the problem -- predicting only 2D/3D poses, generating 2D pressure images, predicting pressure only for certain body regions instead of the full body, or forming indirect approximations to the 3D pressure map. In contrast, we introduce BodyMAP, which jointly predicts the human body mesh and 3D applied pressure map across the entire human body. Our network leverages multiple visual modalities, incorporating both a depth image of a person in bed and its corresponding 2D pressure image acquired from a pressure-sensing mattress. The 3D pressure map is represented as a pressure value at each mesh vertex and thus allows for precise localization of high-pressure regions on the body. Additionally, we present BodyMAP-WS, a new formulation of pressure prediction in which we implicitly learn pressure in 3D by aligning sensed 2D pressure images with a differentiable 2D projection of the predicted 3D pressure maps. In evaluations with real-world human data, our method outperforms the current state-of-the-art technique by 25% on both body mesh and 3D applied pressure map prediction tasks for people in bed.","sentences":["Accurately predicting the 3D human posture and the pressure exerted on the body for people resting in bed, visualized as a body mesh (3D pose & shape) with a 3D pressure map, holds significant promise for healthcare applications, particularly, in the prevention of pressure ulcers.","Current methods focus on singular facets of the problem -- predicting only 2D/3D poses, generating 2D pressure images, predicting pressure only for certain body regions instead of the full body, or forming indirect approximations to the 3D pressure map.","In contrast, we introduce BodyMAP, which jointly predicts the human body mesh and 3D applied pressure map across the entire human body.","Our network leverages multiple visual modalities, incorporating both a depth image of a person in bed and its corresponding 2D pressure image acquired from a pressure-sensing mattress.","The 3D pressure map is represented as a pressure value at each mesh vertex and thus allows for precise localization of high-pressure regions on the body.","Additionally, we present BodyMAP-WS, a new formulation of pressure prediction in which we implicitly learn pressure in 3D by aligning sensed 2D pressure images with a differentiable 2D projection of the predicted 3D pressure maps.","In evaluations with real-world human data, our method outperforms the current state-of-the-art technique by 25% on both body mesh and 3D applied pressure map prediction tasks for people in bed."],"url":"http://arxiv.org/abs/2404.03183v1","category":"cs.CV"}
{"created":"2024-04-04 03:17:01","title":"A comparison between the deflection angles of massive and massless particles in the Shchwarzschild space-time and their consequences on black hole shadows","abstract":"We present comparisons of the deflection angles of massless and massive particles in the Schwarzschild space-time. For the case of photons in a general static space-time, we construct a spatial 3D equation of motion for their path that leads to an implicit formula for the deflection angle. We then compare our results with well known results of the literature in the Schwarzschild space-time. For the case of massive particles we calculate the deflection angle only in the Schwarzschild space-time. The end result is that as the velocity of any massive particle diminishes, the deflection angle increases. To show the relevance of these comparisons, we constructed different black hole shadows for massive particles in order to be compared with a shadow made by of photons.","sentences":["We present comparisons of the deflection angles of massless and massive particles in the Schwarzschild space-time.","For the case of photons in a general static space-time, we construct a spatial 3D equation of motion for their path that leads to an implicit formula for the deflection angle.","We then compare our results with well known results of the literature in the Schwarzschild space-time.","For the case of massive particles we calculate the deflection angle only in the Schwarzschild space-time.","The end result is that as the velocity of any massive particle diminishes, the deflection angle increases.","To show the relevance of these comparisons, we constructed different black hole shadows for massive particles in order to be compared with a shadow made by of photons."],"url":"http://arxiv.org/abs/2404.03174v1","category":"gr-qc"}
{"created":"2024-04-04 01:39:01","title":"DreamWalk: Style Space Exploration using Diffusion Guidance","abstract":"Text-conditioned diffusion models can generate impressive images, but fall short when it comes to fine-grained control. Unlike direct-editing tools like Photoshop, text conditioned models require the artist to perform \"prompt engineering,\" constructing special text sentences to control the style or amount of a particular subject present in the output image. Our goal is to provide fine-grained control over the style and substance specified by the prompt, for example to adjust the intensity of styles in different regions of the image (Figure 1). Our approach is to decompose the text prompt into conceptual elements, and apply a separate guidance term for each element in a single diffusion process. We introduce guidance scale functions to control when in the diffusion process and \\emph{where} in the image to intervene. Since the method is based solely on adjusting diffusion guidance, it does not require fine-tuning or manipulating the internal layers of the diffusion model's neural network, and can be used in conjunction with LoRA- or DreamBooth-trained models (Figure2). Project page: https://mshu1.github.io/dreamwalk.github.io/","sentences":["Text-conditioned diffusion models can generate impressive images, but fall short when it comes to fine-grained control.","Unlike direct-editing tools like Photoshop, text conditioned models require the artist to perform \"prompt engineering,\" constructing special text sentences to control the style or amount of a particular subject present in the output image.","Our goal is to provide fine-grained control over the style and substance specified by the prompt, for example to adjust the intensity of styles in different regions of the image (Figure 1).","Our approach is to decompose the text prompt into conceptual elements, and apply a separate guidance term for each element in a single diffusion process.","We introduce guidance scale functions to control when in the diffusion process and \\emph{where} in the image to intervene.","Since the method is based solely on adjusting diffusion guidance, it does not require fine-tuning or manipulating the internal layers of the diffusion model's neural network, and can be used in conjunction with LoRA- or DreamBooth-trained models (Figure2).","Project page: https://mshu1.github.io/dreamwalk.github.io/"],"url":"http://arxiv.org/abs/2404.03145v1","category":"cs.CV"}
{"created":"2024-04-04 01:24:27","title":"Theoretical and Empirical Insights into the Origins of Degree Bias in Graph Neural Networks","abstract":"Graph Neural Networks (GNNs) often perform better for high-degree nodes than low-degree nodes on node classification tasks. This degree bias can reinforce social marginalization by, e.g., sidelining authors of lowly-cited papers when predicting paper topics in citation networks. While researchers have proposed numerous hypotheses for why GNN degree bias occurs, we find via a survey of 38 degree bias papers that these hypotheses are often not rigorously validated, and can even be contradictory. Thus, we provide an analysis of the origins of degree bias in message-passing GNNs with different graph filters. We prove that high-degree test nodes tend to have a lower probability of misclassification regardless of how GNNs are trained. Moreover, we show that degree bias arises from a variety of factors that are associated with a node's degree (e.g., homophily of neighbors, diversity of neighbors). Furthermore, we show that during training, some GNNs may adjust their loss on low-degree nodes more slowly than on high-degree nodes; however, with sufficiently many epochs of training, message-passing GNNs can achieve their maximum possible training accuracy, which is not significantly limited by their expressive power. Throughout our analysis, we connect our findings to previously-proposed hypotheses for the origins of degree bias, supporting and unifying some while drawing doubt to others. We validate our theoretical findings on 8 common real-world networks, and based on our theoretical and empirical insights, describe a roadmap to alleviate degree bias.","sentences":["Graph Neural Networks (GNNs) often perform better for high-degree nodes than low-degree nodes on node classification tasks.","This degree bias can reinforce social marginalization by, e.g., sidelining authors of lowly-cited papers when predicting paper topics in citation networks.","While researchers have proposed numerous hypotheses for why GNN degree bias occurs, we find via a survey of 38 degree bias papers that these hypotheses are often not rigorously validated, and can even be contradictory.","Thus, we provide an analysis of the origins of degree bias in message-passing GNNs with different graph filters.","We prove that high-degree test nodes tend to have a lower probability of misclassification regardless of how GNNs are trained.","Moreover, we show that degree bias arises from a variety of factors that are associated with a node's degree (e.g., homophily of neighbors, diversity of neighbors).","Furthermore, we show that during training, some GNNs may adjust their loss on low-degree nodes more slowly than on high-degree nodes; however, with sufficiently many epochs of training, message-passing GNNs can achieve their maximum possible training accuracy, which is not significantly limited by their expressive power.","Throughout our analysis, we connect our findings to previously-proposed hypotheses for the origins of degree bias, supporting and unifying some while drawing doubt to others.","We validate our theoretical findings on 8 common real-world networks, and based on our theoretical and empirical insights, describe a roadmap to alleviate degree bias."],"url":"http://arxiv.org/abs/2404.03139v1","category":"cs.LG"}
{"created":"2024-04-04 00:50:39","title":"Preventing mass loss in the standard level set method: New insights from variational analyses","abstract":"For decades, the computational multiphase flow community has grappled with mass loss in the level set method. Numerous solutions have been proposed, from fixing the reinitialization step to combining the level set method with other conservative schemes. However, our work reveals a more fundamental culprit: the smooth Heaviside and delta functions inherent to the standard formulation. Even if reinitialization is done exactly, i.e., the zero contour interface remains stationary, the use of smooth functions lead to violation of mass conservation. We propose a novel approach using variational analysis to incorporate a mass conservation constraint. This introduces a Lagrange multiplier that enforces overall mass balance. Notably, as the delta function sharpens, i.e., approaches the Dirac delta limit, the Lagrange multiplier approaches zero. However, the exact Lagrange multiplier method disrupts the signed distance property of the level set function. This motivates us to develop an approximate version of the Lagrange multiplier that preserves both overall mass and signed distance property of the level set function. Our framework even recovers existing mass-conserving level set methods, revealing some inconsistencies in prior analyses. We extend this approach to three-phase flows for fluid-structure interaction (FSI) simulations. We present variational equations in both immersed and non-immersed forms, demonstrating the convergence of the former formulation to the latter when the body delta function sharpens. Rigorous test problems confirm that the FSI dynamics produced by our simple, easy-to-implement immersed formulation with the approximate Lagrange multiplier method are accurate and match state-of-the-art solvers.","sentences":["For decades, the computational multiphase flow community has grappled with mass loss in the level set method.","Numerous solutions have been proposed, from fixing the reinitialization step to combining the level set method with other conservative schemes.","However, our work reveals a more fundamental culprit: the smooth Heaviside and delta functions inherent to the standard formulation.","Even if reinitialization is done exactly, i.e., the zero contour interface remains stationary, the use of smooth functions lead to violation of mass conservation.","We propose a novel approach using variational analysis to incorporate a mass conservation constraint.","This introduces a Lagrange multiplier that enforces overall mass balance.","Notably, as the delta function sharpens, i.e., approaches the Dirac delta limit, the Lagrange multiplier approaches zero.","However, the exact Lagrange multiplier method disrupts the signed distance property of the level set function.","This motivates us to develop an approximate version of the Lagrange multiplier that preserves both overall mass and signed distance property of the level set function.","Our framework even recovers existing mass-conserving level set methods, revealing some inconsistencies in prior analyses.","We extend this approach to three-phase flows for fluid-structure interaction (FSI) simulations.","We present variational equations in both immersed and non-immersed forms, demonstrating the convergence of the former formulation to the latter when the body delta function sharpens.","Rigorous test problems confirm that the FSI dynamics produced by our simple, easy-to-implement immersed formulation with the approximate Lagrange multiplier method are accurate and match state-of-the-art solvers."],"url":"http://arxiv.org/abs/2404.03132v1","category":"physics.flu-dyn"}
{"created":"2024-04-03 22:18:49","title":"Analyzing Warp Drive Spacetimes with Warp Factory","abstract":"The field of warp research has been dominated by analytical methods to investigate potential solutions. However, these approaches often favor simple metric forms that facilitate analysis but ultimately limit the range of exploration of novel solutions. So far the proposed solutions have been unphysical, requiring energy condition violations and large energy requirements. To overcome the analytical limitations in warp research, we introduce Warp Factory: a numerical toolkit designed for modeling warp drive spacetimes. By leveraging numerical analysis, Warp Factory enables the examination of general warp drive geometries by evaluating the Einstein field equations and computing energy conditions. Furthermore, this comprehensive toolkit provides the determination of metric scalars and insightful visualizations in both 2D and 3D, offering a deeper understanding of metrics and their corresponding stress-energy tensors. The paper delves into the methodology employed by Warp Factory in evaluating the physicality of warp drive spacetimes and highlights its application in assessing commonly modeled warp drive metrics. By leveraging the capabilities of Warp Factory, we aim to further warp drive research and hopefully bring us closer to realizing physically achievable warp drives.","sentences":["The field of warp research has been dominated by analytical methods to investigate potential solutions.","However, these approaches often favor simple metric forms that facilitate analysis but ultimately limit the range of exploration of novel solutions.","So far the proposed solutions have been unphysical, requiring energy condition violations and large energy requirements.","To overcome the analytical limitations in warp research, we introduce Warp Factory: a numerical toolkit designed for modeling warp drive spacetimes.","By leveraging numerical analysis, Warp Factory enables the examination of general warp drive geometries by evaluating the Einstein field equations and computing energy conditions.","Furthermore, this comprehensive toolkit provides the determination of metric scalars and insightful visualizations in both 2D and 3D, offering a deeper understanding of metrics and their corresponding stress-energy tensors.","The paper delves into the methodology employed by Warp Factory in evaluating the physicality of warp drive spacetimes and highlights its application in assessing commonly modeled warp drive metrics.","By leveraging the capabilities of Warp Factory, we aim to further warp drive research and hopefully bring us closer to realizing physically achievable warp drives."],"url":"http://arxiv.org/abs/2404.03095v1","category":"gr-qc"}
{"created":"2024-04-03 21:51:33","title":"Implantable silicon neural probes with nanophotonic phased arrays for single-lobe beam steering","abstract":"In brain activity mapping experiments using optogenetics, patterned illumination is crucial for deterministic and localized stimulation of neurons. However, due to optical scattering in brain tissue, light-emitting implantable devices are needed to bring precise patterned illumination to deep brain regions. A promising solution is silicon neural probes with integrated nanophotonic circuits that form tailored beam emission patterns without lenses. Here, we demonstrate neural probes with grating-based light emitters that generate a single steerable light beam across $> 60\\%$ of the steering range with $\\ge 4$ dB of background suppression for optogenetic photostimulation. The light emitters, optimized for blue or amber light, combine end-fire optical phased arrays with slab gratings to suppress higher-order sidelobes.","sentences":["In brain activity mapping experiments using optogenetics, patterned illumination is crucial for deterministic and localized stimulation of neurons.","However, due to optical scattering in brain tissue, light-emitting implantable devices are needed to bring precise patterned illumination to deep brain regions.","A promising solution is silicon neural probes with integrated nanophotonic circuits that form tailored beam emission patterns without lenses.","Here, we demonstrate neural probes with grating-based light emitters that generate a single steerable light beam across $> 60\\%$ of the steering range with $\\ge 4$ dB of background suppression for optogenetic photostimulation.","The light emitters, optimized for blue or amber light, combine end-fire optical phased arrays with slab gratings to suppress higher-order sidelobes."],"url":"http://arxiv.org/abs/2404.03083v1","category":"physics.optics"}
{"created":"2024-04-03 21:17:25","title":"From equations in coordinate space to Picard-Fuchs and back","abstract":"We continue the development of a position space approach to equations for Feynman multi-loop integrals. The key idea of the approach is that unintegrated products of Greens functions in position space are still loop integral in momentum space. The natural place to start are the famous banana diagrams, which we explore in this paper. In position space, these are just products of $n$ propagators. Firstly, we explain that these functions satisfy an equation of order $2^n$. These should be compared with Picard-Fuchs equations derived for the momentum space integral. We find that the Fourier transform of the position space operator contains the Picard-Fuchs one as a rightmost factor. The order of these operators is a special issue, especially since the order in momentum space is governed by degree in $x$ in position space. For the generic mass case this factorization pattern is complicated and it seems like the order of the Fourier transformed position space operators is much bigger than that of the Picard-Fuchs. Furthermore, one may ask what happens if after factorization we take the Picard-Fuchs operators back into position space. We discover that the result is again factorized, with the rightmost factor being the original position space equation. We demonstrate how this works in examples and discuss implications for more sophisticated Feynman integrals.","sentences":["We continue the development of a position space approach to equations for Feynman multi-loop integrals.","The key idea of the approach is that unintegrated products of Greens functions in position space are still loop integral in momentum space.","The natural place to start are the famous banana diagrams, which we explore in this paper.","In position space, these are just products of $n$ propagators.","Firstly, we explain that these functions satisfy an equation of order $2^n$. These should be compared with Picard-Fuchs equations derived for the momentum space integral.","We find that the Fourier transform of the position space operator contains the Picard-Fuchs one as a rightmost factor.","The order of these operators is a special issue, especially since the order in momentum space is governed by degree in $x$ in position space.","For the generic mass case this factorization pattern is complicated and it seems like the order of the Fourier transformed position space operators is much bigger than that of the Picard-Fuchs.","Furthermore, one may ask what happens if after factorization we take the Picard-Fuchs operators back into position space.","We discover that the result is again factorized, with the rightmost factor being the original position space equation.","We demonstrate how this works in examples and discuss implications for more sophisticated Feynman integrals."],"url":"http://arxiv.org/abs/2404.03069v1","category":"hep-th"}
{"created":"2024-04-03 21:06:08","title":"Consistency of the bootstrap for asymptotically linear estimators based on machine learning","abstract":"The bootstrap is a popular method of constructing confidence intervals due to its ease of use and broad applicability. Theoretical properties of bootstrap procedures have been established in a variety of settings. However, there is limited theoretical research on the use of the bootstrap in the context of estimation of a differentiable functional in a nonparametric or semiparametric model when nuisance functions are estimated using machine learning. In this article, we provide general conditions for consistency of the bootstrap in such scenarios. Our results cover a range of estimator constructions, nuisance estimation methods, bootstrap sampling distributions, and bootstrap confidence interval types. We provide refined results for the empirical bootstrap and smoothed bootstraps, and for one-step estimators, plug-in estimators, empirical mean plug-in estimators, and estimating equations-based estimators. We illustrate the use of our general results by demonstrating the asymptotic validity of bootstrap confidence intervals for the average density value and G-computed conditional mean parameters, and compare their performance in finite samples using numerical studies. Throughout, we emphasize whether and how the bootstrap can produce asymptotically valid confidence intervals when standard methods fail to do so.","sentences":["The bootstrap is a popular method of constructing confidence intervals due to its ease of use and broad applicability.","Theoretical properties of bootstrap procedures have been established in a variety of settings.","However, there is limited theoretical research on the use of the bootstrap in the context of estimation of a differentiable functional in a nonparametric or semiparametric model when nuisance functions are estimated using machine learning.","In this article, we provide general conditions for consistency of the bootstrap in such scenarios.","Our results cover a range of estimator constructions, nuisance estimation methods, bootstrap sampling distributions, and bootstrap confidence interval types.","We provide refined results for the empirical bootstrap and smoothed bootstraps, and for one-step estimators, plug-in estimators, empirical mean plug-in estimators, and estimating equations-based estimators.","We illustrate the use of our general results by demonstrating the asymptotic validity of bootstrap confidence intervals for the average density value and G-computed conditional mean parameters, and compare their performance in finite samples using numerical studies.","Throughout, we emphasize whether and how the bootstrap can produce asymptotically valid confidence intervals when standard methods fail to do so."],"url":"http://arxiv.org/abs/2404.03064v1","category":"math.ST"}
{"created":"2024-04-03 20:45:15","title":"Functionality Optimization for Singlet Fission Rate Screening in the Full-Dimensional Molecular and Intermolecular Coordinate Space","abstract":"In computational chemistry, accurately predicting molecular configurations that exhibit specific properties remains a critical challenge. Its intricacies become especially evident in the study of molecular aggregates, where the light-induced functionality is tied to highly structure-dependent electronic couplings between molecules. Here, we present an efficient strategy for the targeted screening of the structural space employing a \"functionality optimization\" technique, in which a chosen descriptor, constrained by the ground state energy expression, is optimized. The chosen algorithmic differentiation (AD) framework allows to automatically obtain gradients without its tedious implementation. We demonstrate the effectiveness of the approach by identifying Perylene Bisiimide (PBI) dimer motifs with enhanced SF rate. Our findings reveal that certain structural modifications of the PBI monomer, such as helical twisting and bending, as well as slipped-rotated packing arrangements, can significantly increase the SF rate.","sentences":["In computational chemistry, accurately predicting molecular configurations that exhibit specific properties remains a critical challenge.","Its intricacies become especially evident in the study of molecular aggregates, where the light-induced functionality is tied to highly structure-dependent electronic couplings between molecules.","Here, we present an efficient strategy for the targeted screening of the structural space employing a \"functionality optimization\" technique, in which a chosen descriptor, constrained by the ground state energy expression, is optimized.","The chosen algorithmic differentiation (AD) framework allows to automatically obtain gradients without its tedious implementation.","We demonstrate the effectiveness of the approach by identifying Perylene Bisiimide (PBI) dimer motifs with enhanced SF rate.","Our findings reveal that certain structural modifications of the PBI monomer, such as helical twisting and bending, as well as slipped-rotated packing arrangements, can significantly increase the SF rate."],"url":"http://arxiv.org/abs/2404.03056v1","category":"physics.comp-ph"}
{"created":"2024-04-03 20:30:38","title":"Language, Environment, and Robotic Navigation","abstract":"This paper explores the integration of linguistic inputs within robotic navigation systems, drawing upon the symbol interdependency hypothesis to bridge the divide between symbolic and embodied cognition. It examines previous work incorporating language and semantics into Neural Network (NN) and Simultaneous Localization and Mapping (SLAM) approaches, highlighting how these integrations have advanced the field. By contrasting abstract symbol manipulation with sensory-motor grounding, we propose a unified framework where language functions both as an abstract communicative system and as a grounded representation of perceptual experiences. Our review of cognitive models of distributional semantics and their application to autonomous agents underscores the transformative potential of language-integrated systems.","sentences":["This paper explores the integration of linguistic inputs within robotic navigation systems, drawing upon the symbol interdependency hypothesis to bridge the divide between symbolic and embodied cognition.","It examines previous work incorporating language and semantics into Neural Network (NN) and Simultaneous Localization and Mapping (SLAM) approaches, highlighting how these integrations have advanced the field.","By contrasting abstract symbol manipulation with sensory-motor grounding, we propose a unified framework where language functions both as an abstract communicative system and as a grounded representation of perceptual experiences.","Our review of cognitive models of distributional semantics and their application to autonomous agents underscores the transformative potential of language-integrated systems."],"url":"http://arxiv.org/abs/2404.03049v1","category":"cs.CL"}
{"created":"2024-04-03 19:49:24","title":"On a counterexample to Mordell's Pellian equation conjecture: a non-computer based approach","abstract":"In this note, we investigate a recently discovered counterexample to Mordell's Pellian equation conjecture. We provide a verification of this counterexample that can be checked without computer assistance.","sentences":["In this note, we investigate a recently discovered counterexample to Mordell's Pellian equation conjecture.","We provide a verification of this counterexample that can be checked without computer assistance."],"url":"http://arxiv.org/abs/2404.03038v1","category":"math.NT"}
{"created":"2024-04-03 19:41:12","title":"General Effect Modelling (GEM) -- Part 3. GEM applied on proteome data of cerebrospinal fluid of multiple sclerosis and clinically isolated syndrome","abstract":"The novel data analytical platform General Effect Modelling (GEM), is an umbrella platform covering different data analytical methods that handle data with multiple design variables (or pseudo design variables) and multivariate responses. GEM is here demonstrated in an analysis of proteome data from cerebrospinal fluid (CSF) from two independent previously published datasets, one data set comprised of persons with relapsing-remitting multiple sclerosis, persons with other neurological disorders and persons without neurological disorders, and one data set had persons with clinically isolated syndrome (CIS), which is the first clinical symptom of MS, and controls. The primary aim of the present publication is to use these data to demonstrate how patient stratification can be utilised by GEM for multivariate analysis. We also emphasize how the findings shed light on important aspects of the molecular mechanism of MS that may otherwise be lost. We identified proteins involved in neural development as significantly lower for MS/CIS than for their respective controls. This information was only seen after stratification of the persons into two groups, which were found to have different inflammatory patterns and the utilisation of this by GEM. Our conclusion from the study of these data is that disrupted neural development may be an early event in CIS and MS.","sentences":["The novel data analytical platform General Effect Modelling (GEM), is an umbrella platform covering different data analytical methods that handle data with multiple design variables (or pseudo design variables) and multivariate responses.","GEM is here demonstrated in an analysis of proteome data from cerebrospinal fluid (CSF) from two independent previously published datasets, one data set comprised of persons with relapsing-remitting multiple sclerosis, persons with other neurological disorders and persons without neurological disorders, and one data set had persons with clinically isolated syndrome (CIS), which is the first clinical symptom of MS, and controls.","The primary aim of the present publication is to use these data to demonstrate how patient stratification can be utilised by GEM for multivariate analysis.","We also emphasize how the findings shed light on important aspects of the molecular mechanism of MS that may otherwise be lost.","We identified proteins involved in neural development as significantly lower for MS/CIS than for their respective controls.","This information was only seen after stratification of the persons into two groups, which were found to have different inflammatory patterns and the utilisation of this by GEM.","Our conclusion from the study of these data is that disrupted neural development may be an early event in CIS and MS."],"url":"http://arxiv.org/abs/2404.03034v1","category":"stat.ME"}
{"created":"2024-04-03 19:03:15","title":"GeoT: Tensor Centric Library for Graph Neural Network via Efficient Segment Reduction on GPU","abstract":"In recent years, Graph Neural Networks (GNNs) have ignited a surge of innovation, significantly enhancing the processing of geometric data structures such as graphs, point clouds, and meshes. As the domain continues to evolve, a series of frameworks and libraries are being developed to push GNN efficiency to new heights. While graph-centric libraries have achieved success in the past, the advent of efficient tensor compilers has highlighted the urgent need for tensor-centric libraries. Yet, efficient tensor-centric frameworks for GNNs remain scarce due to unique challenges and limitations encountered when implementing segment reduction in GNN contexts.   We introduce GeoT, a cutting-edge tensor-centric library designed specifically for GNNs via efficient segment reduction. GeoT debuts innovative parallel algorithms that not only introduce new design principles but also expand the available design space. Importantly, GeoT is engineered for straightforward fusion within a computation graph, ensuring compatibility with contemporary tensor-centric machine learning frameworks and compilers. Setting a new performance benchmark, GeoT marks a considerable advancement by showcasing an average operator speedup of 1.80x and an end-to-end speedup of 1.68x.","sentences":["In recent years, Graph Neural Networks (GNNs) have ignited a surge of innovation, significantly enhancing the processing of geometric data structures such as graphs, point clouds, and meshes.","As the domain continues to evolve, a series of frameworks and libraries are being developed to push GNN efficiency to new heights.","While graph-centric libraries have achieved success in the past, the advent of efficient tensor compilers has highlighted the urgent need for tensor-centric libraries.","Yet, efficient tensor-centric frameworks for GNNs remain scarce due to unique challenges and limitations encountered when implementing segment reduction in GNN contexts.   ","We introduce GeoT, a cutting-edge tensor-centric library designed specifically for GNNs via efficient segment reduction.","GeoT debuts innovative parallel algorithms that not only introduce new design principles but also expand the available design space.","Importantly, GeoT is engineered for straightforward fusion within a computation graph, ensuring compatibility with contemporary tensor-centric machine learning frameworks and compilers.","Setting a new performance benchmark, GeoT marks a considerable advancement by showcasing an average operator speedup of 1.80x and an end-to-end speedup of 1.68x."],"url":"http://arxiv.org/abs/2404.03019v1","category":"cs.DC"}
{"created":"2024-04-03 18:41:51","title":"DESI 2024 VI: Cosmological Constraints from the Measurements of Baryon Acoustic Oscillations","abstract":"We present cosmological results from the measurement of baryon acoustic oscillations (BAO) in galaxy, quasar and Lyman-$\\alpha$ forest tracers from the first year of observations from the Dark Energy Spectroscopic Instrument (DESI), to be released in the DESI Data Release 1. DESI BAO provide robust measurements of the transverse comoving distance and Hubble rate, or their combination, relative to the sound horizon, in seven redshift bins from over 6 million extragalactic objects in the redshift range $0.1<z<4.2$. DESI BAO data alone are consistent with the standard flat $\\Lambda$CDM cosmological model with a matter density $\\Omega_\\mathrm{m}=0.295\\pm 0.015$. Paired with a BBN prior and the robustly measured acoustic angular scale from the CMB, DESI requires $H_0=(68.52\\pm0.62)$ km/s/Mpc. In conjunction with CMB anisotropies from Planck and CMB lensing data from Planck and ACT, we find $\\Omega_\\mathrm{m}=0.307\\pm 0.005$ and $H_0=(67.97\\pm0.38)$ km/s/Mpc. Extending the baseline model with a constant dark energy equation of state parameter $w$, DESI BAO alone require $w=-0.99^{+0.15}_{-0.13}$. In models with a time-varying dark energy equation of state parametrized by $w_0$ and $w_a$, combinations of DESI with CMB or with SN~Ia individually prefer $w_0>-1$ and $w_a<0$. This preference is 2.6$\\sigma$ for the DESI+CMB combination, and persists or grows when SN~Ia are added in, giving results discrepant with the $\\Lambda$CDM model at the $2.5\\sigma$, $3.5\\sigma$ or $3.9\\sigma$ levels for the addition of Pantheon+, Union3, or DES-SN5YR datasets respectively. For the flat $\\Lambda$CDM model with the sum of neutrino mass $\\sum m_\\nu$ free, combining the DESI and CMB data yields an upper limit $\\sum m_\\nu < 0.072$ $(0.113)$ eV at 95% confidence for a $\\sum m_\\nu>0$ $(\\sum m_\\nu>0.059)$ eV prior. These neutrino-mass constraints are substantially relaxed in models beyond $\\Lambda$CDM. [Abridged.]","sentences":["We present cosmological results from the measurement of baryon acoustic oscillations (BAO) in galaxy, quasar and Lyman-$\\alpha$ forest tracers from the first year of observations from the Dark Energy Spectroscopic Instrument (DESI), to be released in the DESI Data Release 1.","DESI BAO provide robust measurements of the transverse comoving distance and Hubble rate, or their combination, relative to the sound horizon, in seven redshift bins from over 6 million extragalactic objects in the redshift range $0.1<z<4.2$. DESI BAO data alone are consistent with the standard flat $\\Lambda$CDM cosmological model with a matter density $\\Omega_\\mathrm{m}=0.295\\pm 0.015$. Paired with a BBN prior and the robustly measured acoustic angular scale from the CMB, DESI requires $H_0=(68.52\\pm0.62)$ km/s/Mpc.","In conjunction with CMB anisotropies from Planck and CMB lensing data from Planck and ACT, we find $\\Omega_\\mathrm{m}=0.307\\pm 0.005$ and $H_0=(67.97\\pm0.38)$ km/s/Mpc.","Extending the baseline model with a constant dark energy equation of state parameter $w$, DESI BAO alone require $w=-0.99^{+0.15}_{-0.13}$. In models with a time-varying dark energy equation of state parametrized by $w_0$ and $w_a$, combinations of DESI with CMB or with SN~Ia individually prefer $w_0>-1$ and $w_a<0$. This preference is 2.6$\\sigma$ for the DESI+CMB combination, and persists or grows when SN~Ia are added in, giving results discrepant with the $\\Lambda$CDM model at the $2.5\\sigma$, $3.5\\sigma$ or $3.9\\sigma$ levels for the addition of Pantheon+, Union3, or DES-SN5YR datasets respectively.","For the flat $\\Lambda$CDM model with the sum of neutrino mass $\\sum m_\\nu$ free, combining the DESI and CMB data yields an upper limit $\\sum m_\\nu < 0.072$ $(0.113)$ eV at 95% confidence for a $\\sum m_\\nu>0$ $(\\sum m_\\nu>0.059)$ eV prior.","These neutrino-mass constraints are substantially relaxed in models beyond $\\Lambda$CDM.","[Abridged.]"],"url":"http://arxiv.org/abs/2404.03002v1","category":"astro-ph.CO"}
{"created":"2024-04-03 18:40:48","title":"MeshBrush: Painting the Anatomical Mesh with Neural Stylization for Endoscopy","abstract":"Style transfer is a promising approach to close the sim-to-real gap in medical endoscopy. Rendering realistic endoscopic videos by traversing pre-operative scans (such as MRI or CT) can generate realistic simulations as well as ground truth camera poses and depth maps. Although image-to-image (I2I) translation models such as CycleGAN perform well, they are unsuitable for video-to-video synthesis due to the lack of temporal consistency, resulting in artifacts between frames. We propose MeshBrush, a neural mesh stylization method to synthesize temporally consistent videos with differentiable rendering. MeshBrush uses the underlying geometry of patient imaging data while leveraging existing I2I methods. With learned per-vertex textures, the stylized mesh guarantees consistency while producing high-fidelity outputs. We demonstrate that mesh stylization is a promising approach for creating realistic simulations for downstream tasks such as training and preoperative planning. Although our method is tested and designed for ureteroscopy, its components are transferable to general endoscopic and laparoscopic procedures.","sentences":["Style transfer is a promising approach to close the sim-to-real gap in medical endoscopy.","Rendering realistic endoscopic videos by traversing pre-operative scans (such as MRI or CT) can generate realistic simulations as well as ground truth camera poses and depth maps.","Although image-to-image (I2I) translation models such as CycleGAN perform well, they are unsuitable for video-to-video synthesis due to the lack of temporal consistency, resulting in artifacts between frames.","We propose MeshBrush, a neural mesh stylization method to synthesize temporally consistent videos with differentiable rendering.","MeshBrush uses the underlying geometry of patient imaging data while leveraging existing I2I methods.","With learned per-vertex textures, the stylized mesh guarantees consistency while producing high-fidelity outputs.","We demonstrate that mesh stylization is a promising approach for creating realistic simulations for downstream tasks such as training and preoperative planning.","Although our method is tested and designed for ureteroscopy, its components are transferable to general endoscopic and laparoscopic procedures."],"url":"http://arxiv.org/abs/2404.02999v1","category":"eess.IV"}
{"created":"2024-04-03 18:33:40","title":"Poisson structures on wrinkled fibrations","abstract":"We provide local formul{\\ae} for Poisson bivectors and symplectic forms on the leaves of Poisson structures associated to wrinkled fibrations on smooth $4$--manifolds.","sentences":["We provide local formul{\\ae} for Poisson bivectors and symplectic forms on the leaves of Poisson structures associated to wrinkled fibrations on smooth $4$--manifolds."],"url":"http://arxiv.org/abs/2404.02995v1","category":"math.SG"}
{"created":"2024-04-03 18:16:12","title":"Singular solutions for complex second order elliptic equations and their application to time-harmonic diffuse optical tomography","abstract":"We construct singular solutions of a complex elliptic equation of second order, having an isolated singularity of any order. In particular, we extend results obtained for the real partial differential equation in divergence form by Alessandrini in 1990. Our solutions can be applied to the determination of the optical properties of an anisotropic medium in time-harmonic Diffuse Optical Tomography (DOT).","sentences":["We construct singular solutions of a complex elliptic equation of second order, having an isolated singularity of any order.","In particular, we extend results obtained for the real partial differential equation in divergence form by Alessandrini in 1990.","Our solutions can be applied to the determination of the optical properties of an anisotropic medium in time-harmonic Diffuse Optical Tomography (DOT)."],"url":"http://arxiv.org/abs/2404.02987v1","category":"math.AP"}
{"created":"2024-04-03 18:06:54","title":"On the pseudo-Riemann metrizability of SO(3)-symmetric Berwald-Finsler spaces","abstract":"We clarify, for SO(3)-symmetric 4-dimensional Berwald-Finsler spaces of indefinite signature, the question of the existence of an affinely equivalent pseudo-Riemannian metric. It turns out that the answer depends on the holonomy distribution of the canonical affine connection and on the symmetry of its Ricci tensor. In particular, we find all classes of 4-dimensional SO(3)-invariant, symmetric affine connections which do not arise as Levi-Civita connections of any pseudo-Riemannian metric, but can still be metrized by (SO(3)-symmetric) Finsler functions; in Lorentzian signature, these will provide Berwald spacetime structures whose geodesic structure cannot be ascribed to any Lorentzian metric. Some concrete examples are also presented.","sentences":["We clarify, for SO(3)-symmetric 4-dimensional Berwald-Finsler spaces of indefinite signature, the question of the existence of an affinely equivalent pseudo-Riemannian metric.","It turns out that the answer depends on the holonomy distribution of the canonical affine connection and on the symmetry of its Ricci tensor.","In particular, we find all classes of 4-dimensional SO(3)-invariant, symmetric affine connections which do not arise as Levi-Civita connections of any pseudo-Riemannian metric, but can still be metrized by (SO(3)-symmetric) Finsler functions; in Lorentzian signature, these will provide Berwald spacetime structures whose geodesic structure cannot be ascribed to any Lorentzian metric.","Some concrete examples are also presented."],"url":"http://arxiv.org/abs/2404.02980v1","category":"math.DG"}
{"created":"2024-04-03 18:00:12","title":"On the Proof of Chiral Symmetry Breaking through Anomaly Matching in QCD-like Theories: An Exemplification","abstract":"Our recent works revisit the proof of chiral symmetry breaking in the confining phase of four-dimensional QCD-like theories, i.e. $SU(N_c)$ gauge theories with $N_f$ flavors of vectorlike quarks in the fundamental representation. The analysis relies on the structure of 't Hooft anomaly matching and persistent mass conditions for theories with same $N_c$ and different $N_f$. In this paper, we work out concrete examples with $N_c=3$ and $N_c=5$ to support and elucidate the results in the companion papers. Within the same examples, we also test some claims made in earlier works.","sentences":["Our recent works revisit the proof of chiral symmetry breaking in the confining phase of four-dimensional QCD-like theories, i.e. $SU(N_c)$ gauge theories with $N_f$ flavors of vectorlike quarks in the fundamental representation.","The analysis relies on the structure of 't Hooft anomaly matching and persistent mass conditions for theories with same $N_c$ and different $N_f$. In this paper, we work out concrete examples with $N_c=3$ and $N_c=5$ to support and elucidate the results in the companion papers.","Within the same examples, we also test some claims made in earlier works."],"url":"http://arxiv.org/abs/2404.02971v1","category":"hep-th"}
{"created":"2024-04-03 18:00:09","title":"Critical Properties of Weak Measurement Induced Phase Transitions in Random Quantum Circuits","abstract":"The effects of different forms of weak measurements on the nature of the measurement induced phase transition are theoretically studied in hybrid random quantum circuits of qubits. We use a combination of entanglement measures, ancilla purification dynamics, and a transfer matrix approach to compute the critical exponents, the effective central charge, and the multifractal spectrum of the measurement induced transitions. We compare weak measurements with an infinite number of discrete outcomes to a protocol with only a pair of outcomes and find that to within our numerical accuracy the universal critical properties are unaffected by the weak measurement protocols and are consistent with the universality class found for strong projective measurements.","sentences":["The effects of different forms of weak measurements on the nature of the measurement induced phase transition are theoretically studied in hybrid random quantum circuits of qubits.","We use a combination of entanglement measures, ancilla purification dynamics, and a transfer matrix approach to compute the critical exponents, the effective central charge, and the multifractal spectrum of the measurement induced transitions.","We compare weak measurements with an infinite number of discrete outcomes to a protocol with only a pair of outcomes and find that to within our numerical accuracy the universal critical properties are unaffected by the weak measurement protocols and are consistent with the universality class found for strong projective measurements."],"url":"http://arxiv.org/abs/2404.02968v1","category":"quant-ph"}
{"created":"2024-04-03 18:00:00","title":"Chirality-Driven Orbital Angular Momentum and Circular Dichroism in CoSi","abstract":"Chiral crystals and molecules were recently predicted to form an intriguing platform for unconventional orbital physics. Here, we report the observation of chirality-driven orbital textures in the bulk electronic structure of CoSi, a prototype member of the cubic B20 family of chiral crystals. Using circular dichroism in soft X-ray angle-resolved photoemission, we demonstrate the formation of a bulk orbital-angular-momentum texture and monopole-like orbital-momentum locking that depends on crystal handedness. We introduce the intrinsic chiral circular dichroism, icCD, as a differential photoemission observable and a natural probe of chiral electron states. Our findings render chiral crystals promising for spin-orbitronics applications.","sentences":["Chiral crystals and molecules were recently predicted to form an intriguing platform for unconventional orbital physics.","Here, we report the observation of chirality-driven orbital textures in the bulk electronic structure of CoSi, a prototype member of the cubic B20 family of chiral crystals.","Using circular dichroism in soft X-ray angle-resolved photoemission, we demonstrate the formation of a bulk orbital-angular-momentum texture and monopole-like orbital-momentum locking that depends on crystal handedness.","We introduce the intrinsic chiral circular dichroism, icCD, as a differential photoemission observable and a natural probe of chiral electron states.","Our findings render chiral crystals promising for spin-orbitronics applications."],"url":"http://arxiv.org/abs/2404.02952v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-03 17:53:32","title":"Comment on \"Machine learning conservation laws from differential equations\"","abstract":"In lieu of abstract, first paragraph reads: Six months after the author derived a constant of motion for a 1D damped harmonic oscillator [1], a similar result appeared by Liu, Madhavan, and Tegmark [2, 3], without citing the author. However, their derivation contained six serious errors, causing both their method and result to be incorrect. In this Comment, those errors are reviewed.","sentences":["In lieu of abstract, first paragraph reads: Six months after the author derived a constant of motion for a 1D damped harmonic oscillator [1], a similar result appeared by Liu, Madhavan, and Tegmark","[2, 3], without citing the author.","However, their derivation contained six serious errors, causing both their method and result to be incorrect.","In this Comment, those errors are reviewed."],"url":"http://arxiv.org/abs/2404.02896v1","category":"cs.LG"}
{"created":"2024-04-03 17:52:28","title":"Renormalized energy of proper maps and conformal geodesics","abstract":"We introduce a certain renormalized energy of proper maps between conformally compact Einstein manifolds, which is then used to give a holographic description of conformal geodesics on the boundary at infinity, in a way deeply inspired by a work of Fine and Herfray on renormalized area minimization and conformal geodesics.","sentences":["We introduce a certain renormalized energy of proper maps between conformally compact Einstein manifolds, which is then used to give a holographic description of conformal geodesics on the boundary at infinity, in a way deeply inspired by a work of Fine and Herfray on renormalized area minimization and conformal geodesics."],"url":"http://arxiv.org/abs/2404.02895v1","category":"math.DG"}
{"created":"2024-04-03 17:49:41","title":"MODNO: Multi Operator Learning With Distributed Neural Operators","abstract":"The study of operator learning involves the utilization of neural networks to approximate operators. Traditionally, the focus has been on single-operator learning (SOL). However, recent advances have rapidly expanded this to include the approximation of multiple operators using foundation models equipped with millions or billions of trainable parameters, leading to the research of multi-operator learning (MOL). In this paper, we present a novel distributed training approach aimed at enabling a single neural operator with significantly fewer parameters to effectively tackle multi-operator learning challenges, all without incurring additional average costs. Our method is applicable to various Chen-Chen-type neural operators, such as Deep Operator Neural Networks (DON). The core idea is to independently learn the output basis functions for each operator using its dedicated data, while simultaneously centralizing the learning of the input function encoding shared by all operators using the entire dataset. Through a systematic study of five numerical examples, we compare the accuracy and cost of training a single neural operator for each operator independently versus training a MOL model using our proposed method. Our results demonstrate enhanced efficiency and satisfactory accuracy. Moreover, our approach illustrates that some operators with limited data can be more effectively constructed with the aid of data from analogous operators through MOL learning. This highlights another MOL's potential to bolster operator learning.","sentences":["The study of operator learning involves the utilization of neural networks to approximate operators.","Traditionally, the focus has been on single-operator learning (SOL).","However, recent advances have rapidly expanded this to include the approximation of multiple operators using foundation models equipped with millions or billions of trainable parameters, leading to the research of multi-operator learning (MOL).","In this paper, we present a novel distributed training approach aimed at enabling a single neural operator with significantly fewer parameters to effectively tackle multi-operator learning challenges, all without incurring additional average costs.","Our method is applicable to various Chen-Chen-type neural operators, such as Deep Operator Neural Networks (DON).","The core idea is to independently learn the output basis functions for each operator using its dedicated data, while simultaneously centralizing the learning of the input function encoding shared by all operators using the entire dataset.","Through a systematic study of five numerical examples, we compare the accuracy and cost of training a single neural operator for each operator independently versus training a MOL model using our proposed method.","Our results demonstrate enhanced efficiency and satisfactory accuracy.","Moreover, our approach illustrates that some operators with limited data can be more effectively constructed with the aid of data from analogous operators through MOL learning.","This highlights another MOL's potential to bolster operator learning."],"url":"http://arxiv.org/abs/2404.02892v1","category":"cs.LG"}
{"created":"2024-04-03 17:42:22","title":"Learning Quadrupedal Locomotion via Differentiable Simulation","abstract":"The emergence of differentiable simulators enabling analytic gradient computation has motivated a new wave of learning algorithms that hold the potential to significantly increase sample efficiency over traditional Reinforcement Learning (RL) methods. While recent research has demonstrated performance gains in scenarios with comparatively smooth dynamics and, thus, smooth optimization landscapes, research on leveraging differentiable simulators for contact-rich scenarios, such as legged locomotion, is scarce. This may be attributed to the discontinuous nature of contact, which introduces several challenges to optimizing with analytic gradients. The purpose of this paper is to determine if analytic gradients can be beneficial even in the face of contact. Our investigation focuses on the effects of different soft and hard contact models on the learning process, examining optimization challenges through the lens of contact simulation. We demonstrate the viability of employing analytic gradients to learn physically plausible locomotion skills with a quadrupedal robot using Short-Horizon Actor-Critic (SHAC), a learning algorithm leveraging analytic gradients, and draw a comparison to a state-of-the-art RL algorithm, Proximal Policy Optimization (PPO), to understand the benefits of analytic gradients.","sentences":["The emergence of differentiable simulators enabling analytic gradient computation has motivated a new wave of learning algorithms that hold the potential to significantly increase sample efficiency over traditional Reinforcement Learning (RL) methods.","While recent research has demonstrated performance gains in scenarios with comparatively smooth dynamics and, thus, smooth optimization landscapes, research on leveraging differentiable simulators for contact-rich scenarios, such as legged locomotion, is scarce.","This may be attributed to the discontinuous nature of contact, which introduces several challenges to optimizing with analytic gradients.","The purpose of this paper is to determine if analytic gradients can be beneficial even in the face of contact.","Our investigation focuses on the effects of different soft and hard contact models on the learning process, examining optimization challenges through the lens of contact simulation.","We demonstrate the viability of employing analytic gradients to learn physically plausible locomotion skills with a quadrupedal robot using Short-Horizon Actor-Critic (SHAC), a learning algorithm leveraging analytic gradients, and draw a comparison to a state-of-the-art RL algorithm, Proximal Policy Optimization (PPO), to understand the benefits of analytic gradients."],"url":"http://arxiv.org/abs/2404.02887v1","category":"cs.RO"}
{"created":"2024-04-03 17:38:04","title":"Stability of multiphase mean curvature flow beyond circular topology changes","abstract":"We prove a weak-strong uniqueness principle for varifold-BV solutions to planar multiphase mean curvature flow beyond a circular topology change: Assuming that there exists a classical solution with an interface that becomes increasingly circular and shrinks to a point, any varifold-BV solution with the same initial interface must coincide with it, and any varifold-BV solution with similar initial data must undergo the same type of topology change. Our result illustrates the robustness of the relative energy method for establishing weak-strong uniqueness principles for interface evolution equations, showing that it may also be applied beyond certain topological changes.","sentences":["We prove a weak-strong uniqueness principle for varifold-BV solutions to planar multiphase mean curvature flow beyond a circular topology change: Assuming that there exists a classical solution with an interface that becomes increasingly circular and shrinks to a point, any varifold-BV solution with the same initial interface must coincide with it, and any varifold-BV solution with similar initial data must undergo the same type of topology change.","Our result illustrates the robustness of the relative energy method for establishing weak-strong uniqueness principles for interface evolution equations, showing that it may also be applied beyond certain topological changes."],"url":"http://arxiv.org/abs/2404.02884v1","category":"math.AP"}
{"created":"2024-04-03 17:25:18","title":"Pair production in rotating electric fields via quantum kinetic equations: Resolving helicity states","abstract":"We investigate the phenomenon of electron-positron pair production from vacuum in the presence of a strong electric field of circular polarization. By means of a nonperturbative approach based on the quantum kinetic equations (QKEs), we numerically calculate helicity-resolved momentum distributions of the particles produced and analyze the corresponding helicity asymmetry. It is demonstrated that the external rotating field tends to generate left-handed and right-handed particles traveling in opposite directions. Generic symmetry properties of the momentum spectra are examined analytically by means of the QKEs and also confirmed and illustrated by direct numerical computations. The helicity signatures revealed in our study are expected to provide a firmer basis for possible experimental investigations of the fundamental phenomenon of vacuum pair production in strong fields.","sentences":["We investigate the phenomenon of electron-positron pair production from vacuum in the presence of a strong electric field of circular polarization.","By means of a nonperturbative approach based on the quantum kinetic equations (QKEs), we numerically calculate helicity-resolved momentum distributions of the particles produced and analyze the corresponding helicity asymmetry.","It is demonstrated that the external rotating field tends to generate left-handed and right-handed particles traveling in opposite directions.","Generic symmetry properties of the momentum spectra are examined analytically by means of the QKEs and also confirmed and illustrated by direct numerical computations.","The helicity signatures revealed in our study are expected to provide a firmer basis for possible experimental investigations of the fundamental phenomenon of vacuum pair production in strong fields."],"url":"http://arxiv.org/abs/2404.02878v1","category":"hep-ph"}
{"created":"2024-04-03 17:16:37","title":"Subconductance states in a semimicroscopic model for a tetrameric pore","abstract":"A physical model for a structured tetrameric pore is studied. The pore is modeled as a device composed of four subunits, each one exhibiting two possible states (open and closed). The pore is located within a membrane that separates two reservoirs with ionic solutions. All variables of the model follow physical dynamical equations accounting for the internal structure of the pore, derived from a single energy functional and supplemented with thermal noises. An extensive study of the resulting ionic intensity is performed for different values of the control parameters, mainly membrane potential and reservoir ion concentrations. Two possible physical devices are studied: voltage-gated (including a voltage sensor in each subunit) and non-voltage-gated pores. The ionic flux through the pore exhibits several distinct dynamical configurations, in particular subconductance states, which indicate very different dynamical internal states of the subunits. Such subconductance states become much easier to observe in sensorless pores. These results are compared with available experimental data on tetrameric K channels and analytical predictions.","sentences":["A physical model for a structured tetrameric pore is studied.","The pore is modeled as a device composed of four subunits, each one exhibiting two possible states (open and closed).","The pore is located within a membrane that separates two reservoirs with ionic solutions.","All variables of the model follow physical dynamical equations accounting for the internal structure of the pore, derived from a single energy functional and supplemented with thermal noises.","An extensive study of the resulting ionic intensity is performed for different values of the control parameters, mainly membrane potential and reservoir ion concentrations.","Two possible physical devices are studied: voltage-gated (including a voltage sensor in each subunit) and non-voltage-gated pores.","The ionic flux through the pore exhibits several distinct dynamical configurations, in particular subconductance states, which indicate very different dynamical internal states of the subunits.","Such subconductance states become much easier to observe in sensorless pores.","These results are compared with available experimental data on tetrameric K channels and analytical predictions."],"url":"http://arxiv.org/abs/2404.02875v1","category":"physics.bio-ph"}
{"created":"2024-04-03 16:58:03","title":"Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds","abstract":"Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as \"features\" (or, less commonly, as \"embeddings\" or \"feature embeddings\"). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, \"MNIST\" and \"CIFAR-10,\" which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guarantee confidentiality of the inputs to inference with standard deep neural nets, \"ResNet-18\" and \"Swin-T,\" pre-trained on the data set, \"ImageNet-1000,\" which contains 1000 classes. Supplementing the addition of noise to features with other methods for providing confidentiality may be warranted in the case of ImageNet. In all cases, the results reported here limit consideration to amounts of added noise that incur little degradation in the accuracy of classification from the noisy features. Thus, the added noise enhances confidentiality without much reduction in the accuracy on the task of image classification.","sentences":["Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers.","The activations in such layers are known as \"features\" (or, less commonly, as \"embeddings\" or \"feature embeddings\").","The added noise helps prevent reconstruction of the inputs from the noisy features.","Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise.","Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds.","Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, \"MNIST\" and \"CIFAR-10,\" which contain 10 classes each for image classification.","The HCR bounds appear to be insufficient on their own to guarantee confidentiality of the inputs to inference with standard deep neural nets, \"ResNet-18\" and \"Swin-T,\" pre-trained on the data set, \"ImageNet-1000,\" which contains 1000 classes.","Supplementing the addition of noise to features with other methods for providing confidentiality may be warranted in the case of ImageNet.","In all cases, the results reported here limit consideration to amounts of added noise that incur little degradation in the accuracy of classification from the noisy features.","Thus, the added noise enhances confidentiality without much reduction in the accuracy on the task of image classification."],"url":"http://arxiv.org/abs/2404.02866v1","category":"cs.LG"}
{"created":"2024-04-03 16:57:26","title":"End-To-End Self-tuning Self-supervised Time Series Anomaly Detection","abstract":"Time series anomaly detection (TSAD) finds many applications such as monitoring environmental sensors, industry KPIs, patient biomarkers, etc. A two-fold challenge for TSAD is a versatile and unsupervised model that can detect various different types of time series anomalies (spikes, discontinuities, trend shifts, etc.) without any labeled data. Modern neural networks have outstanding ability in modeling complex time series. Self-supervised models in particular tackle unsupervised TSAD by transforming the input via various augmentations to create pseudo anomalies for training. However, their performance is sensitive to the choice of augmentation, which is hard to choose in practice, while there exists no effort in the literature on data augmentation tuning for TSAD without labels. Our work aims to fill this gap. We introduce TSAP for TSA \"on autoPilot\", which can (self-)tune augmentation hyperparameters end-to-end. It stands on two key components: a differentiable augmentation architecture and an unsupervised validation loss to effectively assess the alignment between augmentation type and anomaly type. Case studies show TSAP's ability to effectively select the (discrete) augmentation type and associated (continuous) hyperparameters. In turn, it outperforms established baselines, including SOTA self-supervised models, on diverse TSAD tasks exhibiting different anomaly types.","sentences":["Time series anomaly detection (TSAD) finds many applications such as monitoring environmental sensors, industry KPIs, patient biomarkers, etc.","A two-fold challenge for TSAD is a versatile and unsupervised model that can detect various different types of time series anomalies (spikes, discontinuities, trend shifts, etc.) without any labeled data.","Modern neural networks have outstanding ability in modeling complex time series.","Self-supervised models in particular tackle unsupervised TSAD by transforming the input via various augmentations to create pseudo anomalies for training.","However, their performance is sensitive to the choice of augmentation, which is hard to choose in practice, while there exists no effort in the literature on data augmentation tuning for TSAD without labels.","Our work aims to fill this gap.","We introduce TSAP for TSA \"on autoPilot\", which can (self-)tune augmentation hyperparameters end-to-end.","It stands on two key components: a differentiable augmentation architecture and an unsupervised validation loss to effectively assess the alignment between augmentation type and anomaly type.","Case studies show TSAP's ability to effectively select the (discrete) augmentation type and associated (continuous) hyperparameters.","In turn, it outperforms established baselines, including SOTA self-supervised models, on diverse TSAD tasks exhibiting different anomaly types."],"url":"http://arxiv.org/abs/2404.02865v1","category":"cs.LG"}
{"created":"2024-04-04 17:34:35","title":"Creator Hearts: Investigating the Impact Positive Signals from YouTube Creators in Shaping Comment Section Behavior","abstract":"Much of the research in online moderation focuses on punitive actions. However, emerging research has shown that positive reinforcement is effective at encouraging desirable behavior on online platforms. We extend this research by studying the \"creator heart\" feature on YouTube, quantifying their primary effects on comments that receive hearts and on videos where hearts have been given. We find that creator hearts increased the visibility of comments, and increased the amount of positive engagement they received from other users. We also find that the presence of a creator hearted comment soon after a video is published can incentivize viewers to comment, increasing the total engagement with the video over time. We discuss the potential for creators to use hearts to shape behavior in their communities by highlighting, rewarding, and incentivizing desirable behaviors from users. We discuss avenues for extending our study to understanding positive signals from moderators on other platforms.","sentences":["Much of the research in online moderation focuses on punitive actions.","However, emerging research has shown that positive reinforcement is effective at encouraging desirable behavior on online platforms.","We extend this research by studying the \"creator heart\" feature on YouTube, quantifying their primary effects on comments that receive hearts and on videos where hearts have been given.","We find that creator hearts increased the visibility of comments, and increased the amount of positive engagement they received from other users.","We also find that the presence of a creator hearted comment soon after a video is published can incentivize viewers to comment, increasing the total engagement with the video over time.","We discuss the potential for creators to use hearts to shape behavior in their communities by highlighting, rewarding, and incentivizing desirable behaviors from users.","We discuss avenues for extending our study to understanding positive signals from moderators on other platforms."],"url":"http://arxiv.org/abs/2404.03612v1","category":"cs.HC"}
{"created":"2024-04-04 16:50:10","title":"PAC-learning of free-fermionic states is NP-hard","abstract":"Free-fermionic states, also known as matchgates or Gaussian states, are a fundamental class of quantum states due to their efficient classical simulability and their crucial role across various domains of Physics. With the advent of quantum devices, experiments now yield data from quantum states, including estimates of expectation values. We establish that deciding whether a given dataset, formed by a few Majorana correlation functions estimates, can be consistent with a free-fermionic state is an NP-complete problem. Our result also extends to datasets formed by estimates of Pauli expectation values. This is in stark contrast to the case of stabilizer states, where the analogous problem can be efficiently solved. Moreover, our results directly imply that free-fermionic states are computationally hard to properly PAC-learn, where PAC-learning of quantum states is a learning framework introduced by Aaronson. Remarkably, this is the first class of classically simulable quantum states shown to have this property.","sentences":["Free-fermionic states, also known as matchgates or Gaussian states, are a fundamental class of quantum states due to their efficient classical simulability and their crucial role across various domains of Physics.","With the advent of quantum devices, experiments now yield data from quantum states, including estimates of expectation values.","We establish that deciding whether a given dataset, formed by a few Majorana correlation functions estimates, can be consistent with a free-fermionic state is an NP-complete problem.","Our result also extends to datasets formed by estimates of Pauli expectation values.","This is in stark contrast to the case of stabilizer states, where the analogous problem can be efficiently solved.","Moreover, our results directly imply that free-fermionic states are computationally hard to properly PAC-learn, where PAC-learning of quantum states is a learning framework introduced by Aaronson.","Remarkably, this is the first class of classically simulable quantum states shown to have this property."],"url":"http://arxiv.org/abs/2404.03585v1","category":"quant-ph"}
{"created":"2024-04-04 16:48:40","title":"Towards more realistic human motion prediction with attention to motion coordination","abstract":"Joint relation modeling is a curial component in human motion prediction. Most existing methods rely on skeletal-based graphs to build the joint relations, where local interactive relations between joint pairs are well learned. However, the motion coordination, a global joint relation reflecting the simultaneous cooperation of all joints, is usually weakened because it is learned from part to whole progressively and asynchronously. Thus, the final predicted motions usually appear unrealistic. To tackle this issue, we learn a medium, called coordination attractor (CA), from the spatiotemporal features of motion to characterize the global motion features, which is subsequently used to build new relative joint relations. Through the CA, all joints are related simultaneously, and thus the motion coordination of all joints can be better learned. Based on this, we further propose a novel joint relation modeling module, Comprehensive Joint Relation Extractor (CJRE), to combine this motion coordination with the local interactions between joint pairs in a unified manner. Additionally, we also present a Multi-timescale Dynamics Extractor (MTDE) to extract enriched dynamics from the raw position information for effective prediction. Extensive experiments show that the proposed framework outperforms state-of-the-art methods in both short- and long-term predictions on H3.6M, CMU-Mocap, and 3DPW.","sentences":["Joint relation modeling is a curial component in human motion prediction.","Most existing methods rely on skeletal-based graphs to build the joint relations, where local interactive relations between joint pairs are well learned.","However, the motion coordination, a global joint relation reflecting the simultaneous cooperation of all joints, is usually weakened because it is learned from part to whole progressively and asynchronously.","Thus, the final predicted motions usually appear unrealistic.","To tackle this issue, we learn a medium, called coordination attractor (CA), from the spatiotemporal features of motion to characterize the global motion features, which is subsequently used to build new relative joint relations.","Through the CA, all joints are related simultaneously, and thus the motion coordination of all joints can be better learned.","Based on this, we further propose a novel joint relation modeling module, Comprehensive Joint Relation Extractor (CJRE), to combine this motion coordination with the local interactions between joint pairs in a unified manner.","Additionally, we also present a Multi-timescale Dynamics Extractor (MTDE) to extract enriched dynamics from the raw position information for effective prediction.","Extensive experiments show that the proposed framework outperforms state-of-the-art methods in both short- and long-term predictions on H3.6M, CMU-Mocap, and 3DPW."],"url":"http://arxiv.org/abs/2404.03584v1","category":"cs.CV"}
{"created":"2024-04-04 13:36:51","title":"Science, Technology, Engineering, and Mathematics Undergraduates' Knowledge and Interest in Quantum Careers: Barriers and Opportunities to Building a Diverse Quantum Workforce","abstract":"Efforts to build the workforce in support of the second quantum revolution are growing, including the creation of education programs that will prepare students for jobs in this area. We surveyed 186 undergraduate students with majors across the STEM disciplines and followed up with group interviews to understand their perspectives. The project was designed to understand what these STEM students know about quantum and quantum career opportunities and their level of interest in pursuing a career related to quantum. We found that most of the students know very little about quantum. Nevertheless, except for students in the life sciences, there was an interest in quantum careers. Across STEM majors, women were less likely to express interest in quantum careers than men, but this difference disappeared when we examined only physical and computer science majors. Of the few students who had knowledge of quantum concepts, most learned about this topic from online media, especially online videos. Some students reported learning about quantum in high school classes, where it was taught as an extension beyond the usual topics of the course. The undergraduate STEM students in our study identified multiple ways they would like to learn more about quantum, including short videos, seminars, courses, certificates, and degree programs.","sentences":["Efforts to build the workforce in support of the second quantum revolution are growing, including the creation of education programs that will prepare students for jobs in this area.","We surveyed 186 undergraduate students with majors across the STEM disciplines and followed up with group interviews to understand their perspectives.","The project was designed to understand what these STEM students know about quantum and quantum career opportunities and their level of interest in pursuing a career related to quantum.","We found that most of the students know very little about quantum.","Nevertheless, except for students in the life sciences, there was an interest in quantum careers.","Across STEM majors, women were less likely to express interest in quantum careers than men, but this difference disappeared when we examined only physical and computer science majors.","Of the few students who had knowledge of quantum concepts, most learned about this topic from online media, especially online videos.","Some students reported learning about quantum in high school classes, where it was taught as an extension beyond the usual topics of the course.","The undergraduate STEM students in our study identified multiple ways they would like to learn more about quantum, including short videos, seminars, courses, certificates, and degree programs."],"url":"http://arxiv.org/abs/2404.03439v1","category":"physics.ed-ph"}
{"created":"2024-04-04 12:38:14","title":"Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?","abstract":"Various jailbreak attacks have been proposed to red-team Large Language Models (LLMs) and revealed the vulnerable safeguards of LLMs. Besides, some methods are not limited to the textual modality and extend the jailbreak attack to Multimodal Large Language Models (MLLMs) by perturbing the visual input. However, the absence of a universal evaluation benchmark complicates the performance reproduction and fair comparison. Besides, there is a lack of comprehensive evaluation of closed-source state-of-the-art (SOTA) models, especially MLLMs, such as GPT-4V. To address these issues, this work first builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies. Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models. We then conduct a deep analysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate better robustness against jailbreak attacks compared to open-source LLMs and MLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other open-source models. (3) The transferability of visual jailbreak methods is relatively limited compared to textual jailbreak methods. The dataset and code can be found here https://anonymous.4open.science/r/red_teaming_gpt4-C1CE/README.md .","sentences":["Various jailbreak attacks have been proposed to red-team Large Language Models (LLMs) and revealed the vulnerable safeguards of LLMs.","Besides, some methods are not limited to the textual modality and extend the jailbreak attack to Multimodal Large Language Models (MLLMs) by perturbing the visual input.","However, the absence of a universal evaluation benchmark complicates the performance reproduction and fair comparison.","Besides, there is a lack of comprehensive evaluation of closed-source state-of-the-art (SOTA) models, especially MLLMs, such as GPT-4V. To address these issues, this work first builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies.","Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models.","We then conduct a deep analysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate better robustness against jailbreak attacks compared to open-source LLMs and MLLMs.","(2) Llama2 and Qwen-VL-Chat are more robust compared to other open-source models.","(3) The transferability of visual jailbreak methods is relatively limited compared to textual jailbreak methods.","The dataset and code can be found here https://anonymous.4open.science/r/red_teaming_gpt4-C1CE/README.md ."],"url":"http://arxiv.org/abs/2404.03411v1","category":"cs.LG"}
{"created":"2024-04-04 11:53:37","title":"Background Noise Reduction of Attention Map for Weakly Supervised Semantic Segmentation","abstract":"In weakly-supervised semantic segmentation (WSSS) using only image-level class labels, a problem with CNN-based Class Activation Maps (CAM) is that they tend to activate the most discriminative local regions of objects. On the other hand, methods based on Transformers learn global features but suffer from the issue of background noise contamination. This paper focuses on addressing the issue of background noise in attention weights within the existing WSSS method based on Conformer, known as TransCAM. The proposed method successfully reduces background noise, leading to improved accuracy of pseudo labels. Experimental results demonstrate that our model achieves segmentation performance of 70.5% on the PASCAL VOC 2012 validation data, 71.1% on the test data, and 45.9% on MS COCO 2014 data, outperforming TransCAM in terms of segmentation performance.","sentences":["In weakly-supervised semantic segmentation (WSSS) using only image-level class labels, a problem with CNN-based Class Activation Maps (CAM) is that they tend to activate the most discriminative local regions of objects.","On the other hand, methods based on Transformers learn global features but suffer from the issue of background noise contamination.","This paper focuses on addressing the issue of background noise in attention weights within the existing WSSS method based on Conformer, known as TransCAM.","The proposed method successfully reduces background noise, leading to improved accuracy of pseudo labels.","Experimental results demonstrate that our model achieves segmentation performance of 70.5% on the PASCAL VOC 2012 validation data, 71.1% on the test data, and 45.9% on MS COCO 2014 data, outperforming TransCAM in terms of segmentation performance."],"url":"http://arxiv.org/abs/2404.03394v1","category":"cs.CV"}
{"created":"2024-04-04 11:49:56","title":"Two Tricks to Improve Unsupervised Segmentation Learning","abstract":"We present two practical improvement techniques for unsupervised segmentation learning. These techniques address limitations in the resolution and accuracy of predicted segmentation maps of recent state-of-the-art methods. Firstly, we leverage image post-processing techniques such as guided filtering to refine the output masks, improving accuracy while avoiding substantial computational costs. Secondly, we introduce a multi-scale consistency criterion, based on a teacher-student training scheme. This criterion matches segmentation masks predicted from regions of the input image extracted at different resolutions to each other. Experimental results on several benchmarks used in unsupervised segmentation learning demonstrate the effectiveness of our proposed techniques.","sentences":["We present two practical improvement techniques for unsupervised segmentation learning.","These techniques address limitations in the resolution and accuracy of predicted segmentation maps of recent state-of-the-art methods.","Firstly, we leverage image post-processing techniques such as guided filtering to refine the output masks, improving accuracy while avoiding substantial computational costs.","Secondly, we introduce a multi-scale consistency criterion, based on a teacher-student training scheme.","This criterion matches segmentation masks predicted from regions of the input image extracted at different resolutions to each other.","Experimental results on several benchmarks used in unsupervised segmentation learning demonstrate the effectiveness of our proposed techniques."],"url":"http://arxiv.org/abs/2404.03392v1","category":"cs.CV"}
{"created":"2024-04-04 11:12:33","title":"Data harvesting vs data farming: A study of the importance of variation vs sample size in deep learning-based auto-segmentation for breast cancer patients","abstract":"The aim of this study was to investigate the difference in output, when training a model in three different scenarios: a large clinical delineated data set (with 700/78 patients for training/testing, from the Danish Breast Cancer Group (DBCG) RT Nation Study), a clinical but curated dataset (with 328/36 patients for training/testing, from the DBCG RT Nation Study) and a smaller, but dedicated data set created by delineation experts (with 123/14 patients for training/testing, consensus delineations created by delineation experts). The model performance was estimated based on the performance metrics dice similarity coefficient (DSC), Hausdorff 95th percentile (HD95) and mean surface distance (MSD). Models were tested in test sets from their own cohort, and afterwards also compared in the dedicated data test set. The difference between model output was finally estimated by measuring the mean width and cranial caudal length of the model output for the models. When testing the model output between the clinical models and the dedicated models in their own test set, the two clinical models had a poorer performance, than the dedicated models, but not all metrics showed statistically significance. When testing the models in the dedicated data, the dedicated model showed a slightly better performance, along with fewer segmentation outliers. As a way of taking advantage of the strength from both types of data set, it could be an option to use a large clinical data set as a baseline model, and then finetune with smaller sized cohorts with dedicated delineations.","sentences":["The aim of this study was to investigate the difference in output, when training a model in three different scenarios: a large clinical delineated data set (with 700/78 patients for training/testing, from the Danish Breast Cancer Group (DBCG) RT Nation Study), a clinical but curated dataset (with 328/36 patients for training/testing, from the DBCG RT Nation Study) and a smaller, but dedicated data set created by delineation experts (with 123/14 patients for training/testing, consensus delineations created by delineation experts).","The model performance was estimated based on the performance metrics dice similarity coefficient (DSC), Hausdorff 95th percentile (HD95) and mean surface distance (MSD).","Models were tested in test sets from their own cohort, and afterwards also compared in the dedicated data test set.","The difference between model output was finally estimated by measuring the mean width and cranial caudal length of the model output for the models.","When testing the model output between the clinical models and the dedicated models in their own test set, the two clinical models had a poorer performance, than the dedicated models, but not all metrics showed statistically significance.","When testing the models in the dedicated data, the dedicated model showed a slightly better performance, along with fewer segmentation outliers.","As a way of taking advantage of the strength from both types of data set, it could be an option to use a large clinical data set as a baseline model, and then finetune with smaller sized cohorts with dedicated delineations."],"url":"http://arxiv.org/abs/2404.03369v1","category":"physics.med-ph"}
{"created":"2024-04-04 07:09:43","title":"Multi-task learning via robust regularized clustering with non-convex group penalties","abstract":"Multi-task learning (MTL) aims to improve estimation and prediction performance by sharing common information among related tasks. One natural assumption in MTL is that tasks are classified into clusters based on their characteristics. However, existing MTL methods based on this assumption often ignore outlier tasks that have large task-specific components or no relation to other tasks. To address this issue, we propose a novel MTL method called Multi-Task Learning via Robust Regularized Clustering (MTLRRC). MTLRRC incorporates robust regularization terms inspired by robust convex clustering, which is further extended to handle non-convex and group-sparse penalties. The extension allows MTLRRC to simultaneously perform robust task clustering and outlier task detection. The connection between the extended robust clustering and the multivariate M-estimator is also established. This provides an interpretation of the robustness of MTLRRC against outlier tasks. An efficient algorithm based on a modified alternating direction method of multipliers is developed for the estimation of the parameters. The effectiveness of MTLRRC is demonstrated through simulation studies and application to real data.","sentences":["Multi-task learning (MTL) aims to improve estimation and prediction performance by sharing common information among related tasks.","One natural assumption in MTL is that tasks are classified into clusters based on their characteristics.","However, existing MTL methods based on this assumption often ignore outlier tasks that have large task-specific components or no relation to other tasks.","To address this issue, we propose a novel MTL method called Multi-Task Learning via Robust Regularized Clustering (MTLRRC).","MTLRRC incorporates robust regularization terms inspired by robust convex clustering, which is further extended to handle non-convex and group-sparse penalties.","The extension allows MTLRRC to simultaneously perform robust task clustering and outlier task detection.","The connection between the extended robust clustering and the multivariate M-estimator is also established.","This provides an interpretation of the robustness of MTLRRC against outlier tasks.","An efficient algorithm based on a modified alternating direction method of multipliers is developed for the estimation of the parameters.","The effectiveness of MTLRRC is demonstrated through simulation studies and application to real data."],"url":"http://arxiv.org/abs/2404.03250v1","category":"stat.ME"}
{"created":"2024-04-04 07:07:34","title":"Learning Transferable Negative Prompts for Out-of-Distribution Detection","abstract":"Existing prompt learning methods have shown certain capabilities in Out-of-Distribution (OOD) detection, but the lack of OOD images in the target dataset in their training can lead to mismatches between OOD images and In-Distribution (ID) categories, resulting in a high false positive rate. To address this issue, we introduce a novel OOD detection method, named 'NegPrompt', to learn a set of negative prompts, each representing a negative connotation of a given class label, for delineating the boundaries between ID and OOD images. It learns such negative prompts with ID data only, without any reliance on external outlier data. Further, current methods assume the availability of samples of all ID classes, rendering them ineffective in open-vocabulary learning scenarios where the inference stage can contain novel ID classes not present during training. In contrast, our learned negative prompts are transferable to novel class labels. Experiments on various ImageNet benchmarks show that NegPrompt surpasses state-of-the-art prompt-learning-based OOD detection methods and maintains a consistent lead in hard OOD detection in closed- and open-vocabulary classification scenarios. Code is available at https://github.com/mala-lab/negprompt.","sentences":["Existing prompt learning methods have shown certain capabilities in Out-of-Distribution (OOD) detection, but the lack of OOD images in the target dataset in their training can lead to mismatches between OOD images and In-Distribution (ID) categories, resulting in a high false positive rate.","To address this issue, we introduce a novel OOD detection method, named 'NegPrompt', to learn a set of negative prompts, each representing a negative connotation of a given class label, for delineating the boundaries between ID and OOD images.","It learns such negative prompts with ID data only, without any reliance on external outlier data.","Further, current methods assume the availability of samples of all ID classes, rendering them ineffective in open-vocabulary learning scenarios where the inference stage can contain novel ID classes not present during training.","In contrast, our learned negative prompts are transferable to novel class labels.","Experiments on various ImageNet benchmarks show that NegPrompt surpasses state-of-the-art prompt-learning-based OOD detection methods and maintains a consistent lead in hard OOD detection in closed- and open-vocabulary classification scenarios.","Code is available at https://github.com/mala-lab/negprompt."],"url":"http://arxiv.org/abs/2404.03248v1","category":"cs.CV"}
{"created":"2024-04-04 06:20:22","title":"FACTUAL: A Novel Framework for Contrastive Learning Based Robust SAR Image Classification","abstract":"Deep Learning (DL) Models for Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR), while delivering improved performance, have been shown to be quite vulnerable to adversarial attacks. Existing works improve robustness by training models on adversarial samples. However, by focusing mostly on attacks that manipulate images randomly, they neglect the real-world feasibility of such attacks. In this paper, we propose FACTUAL, a novel Contrastive Learning framework for Adversarial Training and robust SAR classification. FACTUAL consists of two components: (1) Differing from existing works, a novel perturbation scheme that incorporates realistic physical adversarial attacks (such as OTSA) to build a supervised adversarial pre-training network. This network utilizes class labels for clustering clean and perturbed images together into a more informative feature space. (2) A linear classifier cascaded after the encoder to use the computed representations to predict the target labels. By pre-training and fine-tuning our model on both clean and adversarial samples, we show that our model achieves high prediction accuracy on both cases. Our model achieves 99.7% accuracy on clean samples, and 89.6% on perturbed samples, both outperforming previous state-of-the-art methods.","sentences":["Deep Learning (DL) Models for Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR), while delivering improved performance, have been shown to be quite vulnerable to adversarial attacks.","Existing works improve robustness by training models on adversarial samples.","However, by focusing mostly on attacks that manipulate images randomly, they neglect the real-world feasibility of such attacks.","In this paper, we propose FACTUAL, a novel Contrastive Learning framework for Adversarial Training and robust SAR classification.","FACTUAL consists of two components: (1) Differing from existing works, a novel perturbation scheme that incorporates realistic physical adversarial attacks (such as OTSA) to build a supervised adversarial pre-training network.","This network utilizes class labels for clustering clean and perturbed images together into a more informative feature space.","(2) A linear classifier cascaded after the encoder to use the computed representations to predict the target labels.","By pre-training and fine-tuning our model on both clean and adversarial samples, we show that our model achieves high prediction accuracy on both cases.","Our model achieves 99.7% accuracy on clean samples, and 89.6% on perturbed samples, both outperforming previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.03225v1","category":"cs.CV"}
{"created":"2024-04-04 04:16:31","title":"Classification of Nasopharyngeal Cases using DenseNet Deep Learning Architecture","abstract":"Nasopharyngeal carcinoma (NPC) is one of the understudied yet deadliest cancers in South East Asia. In Malaysia, the prevalence is identified mainly in Sarawak, among the ethnic of Bidayuh. NPC is often late-diagnosed because it is asymptomatic at the early stage. There are several tissue representations from the nasopharynx biopsy, such as nasopharyngeal inflammation (NPI), lymphoid hyperplasia (LHP), nasopharyngeal carcinoma (NPC) and normal tissue. This paper is our first initiative to identify the difference between NPC, NPI and normal cases. Seven whole slide images (WSIs) with gigapixel resolutions from seven different patients and two hospitals were experimented with using two test setups, consisting of a different set of images. The tissue regions are patched into smaller blocks and classified using DenseNet architecture with 21 dense layers. Two tests are carried out, each for proof of concept (Test 1) and real-test scenario (Test 2). The accuracy achieved for NPC class is 94.8% for Test 1 and 67.0% for Test 2.","sentences":["Nasopharyngeal carcinoma (NPC) is one of the understudied yet deadliest cancers in South East Asia.","In Malaysia, the prevalence is identified mainly in Sarawak, among the ethnic of Bidayuh.","NPC is often late-diagnosed because it is asymptomatic at the early stage.","There are several tissue representations from the nasopharynx biopsy, such as nasopharyngeal inflammation (NPI), lymphoid hyperplasia (LHP), nasopharyngeal carcinoma (NPC) and normal tissue.","This paper is our first initiative to identify the difference between NPC, NPI and normal cases.","Seven whole slide images (WSIs) with gigapixel resolutions from seven different patients and two hospitals were experimented with using two test setups, consisting of a different set of images.","The tissue regions are patched into smaller blocks and classified using DenseNet architecture with 21 dense layers.","Two tests are carried out, each for proof of concept (Test 1) and real-test scenario (Test 2).","The accuracy achieved for NPC class is 94.8% for Test 1 and 67.0% for Test 2."],"url":"http://arxiv.org/abs/2404.03188v1","category":"eess.IV"}
{"created":"2024-04-04 03:28:57","title":"UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization","abstract":"Video localization tasks aim to temporally locate specific instances in videos, including temporal action localization (TAL), sound event detection (SED) and audio-visual event localization (AVEL). Existing methods over-specialize on each task, overlooking the fact that these instances often occur in the same video to form the complete video content. In this work, we present UniAV, a Unified Audio-Visual perception network, to achieve joint learning of TAL, SED and AVEL tasks for the first time. UniAV can leverage diverse data available in task-specific datasets, allowing the model to learn and share mutually beneficial knowledge across tasks and modalities. To tackle the challenges posed by substantial variations in datasets (size/domain/duration) and distinct task characteristics, we propose to uniformly encode visual and audio modalities of all videos to derive generic representations, while also designing task-specific experts to capture unique knowledge for each task. Besides, we develop a unified language-aware classifier by utilizing a pre-trained text encoder, enabling the model to flexibly detect various types of instances and previously unseen ones by simply changing prompts during inference. UniAV outperforms its single-task counterparts by a large margin with fewer parameters, achieving on-par or superior performances compared to state-of-the-art task-specific methods across ActivityNet 1.3, DESED and UnAV-100 benchmarks.","sentences":["Video localization tasks aim to temporally locate specific instances in videos, including temporal action localization (TAL), sound event detection (SED) and audio-visual event localization (AVEL).","Existing methods over-specialize on each task, overlooking the fact that these instances often occur in the same video to form the complete video content.","In this work, we present UniAV, a Unified Audio-Visual perception network, to achieve joint learning of TAL, SED and AVEL tasks for the first time.","UniAV can leverage diverse data available in task-specific datasets, allowing the model to learn and share mutually beneficial knowledge across tasks and modalities.","To tackle the challenges posed by substantial variations in datasets (size/domain/duration) and distinct task characteristics, we propose to uniformly encode visual and audio modalities of all videos to derive generic representations, while also designing task-specific experts to capture unique knowledge for each task.","Besides, we develop a unified language-aware classifier by utilizing a pre-trained text encoder, enabling the model to flexibly detect various types of instances and previously unseen ones by simply changing prompts during inference.","UniAV outperforms its single-task counterparts by a large margin with fewer parameters, achieving on-par or superior performances compared to state-of-the-art task-specific methods across ActivityNet 1.3, DESED and UnAV-100 benchmarks."],"url":"http://arxiv.org/abs/2404.03179v1","category":"cs.CV"}
{"created":"2024-04-04 03:03:38","title":"Multi-modal Learning for WebAssembly Reverse Engineering","abstract":"The increasing adoption of WebAssembly (Wasm) for performance-critical and security-sensitive tasks drives the demand for WebAssembly program comprehension and reverse engineering. Recent studies have introduced machine learning (ML)-based WebAssembly reverse engineering tools. Yet, the generalization of task-specific ML solutions remains challenging, because their effectiveness hinges on the availability of an ample supply of high-quality task-specific labeled data. Moreover, previous works overlook the high-level semantics present in source code and its documentation. Acknowledging the abundance of available source code with documentation, which can be compiled into WebAssembly, we propose to learn representations of them concurrently and harness their mutual relationships for effective WebAssembly reverse engineering.   In this paper, we present WasmRev, the first multi-modal pre-trained language model for WebAssembly reverse engineering. WasmRev is pre-trained using self-supervised learning on a large-scale multi-modal corpus encompassing source code, code documentation and the compiled WebAssembly, without requiring labeled data. WasmRev incorporates three tailored multi-modal pre-training tasks to capture various characteristics of WebAssembly and cross-modal relationships. WasmRev is only trained once to produce general-purpose representations that can broadly support WebAssembly reverse engineering tasks through few-shot fine-tuning with much less labeled data, improving data efficiency. We fine-tune WasmRev onto three important reverse engineering tasks: type recovery, function purpose identification and WebAssembly summarization. Our results show that WasmRev pre-trained on the corpus of multi-modal samples establishes a robust foundation for these tasks, achieving high task accuracy and outperforming the state-of-the-art ML methods for WebAssembly reverse engineering.","sentences":["The increasing adoption of WebAssembly (Wasm) for performance-critical and security-sensitive tasks drives the demand for WebAssembly program comprehension and reverse engineering.","Recent studies have introduced machine learning (ML)-based WebAssembly reverse engineering tools.","Yet, the generalization of task-specific ML solutions remains challenging, because their effectiveness hinges on the availability of an ample supply of high-quality task-specific labeled data.","Moreover, previous works overlook the high-level semantics present in source code and its documentation.","Acknowledging the abundance of available source code with documentation, which can be compiled into WebAssembly, we propose to learn representations of them concurrently and harness their mutual relationships for effective WebAssembly reverse engineering.   ","In this paper, we present WasmRev, the first multi-modal pre-trained language model for WebAssembly reverse engineering.","WasmRev is pre-trained using self-supervised learning on a large-scale multi-modal corpus encompassing source code, code documentation and the compiled WebAssembly, without requiring labeled data.","WasmRev incorporates three tailored multi-modal pre-training tasks to capture various characteristics of WebAssembly and cross-modal relationships.","WasmRev is only trained once to produce general-purpose representations that can broadly support WebAssembly reverse engineering tasks through few-shot fine-tuning with much less labeled data, improving data efficiency.","We fine-tune WasmRev onto three important reverse engineering tasks: type recovery, function purpose identification and WebAssembly summarization.","Our results show that WasmRev pre-trained on the corpus of multi-modal samples establishes a robust foundation for these tasks, achieving high task accuracy and outperforming the state-of-the-art ML methods for WebAssembly reverse engineering."],"url":"http://arxiv.org/abs/2404.03171v1","category":"cs.SE"}
{"created":"2024-04-03 23:38:31","title":"Deep Learning-Based Weather-Related Power Outage Prediction with Socio-Economic and Power Infrastructure Data","abstract":"This paper presents a deep learning-based approach for hourly power outage probability prediction within census tracts encompassing a utility company's service territory. Two distinct deep learning models, conditional Multi-Layer Perceptron (MLP) and unconditional MLP, were developed to forecast power outage probabilities, leveraging a rich array of input features gathered from publicly available sources including weather data, weather station locations, power infrastructure maps, socio-economic and demographic statistics, and power outage records. Given a one-hour-ahead weather forecast, the models predict the power outage probability for each census tract, taking into account both the weather prediction and the location's characteristics. The deep learning models employed different loss functions to optimize prediction performance. Our experimental results underscore the significance of socio-economic factors in enhancing the accuracy of power outage predictions at the census tract level.","sentences":["This paper presents a deep learning-based approach for hourly power outage probability prediction within census tracts encompassing a utility company's service territory.","Two distinct deep learning models, conditional Multi-Layer Perceptron (MLP) and unconditional MLP, were developed to forecast power outage probabilities, leveraging a rich array of input features gathered from publicly available sources including weather data, weather station locations, power infrastructure maps, socio-economic and demographic statistics, and power outage records.","Given a one-hour-ahead weather forecast, the models predict the power outage probability for each census tract, taking into account both the weather prediction and the location's characteristics.","The deep learning models employed different loss functions to optimize prediction performance.","Our experimental results underscore the significance of socio-economic factors in enhancing the accuracy of power outage predictions at the census tract level."],"url":"http://arxiv.org/abs/2404.03115v1","category":"cs.LG"}
{"created":"2024-04-03 22:13:04","title":"Unsupervised, Bottom-up Category Discovery for Symbol Grounding with a Curious Robot","abstract":"Towards addressing the Symbol Grounding Problem and motivated by early childhood language development, we leverage a robot which has been equipped with an approximate model of curiosity with particular focus on bottom-up building of unsupervised categories grounded in the physical world. That is, rather than starting with a top-down symbol (e.g., a word referring to an object) and providing meaning through the application of predetermined samples, the robot autonomously and gradually breaks up its exploration space into a series of increasingly specific unlabeled categories at which point an external expert may optionally provide a symbol association. We extend prior work by using a robot that can observe the visual world, introducing a higher dimensional sensory space, and using a more generalizable method of category building. Our experiments show that the robot learns categories based on actions and what it visually observes, and that those categories can be symbolically grounded into.https://info.arxiv.org/help/prep#comments","sentences":["Towards addressing the Symbol Grounding Problem and motivated by early childhood language development, we leverage a robot which has been equipped with an approximate model of curiosity with particular focus on bottom-up building of unsupervised categories grounded in the physical world.","That is, rather than starting with a top-down symbol (e.g., a word referring to an object) and providing meaning through the application of predetermined samples, the robot autonomously and gradually breaks up its exploration space into a series of increasingly specific unlabeled categories at which point an external expert may optionally provide a symbol association.","We extend prior work by using a robot that can observe the visual world, introducing a higher dimensional sensory space, and using a more generalizable method of category building.","Our experiments show that the robot learns categories based on actions and what it visually observes, and that those categories can be symbolically grounded into.https://info.arxiv.org/help/prep#comments"],"url":"http://arxiv.org/abs/2404.03092v1","category":"cs.CL"}
{"created":"2024-04-03 21:47:44","title":"Machine Learning and Data Analysis Using Posets: A Survey","abstract":"Posets are discrete mathematical structures which are ubiquitous in a broad range of data analysis and machine learning applications. Research connecting posets to the data science domain has been ongoing for many years. In this paper, a comprehensive review of a wide range of studies on data analysis amd machine learning using posets are examined in terms of their theory, algorithms and applications. In addition, the applied lattice theory domain of formal concept analysis will also be highlighted in terms of its machine learning applications.","sentences":["Posets are discrete mathematical structures which are ubiquitous in a broad range of data analysis and machine learning applications.","Research connecting posets to the data science domain has been ongoing for many years.","In this paper, a comprehensive review of a wide range of studies on data analysis amd machine learning using posets are examined in terms of their theory, algorithms and applications.","In addition, the applied lattice theory domain of formal concept analysis will also be highlighted in terms of its machine learning applications."],"url":"http://arxiv.org/abs/2404.03082v1","category":"cs.LG"}
{"created":"2024-04-03 21:29:40","title":"Mai Ho'om\u0101una i ka 'Ai: Language Models Improve Automatic Speech Recognition in Hawaiian","abstract":"In this paper we address the challenge of improving Automatic Speech Recognition (ASR) for a low-resource language, Hawaiian, by incorporating large amounts of independent text data into an ASR foundation model, Whisper. To do this, we train an external language model (LM) on ~1.5M words of Hawaiian text. We then use the LM to rescore Whisper and compute word error rates (WERs) on a manually curated test set of labeled Hawaiian data. As a baseline, we use Whisper without an external LM. Experimental results reveal a small but significant improvement in WER when ASR outputs are rescored with a Hawaiian LM. The results support leveraging all available data in the development of ASR systems for underrepresented languages.","sentences":["In this paper we address the challenge of improving Automatic Speech Recognition (ASR) for a low-resource language, Hawaiian, by incorporating large amounts of independent text data into an ASR foundation model, Whisper.","To do this, we train an external language model (LM) on ~1.5M words of Hawaiian text.","We then use the LM to rescore Whisper and compute word error rates (WERs) on a manually curated test set of labeled Hawaiian data.","As a baseline, we use Whisper without an external LM.","Experimental results reveal a small but significant improvement in WER when ASR outputs are rescored with a Hawaiian LM.","The results support leveraging all available data in the development of ASR systems for underrepresented languages."],"url":"http://arxiv.org/abs/2404.03073v1","category":"cs.CL"}
{"created":"2024-04-03 21:18:27","title":"Behind the Veil: Enhanced Indoor 3D Scene Reconstruction with Occluded Surfaces Completion","abstract":"In this paper, we present a novel indoor 3D reconstruction method with occluded surface completion, given a sequence of depth readings. Prior state-of-the-art (SOTA) methods only focus on the reconstruction of the visible areas in a scene, neglecting the invisible areas due to the occlusions, e.g., the contact surface between furniture, occluded wall and floor. Our method tackles the task of completing the occluded scene surfaces, resulting in a complete 3D scene mesh. The core idea of our method is learning 3D geometry prior from various complete scenes to infer the occluded geometry of an unseen scene from solely depth measurements. We design a coarse-fine hierarchical octree representation coupled with a dual-decoder architecture, i.e., Geo-decoder and 3D Inpainter, which jointly reconstructs the complete 3D scene geometry. The Geo-decoder with detailed representation at fine levels is optimized online for each scene to reconstruct visible surfaces. The 3D Inpainter with abstract representation at coarse levels is trained offline using various scenes to complete occluded surfaces. As a result, while the Geo-decoder is specialized for an individual scene, the 3D Inpainter can be generally applied across different scenes. We evaluate the proposed method on the 3D Completed Room Scene (3D-CRS) and iTHOR datasets, significantly outperforming the SOTA methods by a gain of 16.8% and 24.2% in terms of the completeness of 3D reconstruction. 3D-CRS dataset including a complete 3D mesh of each scene is provided at project webpage.","sentences":["In this paper, we present a novel indoor 3D reconstruction method with occluded surface completion, given a sequence of depth readings.","Prior state-of-the-art (SOTA) methods only focus on the reconstruction of the visible areas in a scene, neglecting the invisible areas due to the occlusions, e.g., the contact surface between furniture, occluded wall and floor.","Our method tackles the task of completing the occluded scene surfaces, resulting in a complete 3D scene mesh.","The core idea of our method is learning 3D geometry prior from various complete scenes to infer the occluded geometry of an unseen scene from solely depth measurements.","We design a coarse-fine hierarchical octree representation coupled with a dual-decoder architecture, i.e., Geo-decoder and 3D Inpainter, which jointly reconstructs the complete 3D scene geometry.","The Geo-decoder with detailed representation at fine levels is optimized online for each scene to reconstruct visible surfaces.","The 3D Inpainter with abstract representation at coarse levels is trained offline using various scenes to complete occluded surfaces.","As a result, while the Geo-decoder is specialized for an individual scene, the 3D Inpainter can be generally applied across different scenes.","We evaluate the proposed method on the 3D Completed Room Scene (3D-CRS) and iTHOR datasets, significantly outperforming the SOTA methods by a gain of 16.8% and 24.2% in terms of the completeness of 3D reconstruction.","3D-CRS dataset including a complete 3D mesh of each scene is provided at project webpage."],"url":"http://arxiv.org/abs/2404.03070v1","category":"cs.CV"}
{"created":"2024-04-03 20:35:36","title":"GPT-DETOX: An In-Context Learning-Based Paraphraser for Text Detoxification","abstract":"Harmful and offensive communication or content is detrimental to social bonding and the mental state of users on social media platforms. Text detoxification is a crucial task in natural language processing (NLP), where the goal is removing profanity and toxicity from text while preserving its content. Supervised and unsupervised learning are common approaches for designing text detoxification solutions. However, these methods necessitate fine-tuning, leading to computational overhead. In this paper, we propose GPT-DETOX as a framework for prompt-based in-context learning for text detoxification using GPT-3.5 Turbo. We utilize zero-shot and few-shot prompting techniques for detoxifying input sentences. To generate few-shot prompts, we propose two methods: word-matching example selection (WMES) and context-matching example selection (CMES). We additionally take into account ensemble in-context learning (EICL) where the ensemble is shaped by base prompts from zero-shot and all few-shot settings. We use ParaDetox and APPDIA as benchmark detoxification datasets. Our experimental results show that the zero-shot solution achieves promising performance, while our best few-shot setting outperforms the state-of-the-art models on ParaDetox and shows comparable results on APPDIA. Our EICL solutions obtain the greatest performance, adding at least 10% improvement, against both datasets.","sentences":["Harmful and offensive communication or content is detrimental to social bonding and the mental state of users on social media platforms.","Text detoxification is a crucial task in natural language processing (NLP), where the goal is removing profanity and toxicity from text while preserving its content.","Supervised and unsupervised learning are common approaches for designing text detoxification solutions.","However, these methods necessitate fine-tuning, leading to computational overhead.","In this paper, we propose GPT-DETOX as a framework for prompt-based in-context learning for text detoxification using GPT-3.5 Turbo.","We utilize zero-shot and few-shot prompting techniques for detoxifying input sentences.","To generate few-shot prompts, we propose two methods: word-matching example selection (WMES) and context-matching example selection (CMES).","We additionally take into account ensemble in-context learning (EICL) where the ensemble is shaped by base prompts from zero-shot and all few-shot settings.","We use ParaDetox and APPDIA as benchmark detoxification datasets.","Our experimental results show that the zero-shot solution achieves promising performance, while our best few-shot setting outperforms the state-of-the-art models on ParaDetox and shows comparable results on APPDIA.","Our EICL solutions obtain the greatest performance, adding at least 10% improvement, against both datasets."],"url":"http://arxiv.org/abs/2404.03052v1","category":"cs.CL"}
{"created":"2024-04-03 20:34:18","title":"ANOVA-boosting for Random Fourier Features","abstract":"We propose two algorithms for boosting random Fourier feature models for approximating high-dimensional functions. These methods utilize the classical and generalized analysis of variance (ANOVA) decomposition to learn low-order functions, where there are few interactions between the variables. Our algorithms are able to find an index set of important input variables and variable interactions reliably. Furthermore, we generalize already existing random Fourier feature models to an ANOVA setting, where terms of different order can be used. Our algorithms have the advantage of interpretability, meaning that the influence of every input variable is known in the learned model, even for dependent input variables. We give theoretical as well as numerical results that our algorithms perform well for sensitivity analysis. The ANOVA-boosting step reduces the approximation error of existing methods significantly.","sentences":["We propose two algorithms for boosting random Fourier feature models for approximating high-dimensional functions.","These methods utilize the classical and generalized analysis of variance (ANOVA) decomposition to learn low-order functions, where there are few interactions between the variables.","Our algorithms are able to find an index set of important input variables and variable interactions reliably.","Furthermore, we generalize already existing random Fourier feature models to an ANOVA setting, where terms of different order can be used.","Our algorithms have the advantage of interpretability, meaning that the influence of every input variable is known in the learned model, even for dependent input variables.","We give theoretical as well as numerical results that our algorithms perform well for sensitivity analysis.","The ANOVA-boosting step reduces the approximation error of existing methods significantly."],"url":"http://arxiv.org/abs/2404.03050v1","category":"cs.LG"}
{"created":"2024-04-03 20:29:40","title":"Decentralised Moderation for Interoperable Social Networks: A Conversation-based Approach for Pleroma and the Fediverse","abstract":"The recent development of decentralised and interoperable social networks (such as the \"fediverse\") creates new challenges for content moderators. This is because millions of posts generated on one server can easily \"spread\" to another, even if the recipient server has very different moderation policies. An obvious solution would be to leverage moderation tools to automatically tag (and filter) posts that contravene moderation policies, e.g. related to toxic speech. Recent work has exploited the conversational context of a post to improve this automatic tagging, e.g. using the replies to a post to help classify if it contains toxic speech. This has shown particular potential in environments with large training sets that contain complete conversations. This, however, creates challenges in a decentralised context, as a single conversation may be fragmented across multiple servers. Thus, each server only has a partial view of an entire conversation because conversations are often federated across servers in a non-synchronized fashion. To address this, we propose a decentralised conversation-aware content moderation approach suitable for the fediverse. Our approach employs a graph deep learning model (GraphNLI) trained locally on each server. The model exploits local data to train a model that combines post and conversational information captured through random walks to detect toxicity. We evaluate our approach with data from Pleroma, a major decentralised and interoperable micro-blogging network containing 2 million conversations. Our model effectively detects toxicity on larger instances, exclusively trained using their local post information (0.8837 macro-F1). Our approach has considerable scope to improve moderation in decentralised and interoperable social networks such as Pleroma or Mastodon.","sentences":["The recent development of decentralised and interoperable social networks (such as the \"fediverse\") creates new challenges for content moderators.","This is because millions of posts generated on one server can easily \"spread\" to another, even if the recipient server has very different moderation policies.","An obvious solution would be to leverage moderation tools to automatically tag (and filter) posts that contravene moderation policies, e.g. related to toxic speech.","Recent work has exploited the conversational context of a post to improve this automatic tagging, e.g. using the replies to a post to help classify if it contains toxic speech.","This has shown particular potential in environments with large training sets that contain complete conversations.","This, however, creates challenges in a decentralised context, as a single conversation may be fragmented across multiple servers.","Thus, each server only has a partial view of an entire conversation because conversations are often federated across servers in a non-synchronized fashion.","To address this, we propose a decentralised conversation-aware content moderation approach suitable for the fediverse.","Our approach employs a graph deep learning model (GraphNLI) trained locally on each server.","The model exploits local data to train a model that combines post and conversational information captured through random walks to detect toxicity.","We evaluate our approach with data from Pleroma, a major decentralised and interoperable micro-blogging network containing 2 million conversations.","Our model effectively detects toxicity on larger instances, exclusively trained using their local post information (0.8837 macro-F1).","Our approach has considerable scope to improve moderation in decentralised and interoperable social networks such as Pleroma or Mastodon."],"url":"http://arxiv.org/abs/2404.03048v1","category":"cs.CY"}
{"created":"2024-04-03 20:04:44","title":"AWOL: Analysis WithOut synthesis using Language","abstract":"Many classical parametric 3D shape models exist, but creating novel shapes with such models requires expert knowledge of their parameters. For example, imagine creating a specific type of tree using procedural graphics or a new kind of animal from a statistical shape model. Our key idea is to leverage language to control such existing models to produce novel shapes. This involves learning a mapping between the latent space of a vision-language model and the parameter space of the 3D model, which we do using a small set of shape and text pairs. Our hypothesis is that mapping from language to parameters allows us to generate parameters for objects that were never seen during training. If the mapping between language and parameters is sufficiently smooth, then interpolation or generalization in language should translate appropriately into novel 3D shapes. We test our approach with two very different types of parametric shape models (quadrupeds and arboreal trees). We use a learned statistical shape model of quadrupeds and show that we can use text to generate new animals not present during training. In particular, we demonstrate state-of-the-art shape estimation of 3D dogs. This work also constitutes the first language-driven method for generating 3D trees. Finally, embedding images in the CLIP latent space enables us to generate animals and trees directly from images.","sentences":["Many classical parametric 3D shape models exist, but creating novel shapes with such models requires expert knowledge of their parameters.","For example, imagine creating a specific type of tree using procedural graphics or a new kind of animal from a statistical shape model.","Our key idea is to leverage language to control such existing models to produce novel shapes.","This involves learning a mapping between the latent space of a vision-language model and the parameter space of the 3D model, which we do using a small set of shape and text pairs.","Our hypothesis is that mapping from language to parameters allows us to generate parameters for objects that were never seen during training.","If the mapping between language and parameters is sufficiently smooth, then interpolation or generalization in language should translate appropriately into novel 3D shapes.","We test our approach with two very different types of parametric shape models (quadrupeds and arboreal trees).","We use a learned statistical shape model of quadrupeds and show that we can use text to generate new animals not present during training.","In particular, we demonstrate state-of-the-art shape estimation of 3D dogs.","This work also constitutes the first language-driven method for generating 3D trees.","Finally, embedding images in the CLIP latent space enables us to generate animals and trees directly from images."],"url":"http://arxiv.org/abs/2404.03042v1","category":"cs.CV"}
{"created":"2024-04-03 19:20:57","title":"General Effect Modelling (GEM) -- Part 1. Method description","abstract":"We present a flexible tool, called General Effect Modelling (GEM), for the analysis of any multivariate data influenced by one or more qualitative (categorical) or quantitative (continuous) input variables. The variables can be design factors or observed values, e.g., age, sex, or income, or they may represent subgroups of the samples found by data exploration. The first step in GEM separates the variation in the multivariate data into effect matrices associated with each of the influencing variables (and possibly interactions between these) by applying a general linear model. The residuals of the model are added to each of the effect matrices and the results are called Effect plus Residual (ER) values. The tables of ER values have the same dimensions as the original multivariate data. The second step of GEM is a multi- or univariate exploration of the ER tables to learn more about the multivariate data in relation to each input variables. The exploration is simplified as it addresses one input variable at the time, or if preferred, a combination of input variables. One example is a study to identify molecular fingerprints associated with a disease that is not influenced by age where individuals at different ages with and without the disease are included in the experiment. This situation can be described as an experiment with two input variables: the targeted disease and the individual age. Through GEM, the effect of age can be removed, thus focusing further analysis on the targeted disease without the influence of the confounding effect of age. ER values can also be the combined effect of several input variables. This publication has three parts: the first part describes the GEM methodology, Part 2 is a consideration of multivariate data and the benefits of treating such data by multivariate analysis, with a focus on omics data, and Part 3 is a case study in Multiple Sclerosis (MS).","sentences":["We present a flexible tool, called General Effect Modelling (GEM), for the analysis of any multivariate data influenced by one or more qualitative (categorical) or quantitative (continuous) input variables.","The variables can be design factors or observed values, e.g., age, sex, or income, or they may represent subgroups of the samples found by data exploration.","The first step in GEM separates the variation in the multivariate data into effect matrices associated with each of the influencing variables (and possibly interactions between these) by applying a general linear model.","The residuals of the model are added to each of the effect matrices and the results are called Effect plus Residual (ER) values.","The tables of ER values have the same dimensions as the original multivariate data.","The second step of GEM is a multi- or univariate exploration of the ER tables to learn more about the multivariate data in relation to each input variables.","The exploration is simplified as it addresses one input variable at the time, or if preferred, a combination of input variables.","One example is a study to identify molecular fingerprints associated with a disease that is not influenced by age where individuals at different ages with and without the disease are included in the experiment.","This situation can be described as an experiment with two input variables: the targeted disease and the individual age.","Through GEM, the effect of age can be removed, thus focusing further analysis on the targeted disease without the influence of the confounding effect of age.","ER values can also be the combined effect of several input variables.","This publication has three parts: the first part describes the GEM methodology, Part 2 is a consideration of multivariate data and the benefits of treating such data by multivariate analysis, with a focus on omics data, and Part 3 is a case study in Multiple Sclerosis (MS)."],"url":"http://arxiv.org/abs/2404.03024v1","category":"stat.ME"}
{"created":"2024-04-03 19:17:43","title":"BCAmirs at SemEval-2024 Task 4: Beyond Words: A Multimodal and Multilingual Exploration of Persuasion in Memes","abstract":"Memes, combining text and images, frequently use metaphors to convey persuasive messages, shaping public opinion. Motivated by this, our team engaged in SemEval-2024 Task 4, a hierarchical multi-label classification task designed to identify rhetorical and psychological persuasion techniques embedded within memes. To tackle this problem, we introduced a caption generation step to assess the modality gap and the impact of additional semantic information from images, which improved our result. Our best model utilizes GPT-4 generated captions alongside meme text to fine-tune RoBERTa as the text encoder and CLIP as the image encoder. It outperforms the baseline by a large margin in all 12 subtasks. In particular, it ranked in top-3 across all languages in Subtask 2a, and top-4 in Subtask 2b, demonstrating quantitatively strong performance. The improvement achieved by the introduced intermediate step is likely attributable to the metaphorical essence of images that challenges visual encoders. This highlights the potential for improving abstract visual semantics encoding.","sentences":["Memes, combining text and images, frequently use metaphors to convey persuasive messages, shaping public opinion.","Motivated by this, our team engaged in SemEval-2024 Task 4, a hierarchical multi-label classification task designed to identify rhetorical and psychological persuasion techniques embedded within memes.","To tackle this problem, we introduced a caption generation step to assess the modality gap and the impact of additional semantic information from images, which improved our result.","Our best model utilizes GPT-4 generated captions alongside meme text to fine-tune RoBERTa as the text encoder and CLIP as the image encoder.","It outperforms the baseline by a large margin in all 12 subtasks.","In particular, it ranked in top-3 across all languages in Subtask 2a, and top-4 in Subtask 2b, demonstrating quantitatively strong performance.","The improvement achieved by the introduced intermediate step is likely attributable to the metaphorical essence of images that challenges visual encoders.","This highlights the potential for improving abstract visual semantics encoding."],"url":"http://arxiv.org/abs/2404.03022v1","category":"cs.CL"}
{"created":"2024-04-03 18:50:14","title":"Spectral Clustering in Convex and Constrained Settings","abstract":"Spectral clustering methods have gained widespread recognition for their effectiveness in clustering high-dimensional data. Among these techniques, constrained spectral clustering has emerged as a prominent approach, demonstrating enhanced performance by integrating pairwise constraints. However, the application of such constraints to semidefinite spectral clustering, a variant that leverages semidefinite programming to optimize clustering objectives, remains largely unexplored. In this paper, we introduce a novel framework for seamlessly integrating pairwise constraints into semidefinite spectral clustering. Our methodology systematically extends the capabilities of semidefinite spectral clustering to capture complex data structures, thereby addressing real-world clustering challenges more effectively. Additionally, we extend this framework to encompass both active and self-taught learning scenarios, further enhancing its versatility and applicability. Empirical studies conducted on well-known datasets demonstrate the superiority of our proposed framework over existing spectral clustering methods, showcasing its robustness and scalability across diverse datasets and learning settings. By bridging the gap between constrained learning and semidefinite spectral clustering, our work contributes to the advancement of spectral clustering techniques, offering researchers and practitioners a versatile tool for addressing complex clustering challenges in various real-world applications. Access to the data, code, and experimental results is provided for further exploration (https://github.com/swarupbehera/SCCCS).","sentences":["Spectral clustering methods have gained widespread recognition for their effectiveness in clustering high-dimensional data.","Among these techniques, constrained spectral clustering has emerged as a prominent approach, demonstrating enhanced performance by integrating pairwise constraints.","However, the application of such constraints to semidefinite spectral clustering, a variant that leverages semidefinite programming to optimize clustering objectives, remains largely unexplored.","In this paper, we introduce a novel framework for seamlessly integrating pairwise constraints into semidefinite spectral clustering.","Our methodology systematically extends the capabilities of semidefinite spectral clustering to capture complex data structures, thereby addressing real-world clustering challenges more effectively.","Additionally, we extend this framework to encompass both active and self-taught learning scenarios, further enhancing its versatility and applicability.","Empirical studies conducted on well-known datasets demonstrate the superiority of our proposed framework over existing spectral clustering methods, showcasing its robustness and scalability across diverse datasets and learning settings.","By bridging the gap between constrained learning and semidefinite spectral clustering, our work contributes to the advancement of spectral clustering techniques, offering researchers and practitioners a versatile tool for addressing complex clustering challenges in various real-world applications.","Access to the data, code, and experimental results is provided for further exploration (https://github.com/swarupbehera/SCCCS)."],"url":"http://arxiv.org/abs/2404.03012v1","category":"cs.LG"}
{"created":"2024-04-03 18:42:19","title":"Skeleton Recall Loss for Connectivity Conserving and Resource Efficient Segmentation of Thin Tubular Structures","abstract":"Accurately segmenting thin tubular structures, such as vessels, nerves, roads or concrete cracks, is a crucial task in computer vision. Standard deep learning-based segmentation loss functions, such as Dice or Cross-Entropy, focus on volumetric overlap, often at the expense of preserving structural connectivity or topology. This can lead to segmentation errors that adversely affect downstream tasks, including flow calculation, navigation, and structural inspection. Although current topology-focused losses mark an improvement, they introduce significant computational and memory overheads. This is particularly relevant for 3D data, rendering these losses infeasible for larger volumes as well as increasingly important multi-class segmentation problems. To mitigate this, we propose a novel Skeleton Recall Loss, which effectively addresses these challenges by circumventing intensive GPU-based calculations with inexpensive CPU operations. It demonstrates overall superior performance to current state-of-the-art approaches on five public datasets for topology-preserving segmentation, while substantially reducing computational overheads by more than 90%. In doing so, we introduce the first multi-class capable loss function for thin structure segmentation, excelling in both efficiency and efficacy for topology-preservation.","sentences":["Accurately segmenting thin tubular structures, such as vessels, nerves, roads or concrete cracks, is a crucial task in computer vision.","Standard deep learning-based segmentation loss functions, such as Dice or Cross-Entropy, focus on volumetric overlap, often at the expense of preserving structural connectivity or topology.","This can lead to segmentation errors that adversely affect downstream tasks, including flow calculation, navigation, and structural inspection.","Although current topology-focused losses mark an improvement, they introduce significant computational and memory overheads.","This is particularly relevant for 3D data, rendering these losses infeasible for larger volumes as well as increasingly important multi-class segmentation problems.","To mitigate this, we propose a novel Skeleton Recall Loss, which effectively addresses these challenges by circumventing intensive GPU-based calculations with inexpensive CPU operations.","It demonstrates overall superior performance to current state-of-the-art approaches on five public datasets for topology-preserving segmentation, while substantially reducing computational overheads by more than 90%.","In doing so, we introduce the first multi-class capable loss function for thin structure segmentation, excelling in both efficiency and efficacy for topology-preservation."],"url":"http://arxiv.org/abs/2404.03010v1","category":"eess.IV"}
{"created":"2024-04-03 18:09:33","title":"Towards a Fully Interpretable and More Scalable RSA Model for Metaphor Understanding","abstract":"The Rational Speech Act (RSA) model provides a flexible framework to model pragmatic reasoning in computational terms. However, state-of-the-art RSA models are still fairly distant from modern machine learning techniques and present a number of limitations related to their interpretability and scalability. Here, we introduce a new RSA framework for metaphor understanding that addresses these limitations by providing an explicit formula - based on the mutually shared information between the speaker and the listener - for the estimation of the communicative goal and by learning the rationality parameter using gradient-based methods. The model was tested against 24 metaphors, not limited to the conventional $\\textit{John-is-a-shark}$ type. Results suggest an overall strong positive correlation between the distributions generated by the model and the interpretations obtained from the human behavioral data, which increased when the intended meaning capitalized on properties that were inherent to the vehicle concept. Overall, findings suggest that metaphor processing is well captured by a typicality-based Bayesian model, even when more scalable and interpretable, opening up possible applications to other pragmatic phenomena and novel uses for increasing Large Language Models interpretability. Yet, results highlight that the more creative nuances of metaphorical meaning, not strictly encoded in the lexical concepts, are a challenging aspect for machines.","sentences":["The Rational Speech Act (RSA) model provides a flexible framework to model pragmatic reasoning in computational terms.","However, state-of-the-art RSA models are still fairly distant from modern machine learning techniques and present a number of limitations related to their interpretability and scalability.","Here, we introduce a new RSA framework for metaphor understanding that addresses these limitations by providing an explicit formula - based on the mutually shared information between the speaker and the listener - for the estimation of the communicative goal and by learning the rationality parameter using gradient-based methods.","The model was tested against 24 metaphors, not limited to the conventional $\\textit{John-is-a-shark}$ type.","Results suggest an overall strong positive correlation between the distributions generated by the model and the interpretations obtained from the human behavioral data, which increased when the intended meaning capitalized on properties that were inherent to the vehicle concept.","Overall, findings suggest that metaphor processing is well captured by a typicality-based Bayesian model, even when more scalable and interpretable, opening up possible applications to other pragmatic phenomena and novel uses for increasing Large Language Models interpretability.","Yet, results highlight that the more creative nuances of metaphorical meaning, not strictly encoded in the lexical concepts, are a challenging aspect for machines."],"url":"http://arxiv.org/abs/2404.02983v1","category":"cs.CL"}
{"created":"2024-04-03 17:54:37","title":"Deep Image Composition Meets Image Forgery","abstract":"Image forgery is a topic that has been studied for many years. Before the breakthrough of deep learning, forged images were detected using handcrafted features that did not require training. These traditional methods failed to perform satisfactorily even on datasets much worse in quality than real-life image manipulations. Advances in deep learning have impacted image forgery detection as much as they have impacted other areas of computer vision and have improved the state of the art. Deep learning models require large amounts of labeled data for training. In the case of image forgery, labeled data at the pixel level is a very important factor for the models to learn. None of the existing datasets have sufficient size, realism and pixel-level labeling at the same time. This is due to the high cost of producing and labeling quality images. It can take hours for an image editing expert to manipulate just one image. To bridge this gap, we automate data generation using image composition techniques that are very related to image forgery. Unlike other automated data generation frameworks, we use state of the art image composition deep learning models to generate spliced images close to the quality of real-life manipulations. Finally, we test the generated dataset on the SOTA image manipulation detection model and show that its prediction performance is lower compared to existing datasets, i.e. we produce realistic images that are more difficult to detect. Dataset will be available at https://github.com/99eren99/DIS25k .","sentences":["Image forgery is a topic that has been studied for many years.","Before the breakthrough of deep learning, forged images were detected using handcrafted features that did not require training.","These traditional methods failed to perform satisfactorily even on datasets much worse in quality than real-life image manipulations.","Advances in deep learning have impacted image forgery detection as much as they have impacted other areas of computer vision and have improved the state of the art.","Deep learning models require large amounts of labeled data for training.","In the case of image forgery, labeled data at the pixel level is a very important factor for the models to learn.","None of the existing datasets have sufficient size, realism and pixel-level labeling at the same time.","This is due to the high cost of producing and labeling quality images.","It can take hours for an image editing expert to manipulate just one image.","To bridge this gap, we automate data generation using image composition techniques that are very related to image forgery.","Unlike other automated data generation frameworks, we use state of the art image composition deep learning models to generate spliced images close to the quality of real-life manipulations.","Finally, we test the generated dataset on the SOTA image manipulation detection model and show that its prediction performance is lower compared to existing datasets, i.e. we produce realistic images that are more difficult to detect.","Dataset will be available at https://github.com/99eren99/DIS25k ."],"url":"http://arxiv.org/abs/2404.02897v1","category":"cs.CV"}
{"created":"2024-04-03 17:38:15","title":"PoCo: Point Context Cluster for RGBD Indoor Place Recognition","abstract":"We present a novel end-to-end algorithm (PoCo) for the indoor RGB-D place recognition task, aimed at identifying the most likely match for a given query frame within a reference database. The task presents inherent challenges attributed to the constrained field of view and limited range of perception sensors. We propose a new network architecture, which generalizes the recent Context of Clusters (CoCs) to extract global descriptors directly from the noisy point clouds through end-to-end learning. Moreover, we develop the architecture by integrating both color and geometric modalities into the point features to enhance the global descriptor representation. We conducted evaluations on public datasets ScanNet-PR and ARKit with 807 and 5047 scenarios, respectively. PoCo achieves SOTA performance: on ScanNet-PR, we achieve R@1 of 64.63%, a 5.7% improvement from the best-published result CGis (61.12%); on Arkit, we achieve R@1 of 45.12%, a 13.3% improvement from the best-published result CGis (39.82%). In addition, PoCo shows higher efficiency than CGis in inference time (1.75X-faster), and we demonstrate the effectiveness of PoCo in recognizing places within a real-world laboratory environment.","sentences":["We present a novel end-to-end algorithm (PoCo) for the indoor RGB-D place recognition task, aimed at identifying the most likely match for a given query frame within a reference database.","The task presents inherent challenges attributed to the constrained field of view and limited range of perception sensors.","We propose a new network architecture, which generalizes the recent Context of Clusters (CoCs) to extract global descriptors directly from the noisy point clouds through end-to-end learning.","Moreover, we develop the architecture by integrating both color and geometric modalities into the point features to enhance the global descriptor representation.","We conducted evaluations on public datasets ScanNet-PR and ARKit with 807 and 5047 scenarios, respectively.","PoCo achieves SOTA performance: on ScanNet-PR, we achieve R@1 of 64.63%, a 5.7% improvement from the best-published result CGis (61.12%); on Arkit, we achieve R@1 of 45.12%, a 13.3% improvement from the best-published result CGis (39.82%).","In addition, PoCo shows higher efficiency than CGis in inference time (1.75X-faster), and we demonstrate the effectiveness of PoCo in recognizing places within a real-world laboratory environment."],"url":"http://arxiv.org/abs/2404.02885v1","category":"cs.CV"}
{"created":"2024-04-03 17:09:25","title":"Gaussian Process Regression with Soft Inequality and Monotonicity Constraints","abstract":"Gaussian process (GP) regression is a non-parametric, Bayesian framework to approximate complex models. Standard GP regression can lead to an unbounded model in which some points can take infeasible values. We introduce a new GP method that enforces the physical constraints in a probabilistic manner. This GP model is trained by the quantum-inspired Hamiltonian Monte Carlo (QHMC). QHMC is an efficient way to sample from a broad class of distributions. Unlike the standard Hamiltonian Monte Carlo algorithm in which a particle has a fixed mass, QHMC allows a particle to have a random mass matrix with a probability distribution. Introducing the QHMC method to the inequality and monotonicity constrained GP regression in the probabilistic sense, our approach improves the accuracy and reduces the variance in the resulting GP model. According to our experiments on several datasets, the proposed approach serves as an efficient method as it accelerates the sampling process while maintaining the accuracy, and it is applicable to high dimensional problems.","sentences":["Gaussian process (GP) regression is a non-parametric, Bayesian framework to approximate complex models.","Standard GP regression can lead to an unbounded model in which some points can take infeasible values.","We introduce a new GP method that enforces the physical constraints in a probabilistic manner.","This GP model is trained by the quantum-inspired Hamiltonian Monte Carlo (QHMC).","QHMC is an efficient way to sample from a broad class of distributions.","Unlike the standard Hamiltonian Monte Carlo algorithm in which a particle has a fixed mass, QHMC allows a particle to have a random mass matrix with a probability distribution.","Introducing the QHMC method to the inequality and monotonicity constrained GP regression in the probabilistic sense, our approach improves the accuracy and reduces the variance in the resulting GP model.","According to our experiments on several datasets, the proposed approach serves as an efficient method as it accelerates the sampling process while maintaining the accuracy, and it is applicable to high dimensional problems."],"url":"http://arxiv.org/abs/2404.02873v1","category":"stat.ML"}
{"created":"2024-04-03 16:37:38","title":"An Information Bottleneck Approach for Markov Model Construction","abstract":"Markov state models (MSMs) are valuable for studying dynamics of protein conformational changes via statistical analysis of molecular dynamics (MD) simulations. In MSMs, the complex configuration space is coarse-grained into conformational states, with the dynamics modeled by a series of Markovian transitions among these states at discrete lag times. Constructing the Markovian model at a specific lag time requires state defined without significant internal energy barriers, enabling internal dynamics relaxation within the lag time. This process coarse grains time and space, integrating out rapid motions within metastable states. This work introduces a continuous embedding approach for molecular conformations using the state predictive information bottleneck (SPIB), which unifies dimensionality reduction and state space partitioning via a continuous, machine learned basis set. Without explicit optimization of VAMP-based scores, SPIB demonstrates state-of-the-art performance in identifying slow dynamical processes and constructing predictive multi-resolution Markovian models. When applied to mini-proteins trajectories, SPIB showcases unique advantages compared to competing methods. It automatically adjusts the number of metastable states based on a specified minimal time resolution, eliminating the need for manual tuning. While maintaining efficacy in dynamical properties, SPIB excels in accurately distinguishing metastable states and capturing numerous well-populated macrostates. Furthermore, SPIB's ability to learn a low-dimensional continuous embedding of the underlying MSMs enhances the interpretation of dynamic pathways. Accordingly, we propose SPIB as an easy-to-implement methodology for end-to-end MSM construction.","sentences":["Markov state models (MSMs) are valuable for studying dynamics of protein conformational changes via statistical analysis of molecular dynamics (MD) simulations.","In MSMs, the complex configuration space is coarse-grained into conformational states, with the dynamics modeled by a series of Markovian transitions among these states at discrete lag times.","Constructing the Markovian model at a specific lag time requires state defined without significant internal energy barriers, enabling internal dynamics relaxation within the lag time.","This process coarse grains time and space, integrating out rapid motions within metastable states.","This work introduces a continuous embedding approach for molecular conformations using the state predictive information bottleneck (SPIB), which unifies dimensionality reduction and state space partitioning via a continuous, machine learned basis set.","Without explicit optimization of VAMP-based scores, SPIB demonstrates state-of-the-art performance in identifying slow dynamical processes and constructing predictive multi-resolution Markovian models.","When applied to mini-proteins trajectories, SPIB showcases unique advantages compared to competing methods.","It automatically adjusts the number of metastable states based on a specified minimal time resolution, eliminating the need for manual tuning.","While maintaining efficacy in dynamical properties, SPIB excels in accurately distinguishing metastable states and capturing numerous well-populated macrostates.","Furthermore, SPIB's ability to learn a low-dimensional continuous embedding of the underlying MSMs enhances the interpretation of dynamic pathways.","Accordingly, we propose SPIB as an easy-to-implement methodology for end-to-end MSM construction."],"url":"http://arxiv.org/abs/2404.02856v1","category":"physics.bio-ph"}
{"created":"2024-04-04 15:44:52","title":"On the penalization by the perimeter in shape optimization applied to Dirichlet inverse obstacle problem","abstract":"This paper is devoted to the understanding of regularisation process in the shape optimization approach to the so-called Dirichlet inverse obstacle problem for elliptic operators. More precisely, we study two different regularisations of the very classical shape optimization approach consisting in minimizing a mismatched functional. The first one is an implicit regularisation when working in the class of inclusion having a uniform $\\varepsilon$-cone property, a natural class in shape optimization. As this regularity is not trivial to guarantee numerically, we discuss the regularisation by perimeter penalization. We show that this second regularisation provides a stability gain in the minimization process.","sentences":["This paper is devoted to the understanding of regularisation process in the shape optimization approach to the so-called Dirichlet inverse obstacle problem for elliptic operators.","More precisely, we study two different regularisations of the very classical shape optimization approach consisting in minimizing a mismatched functional.","The first one is an implicit regularisation when working in the class of inclusion having a uniform $\\varepsilon$-cone property, a natural class in shape optimization.","As this regularity is not trivial to guarantee numerically, we discuss the regularisation by perimeter penalization.","We show that this second regularisation provides a stability gain in the minimization process."],"url":"http://arxiv.org/abs/2404.03536v1","category":"math.OC"}
{"created":"2024-04-04 15:05:50","title":"A QAOA approach with fake devices: A case study for the maximum cut in ring graphs","abstract":"The quantum approximate optimization algorithm (QAOA) can require considerable processing time for developers to test and debug their codes on expensive quantum devices. One avenue to circumvent this difficulty is to use the error maps of quantum devices, where a local simulator can be automatically configured to mimic an actual device backend. In our work, we evaluated some error maps of quantum devices, known as fake devices, that are freely available in the cloud. The QAOA and the problem of maximum cut in 2-regular connected graphs, known as ring of disagrees, were used as tools for the noise analysis. The approximation ratio, the expectation energy and the probability of success for this problem have been evaluated in two scenarios. First, the quantities were studied through noisy simulations using fake devices. Second, error mitigation methods such as optimization levels and translation (connectivity mapping) of the original problem were applied. These results were then compared with the analytical solution of the ring graph. The study shows that error mitigation methods were crucial in obtaining better results for the expectation value of the energy, the approximation ratio, and the probability of success for the ring graphs.","sentences":["The quantum approximate optimization algorithm (QAOA) can require considerable processing time for developers to test and debug their codes on expensive quantum devices.","One avenue to circumvent this difficulty is to use the error maps of quantum devices, where a local simulator can be automatically configured to mimic an actual device backend.","In our work, we evaluated some error maps of quantum devices, known as fake devices, that are freely available in the cloud.","The QAOA and the problem of maximum cut in 2-regular connected graphs, known as ring of disagrees, were used as tools for the noise analysis.","The approximation ratio, the expectation energy and the probability of success for this problem have been evaluated in two scenarios.","First, the quantities were studied through noisy simulations using fake devices.","Second, error mitigation methods such as optimization levels and translation (connectivity mapping) of the original problem were applied.","These results were then compared with the analytical solution of the ring graph.","The study shows that error mitigation methods were crucial in obtaining better results for the expectation value of the energy, the approximation ratio, and the probability of success for the ring graphs."],"url":"http://arxiv.org/abs/2404.03501v1","category":"quant-ph"}
{"created":"2024-04-04 13:46:49","title":"Radiative corrections to the spin asymmetry in elastic polarized electron - nucleus collisions at high energy","abstract":"Improving the numerical precision, dispersion corrections to the beam-normal spin asymmetry which arise from the dominant nuclear excitations, are estimated up to a collision energy of 1 GeV. A nonperturbative calculation of vacuum polarization and the vertex plus self-energy correction, using optimized potentials, indicates that for small scattering angles both these quantum electrodynamical (QED) effects on the spin asymmetry decrease with energy above 200 MeV and can mostly be neglected at high energies. Examples are given for the 12C and 208Pb nuclei. While our results disagree with the measured high-energy spin asymmetry for 12C, they are able to explain the data on 208Pb near 1 GeV.","sentences":["Improving the numerical precision, dispersion corrections to the beam-normal spin asymmetry which arise from the dominant nuclear excitations, are estimated up to a collision energy of 1 GeV. A nonperturbative calculation of vacuum polarization and the vertex plus self-energy correction, using optimized potentials, indicates that for small scattering angles both these quantum electrodynamical (QED) effects on the spin asymmetry decrease with energy above 200 MeV and can mostly be neglected at high energies.","Examples are given for the 12C and 208Pb nuclei.","While our results disagree with the measured high-energy spin asymmetry for 12C, they are able to explain the data on 208Pb near 1 GeV."],"url":"http://arxiv.org/abs/2404.03445v1","category":"nucl-th"}
{"created":"2024-04-04 13:38:42","title":"Design and Optimization of Cooperative Sensing With Limited Backhaul Capacity","abstract":"This paper introduces a cooperative sensing framework designed for integrated sensing and communication cellular networks. The framework comprises one base station (BS) functioning as the sensing transmitter, while several nearby BSs act as sensing receivers. The primary objective is to facilitate cooperative target localization by enabling each receiver to share specific information with a fusion center (FC) over a limited capacity backhaul link. To achieve this goal, we propose an advanced cooperative sensing design that enhances the communication process between the receivers and the FC. Each receiver independently estimates the time delay and the reflecting coefficient associated with the reflected path from the target. Subsequently, each receiver transmits the estimated values and the received signal samples centered around the estimated time delay to the FC. To efficiently quantize the signal samples, a Karhunen-Lo\\`eve Transform coding scheme is employed. Furthermore, an optimization problem is formulated to allocate backhaul resources for quantizing different samples, improving target localization. Numerical results validate the effectiveness of our proposed advanced design and demonstrate its superiority over a baseline design, where only the locally estimated values are transmitted from each receiver to the FC.","sentences":["This paper introduces a cooperative sensing framework designed for integrated sensing and communication cellular networks.","The framework comprises one base station (BS) functioning as the sensing transmitter, while several nearby BSs act as sensing receivers.","The primary objective is to facilitate cooperative target localization by enabling each receiver to share specific information with a fusion center (FC) over a limited capacity backhaul link.","To achieve this goal, we propose an advanced cooperative sensing design that enhances the communication process between the receivers and the FC.","Each receiver independently estimates the time delay and the reflecting coefficient associated with the reflected path from the target.","Subsequently, each receiver transmits the estimated values and the received signal samples centered around the estimated time delay to the FC.","To efficiently quantize the signal samples, a Karhunen-Lo\\`eve Transform coding scheme is employed.","Furthermore, an optimization problem is formulated to allocate backhaul resources for quantizing different samples, improving target localization.","Numerical results validate the effectiveness of our proposed advanced design and demonstrate its superiority over a baseline design, where only the locally estimated values are transmitted from each receiver to the FC."],"url":"http://arxiv.org/abs/2404.03440v1","category":"cs.IT"}
{"created":"2024-04-04 12:49:42","title":"Future Predictive Success-or-Failure Classification for Long-Horizon Robotic Tasks","abstract":"Automating long-horizon tasks with a robotic arm has been a central research topic in robotics. Optimization-based action planning is an efficient approach for creating an action plan to complete a given task. Construction of a reliable planning method requires a design process of conditions, e.g., to avoid collision between objects. The design process, however, has two critical issues: 1) iterative trials--the design process is time-consuming due to the trial-and-error process of modifying conditions, and 2) manual redesign--it is difficult to cover all the necessary conditions manually. To tackle these issues, this paper proposes a future-predictive success-or-failure-classification method to obtain conditions automatically. The key idea behind the proposed method is an end-to-end approach for determining whether the action plan can complete a given task instead of manually redesigning the conditions. The proposed method uses a long-horizon future-prediction method to enable success-or-failure classification without the execution of an action plan. This paper also proposes a regularization term called transition consistency regularization to provide easy-to-predict feature distribution. The regularization term improves future prediction and classification performance. The effectiveness of our method is demonstrated through classification and robotic-manipulation experiments.","sentences":["Automating long-horizon tasks with a robotic arm has been a central research topic in robotics.","Optimization-based action planning is an efficient approach for creating an action plan to complete a given task.","Construction of a reliable planning method requires a design process of conditions, e.g., to avoid collision between objects.","The design process, however, has two critical issues: 1) iterative trials--the design process is time-consuming due to the trial-and-error process of modifying conditions, and 2) manual redesign--it is difficult to cover all the necessary conditions manually.","To tackle these issues, this paper proposes a future-predictive success-or-failure-classification method to obtain conditions automatically.","The key idea behind the proposed method is an end-to-end approach for determining whether the action plan can complete a given task instead of manually redesigning the conditions.","The proposed method uses a long-horizon future-prediction method to enable success-or-failure classification without the execution of an action plan.","This paper also proposes a regularization term called transition consistency regularization to provide easy-to-predict feature distribution.","The regularization term improves future prediction and classification performance.","The effectiveness of our method is demonstrated through classification and robotic-manipulation experiments."],"url":"http://arxiv.org/abs/2404.03415v1","category":"cs.RO"}
{"created":"2024-04-04 10:45:07","title":"Towards Pareto Optimal Throughput in Small Language Model Serving","abstract":"Large language models (LLMs) have revolutionized the state-of-the-art of many different natural language processing tasks. Although serving LLMs is computationally and memory demanding, the rise of Small Language Models (SLMs) offers new opportunities for resource-constrained users, who now are able to serve small models with cutting-edge performance. In this paper, we present a set of experiments designed to benchmark SLM inference at performance and energy levels. Our analysis provides a new perspective in serving, highlighting that the small memory footprint of SLMs allows for reaching the Pareto-optimal throughput within the resource capacity of a single accelerator. In this regard, we present an initial set of findings demonstrating how model replication can effectively improve resource utilization for serving SLMs.","sentences":["Large language models (LLMs) have revolutionized the state-of-the-art of many different natural language processing tasks.","Although serving LLMs is computationally and memory demanding, the rise of Small Language Models (SLMs) offers new opportunities for resource-constrained users, who now are able to serve small models with cutting-edge performance.","In this paper, we present a set of experiments designed to benchmark SLM inference at performance and energy levels.","Our analysis provides a new perspective in serving, highlighting that the small memory footprint of SLMs allows for reaching the Pareto-optimal throughput within the resource capacity of a single accelerator.","In this regard, we present an initial set of findings demonstrating how model replication can effectively improve resource utilization for serving SLMs."],"url":"http://arxiv.org/abs/2404.03353v1","category":"cs.CL"}
{"created":"2024-04-04 09:22:18","title":"Spatio-Spectral Structure Tensor Total Variation for Hyperspectral Image Denoising and Destriping","abstract":"This paper proposes a novel regularization method, named Spatio-Spectral Structure Tensor Total Variation (S3TTV), for denoising and destriping of hyperspectral (HS) images. HS images are inevitably contaminated by various types of noise, during acquisition process, due to the measurement equipment and the environment. For HS image denoising and destriping tasks, Spatio-Spectral Total Variation (SSTV), defined using second-order spatio-spectral differences, is widely known as a powerful regularization approach that models the underlying spatio-spectral properties. However, since SSTV refers only to adjacent pixels/bands, semi-local spatial structures are not preserved during denoising process. To address this problem, we newly design S3TTV, defined by the sum of the nuclear norms of matrices consisting of second-order spatio-spectral differences in small spectral blocks (we call these matrices as spatio-spectral structure tensors). The proposed regularization method simultaneously models the spatial piecewise-smoothness, the spatial similarity between adjacent bands, and the spectral correlation across all bands in small spectral blocks, leading to effective noise removal while preserving the semi-local spatial structures. Furthermore, we formulate the HS image denoising and destriping problem as a convex optimization problem involving S3TTV and develop an algorithm based on a preconditioned primal-dual splitting method to solve this problem efficiently. Finally, we demonstrate the effectiveness of S3TTV by comparing it with existing methods, including state-of-the-art ones through denoising and destriping experiments.","sentences":["This paper proposes a novel regularization method, named Spatio-Spectral Structure Tensor Total Variation (S3TTV), for denoising and destriping of hyperspectral (HS) images.","HS images are inevitably contaminated by various types of noise, during acquisition process, due to the measurement equipment and the environment.","For HS image denoising and destriping tasks, Spatio-Spectral Total Variation (SSTV), defined using second-order spatio-spectral differences, is widely known as a powerful regularization approach that models the underlying spatio-spectral properties.","However, since SSTV refers only to adjacent pixels/bands, semi-local spatial structures are not preserved during denoising process.","To address this problem, we newly design S3TTV, defined by the sum of the nuclear norms of matrices consisting of second-order spatio-spectral differences in small spectral blocks (we call these matrices as spatio-spectral structure tensors).","The proposed regularization method simultaneously models the spatial piecewise-smoothness, the spatial similarity between adjacent bands, and the spectral correlation across all bands in small spectral blocks, leading to effective noise removal while preserving the semi-local spatial structures.","Furthermore, we formulate the HS image denoising and destriping problem as a convex optimization problem involving S3TTV and develop an algorithm based on a preconditioned primal-dual splitting method to solve this problem efficiently.","Finally, we demonstrate the effectiveness of S3TTV by comparing it with existing methods, including state-of-the-art ones through denoising and destriping experiments."],"url":"http://arxiv.org/abs/2404.03313v1","category":"eess.SP"}
{"created":"2024-04-04 08:21:32","title":"Adsorption of Gases on Ti3C2Tx MXene: Implications from X-ray Photoelectron Spectroscopy","abstract":"One of the most explored MXenes is the Ti3C2Tx, where Tx is designated to inherently formed termination species. Among many applications, Ti3C2Tx is an excellent material for energy storage, energy converting, and CO2-capturing devices. However, active sites for adsorption and surface reactions on the Ti3C2Tx-surface are still open questions to explore, which have implications for preparation methods when to obtain correct and optimized surface requirements. Here we use X-ray photoelectron spectroscopy to study adsorption of common gas molecules such as H2, CO2, and H2O, which all might be present in energy storage, energy converting, and CO2-capturing devices based on 2D flakes of Ti3C2Tx. The study shows that H2O, with a strong bonding to the Ti-Ti bridge-sites, can be considered as a termination species. An H2O terminated Ti3C2Tx-surface restricts the CO2 adsorption to the Ti on-top sites and reduces the ability to store positive ions, such as Li+ and Na+. On the other hand, an H2O terminated Ti3C2Tx-surface shows the capability to split water. The study further shows that H2 has the ability to remove F at moderate temperatures. The results from this study have implications for correct selection of MXene preparations and the environment around the MXene in different implementations.","sentences":["One of the most explored MXenes is the Ti3C2Tx, where Tx is designated to inherently formed termination species.","Among many applications, Ti3C2Tx is an excellent material for energy storage, energy converting, and CO2-capturing devices.","However, active sites for adsorption and surface reactions on the Ti3C2Tx-surface are still open questions to explore, which have implications for preparation methods when to obtain correct and optimized surface requirements.","Here we use X-ray photoelectron spectroscopy to study adsorption of common gas molecules such as H2, CO2, and H2O, which all might be present in energy storage, energy converting, and CO2-capturing devices based on 2D flakes of Ti3C2Tx.","The study shows that H2O, with a strong bonding to the Ti-Ti bridge-sites, can be considered as a termination species.","An H2O terminated Ti3C2Tx-surface restricts the CO2 adsorption to the Ti on-top sites and reduces the ability to store positive ions, such as Li+ and Na+.","On the other hand, an H2O terminated Ti3C2Tx-surface shows the capability to split water.","The study further shows that H2 has the ability to remove F at moderate temperatures.","The results from this study have implications for correct selection of MXene preparations and the environment around the MXene in different implementations."],"url":"http://arxiv.org/abs/2404.03287v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 03:42:17","title":"Direct interpolative construction of the discrete Fourier transform as a matrix product operator","abstract":"The quantum Fourier transform (QFT), which can be viewed as a reindexing of the discrete Fourier transform (DFT), has been shown to be compressible as a low-rank matrix product operator (MPO) or quantized tensor train (QTT) operator. However, the original proof of this fact does not furnish a construction of the MPO with a guaranteed error bound. Meanwhile, the existing practical construction of this MPO, based on the compression of a quantum circuit, is not as efficient as possible. We present a simple closed-form construction of the QFT MPO using the interpolative decomposition, with guaranteed near-optimal compression error for a given rank. This construction can speed up the application of the QFT and the DFT, respectively, in quantum circuit simulations and QTT applications. We also connect our interpolative construction to the approximate quantum Fourier transform (AQFT) by demonstrating that the AQFT can be viewed as an MPO constructed using a different interpolation scheme.","sentences":["The quantum Fourier transform (QFT), which can be viewed as a reindexing of the discrete Fourier transform (DFT), has been shown to be compressible as a low-rank matrix product operator (MPO) or quantized tensor train (QTT) operator.","However, the original proof of this fact does not furnish a construction of the MPO with a guaranteed error bound.","Meanwhile, the existing practical construction of this MPO, based on the compression of a quantum circuit, is not as efficient as possible.","We present a simple closed-form construction of the QFT MPO using the interpolative decomposition, with guaranteed near-optimal compression error for a given rank.","This construction can speed up the application of the QFT and the DFT, respectively, in quantum circuit simulations and QTT applications.","We also connect our interpolative construction to the approximate quantum Fourier transform (AQFT) by demonstrating that the AQFT can be viewed as an MPO constructed using a different interpolation scheme."],"url":"http://arxiv.org/abs/2404.03182v1","category":"quant-ph"}
{"created":"2024-04-04 02:45:28","title":"Asymptotic Purification of Disordered Quantum Trajectories","abstract":"The theory of quantum trajectories in a time-dependent disordered environment is studied. We develop a general framework in which one can study the behavior of quantum trajectories that are obtained by repeated not-necessarily-independent but stationary measurements. Using this framework, we are able to generalize the asymptotic purification results of K\\\"ummerer and Maassen [KM] to the current setting. Most notably, the concept of a dark subspace from [KM] is generalized to the current setting, which enables us to lift the main theorem of [KM] to the disordered case.","sentences":["The theory of quantum trajectories in a time-dependent disordered environment is studied.","We develop a general framework in which one can study the behavior of quantum trajectories that are obtained by repeated not-necessarily-independent but stationary measurements.","Using this framework, we are able to generalize the asymptotic purification results of K\\\"ummerer and Maassen [KM] to the current setting.","Most notably, the concept of a dark subspace from [KM] is generalized to the current setting, which enables us to lift the main theorem of [KM] to the disordered case."],"url":"http://arxiv.org/abs/2404.03168v1","category":"quant-ph"}
{"created":"2024-04-04 02:36:24","title":"Non-variational Quantum Combinatorial Optimisation","abstract":"This paper introduces a non-variational quantum algorithm designed to solve a wide range of combinatorial optimisation problems. The algorithm leverages an engineered interference process achieved through repeated application of two unitaries; one inducing phase-shifts dependent on objective function values, and the other mixing phase-shifted probability amplitudes via a continuous-time quantum walk (CTQW) on a problem-specific graph. The algorithm's versatility is demonstrated through its application to various problems, namely those for which solutions are characterised by either a vector of binary variables, a vector of non-binary integer variables, or permutations (a vector of integer variables without repetition). An efficient quantum circuit implementation of the CTQW for each of these problem types is also discussed. A penalty function approach for constrained problems is also introduced, including a method for optimising the penalty function. The algorithm's performance is demonstrated through numerical simulation for randomly generated instances of the following problems (and problem sizes): weighted maxcut (18 vertices), maximum independent set (18 vertices), k-means clustering (12 datapoints, 3 clusters), capacitated facility location (12 customers, 3 facility locations), and the quadratic assignment problem (9 locations). For each problem instance, the algorithm finds a globally optimal solution with a small number of iterations.","sentences":["This paper introduces a non-variational quantum algorithm designed to solve a wide range of combinatorial optimisation problems.","The algorithm leverages an engineered interference process achieved through repeated application of two unitaries; one inducing phase-shifts dependent on objective function values, and the other mixing phase-shifted probability amplitudes via a continuous-time quantum walk (CTQW) on a problem-specific graph.","The algorithm's versatility is demonstrated through its application to various problems, namely those for which solutions are characterised by either a vector of binary variables, a vector of non-binary integer variables, or permutations (a vector of integer variables without repetition).","An efficient quantum circuit implementation of the CTQW for each of these problem types is also discussed.","A penalty function approach for constrained problems is also introduced, including a method for optimising the penalty function.","The algorithm's performance is demonstrated through numerical simulation for randomly generated instances of the following problems (and problem sizes): weighted maxcut (18 vertices), maximum independent set (18 vertices), k-means clustering (12 datapoints, 3 clusters), capacitated facility location (12 customers, 3 facility locations), and the quadratic assignment problem (9 locations).","For each problem instance, the algorithm finds a globally optimal solution with a small number of iterations."],"url":"http://arxiv.org/abs/2404.03167v1","category":"quant-ph"}
{"created":"2024-04-04 01:22:23","title":"Discontinuity-preserving Normal Integration with Auxiliary Edges","abstract":"Many surface reconstruction methods incorporate normal integration, which is a process to obtain a depth map from surface gradients. In this process, the input may represent a surface with discontinuities, e.g., due to self-occlusion. To reconstruct an accurate depth map from the input normal map, hidden surface gradients occurring from the jumps must be handled. To model these jumps correctly, we design a novel discretization scheme for the domain of normal integration. Our key idea is to introduce auxiliary edges, which bridge between piecewise-smooth patches in the domain so that the magnitude of hidden jumps can be explicitly expressed. Using the auxiliary edges, we design a novel algorithm to optimize the discontinuity and the depth map from the input normal map. Our method optimizes discontinuities by using a combination of iterative re-weighted least squares and iterative filtering of the jump magnitudes on auxiliary edges to provide strong sparsity regularization. Compared to previous discontinuity-preserving normal integration methods, which model the magnitudes of jumps only implicitly, our method reconstructs subtle discontinuities accurately thanks to our explicit representation of jumps allowing for strong sparsity regularization.","sentences":["Many surface reconstruction methods incorporate normal integration, which is a process to obtain a depth map from surface gradients.","In this process, the input may represent a surface with discontinuities, e.g., due to self-occlusion.","To reconstruct an accurate depth map from the input normal map, hidden surface gradients occurring from the jumps must be handled.","To model these jumps correctly, we design a novel discretization scheme for the domain of normal integration.","Our key idea is to introduce auxiliary edges, which bridge between piecewise-smooth patches in the domain so that the magnitude of hidden jumps can be explicitly expressed.","Using the auxiliary edges, we design a novel algorithm to optimize the discontinuity and the depth map from the input normal map.","Our method optimizes discontinuities by using a combination of iterative re-weighted least squares and iterative filtering of the jump magnitudes on auxiliary edges to provide strong sparsity regularization.","Compared to previous discontinuity-preserving normal integration methods, which model the magnitudes of jumps only implicitly, our method reconstructs subtle discontinuities accurately thanks to our explicit representation of jumps allowing for strong sparsity regularization."],"url":"http://arxiv.org/abs/2404.03138v1","category":"cs.CV"}
{"created":"2024-04-03 23:24:25","title":"Ego-Motion Aware Target Prediction Module for Robust Multi-Object Tracking","abstract":"Multi-object tracking (MOT) is a prominent task in computer vision with application in autonomous driving, responsible for the simultaneous tracking of multiple object trajectories. Detection-based multi-object tracking (DBT) algorithms detect objects using an independent object detector and predict the imminent location of each target. Conventional prediction methods in DBT utilize Kalman Filter(KF) to extrapolate the target location in the upcoming frames by supposing a constant velocity motion model. These methods are especially hindered in autonomous driving applications due to dramatic camera motion or unavailable detections. Such limitations lead to tracking failures manifested by numerous identity switches and disrupted trajectories. In this paper, we introduce a novel KF-based prediction module called the Ego-motion Aware Target Prediction (EMAP) module by focusing on the integration of camera motion and depth information with object motion models. Our proposed method decouples the impact of camera rotational and translational velocity from the object trajectories by reformulating the Kalman Filter. This reformulation enables us to reject the disturbances caused by camera motion and maximizes the reliability of the object motion model. We integrate our module with four state-of-the-art base MOT algorithms, namely OC-SORT, Deep OC-SORT, ByteTrack, and BoT-SORT. In particular, our evaluation on the KITTI MOT dataset demonstrates that EMAP remarkably drops the number of identity switches (IDSW) of OC-SORT and Deep OC-SORT by 73% and 21%, respectively. At the same time, it elevates other performance metrics such as HOTA by more than 5%. Our source code is available at https://github.com/noyzzz/EMAP.","sentences":["Multi-object tracking (MOT) is a prominent task in computer vision with application in autonomous driving, responsible for the simultaneous tracking of multiple object trajectories.","Detection-based multi-object tracking (DBT) algorithms detect objects using an independent object detector and predict the imminent location of each target.","Conventional prediction methods in DBT utilize Kalman Filter(KF) to extrapolate the target location in the upcoming frames by supposing a constant velocity motion model.","These methods are especially hindered in autonomous driving applications due to dramatic camera motion or unavailable detections.","Such limitations lead to tracking failures manifested by numerous identity switches and disrupted trajectories.","In this paper, we introduce a novel KF-based prediction module called the Ego-motion Aware Target Prediction (EMAP) module by focusing on the integration of camera motion and depth information with object motion models.","Our proposed method decouples the impact of camera rotational and translational velocity from the object trajectories by reformulating the Kalman Filter.","This reformulation enables us to reject the disturbances caused by camera motion and maximizes the reliability of the object motion model.","We integrate our module with four state-of-the-art base MOT algorithms, namely OC-SORT, Deep OC-SORT, ByteTrack, and BoT-SORT.","In particular, our evaluation on the KITTI MOT dataset demonstrates that EMAP remarkably drops the number of identity switches (IDSW) of OC-SORT and Deep OC-SORT by 73% and 21%, respectively.","At the same time, it elevates other performance metrics such as HOTA by more than 5%.","Our source code is available at https://github.com/noyzzz/EMAP."],"url":"http://arxiv.org/abs/2404.03110v1","category":"cs.CV"}
{"created":"2024-04-03 22:16:49","title":"Low Frequency Sampling in Model Predictive Path Integral Control","abstract":"Sampling-based model-predictive controllers have become a powerful optimization tool for planning and control problems in various challenging environments. In this paper, we show how the default choice of uncorrelated Gaussian distributions can be improved upon with the use of a colored noise distribution. Our choice of distribution allows for the emphasis on low frequency control signals, which can result in smoother and more exploratory samples. We use this frequency-based sampling distribution with Model Predictive Path Integral (MPPI) in both hardware and simulation experiments to show better or equal performance on systems with various speeds of input response.","sentences":["Sampling-based model-predictive controllers have become a powerful optimization tool for planning and control problems in various challenging environments.","In this paper, we show how the default choice of uncorrelated Gaussian distributions can be improved upon with the use of a colored noise distribution.","Our choice of distribution allows for the emphasis on low frequency control signals, which can result in smoother and more exploratory samples.","We use this frequency-based sampling distribution with Model Predictive Path Integral (MPPI) in both hardware and simulation experiments to show better or equal performance on systems with various speeds of input response."],"url":"http://arxiv.org/abs/2404.03094v1","category":"cs.RO"}
{"created":"2024-04-03 22:07:30","title":"The circle packing problem: a theoretical comparison of various convexification techniques","abstract":"We consider the problem of packing congruent circles with the maximum radius in a unit square as a mathematical optimization problem. Due to the presence of non-overlapping constraints, this problem is a notoriously difficult nonconvex quadratically constrained optimization problem, which possesses many local optima. We consider several popular convexification techniques, giving rise to linear programming relaxations and semidefinite programming relaxations for the circle packing problem. We compare the strength of these relaxations theoretically, thereby proving the conjectures by Anstreicher (JOGO, 2009). Our results serve as a theoretical justification for the ineffectiveness of existing machinery for convexification of non-overlapping constraints.","sentences":["We consider the problem of packing congruent circles with the maximum radius in a unit square as a mathematical optimization problem.","Due to the presence of non-overlapping constraints, this problem is a notoriously difficult nonconvex quadratically constrained optimization problem, which possesses many local optima.","We consider several popular convexification techniques, giving rise to linear programming relaxations and semidefinite programming relaxations for the circle packing problem.","We compare the strength of these relaxations theoretically, thereby proving the conjectures by Anstreicher (JOGO, 2009).","Our results serve as a theoretical justification for the ineffectiveness of existing machinery for convexification of non-overlapping constraints."],"url":"http://arxiv.org/abs/2404.03091v1","category":"math.OC"}
{"created":"2024-04-03 21:27:50","title":"A Hybrid BLE/UWB Localization Technique with Automatic Radio Map Creation","abstract":"Localization systems intended for home use by people with mild cognitive impairment should comply with specific requirements. They should provide the users with sub-meter accuracy allowing for analyzing patient's movement trajectory and be energy effective, so the devices do not need frequent charging. Such requirements could be satisfied by employing a hybrid positioning system combining accurate UWB with energy efficient Bluetooth Low Energy (BLE) technology. In the paper, such a solution is presented and experimentally verified. In the proposed system, user's location is derived using BLE based fingerprinting. A radio map utilized by the algorithm is created automatically during system operation with the support of UWB subsystem. Such an approach allows the users to repeat system calibration as often as possible, which raises systems resistance to environmental changes.","sentences":["Localization systems intended for home use by people with mild cognitive impairment should comply with specific requirements.","They should provide the users with sub-meter accuracy allowing for analyzing patient's movement trajectory and be energy effective, so the devices do not need frequent charging.","Such requirements could be satisfied by employing a hybrid positioning system combining accurate UWB with energy efficient Bluetooth Low Energy (BLE) technology.","In the paper, such a solution is presented and experimentally verified.","In the proposed system, user's location is derived using BLE based fingerprinting.","A radio map utilized by the algorithm is created automatically during system operation with the support of UWB subsystem.","Such an approach allows the users to repeat system calibration as often as possible, which raises systems resistance to environmental changes."],"url":"http://arxiv.org/abs/2404.03072v1","category":"eess.SP"}
{"created":"2024-04-03 21:17:02","title":"Multiple UAV-Assisted Cooperative DF Relaying in Multi-User Massive MIMO IoT Systems","abstract":"This work considers a multi-user massive multiple-input multiple-output (MU-mMIMO) Internet-of-Things (IoT) system, where multiple unmanned aerial vehicles (UAVs) operating as decode-and-forward (DF) relays connect the base station (BS) to a large number of IoT devices. To maximize the total achievable rate, we propose a novel joint optimization problem of hybrid beamforming (HBF), multiple UAV relay positioning, and power allocation (PA) to multiple IoT users. The study adopts a geometry-based millimeter-wave (mmWave) channel model for both links and utilizes sequential optimization based on K-means UAV-user association. The radio frequency (RF) stages are designed based on the slow time-varying angular information, while the baseband (BB) stages are designed utilizing the reduced-dimension effective channel matrices. The illustrative results show that multiple UAV-assisted cooperative relaying systems outperform a single UAV system in practical user distributions. Moreover, compared to fixed positions and equal PA of UAVs and BS, the joint optimization of UAV location and PA substantially enhances the total achievable rate.","sentences":["This work considers a multi-user massive multiple-input multiple-output (MU-mMIMO) Internet-of-Things (IoT) system, where multiple unmanned aerial vehicles (UAVs) operating as decode-and-forward (DF) relays connect the base station (BS) to a large number of IoT devices.","To maximize the total achievable rate, we propose a novel joint optimization problem of hybrid beamforming (HBF), multiple UAV relay positioning, and power allocation (PA) to multiple IoT users.","The study adopts a geometry-based millimeter-wave (mmWave) channel model for both links and utilizes sequential optimization based on K-means UAV-user association.","The radio frequency (RF) stages are designed based on the slow time-varying angular information, while the baseband (BB) stages are designed utilizing the reduced-dimension effective channel matrices.","The illustrative results show that multiple UAV-assisted cooperative relaying systems outperform a single UAV system in practical user distributions.","Moreover, compared to fixed positions and equal PA of UAVs and BS, the joint optimization of UAV location and PA substantially enhances the total achievable rate."],"url":"http://arxiv.org/abs/2404.03068v1","category":"cs.IT"}
{"created":"2024-04-03 21:13:15","title":"Traffic Divergence Theory: An Analysis Formalism for Dynamic Networks","abstract":"Traffic dynamics is universally crucial in analyzing and designing almost any network. This article introduces a novel theoretical approach to analyzing network traffic dynamics. This theory's machinery is based on the notion of traffic divergence, which captures the flow (im)balance of network nodes and links. It features various analytical probes to investigate both spatial and temporal traffic dynamics. In particular, the maximal traffic distribution in a network can be characterized by spatial traffic divergence rate, which reveals the relative difference among node traffic divergence. To illustrate the usefulness, we apply the theory to two network-driven problems: throughput estimation of data center networks and power-optimized communication planning for robot networks, and show the merits of the proposed theory through simulations.","sentences":["Traffic dynamics is universally crucial in analyzing and designing almost any network.","This article introduces a novel theoretical approach to analyzing network traffic dynamics.","This theory's machinery is based on the notion of traffic divergence, which captures the flow (im)balance of network nodes and links.","It features various analytical probes to investigate both spatial and temporal traffic dynamics.","In particular, the maximal traffic distribution in a network can be characterized by spatial traffic divergence rate, which reveals the relative difference among node traffic divergence.","To illustrate the usefulness, we apply the theory to two network-driven problems: throughput estimation of data center networks and power-optimized communication planning for robot networks, and show the merits of the proposed theory through simulations."],"url":"http://arxiv.org/abs/2404.03066v1","category":"cs.MA"}
{"created":"2024-04-03 18:41:56","title":"Semi-analytical covariance matrices for two-point correlation function for DESI 2024 data","abstract":"We present an optimized way of producing the fast semi-analytical covariance matrices for the Legendre moments of the two-point correlation function, taking into account survey geometry and mimicking the non-Gaussian effects. We validate the approach on simulated (mock) catalogs for different galaxy types, representative of the Dark Energy Spectroscopic Instrument (DESI) Data Release 1, used in 2024 analyses. We find only a few percent differences between the mock sample covariance matrix and our results, which can be expected given the approximate nature of the mocks, although we do identify discrepancies between the shot-noise properties of the DESI fiber assignment algorithm and the faster approximation used in the mocks. Importantly, we find a close agreement (<~ 5% relative differences) in the projected errorbars for distance scale parameters for the baryon acoustic oscillation measurements. This confirms our method as an attractive alternative to simulation-based covariance matrices, especially for non-standard models or galaxy sample selections, in particular, relevant to the broad current and future analyses of DESI data.","sentences":["We present an optimized way of producing the fast semi-analytical covariance matrices for the Legendre moments of the two-point correlation function, taking into account survey geometry and mimicking the non-Gaussian effects.","We validate the approach on simulated (mock) catalogs for different galaxy types, representative of the Dark Energy Spectroscopic Instrument (DESI) Data Release 1, used in 2024 analyses.","We find only a few percent differences between the mock sample covariance matrix and our results, which can be expected given the approximate nature of the mocks, although we do identify discrepancies between the shot-noise properties of the DESI fiber assignment algorithm and the faster approximation used in the mocks.","Importantly, we find a close agreement (<~ 5% relative differences) in the projected errorbars for distance scale parameters for the baryon acoustic oscillation measurements.","This confirms our method as an attractive alternative to simulation-based covariance matrices, especially for non-standard models or galaxy sample selections, in particular, relevant to the broad current and future analyses of DESI data."],"url":"http://arxiv.org/abs/2404.03007v1","category":"astro-ph.CO"}
{"created":"2024-04-03 18:41:54","title":"Optimal Reconstruction of Baryon Acoustic Oscillations for DESI 2024","abstract":"Baryon acoustic oscillations (BAO) provide a robust standard ruler to measure the expansion history of the Universe through galaxy clustering. Density-field reconstruction is now a widely adopted procedure for increasing the precision and accuracy of the BAO detection. With the goal of finding the optimal reconstruction settings to be used in the DESI 2024 galaxy BAO analysis, we assess the sensitivity of the post-reconstruction BAO constraints to different choices in our analysis configuration, performing tests on blinded data from the first year of DESI observations (DR1), as well as on mocks that mimic the expected clustering and selection properties of the DESI DR1 target samples. Overall, we find that BAO constraints remain robust against multiple aspects in the reconstruction process, including the choice of smoothing scale, treatment of redshift-space distortions, fiber assignment incompleteness, and parameterizations of the BAO model. We also present a series of tests that DESI followed in order to assess the maturity of the end-to-end galaxy BAO pipeline before the unblinding of the large-scale structure catalogs.","sentences":["Baryon acoustic oscillations (BAO) provide a robust standard ruler to measure the expansion history of the Universe through galaxy clustering.","Density-field reconstruction is now a widely adopted procedure for increasing the precision and accuracy of the BAO detection.","With the goal of finding the optimal reconstruction settings to be used in the DESI 2024 galaxy BAO analysis, we assess the sensitivity of the post-reconstruction BAO constraints to different choices in our analysis configuration, performing tests on blinded data from the first year of DESI observations (DR1), as well as on mocks that mimic the expected clustering and selection properties of the DESI DR1 target samples.","Overall, we find that BAO constraints remain robust against multiple aspects in the reconstruction process, including the choice of smoothing scale, treatment of redshift-space distortions, fiber assignment incompleteness, and parameterizations of the BAO model.","We also present a series of tests that DESI followed in order to assess the maturity of the end-to-end galaxy BAO pipeline before the unblinding of the large-scale structure catalogs."],"url":"http://arxiv.org/abs/2404.03005v1","category":"astro-ph.CO"}
{"created":"2024-04-03 18:00:01","title":"Spatiotemporal Quenches for Efficient Critical Ground State Preparation in Two-Dimensional Quantum Systems","abstract":"Quantum simulators have the potential to shed light on the study of quantum many-body systems and materials, offering unique insights into various quantum phenomena. While adiabatic evolution has been conventionally employed for state preparation, it faces challenges when the system evolves too quickly or the coherence time is limited. In such cases, shortcuts to adiabaticity, such as spatiotemporal quenches, provide a promising alternative. This paper numerically investigates the application of spatiotemporal quenches in the two-dimensional transverse field Ising model with ferromagnetic interactions, focusing on the emergence of the ground state and its correlation properties at criticality when the gap vanishes. We demonstrate the effectiveness of these quenches in rapidly preparing ground states in critical systems. Our simulations reveal the existence of an optimal quench front velocity at the emergent speed of light, leading to minimal excitation energy density and correlation lengths of the order of finite system sizes we can simulate. These findings emphasize the potential of spatiotemporal quenches for efficient ground state preparation in quantum systems, with implications for the exploration of strongly correlated phases and programmable quantum computing.","sentences":["Quantum simulators have the potential to shed light on the study of quantum many-body systems and materials, offering unique insights into various quantum phenomena.","While adiabatic evolution has been conventionally employed for state preparation, it faces challenges when the system evolves too quickly or the coherence time is limited.","In such cases, shortcuts to adiabaticity, such as spatiotemporal quenches, provide a promising alternative.","This paper numerically investigates the application of spatiotemporal quenches in the two-dimensional transverse field Ising model with ferromagnetic interactions, focusing on the emergence of the ground state and its correlation properties at criticality when the gap vanishes.","We demonstrate the effectiveness of these quenches in rapidly preparing ground states in critical systems.","Our simulations reveal the existence of an optimal quench front velocity at the emergent speed of light, leading to minimal excitation energy density and correlation lengths of the order of finite system sizes we can simulate.","These findings emphasize the potential of spatiotemporal quenches for efficient ground state preparation in quantum systems, with implications for the exploration of strongly correlated phases and programmable quantum computing."],"url":"http://arxiv.org/abs/2404.02957v1","category":"quant-ph"}
{"created":"2024-04-03 18:00:00","title":"Surrogate optimization of variational quantum circuits","abstract":"Variational quantum eigensolvers are touted as a near-term algorithm capable of impacting many applications. However, the potential has not yet been realized, with few claims of quantum advantage and high resource estimates, especially due to the need for optimization in the presence of noise. Finding algorithms and methods to improve convergence is important to accelerate the capabilities of near-term hardware for VQE or more broad applications of hybrid methods in which optimization is required. To this goal, we look to use modern approaches developed in circuit simulations and stochastic classical optimization, which can be combined to form a surrogate optimization approach to quantum circuits. Using an approximate (classical CPU/GPU) state vector simulator as a surrogate model, we efficiently calculate an approximate Hessian, passed as an input for a quantum processing unit or exact circuit simulator. This method will lend itself well to parallelization across quantum processing units. We demonstrate the capabilities of such an approach with and without sampling noise and a proof-of-principle demonstration on a quantum processing unit utilizing 40 qubits.","sentences":["Variational quantum eigensolvers are touted as a near-term algorithm capable of impacting many applications.","However, the potential has not yet been realized, with few claims of quantum advantage and high resource estimates, especially due to the need for optimization in the presence of noise.","Finding algorithms and methods to improve convergence is important to accelerate the capabilities of near-term hardware for VQE or more broad applications of hybrid methods in which optimization is required.","To this goal, we look to use modern approaches developed in circuit simulations and stochastic classical optimization, which can be combined to form a surrogate optimization approach to quantum circuits.","Using an approximate (classical CPU/GPU) state vector simulator as a surrogate model, we efficiently calculate an approximate Hessian, passed as an input for a quantum processing unit or exact circuit simulator.","This method will lend itself well to parallelization across quantum processing units.","We demonstrate the capabilities of such an approach with and without sampling noise and a proof-of-principle demonstration on a quantum processing unit utilizing 40 qubits."],"url":"http://arxiv.org/abs/2404.02951v1","category":"quant-ph"}
{"created":"2024-04-03 17:55:20","title":"A Mean Field Game Model for Timely Computation in Edge Computing Systems","abstract":"We consider the problem of task offloading in multi-access edge computing (MEC) systems constituting $N$ devices assisted by an edge server (ES), where the devices can split task execution between a local processor and the ES. Since the local task execution and communication with the ES both consume power, each device must judiciously choose between the two. We model the problem as a large population non-cooperative game among the $N$ devices. Since computation of an equilibrium in this scenario is difficult due to the presence of a large number of devices, we employ the mean-field game framework to reduce the finite-agent game problem to a generic user's multi-objective optimization problem, with a coupled consistency condition. By leveraging the novel age of information (AoI) metric, we invoke techniques from stochastic hybrid systems (SHS) theory and study the tradeoffs between increasing information freshness and reducing power consumption. In numerical simulations, we validate that a higher load at the ES may lead devices to upload their task to the ES less often.","sentences":["We consider the problem of task offloading in multi-access edge computing (MEC) systems constituting $N$ devices assisted by an edge server (ES), where the devices can split task execution between a local processor and the ES.","Since the local task execution and communication with the ES both consume power, each device must judiciously choose between the two.","We model the problem as a large population non-cooperative game among the $N$ devices.","Since computation of an equilibrium in this scenario is difficult due to the presence of a large number of devices, we employ the mean-field game framework to reduce the finite-agent game problem to a generic user's multi-objective optimization problem, with a coupled consistency condition.","By leveraging the novel age of information (AoI) metric, we invoke techniques from stochastic hybrid systems (SHS) theory and study the tradeoffs between increasing information freshness and reducing power consumption.","In numerical simulations, we validate that a higher load at the ES may lead devices to upload their task to the ES less often."],"url":"http://arxiv.org/abs/2404.02898v1","category":"cs.IT"}
{"created":"2024-04-03 16:50:05","title":"The Life Care Annuity: enhancing product features and refining pricing methods","abstract":"In this paper we provide more general features for the variable annuity contract with LTC payouts and GLWB proposed by the state-of-the-art and we refine its pricing methods. In particular, as to product features, we allow dynamic withdrawal strategies, including the surrender option. Furthermore, we consider stochastic interest rate, described by a Cox-Ingersoll-Ross (CIR) process. As to the numerical methods, we solve the stochastic control problem involved by the selection of the optimal withdrawal strategy by means of a robust tree method. We use such a method to estimate the fair price of the product. Furthermore, our numerical results show how the optimal withdrawal strategy varies over time with the health status of the policyholder. Our proposed tree method, we name Tree-LTC, proves to be efficient and reliable, when tested against the Monte Carlo approach.","sentences":["In this paper we provide more general features for the variable annuity contract with LTC payouts and GLWB proposed by the state-of-the-art and we refine its pricing methods.","In particular, as to product features, we allow dynamic withdrawal strategies, including the surrender option.","Furthermore, we consider stochastic interest rate, described by a Cox-Ingersoll-Ross (CIR) process.","As to the numerical methods, we solve the stochastic control problem involved by the selection of the optimal withdrawal strategy by means of a robust tree method.","We use such a method to estimate the fair price of the product.","Furthermore, our numerical results show how the optimal withdrawal strategy varies over time with the health status of the policyholder.","Our proposed tree method, we name Tree-LTC, proves to be efficient and reliable, when tested against the Monte Carlo approach."],"url":"http://arxiv.org/abs/2404.02858v1","category":"q-fin.CP"}
{"created":"2024-04-04 17:49:30","title":"A Canonical Quantization of Poisson Manifolds: a 2-Groupoid Scheme","abstract":"We canonically quantize a Poisson manifold to a Lie 2-groupoid, complete with a quantization map, and show that it relates geometric and deformation quantization: the perturbative expansion in $\\hbar$ of the (formal) convolution of two quantized functions yields Kontsevich's star product. Meanwhile, we can push forward this quantization map (by integrating over homotopies of paths) to obtain a quantization map in traditional geometric quantization. This gives a polarization-free, path integral definition of the quantization map, which does not have a prior definition for Poisson manifolds, and which only has a partial prescription for symplectic manifolds. We construct conventional quantum mechanics from this perspective.","sentences":["We canonically quantize a Poisson manifold to a Lie 2-groupoid, complete with a quantization map, and show that it relates geometric and deformation quantization: the perturbative expansion in $\\hbar$ of the (formal) convolution of two quantized functions yields Kontsevich's star product.","Meanwhile, we can push forward this quantization map (by integrating over homotopies of paths) to obtain a quantization map in traditional geometric quantization.","This gives a polarization-free, path integral definition of the quantization map, which does not have a prior definition for Poisson manifolds, and which only has a partial prescription for symplectic manifolds.","We construct conventional quantum mechanics from this perspective."],"url":"http://arxiv.org/abs/2404.03628v1","category":"math.SG"}
{"created":"2024-04-04 17:43:34","title":"Identifying Quasars from the DESI Bright Galaxy Survey","abstract":"The Dark Energy Spectroscopic Instrument (DESI) cosmology survey includes a Bright Galaxy Survey (BGS) which will yield spectra for over ten million bright galaxies (r<20.2 AB mag). The resulting sample will be valuable for both cosmological and astrophysical studies. However, the star/galaxy separation criterion implemented in the nominal BGS target selection algorithm excludes quasar host galaxies in addition to bona fide stars. While this excluded population is comparatively rare (~3-4 per square degrees), it may hold interesting clues regarding galaxy and quasar physics. Therefore, we present a target selection strategy that was implemented to recover these missing active galactic nuclei (AGN) from the BGS sample. The design of the selection criteria was both motivated and confirmed using spectroscopy. The resulting BGS-AGN sample is uniformly distributed over the entire DESI footprint. According to DESI survey validation data, the sample comprises 93% quasi-stellar objects (QSOs), 3% narrow-line AGN or blazars with a galaxy contamination rate of 2% and a stellar contamination rate of 2%. Peaking around redshift z=0.5, the BGS-AGN sample is intermediary between quasars from the rest of the BGS and those from the DESI QSO sample in terms of redshifts and AGN luminosities. The stacked spectrum is nearly identical to that of the DESI QSO targets, confirming that the sample is dominated by quasars. We highlight interesting small populations reaching z>2 which are either faint quasars with nearby projected companions or very bright quasars with strong absorption features including the Lyman-apha forest, metal absorbers and/or broad absorption lines.","sentences":["The Dark Energy Spectroscopic Instrument (DESI) cosmology survey includes a Bright Galaxy Survey (BGS) which will yield spectra for over ten million bright galaxies (r<20.2 AB mag).","The resulting sample will be valuable for both cosmological and astrophysical studies.","However, the star/galaxy separation criterion implemented in the nominal BGS target selection algorithm excludes quasar host galaxies in addition to bona fide stars.","While this excluded population is comparatively rare (~3-4 per square degrees), it may hold interesting clues regarding galaxy and quasar physics.","Therefore, we present a target selection strategy that was implemented to recover these missing active galactic nuclei (AGN) from the BGS sample.","The design of the selection criteria was both motivated and confirmed using spectroscopy.","The resulting BGS-AGN sample is uniformly distributed over the entire DESI footprint.","According to DESI survey validation data, the sample comprises 93% quasi-stellar objects (QSOs), 3% narrow-line AGN or blazars with a galaxy contamination rate of 2% and a stellar contamination rate of 2%.","Peaking around redshift z=0.5, the BGS-AGN sample is intermediary between quasars from the rest of the BGS and those from the DESI QSO sample in terms of redshifts and AGN luminosities.","The stacked spectrum is nearly identical to that of the DESI QSO targets, confirming that the sample is dominated by quasars.","We highlight interesting small populations reaching z>2 which are either faint quasars with nearby projected companions or very bright quasars with strong absorption features including the Lyman-apha forest, metal absorbers and/or broad absorption lines."],"url":"http://arxiv.org/abs/2404.03621v1","category":"astro-ph.GA"}
{"created":"2024-04-04 17:02:28","title":"DiffDet4SAR: Diffusion-based Aircraft Target Detection Network for SAR Images","abstract":"Aircraft target detection in SAR images is a challenging task due to the discrete scattering points and severe background clutter interference. Currently, methods with convolution-based or transformer-based paradigms cannot adequately address these issues. In this letter, we explore diffusion models for SAR image aircraft target detection for the first time and propose a novel \\underline{Diff}usion-based aircraft target \\underline{Det}ection network \\underline{for} \\underline{SAR} images (DiffDet4SAR). Specifically, the proposed DiffDet4SAR yields two main advantages for SAR aircraft target detection: 1) DiffDet4SAR maps the SAR aircraft target detection task to a denoising diffusion process of bounding boxes without heuristic anchor size selection, effectively enabling large variations in aircraft sizes to be accommodated; and 2) the dedicatedly designed Scattering Feature Enhancement (SFE) module further reduces the clutter intensity and enhances the target saliency during inference. Extensive experimental results on the SAR-AIRcraft-1.0 dataset show that the proposed DiffDet4SAR achieves 88.4\\% mAP$_{50}$, outperforming the state-of-the-art methods by 6\\%. Code is availabel at \\href{https://github.com/JoyeZLearning/DiffDet4SAR}.","sentences":["Aircraft target detection in SAR images is a challenging task due to the discrete scattering points and severe background clutter interference.","Currently, methods with convolution-based or transformer-based paradigms cannot adequately address these issues.","In this letter, we explore diffusion models for SAR image aircraft target detection for the first time and propose a novel \\underline{Diff}usion-based aircraft target \\underline{Det}ection network \\underline{for} \\underline{SAR} images (DiffDet4SAR).","Specifically, the proposed DiffDet4SAR yields two main advantages for SAR aircraft target detection: 1) DiffDet4SAR maps the SAR aircraft target detection task to a denoising diffusion process of bounding boxes without heuristic anchor size selection, effectively enabling large variations in aircraft sizes to be accommodated; and 2) the dedicatedly designed Scattering Feature Enhancement (SFE) module further reduces the clutter intensity and enhances the target saliency during inference.","Extensive experimental results on the SAR-AIRcraft-1.0 dataset show that the proposed DiffDet4SAR achieves 88.4\\% mAP$_{50}$, outperforming the state-of-the-art methods by 6\\%.","Code is availabel at \\href{https://github.com/JoyeZLearning/DiffDet4SAR}."],"url":"http://arxiv.org/abs/2404.03595v1","category":"eess.IV"}
{"created":"2024-04-04 16:55:24","title":"$\\bar b \\bar b u d$ and $\\bar b \\bar b u s$ tetraquarks from lattice QCD using symmetric correlation matrices with both local and scattering interpolating operators","abstract":"We study the $\\bar b \\bar b u d$ tetraquark with quantum numbers $I(J^P) = 0(1^+)$ as well as the $\\bar b \\bar b u s$ tetraquark with quantum numbers $J^P = 1^+$ using lattice QCD. We improve on existing work by including both local and scattering interpolating operators on both sides of the correlation functions and use symmetric correlation matrices. This allows not only a reliable determination of the energies of QCD-stable tetraquark ground states, but also of low-lying excited states, which are meson-meson scattering states. The latter is particularly important for future finite-volume scattering analyses. Here, we perform chiral and continuum extrapolations of just the ground-state energies, for which finite-volume effects are expected to be small. Our resulting tetraquark binding energies, $-100 \\pm 10\\:^{+36}_{-43}\\:\\:{\\rm MeV}$ for $\\bar b \\bar b u d$ and $-30 \\pm 3\\:^{+11}_{-31}\\:\\:{\\rm MeV}$ for $\\bar b \\bar b u s$, are consistent with other recent lattice-QCD predictions.","sentences":["We study the $\\bar b \\bar b u d$ tetraquark with quantum numbers $I(J^P)","= 0(1^+)$ as well as the $\\bar b \\bar b u s$ tetraquark with quantum numbers $J^P = 1^+$ using lattice QCD.","We improve on existing work by including both local and scattering interpolating operators on both sides of the correlation functions and use symmetric correlation matrices.","This allows not only a reliable determination of the energies of QCD-stable tetraquark ground states, but also of low-lying excited states, which are meson-meson scattering states.","The latter is particularly important for future finite-volume scattering analyses.","Here, we perform chiral and continuum extrapolations of just the ground-state energies, for which finite-volume effects are expected to be small.","Our resulting tetraquark binding energies, $-100 \\pm 10\\:^{+36}_{-43}\\:\\:{\\rm MeV}$ for $\\bar b \\bar b u d$ and $-30 \\pm 3\\:^{+11}_{-31}\\:\\:{\\rm MeV}$ for $\\bar b \\bar b u s$, are consistent with other recent lattice-QCD predictions."],"url":"http://arxiv.org/abs/2404.03588v1","category":"hep-lat"}
{"created":"2024-04-04 16:42:16","title":"The influence of substantial intragranular orientation gradients on the micromechanical response of heavily-worked material","abstract":"In this study, we examine the role of relatively large amounts of intragranular lattice misorientation - present after many thermomechanical processes - through three-dimensional (3D) micromechanical simulations of forged Al-7085 with a modified T7452 temper. We utilize near-field high-energy X-ray diffraction microscopy (HEDM) to measure 3D spatial orientation fields, and employ a novel reconstruction method that utilizes grain orientation envelopes measured using far-field HEDM to enable reconstruction of grains with intragranular orientation spreads >10{\\deg}. We construct two primary virtual polycrystalline specimens for use in crystal plasticity simulations to assess the effect of appreciable intragranular misorientation on the predicted deformation response: the first a faithful representation of the HEDM reconstruction, the second a microstructure with no intragranular misorientation (i.e., grain-averaged orientations). We find significant differences in the predicted deformation mechanism activation, distribution of stress, and distribution of plastic strain between simulations containing intragranular misorientation and those with homogenized orientations. The influence of elastic anisotropy is discussed, along with the effects of intragranular misorientation on fatigue life through the calculation and analysis of fatigue indicator parameters.","sentences":["In this study, we examine the role of relatively large amounts of intragranular lattice misorientation - present after many thermomechanical processes - through three-dimensional (3D) micromechanical simulations of forged Al-7085 with a modified T7452 temper.","We utilize near-field high-energy X-ray diffraction microscopy (HEDM) to measure 3D spatial orientation fields, and employ a novel reconstruction method that utilizes grain orientation envelopes measured using far-field HEDM to enable reconstruction of grains with intragranular orientation spreads >10{\\deg}.","We construct two primary virtual polycrystalline specimens for use in crystal plasticity simulations to assess the effect of appreciable intragranular misorientation on the predicted deformation response: the first a faithful representation of the HEDM reconstruction, the second a microstructure with no intragranular misorientation (i.e., grain-averaged orientations).","We find significant differences in the predicted deformation mechanism activation, distribution of stress, and distribution of plastic strain between simulations containing intragranular misorientation and those with homogenized orientations.","The influence of elastic anisotropy is discussed, along with the effects of intragranular misorientation on fatigue life through the calculation and analysis of fatigue indicator parameters."],"url":"http://arxiv.org/abs/2404.03579v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 16:39:21","title":"The Rise of Faint, Red AGN at $z>4$: A Sample of Little Red Dots in the JWST Extragalactic Legacy Fields","abstract":"We present a sample of 341 \"little red dots\" (LRDs) spanning the redshift range $z\\sim2-11$ using data from the CEERS, PRIMER, JADES, UNCOVER and NGDEEP surveys. These sources are likely heavily-reddened AGN that trace a previously-hidden phase of dust-obscured black hole growth in the early Universe. Unlike past use of color indices to identify LRDs, we employ continuum slope fitting using shifting bandpasses to sample the same rest-frame emission blueward and redward of the Balmer break. This approach allows us to identify LRDs over a wider redshift range and is less susceptible to contamination from galaxies with strong breaks that otherwise lack a rising red continuum. The redshift distribution of our sample increases at $z<8$ and then undergoes a rapid decline at $z\\sim4.5$, which may tie the emergence, and obscuration, of these sources to the inside-out growth that galaxies experience during this epoch. We find that LRDs are 2-3 dex more numerous than bright quasars at $z\\sim5-7$, but their number density is only 0.6-1 dex higher than X-ray and UV selected AGN at these redshifts. Within our sample, we have identified the first X-ray detected LRDs at $z=3.1$ and $z=4.66$. An X-ray spectral analysis confirms that these AGN are moderately obscured with $\\log\\,(N_{\\rm H}/{\\rm cm}^{2}$) of $23.3^{+0.4}_{-1.3}$ and $22.72^{+0.13}_{-0.16}$. Our analysis reveals that reddened AGN emission dominates their rest-optical light, while the rest-UV originates from their host galaxies. We also present NIRSpec follow-up spectroscopy of 17 LRDs that show broad emission lines consistent with AGN activity. The confirmed AGN fraction of our sample is $71\\%$ for sources with F444W$<26.5$. In addition, we find three LRDs with narrow blue-shifted Balmer absorption features in their spectra, suggesting an outflow of high-density, low ionization gas from near the central engine of these faint, red AGN.","sentences":["We present a sample of 341 \"little red dots\" (LRDs) spanning the redshift range $z\\sim2-11$ using data from the CEERS, PRIMER, JADES, UNCOVER and NGDEEP surveys.","These sources are likely heavily-reddened AGN that trace a previously-hidden phase of dust-obscured black hole growth in the early Universe.","Unlike past use of color indices to identify LRDs, we employ continuum slope fitting using shifting bandpasses to sample the same rest-frame emission blueward and redward of the Balmer break.","This approach allows us to identify LRDs over a wider redshift range and is less susceptible to contamination from galaxies with strong breaks that otherwise lack a rising red continuum.","The redshift distribution of our sample increases at $z<8$ and then undergoes a rapid decline at $z\\sim4.5$, which may tie the emergence, and obscuration, of these sources to the inside-out growth that galaxies experience during this epoch.","We find that LRDs are 2-3 dex more numerous than bright quasars at $z\\sim5-7$, but their number density is only 0.6-1 dex higher than X-ray and UV selected AGN at these redshifts.","Within our sample, we have identified the first X-ray detected LRDs at $z=3.1$ and $z=4.66$. An X-ray spectral analysis confirms that these AGN are moderately obscured with $\\log\\,(N_{\\rm H}/{\\rm cm}^{2}$) of $23.3^{+0.4}_{-1.3}$ and $22.72^{+0.13}_{-0.16}$. Our analysis reveals that reddened AGN emission dominates their rest-optical light, while the rest-UV originates from their host galaxies.","We also present NIRSpec follow-up spectroscopy of 17 LRDs that show broad emission lines consistent with AGN activity.","The confirmed AGN fraction of our sample is $71\\%$ for sources with F444W$<26.5$. In addition, we find three LRDs with narrow blue-shifted Balmer absorption features in their spectra, suggesting an outflow of high-density, low ionization gas from near the central engine of these faint, red AGN."],"url":"http://arxiv.org/abs/2404.03576v1","category":"astro-ph.GA"}
{"created":"2024-04-04 16:30:53","title":"Extra Higgs boson searches at the LHC","abstract":"Many searches for additional Higgs bosons, which are predicted by a lot of interesting models beyond the standard model, have been performed at the LHC. Some selected latest results of the searches for extra Higgs bosons at the LHC are presented. These additional Higgs bosons could be produced either directly from the parton interactions or from the decays of the observed standard model Higgs boson or a new heavier resonance. The searches used the data from proton-proton collisions delivered by the LHC at a centre-of-mass energy of $\\sqrt{s}=13~\\TeV$ and recorded with the ATLAS and CMS detectors. No direct evidence of new physics has been observed yet. Several mild excesses were observed in some final states. More data is needed to conclude on the nature of these excesses.","sentences":["Many searches for additional Higgs bosons, which are predicted by a lot of interesting models beyond the standard model, have been performed at the LHC.","Some selected latest results of the searches for extra Higgs bosons at the LHC are presented.","These additional Higgs bosons could be produced either directly from the parton interactions or from the decays of the observed standard model Higgs boson or a new heavier resonance.","The searches used the data from proton-proton collisions delivered by the LHC at a centre-of-mass energy of $\\sqrt{s}=13~\\TeV$ and recorded with the ATLAS and CMS detectors.","No direct evidence of new physics has been observed yet.","Several mild excesses were observed in some final states.","More data is needed to conclude on the nature of these excesses."],"url":"http://arxiv.org/abs/2404.03571v1","category":"hep-ex"}
{"created":"2024-04-04 15:37:50","title":"Impact of the Magnetic Horizon on the Interpretation of the Pierre Auger Observatory Spectrum and Composition Data","abstract":"The flux of ultra-high energy cosmic rays reaching Earth above the ankle energy (5 EeV) can be described as a mixture of nuclei injected by extragalactic sources with very hard spectra and a low rigidity cutoff. Extragalactic magnetic fields existing between the Earth and the closest sources can affect the observed CR spectrum by reducing the flux of low-rigidity particles reaching Earth. We perform a combined fit of the spectrum and distributions of depth of shower maximum measured with the Pierre Auger Observatory including the effect of this magnetic horizon in the propagation of UHECRs in the intergalactic space. We find that, within a specific range of the various experimental and phenomenological systematics, the magnetic horizon effect can be relevant for turbulent magnetic field strengths in the local neighbourhood of order $B_{\\rm rms}\\simeq (50-100)\\,{\\rm nG}\\,(20\\rm{Mpc}/{d_{\\rm s})( 100\\,\\rm{kpc}/L_{\\rm coh}})^{1/2}$, with $d_{\\rm s}$ the typical intersource separation and $L_{\\rm coh}$ the magnetic field coherence length. When this is the case, the inferred slope of the source spectrum becomes softer and can be closer to the expectations of diffusive shock acceleration, i.e., $\\propto E^{-2}$. An additional cosmic-ray population with higher source density and softer spectra, presumably also extragalactic and dominating the cosmic-ray flux at EeV energies, is also required to reproduce the overall spectrum and composition results for all energies down to 0.6~EeV.","sentences":["The flux of ultra-high energy cosmic rays reaching Earth above the ankle energy (5 EeV) can be described as a mixture of nuclei injected by extragalactic sources with very hard spectra and a low rigidity cutoff.","Extragalactic magnetic fields existing between the Earth and the closest sources can affect the observed CR spectrum by reducing the flux of low-rigidity particles reaching Earth.","We perform a combined fit of the spectrum and distributions of depth of shower maximum measured with the Pierre Auger Observatory including the effect of this magnetic horizon in the propagation of UHECRs in the intergalactic space.","We find that, within a specific range of the various experimental and phenomenological systematics, the magnetic horizon effect can be relevant for turbulent magnetic field strengths in the local neighbourhood of order $B_{\\rm rms}\\simeq (50-100)\\,{\\rm nG}\\,(20\\rm{Mpc}/{d_{\\rm s})( 100\\,\\rm{kpc}/L_{\\rm coh}})^{1/2}$, with $d_{\\rm s}$ the typical intersource separation and $L_{\\rm coh}$ the magnetic field coherence length.","When this is the case, the inferred slope of the source spectrum becomes softer and can be closer to the expectations of diffusive shock acceleration,","i.e., $\\propto E^{-2}$. An additional cosmic-ray population with higher source density and softer spectra, presumably also extragalactic and dominating the cosmic-ray flux at EeV energies, is also required to reproduce the overall spectrum and composition results for all energies down to 0.6~EeV."],"url":"http://arxiv.org/abs/2404.03533v1","category":"astro-ph.HE"}
{"created":"2024-04-04 15:24:03","title":"Correlation and Spectral Density Functions in Mode-Stirred Reverberation -- II. Spectral Moments, Sampling, Noise, EMI and Understirring","abstract":"In part I, spectral moments and kurtosis were established as parameters in analytic models of correlation and spectral density functions for dynamic reverberation fields. In this part II, several practical limitations affecting the accuracy of estimating these parameters from measured stir sweep data are investigated. For sampled fields, the contributions of finite differencing and aliasing are evaluated. Finite differencing results in a negative bias that depends, to leading order, quadratically on the product of the sampling time interval and the stir bandwidth. Numerical estimates of moments extracted directly from sampled stir sweeps show good agreement with values obtained by an autocovariance method. The effects of data decimation and noise-to-stir ratios of RMS amplitudes are determined and experimentally verified. In addition, the dependencies on the noise-to-stir-bandwidth ratio, EMI, and unstirred energy are characterized.","sentences":["In part I, spectral moments and kurtosis were established as parameters in analytic models of correlation and spectral density functions for dynamic reverberation fields.","In this part II, several practical limitations affecting the accuracy of estimating these parameters from measured stir sweep data are investigated.","For sampled fields, the contributions of finite differencing and aliasing are evaluated.","Finite differencing results in a negative bias that depends, to leading order, quadratically on the product of the sampling time interval and the stir bandwidth.","Numerical estimates of moments extracted directly from sampled stir sweeps show good agreement with values obtained by an autocovariance method.","The effects of data decimation and noise-to-stir ratios of RMS amplitudes are determined and experimentally verified.","In addition, the dependencies on the noise-to-stir-bandwidth ratio, EMI, and unstirred energy are characterized."],"url":"http://arxiv.org/abs/2404.03520v1","category":"physics.class-ph"}
{"created":"2024-04-04 15:22:30","title":"Inclusive $\\bar{B}\\to X_s \\ell^+\\ell^-$ at the LHC: theory predictions and new-physics reach","abstract":"We present theoretical predictions for observables in inclusive $\\bar{B}\\to X_s \\ell^+\\ell^-$ suitable for measurements at hadron colliders through a sum-over-exclusive approach. At low $q^2$ we calculate the branching ratio and three angular observables. At high $q^2$ we provide the branching ratio and the ratio of the $\\bar B \\to X_s \\ell^+\\ell^-$ rate with respect to the inclusive $\\bar B \\to X_u \\ell \\bar\\nu$ rate with the same phase-space cut. We compare our predictions to an extraction of the experimental rate through a sum-over-exclusive method using branching ratios of the exclusive $\\bar B \\to K^{(*)}\\mu^+\\mu^-$ modes measured at LHCb. Our analysis does not support a recent claim about a deficit in the inclusive branching ratio in the high-$q^2$ region. Finally, we present current model-independent bounds on new physics and emphasize the potential of complementary analyses of $\\bar{B}\\to X_s \\ell^+\\ell^-$ at Belle II and the LHC.","sentences":["We present theoretical predictions for observables in inclusive $\\bar{B}\\to X_s \\ell^+\\ell^-$ suitable for measurements at hadron colliders through a sum-over-exclusive approach.","At low $q^2$ we calculate the branching ratio and three angular observables.","At high $q^2$ we provide the branching ratio and the ratio of the $\\bar B \\to X_s \\ell^+\\ell^-$ rate with respect to the inclusive $\\bar B \\to X_u \\ell \\bar\\nu$ rate with the same phase-space cut.","We compare our predictions to an extraction of the experimental rate through a sum-over-exclusive method using branching ratios of the exclusive $\\bar B \\to K^{(*)}\\mu^+\\mu^-$ modes measured at LHCb.","Our analysis does not support a recent claim about a deficit in the inclusive branching ratio in the high-$q^2$ region.","Finally, we present current model-independent bounds on new physics and emphasize the potential of complementary analyses of $\\bar{B}\\to X_s \\ell^+\\ell^-$ at Belle II and the LHC."],"url":"http://arxiv.org/abs/2404.03517v1","category":"hep-ph"}
{"created":"2024-04-04 15:08:55","title":"Wilson Loops and Random Matrices","abstract":"Linear confinement with Casimir scaling of the string tension in confining gauge theories is a consequence of a certain property of the Polyakov loop related to random matrices. This mechanism does not depend on the details of the theories (neither the gauge group nor dimensions) and explains approximate Casimir scaling below string-breaking length. In this paper, we study 3d SU(2) pure Yang-Mills theory numerically and find the same random-matrix behavior for rectangular Wilson loops. We conjecture that this is a universal feature of strongly coupled confining gauge theories.","sentences":["Linear confinement with Casimir scaling of the string tension in confining gauge theories is a consequence of a certain property of the Polyakov loop related to random matrices.","This mechanism does not depend on the details of the theories (neither the gauge group nor dimensions) and explains approximate Casimir scaling below string-breaking length.","In this paper, we study 3d SU(2) pure Yang-Mills theory numerically and find the same random-matrix behavior for rectangular Wilson loops.","We conjecture that this is a universal feature of strongly coupled confining gauge theories."],"url":"http://arxiv.org/abs/2404.03503v1","category":"hep-th"}
{"created":"2024-04-04 14:21:35","title":"Integrated correlators at strong coupling in an orbifold of $\\mathcal{N}=4$ SYM","abstract":"We consider the $4d$ $\\mathcal{N}=2$ superconformal quiver gauge theory obtained by a $\\mathbb{Z}_2$ orbifold of $\\mathcal{N}=4$ super Yang-Mills (SYM). By exploiting supersymmetric localization, we study the integrated correlator of two Coulomb branch and two moment map operators and the integrated correlator of four moment map operators, determining exact expressions valid for any value of the 't Hooft coupling in the planar limit. Additionally, for the second correlator, we obtain an exact expression also for the next-to-planar contribution. Then, we derive the leading terms of their strong-coupling expansions and outline the differences with respect to the $\\mathcal{N}=4$ SYM theory.","sentences":["We consider the $4d$ $\\mathcal{N}=2$ superconformal quiver gauge theory obtained by a $\\mathbb{Z}_2$ orbifold of $\\mathcal{N}=4$ super Yang-Mills (SYM).","By exploiting supersymmetric localization, we study the integrated correlator of two Coulomb branch and two moment map operators and the integrated correlator of four moment map operators, determining exact expressions valid for any value of the 't Hooft coupling in the planar limit.","Additionally, for the second correlator, we obtain an exact expression also for the next-to-planar contribution.","Then, we derive the leading terms of their strong-coupling expansions and outline the differences with respect to the $\\mathcal{N}=4$ SYM theory."],"url":"http://arxiv.org/abs/2404.03466v1","category":"hep-th"}
{"created":"2024-04-04 14:20:45","title":"Superclimbing modes in transverse quantum fluids: signature statistical and dynamical features","abstract":"Superclimbing modes are hallmark degrees of freedom of transverse quantum fluids describing wide superfluid one-dimensional interfaces and/or edges with negligible Peierls barrier. We report the first direct numeric evidence of quantum shape fluctuations -- caused by superclimbing modes -- in simple lattice models, as well as at the free edge of an incomplete solid monolayer of $^4$He adsorbed on graphite. Our data unambiguously reveals the defining feature of the superclimbing modes -- canonical conjugation of the edge displacement field to the field of superfluid phase -- and its unexpected implication, i.e., that superfluid stiffness can be inferred from density snapshots.","sentences":["Superclimbing modes are hallmark degrees of freedom of transverse quantum fluids describing wide superfluid one-dimensional interfaces and/or edges with negligible Peierls barrier.","We report the first direct numeric evidence of quantum shape fluctuations -- caused by superclimbing modes -- in simple lattice models, as well as at the free edge of an incomplete solid monolayer of $^4$He adsorbed on graphite.","Our data unambiguously reveals the defining feature of the superclimbing modes -- canonical conjugation of the edge displacement field to the field of superfluid phase -- and its unexpected implication, i.e., that superfluid stiffness can be inferred from density snapshots."],"url":"http://arxiv.org/abs/2404.03465v1","category":"cond-mat.other"}
{"created":"2024-04-04 14:09:04","title":"Heating of Millisecond Pulsars by Magnetic Field Decay","abstract":"Millisecond pulsars (MSPs) are believed to be very old neutron stars (NSs) whose age may exceed significantly $10^8$ yrs. Although cooling scenarios of isolated NSs predict for that age a surface temperature $T_s\\sim 10^4$ K, observations of the nearest MSP J0437-4715 indicate $T_s$ well above that value. Besides the heating of the polar cap surface by backflowing charged particles, Joule heating in the crust can contribute to the overall heat budget of MSPs. Since the dipolar field component, derived from $P$ and $\\dot{P}$ measurements, is much too weak for remarkable heating, smaller-scale structures should be analysed whether they can supply the demanded heat. For this purpose we study the small scale field structure of radio pulsars. Magnetic field components, significantly stronger than the dipolar one, may exist especially at the surface of MSPs. We assign upper limits to the strength of single field components up to a multipolarity of $l=10$ and the corresponding deviations from axial symmetry $m \\le l$. Arguments are provided that the decay of the small-scale components with $l=3$ or $l=4$ of the crustal magnetic field may cause the relatively high surface temperature of isolated MSPs.","sentences":["Millisecond pulsars (MSPs) are believed to be very old neutron stars (NSs) whose age may exceed significantly $10^8$ yrs.","Although cooling scenarios of isolated NSs predict for that age a surface temperature $T_s\\sim 10^4$ K, observations of the nearest MSP J0437-4715 indicate $T_s$ well above that value.","Besides the heating of the polar cap surface by backflowing charged particles, Joule heating in the crust can contribute to the overall heat budget of MSPs.","Since the dipolar field component, derived from $P$ and $\\dot{P}$ measurements, is much too weak for remarkable heating, smaller-scale structures should be analysed whether they can supply the demanded heat.","For this purpose we study the small scale field structure of radio pulsars.","Magnetic field components, significantly stronger than the dipolar one, may exist especially at the surface of MSPs.","We assign upper limits to the strength of single field components up to a multipolarity of $l=10$ and the corresponding deviations from axial symmetry $m \\le l$. Arguments are provided that the decay of the small-scale components with $l=3$ or $l=4$ of the crustal magnetic field may cause the relatively high surface temperature of isolated MSPs."],"url":"http://arxiv.org/abs/2404.03458v1","category":"astro-ph.HE"}
{"created":"2024-04-04 13:49:47","title":"CO radial gradients in the bulge of M31","abstract":"We present new H- and K-band spectroscopy for the bulge of M31, taken with the LUCI spectrograph at the Large Binocular Telescope (LBT). We studied radial trends of CO absorption features (namely, CO1.58, CO1.60, CO1.64, CO1.66, CO1.68, CO2.30, CO2.32, CO2.35) in the bulge of M31, out to a galactocentric distance of 100'' (380pc). We find that most COs do not exhibit a strong radial gradient, despite the strong metallicity gradient inferred from the optical spectral range, except for CO1.64, showing a steep increase in the center. We compared the observed line strengths to predictions of different state-of-the-art stellar population models, including an updated version of EMILES models, which also uses the extended IRTF spectral library. The observed COs are close to models' predictions, but in some models they turn out to be underestimated. We find that the lack of radial gradients is due to the combination of increasing CO strength with metallicity and C abundance, and decreasing CO strength with IMF slope and O abundance. We speculate that the steep gradient of CO1.64 might be due to Na overabundance. Remarkably, we were able to fit, at the same time, optical indices and all the NIR COs except for CO1.68, leaving abundance ratios (i.e., [C/Fe], [O/Fe], and [Mg/Fe]) as free-fitting parameters, imposing age and metallicity constraints from the optical, with no significant contribution from intermediate-age populations. For the majority of the bulge, we find [Mg/Fe]~0.15dex, [O/Fe] larger than [Mg/Fe] (by ~0.1dex), and C abundance consistent with that of Mg. In the central (few arcsec) region, we still find an enhancement of O and Mg, but significantly lower [C/Fe]. We find that the COs' line strengths of the bulge are significantly lower than those of massive galaxies, possibly because of a difference in carbon abundance, as well as, to some extent, total metallicity.","sentences":["We present new H- and K-band spectroscopy for the bulge of M31, taken with the LUCI spectrograph at the Large Binocular Telescope (LBT).","We studied radial trends of CO absorption features (namely, CO1.58, CO1.60, CO1.64, CO1.66, CO1.68, CO2.30, CO2.32, CO2.35) in the bulge of M31, out to a galactocentric distance of 100'' (380pc).","We find that most COs do not exhibit a strong radial gradient, despite the strong metallicity gradient inferred from the optical spectral range, except for CO1.64, showing a steep increase in the center.","We compared the observed line strengths to predictions of different state-of-the-art stellar population models, including an updated version of EMILES models, which also uses the extended IRTF spectral library.","The observed COs are close to models' predictions, but in some models they turn out to be underestimated.","We find that the lack of radial gradients is due to the combination of increasing CO strength with metallicity and C abundance, and decreasing CO strength with IMF slope and O abundance.","We speculate that the steep gradient of CO1.64 might be due to Na overabundance.","Remarkably, we were able to fit, at the same time, optical indices and all the NIR COs except for CO1.68, leaving abundance ratios (i.e., [C/Fe], [O/Fe], and [Mg/Fe]) as free-fitting parameters, imposing age and metallicity constraints from the optical, with no significant contribution from intermediate-age populations.","For the majority of the bulge, we find [Mg/Fe]~0.15dex, [O/Fe] larger than [Mg/Fe] (by ~0.1dex), and C abundance consistent with that of Mg.","In the central (few arcsec) region, we still find an enhancement of O and Mg, but significantly lower [C/Fe].","We find that the COs' line strengths of the bulge are significantly lower than those of massive galaxies, possibly because of a difference in carbon abundance, as well as, to some extent, total metallicity."],"url":"http://arxiv.org/abs/2404.03448v1","category":"astro-ph.GA"}
{"created":"2024-04-04 12:58:23","title":"Modeling temporal dependency of longitudinal data: use of multivariate geometric skew-normal copula","abstract":"Use of copula for the purpose of modeling dependence has been receiving considerable attention in recent times. On the other hand, search for multivariate copulas with desirable dependence properties also is an important area of research. When fitting regression models to non-Gaussian longitudinal data, multivariate Gaussian copula is commonly used to account for temporal dependence of the repeated measurements. But using symmetric multivariate Gaussian copula is not preferable in every situation, since it can not capture non-exchangeable dependence or tail dependence, if present in the data. Hence to ensure reliable inference, it is important to look beyond the Gaussian dependence assumption. In this paper, we construct geometric skew-normal copula from multivariate geometric skew-normal (MGSN) distribution proposed by Kundu (2014) and Kundu (2017) in order to model temporal dependency of non-Gaussian longitudinal data. First we investigate the theoretical properties of the proposed multivariate copula, and then develop regression models for both continuous and discrete longitudinal data. The quantile function of this copula is independent of the correlation matrix of its respective multivariate distribution, which provides computational advantage in terms of likelihood inference compared to the class of copulas derived from skew-elliptical distributions by Azzalini & Valle (1996). Moreover, composite likelihood inference is possible for this multivariate copula, which facilitates to estimate parameters from ordered probit model with same dependence structure as geometric skew-normal distribution. We conduct extensive simulation studies to validate our proposed models and therefore apply them to analyze the longitudinal dependence of two real world data sets. Finally, we report our findings in terms of improvements over multivariate Gaussian copula based regression models.","sentences":["Use of copula for the purpose of modeling dependence has been receiving considerable attention in recent times.","On the other hand, search for multivariate copulas with desirable dependence properties also is an important area of research.","When fitting regression models to non-Gaussian longitudinal data, multivariate Gaussian copula is commonly used to account for temporal dependence of the repeated measurements.","But using symmetric multivariate Gaussian copula is not preferable in every situation, since it can not capture non-exchangeable dependence or tail dependence, if present in the data.","Hence to ensure reliable inference, it is important to look beyond the Gaussian dependence assumption.","In this paper, we construct geometric skew-normal copula from multivariate geometric skew-normal (MGSN) distribution proposed by Kundu (2014) and Kundu (2017) in order to model temporal dependency of non-Gaussian longitudinal data.","First we investigate the theoretical properties of the proposed multivariate copula, and then develop regression models for both continuous and discrete longitudinal data.","The quantile function of this copula is independent of the correlation matrix of its respective multivariate distribution, which provides computational advantage in terms of likelihood inference compared to the class of copulas derived from skew-elliptical distributions by Azzalini & Valle (1996).","Moreover, composite likelihood inference is possible for this multivariate copula, which facilitates to estimate parameters from ordered probit model with same dependence structure as geometric skew-normal distribution.","We conduct extensive simulation studies to validate our proposed models and therefore apply them to analyze the longitudinal dependence of two real world data sets.","Finally, we report our findings in terms of improvements over multivariate Gaussian copula based regression models."],"url":"http://arxiv.org/abs/2404.03420v1","category":"stat.ME"}
{"created":"2024-04-04 12:49:48","title":"Towards tailored magnetic anisotropy: A first-principles study of L1$_0$ FeNi ultrathin films","abstract":"In previous experiments, thin films of L1$\\mathrm{_0}$ FeNi with different surfaces, including (001), (110) and (111), were produced and studied. Each surface defines a different alignment of the crystallographic tetragonal axis with respect to the film's plane, resulting in different magnetic anisotropies. In this study, we use density functional theory calculations to examine three series of L1$\\mathrm{_0}$ FeNi films with surfaces (001), (010), and (111), and with thicknesses ranging from 0.5 to 3 nm (from 4 to 16 atomic monolayers). Our results show that films (001) have perpendicular magnetic anisotropy, while (010) favor in-plane magnetization, with a clear preference for the tetragonal axis [001]. We propose calling this type of in-plane anisotropy fixed in plane. A film with surface (111) and a thickness of four atomic monolayers has the magnetization easy axis almost perpendicular to the plane of the film. As the thickness of the (111) film increases, the direction of magnetization rotates towards a tetragonal axis [001], positioned at an angle of about 45$^{o}$ to the plane of the film. Furthermore, the magnetic moment of ultrathin films increases by a maximum of 5%, and the most significant changes in spin and orbital magnetic moments occur at a depth of about three near-surface atomic monolayers. The presented results could be useful for experimental efforts to synthesize ultrathin L1$\\mathrm{_0}$ FeNi films with different surfaces. Ultrathin L1$\\mathrm{_0}$ FeNi films with varying magnetic anisotropies may find applications in spintronic devices.","sentences":["In previous experiments, thin films of L1$\\mathrm{_0}$ FeNi with different surfaces, including (001), (110) and (111), were produced and studied.","Each surface defines a different alignment of the crystallographic tetragonal axis with respect to the film's plane, resulting in different magnetic anisotropies.","In this study, we use density functional theory calculations to examine three series of L1$\\mathrm{_0}$ FeNi films with surfaces (001), (010), and (111), and with thicknesses ranging from 0.5 to 3 nm (from 4 to 16 atomic monolayers).","Our results show that films (001) have perpendicular magnetic anisotropy, while (010) favor in-plane magnetization, with a clear preference for the tetragonal axis","[001].","We propose calling this type of in-plane anisotropy fixed in plane.","A film with surface (111) and a thickness of four atomic monolayers has the magnetization easy axis almost perpendicular to the plane of the film.","As the thickness of the (111) film increases, the direction of magnetization rotates towards a tetragonal axis [001], positioned at an angle of about 45$^{o}$ to the plane of the film.","Furthermore, the magnetic moment of ultrathin films increases by a maximum of 5%, and the most significant changes in spin and orbital magnetic moments occur at a depth of about three near-surface atomic monolayers.","The presented results could be useful for experimental efforts to synthesize ultrathin L1$\\mathrm{_0}$ FeNi films with different surfaces.","Ultrathin L1$\\mathrm{_0}$ FeNi films with varying magnetic anisotropies may find applications in spintronic devices."],"url":"http://arxiv.org/abs/2404.03416v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 12:03:56","title":"Chandra X-ray Analysis of Herbig Ae/Be Stars","abstract":"Herbig Ae/Be (HAeBe) stars are intermediate-mass pre-main sequence stars, characterized by infrared excess and emission lines. They are observed to emit X-rays, whose origin is a matter of discussion and not settled yet. X-ray emission is not expected in HAeBe stars, as they lack the sub-surface convective zone. In this study, we retrieved observations from the Chandra archive for 62 HAeBe stars, among which 44 sources (detection fraction $\\sim$71%) were detected in X-rays, with 7 being new detections. We use this sample as a test bed to conduct a comparative analysis of the X-ray properties of HAeBe stars and their low-mass counterparts, T Tauri Stars (TTSs). Further, we compare the X-ray properties of HAeBe stars and TTSs with optical and IR properties to constrain the X-ray emission mechanism in HAeBe stars. We found no correlation between X-ray emission and disk properties of HAeBe stars, confirming that X-rays are not related to accretion shocks. About 56% of HAeBe stars without any known sub-arcsec companions have lower plasma temperatures (kT $\\leq$ 2 keV). We observe flaring/variability in HAeBe stars with confirmed low-mass companions. These stars show plasma temperatures > 2 keV, similar to TTSs. Guided by this information we discuss the role of a T Tauri companion for X-ray emission seen in our sample of HAeBe stars. From the results obtained in this paper, we suggest that X-ray emission from HAeBe stars may not be related to accretion shocks or hidden TTS, but rather can be due to magnetically driven coronal emission.","sentences":["Herbig Ae/Be (HAeBe) stars are intermediate-mass pre-main sequence stars, characterized by infrared excess and emission lines.","They are observed to emit X-rays, whose origin is a matter of discussion and not settled yet.","X-ray emission is not expected in HAeBe stars, as they lack the sub-surface convective zone.","In this study, we retrieved observations from the Chandra archive for 62 HAeBe stars, among which 44 sources (detection fraction $\\sim$71%) were detected in X-rays, with 7 being new detections.","We use this sample as a test bed to conduct a comparative analysis of the X-ray properties of HAeBe stars and their low-mass counterparts, T Tauri Stars (TTSs).","Further, we compare the X-ray properties of HAeBe stars and TTSs with optical and IR properties to constrain the X-ray emission mechanism in HAeBe stars.","We found no correlation between X-ray emission and disk properties of HAeBe stars, confirming that X-rays are not related to accretion shocks.","About 56% of HAeBe stars without any known sub-arcsec companions have lower plasma temperatures (kT $\\leq$ 2 keV).","We observe flaring/variability in HAeBe stars with confirmed low-mass companions.","These stars show plasma temperatures > 2 keV, similar to TTSs.","Guided by this information we discuss the role of a T Tauri companion for X-ray emission seen in our sample of HAeBe stars.","From the results obtained in this paper, we suggest that X-ray emission from HAeBe stars may not be related to accretion shocks or hidden TTS, but rather can be due to magnetically driven coronal emission."],"url":"http://arxiv.org/abs/2404.03403v1","category":"astro-ph.SR"}
{"created":"2024-04-04 11:58:03","title":"Controllable non-Hermitian qubit-qubit Coupling in Superconducting quantum Circuit","abstract":"With a high-loss resonator supplying the non-Hermiticity, we study the Energy level degeneracy and quantum state evolution in tunable coupling superconducting quantum circuit. The qubit's effective energy level and damping rate can be continually tuned in superconducting circuit, and the positions and numbers of level degenerate points are controllable. The efficient of quantum state exchange and the asymmetry of quantum state evolution can be tuned with non-hermitian and nonreciprocal coupling between two qubits. The controllable non-Hermiticity provides new insights and methods for exploring the unconventional quantum effects in superconducting quantum circuit.","sentences":["With a high-loss resonator supplying the non-Hermiticity, we study the Energy level degeneracy and quantum state evolution in tunable coupling superconducting quantum circuit.","The qubit's effective energy level and damping rate can be continually tuned in superconducting circuit, and the positions and numbers of level degenerate points are controllable.","The efficient of quantum state exchange and the asymmetry of quantum state evolution can be tuned with non-hermitian and nonreciprocal coupling between two qubits.","The controllable non-Hermiticity provides new insights and methods for exploring the unconventional quantum effects in superconducting quantum circuit."],"url":"http://arxiv.org/abs/2404.03397v1","category":"quant-ph"}
{"created":"2024-04-04 11:23:33","title":"Search for the $B_s^0 \\rightarrow \u03bc^+\u03bc^-\u03b3$ decay","abstract":"A search for the fully reconstructed $B_s^0 \\rightarrow \\mu^+\\mu^-\\gamma$ decay is performed at the LHCb experiment using proton-proton collisions at $\\sqrt{s}=13$\\,TeV corresponding to an integrated luminosity of $5.4\\,\\mathrm{fb^{-1}}$. No significant signal is found and upper limits on the branching fraction in intervals of the dimuon mass are set   \\begin{align}   {\\cal B}(B_s^0 \\rightarrow \\mu^+\\mu^-\\gamma) < 4.2\\times10^{-8},~&m(\\mu\\mu)\\in[2m_\\mu,~1.70]\\,\\mathrm{GeV/c^2} ,\\nonumber   {\\cal B}(B_s^0 \\rightarrow \\mu^+\\mu^-\\gamma) < 7.7\\times10^{-8},~&m(\\mu\\mu)\\in[1.70,~2.88]\\,\\mathrm{GeV/c^2},\\nonumber   {\\cal B}(B_s^0 \\rightarrow \\mu^+\\mu^-\\gamma) < 4.2\\times10^{-8},~&m(\\mu\\mu)\\in[3.92 ,~m_{B_s^0}]\\,\\mathrm{GeV/c^2},\\nonumber \\end{align} at 95\\% confidence level. Additionally, upper limits are set on the branching fraction in the $[2m_\\mu,~1.70]\\,\\mathrm{GeV/c^2}$ dimuon mass region excluding the contribution from the intermediate $\\phi(1020)$ meson, and in the region combining all dimuon-mass intervals.","sentences":["A search for the fully reconstructed $B_s^0 \\rightarrow \\mu^+\\mu^-\\gamma$ decay is performed at the LHCb experiment using proton-proton collisions at $\\sqrt{s}=13$\\,TeV corresponding to an integrated luminosity of $5.4\\,\\mathrm{fb^{-1}}$. No significant signal is found and upper limits on the branching fraction in intervals of the dimuon mass are set   \\begin{align}   {\\cal B}(B_s^0 \\rightarrow \\mu^+\\mu^-\\gamma) <","4.2\\times10^{-8},~&m(\\mu\\mu)\\in[2m_\\mu,~1.70]\\,\\mathrm{GeV/c^2} ,\\nonumber   {\\cal B}(B_s^0 \\rightarrow \\mu^+\\mu^-\\gamma) <","7.7\\times10^{-8},~&m(\\mu\\mu)\\in[1.70,~2.88]\\,\\mathrm{GeV/c^2},\\nonumber   {\\cal B}(B_s^0 \\rightarrow \\mu^+\\mu^-\\gamma) <","4.2\\times10^{-8},~&m(\\mu\\mu)\\in[3.92 ,~m_{B_s^0}]\\,\\mathrm{GeV/c^2},\\nonumber \\end{align} at 95\\% confidence level.","Additionally, upper limits are set on the branching fraction in the $[2m_\\mu,~1.70]\\,\\mathrm{GeV/c^2}$ dimuon mass region excluding the contribution from the intermediate $\\phi(1020)$ meson, and in the region combining all dimuon-mass intervals."],"url":"http://arxiv.org/abs/2404.03375v1","category":"hep-ex"}
{"created":"2024-04-04 11:06:50","title":"Electronic structure of noncentrosymmetric B20 compound HfSn and tuning of multifold band-crossing points","abstract":"We present a detailed theoretical study of the electronic structure of hafnium tin HfSn crystallizing in a B$20$ structure, renowned for the diversity of physical and peculiar topological properties. The chiral crystal structure of these materials protects multifold band crossings located at high symmetry points. We employ density functional methods to reveal basic features of the band structure and Fermi surface topology of HfSn, on top of which an effective tight-binding model is built. The compound exhibits a fourfold band crossing pinned at the ${\\Gamma}$ point. We investigate routes that can shift such crossings towards the Fermi level, offering a unique way to possibly tune the compound`s properties. Specifically, we show that the energy position of the fourfold crossing can be easily manipulated via external perturbations such as strain and pressure. Considering that this point carries a topological charge larger than one, such tuning is of great importance. We anticipate that the approach presented in the current study can be utilized to investigate symmetry protected crossings in a wide class of materials.","sentences":["We present a detailed theoretical study of the electronic structure of hafnium tin HfSn crystallizing in a B$20$ structure, renowned for the diversity of physical and peculiar topological properties.","The chiral crystal structure of these materials protects multifold band crossings located at high symmetry points.","We employ density functional methods to reveal basic features of the band structure and Fermi surface topology of HfSn, on top of which an effective tight-binding model is built.","The compound exhibits a fourfold band crossing pinned at the ${\\Gamma}$ point.","We investigate routes that can shift such crossings towards the Fermi level, offering a unique way to possibly tune the compound`s properties.","Specifically, we show that the energy position of the fourfold crossing can be easily manipulated via external perturbations such as strain and pressure.","Considering that this point carries a topological charge larger than one, such tuning is of great importance.","We anticipate that the approach presented in the current study can be utilized to investigate symmetry protected crossings in a wide class of materials."],"url":"http://arxiv.org/abs/2404.03365v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 10:08:54","title":"Significantly Enhanced Vacancy Diffusion in Mn-containing Alloys","abstract":"Manipulating point defects for tailored macroscopic properties remains a formidable challenge in materials science. This study demonstrates a proof-of-principle for a universal law involving element Mn, significantly enhancing vacancy diffusion through an unprecedented anomalous Friedel Oscillations phenomenon, across most metals in the periodic table. The correlation between Mn-induced point-defect dynamic changes and intrinsic macro-properties is robustly validated through the first-principles theory and well-designed experiments. The physical origin stems from Mn's exceptionally large effective intra-elemental 3d electron interactions, surpassing the Coulomb attraction induced by vacancy and disrupting the electron screening effect. Given the ubiquitous nature of vacancies and their recognition as the most crucial defects influencing nearly all physical and mechanical properties of crystalline materials, this outcome may drive advances in a broad domain.","sentences":["Manipulating point defects for tailored macroscopic properties remains a formidable challenge in materials science.","This study demonstrates a proof-of-principle for a universal law involving element Mn, significantly enhancing vacancy diffusion through an unprecedented anomalous Friedel Oscillations phenomenon, across most metals in the periodic table.","The correlation between Mn-induced point-defect dynamic changes and intrinsic macro-properties is robustly validated through the first-principles theory and well-designed experiments.","The physical origin stems from Mn's exceptionally large effective intra-elemental 3d electron interactions, surpassing the Coulomb attraction induced by vacancy and disrupting the electron screening effect.","Given the ubiquitous nature of vacancies and their recognition as the most crucial defects influencing nearly all physical and mechanical properties of crystalline materials, this outcome may drive advances in a broad domain."],"url":"http://arxiv.org/abs/2404.03339v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 10:01:40","title":"The Impact-driven Atmospheric Loss of Super-Earths around Different Spectral Type Host Stars","abstract":"The planet's mass loss is important for the planet's formation and evolution. The radius valley (RV) is believed to be triggered by evaporation-induced mass loss. As an alternative mechanism for the RV, the mass loss of post-impact planets is thoroughly investigated in this work. The impact energy is converted to the planet's internal energy, enhancing its core energy and accelerating mass loss and orbital migration. As the host star changes from K-type to F-type, the planet's mass loss and orbital migration increase. When the initial gas-to-core mass ratio (GCR) is small, the migration efficiency for planets around K-type stars will increase, which helps to suppress mass loss and retain the planet's mass and radius within a specific range. On the contrary, planets around more massive F-type stars experience more substantial mass loss, potentially leading to complete mass loss, and migrate to orbits with longer periods. Our calculation shows that planets around different spectral types of host stars give rise to an RV ranging from 1.3-2.0 $R_{\\oplus}$, consistent with the observed range of 1.3-2.6 $R_{\\oplus}$. Despite the presence of uncertain parameters, the planetesimal impact can promote the RV establishment for planets around host stars of different spectral types.","sentences":["The planet's mass loss is important for the planet's formation and evolution.","The radius valley (RV) is believed to be triggered by evaporation-induced mass loss.","As an alternative mechanism for the RV, the mass loss of post-impact planets is thoroughly investigated in this work.","The impact energy is converted to the planet's internal energy, enhancing its core energy and accelerating mass loss and orbital migration.","As the host star changes from K-type to F-type, the planet's mass loss and orbital migration increase.","When the initial gas-to-core mass ratio (GCR) is small, the migration efficiency for planets around K-type stars will increase, which helps to suppress mass loss and retain the planet's mass and radius within a specific range.","On the contrary, planets around more massive F-type stars experience more substantial mass loss, potentially leading to complete mass loss, and migrate to orbits with longer periods.","Our calculation shows that planets around different spectral types of host stars give rise to an RV ranging from 1.3-2.0 $R_{\\oplus}$, consistent with the observed range of 1.3-2.6 $R_{\\oplus}$. Despite the presence of uncertain parameters, the planetesimal impact can promote the RV establishment for planets around host stars of different spectral types."],"url":"http://arxiv.org/abs/2404.03333v1","category":"astro-ph.EP"}
{"created":"2024-04-04 08:31:53","title":"On bipartite and tripartite entanglement at present and future particle colliders","abstract":"Entanglement, rooted in the non-deterministic, non-local nature of quantum mechanics, serves as a fundamental correlation. High-energy particle colliders offer a unique platform for exploring entanglement in the relativistic regime. The recent observation of entanglement in $t\\bar{t}$ production by ATLAS has sparked significant interest in investigating entanglement phenomena at colliders. While bipartite entanglement receives extensive attention, tripartite entanglement remains relatively uncharted. We investigate tripartite entanglement in $t\\bar{t}Z$ production at the Large Hadron Collider (LHC) within the Standard Model and with a dimension-$8$ effective operator. Additionally, we explore bipartite entanglement in $t\\bar{t}$, $tW^-$, and di-boson production processes, namely $W^+W^-$, $ZZ$, and $W^+Z$, at the LHC and future $e^+e^-$ collider. We numerically compute various measures of entanglement through Monte Carlo events based on the spin density matrix, with its elements (polarization and spin correlation) obtained by analyzing the angular distribution of the final decayed leptons.","sentences":["Entanglement, rooted in the non-deterministic, non-local nature of quantum mechanics, serves as a fundamental correlation.","High-energy particle colliders offer a unique platform for exploring entanglement in the relativistic regime.","The recent observation of entanglement in $t\\bar{t}$ production by ATLAS has sparked significant interest in investigating entanglement phenomena at colliders.","While bipartite entanglement receives extensive attention, tripartite entanglement remains relatively uncharted.","We investigate tripartite entanglement in $t\\bar{t}Z$ production at the Large Hadron Collider (LHC) within the Standard Model and with a dimension-$8$ effective operator.","Additionally, we explore bipartite entanglement in $t\\bar{t}$, $tW^-$, and di-boson production processes, namely $W^+W^-$, $ZZ$, and $W^+Z$, at the LHC and future $e^+e^-$ collider.","We numerically compute various measures of entanglement through Monte Carlo events based on the spin density matrix, with its elements (polarization and spin correlation) obtained by analyzing the angular distribution of the final decayed leptons."],"url":"http://arxiv.org/abs/2404.03292v1","category":"hep-ph"}
{"created":"2024-04-04 08:28:18","title":"$\\texttt{globin}$: A spectropolarimetric inversion code for the coupled inference of atomic line parameters","abstract":"For many transitions, atomic data, such as the oscillator strength (log(gf)) and the central wavelength of the line, are poorly constrained or even unknown. We present and test a new inversion method that infers atomic line parameters and the height stratification of the atmospheric parameters from spatially resolved spectropolarimetric observations of the Sun. This method is implemented in the new inversion code $\\texttt{globin}$. The new method imposes a spatial coupling in inversion parameters common to all pixels, such as the atomic parameters of the observed spectral lines, and infers atmospheric parameters for each spatial pixel individually. The uniqueness of this method lies in its ability to retrieve reliable atomic parameters even for heavily blended spectral lines. We tested the method by applying it to a set of 18 spectral lines between 4015 \\r{A} and 4017 \\r{A}, synthesized from a 3D magnetohydrodynamic simulation containing a sunspot and the quiet Sun region around it. The results were then compared with a previously used inversion method where atomic parameters were determined for every pixel independently (pixel-by-pixel method). The new method was able to retrieve the log(gf) values of all lines to an accuracy of 0.004 dex, while the pixel-by-pixel method retrieved the same parameter to an accuracy of only 0.025 dex. The largest differences between the two methods are evident for the heavily blended lines, with the former method performing better than the latter. In addition, the new method is also able to infer reliable atmospheric parameters in all the inverted pixels by successfully disentangling the degeneracies between the atomic and atmospheric parameters. The new method is well suited for the reliable determination of both atomic and atmospheric parameters and works well on all spectral lines, including those that are weak and/or severely blended.","sentences":["For many transitions, atomic data, such as the oscillator strength (log(gf)) and the central wavelength of the line, are poorly constrained or even unknown.","We present and test a new inversion method that infers atomic line parameters and the height stratification of the atmospheric parameters from spatially resolved spectropolarimetric observations of the Sun.","This method is implemented in the new inversion code $\\texttt{globin}$. The new method imposes a spatial coupling in inversion parameters common to all pixels, such as the atomic parameters of the observed spectral lines, and infers atmospheric parameters for each spatial pixel individually.","The uniqueness of this method lies in its ability to retrieve reliable atomic parameters even for heavily blended spectral lines.","We tested the method by applying it to a set of 18 spectral lines between 4015 \\r{A} and 4017 \\r{A}, synthesized from a 3D magnetohydrodynamic simulation containing a sunspot and the quiet Sun region around it.","The results were then compared with a previously used inversion method where atomic parameters were determined for every pixel independently (pixel-by-pixel method).","The new method was able to retrieve the log(gf) values of all lines to an accuracy of 0.004 dex, while the pixel-by-pixel method retrieved the same parameter to an accuracy of only 0.025 dex.","The largest differences between the two methods are evident for the heavily blended lines, with the former method performing better than the latter.","In addition, the new method is also able to infer reliable atmospheric parameters in all the inverted pixels by successfully disentangling the degeneracies between the atomic and atmospheric parameters.","The new method is well suited for the reliable determination of both atomic and atmospheric parameters and works well on all spectral lines, including those that are weak and/or severely blended."],"url":"http://arxiv.org/abs/2404.03291v1","category":"astro-ph.SR"}
{"created":"2024-04-04 07:28:28","title":"A New Tidal Stream Discovered in Gaia DR3","abstract":"Thanks to the precise astrometric measurements of proper motions by the Gaia mission, a new tidal stellar stream has been discovered in the northern hemisphere. The distribution of star count shows that the stream is approximately $80$ degrees long and $1.70$ degrees wide. Observations of $21$ member stars, including 14 RR Lyrae stars, indicate that the stream has an eccentric and retrograde orbit with $e=0.58$. The low metallicity, high total energy, and large angular momentum suggest that it is associated with the merging event Sequoia. This discovery suggests the possibility of finding more substructures with high eccentricity orbits, even in the inner halo.","sentences":["Thanks to the precise astrometric measurements of proper motions by the Gaia mission, a new tidal stellar stream has been discovered in the northern hemisphere.","The distribution of star count shows that the stream is approximately $80$ degrees long and $1.70$ degrees wide.","Observations of $21$ member stars, including 14 RR Lyrae stars, indicate that the stream has an eccentric and retrograde orbit with $e=0.58$. The low metallicity, high total energy, and large angular momentum suggest that it is associated with the merging event Sequoia.","This discovery suggests the possibility of finding more substructures with high eccentricity orbits, even in the inner halo."],"url":"http://arxiv.org/abs/2404.03257v1","category":"astro-ph.GA"}
{"created":"2024-04-04 07:16:59","title":"Precision tests of bulk entanglement entropy","abstract":"We consider linear superpositions of single particle excitations in a scalar field theory on $AdS_3$ and evaluate their contribution to the bulk entanglement entropy across the Ryu-Takayanagi surface. We compare the entanglement entropy of these excitations obtained using the Faulkner-Lewkowycz-Maldacena formula to the entanglement entropy of linear superposition of global descendants of a conformal primary in a large $c$ CFT obtained using the replica trick. We show that the closed from expressions for the entanglement entropy in the small interval expansion both in gravity and the CFT precisely agree. The agreement serves as a non-trivial check of the FLM formula for the quantum corrections to holographic entropy which also involves a contribution from the back reacted minimal area. Our checks includes an example in which the state is time dependent and spatially in-homogenous as well another example involving a coherent state with a Ba\\~{n}ados geometry as its holographic dual.","sentences":["We consider linear superpositions of single particle excitations in a scalar field theory on $AdS_3$ and evaluate their contribution to the bulk entanglement entropy across the Ryu-Takayanagi surface.","We compare the entanglement entropy of these excitations obtained using the Faulkner-Lewkowycz-Maldacena formula to the entanglement entropy of linear superposition of global descendants of a conformal primary in a large $c$ CFT obtained using the replica trick.","We show that the closed from expressions for the entanglement entropy in the small interval expansion both in gravity and the CFT precisely agree.","The agreement serves as a non-trivial check of the FLM formula for the quantum corrections to holographic entropy which also involves a contribution from the back reacted minimal area.","Our checks includes an example in which the state is time dependent and spatially in-homogenous as well another example involving a coherent state with a Ba\\~{n}ados geometry as its holographic dual."],"url":"http://arxiv.org/abs/2404.03252v1","category":"hep-th"}
{"created":"2024-04-04 07:01:14","title":"Scale-dependent local primordial non-Gaussianity as a solution to the $S_8$ tension","abstract":"For the last decade, several probes have pointed to a cosmological tension between the amplitude of density fluctuations extrapolated from the cosmic microwave background within the standard cosmological model and the one encapsulated by the $S_8$ parameter from large scale structure. The origin of this $S_8$ tension has not yet been elucidated and may hint at systematics in the data, unaccounted effects from baryonic physics, or new physics beyond the standard model of cosmology. Baryonic physics may in principle provide a nonlinear solution to the tension by suppressing the matter power spectrum more strongly on nonlinear scales than is traditionally assumed. Such a solution would not worsen the Hubble tension, contrary to many other proposed solutions to the $S_8$ tension. However, no realistic baryonic feedback in hydrodynamical simulations provides the needed suppression as a function of redshift. Here, we point out that a scale-dependence of local-type primordial non-Gaussianities (PNG), with significant PNG at scales of a few Mpc, can provide the needed suppression, since such PNG can suppress the power spectrum at slightly larger scales than baryons do. We demonstrate this by devising collisionless numerical simulations of structure formation in boxes of 0.5 Gpc/h with scale-dependent local-type PNG. Our simple models show that, as a proof of principle, scale-dependent PNG, with a Gaussian random field for primodial density fluctuations on large scales and $f_{\\rm NL} \\simeq -300$ at $\\lesssim 10$ Mpc scales, together with state-of-the-art baryonification of the matter power spectrum, can in principle solve the $S_8$ tension. The $S_8$ tension would then be a smoking-gun of non-trivial inflationary physics.","sentences":["For the last decade, several probes have pointed to a cosmological tension between the amplitude of density fluctuations extrapolated from the cosmic microwave background within the standard cosmological model and the one encapsulated by the $S_8$ parameter from large scale structure.","The origin of this $S_8$ tension has not yet been elucidated and may hint at systematics in the data, unaccounted effects from baryonic physics, or new physics beyond the standard model of cosmology.","Baryonic physics may in principle provide a nonlinear solution to the tension by suppressing the matter power spectrum more strongly on nonlinear scales than is traditionally assumed.","Such a solution would not worsen the Hubble tension, contrary to many other proposed solutions to the $S_8$ tension.","However, no realistic baryonic feedback in hydrodynamical simulations provides the needed suppression as a function of redshift.","Here, we point out that a scale-dependence of local-type primordial non-Gaussianities (PNG), with significant PNG at scales of a few Mpc, can provide the needed suppression, since such PNG can suppress the power spectrum at slightly larger scales than baryons do.","We demonstrate this by devising collisionless numerical simulations of structure formation in boxes of 0.5 Gpc/h with scale-dependent local-type PNG.","Our simple models show that, as a proof of principle, scale-dependent PNG, with a Gaussian random field for primodial density fluctuations on large scales and $f_{\\rm NL} \\simeq -300$ at $\\lesssim 10$ Mpc scales, together with state-of-the-art baryonification of the matter power spectrum, can in principle solve the $S_8$ tension.","The $S_8$ tension would then be a smoking-gun of non-trivial inflationary physics."],"url":"http://arxiv.org/abs/2404.03244v1","category":"astro-ph.CO"}
{"created":"2024-04-04 06:37:15","title":"Discrete origins of matter","abstract":"We discuss models of the flavour problem and dark matter based on the discrete $\\mathcal{Z}_{\\rm N} \\times \\mathcal{Z}_{\\rm M} \\times \\mathcal{Z}_{\\rm P}$ flavour symmetry. A new class of dark-matter emerges out of these models, which is defined as the flavonic dark matter. An ultra-violet completion of these models based on the dark-technicolour paradigm is also presented.","sentences":["We discuss models of the flavour problem and dark matter based on the discrete $\\mathcal{Z}_{\\rm N} \\times \\mathcal{Z}_{\\rm M} \\times \\mathcal{Z}_{\\rm P}$ flavour symmetry.","A new class of dark-matter emerges out of these models, which is defined as the flavonic dark matter.","An ultra-violet completion of these models based on the dark-technicolour paradigm is also presented."],"url":"http://arxiv.org/abs/2404.03232v1","category":"hep-ph"}
{"created":"2024-04-04 05:39:48","title":"When Strings Surprise","abstract":"We argue that on-shell excitations with large negative energies are created rapidly when the string coupling increases with time. This does not indicate an inconsistency in string theory since the negative energy on-shell excitation is always entangled with an on-shell excitation with a positive energy. The total energy of this energy-EPR state vanishes. We discuss the reason the energy-EPR states appear in string theory and the role they might play in black hole physics.","sentences":["We argue that on-shell excitations with large negative energies are created rapidly when the string coupling increases with time.","This does not indicate an inconsistency in string theory since the negative energy on-shell excitation is always entangled with an on-shell excitation with a positive energy.","The total energy of this energy-EPR state vanishes.","We discuss the reason the energy-EPR states appear in string theory and the role they might play in black hole physics."],"url":"http://arxiv.org/abs/2404.03215v1","category":"hep-th"}
{"created":"2024-04-04 05:08:34","title":"Negative-energy waves in the vertical threads of a solar prominence","abstract":"Solar prominences, intricate structures on the Sun's limb, have been a subject of fascination due to their thread-like features and dynamic behaviors. Utilizing data from the New Vacuum Solar Telescope (NVST), Chinese H_alpha Solar Explorer (CHASE), and Solar Dynamics Observatory (SDO), this study investigates the transverse swaying motions observed in the vertical threads of a solar prominence during its eruption onset on May 11, 2023. The transverse swaying motions were observed to propagate upward, accompanied by upflowing materials at an inclination of 31 degrees relative to the plane of the sky. These motions displayed small-amplitude oscillations with corrected velocities of around 3-4 km/s and periods of 13-17 minutes. Over time, the oscillations of swaying motion exhibited an increasing pattern in displacement amplitudes, oscillatory periods, and projected velocity amplitudes. Their phase velocities are estimated to be about 26-34 km/s. An important finding is that these oscillations'phase velocities are comparable to the upward flow velocities, measured to be around 30-34 km/s. We propose that this phenomenon is associated with negative-energy wave instabilities, which require comparable velocities of the waves and flows, as indicated by our findings. This phenomenon may contribute to the instability and observed disruption of the prominence. By using prominence seismology, the Alfven speed and magnetic field strength of the vertical threads have been estimated to be approximately 21.5 km/s and 2 Gauss, respectively. This study reveals the dynamics and magnetic properties of solar prominences, contributing to our understanding of their behavior in the solar atmosphere.","sentences":["Solar prominences, intricate structures on the Sun's limb, have been a subject of fascination due to their thread-like features and dynamic behaviors.","Utilizing data from the New Vacuum Solar Telescope (NVST), Chinese H_alpha Solar Explorer (CHASE), and Solar Dynamics Observatory (SDO), this study investigates the transverse swaying motions observed in the vertical threads of a solar prominence during its eruption onset on May 11, 2023.","The transverse swaying motions were observed to propagate upward, accompanied by upflowing materials at an inclination of 31 degrees relative to the plane of the sky.","These motions displayed small-amplitude oscillations with corrected velocities of around 3-4 km/s and periods of 13-17 minutes.","Over time, the oscillations of swaying motion exhibited an increasing pattern in displacement amplitudes, oscillatory periods, and projected velocity amplitudes.","Their phase velocities are estimated to be about 26-34 km/s. An important finding is that these oscillations'phase velocities are comparable to the upward flow velocities, measured to be around 30-34 km/s. We propose that this phenomenon is associated with negative-energy wave instabilities, which require comparable velocities of the waves and flows, as indicated by our findings.","This phenomenon may contribute to the instability and observed disruption of the prominence.","By using prominence seismology, the Alfven speed and magnetic field strength of the vertical threads have been estimated to be approximately 21.5 km/s and 2 Gauss, respectively.","This study reveals the dynamics and magnetic properties of solar prominences, contributing to our understanding of their behavior in the solar atmosphere."],"url":"http://arxiv.org/abs/2404.03199v1","category":"astro-ph.SR"}
{"created":"2024-04-04 04:34:37","title":"On type-II Spacetimes and the Double Copy for Fluids Metrics","abstract":"In our previous paper (arXiv:2005.04242) we discussed type-D and type-N fluid-dual spacetimes and provided their associated single copies in the context of the Weyl double copy. In this work we extend our analysis to more general fluids thereby requiring the application of the double copy picture to type-II space-times. By combining our previous type-D and type-N fluids via their associated stream functions we demonstrate an example of a viable type-II double copy. Further we use an explicitly perturbative approach in the near horizon expansion to generalize the type II double copy for the fluid-dual space-times. We show a Maxwell spinor ansatz containing a heterogeneous bi-spinor component is necessary to provide a viable type-II double copy at the lowest order.","sentences":["In our previous paper (arXiv:2005.04242) we discussed type-D and type-N fluid-dual spacetimes and provided their associated single copies in the context of the Weyl double copy.","In this work we extend our analysis to more general fluids thereby requiring the application of the double copy picture to type-II space-times.","By combining our previous type-D and type-N fluids via their associated stream functions we demonstrate an example of a viable type-II double copy.","Further we use an explicitly perturbative approach in the near horizon expansion to generalize the type II double copy for the fluid-dual space-times.","We show a Maxwell spinor ansatz containing a heterogeneous bi-spinor component is necessary to provide a viable type-II double copy at the lowest order."],"url":"http://arxiv.org/abs/2404.03195v1","category":"hep-th"}
{"created":"2024-04-04 02:35:39","title":"Searching for binary black hole sub-populations in gravitational wave data using binned Gaussian processes","abstract":"Astrophysically motivated population models for binary black hole observables are often insufficient to capture the imprints of multiple formation channels. This is mainly due to the strongly parametrized nature of such investigations. Using a non-parametric model for the joint population-level distributions of binary black hole component masses and effective inspiral spins, we find hints of multiple subpopulations in the third gravitational wave transient catalog. The higher (more positive) spin subpopulation is found to have a mass-spectrum without any feature at the $30-40M_{\\odot}$, which is consistent with the predictions of isolated stellar binary evolution, simulations for which place the pile up due to pulsational pair-instability supernovae near $50M_{\\odot}$ or higher. The other sub-population with effective spins closer to zero shows a feature at $30-40M_{\\odot}$ and is consistent with binary black holes formed dynamically in globular clusters, which are expected to peak around $30M_{\\odot}$. We also compute merger rates for these two subpopulations and find that they are consistent with the theoretical predictions of the corresponding formation channels. We validate our results by checking their robustness against variations of several model configurations and by analyzing large simulated catalogs with the same model.","sentences":["Astrophysically motivated population models for binary black hole observables are often insufficient to capture the imprints of multiple formation channels.","This is mainly due to the strongly parametrized nature of such investigations.","Using a non-parametric model for the joint population-level distributions of binary black hole component masses and effective inspiral spins, we find hints of multiple subpopulations in the third gravitational wave transient catalog.","The higher (more positive) spin subpopulation is found to have a mass-spectrum without any feature at the $30-40M_{\\odot}$, which is consistent with the predictions of isolated stellar binary evolution, simulations for which place the pile up due to pulsational pair-instability supernovae near $50M_{\\odot}$ or higher.","The other sub-population with effective spins closer to zero shows a feature at $30-40M_{\\odot}$ and is consistent with binary black holes formed dynamically in globular clusters, which are expected to peak around $30M_{\\odot}$. We also compute merger rates for these two subpopulations and find that they are consistent with the theoretical predictions of the corresponding formation channels.","We validate our results by checking their robustness against variations of several model configurations and by analyzing large simulated catalogs with the same model."],"url":"http://arxiv.org/abs/2404.03166v1","category":"astro-ph.HE"}
{"created":"2024-04-04 01:44:40","title":"Convexity and Osculation in Normed Spaces","abstract":"Constructive properties of uniform convexity, strict convexity, near convexity, and metric convexity in real normed linear spaces are considered. Examples show that certain classical theorems, such as the existence of points of osculation, are constructively invalid. The methods used are in accord with principles introduced by Errett Bishop","sentences":["Constructive properties of uniform convexity, strict convexity, near convexity, and metric convexity in real normed linear spaces are considered.","Examples show that certain classical theorems, such as the existence of points of osculation, are constructively invalid.","The methods used are in accord with principles introduced by Errett Bishop"],"url":"http://arxiv.org/abs/2404.03148v1","category":"math.FA"}
{"created":"2024-04-04 01:31:27","title":"A model for galaxy-galaxy strong lensing statistics in surveys","abstract":"Photometric wide-area observations in the next decade will be capable of detecting a large number of galaxy-scale strong gravitational lenses, increasing the gravitational lens sample size by orders of magnitude. To aid in forecasting and analysis of these surveys, we construct a flexible model based on observed distributions for the lens and source properties and test it on the results of past lens searches, including SL2S, SuGOHI and searches on the COSMOS HST and DES fields. We use this model to estimate the expected yields of some current and planned surveys, including Euclid Wide, Vera Rubin LSST, and Roman High Latitude Wide Area. The model proposed includes a set of free parameters to constrain on the identifiability of a lens in an image, allowing construction of prior probability distributions for different lens detection methods. The code used in this work is made publicly available.","sentences":["Photometric wide-area observations in the next decade will be capable of detecting a large number of galaxy-scale strong gravitational lenses, increasing the gravitational lens sample size by orders of magnitude.","To aid in forecasting and analysis of these surveys, we construct a flexible model based on observed distributions for the lens and source properties and test it on the results of past lens searches, including SL2S, SuGOHI and searches on the COSMOS HST and DES fields.","We use this model to estimate the expected yields of some current and planned surveys, including Euclid Wide, Vera Rubin LSST, and Roman High Latitude Wide Area.","The model proposed includes a set of free parameters to constrain on the identifiability of a lens in an image, allowing construction of prior probability distributions for different lens detection methods.","The code used in this work is made publicly available."],"url":"http://arxiv.org/abs/2404.03143v1","category":"astro-ph.CO"}
{"created":"2024-04-04 01:26:50","title":"The Role of Torsion in Trans-Planckian Physics","abstract":"The torsion of spacetime, if exists, plays an important role at the very early universe when the spin density of particles was very high. It is generally believed that in extremely high energies of the early universe a new physics called trans-Planckian physics should be considered. Since the initial conditions for inflation are probably a result of this new physics, here we consider spin and torsion as trans- Planckian effects and analyze their influence on the power spectrum of scalar and tensor perturbations at the end of the inflationary era.","sentences":["The torsion of spacetime, if exists, plays an important role at the very early universe when the spin density of particles was very high.","It is generally believed that in extremely high energies of the early universe a new physics called trans-Planckian physics should be considered.","Since the initial conditions for inflation are probably a result of this new physics, here we consider spin and torsion as trans- Planckian effects and analyze their influence on the power spectrum of scalar and tensor perturbations at the end of the inflationary era."],"url":"http://arxiv.org/abs/2404.03141v1","category":"gr-qc"}
{"created":"2024-04-04 01:25:10","title":"A first extraction of the weak magnetism form factor and Fierz interference term from the $^{114}$In $\\rightarrow$ $^{114}$Sn Gamow-Teller transition","abstract":"Spectrum shape measurements in nuclear $\\beta$ decay can be used to test physics beyond the Standard Model with results being complementary to high-energy collider experiments. In particular, Beyond Standard Model sensitivity of the weak interaction is expressed through the so-called Fierz interference term. Additionally, the $\\beta$ spectrum shape is a useful tool to probe Standard Model effects, among which the most prominent is \\textit{weak magnetism}, a higher-order recoil correction induced by nuclear pion exchange. To study effects in the $\\beta$ spectrum shape at a precision level competitive with the LHC, a new spectrometer was designed and built. It consists of a 3D low-pressure gas electron tracker and a plastic scintillator used for triggering the data acquisition and recording the $\\beta$ particle energy. In this Letter, the results from $\\beta$ spectrum shape measurements on the allowed Gamow-Teller transition $^{114}\\text{In} \\rightarrow ^{114}\\text{Sn}$ are presented, including a first extraction of the weak magnetism form factor in the high nuclear mass range and a new estimate of the $90\\%$ confidence interval for the Fierz interference term.","sentences":["Spectrum shape measurements in nuclear $\\beta$ decay can be used to test physics beyond the Standard Model with results being complementary to high-energy collider experiments.","In particular, Beyond Standard Model sensitivity of the weak interaction is expressed through the so-called Fierz interference term.","Additionally, the $\\beta$ spectrum shape is a useful tool to probe Standard Model effects, among which the most prominent is \\textit{weak magnetism}, a higher-order recoil correction induced by nuclear pion exchange.","To study effects in the $\\beta$ spectrum shape at a precision level competitive with the LHC, a new spectrometer was designed and built.","It consists of a 3D low-pressure gas electron tracker and a plastic scintillator used for triggering the data acquisition and recording the $\\beta$ particle energy.","In this Letter, the results from $\\beta$ spectrum shape measurements on the allowed Gamow-Teller transition $^{114}\\text{In} \\rightarrow ^{114}\\text{Sn}$ are presented, including a first extraction of the weak magnetism form factor in the high nuclear mass range and a new estimate of the $90\\%$ confidence interval for the Fierz interference term."],"url":"http://arxiv.org/abs/2404.03140v1","category":"nucl-ex"}
{"created":"2024-04-04 01:07:14","title":"Robust Pronoun Use Fidelity with English LLMs: Are they Reasoning, Repeating, or Just Biased?","abstract":"Robust, faithful and harm-free pronoun use for individuals is an important goal for language models as their use increases, but prior work tends to study only one or two of these components at a time. To measure progress towards the combined goal, we introduce the task of pronoun use fidelity: given a context introducing a co-referring entity and pronoun, the task is to reuse the correct pronoun later, independent of potential distractors. We present a carefully-designed dataset of over 5 million instances to evaluate pronoun use fidelity in English, and we use it to evaluate 37 popular large language models across architectures (encoder-only, decoder-only and encoder-decoder) and scales (11M-70B parameters). We find that while models can mostly faithfully reuse previously-specified pronouns in the presence of no distractors, they are significantly worse at processing she/her/her, singular they and neopronouns. Additionally, models are not robustly faithful to pronouns, as they are easily distracted. With even one additional sentence containing a distractor pronoun, accuracy drops on average by 34%. With 5 distractor sentences, accuracy drops by 52% for decoder-only models and 13% for encoder-only models. We show that widely-used large language models are still brittle, with large gaps in reasoning and in processing different pronouns in a setting that is very simple for humans, and we encourage researchers in bias and reasoning to bridge them.","sentences":["Robust, faithful and harm-free pronoun use for individuals is an important goal for language models as their use increases, but prior work tends to study only one or two of these components at a time.","To measure progress towards the combined goal, we introduce the task of pronoun use fidelity: given a context introducing a co-referring entity and pronoun, the task is to reuse the correct pronoun later, independent of potential distractors.","We present a carefully-designed dataset of over 5 million instances to evaluate pronoun use fidelity in English, and we use it to evaluate 37 popular large language models across architectures (encoder-only, decoder-only and encoder-decoder) and scales (11M-70B parameters).","We find that while models can mostly faithfully reuse previously-specified pronouns in the presence of no distractors, they are significantly worse at processing she/her/her, singular they and neopronouns.","Additionally, models are not robustly faithful to pronouns, as they are easily distracted.","With even one additional sentence containing a distractor pronoun, accuracy drops on average by 34%.","With 5 distractor sentences, accuracy drops by 52% for decoder-only models and 13% for encoder-only models.","We show that widely-used large language models are still brittle, with large gaps in reasoning and in processing different pronouns in a setting that is very simple for humans, and we encourage researchers in bias and reasoning to bridge them."],"url":"http://arxiv.org/abs/2404.03134v1","category":"cs.CL"}
{"created":"2024-04-03 23:44:34","title":"ALAAMEE: Open-source software for fitting autologistic actor attribute models","abstract":"The autologistic actor attribute model (ALAAM) is a model for social influence, derived from the more widely known exponential-family random graph model (ERGM). ALAAMs can be used to estimate parameters corresponding to multiple forms of social contagion associated with network structure and actor covariates. This work introduces ALAAMEE, open-source Python software for estimation, simulation, and goodness-of-fit testing for ALAAM models. ALAAMEE implements both the stochastic approximation and equilibrium expectation (EE) algorithms for ALAAM parameter estimation, including estimation from snowball sampled network data. It implements data structures and statistics for undirected, directed, and bipartite networks. We use a simulation study to assess the accuracy of the EE algorithm for ALAAM parameter estimation and statistical inference, and demonstrate the use of ALAAMEE with empirical examples using both small (fewer than 100 nodes) and large (more than 10 000 nodes) networks.","sentences":["The autologistic actor attribute model (ALAAM) is a model for social influence, derived from the more widely known exponential-family random graph model (ERGM).","ALAAMs can be used to estimate parameters corresponding to multiple forms of social contagion associated with network structure and actor covariates.","This work introduces ALAAMEE, open-source Python software for estimation, simulation, and goodness-of-fit testing for ALAAM models.","ALAAMEE implements both the stochastic approximation and equilibrium expectation (EE) algorithms for ALAAM parameter estimation, including estimation from snowball sampled network data.","It implements data structures and statistics for undirected, directed, and bipartite networks.","We use a simulation study to assess the accuracy of the EE algorithm for ALAAM parameter estimation and statistical inference, and demonstrate the use of ALAAMEE with empirical examples using both small (fewer than 100 nodes) and large (more than 10 000 nodes) networks."],"url":"http://arxiv.org/abs/2404.03116v1","category":"stat.CO"}
{"created":"2024-04-03 23:25:18","title":"On the Formation of the W-shaped O II Lines in Spectra of Type I Superluminous Supernovae","abstract":"H-poor superluminous supernovae (SLSNe-I) are characterized by O II lines around 4,000 - 4,500 A in pre-/near-maximum spectra, so-called W-shaped O II lines. As these lines are from relatively high excitation levels, they have been considered a sign of non-thermal processes, which may give a hint of power sources of SLSNe-I. However, the conditions for these lines to appear have not been understood well. In this work, we systematically calculate synthetic spectra to reproduce observed spectra of eight SLSNe-I, parameterizing departure coefficients from the nebular approximation in the SN ejecta (expressed as b_neb). We find that most of the observed spectra can be reproduced well with b_neb ~< 10, which means that no strong departure is necessary for the formation of the W-shaped O II lines. We also show that the appearance of the W-shaped O II lines is sensitive to temperature; only spectra with temperatures T ~ 14,000 - 16,000 K can produce the W-shaped O II lines without large departures. Based on this, we constrain the non-thermal ionization rate near the photosphere. Our results suggest that spectral features of SLSNe-I can give independent constraints on the power source through the non-thermal ionization rates.","sentences":["H-poor superluminous supernovae (SLSNe-I) are characterized by O II lines around 4,000 - 4,500 A in pre-/near-maximum spectra, so-called W-shaped O II lines.","As these lines are from relatively high excitation levels, they have been considered a sign of non-thermal processes, which may give a hint of power sources of SLSNe-I. However, the conditions for these lines to appear have not been understood well.","In this work, we systematically calculate synthetic spectra to reproduce observed spectra of eight SLSNe-I, parameterizing departure coefficients from the nebular approximation in the SN ejecta (expressed as b_neb).","We find that most of the observed spectra can be reproduced well with b_neb ~< 10, which means that no strong departure is necessary for the formation of the W-shaped O II lines.","We also show that the appearance of the W-shaped O II lines is sensitive to temperature; only spectra with temperatures T ~ 14,000 - 16,000 K can produce the W-shaped O II lines without large departures.","Based on this, we constrain the non-thermal ionization rate near the photosphere.","Our results suggest that spectral features of SLSNe-I can give independent constraints on the power source through the non-thermal ionization rates."],"url":"http://arxiv.org/abs/2404.03112v1","category":"astro-ph.HE"}
{"created":"2024-04-03 23:02:17","title":"Direct Experimental Constraints on the Spatial Extent of a Neutrino Wavepacket","abstract":"Despite their high relative abundance in our Universe, neutrinos are the least understood fundamental particles of nature. They also provide a unique system to study quantum coherence in fundamental systems due to their extremely weak interaction probabilities. The quantum properties of neutrinos emitted in experimentally relevant sources are virtually unknown and theoretical predictions for the spatial width of neutrino wavepackets vary by many orders of magnitude. In weak nuclear decay, the size of a neutrino wavepacket, $\\sigma_{\\nu,x}$, is related to the spatial wavefunction of its parent at production. Here, we present the first direct limits of this quantity through a new experimental concept to extract the energy width, $\\sigma_{\\textrm{N},E}$, of the recoil daughter nucleus emitted in the nuclear electron capture (EC) decay of $^7$Be. The final state in the EC decay process contains a recoiling $^7$Li nucleus and an electron neutrino ($\\nu_e$) which are entangled at their creation. The $^7$Li energy spectrum is measured to high precision by directly embedding $^7$Be radioisotopes into a high resolution superconducting tunnel junction that is operated as a cryogenic charge sensitive detector. The lower limit on the spatial coherence of the recoil daughter was found to be $\\sigma_{\\textrm{N}, x} \\geq 6.2$ pm, which implies the system remains in a spatially coherent state much larger than the nuclear scale. Further, this implies a lower limit on the size of a neutrino wavepacket, $\\sigma_{\\nu,x} \\geq 35$ nm, which is more than five orders of magnitude more stringent than the limits from all combined reactor oscillation experiments. These results have wide-reaching implications in several areas including quantum coherence, the nature of spatial localization at sub-atomic scales, interpretation of neutrino physics data, and the potential reach of future large-scale experiments.","sentences":["Despite their high relative abundance in our Universe, neutrinos are the least understood fundamental particles of nature.","They also provide a unique system to study quantum coherence in fundamental systems due to their extremely weak interaction probabilities.","The quantum properties of neutrinos emitted in experimentally relevant sources are virtually unknown and theoretical predictions for the spatial width of neutrino wavepackets vary by many orders of magnitude.","In weak nuclear decay, the size of a neutrino wavepacket, $\\sigma_{\\nu,x}$, is related to the spatial wavefunction of its parent at production.","Here, we present the first direct limits of this quantity through a new experimental concept to extract the energy width, $\\sigma_{\\textrm{N},E}$, of the recoil daughter nucleus emitted in the nuclear electron capture (EC) decay of $^7$Be.","The final state in the EC decay process contains a recoiling $^7$Li nucleus and an electron neutrino ($\\nu_e$) which are entangled at their creation.","The $^7$Li energy spectrum is measured to high precision by directly embedding $^7$Be radioisotopes into a high resolution superconducting tunnel junction that is operated as a cryogenic charge sensitive detector.","The lower limit on the spatial coherence of the recoil daughter was found to be $\\sigma_{\\textrm{N}, x} \\geq 6.2$ pm, which implies the system remains in a spatially coherent state much larger than the nuclear scale.","Further, this implies a lower limit on the size of a neutrino wavepacket, $\\sigma_{\\nu,x} \\geq 35$ nm, which is more than five orders of magnitude more stringent than the limits from all combined reactor oscillation experiments.","These results have wide-reaching implications in several areas including quantum coherence, the nature of spatial localization at sub-atomic scales, interpretation of neutrino physics data, and the potential reach of future large-scale experiments."],"url":"http://arxiv.org/abs/2404.03102v1","category":"nucl-ex"}
{"created":"2024-04-03 21:36:51","title":"Comparison of Extended and Unscented Kalman Filters Performance in a Hybrid BLE-UWB Localization System","abstract":"The paper presents a comparison of performance of two Kalman Filters: extended Kalman filter (EKF) and unscented Kalman filter (UKF) in a hybrid Bluetooth-Low-Energy-ultra-wideband (BLE-UWB) based localization system. In the system, the user is localized primarily based on Received Signal Strength (RSS) measurements of BLE signals. The UWB part of the system is periodically used to improve localization accuracy by supplying the algorithm with measured UWB packets time difference of arrival (TDOA). The proposed scheme was experimentally validated using two algorithms: the EKF and the UKF. The localization accuracy of both algorithms is compared.","sentences":["The paper presents a comparison of performance of two Kalman Filters: extended Kalman filter (EKF) and unscented Kalman filter (UKF) in a hybrid Bluetooth-Low-Energy-ultra-wideband (BLE-UWB) based localization system.","In the system, the user is localized primarily based on Received Signal Strength (RSS) measurements of BLE signals.","The UWB part of the system is periodically used to improve localization accuracy by supplying the algorithm with measured UWB packets time difference of arrival (TDOA).","The proposed scheme was experimentally validated using two algorithms: the EKF and the UKF.","The localization accuracy of both algorithms is compared."],"url":"http://arxiv.org/abs/2404.03077v1","category":"eess.SP"}
{"created":"2024-04-03 21:31:50","title":"The Independence of Magnetic Turbulent Power Spectra to the Presence of Switchbacks in the Inner Heliosphere","abstract":"An outstanding gap in our knowledge of the solar wind is the relationship between switchbacks and solar wind turbulence. Switchbacks are large fluctuations, even reversals, of the background magnetic field embedded in the solar wind flow. It has been proposed that switchbacks may form as a product of turbulence and decay via coupling with the turbulent cascade. In this work, we examine how properties of solar wind magnetic field turbulence vary in the presence or absence of switchbacks. Specifically, we use in-situ particle and fields measurements from Parker Solar Probe to measure magnetic field turbulent wave power, separately in the inertial and kinetic ranges, as a function of switchback magnetic deflection angle. We demonstrate that the angle between the background magnetic field and the solar wind velocity in the spacecraft frame ($\\theta_{vB}$) strongly determines whether Parker Solar Probe samples wave power parallel or perpendicular to the background magnetic field. Further, we show that $\\theta_{vB}$ is strongly modulated by the switchback magnetic deflection angle. In this analysis, we demonstrate that switchback deflection angle does not correspond to any significant increase in wave power in either the inertial range or at kinetic scales. This result implies that switchbacks do not strongly couple to the turbulent cascade in the inertial or kinetic ranges via turbulent wave-particle interactions. Therefore, we do not expect switchbacks to contribute significantly to solar wind heating through this type of energy conversion pathway, although contributions via other mechanisms, such as magnetic reconnection may still be significant.","sentences":["An outstanding gap in our knowledge of the solar wind is the relationship between switchbacks and solar wind turbulence.","Switchbacks are large fluctuations, even reversals, of the background magnetic field embedded in the solar wind flow.","It has been proposed that switchbacks may form as a product of turbulence and decay via coupling with the turbulent cascade.","In this work, we examine how properties of solar wind magnetic field turbulence vary in the presence or absence of switchbacks.","Specifically, we use in-situ particle and fields measurements from Parker Solar Probe to measure magnetic field turbulent wave power, separately in the inertial and kinetic ranges, as a function of switchback magnetic deflection angle.","We demonstrate that the angle between the background magnetic field and the solar wind velocity in the spacecraft frame ($\\theta_{vB}$) strongly determines whether Parker Solar Probe samples wave power parallel or perpendicular to the background magnetic field.","Further, we show that $\\theta_{vB}$ is strongly modulated by the switchback magnetic deflection angle.","In this analysis, we demonstrate that switchback deflection angle does not correspond to any significant increase in wave power in either the inertial range or at kinetic scales.","This result implies that switchbacks do not strongly couple to the turbulent cascade in the inertial or kinetic ranges via turbulent wave-particle interactions.","Therefore, we do not expect switchbacks to contribute significantly to solar wind heating through this type of energy conversion pathway, although contributions via other mechanisms, such as magnetic reconnection may still be significant."],"url":"http://arxiv.org/abs/2404.03075v1","category":"astro-ph.SR"}
{"created":"2024-04-03 21:04:56","title":"Simulating Holographic Conformal Field Theories on Hyperbolic Lattices","abstract":"We demonstrate how table-top settings combining hyperbolic lattices with nonlinear dynamics universally lead to a genuine realization of the bulk-boundary-correspondence between gravity in anti-de-Sitter (AdS) space and conformal field theory (CFT). Our concrete and broadly applicable holographic toy model simulates gravitational self-interactions in the bulk and features an emergent CFT with nontrivial correlations on the boundary. We measure the CFT data contained in the two- and three-point functions and clarify how a thermal CFT is simulated through an effective black hole geometry. As a concrete example, we propose and simulate an experimentally feasible protocol to measure the holographic CFT using electrical circuits.","sentences":["We demonstrate how table-top settings combining hyperbolic lattices with nonlinear dynamics universally lead to a genuine realization of the bulk-boundary-correspondence between gravity in anti-de-Sitter (AdS) space and conformal field theory (CFT).","Our concrete and broadly applicable holographic toy model simulates gravitational self-interactions in the bulk and features an emergent CFT with nontrivial correlations on the boundary.","We measure the CFT data contained in the two- and three-point functions and clarify how a thermal CFT is simulated through an effective black hole geometry.","As a concrete example, we propose and simulate an experimentally feasible protocol to measure the holographic CFT using electrical circuits."],"url":"http://arxiv.org/abs/2404.03062v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-03 21:02:06","title":"Oscillatory free boundary problems in stochastic materials","abstract":"We investigate a class of free boundary problems with oscillatory singularities within stochastic materials. Our main result yields sharp regularity estimates along the free boundary, provided the power of the singularity varies in a Dini-continuous fashion below a certain threshold. We also reveal an interesting repelling estimate preventing the free boundary from touching the region where the singularity power oscillates above the threshold.","sentences":["We investigate a class of free boundary problems with oscillatory singularities within stochastic materials.","Our main result yields sharp regularity estimates along the free boundary, provided the power of the singularity varies in a Dini-continuous fashion below a certain threshold.","We also reveal an interesting repelling estimate preventing the free boundary from touching the region where the singularity power oscillates above the threshold."],"url":"http://arxiv.org/abs/2404.03060v1","category":"math.AP"}
