{"created":"2024-04-10 17:59:59","title":"GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models","abstract":"In this paper, we introduce GoodDrag, a novel approach to improve the stability and image quality of drag editing. Unlike existing methods that struggle with accumulated perturbations and often result in distortions, GoodDrag introduces an AlDD framework that alternates between drag and denoising operations within the diffusion process, effectively improving the fidelity of the result. We also propose an information-preserving motion supervision operation that maintains the original features of the starting point for precise manipulation and artifact reduction. In addition, we contribute to the benchmarking of drag editing by introducing a new dataset, Drag100, and developing dedicated quality assessment metrics, Dragging Accuracy Index and Gemini Score, utilizing Large Multimodal Models. Extensive experiments demonstrate that the proposed GoodDrag compares favorably against the state-of-the-art approaches both qualitatively and quantitatively. The project page is https://gooddrag.github.io.","sentences":["In this paper, we introduce GoodDrag, a novel approach to improve the stability and image quality of drag editing.","Unlike existing methods that struggle with accumulated perturbations and often result in distortions, GoodDrag introduces an AlDD framework that alternates between drag and denoising operations within the diffusion process, effectively improving the fidelity of the result.","We also propose an information-preserving motion supervision operation that maintains the original features of the starting point for precise manipulation and artifact reduction.","In addition, we contribute to the benchmarking of drag editing by introducing a new dataset, Drag100, and developing dedicated quality assessment metrics, Dragging Accuracy Index and Gemini Score, utilizing Large Multimodal Models.","Extensive experiments demonstrate that the proposed GoodDrag compares favorably against the state-of-the-art approaches both qualitatively and quantitatively.","The project page is https://gooddrag.github.io."],"url":"http://arxiv.org/abs/2404.07206v1","category":"cs.CV"}
{"created":"2024-04-10 17:59:45","title":"BRAVE: Broadening the visual encoding of vision-language models","abstract":"Vision-language models (VLMs) are typically composed of a vision encoder, e.g. CLIP, and a language model (LM) that interprets the encoded features to solve downstream tasks. Despite remarkable progress, VLMs are subject to several shortcomings due to the limited capabilities of vision encoders, e.g. \"blindness\" to certain image features, visual hallucination, etc. To address these issues, we study broadening the visual encoding capabilities of VLMs. We first comprehensively benchmark several vision encoders with different inductive biases for solving VLM tasks. We observe that there is no single encoding configuration that consistently achieves top performance across different tasks, and encoders with different biases can perform surprisingly similarly. Motivated by this, we introduce a method, named BRAVE, that consolidates features from multiple frozen encoders into a more versatile representation that can be directly fed as the input to a frozen LM. BRAVE achieves state-of-the-art performance on a broad range of captioning and VQA benchmarks and significantly reduces the aforementioned issues of VLMs, while requiring a smaller number of trainable parameters than existing methods and having a more compressed representation. Our results highlight the potential of incorporating different visual biases for a more broad and contextualized visual understanding of VLMs.","sentences":["Vision-language models (VLMs) are typically composed of a vision encoder, e.g. CLIP, and a language model (LM) that interprets the encoded features to solve downstream tasks.","Despite remarkable progress, VLMs are subject to several shortcomings due to the limited capabilities of vision encoders, e.g. \"blindness\" to certain image features, visual hallucination, etc.","To address these issues, we study broadening the visual encoding capabilities of VLMs.","We first comprehensively benchmark several vision encoders with different inductive biases for solving VLM tasks.","We observe that there is no single encoding configuration that consistently achieves top performance across different tasks, and encoders with different biases can perform surprisingly similarly.","Motivated by this, we introduce a method, named BRAVE, that consolidates features from multiple frozen encoders into a more versatile representation that can be directly fed as the input to a frozen LM.","BRAVE achieves state-of-the-art performance on a broad range of captioning and VQA benchmarks and significantly reduces the aforementioned issues of VLMs, while requiring a smaller number of trainable parameters than existing methods and having a more compressed representation.","Our results highlight the potential of incorporating different visual biases for a more broad and contextualized visual understanding of VLMs."],"url":"http://arxiv.org/abs/2404.07204v1","category":"cs.CV"}
{"created":"2024-04-10 17:59:30","title":"Searching for Cosmological Collider in the Planck CMB Data","abstract":"In this paper, we present the first comprehensive CMB data analysis of cosmological collider physics. New heavy particles during inflation can leave imprints in the primordial correlators which are observable in today's cosmological surveys. This remarkable detection channel provides an unsurpassed opportunity to probe new physics at extremely high energies. Here we initiate the search for these relic signals in the cosmic microwave background (CMB) data from the Planck legacy release. On the theory side, guided by recent progress from the cosmological bootstrap, we first propose a family of analytic bispectrum templates that incorporate the distinctive signatures of cosmological collider physics. Our consideration includes the oscillatory signals in the squeezed limit, the angular dependence from spinning fields, and several new shapes from nontrivial sound speed effects. On the observational side, we apply the recently developed pipeline, CMB Bispectrum Estimator (CMB-BEST), to efficiently analyze the three-point statistics and search directly for these new templates in the Planck 2018 temperature and polarization data. We report stringent CMB constraints on these new templates. Furthermore, we perform parameter scans to search for the best-fit values with maximum significance. For a benchmark example of collider templates, we find $f_{NL}=-91\\pm40$ at the $68\\%$ confidence level. After accounting for the look-elsewhere effect, the biggest adjusted significance we get is $1.8\\sigma$. In general, we find no significant evidence of cosmological collider signals in the Planck data. However, this innovative analysis demonstrates the potential for discovering new heavy particles during inflation in forthcoming cosmological surveys.","sentences":["In this paper, we present the first comprehensive CMB data analysis of cosmological collider physics.","New heavy particles during inflation can leave imprints in the primordial correlators which are observable in today's cosmological surveys.","This remarkable detection channel provides an unsurpassed opportunity to probe new physics at extremely high energies.","Here we initiate the search for these relic signals in the cosmic microwave background (CMB) data from the Planck legacy release.","On the theory side, guided by recent progress from the cosmological bootstrap, we first propose a family of analytic bispectrum templates that incorporate the distinctive signatures of cosmological collider physics.","Our consideration includes the oscillatory signals in the squeezed limit, the angular dependence from spinning fields, and several new shapes from nontrivial sound speed effects.","On the observational side, we apply the recently developed pipeline, CMB Bispectrum Estimator (CMB-BEST), to efficiently analyze the three-point statistics and search directly for these new templates in the Planck 2018 temperature and polarization data.","We report stringent CMB constraints on these new templates.","Furthermore, we perform parameter scans to search for the best-fit values with maximum significance.","For a benchmark example of collider templates, we find $f_{NL}=-91\\pm40$ at the $68\\%$ confidence level.","After accounting for the look-elsewhere effect, the biggest adjusted significance we get is $1.8\\sigma$. In general, we find no significant evidence of cosmological collider signals in the Planck data.","However, this innovative analysis demonstrates the potential for discovering new heavy particles during inflation in forthcoming cosmological surveys."],"url":"http://arxiv.org/abs/2404.07203v1","category":"astro-ph.CO"}
{"created":"2024-04-10 17:59:20","title":"UMBRAE: Unified Multimodal Decoding of Brain Signals","abstract":"We address prevailing challenges of the brain-powered research, departing from the observation that the literature hardly recover accurate spatial information and require subject-specific models. To address these challenges, we propose UMBRAE, a unified multimodal decoding of brain signals. First, to extract instance-level conceptual and spatial details from neural signals, we introduce an efficient universal brain encoder for multimodal-brain alignment and recover object descriptions at multiple levels of granularity from subsequent multimodal large language model (MLLM). Second, we introduce a cross-subject training strategy mapping subject-specific features to a common feature space. This allows a model to be trained on multiple subjects without extra resources, even yielding superior results compared to subject-specific models. Further, we demonstrate this supports weakly-supervised adaptation to new subjects, with only a fraction of the total training data. Experiments demonstrate that UMBRAE not only achieves superior results in the newly introduced tasks but also outperforms methods in well established tasks. To assess our method, we construct and share with the community a comprehensive brain understanding benchmark BrainHub. Our code and benchmark are available at https://weihaox.github.io/UMBRAE.","sentences":["We address prevailing challenges of the brain-powered research, departing from the observation that the literature hardly recover accurate spatial information and require subject-specific models.","To address these challenges, we propose UMBRAE, a unified multimodal decoding of brain signals.","First, to extract instance-level conceptual and spatial details from neural signals, we introduce an efficient universal brain encoder for multimodal-brain alignment and recover object descriptions at multiple levels of granularity from subsequent multimodal large language model (MLLM).","Second, we introduce a cross-subject training strategy mapping subject-specific features to a common feature space.","This allows a model to be trained on multiple subjects without extra resources, even yielding superior results compared to subject-specific models.","Further, we demonstrate this supports weakly-supervised adaptation to new subjects, with only a fraction of the total training data.","Experiments demonstrate that UMBRAE not only achieves superior results in the newly introduced tasks but also outperforms methods in well established tasks.","To assess our method, we construct and share with the community a comprehensive brain understanding benchmark BrainHub.","Our code and benchmark are available at https://weihaox.github.io/UMBRAE."],"url":"http://arxiv.org/abs/2404.07202v1","category":"cs.CV"}
{"created":"2024-04-10 17:57:41","title":"RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion","abstract":"We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D Gaussian Splatting representation to match complex text prompts. We initialize these splats by utilizing the state-of-the-art text-to-image generators, lifting their samples into 3D, and computing the occlusion volume. We then optimize this representation across multiple views as a 3D inpainting task with image-conditional diffusion models. To learn correct geometric structure, we incorporate a depth diffusion model by conditioning on the samples from the inpainting model, giving rich geometric structure. Finally, we finetune the model using sharpened samples from image generators. Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects. Its generality additionally allows 3D synthesis from a single image.","sentences":["We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions.","Our technique optimizes a 3D Gaussian Splatting representation to match complex text prompts.","We initialize these splats by utilizing the state-of-the-art text-to-image generators, lifting their samples into 3D, and computing the occlusion volume.","We then optimize this representation across multiple views as a 3D inpainting task with image-conditional diffusion models.","To learn correct geometric structure, we incorporate a depth diffusion model by conditioning on the samples from the inpainting model, giving rich geometric structure.","Finally, we finetune the model using sharpened samples from image generators.","Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects.","Its generality additionally allows 3D synthesis from a single image."],"url":"http://arxiv.org/abs/2404.07199v1","category":"cs.CV"}
{"created":"2024-04-10 17:56:07","title":"Zero-shot Logical Query Reasoning on any Knowledge Graph","abstract":"Complex logical query answering (CLQA) in knowledge graphs (KGs) goes beyond simple KG completion and aims at answering compositional queries comprised of multiple projections and logical operations. Existing CLQA methods that learn parameters bound to certain entity or relation vocabularies can only be applied to the graph they are trained on which requires substantial training time before being deployed on a new graph. Here we present UltraQuery, an inductive reasoning model that can zero-shot answer logical queries on any KG. The core idea of UltraQuery is to derive both projections and logical operations as vocabulary-independent functions which generalize to new entities and relations in any KG. With the projection operation initialized from a pre-trained inductive KG reasoning model, UltraQuery can solve CLQA on any KG even if it is only finetuned on a single dataset. Experimenting on 23 datasets, UltraQuery in the zero-shot inference mode shows competitive or better query answering performance than best available baselines and sets a new state of the art on 14 of them.","sentences":["Complex logical query answering (CLQA) in knowledge graphs (KGs) goes beyond simple KG completion and aims at answering compositional queries comprised of multiple projections and logical operations.","Existing CLQA methods that learn parameters bound to certain entity or relation vocabularies can only be applied to the graph they are trained on which requires substantial training time before being deployed on a new graph.","Here we present UltraQuery, an inductive reasoning model that can zero-shot answer logical queries on any KG.","The core idea of UltraQuery is to derive both projections and logical operations as vocabulary-independent functions which generalize to new entities and relations in any KG.","With the projection operation initialized from a pre-trained inductive KG reasoning model, UltraQuery can solve CLQA on any KG even if it is only finetuned on a single dataset.","Experimenting on 23 datasets, UltraQuery in the zero-shot inference mode shows competitive or better query answering performance than best available baselines and sets a new state of the art on 14 of them."],"url":"http://arxiv.org/abs/2404.07198v1","category":"cs.AI"}
{"created":"2024-04-10 17:55:58","title":"An indeterminacy-based ontology for quantum theory","abstract":"I present and defend a new ontology for quantum theories (or \"interpretations\" of quantum theory) called Generative Quantum Theory (GQT). The ontology assumes that the world can be fundamentally indeterminate in the sense of being constituted by entities with indeterminate properties. The process via which determinate values arise and persist differs in different quantum theories. I will focus on the following quantum theories: GRW, the Many-Worlds Interpretation and single-world relationalist theories, Bohmian Mechanics, hybrid classical-quantum theories, and Environmental Determinacy-based (EnD) Quantum Theory. Moreover, quantum states represent quantum properties and structures that give rise to determinacy, and each quantum theory specifies a structure with specific features. I will argue that GQT should be taken seriously because it provides a series of benefits that current influential ontologies lack, namely, wavefunction realism and primitive ontology, without some of their costs. Furthermore, it allows for the formulation of quantum theories that are clearly compatible with relativistic causality, such as EnD Quantum Theory. Finally, I will show how GQT provides a new way to compare and evaluate different quantum theories.","sentences":["I present and defend a new ontology for quantum theories (or \"interpretations\" of quantum theory) called Generative Quantum Theory (GQT).","The ontology assumes that the world can be fundamentally indeterminate in the sense of being constituted by entities with indeterminate properties.","The process via which determinate values arise and persist differs in different quantum theories.","I will focus on the following quantum theories: GRW, the Many-Worlds Interpretation and single-world relationalist theories, Bohmian Mechanics, hybrid classical-quantum theories, and Environmental Determinacy-based (EnD)","Quantum Theory.","Moreover, quantum states represent quantum properties and structures that give rise to determinacy, and each quantum theory specifies a structure with specific features.","I will argue that GQT should be taken seriously because it provides a series of benefits that current influential ontologies lack, namely, wavefunction realism and primitive ontology, without some of their costs.","Furthermore, it allows for the formulation of quantum theories that are clearly compatible with relativistic causality, such as EnD Quantum Theory.","Finally, I will show how GQT provides a new way to compare and evaluate different quantum theories."],"url":"http://arxiv.org/abs/2404.07197v1","category":"quant-ph"}
{"created":"2024-04-10 17:54:36","title":"One-loop power spectrum in ultra slow-roll inflation and implications for primordial black hole dark matter","abstract":"We apply the in-in formalism to address the question of whether the size of the one-loop spectrum of curvature fluctuations in ultra-slow-roll inflation models designed for producing a large population of primordial black holes implies a breakdown of perturbation theory. We consider a simplified piece-wise description of inflation, in which the ultra-slow-roll phase is preceded and followed by slow-roll phases linked by transitional periods. We work in the $\\delta\\phi$-gauge, including all relevant cubic and quartic interactions and the necessary counterterms to renormalize the ultraviolet divergences, regularized by a cutoff. The ratio of the one-loop to the tree-level contributions to the spectrum of curvature perturbations is controlled by the duration of the ultra-slow-roll phase and of the transitions. Our results indicate that perturbation theory does not necessarily break in well-known models proposed to account for all the dark matter in the form of primordial black holes.","sentences":["We apply the in-in formalism to address the question of whether the size of the one-loop spectrum of curvature fluctuations in ultra-slow-roll inflation models designed for producing a large population of primordial black holes implies a breakdown of perturbation theory.","We consider a simplified piece-wise description of inflation, in which the ultra-slow-roll phase is preceded and followed by slow-roll phases linked by transitional periods.","We work in the $\\delta\\phi$-gauge, including all relevant cubic and quartic interactions and the necessary counterterms to renormalize the ultraviolet divergences, regularized by a cutoff.","The ratio of the one-loop to the tree-level contributions to the spectrum of curvature perturbations is controlled by the duration of the ultra-slow-roll phase and of the transitions.","Our results indicate that perturbation theory does not necessarily break in well-known models proposed to account for all the dark matter in the form of primordial black holes."],"url":"http://arxiv.org/abs/2404.07196v1","category":"astro-ph.CO"}
{"created":"2024-04-10 17:50:29","title":"VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification","abstract":"Being able to identify regions within or around proteins, to which ligands can potentially bind, is an essential step to develop new drugs. Binding site identification methods can now profit from the availability of large amounts of 3D structures in protein structure databases or from AlphaFold predictions. Current binding site identification methods heavily rely on graph neural networks (GNNs), usually designed to output E(3)-equivariant predictions. Such methods turned out to be very beneficial for physics-related tasks like binding energy or motion trajectory prediction. However, the performance of GNNs at binding site identification is still limited potentially due to the lack of dedicated nodes that model hidden geometric entities, such as binding pockets. In this work, we extend E(n)-Equivariant Graph Neural Networks (EGNNs) by adding virtual nodes and applying an extended message passing scheme. The virtual nodes in these graphs are dedicated quantities to learn representations of binding sites, which leads to improved predictive performance. In our experiments, we show that our proposed method VN-EGNN sets a new state-of-the-art at locating binding site centers on COACH420, HOLO4K and PDBbind2020.","sentences":["Being able to identify regions within or around proteins, to which ligands can potentially bind, is an essential step to develop new drugs.","Binding site identification methods can now profit from the availability of large amounts of 3D structures in protein structure databases or from AlphaFold predictions.","Current binding site identification methods heavily rely on graph neural networks (GNNs), usually designed to output E(3)-equivariant predictions.","Such methods turned out to be very beneficial for physics-related tasks like binding energy or motion trajectory prediction.","However, the performance of GNNs at binding site identification is still limited potentially due to the lack of dedicated nodes that model hidden geometric entities, such as binding pockets.","In this work, we extend E(n)-Equivariant Graph Neural Networks (EGNNs) by adding virtual nodes and applying an extended message passing scheme.","The virtual nodes in these graphs are dedicated quantities to learn representations of binding sites, which leads to improved predictive performance.","In our experiments, we show that our proposed method VN-EGNN sets a new state-of-the-art at locating binding site centers on COACH420, HOLO4K and PDBbind2020."],"url":"http://arxiv.org/abs/2404.07194v1","category":"cs.LG"}
{"created":"2024-04-10 17:49:34","title":"Optimal Reeb graphs on two- and three-connected planar polygon","abstract":"To investigate the topological structure of planar polygon decomposition on trapezoids, which is formed by height functions. We use the oriented Reeb graph of the function with a marked vertex. We describe all possible optimal Reeb graphs in the case of polygon in general position with one local minimum and one local maximum. By optimal Reeb graph we mean Reeb graph, which cann't be obtaned from other Reeb graph by subdivision of an edge or a leaf attaching. In this case polygon is a triangle with triangle holes. Constructed Reeb graphs give topological structures of trapezoid maps on two-connected polygon with six vertexes and three-connected polygon with nine vertexes.","sentences":["To investigate the topological structure of planar polygon decomposition on trapezoids, which is formed by height functions.","We use the oriented Reeb graph of the function with a marked vertex.","We describe all possible optimal Reeb graphs in the case of polygon in general position with one local minimum and one local maximum.","By optimal Reeb graph we mean Reeb graph, which cann't be obtaned from other Reeb graph by subdivision of an edge or a leaf attaching.","In this case polygon is a triangle with triangle holes.","Constructed Reeb graphs give topological structures of trapezoid maps on two-connected polygon with six vertexes and three-connected polygon with nine vertexes."],"url":"http://arxiv.org/abs/2404.07193v1","category":"math.GT"}
{"created":"2024-04-10 17:49:21","title":"Quantum Charged Black Holes","abstract":"In the framework of braneworld holography, we construct a quantum charged black hole that is localized on a three-dimensional anti-de Sitter brane and incorporates quantum backreaction effects from the boundary field theory. The field on the brane consists of higher curvature gravitation coupled with a nonlinear electromagnetic field, and it does not exhibit conformal symmetry. We also investigate the thermodynamics of these quantum charged black holes from three distinct perspectives: a pure bulk description, where the bulk gravitation interacts with a brane; a brane description, where local dynamical gravitation is subject to quantum backreaction from the dual quantum conformal field; a boundary description, where the degrees of freedom for defect quantum conformal matters are considered. In so doing we obtain doubly holographic formulations of both the first law of thermodynamics and the Smarr (energy) relations.","sentences":["In the framework of braneworld holography, we construct a quantum charged black hole that is localized on a three-dimensional anti-de Sitter brane and incorporates quantum backreaction effects from the boundary field theory.","The field on the brane consists of higher curvature gravitation coupled with a nonlinear electromagnetic field, and it does not exhibit conformal symmetry.","We also investigate the thermodynamics of these quantum charged black holes from three distinct perspectives: a pure bulk description, where the bulk gravitation interacts with a brane; a brane description, where local dynamical gravitation is subject to quantum backreaction from the dual quantum conformal field; a boundary description, where the degrees of freedom for defect quantum conformal matters are considered.","In so doing we obtain doubly holographic formulations of both the first law of thermodynamics and the Smarr (energy) relations."],"url":"http://arxiv.org/abs/2404.07192v1","category":"hep-th"}
{"created":"2024-04-10 17:48:37","title":"InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models","abstract":"We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability. By synergizing the strengths of an off-the-shelf multiview diffusion model and a sparse-view reconstruction model based on the LRM architecture, InstantMesh is able to create diverse 3D assets within 10 seconds. To enhance the training efficiency and exploit more geometric supervisions, e.g, depths and normals, we integrate a differentiable iso-surface extraction module into our framework and directly optimize on the mesh representation. Experimental results on public datasets demonstrate that InstantMesh significantly outperforms other latest image-to-3D baselines, both qualitatively and quantitatively. We release all the code, weights, and demo of InstantMesh, with the intention that it can make substantial contributions to the community of 3D generative AI and empower both researchers and content creators.","sentences":["We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability.","By synergizing the strengths of an off-the-shelf multiview diffusion model and a sparse-view reconstruction model based on the LRM architecture, InstantMesh is able to create diverse 3D assets within 10 seconds.","To enhance the training efficiency and exploit more geometric supervisions, e.g, depths and normals, we integrate a differentiable iso-surface extraction module into our framework and directly optimize on the mesh representation.","Experimental results on public datasets demonstrate that InstantMesh significantly outperforms other latest image-to-3D baselines, both qualitatively and quantitatively.","We release all the code, weights, and demo of InstantMesh, with the intention that it can make substantial contributions to the community of 3D generative AI and empower both researchers and content creators."],"url":"http://arxiv.org/abs/2404.07191v1","category":"cs.CV"}
{"created":"2024-04-10 17:40:34","title":"Wave optics lensing of gravitational waves: theory and phenomenology of triple systems in the LISA band","abstract":"We study lensing of gravitational waves by a black hole in the deep wave optics regime, i.e. when the wavelength is much larger than the black hole Schwarzschild radius. We apply it to triple systems, with a binary of stellar mass objects in the inspiraling phase orbiting around a central massive black hole. We describe the full polarisation structure of the wave and derive predictions for the polarisation modes of the scattered wave measured by the observer. We show that lensing in the wave optics regime is not helicity preserving, as opposed to lensing in the geometric optics regime. The amplitude of the total wave is modulated due to interference between the directly transmitted and lensed components. The relative amplitude of the modulation is fixed by the lensing geometry and can reach unity in the most favourable settings. This indicates that wave optics lensing is potentially detectable by LISA for sufficiently high SNR systems. Our findings show that in the wave optics regime it is necessary to go beyond the usual lensing description where the amplification factor is assumed to be the same for both helicity modes. While motivated by GW190521 and the AGN formation scenario, our results apply more broadly to stellar-mass binaries orbiting a third body described as a Schwarzschild black hole, with a period comparable to the GW observation time.","sentences":["We study lensing of gravitational waves by a black hole in the deep wave optics regime, i.e. when the wavelength is much larger than the black hole Schwarzschild radius.","We apply it to triple systems, with a binary of stellar mass objects in the inspiraling phase orbiting around a central massive black hole.","We describe the full polarisation structure of the wave and derive predictions for the polarisation modes of the scattered wave measured by the observer.","We show that lensing in the wave optics regime is not helicity preserving, as opposed to lensing in the geometric optics regime.","The amplitude of the total wave is modulated due to interference between the directly transmitted and lensed components.","The relative amplitude of the modulation is fixed by the lensing geometry and can reach unity in the most favourable settings.","This indicates that wave optics lensing is potentially detectable by LISA for sufficiently high SNR systems.","Our findings show that in the wave optics regime it is necessary to go beyond the usual lensing description where the amplification factor is assumed to be the same for both helicity modes.","While motivated by GW190521 and the AGN formation scenario, our results apply more broadly to stellar-mass binaries orbiting a third body described as a Schwarzschild black hole, with a period comparable to the GW observation time."],"url":"http://arxiv.org/abs/2404.07186v1","category":"gr-qc"}
{"created":"2024-04-10 17:40:27","title":"Reward Learning from Suboptimal Demonstrations with Applications in Surgical Electrocautery","abstract":"Automating robotic surgery via learning from demonstration (LfD) techniques is extremely challenging. This is because surgical tasks often involve sequential decision-making processes with complex interactions of physical objects and have low tolerance for mistakes. Prior works assume that all demonstrations are fully observable and optimal, which might not be practical in the real world. This paper introduces a sample-efficient method that learns a robust reward function from a limited amount of ranked suboptimal demonstrations consisting of partial-view point cloud observations. The method then learns a policy by optimizing the learned reward function using reinforcement learning (RL). We show that using a learned reward function to obtain a policy is more robust than pure imitation learning. We apply our approach on a physical surgical electrocautery task and demonstrate that our method can perform well even when the provided demonstrations are suboptimal and the observations are high-dimensional point clouds.","sentences":["Automating robotic surgery via learning from demonstration (LfD) techniques is extremely challenging.","This is because surgical tasks often involve sequential decision-making processes with complex interactions of physical objects and have low tolerance for mistakes.","Prior works assume that all demonstrations are fully observable and optimal, which might not be practical in the real world.","This paper introduces a sample-efficient method that learns a robust reward function from a limited amount of ranked suboptimal demonstrations consisting of partial-view point cloud observations.","The method then learns a policy by optimizing the learned reward function using reinforcement learning (RL).","We show that using a learned reward function to obtain a policy is more robust than pure imitation learning.","We apply our approach on a physical surgical electrocautery task and demonstrate that our method can perform well even when the provided demonstrations are suboptimal and the observations are high-dimensional point clouds."],"url":"http://arxiv.org/abs/2404.07185v1","category":"cs.RO"}
{"created":"2024-04-10 17:33:11","title":"Studying the Supernova Absolute Magnitude Constancy with Baryonic Acoustic Oscillations","abstract":"In this proceeding we review and expand on our recent work investigating the constancy of the absolute magnitude $M_B$ of Type Ia supernovae. In it, we used baryonic acoustic oscillations (BAO) to calibrate the supernova data and to check whether the resulting $M_B$ is constant. We used non-parametric methods like Gaussian processes and artificial neural networks to reconstruct $M_B(z)$. Here we elaborate on the results by putting them in the context of other studies investigating possible non-constant $M_B$ and the impact of the distance-duality relation. We also present some numerical details on the calculations in the original paper and new non-parametric reconstructions, including a conservative model-independent fit, confirming its main results. Notably, we see that $M_B$ remains constant within $1\\sigma$, with a possible jump around $z = 0.01 - 0.15$. Furthermore, the observed distribution of $M_B(z)$ cannot be described by a single Gaussian, displaying multiple peaks and tails. The choice of the only remaining parameter -- the sound horizon $r_d$ leads to a tension in the $M_B-r_d$ plane. Fitting different non-constant $M_B(z)$ models does not significantly improve the fit and there is no preference for any of the models by the statistical measures we employ.","sentences":["In this proceeding we review and expand on our recent work investigating the constancy of the absolute magnitude $M_B$ of Type Ia supernovae.","In it, we used baryonic acoustic oscillations (BAO) to calibrate the supernova data and to check whether the resulting $M_B$ is constant.","We used non-parametric methods like Gaussian processes and artificial neural networks to reconstruct $M_B(z)$. Here we elaborate on the results by putting them in the context of other studies investigating possible non-constant $M_B$ and the impact of the distance-duality relation.","We also present some numerical details on the calculations in the original paper and new non-parametric reconstructions, including a conservative model-independent fit, confirming its main results.","Notably, we see that $M_B$ remains constant within $1\\sigma$, with a possible jump around $z = 0.01 - 0.15$.","Furthermore, the observed distribution of $M_B(z)$ cannot be described by a single Gaussian, displaying multiple peaks and tails.","The choice of the only remaining parameter -- the sound horizon $r_d$ leads to a tension in the $M_B-r_d$ plane.","Fitting different non-constant $M_B(z)$ models does not significantly improve the fit and there is no preference for any of the models by the statistical measures we employ."],"url":"http://arxiv.org/abs/2404.07182v1","category":"astro-ph.CO"}
{"created":"2024-04-10 17:28:16","title":"Move Anything with Layered Scene Diffusion","abstract":"Diffusion models generate images with an unprecedented level of quality, but how can we freely rearrange image layouts? Recent works generate controllable scenes via learning spatially disentangled latent codes, but these methods do not apply to diffusion models due to their fixed forward process. In this work, we propose SceneDiffusion to optimize a layered scene representation during the diffusion sampling process. Our key insight is that spatial disentanglement can be obtained by jointly denoising scene renderings at different spatial layouts. Our generated scenes support a wide range of spatial editing operations, including moving, resizing, cloning, and layer-wise appearance editing operations, including object restyling and replacing. Moreover, a scene can be generated conditioned on a reference image, thus enabling object moving for in-the-wild images. Notably, this approach is training-free, compatible with general text-to-image diffusion models, and responsive in less than a second.","sentences":["Diffusion models generate images with an unprecedented level of quality, but how can we freely rearrange image layouts?","Recent works generate controllable scenes via learning spatially disentangled latent codes, but these methods do not apply to diffusion models due to their fixed forward process.","In this work, we propose SceneDiffusion to optimize a layered scene representation during the diffusion sampling process.","Our key insight is that spatial disentanglement can be obtained by jointly denoising scene renderings at different spatial layouts.","Our generated scenes support a wide range of spatial editing operations, including moving, resizing, cloning, and layer-wise appearance editing operations, including object restyling and replacing.","Moreover, a scene can be generated conditioned on a reference image, thus enabling object moving for in-the-wild images.","Notably, this approach is training-free, compatible with general text-to-image diffusion models, and responsive in less than a second."],"url":"http://arxiv.org/abs/2404.07178v1","category":"cs.CV"}
{"created":"2024-04-10 17:12:45","title":"Microstates of accelerating and supersymmetric AdS$_4$ black holes from the spindle index","abstract":"We provide a first principles derivation of the microscopic entropy of a very general class of supersymmetric, rotating and accelerating black holes in AdS$_4$. This is achieved by analysing the large-$N$ limit of the spindle index and completes the construction of the first example of a holographic duality involving supersymmetric field theories defined on orbifolds with conical singularities.","sentences":["We provide a first principles derivation of the microscopic entropy of a very general class of supersymmetric, rotating and accelerating black holes in AdS$_4$. This is achieved by analysing the large-$N$ limit of the spindle index and completes the construction of the first example of a holographic duality involving supersymmetric field theories defined on orbifolds with conical singularities."],"url":"http://arxiv.org/abs/2404.07173v1","category":"hep-th"}
{"created":"2024-04-10 17:08:46","title":"A Gauss-Newton Approach for Min-Max Optimization in Generative Adversarial Networks","abstract":"A novel first-order method is proposed for training generative adversarial networks (GANs). It modifies the Gauss-Newton method to approximate the min-max Hessian and uses the Sherman-Morrison inversion formula to calculate the inverse. The method corresponds to a fixed-point method that ensures necessary contraction. To evaluate its effectiveness, numerical experiments are conducted on various datasets commonly used in image generation tasks, such as MNIST, Fashion MNIST, CIFAR10, FFHQ, and LSUN. Our method is capable of generating high-fidelity images with greater diversity across multiple datasets. It also achieves the highest inception score for CIFAR10 among all compared methods, including state-of-the-art second-order methods. Additionally, its execution time is comparable to that of first-order min-max methods.","sentences":["A novel first-order method is proposed for training generative adversarial networks (GANs).","It modifies the Gauss-Newton method to approximate the min-max Hessian and uses the Sherman-Morrison inversion formula to calculate the inverse.","The method corresponds to a fixed-point method that ensures necessary contraction.","To evaluate its effectiveness, numerical experiments are conducted on various datasets commonly used in image generation tasks, such as MNIST, Fashion MNIST, CIFAR10, FFHQ, and LSUN.","Our method is capable of generating high-fidelity images with greater diversity across multiple datasets.","It also achieves the highest inception score for CIFAR10 among all compared methods, including state-of-the-art second-order methods.","Additionally, its execution time is comparable to that of first-order min-max methods."],"url":"http://arxiv.org/abs/2404.07172v1","category":"cs.LG"}
{"created":"2024-04-10 17:05:12","title":"Worst-Case Convergence Time of ML Algorithms via Extreme Value Theory","abstract":"This paper leverages the statistics of extreme values to predict the worst-case convergence times of machine learning algorithms. Timing is a critical non-functional property of ML systems, and providing the worst-case converge times is essential to guarantee the availability of ML and its services. However, timing properties such as worst-case convergence times (WCCT) are difficult to verify since (1) they are not encoded in the syntax or semantics of underlying programming languages of AI, (2) their evaluations depend on both algorithmic implementations and underlying systems, and (3) their measurements involve uncertainty and noise. Therefore, prevalent formal methods and statistical models fail to provide rich information on the amounts and likelihood of WCCT.   Our key observation is that the timing information we seek represents the extreme tail of execution times. Therefore, extreme value theory (EVT), a statistical discipline that focuses on understanding and predicting the distribution of extreme values in the tail of outcomes, provides an ideal framework to model and analyze WCCT in the training and inference phases of ML paradigm. Building upon the mathematical tools from EVT, we propose a practical framework to predict the worst-case timing properties of ML. Over a set of linear ML training algorithms, we show that EVT achieves a better accuracy for predicting WCCTs than relevant statistical methods such as the Bayesian factor. On the set of larger machine learning training algorithms and deep neural network inference, we show the feasibility and usefulness of EVT models to accurately predict WCCTs, their expected return periods, and their likelihood.","sentences":["This paper leverages the statistics of extreme values to predict the worst-case convergence times of machine learning algorithms.","Timing is a critical non-functional property of ML systems, and providing the worst-case converge times is essential to guarantee the availability of ML and its services.","However, timing properties such as worst-case convergence times (WCCT) are difficult to verify since (1) they are not encoded in the syntax or semantics of underlying programming languages of AI, (2) their evaluations depend on both algorithmic implementations and underlying systems, and (3) their measurements involve uncertainty and noise.","Therefore, prevalent formal methods and statistical models fail to provide rich information on the amounts and likelihood of WCCT.   ","Our key observation is that the timing information we seek represents the extreme tail of execution times.","Therefore, extreme value theory (EVT), a statistical discipline that focuses on understanding and predicting the distribution of extreme values in the tail of outcomes, provides an ideal framework to model and analyze WCCT in the training and inference phases of ML paradigm.","Building upon the mathematical tools from EVT, we propose a practical framework to predict the worst-case timing properties of ML.","Over a set of linear ML training algorithms, we show that EVT achieves a better accuracy for predicting WCCTs than relevant statistical methods such as the Bayesian factor.","On the set of larger machine learning training algorithms and deep neural network inference, we show the feasibility and usefulness of EVT models to accurately predict WCCTs, their expected return periods, and their likelihood."],"url":"http://arxiv.org/abs/2404.07170v1","category":"cs.SE"}
{"created":"2024-04-10 17:04:06","title":"Using Neural Networks to Model Hysteretic Kinematics in Tendon-Actuated Continuum Robots","abstract":"The ability to accurately model mechanical hysteretic behavior in tendon-actuated continuum robots using deep learning approaches is a growing area of interest. In this paper, we investigate the hysteretic response of two types of tendon-actuated continuum robots and, ultimately, compare three types of neural network modeling approaches with both forward and inverse kinematic mappings: feedforward neural network (FNN), FNN with a history input buffer, and long short-term memory (LSTM) network. We seek to determine which model best captures temporal dependent behavior. We find that, depending on the robot's design, choosing different kinematic inputs can alter whether hysteresis is exhibited by the system. Furthermore, we present the results of the model fittings, revealing that, in contrast to the standard FNN, both FNN with a history input buffer and the LSTM model exhibit the capacity to model historical dependence with comparable performance in capturing rate-dependent hysteresis.","sentences":["The ability to accurately model mechanical hysteretic behavior in tendon-actuated continuum robots using deep learning approaches is a growing area of interest.","In this paper, we investigate the hysteretic response of two types of tendon-actuated continuum robots and, ultimately, compare three types of neural network modeling approaches with both forward and inverse kinematic mappings: feedforward neural network (FNN), FNN with a history input buffer, and long short-term memory (LSTM) network.","We seek to determine which model best captures temporal dependent behavior.","We find that, depending on the robot's design, choosing different kinematic inputs can alter whether hysteresis is exhibited by the system.","Furthermore, we present the results of the model fittings, revealing that, in contrast to the standard FNN, both FNN with a history input buffer and the LSTM model exhibit the capacity to model historical dependence with comparable performance in capturing rate-dependent hysteresis."],"url":"http://arxiv.org/abs/2404.07168v1","category":"cs.RO"}
{"created":"2024-04-10 17:00:04","title":"Analysis of Distributed Optimization Algorithms on a Real Processing-In-Memory System","abstract":"Machine Learning (ML) training on large-scale datasets is a very expensive and time-consuming workload. Processor-centric architectures (e.g., CPU, GPU) commonly used for modern ML training workloads are limited by the data movement bottleneck, i.e., due to repeatedly accessing the training dataset. As a result, processor-centric systems suffer from performance degradation and high energy consumption. Processing-In-Memory (PIM) is a promising solution to alleviate the data movement bottleneck by placing the computation mechanisms inside or near memory.   Our goal is to understand the capabilities and characteristics of popular distributed optimization algorithms on real-world PIM architectures to accelerate data-intensive ML training workloads. To this end, we 1) implement several representative centralized distributed optimization algorithms on UPMEM's real-world general-purpose PIM system, 2) rigorously evaluate these algorithms for ML training on large-scale datasets in terms of performance, accuracy, and scalability, 3) compare to conventional CPU and GPU baselines, and 4) discuss implications for future PIM hardware and the need to shift to an algorithm-hardware codesign perspective to accommodate decentralized distributed optimization algorithms.   Our results demonstrate three major findings: 1) Modern general-purpose PIM architectures can be a viable alternative to state-of-the-art CPUs and GPUs for many memory-bound ML training workloads, when operations and datatypes are natively supported by PIM hardware, 2) the importance of carefully choosing the optimization algorithm that best fit PIM, and 3) contrary to popular belief, contemporary PIM architectures do not scale approximately linearly with the number of nodes for many data-intensive ML training workloads. To facilitate future research, we aim to open-source our complete codebase.","sentences":["Machine Learning (ML) training on large-scale datasets is a very expensive and time-consuming workload.","Processor-centric architectures (e.g., CPU, GPU) commonly used for modern ML training workloads are limited by the data movement bottleneck, i.e., due to repeatedly accessing the training dataset.","As a result, processor-centric systems suffer from performance degradation and high energy consumption.","Processing-In-Memory (PIM) is a promising solution to alleviate the data movement bottleneck by placing the computation mechanisms inside or near memory.   ","Our goal is to understand the capabilities and characteristics of popular distributed optimization algorithms on real-world PIM architectures to accelerate data-intensive ML training workloads.","To this end, we 1) implement several representative centralized distributed optimization algorithms on UPMEM's real-world general-purpose PIM system, 2) rigorously evaluate these algorithms for ML training on large-scale datasets in terms of performance, accuracy, and scalability, 3) compare to conventional CPU and GPU baselines, and 4) discuss implications for future PIM hardware and the need to shift to an algorithm-hardware codesign perspective to accommodate decentralized distributed optimization algorithms.   ","Our results demonstrate three major findings: 1) Modern general-purpose PIM architectures can be a viable alternative to state-of-the-art CPUs and GPUs for many memory-bound ML training workloads, when operations and datatypes are natively supported by PIM hardware, 2) the importance of carefully choosing the optimization algorithm that best fit PIM, and 3) contrary to popular belief, contemporary PIM architectures do not scale approximately linearly with the number of nodes for many data-intensive ML training workloads.","To facilitate future research, we aim to open-source our complete codebase."],"url":"http://arxiv.org/abs/2404.07164v1","category":"cs.AR"}
{"created":"2024-04-10 16:57:47","title":"Like a candle in the wind: The embers of once aflame, now smouldering galaxies at $5 < z < 8$","abstract":"We develop a photometric search method for identifying smouldering galaxies at $5< z < 8$, which are defined to have weak emission lines and thus generally have low specific star formation rates and may even be in a state of (temporary) quiescence. The deep NIRCam imaging (${\\sim}29.5$ AB mag, 5$\\sigma$) from the JADES second data release is essential for finding these systems, as they are faint, relatively quiescent dwarf galaxies ($M_* \\sim 10^{8}$-$10^9$ $\\mathrm{M}_\\odot)$ in the Epoch of Reionisation (EoR). Moreover, medium-band imaging is key, enabling a clear identification of the lack of emission lines in these galaxies, thus betraying their dormant flame. Owing to the young age of the Universe, combined with the likely bursty star formation in these first dwarf galaxies, conventional colour-selection methods like the UVJ diagram likely miss a large fraction of the quiescent population in the EoR. Indeed, we find that smouldering galaxies constitute a considerable fraction (0.10-0.35) of the EoR dwarf galaxy population ($M_* \\sim 10^{8}$-$10^{9}$ $\\mathrm{M}_\\odot$). As predicted by simulations, these first dwarf galaxies are fragile, the star formation in their shallow potential wells easily snuffed out by feedback-driven winds triggered by secular or merger-driven starbursts, with the smouldering fraction increasing with decreasing stellar mass. Finally, we provide observational constraints on the smouldering galaxy comoving number density (${\\sim}10^{-4}$-$10^{-5}$ dex$^{-1}$ Mpc$^{-3}$), which, although hampered by incompleteness, should aid in our understanding of the primordial baryon cycle, as current simulations greatly disagree on whether these systems are rare (${\\sim}1\\%$) or common (${\\sim}50\\%$) in the EoR.","sentences":["We develop a photometric search method for identifying smouldering galaxies at $5< z < 8$, which are defined to have weak emission lines and thus generally have low specific star formation rates and may even be in a state of (temporary) quiescence.","The deep NIRCam imaging (${\\sim}29.5$ AB mag, 5$\\sigma$) from the JADES second data release is essential for finding these systems, as they are faint, relatively quiescent dwarf galaxies ($M_*","\\sim 10^{8}$-$10^9$ $\\mathrm{M}_\\odot)$ in the Epoch of Reionisation (EoR).","Moreover, medium-band imaging is key, enabling a clear identification of the lack of emission lines in these galaxies, thus betraying their dormant flame.","Owing to the young age of the Universe, combined with the likely bursty star formation in these first dwarf galaxies, conventional colour-selection methods like the UVJ diagram likely miss a large fraction of the quiescent population in the EoR. Indeed, we find that smouldering galaxies constitute a considerable fraction (0.10-0.35) of the EoR dwarf galaxy population ($M_*","\\sim 10^{8}$-$10^{9}$ $\\mathrm{M}_\\odot$).","As predicted by simulations, these first dwarf galaxies are fragile, the star formation in their shallow potential wells easily snuffed out by feedback-driven winds triggered by secular or merger-driven starbursts, with the smouldering fraction increasing with decreasing stellar mass.","Finally, we provide observational constraints on the smouldering galaxy comoving number density (${\\sim}10^{-4}$-$10^{-5}$ dex$^{-1}$ Mpc$^{-3}$), which, although hampered by incompleteness, should aid in our understanding of the primordial baryon cycle, as current simulations greatly disagree on whether these systems are rare (${\\sim}1\\%$) or common (${\\sim}50\\%$) in the EoR."],"url":"http://arxiv.org/abs/2404.07163v1","category":"astro-ph.GA"}
{"created":"2024-04-10 16:49:39","title":"CBFKIT: A Control Barrier Function Toolbox for Robotics Applications","abstract":"This paper introduces CBFKit, a Python/ROS toolbox for safe robotics planning and control under uncertainty. The toolbox provides a general framework for designing control barrier functions for mobility systems within both deterministic and stochastic environments. It can be connected to the ROS open-source robotics middleware, allowing for the setup of multi-robot applications, encoding of environments and maps, and integrations with predictive motion planning algorithms. Additionally, it offers multiple CBF variations and algorithms for robot control. The CBFKit is demonstrated on the Toyota Human Support Robot (HSR) in both simulation and in physical experiments.","sentences":["This paper introduces CBFKit, a Python/ROS toolbox for safe robotics planning and control under uncertainty.","The toolbox provides a general framework for designing control barrier functions for mobility systems within both deterministic and stochastic environments.","It can be connected to the ROS open-source robotics middleware, allowing for the setup of multi-robot applications, encoding of environments and maps, and integrations with predictive motion planning algorithms.","Additionally, it offers multiple CBF variations and algorithms for robot control.","The CBFKit is demonstrated on the Toyota Human Support Robot (HSR) in both simulation and in physical experiments."],"url":"http://arxiv.org/abs/2404.07158v1","category":"cs.RO"}
{"created":"2024-04-10 16:44:43","title":"Understanding Dynamics in Coarse-Grained Models: IV. Connection of Fine-Grained and Coarse-Grained Dynamics with the Stokes-Einstein and Stokes-Einstein-Debye Relations","abstract":"Applying an excess entropy scaling formalism to the coarse-grained (CG) dynamics of liquids, we discovered that missing rotational motions during the CG process are responsible for artificially accelerated CG dynamics. In the context of the dynamic representability between the fine-grained (FG) and CG dynamics, this work introduces the well-known Stokes-Einstein and Stokes-Einstein-Debye relations to unravel the rotational dynamics underlying FG trajectories, thereby allowing for an indirect evaluation of the effective rotations based only on the translational information at the reduced CG resolution. Since the representability issue in CG modeling limits a direct evaluation of the shear stress appearing in the Stokes-Einstein and Stokes-Einstein-Debye relations, we introduce a translational relaxation time as a proxy to employ these relations, and we demonstrate that these relations hold for the ambient conditions studied in our series of work. Additional theoretical links to our previous work are also established. First, we demonstrate that the effective hard sphere radius determined by the classical perturbation theory can approximate the complex hydrodynamic radius value reasonably well. Also, we present a simple derivation of an excess entropy scaling relationship for viscosity by estimating the elliptical integral of molecules. In turn, since the translational and rotational motions at the FG level are correlated to each other, we conclude that the \"entropy-free\" CG diffusion only depends on the shape of the reference molecule. Our results and analyses impart an alternative way of recovering the FG diffusion from the CG description by coupling the translational and rotational motions at the hydrodynamic level.","sentences":["Applying an excess entropy scaling formalism to the coarse-grained (CG) dynamics of liquids, we discovered that missing rotational motions during the CG process are responsible for artificially accelerated CG dynamics.","In the context of the dynamic representability between the fine-grained (FG) and CG dynamics, this work introduces the well-known Stokes-Einstein and Stokes-Einstein-Debye relations to unravel the rotational dynamics underlying FG trajectories, thereby allowing for an indirect evaluation of the effective rotations based only on the translational information at the reduced CG resolution.","Since the representability issue in CG modeling limits a direct evaluation of the shear stress appearing in the Stokes-Einstein and Stokes-Einstein-Debye relations, we introduce a translational relaxation time as a proxy to employ these relations, and we demonstrate that these relations hold for the ambient conditions studied in our series of work.","Additional theoretical links to our previous work are also established.","First, we demonstrate that the effective hard sphere radius determined by the classical perturbation theory can approximate the complex hydrodynamic radius value reasonably well.","Also, we present a simple derivation of an excess entropy scaling relationship for viscosity by estimating the elliptical integral of molecules.","In turn, since the translational and rotational motions at the FG level are correlated to each other, we conclude that the \"entropy-free\" CG diffusion only depends on the shape of the reference molecule.","Our results and analyses impart an alternative way of recovering the FG diffusion from the CG description by coupling the translational and rotational motions at the hydrodynamic level."],"url":"http://arxiv.org/abs/2404.07156v1","category":"physics.chem-ph"}
{"created":"2024-04-10 16:44:11","title":"Unified Language-driven Zero-shot Domain Adaptation","abstract":"This paper introduces Unified Language-driven Zero-shot Domain Adaptation (ULDA), a novel task setting that enables a single model to adapt to diverse target domains without explicit domain-ID knowledge. We identify the constraints in the existing language-driven zero-shot domain adaptation task, particularly the requirement for domain IDs and domain-specific models, which may restrict flexibility and scalability. To overcome these issues, we propose a new framework for ULDA, consisting of Hierarchical Context Alignment (HCA), Domain Consistent Representation Learning (DCRL), and Text-Driven Rectifier (TDR). These components work synergistically to align simulated features with target text across multiple visual levels, retain semantic correlations between different regional representations, and rectify biases between simulated and real target visual features, respectively. Our extensive empirical evaluations demonstrate that this framework achieves competitive performance in both settings, surpassing even the model that requires domain-ID, showcasing its superiority and generalization ability. The proposed method is not only effective but also maintains practicality and efficiency, as it does not introduce additional computational costs during inference. Our project page is https://senqiaoyang.com/project/ULDA .","sentences":["This paper introduces Unified Language-driven Zero-shot Domain Adaptation (ULDA), a novel task setting that enables a single model to adapt to diverse target domains without explicit domain-ID knowledge.","We identify the constraints in the existing language-driven zero-shot domain adaptation task, particularly the requirement for domain IDs and domain-specific models, which may restrict flexibility and scalability.","To overcome these issues, we propose a new framework for ULDA, consisting of Hierarchical Context Alignment (HCA), Domain Consistent Representation Learning (DCRL), and Text-Driven Rectifier (TDR).","These components work synergistically to align simulated features with target text across multiple visual levels, retain semantic correlations between different regional representations, and rectify biases between simulated and real target visual features, respectively.","Our extensive empirical evaluations demonstrate that this framework achieves competitive performance in both settings, surpassing even the model that requires domain-ID, showcasing its superiority and generalization ability.","The proposed method is not only effective but also maintains practicality and efficiency, as it does not introduce additional computational costs during inference.","Our project page is https://senqiaoyang.com/project/ULDA ."],"url":"http://arxiv.org/abs/2404.07155v1","category":"cs.CV"}
{"created":"2024-04-10 16:33:55","title":"Adaptive behavior with stable synapses","abstract":"Behavioral changes in animals and humans, as a consequence of an error or a verbal instruction, can be extremely rapid. Improvement in behavioral performances are usually associated in machine learning and reinforcement learning to synaptic plasticity, and, in general, to changes and optimization of network parameters. However, such rapid changes are not coherent with the timescales of synaptic plasticity, suggesting that the mechanism responsible for that could be a dynamical network reconfiguration. In the last few years, similar capabilities have been observed in transformers, foundational architecture in the field of machine learning that are widely used in applications such as natural language and image processing. Transformers are capable of in-context learning, the ability to adapt and acquire new information dynamically within the context of the task or environment they are currently engaged in, without the need for significant changes to their underlying parameters. Building upon the notion of something unique within transformers enabling the emergence of this property, we claim that it could also be supported by input segregation and dendritic amplification, features extensively observed in biological networks. We propose an architecture composed of gain-modulated recurrent networks that excels at in-context learning, showing abilities inaccessible to standard networks. We argue that such a framework can describe the psychometry of context-dependent tasks on humans and other species, solving the incoherence of plasticity timescales. When the context is changed, the network is dynamically reconfigured, and the predicted output undergoes dynamic updates until it aligns with the information embedded in the context.","sentences":["Behavioral changes in animals and humans, as a consequence of an error or a verbal instruction, can be extremely rapid.","Improvement in behavioral performances are usually associated in machine learning and reinforcement learning to synaptic plasticity, and, in general, to changes and optimization of network parameters.","However, such rapid changes are not coherent with the timescales of synaptic plasticity, suggesting that the mechanism responsible for that could be a dynamical network reconfiguration.","In the last few years, similar capabilities have been observed in transformers, foundational architecture in the field of machine learning that are widely used in applications such as natural language and image processing.","Transformers are capable of in-context learning, the ability to adapt and acquire new information dynamically within the context of the task or environment they are currently engaged in, without the need for significant changes to their underlying parameters.","Building upon the notion of something unique within transformers enabling the emergence of this property, we claim that it could also be supported by input segregation and dendritic amplification, features extensively observed in biological networks.","We propose an architecture composed of gain-modulated recurrent networks that excels at in-context learning, showing abilities inaccessible to standard networks.","We argue that such a framework can describe the psychometry of context-dependent tasks on humans and other species, solving the incoherence of plasticity timescales.","When the context is changed, the network is dynamically reconfigured, and the predicted output undergoes dynamic updates until it aligns with the information embedded in the context."],"url":"http://arxiv.org/abs/2404.07150v1","category":"q-bio.NC"}
{"created":"2024-04-10 16:29:21","title":"How Consistent are Clinicians? Evaluating the Predictability of Sepsis Disease Progression with Dynamics Models","abstract":"Reinforcement learning (RL) is a promising approach to generate treatment policies for sepsis patients in intensive care. While retrospective evaluation metrics show decreased mortality when these policies are followed, studies with clinicians suggest their recommendations are often spurious. We propose that these shortcomings may be due to lack of diversity in observed actions and outcomes in the training data, and we construct experiments to investigate the feasibility of predicting sepsis disease severity changes due to clinician actions. Preliminary results suggest incorporating action information does not significantly improve model performance, indicating that clinician actions may not be sufficiently variable to yield measurable effects on disease progression. We discuss the implications of these findings for optimizing sepsis treatment.","sentences":["Reinforcement learning (RL) is a promising approach to generate treatment policies for sepsis patients in intensive care.","While retrospective evaluation metrics show decreased mortality when these policies are followed, studies with clinicians suggest their recommendations are often spurious.","We propose that these shortcomings may be due to lack of diversity in observed actions and outcomes in the training data, and we construct experiments to investigate the feasibility of predicting sepsis disease severity changes due to clinician actions.","Preliminary results suggest incorporating action information does not significantly improve model performance, indicating that clinician actions may not be sufficiently variable to yield measurable effects on disease progression.","We discuss the implications of these findings for optimizing sepsis treatment."],"url":"http://arxiv.org/abs/2404.07148v1","category":"cs.LG"}
{"created":"2024-04-10 16:24:51","title":"On noise in swap ASAP repeater chains: exact analytics, distributions and tight approximations","abstract":"Losses are one of the main bottlenecks for the distribution of entanglement in quantum networks, which can be overcome by the implementation of quantum repeaters. The most basic form of a quantum repeater chain is the swap ASAP repeater chain. In such a repeater chain, elementary links are probabilistically generated and deterministically swapped as soon as two adjacent links have been generated. As each entangled state is waiting to be swapped, decoherence is experienced, turning the fidelity of the entangled state between the end nodes of the chain into a random variable. Fully characterizing the (average) fidelity as the repeater chain grows is still an open problem. Here, we analytically investigate the case of equally-spaced repeaters, where we find exact analytic formulae for all moments of the fidelity up to 25 segments. We obtain these formulae by providing a general solution in terms of a generating function; a function whose n'th term in its Maclaurin series yields the moments of the fidelity for n segments. We generalize this approaches as well to a global cut-off policy -- a method for increasing fidelity at the cost of longer entanglement delivery times -- allowing for fast optimization of the cut-off parameter by eliminating the need for Monte Carlo simulation. We furthermore find simple approximations of the average fidelity that are exponentially tight, and, for up to 10 segments, the full distribution of the delivered fidelity. We use this to analytically calculate the secret-key rate when the distributed entanglement is used for quantum-key distribution, both with and without binning methods. In follow-up work we exploit a connection to a model in statistical physics to numerically calculate quantities of interest for the inhomogeneous multipartite case.","sentences":["Losses are one of the main bottlenecks for the distribution of entanglement in quantum networks, which can be overcome by the implementation of quantum repeaters.","The most basic form of a quantum repeater chain is the swap ASAP repeater chain.","In such a repeater chain, elementary links are probabilistically generated and deterministically swapped as soon as two adjacent links have been generated.","As each entangled state is waiting to be swapped, decoherence is experienced, turning the fidelity of the entangled state between the end nodes of the chain into a random variable.","Fully characterizing the (average) fidelity as the repeater chain grows is still an open problem.","Here, we analytically investigate the case of equally-spaced repeaters, where we find exact analytic formulae for all moments of the fidelity up to 25 segments.","We obtain these formulae by providing a general solution in terms of a generating function; a function whose n'th term in its Maclaurin series yields the moments of the fidelity for n segments.","We generalize this approaches as well to a global cut-off policy -- a method for increasing fidelity at the cost of longer entanglement delivery times -- allowing for fast optimization of the cut-off parameter by eliminating the need for Monte Carlo simulation.","We furthermore find simple approximations of the average fidelity that are exponentially tight, and, for up to 10 segments, the full distribution of the delivered fidelity.","We use this to analytically calculate the secret-key rate when the distributed entanglement is used for quantum-key distribution, both with and without binning methods.","In follow-up work we exploit a connection to a model in statistical physics to numerically calculate quantities of interest for the inhomogeneous multipartite case."],"url":"http://arxiv.org/abs/2404.07146v1","category":"quant-ph"}
{"created":"2024-04-10 16:18:42","title":"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention","abstract":"This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.","sentences":["This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation.","A key component in our proposed approach is a new attention technique dubbed Infini-attention.","The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block.","We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs.","Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs."],"url":"http://arxiv.org/abs/2404.07143v1","category":"cs.CL"}
{"created":"2024-04-10 16:18:11","title":"Bridging Gaps, Building Futures: Advancing Software Developer Diversity and Inclusion Through Future-Oriented Research","abstract":"Software systems are responsible for nearly all aspects of modern life and society. However, the demographics of software development teams that are tasked with designing and maintaining these software systems rarely match the demographics of users. As the landscape of software engineering (SE) evolves due to technological innovations, such as the rise of automated programming assistants powered by artificial intelligence (AI) and machine learning, more effort is needed to promote software developer diversity and inclusion (SDDI) to ensure inclusive work environments for development teams and usable software for diverse populations. To this end, we present insights from SE researchers and practitioners on challenges and solutions regarding diversity and inclusion in SE. Based on these findings, we share potential utopian and dystopian visions of the future and provide future research directions and implications for academia and industry to promote SDDI in the age of AI-driven SE.","sentences":["Software systems are responsible for nearly all aspects of modern life and society.","However, the demographics of software development teams that are tasked with designing and maintaining these software systems rarely match the demographics of users.","As the landscape of software engineering (SE) evolves due to technological innovations, such as the rise of automated programming assistants powered by artificial intelligence (AI) and machine learning, more effort is needed to promote software developer diversity and inclusion (SDDI) to ensure inclusive work environments for development teams and usable software for diverse populations.","To this end, we present insights from SE researchers and practitioners on challenges and solutions regarding diversity and inclusion in SE.","Based on these findings, we share potential utopian and dystopian visions of the future and provide future research directions and implications for academia and industry to promote SDDI in the age of AI-driven SE."],"url":"http://arxiv.org/abs/2404.07142v1","category":"cs.SE"}
{"created":"2024-04-10 16:18:07","title":"High-dimensional copula-based Wasserstein dependence","abstract":"We generalize 2-Wasserstein dependence coefficients to measure dependence between a finite number of random vectors. This generalization includes theoretical properties, and in particular focuses on an interpretation of maximal dependence and an asymptotic normality result for a proposed semi-parametric estimator under a Gaussian copula assumption. In addition, we discuss general axioms for dependence measures between multiple random vectors, other plausible normalizations, and various examples. Afterwards, we look into plug-in estimators based on penalized empirical covariance matrices in order to deal with high dimensionality issues and take possible marginal independencies into account by inducing (block) sparsity. The latter ideas are investigated via a simulation study, considering other dependence coefficients as well. We illustrate the use of the developed methods in two real data applications.","sentences":["We generalize 2-Wasserstein dependence coefficients to measure dependence between a finite number of random vectors.","This generalization includes theoretical properties, and in particular focuses on an interpretation of maximal dependence and an asymptotic normality result for a proposed semi-parametric estimator under a Gaussian copula assumption.","In addition, we discuss general axioms for dependence measures between multiple random vectors, other plausible normalizations, and various examples.","Afterwards, we look into plug-in estimators based on penalized empirical covariance matrices in order to deal with high dimensionality issues and take possible marginal independencies into account by inducing (block) sparsity.","The latter ideas are investigated via a simulation study, considering other dependence coefficients as well.","We illustrate the use of the developed methods in two real data applications."],"url":"http://arxiv.org/abs/2404.07141v1","category":"stat.ME"}
{"created":"2024-04-10 16:14:05","title":"Towards a Game-theoretic Understanding of Explanation-based Membership Inference Attacks","abstract":"Model explanations improve the transparency of black-box machine learning (ML) models and their decisions; however, they can also be exploited to carry out privacy threats such as membership inference attacks (MIA). Existing works have only analyzed MIA in a single \"what if\" interaction scenario between an adversary and the target ML model; thus, it does not discern the factors impacting the capabilities of an adversary in launching MIA in repeated interaction settings. Additionally, these works rely on assumptions about the adversary's knowledge of the target model's structure and, thus, do not guarantee the optimality of the predefined threshold required to distinguish the members from non-members. In this paper, we delve into the domain of explanation-based threshold attacks, where the adversary endeavors to carry out MIA attacks by leveraging the variance of explanations through iterative interactions with the system comprising of the target ML model and its corresponding explanation method. We model such interactions by employing a continuous-time stochastic signaling game framework. In our framework, an adversary plays a stopping game, interacting with the system (having imperfect information about the type of an adversary, i.e., honest or malicious) to obtain explanation variance information and computing an optimal threshold to determine the membership of a datapoint accurately. First, we propose a sound mathematical formulation to prove that such an optimal threshold exists, which can be used to launch MIA. Then, we characterize the conditions under which a unique Markov perfect equilibrium (or steady state) exists in this dynamic system. By means of a comprehensive set of simulations of the proposed game model, we assess different factors that can impact the capability of an adversary to launch MIA in such repeated interaction settings.","sentences":["Model explanations improve the transparency of black-box machine learning (ML) models and their decisions; however, they can also be exploited to carry out privacy threats such as membership inference attacks (MIA).","Existing works have only analyzed MIA in a single \"what if\" interaction scenario between an adversary and the target ML model; thus, it does not discern the factors impacting the capabilities of an adversary in launching MIA in repeated interaction settings.","Additionally, these works rely on assumptions about the adversary's knowledge of the target model's structure and, thus, do not guarantee the optimality of the predefined threshold required to distinguish the members from non-members.","In this paper, we delve into the domain of explanation-based threshold attacks, where the adversary endeavors to carry out MIA attacks by leveraging the variance of explanations through iterative interactions with the system comprising of the target ML model and its corresponding explanation method.","We model such interactions by employing a continuous-time stochastic signaling game framework.","In our framework, an adversary plays a stopping game, interacting with the system (having imperfect information about the type of an adversary, i.e., honest or malicious) to obtain explanation variance information and computing an optimal threshold to determine the membership of a datapoint accurately.","First, we propose a sound mathematical formulation to prove that such an optimal threshold exists, which can be used to launch MIA.","Then, we characterize the conditions under which a unique Markov perfect equilibrium (or steady state) exists in this dynamic system.","By means of a comprehensive set of simulations of the proposed game model, we assess different factors that can impact the capability of an adversary to launch MIA in such repeated interaction settings."],"url":"http://arxiv.org/abs/2404.07139v1","category":"cs.AI"}
{"created":"2024-04-10 16:12:50","title":"Towards Robustness of Text-to-Visualization Translation against Lexical and Phrasal Variability","abstract":"Text-to-Vis is an emerging task in the natural language processing (NLP) area that aims to automatically generate data visualizations from natural language questions (NLQs). Despite their progress, existing text-to-vis models often heavily rely on lexical matching between words in the questions and tokens in data schemas. This overreliance on lexical matching may lead to a diminished level of model robustness against input variations. In this study, we thoroughly examine the robustness of current text-to-vis models, an area that has not previously been explored. In particular, we construct the first robustness dataset nvBench-Rob, which contains diverse lexical and phrasal variations based on the original text-to-vis benchmark nvBench. Then, we found that the performance of existing text-to-vis models on this new dataset dramatically drops, implying that these methods exhibit inadequate robustness overall. Finally, we propose a novel framework based on Retrieval-Augmented Generation (RAG) technique, named GRED, specifically designed to address input perturbations in these two variants. The framework consists of three parts: NLQ-Retrieval Generator, Visualization Query-Retrieval Retuner and Annotation-based Debugger, which are used to tackle the challenges posed by natural language variants, programming style differences and data schema variants, respectively. Extensive experimental evaluations show that, compared to the state-of-the-art model RGVisNet in the Text-to-Vis field, GRED performs better in terms of model robustness, with a 32% increase in accuracy on the proposed nvBench-Rob dataset.","sentences":["Text-to-Vis is an emerging task in the natural language processing (NLP) area that aims to automatically generate data visualizations from natural language questions (NLQs).","Despite their progress, existing text-to-vis models often heavily rely on lexical matching between words in the questions and tokens in data schemas.","This overreliance on lexical matching may lead to a diminished level of model robustness against input variations.","In this study, we thoroughly examine the robustness of current text-to-vis models, an area that has not previously been explored.","In particular, we construct the first robustness dataset nvBench-Rob, which contains diverse lexical and phrasal variations based on the original text-to-vis benchmark nvBench.","Then, we found that the performance of existing text-to-vis models on this new dataset dramatically drops, implying that these methods exhibit inadequate robustness overall.","Finally, we propose a novel framework based on Retrieval-Augmented Generation (RAG) technique, named GRED, specifically designed to address input perturbations in these two variants.","The framework consists of three parts: NLQ-Retrieval Generator, Visualization Query-Retrieval Retuner and Annotation-based Debugger, which are used to tackle the challenges posed by natural language variants, programming style differences and data schema variants, respectively.","Extensive experimental evaluations show that, compared to the state-of-the-art model RGVisNet in the Text-to-Vis field, GRED performs better in terms of model robustness, with a 32% increase in accuracy on the proposed nvBench-Rob dataset."],"url":"http://arxiv.org/abs/2404.07135v2","category":"cs.CL"}
{"created":"2024-04-10 16:09:47","title":"Hedonic Models Incorporating ESG Factors for Time Series of Average Annual Home Prices","abstract":"Using data from 2000 through 2022, we analyze the predictive capability of the annual numbers of new home constructions and four available environmental, social, and governance factors on the average annual price of homes sold in eight major U.S. cities. We contrast the predictive capability of a P-spline generalized additive model (GAM) against a strictly linear version of the commonly used generalized linear model (GLM). As the data for the annual price and predictor variables constitute non-stationary time series, to avoid spurious correlations in the analysis we transform each time series appropriately to produce stationary series for use in the GAM and GLM models. While arithmetic returns or first differences are adequate transformations for the predictor variables, for the average price response variable we utilize the series of innovations obtained from AR(q)-ARCH(1) fits. Based on the GAM results, we find that the influence of ESG factors varies markedly by city, reflecting geographic diversity. Notably, the presence of air conditioning emerges as a strong factor. Despite limitations on the length of available time series, this study represents a pivotal step toward integrating ESG considerations into predictive real estate models.","sentences":["Using data from 2000 through 2022, we analyze the predictive capability of the annual numbers of new home constructions and four available environmental, social, and governance factors on the average annual price of homes sold in eight major U.S. cities.","We contrast the predictive capability of a P-spline generalized additive model (GAM) against a strictly linear version of the commonly used generalized linear model (GLM).","As the data for the annual price and predictor variables constitute non-stationary time series, to avoid spurious correlations in the analysis we transform each time series appropriately to produce stationary series for use in the GAM and GLM models.","While arithmetic returns or first differences are adequate transformations for the predictor variables, for the average price response variable we utilize the series of innovations obtained from AR(q)-ARCH(1) fits.","Based on the GAM results, we find that the influence of ESG factors varies markedly by city, reflecting geographic diversity.","Notably, the presence of air conditioning emerges as a strong factor.","Despite limitations on the length of available time series, this study represents a pivotal step toward integrating ESG considerations into predictive real estate models."],"url":"http://arxiv.org/abs/2404.07132v1","category":"q-fin.CP"}
{"created":"2024-04-10 16:07:29","title":"Learning of deep convolutional network image classifiers via stochastic gradient descent and over-parametrization","abstract":"Image classification from independent and identically distributed random variables is considered. Image classifiers are defined which are based on a linear combination of deep convolutional networks with max-pooling layer. Here all the weights are learned by stochastic gradient descent. A general result is presented which shows that the image classifiers are able to approximate the best possible deep convolutional network. In case that the a posteriori probability satisfies a suitable hierarchical composition model it is shown that the corresponding deep convolutional neural network image classifier achieves a rate of convergence which is independent of the dimension of the images.","sentences":["Image classification from independent and identically distributed random variables is considered.","Image classifiers are defined which are based on a linear combination of deep convolutional networks with max-pooling layer.","Here all the weights are learned by stochastic gradient descent.","A general result is presented which shows that the image classifiers are able to approximate the best possible deep convolutional network.","In case that the a posteriori probability satisfies a suitable hierarchical composition model it is shown that the corresponding deep convolutional neural network image classifier achieves a rate of convergence which is independent of the dimension of the images."],"url":"http://arxiv.org/abs/2404.07128v1","category":"math.ST"}
{"created":"2024-04-10 16:05:37","title":"Iterative solvers in adaptive FEM","abstract":"This chapter provides an overview of state-of-the-art adaptive finite element methods (AFEMs) for the numerical solution of second-order elliptic partial differential equations (PDEs), where the primary focus is on the optimal interplay of local mesh refinement and iterative solution of the arising discrete systems. Particular emphasis is placed on the thorough description of the essential ingredients necessary to design adaptive algorithms of optimal complexity, i.e., algorithms that mathematically guarantee the optimal rate of convergence with respect to the overall computational cost and, hence, time. Crucially, adaptivity induces reliability of the computed numerical approximations by means of a-posteriori error control. This ensures that the error committed by the numerical scheme is bounded from above by computable quantities. The analysis of the adaptive algorithms is based on the study of appropriate quasi-error quantities that include and balance different components of the overall error. Importantly, the quasi-errors stemming from an adaptive algorithm with contractive iterative solver satisfy a centerpiece concept, namely, full R-linear convergence. This guarantees that the adaptive algorithm is essentially contracting this quasi-error at each step and it turns out to be the cornerstone for the optimal complexity of AFEM. The unified analysis of the adaptive algorithms is presented in the context of symmetric linear PDEs. Extensions to goal-oriented, non-symmetric, as well as non-linear PDEs are presented with suitable nested iterative solvers fitting into the general analytical framework of the linear symmetric case. Numerical experiments highlight the theoretical results and emphasize the practical relevance and gain of adaptivity with iterative solvers for numerical simulations with optimal complexity.","sentences":["This chapter provides an overview of state-of-the-art adaptive finite element methods (AFEMs) for the numerical solution of second-order elliptic partial differential equations (PDEs), where the primary focus is on the optimal interplay of local mesh refinement and iterative solution of the arising discrete systems.","Particular emphasis is placed on the thorough description of the essential ingredients necessary to design adaptive algorithms of optimal complexity, i.e., algorithms that mathematically guarantee the optimal rate of convergence with respect to the overall computational cost and, hence, time.","Crucially, adaptivity induces reliability of the computed numerical approximations by means of a-posteriori error control.","This ensures that the error committed by the numerical scheme is bounded from above by computable quantities.","The analysis of the adaptive algorithms is based on the study of appropriate quasi-error quantities that include and balance different components of the overall error.","Importantly, the quasi-errors stemming from an adaptive algorithm with contractive iterative solver satisfy a centerpiece concept, namely, full R-linear convergence.","This guarantees that the adaptive algorithm is essentially contracting this quasi-error at each step and it turns out to be the cornerstone for the optimal complexity of AFEM.","The unified analysis of the adaptive algorithms is presented in the context of symmetric linear PDEs.","Extensions to goal-oriented, non-symmetric, as well as non-linear PDEs are presented with suitable nested iterative solvers fitting into the general analytical framework of the linear symmetric case.","Numerical experiments highlight the theoretical results and emphasize the practical relevance and gain of adaptivity with iterative solvers for numerical simulations with optimal complexity."],"url":"http://arxiv.org/abs/2404.07126v1","category":"math.NA"}
{"created":"2024-04-10 16:04:21","title":"Measuring proximity to standard planes during fetal brain ultrasound scanning","abstract":"This paper introduces a novel pipeline designed to bring ultrasound (US) plane pose estimation closer to clinical use for more effective navigation to the standard planes (SPs) in the fetal brain. We propose a semi-supervised segmentation model utilizing both labeled SPs and unlabeled 3D US volume slices. Our model enables reliable segmentation across a diverse set of fetal brain images. Furthermore, the model incorporates a classification mechanism to identify the fetal brain precisely. Our model not only filters out frames lacking the brain but also generates masks for those containing it, enhancing the relevance of plane pose regression in clinical settings. We focus on fetal brain navigation from 2D ultrasound (US) video analysis and combine this model with a US plane pose regression network to provide sensorless proximity detection to SPs and non-SPs planes; we emphasize the importance of proximity detection to SPs for guiding sonographers, offering a substantial advantage over traditional methods by allowing earlier and more precise adjustments during scanning. We demonstrate the practical applicability of our approach through validation on real fetal scan videos obtained from sonographers of varying expertise levels. Our findings demonstrate the potential of our approach to complement existing fetal US technologies and advance prenatal diagnostic practices.","sentences":["This paper introduces a novel pipeline designed to bring ultrasound (US) plane pose estimation closer to clinical use for more effective navigation to the standard planes (SPs) in the fetal brain.","We propose a semi-supervised segmentation model utilizing both labeled SPs and unlabeled 3D US volume slices.","Our model enables reliable segmentation across a diverse set of fetal brain images.","Furthermore, the model incorporates a classification mechanism to identify the fetal brain precisely.","Our model not only filters out frames lacking the brain but also generates masks for those containing it, enhancing the relevance of plane pose regression in clinical settings.","We focus on fetal brain navigation from 2D ultrasound (US) video analysis and combine this model with a US plane pose regression network to provide sensorless proximity detection to SPs and non-SPs planes; we emphasize the importance of proximity detection to SPs for guiding sonographers, offering a substantial advantage over traditional methods by allowing earlier and more precise adjustments during scanning.","We demonstrate the practical applicability of our approach through validation on real fetal scan videos obtained from sonographers of varying expertise levels.","Our findings demonstrate the potential of our approach to complement existing fetal US technologies and advance prenatal diagnostic practices."],"url":"http://arxiv.org/abs/2404.07124v1","category":"cs.CV"}
{"created":"2024-04-10 16:04:07","title":"Semantically-correlated memories in a dense associative model","abstract":"I introduce a novel associative memory model named Correlated Dense Associative Memory (CDAM), which integrates both auto- and hetero-association in a unified framework for continuous-valued memory patterns. Employing an arbitrary graph structure to semantically link memory patterns, CDAM is theoretically and numerically analysed, revealing four distinct dynamical modes: auto-association, narrow hetero-association, wide hetero-association, and neutral quiescence. Drawing inspiration from inhibitory modulation studies, I employ anti-Hebbian learning rules to control the range of hetero-association, extract multi-scale representations of community structures in graphs, and stabilise the recall of temporal sequences. Experimental demonstrations showcase CDAM's efficacy in handling real-world data, replicating a classical neuroscience experiment, performing image retrieval, and simulating arbitrary finite automata.","sentences":["I introduce a novel associative memory model named Correlated Dense Associative Memory (CDAM), which integrates both auto- and hetero-association in a unified framework for continuous-valued memory patterns.","Employing an arbitrary graph structure to semantically link memory patterns, CDAM is theoretically and numerically analysed, revealing four distinct dynamical modes: auto-association, narrow hetero-association, wide hetero-association, and neutral quiescence.","Drawing inspiration from inhibitory modulation studies, I employ anti-Hebbian learning rules to control the range of hetero-association, extract multi-scale representations of community structures in graphs, and stabilise the recall of temporal sequences.","Experimental demonstrations showcase CDAM's efficacy in handling real-world data, replicating a classical neuroscience experiment, performing image retrieval, and simulating arbitrary finite automata."],"url":"http://arxiv.org/abs/2404.07123v2","category":"cs.NE"}
{"created":"2024-04-10 15:59:48","title":"Digital Over-the-Air Computation: Achieving High Reliability via Bit-Slicing","abstract":"6G mobile networks aim to realize ubiquitous intelligence at the network edge via distributed learning, sensing, and data analytics. Their common operation is to aggregate high-dimensional data, which causes a communication bottleneck that cannot be resolved using traditional orthogonal multi-access schemes. A promising solution, called over-the-air computation (AirComp), exploits channels' waveform superposition property to enable simultaneous access, thereby overcoming the bottleneck. Nevertheless, its reliance on uncoded linear analog modulation exposes data to perturbation by noise and interference. Hence, the traditional analog AirComp falls short of meeting the high-reliability requirement for 6G. Overcoming the limitation of analog AirComp motivates this work, which focuses on developing a framework for digital AirComp. The proposed framework features digital modulation of each data value, integrated with the bit-slicing technique to allocate its bits to multiple symbols, thereby increasing the AirComp reliability. To optimally detect the aggregated digital symbols, we derive the optimal maximum a posteriori detector that is shown to outperform the traditional maximum likelihood detector. Furthermore, a comparative performance analysis of digital AirComp with respect to its analog counterpart with repetition coding is conducted to quantify the practical signal-to-noise ratio (SNR) regime favoring the proposed scheme. On the other hand, digital AirComp is enhanced by further development to feature awareness of heterogeneous bit importance levels and its exploitation in channel adaptation. Lastly, simulation results demonstrate the achivability of substantial reliability improvement of digital AirComp over its analog counterpart given the same channel uses.","sentences":["6G mobile networks aim to realize ubiquitous intelligence at the network edge via distributed learning, sensing, and data analytics.","Their common operation is to aggregate high-dimensional data, which causes a communication bottleneck that cannot be resolved using traditional orthogonal multi-access schemes.","A promising solution, called over-the-air computation (AirComp), exploits channels' waveform superposition property to enable simultaneous access, thereby overcoming the bottleneck.","Nevertheless, its reliance on uncoded linear analog modulation exposes data to perturbation by noise and interference.","Hence, the traditional analog AirComp falls short of meeting the high-reliability requirement for 6G. Overcoming the limitation of analog AirComp motivates this work, which focuses on developing a framework for digital AirComp.","The proposed framework features digital modulation of each data value, integrated with the bit-slicing technique to allocate its bits to multiple symbols, thereby increasing the AirComp reliability.","To optimally detect the aggregated digital symbols, we derive the optimal maximum a posteriori detector that is shown to outperform the traditional maximum likelihood detector.","Furthermore, a comparative performance analysis of digital AirComp with respect to its analog counterpart with repetition coding is conducted to quantify the practical signal-to-noise ratio (SNR) regime favoring the proposed scheme.","On the other hand, digital AirComp is enhanced by further development to feature awareness of heterogeneous bit importance levels and its exploitation in channel adaptation.","Lastly, simulation results demonstrate the achivability of substantial reliability improvement of digital AirComp over its analog counterpart given the same channel uses."],"url":"http://arxiv.org/abs/2404.07121v1","category":"cs.IT"}
{"created":"2024-04-10 15:55:07","title":"Continuous Language Model Interpolation for Dynamic and Controllable Text Generation","abstract":"As large language models (LLMs) have gained popularity for a variety of use cases, making them adaptable and controllable has become increasingly important, especially for user-facing applications. While the existing literature on LLM adaptation primarily focuses on finding a model (or models) that optimizes a single predefined objective, here we focus on the challenging case where the model must dynamically adapt to diverse -- and often changing -- user preferences. For this, we leverage adaptation methods based on linear weight interpolation, casting them as continuous multi-domain interpolators that produce models with specific prescribed generation characteristics on-the-fly. Specifically, we use low-rank updates to fine-tune a base model to various different domains, yielding a set of anchor models with distinct generation profiles. Then, we use the weight updates of these anchor models to parametrize the entire (infinite) class of models contained within their convex hull. We empirically show that varying the interpolation weights yields predictable and consistent change in the model outputs with respect to all of the controlled attributes. We find that there is little entanglement between most attributes and identify and discuss the pairs of attributes for which this is not the case. Our results suggest that linearly interpolating between the weights of fine-tuned models facilitates predictable, fine-grained control of model outputs with respect to multiple stylistic characteristics simultaneously.","sentences":["As large language models (LLMs) have gained popularity for a variety of use cases, making them adaptable and controllable has become increasingly important, especially for user-facing applications.","While the existing literature on LLM adaptation primarily focuses on finding a model (or models) that optimizes a single predefined objective, here we focus on the challenging case where the model must dynamically adapt to diverse -- and often changing -- user preferences.","For this, we leverage adaptation methods based on linear weight interpolation, casting them as continuous multi-domain interpolators that produce models with specific prescribed generation characteristics on-the-fly.","Specifically, we use low-rank updates to fine-tune a base model to various different domains, yielding a set of anchor models with distinct generation profiles.","Then, we use the weight updates of these anchor models to parametrize the entire (infinite) class of models contained within their convex hull.","We empirically show that varying the interpolation weights yields predictable and consistent change in the model outputs with respect to all of the controlled attributes.","We find that there is little entanglement between most attributes and identify and discuss the pairs of attributes for which this is not the case.","Our results suggest that linearly interpolating between the weights of fine-tuned models facilitates predictable, fine-grained control of model outputs with respect to multiple stylistic characteristics simultaneously."],"url":"http://arxiv.org/abs/2404.07117v1","category":"cs.CL"}
{"created":"2024-04-10 15:53:54","title":"Photonic next-generation reservoir computer based on distributed feedback in optical fiber","abstract":"Reservoir computing (RC) is a machine learning paradigm that excels at dynamical systems analysis. Photonic RCs, which perform implicit computation through optical interactions, have attracted increasing attention due to their potential for low latency predictions. However, most existing photonic RCs rely on a nonlinear physical cavity to implement system memory, limiting control over the memory structure and requiring long warm-up times to eliminate transients. In this work, we resolve these issues by demonstrating a photonic next-generation reservoir computer (NG-RC) using a fiber optic platform. Our photonic NG-RC eliminates the need for a cavity by generating feature vectors directly from nonlinear combinations of the input data with varying delays. Our approach uses Rayleigh backscattering to produce output feature vectors by an unconventional nonlinearity resulting from coherent, interferometric mixing followed by a quadratic readout. Performing linear optimization on these feature vectors, our photonic NG-RC demonstrates state-of-the-art performance for the observer (cross-prediction) task applied to the R\\\"ossler, Lorenz, and Kuramoto-Sivashinsky systems. In contrast to digital NG-RC implementations, this scheme is easily scalable to high-dimensional systems while maintaining low latency and low power consumption.","sentences":["Reservoir computing (RC) is a machine learning paradigm that excels at dynamical systems analysis.","Photonic RCs, which perform implicit computation through optical interactions, have attracted increasing attention due to their potential for low latency predictions.","However, most existing photonic RCs rely on a nonlinear physical cavity to implement system memory, limiting control over the memory structure and requiring long warm-up times to eliminate transients.","In this work, we resolve these issues by demonstrating a photonic next-generation reservoir computer (NG-RC) using a fiber optic platform.","Our photonic NG-RC eliminates the need for a cavity by generating feature vectors directly from nonlinear combinations of the input data with varying delays.","Our approach uses Rayleigh backscattering to produce output feature vectors by an unconventional nonlinearity resulting from coherent, interferometric mixing followed by a quadratic readout.","Performing linear optimization on these feature vectors, our photonic NG-RC demonstrates state-of-the-art performance for the observer (cross-prediction) task applied to the R\\\"ossler, Lorenz, and Kuramoto-Sivashinsky systems.","In contrast to digital NG-RC implementations, this scheme is easily scalable to high-dimensional systems while maintaining low latency and low power consumption."],"url":"http://arxiv.org/abs/2404.07116v1","category":"physics.optics"}
{"created":"2024-04-10 15:51:46","title":"Unfolding ADMM for Enhanced Subspace Clustering of Hyperspectral Images","abstract":"Deep subspace clustering methods are now prominent in clustering, typically using fully connected networks and a self-representation loss function. However, these methods often struggle with overfitting and lack interpretability. In this paper, we explore an alternative clustering approach based on deep unfolding. By unfolding iterative optimization methods into neural networks, this approach offers enhanced interpretability and reliability compared to data-driven deep learning methods, and greater adaptability and generalization than model-based approaches. Hence, unfolding has become widely used in inverse imaging problems, such as image restoration, reconstruction, and super-resolution, but has not been sufficiently explored yet in the context of clustering. In this work, we introduce an innovative clustering architecture for hyperspectral images (HSI) by unfolding an iterative solver based on the Alternating Direction Method of Multipliers (ADMM) for sparse subspace clustering. To our knowledge, this is the first attempt to apply unfolding ADMM for computing the self-representation matrix in subspace clustering. Moreover, our approach captures well the structural characteristics of HSI data by employing the K nearest neighbors algorithm as part of a structure preservation module. Experimental evaluation of three established HSI datasets shows clearly the potential of the unfolding approach in HSI clustering and even demonstrates superior performance compared to state-of-the-art techniques.","sentences":["Deep subspace clustering methods are now prominent in clustering, typically using fully connected networks and a self-representation loss function.","However, these methods often struggle with overfitting and lack interpretability.","In this paper, we explore an alternative clustering approach based on deep unfolding.","By unfolding iterative optimization methods into neural networks, this approach offers enhanced interpretability and reliability compared to data-driven deep learning methods, and greater adaptability and generalization than model-based approaches.","Hence, unfolding has become widely used in inverse imaging problems, such as image restoration, reconstruction, and super-resolution, but has not been sufficiently explored yet in the context of clustering.","In this work, we introduce an innovative clustering architecture for hyperspectral images (HSI) by unfolding an iterative solver based on the Alternating Direction Method of Multipliers (ADMM) for sparse subspace clustering.","To our knowledge, this is the first attempt to apply unfolding ADMM for computing the self-representation matrix in subspace clustering.","Moreover, our approach captures well the structural characteristics of HSI data by employing the K nearest neighbors algorithm as part of a structure preservation module.","Experimental evaluation of three established HSI datasets shows clearly the potential of the unfolding approach in HSI clustering and even demonstrates superior performance compared to state-of-the-art techniques."],"url":"http://arxiv.org/abs/2404.07112v1","category":"cs.CV"}
{"created":"2024-04-10 15:51:40","title":"The generic dual of p-adic groups and applications","abstract":"In this paper, we give a uniform classification of the generic dual of quasi-split classical groups, their similitude counterparts, and general spin groups. As applications, for quasi-split classical groups, we show that the functorial lifting maps constructed by Cogdell, Kim, Piatetski-Shapiro and Shahidi are surjective. We also analyze structures of general local Langlands parameters and explicitly construct a distinguished element for each local L-packet.","sentences":["In this paper, we give a uniform classification of the generic dual of quasi-split classical groups, their similitude counterparts, and general spin groups.","As applications, for quasi-split classical groups, we show that the functorial lifting maps constructed by Cogdell, Kim, Piatetski-Shapiro and Shahidi are surjective.","We also analyze structures of general local Langlands parameters and explicitly construct a distinguished element for each local L-packet."],"url":"http://arxiv.org/abs/2404.07111v1","category":"math.RT"}
{"created":"2024-04-10 15:47:35","title":"Wild Visual Navigation: Fast Traversability Learning via Pre-Trained Models and Online Self-Supervision","abstract":"Natural environments such as forests and grasslands are challenging for robotic navigation because of the false perception of rigid obstacles from high grass, twigs, or bushes. In this work, we present Wild Visual Navigation (WVN), an online self-supervised learning system for visual traversability estimation. The system is able to continuously adapt from a short human demonstration in the field, only using onboard sensing and computing. One of the key ideas to achieve this is the use of high-dimensional features from pre-trained self-supervised models, which implicitly encode semantic information that massively simplifies the learning task. Further, the development of an online scheme for supervision generator enables concurrent training and inference of the learned model in the wild. We demonstrate our approach through diverse real-world deployments in forests, parks, and grasslands. Our system is able to bootstrap the traversable terrain segmentation in less than 5 min of in-field training time, enabling the robot to navigate in complex, previously unseen outdoor terrains. Code: https://bit.ly/498b0CV - Project page:https://bit.ly/3M6nMHH","sentences":["Natural environments such as forests and grasslands are challenging for robotic navigation because of the false perception of rigid obstacles from high grass, twigs, or bushes.","In this work, we present Wild Visual Navigation (WVN), an online self-supervised learning system for visual traversability estimation.","The system is able to continuously adapt from a short human demonstration in the field, only using onboard sensing and computing.","One of the key ideas to achieve this is the use of high-dimensional features from pre-trained self-supervised models, which implicitly encode semantic information that massively simplifies the learning task.","Further, the development of an online scheme for supervision generator enables concurrent training and inference of the learned model in the wild.","We demonstrate our approach through diverse real-world deployments in forests, parks, and grasslands.","Our system is able to bootstrap the traversable terrain segmentation in less than 5 min of in-field training time, enabling the robot to navigate in complex, previously unseen outdoor terrains.","Code: https://bit.ly/498b0CV - Project page:https://bit.ly/3M6nMHH"],"url":"http://arxiv.org/abs/2404.07110v1","category":"cs.RO"}
{"created":"2024-04-10 15:46:21","title":"Coupling Molecular Density Functional Theory with Converged Selected Configuration Interaction Methods to Study Excited states in Aqueous Solution","abstract":"This paper presents the first implementation of a coupling between advanced wave function theories and molecular density functional theory (MDFT). This method enables the modeling of solvent effect into quantum mechanical (QM) calculations by incorporating an electrostatic potential generated by solvent charges into the electronic Hamiltonian. Solvent charges are deduced from the spatially and angularly dependent solvent particle density. Such density is obtained through the minimization of the functional associated to the molecular mechanics (MM) Hamiltonian describing the interaction between the fluid particles. The introduced QM/MDFT framework belongs to QM/MM family of methods but its originality lies in the use of MDFT as the MM solver, offering two main advantages. Firstly, its functional formulation makes it competitive with respect to sampling-based molecular mechanics. Secondly, it preserves a molecular-level description lost in macroscopic continuum approaches. Excited states properties of water and formaldehyde molecules solvated into water have been computed at the selected configuration interaction (SCI) level. Excitation energies and dipole moment have been compared with experimental data and previous theoretical work. A key finding is that using the Hartree-Fock method to describe the solute allows for predicting the solvent charge around the ground-state with sufficient precision for the subsequent SCI calculations of excited-states. This significantly reduces the computational cost of the described procedure, paving the way for the study of more complex molecules.","sentences":["This paper presents the first implementation of a coupling between advanced wave function theories and molecular density functional theory (MDFT).","This method enables the modeling of solvent effect into quantum mechanical (QM) calculations by incorporating an electrostatic potential generated by solvent charges into the electronic Hamiltonian.","Solvent charges are deduced from the spatially and angularly dependent solvent particle density.","Such density is obtained through the minimization of the functional associated to the molecular mechanics (MM) Hamiltonian describing the interaction between the fluid particles.","The introduced QM/MDFT framework belongs to QM/MM family of methods but its originality lies in the use of MDFT as the MM solver, offering two main advantages.","Firstly, its functional formulation makes it competitive with respect to sampling-based molecular mechanics.","Secondly, it preserves a molecular-level description lost in macroscopic continuum approaches.","Excited states properties of water and formaldehyde molecules solvated into water have been computed at the selected configuration interaction (SCI) level.","Excitation energies and dipole moment have been compared with experimental data and previous theoretical work.","A key finding is that using the Hartree-Fock method to describe the solute allows for predicting the solvent charge around the ground-state with sufficient precision for the subsequent SCI calculations of excited-states.","This significantly reduces the computational cost of the described procedure, paving the way for the study of more complex molecules."],"url":"http://arxiv.org/abs/2404.07109v1","category":"physics.chem-ph"}
{"created":"2024-04-10 15:46:08","title":"From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications","abstract":"Evaluating large language models (LLMs) is fundamental, particularly in the context of practical applications. Conventional evaluation methods, typically designed primarily for LLM development, yield numerical scores that ignore the user experience. Therefore, our study shifts the focus from model-centered to human-centered evaluation in the context of AI-powered writing assistance applications. Our proposed metric, termed ``Revision Distance,'' utilizes LLMs to suggest revision edits that mimic the human writing process. It is determined by counting the revision edits generated by LLMs. Benefiting from the generated revision edit details, our metric can provide a self-explained text evaluation result in a human-understandable manner beyond the context-independent score. Our results show that for the easy-writing task, ``Revision Distance'' is consistent with established metrics (ROUGE, Bert-score, and GPT-score), but offers more insightful, detailed feedback and better distinguishes between texts. Moreover, in the context of challenging academic writing tasks, our metric still delivers reliable evaluations where other metrics tend to struggle. Furthermore, our metric also holds significant potential for scenarios lacking reference texts.","sentences":["Evaluating large language models (LLMs) is fundamental, particularly in the context of practical applications.","Conventional evaluation methods, typically designed primarily for LLM development, yield numerical scores that ignore the user experience.","Therefore, our study shifts the focus from model-centered to human-centered evaluation in the context of AI-powered writing assistance applications.","Our proposed metric, termed ``Revision Distance,'' utilizes LLMs to suggest revision edits that mimic the human writing process.","It is determined by counting the revision edits generated by LLMs.","Benefiting from the generated revision edit details, our metric can provide a self-explained text evaluation result in a human-understandable manner beyond the context-independent score.","Our results show that for the easy-writing task, ``Revision Distance'' is consistent with established metrics (ROUGE, Bert-score, and GPT-score), but offers more insightful, detailed feedback and better distinguishes between texts.","Moreover, in the context of challenging academic writing tasks, our metric still delivers reliable evaluations where other metrics tend to struggle.","Furthermore, our metric also holds significant potential for scenarios lacking reference texts."],"url":"http://arxiv.org/abs/2404.07108v2","category":"cs.CL"}
{"created":"2024-04-10 15:45:50","title":"Entanglement distribution through separable states via a zero-added-loss photon multiplexing inspired protocol","abstract":"The recently proposed zero-added-loss multiplexing (ZALM) source of entangled photons enables higher efficiency in entanglement distribution than SPDC sources and can be carried out using both space-to-ground and ground-to-ground links. We demonstrate the flexibility of ZALM architectures to be adapted to alternative entanglement distribution protocols. Focusing on the counter-intuitive result that entanglement can be generated between distant parties without using any entanglement as a resource, we analyze two protocols for entanglement distribution to memories via separable states. Modelling them in a ZALM setup, we consider the effects of noise both in the communication channels and in the memories. We thereby identify the optimal protocol to use, with respect to the highest entanglement generated, given the noise conditions of the network.","sentences":["The recently proposed zero-added-loss multiplexing (ZALM) source of entangled photons enables higher efficiency in entanglement distribution than SPDC sources and can be carried out using both space-to-ground and ground-to-ground links.","We demonstrate the flexibility of ZALM architectures to be adapted to alternative entanglement distribution protocols.","Focusing on the counter-intuitive result that entanglement can be generated between distant parties without using any entanglement as a resource, we analyze two protocols for entanglement distribution to memories via separable states.","Modelling them in a ZALM setup, we consider the effects of noise both in the communication channels and in the memories.","We thereby identify the optimal protocol to use, with respect to the highest entanglement generated, given the noise conditions of the network."],"url":"http://arxiv.org/abs/2404.07107v1","category":"quant-ph"}
{"created":"2024-04-10 15:45:03","title":"3DMambaComplete: Exploring Structured State Space Model for Point Cloud Completion","abstract":"Point cloud completion aims to generate a complete and high-fidelity point cloud from an initially incomplete and low-quality input. A prevalent strategy involves leveraging Transformer-based models to encode global features and facilitate the reconstruction process. However, the adoption of pooling operations to obtain global feature representations often results in the loss of local details within the point cloud. Moreover, the attention mechanism inherent in Transformers introduces additional computational complexity, rendering it challenging to handle long sequences effectively. To address these issues, we propose 3DMambaComplete, a point cloud completion network built on the novel Mamba framework. It comprises three modules: HyperPoint Generation encodes point cloud features using Mamba's selection mechanism and predicts a set of Hyperpoints. A specific offset is estimated, and the down-sampled points become HyperPoints. The HyperPoint Spread module disperses these HyperPoints across different spatial locations to avoid concentration. Finally, a deformation method transforms the 2D mesh representation of HyperPoints into a fine-grained 3D structure for point cloud reconstruction. Extensive experiments conducted on various established benchmarks demonstrate that 3DMambaComplete surpasses state-of-the-art point cloud completion methods, as confirmed by qualitative and quantitative analyses.","sentences":["Point cloud completion aims to generate a complete and high-fidelity point cloud from an initially incomplete and low-quality input.","A prevalent strategy involves leveraging Transformer-based models to encode global features and facilitate the reconstruction process.","However, the adoption of pooling operations to obtain global feature representations often results in the loss of local details within the point cloud.","Moreover, the attention mechanism inherent in Transformers introduces additional computational complexity, rendering it challenging to handle long sequences effectively.","To address these issues, we propose 3DMambaComplete, a point cloud completion network built on the novel Mamba framework.","It comprises three modules: HyperPoint Generation encodes point cloud features using Mamba's selection mechanism and predicts a set of Hyperpoints.","A specific offset is estimated, and the down-sampled points become HyperPoints.","The HyperPoint Spread module disperses these HyperPoints across different spatial locations to avoid concentration.","Finally, a deformation method transforms the 2D mesh representation of HyperPoints into a fine-grained 3D structure for point cloud reconstruction.","Extensive experiments conducted on various established benchmarks demonstrate that 3DMambaComplete surpasses state-of-the-art point cloud completion methods, as confirmed by qualitative and quantitative analyses."],"url":"http://arxiv.org/abs/2404.07106v1","category":"cs.CV"}
{"created":"2024-04-10 15:41:53","title":"Empowering AlphaFold2 for protein conformation selective drug discovery with AlphaFold2-RAVE","abstract":"Small molecule drug design hinges on obtaining co-crystallized ligand-protein structures. Despite AlphaFold2's strides in protein native structure prediction, its focus on apo structures overlooks ligands and associated holo structures. Moreover, designing selective drugs often benefits from the targeting of diverse metastable conformations. Therefore, direct application of AlphaFold2 models in virtual screening and drug discovery remains tentative. Here, we demonstrate an AlphaFold2 based framework combined with all-atom enhanced sampling molecular dynamics and induced fit docking, named AF2RAVE-Glide, to conduct computational model based small molecule binding of metastable protein kinase conformations, initiated from protein sequences. We demonstrate the AF2RAVE-Glide workflow on protein kinases and their inhibitors, with special emphasis on binding of known type II kinase inhibitors which target the metastable classical DFG-out state. These states are not easy to sample from AlphaFold2. Here we demonstrate how with AF2RAVE these metastable conformations can be sampled for different kinases with high enough accuracy to enable subsequent docking of known type II kinase inhibitors with more than 50% success rates across docking calculations. We believe the protocol should be deployable for other kinases and more proteins generally.","sentences":["Small molecule drug design hinges on obtaining co-crystallized ligand-protein structures.","Despite AlphaFold2's strides in protein native structure prediction, its focus on apo structures overlooks ligands and associated holo structures.","Moreover, designing selective drugs often benefits from the targeting of diverse metastable conformations.","Therefore, direct application of AlphaFold2 models in virtual screening and drug discovery remains tentative.","Here, we demonstrate an AlphaFold2 based framework combined with all-atom enhanced sampling molecular dynamics and induced fit docking, named AF2RAVE-Glide, to conduct computational model based small molecule binding of metastable protein kinase conformations, initiated from protein sequences.","We demonstrate the AF2RAVE-Glide workflow on protein kinases and their inhibitors, with special emphasis on binding of known type II kinase inhibitors which target the metastable classical DFG-out state.","These states are not easy to sample from AlphaFold2.","Here we demonstrate how with AF2RAVE these metastable conformations can be sampled for different kinases with high enough accuracy to enable subsequent docking of known type II kinase inhibitors with more than 50% success rates across docking calculations.","We believe the protocol should be deployable for other kinases and more proteins generally."],"url":"http://arxiv.org/abs/2404.07102v1","category":"physics.bio-ph"}
{"created":"2024-04-10 15:39:49","title":"Rethinking Out-of-Distribution Detection for Reinforcement Learning: Advancing Methods for Evaluation and Detection","abstract":"While reinforcement learning (RL) algorithms have been successfully applied across numerous sequential decision-making problems, their generalization to unforeseen testing environments remains a significant concern. In this paper, we study the problem of out-of-distribution (OOD) detection in RL, which focuses on identifying situations at test time that RL agents have not encountered in their training environments. We first propose a clarification of terminology for OOD detection in RL, which aligns it with the literature from other machine learning domains. We then present new benchmark scenarios for OOD detection, which introduce anomalies with temporal autocorrelation into different components of the agent-environment loop. We argue that such scenarios have been understudied in the current literature, despite their relevance to real-world situations. Confirming our theoretical predictions, our experimental results suggest that state-of-the-art OOD detectors are not able to identify such anomalies. To address this problem, we propose a novel method for OOD detection, which we call DEXTER (Detection via Extraction of Time Series Representations). By treating environment observations as time series data, DEXTER extracts salient time series features, and then leverages an ensemble of isolation forest algorithms to detect anomalies. We find that DEXTER can reliably identify anomalies across benchmark scenarios, exhibiting superior performance compared to both state-of-the-art OOD detectors and high-dimensional changepoint detectors adopted from statistics.","sentences":["While reinforcement learning (RL) algorithms have been successfully applied across numerous sequential decision-making problems, their generalization to unforeseen testing environments remains a significant concern.","In this paper, we study the problem of out-of-distribution (OOD) detection in RL, which focuses on identifying situations at test time that RL agents have not encountered in their training environments.","We first propose a clarification of terminology for OOD detection in RL, which aligns it with the literature from other machine learning domains.","We then present new benchmark scenarios for OOD detection, which introduce anomalies with temporal autocorrelation into different components of the agent-environment loop.","We argue that such scenarios have been understudied in the current literature, despite their relevance to real-world situations.","Confirming our theoretical predictions, our experimental results suggest that state-of-the-art OOD detectors are not able to identify such anomalies.","To address this problem, we propose a novel method for OOD detection, which we call DEXTER (Detection via Extraction of Time Series Representations).","By treating environment observations as time series data, DEXTER extracts salient time series features, and then leverages an ensemble of isolation forest algorithms to detect anomalies.","We find that DEXTER can reliably identify anomalies across benchmark scenarios, exhibiting superior performance compared to both state-of-the-art OOD detectors and high-dimensional changepoint detectors adopted from statistics."],"url":"http://arxiv.org/abs/2404.07099v1","category":"cs.LG"}
{"created":"2024-04-10 15:37:00","title":"Learning Priors for Non Rigid SfM from Casual Videos","abstract":"We tackle the long-standing challenge of reconstructing 3D structures and camera positions from videos. The problem is particularly hard when objects are transformed in a non-rigid way. Current approaches to this problem make unrealistic assumptions or require a long optimization time.   We present TracksTo4D, a novel deep learning-based approach that enables inferring 3D structure and camera positions from dynamic content originating from in-the-wild videos using a single feed-forward pass on a sparse point track matrix. To achieve this, we leverage recent advances in 2D point tracking and design an equivariant neural architecture tailored for directly processing 2D point tracks by leveraging their symmetries. TracksTo4D is trained on a dataset of in-the-wild videos utilizing only the 2D point tracks extracted from the videos, without any 3D supervision. Our experiments demonstrate that TracksTo4D generalizes well to unseen videos of unseen semantic categories at inference time, producing equivalent results to state-of-the-art methods while significantly reducing the runtime compared to other baselines.","sentences":["We tackle the long-standing challenge of reconstructing 3D structures and camera positions from videos.","The problem is particularly hard when objects are transformed in a non-rigid way.","Current approaches to this problem make unrealistic assumptions or require a long optimization time.   ","We present TracksTo4D, a novel deep learning-based approach that enables inferring 3D structure and camera positions from dynamic content originating from in-the-wild videos using a single feed-forward pass on a sparse point track matrix.","To achieve this, we leverage recent advances in 2D point tracking and design an equivariant neural architecture tailored for directly processing 2D point tracks by leveraging their symmetries.","TracksTo4D is trained on a dataset of in-the-wild videos utilizing only the 2D point tracks extracted from the videos, without any 3D supervision.","Our experiments demonstrate that TracksTo4D generalizes well to unseen videos of unseen semantic categories at inference time, producing equivalent results to state-of-the-art methods while significantly reducing the runtime compared to other baselines."],"url":"http://arxiv.org/abs/2404.07097v1","category":"cs.CV"}
{"created":"2024-04-10 15:36:59","title":"TransTARec: Time-Adaptive Translating Embedding Model for Next POI Recommendation","abstract":"The rapid growth of location acquisition technologies makes Point-of-Interest(POI) recommendation possible due to redundant user check-in records. In this paper, we focus on next POI recommendation in which next POI is based on previous POI. We observe that time plays an important role in next POI recommendation but is neglected in the recent proposed translating embedding methods. To tackle this shortage, we propose a time-adaptive translating embedding model (TransTARec) for next POI recommendation that naturally incorporates temporal influence, sequential dynamics, and user preference within a single component. Methodologically, we treat a (previous timestamp, user, next timestamp) triplet as a union translation vector and develop a neural-based fusion operation to fuse user preference and temporal influence. The superiority of TransTARec, which is confirmed by extensive experiments on real-world datasets, comes from not only the introduction of temporal influence but also the direct unification with user preference and sequential dynamics.","sentences":["The rapid growth of location acquisition technologies makes Point-of-Interest(POI) recommendation possible due to redundant user check-in records.","In this paper, we focus on next POI recommendation in which next POI is based on previous POI.","We observe that time plays an important role in next POI recommendation but is neglected in the recent proposed translating embedding methods.","To tackle this shortage, we propose a time-adaptive translating embedding model (TransTARec) for next POI recommendation that naturally incorporates temporal influence, sequential dynamics, and user preference within a single component.","Methodologically, we treat a (previous timestamp, user, next timestamp) triplet as a union translation vector and develop a neural-based fusion operation to fuse user preference and temporal influence.","The superiority of TransTARec, which is confirmed by extensive experiments on real-world datasets, comes from not only the introduction of temporal influence but also the direct unification with user preference and sequential dynamics."],"url":"http://arxiv.org/abs/2404.07096v1","category":"cs.IR"}
{"created":"2024-04-10 15:35:27","title":"Particle Scattering and Fusion for the Ablowitz-Ladik Chain","abstract":"The Ablowitz-Ladik chain is an integrable discretized version of the nonlinear Schr\\\"{o}dinger equation. We report on a novel underlying Hamiltonian particle system with properties similar to the ones known for the classical Toda chain and Calogero fluid with $1/\\sinh^2$ pair interaction. Boundary conditions are imposed such that, both in the distant past and future, particles have a constant velocity. We establish the many-particle scattering for the Ablowitz-Ladik chain and obtain properties known for generic integrable many-body systems. For a specific choice of the chain, real initial data remain real in the course of time. Then, asymptotically, particles move in pairs with a velocity-dependent size and scattering shifts are governed by the fusion rule.","sentences":["The Ablowitz-Ladik chain is an integrable discretized version of the nonlinear Schr\\\"{o}dinger equation.","We report on a novel underlying Hamiltonian particle system with properties similar to the ones known for the classical Toda chain and Calogero fluid with $1/\\sinh^2$ pair interaction.","Boundary conditions are imposed such that, both in the distant past and future, particles have a constant velocity.","We establish the many-particle scattering for the Ablowitz-Ladik chain and obtain properties known for generic integrable many-body systems.","For a specific choice of the chain, real initial data remain real in the course of time.","Then, asymptotically, particles move in pairs with a velocity-dependent size and scattering shifts are governed by the fusion rule."],"url":"http://arxiv.org/abs/2404.07095v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-10 15:29:29","title":"LaTiM: Longitudinal representation learning in continuous-time models to predict disease progression","abstract":"This work proposes a novel framework for analyzing disease progression using time-aware neural ordinary differential equations (NODE). We introduce a \"time-aware head\" in a framework trained through self-supervised learning (SSL) to leverage temporal information in latent space for data augmentation. This approach effectively integrates NODEs with SSL, offering significant performance improvements compared to traditional methods that lack explicit temporal integration. We demonstrate the effectiveness of our strategy for diabetic retinopathy progression prediction using the OPHDIAT database. Compared to the baseline, all NODE architectures achieve statistically significant improvements in area under the ROC curve (AUC) and Kappa metrics, highlighting the efficacy of pre-training with SSL-inspired approaches. Additionally, our framework promotes stable training for NODEs, a commonly encountered challenge in time-aware modeling.","sentences":["This work proposes a novel framework for analyzing disease progression using time-aware neural ordinary differential equations (NODE).","We introduce a \"time-aware head\" in a framework trained through self-supervised learning (SSL) to leverage temporal information in latent space for data augmentation.","This approach effectively integrates NODEs with SSL, offering significant performance improvements compared to traditional methods that lack explicit temporal integration.","We demonstrate the effectiveness of our strategy for diabetic retinopathy progression prediction using the OPHDIAT database.","Compared to the baseline, all NODE architectures achieve statistically significant improvements in area under the ROC curve (AUC) and Kappa metrics, highlighting the efficacy of pre-training with SSL-inspired approaches.","Additionally, our framework promotes stable training for NODEs, a commonly encountered challenge in time-aware modeling."],"url":"http://arxiv.org/abs/2404.07091v1","category":"cs.LG"}
{"created":"2024-04-10 15:18:13","title":"An efficient tidal dissipation mechanism via stellar magnetic fields","abstract":"Recent work suggests that inwardly propagating internal gravity waves (IGWs) within a star can be fully converted to outward magnetic waves (MWs) if they encounter a sufficiently strong magnetic field. The resulting magnetic waves dissipate as they propagate outward to regions with lower Alfv\\'{e}n velocity. While tidal forcing is known to excite IGWs, this conversion and subsequent damping of magnetic waves has not been explored as a tidal dissipation mechanism. In particular, stars with sufficiently strong magnetic fields could fully dissipate tidally excited waves, yielding the same tidal evolution as the previously-studied ``travelling wave regime''. Here, we evaluate the viability of this mechanism using stellar models of stars with convective cores (F-type stars in the mass range of $1.2$-$1.6M_\\odot$) which were previously thought to be weakly tidally dissipative (due to the absence of nonlinear gravity wave breaking). The criterion for wave conversion to operate is evaluated for each stellar mass using the properties of each star's interior along with estimates of the magnetic field produced by a convective core dynamo under the assumption of equipartition between kinetic (convective) and magnetic energies. Our main result is that this previously unexplored source of efficient tidal dissipation can operate in stars within this mass range for significant fractions of their lifetimes. This tidal dissipation mechanism appears to be consistent with the observed inspiral of WASP-12b, and more generally could play an important role in the orbital evolution of hot Jupiters -- and to lower mass ultra-short period planets -- orbiting F-type stars.","sentences":["Recent work suggests that inwardly propagating internal gravity waves (IGWs) within a star can be fully converted to outward magnetic waves (MWs) if they encounter a sufficiently strong magnetic field.","The resulting magnetic waves dissipate as they propagate outward to regions with lower Alfv\\'{e}n velocity.","While tidal forcing is known to excite IGWs, this conversion and subsequent damping of magnetic waves has not been explored as a tidal dissipation mechanism.","In particular, stars with sufficiently strong magnetic fields could fully dissipate tidally excited waves, yielding the same tidal evolution as the previously-studied ``travelling wave regime''.","Here, we evaluate the viability of this mechanism using stellar models of stars with convective cores (F-type stars in the mass range of $1.2$-$1.6M_\\odot$) which were previously thought to be weakly tidally dissipative (due to the absence of nonlinear gravity wave breaking).","The criterion for wave conversion to operate is evaluated for each stellar mass using the properties of each star's interior along with estimates of the magnetic field produced by a convective core dynamo under the assumption of equipartition between kinetic (convective) and magnetic energies.","Our main result is that this previously unexplored source of efficient tidal dissipation can operate in stars within this mass range for significant fractions of their lifetimes.","This tidal dissipation mechanism appears to be consistent with the observed inspiral of WASP-12b, and more generally could play an important role in the orbital evolution of hot Jupiters -- and to lower mass ultra-short period planets -- orbiting F-type stars."],"url":"http://arxiv.org/abs/2404.07085v1","category":"astro-ph.SR"}
{"created":"2024-04-10 15:17:17","title":"Dynamic Generation of Personalities with Large Language Models","abstract":"In the realm of mimicking human deliberation, large language models (LLMs) show promising performance, thereby amplifying the importance of this research area. Deliberation is influenced by both logic and personality. However, previous studies predominantly focused on the logic of LLMs, neglecting the exploration of personality aspects. In this work, we introduce Dynamic Personality Generation (DPG), a dynamic personality generation method based on Hypernetworks. Initially, we embed the Big Five personality theory into GPT-4 to form a personality assessment machine, enabling it to evaluate characters' personality traits from dialogues automatically. We propose a new metric to assess personality generation capability based on this evaluation method. Then, we use this personality assessment machine to evaluate dialogues in script data, resulting in a personality-dialogue dataset. Finally, we fine-tune DPG on the personality-dialogue dataset. Experiments prove that DPG's personality generation capability is stronger after fine-tuning on this dataset than traditional fine-tuning methods, surpassing prompt-based GPT-4.","sentences":["In the realm of mimicking human deliberation, large language models (LLMs) show promising performance, thereby amplifying the importance of this research area.","Deliberation is influenced by both logic and personality.","However, previous studies predominantly focused on the logic of LLMs, neglecting the exploration of personality aspects.","In this work, we introduce Dynamic Personality Generation (DPG), a dynamic personality generation method based on Hypernetworks.","Initially, we embed the Big Five personality theory into GPT-4 to form a personality assessment machine, enabling it to evaluate characters' personality traits from dialogues automatically.","We propose a new metric to assess personality generation capability based on this evaluation method.","Then, we use this personality assessment machine to evaluate dialogues in script data, resulting in a personality-dialogue dataset.","Finally, we fine-tune DPG on the personality-dialogue dataset.","Experiments prove that DPG's personality generation capability is stronger after fine-tuning on this dataset than traditional fine-tuning methods, surpassing prompt-based GPT-4."],"url":"http://arxiv.org/abs/2404.07084v1","category":"cs.CL"}
{"created":"2024-04-10 15:16:04","title":"Minimizing Chebyshev Prototype Risk Magically Mitigates the Perils of Overfitting","abstract":"Overparameterized deep neural networks (DNNs), if not sufficiently regularized, are susceptible to overfitting their training examples and not generalizing well to test data. To discourage overfitting, researchers have developed multicomponent loss functions that reduce intra-class feature correlation and maximize inter-class feature distance in one or more layers of the network. By analyzing the penultimate feature layer activations output by a DNN's feature extraction section prior to the linear classifier, we find that modified forms of the intra-class feature covariance and inter-class prototype separation are key components of a fundamental Chebyshev upper bound on the probability of misclassification, which we designate the Chebyshev Prototype Risk (CPR). While previous approaches' covariance loss terms scale quadratically with the number of network features, our CPR bound indicates that an approximate covariance loss in log-linear time is sufficient to reduce the bound and is scalable to large architectures. We implement the terms of the CPR bound into our Explicit CPR (exCPR) loss function and observe from empirical results on multiple datasets and network architectures that our training algorithm reduces overfitting and improves upon previous approaches in many settings. Our code is available $\\href{https://github.com/Deano1718/Regularization_exCPR}{here}$.","sentences":["Overparameterized deep neural networks (DNNs), if not sufficiently regularized, are susceptible to overfitting their training examples and not generalizing well to test data.","To discourage overfitting, researchers have developed multicomponent loss functions that reduce intra-class feature correlation and maximize inter-class feature distance in one or more layers of the network.","By analyzing the penultimate feature layer activations output by a DNN's feature extraction section prior to the linear classifier, we find that modified forms of the intra-class feature covariance and inter-class prototype separation are key components of a fundamental Chebyshev upper bound on the probability of misclassification, which we designate the Chebyshev Prototype Risk (CPR).","While previous approaches' covariance loss terms scale quadratically with the number of network features, our CPR bound indicates that an approximate covariance loss in log-linear time is sufficient to reduce the bound and is scalable to large architectures.","We implement the terms of the CPR bound into our Explicit CPR (exCPR) loss function and observe from empirical results on multiple datasets and network architectures that our training algorithm reduces overfitting and improves upon previous approaches in many settings.","Our code is available $\\href{https://github.com/Deano1718/Regularization_exCPR}{here}$."],"url":"http://arxiv.org/abs/2404.07083v1","category":"cs.LG"}
{"created":"2024-04-10 15:11:59","title":"Hilbert space representation for quasi-Hermitian position-deformed Heisenberg algebra and Path integral formulation","abstract":"Position deformation of a Heisenberg algebra and Hilbert space representation of both maximal length and minimal momentum uncertainties may lead to loss of Hermiticity of some operators that generate this algebra. Consequently, the Hamiltonian operator constructed from these operators are also not Hermitian. In the present paper, with an appropriate positive-definite Dyson map, we establish the Hermiticity of these operators by means of a quasi-similarity transformation. We then construct Hilbert space representations associated with these quasi-Hermitian operators that generate a quasi-Hermitian Heisenberg algebra. With the help of these representations we establish the path integral formulation of any systems in this quasi-Hermitian algebra. Finally, using the path integral of a free particle as an example, we demonstrate that the Euclidean propagator, action, and kinetic energy of this system are constrained by the standard classical mechanics limits.","sentences":["Position deformation of a Heisenberg algebra and Hilbert space representation of both maximal length and minimal momentum uncertainties may lead to loss of Hermiticity of some operators that generate this algebra.","Consequently, the Hamiltonian operator constructed from these operators are also not Hermitian.","In the present paper, with an appropriate positive-definite Dyson map, we establish the Hermiticity of these operators by means of a quasi-similarity transformation.","We then construct Hilbert space representations associated with these quasi-Hermitian operators that generate a quasi-Hermitian Heisenberg algebra.","With the help of these representations we establish the path integral formulation of any systems in this quasi-Hermitian algebra.","Finally, using the path integral of a free particle as an example, we demonstrate that the Euclidean propagator, action, and kinetic energy of this system are constrained by the standard classical mechanics limits."],"url":"http://arxiv.org/abs/2404.07082v1","category":"math-ph"}
{"created":"2024-04-10 15:10:53","title":"Neutrino at different epochs of the Friedmann Universe","abstract":"Nowadays, at least two relics of the Big Bang have survived - the cosmological microwave background (CMB) and the cosmological neutrino background (C$\\nu$B). Being the second most abundant particle in the Universe, the neutrino has a significant impact on its evolution from the Big Bang to the present day. Neutrinos affect the following cosmological processes: the expansion rate of the Universe, its chemical and isotopic composition, the CMB anisotropy and the formation of the large-scale structure of the Universe. Another relic neutrino background is theoretically predicted, it consists of non-equilibrium antineutrinos of Primordial Nucleosynthesis arising as a result of the decays of neutrons and tritium nuclei. Such antineutrinos are an indicator of the baryon asymmetry of the Universe. In addition to experimentally detectable active neutrinos, the existence of sterile neutrinos is theoretically predicted to generate neutrino masses and explain their oscillations. Sterile neutrinos can also solve such cosmological problems as the baryonic asymmetry of the Universe and the nature of dark matter. The recent results of several independent experiments point to the possibility of the existence of a light sterile neutrino. However, the existence of such a neutrino is inconsistent with the predictions of the Standard Cosmological Model. The inclusion of a non-zero lepton asymmetry of the Universe and/or increasing the energy density of active neutrinos can eliminate these contradictions and reconcile the possible existence of sterile neutrinos with Primordial Nucleosynthesis, the CMB anisotropy, and also reduce the H$_0$-tension. In this brief review, we discuss the influence of the physical properties of active and sterile neutrinos on the evolution of the Universe from the Big Bang to the present day.","sentences":["Nowadays, at least two relics of the Big Bang have survived - the cosmological microwave background (CMB) and the cosmological neutrino background (C$\\nu$B).","Being the second most abundant particle in the Universe, the neutrino has a significant impact on its evolution from the Big Bang to the present day.","Neutrinos affect the following cosmological processes: the expansion rate of the Universe, its chemical and isotopic composition, the CMB anisotropy and the formation of the large-scale structure of the Universe.","Another relic neutrino background is theoretically predicted, it consists of non-equilibrium antineutrinos of Primordial Nucleosynthesis arising as a result of the decays of neutrons and tritium nuclei.","Such antineutrinos are an indicator of the baryon asymmetry of the Universe.","In addition to experimentally detectable active neutrinos, the existence of sterile neutrinos is theoretically predicted to generate neutrino masses and explain their oscillations.","Sterile neutrinos can also solve such cosmological problems as the baryonic asymmetry of the Universe and the nature of dark matter.","The recent results of several independent experiments point to the possibility of the existence of a light sterile neutrino.","However, the existence of such a neutrino is inconsistent with the predictions of the Standard Cosmological Model.","The inclusion of a non-zero lepton asymmetry of the Universe and/or increasing the energy density of active neutrinos can eliminate these contradictions and reconcile the possible existence of sterile neutrinos with Primordial Nucleosynthesis, the CMB anisotropy, and also reduce the H$_0$-tension.","In this brief review, we discuss the influence of the physical properties of active and sterile neutrinos on the evolution of the Universe from the Big Bang to the present day."],"url":"http://arxiv.org/abs/2404.07081v1","category":"astro-ph.CO"}
{"created":"2024-04-10 15:09:15","title":"VLLMs Provide Better Context for Emotion Understanding Through Common Sense Reasoning","abstract":"Recognising emotions in context involves identifying the apparent emotions of an individual, taking into account contextual cues from the surrounding scene. Previous approaches to this task have involved the design of explicit scene-encoding architectures or the incorporation of external scene-related information, such as captions. However, these methods often utilise limited contextual information or rely on intricate training pipelines. In this work, we leverage the groundbreaking capabilities of Vision-and-Large-Language Models (VLLMs) to enhance in-context emotion classification without introducing complexity to the training process in a two-stage approach. In the first stage, we propose prompting VLLMs to generate descriptions in natural language of the subject's apparent emotion relative to the visual context. In the second stage, the descriptions are used as contextual information and, along with the image input, are used to train a transformer-based architecture that fuses text and visual features before the final classification task. Our experimental results show that the text and image features have complementary information, and our fused architecture significantly outperforms the individual modalities without any complex training methods. We evaluate our approach on three different datasets, namely, EMOTIC, CAER-S, and BoLD, and achieve state-of-the-art or comparable accuracy across all datasets and metrics compared to much more complex approaches. The code will be made publicly available on github: https://github.com/NickyFot/EmoCommonSense.git","sentences":["Recognising emotions in context involves identifying the apparent emotions of an individual, taking into account contextual cues from the surrounding scene.","Previous approaches to this task have involved the design of explicit scene-encoding architectures or the incorporation of external scene-related information, such as captions.","However, these methods often utilise limited contextual information or rely on intricate training pipelines.","In this work, we leverage the groundbreaking capabilities of Vision-and-Large-Language Models (VLLMs) to enhance in-context emotion classification without introducing complexity to the training process in a two-stage approach.","In the first stage, we propose prompting VLLMs to generate descriptions in natural language of the subject's apparent emotion relative to the visual context.","In the second stage, the descriptions are used as contextual information and, along with the image input, are used to train a transformer-based architecture that fuses text and visual features before the final classification task.","Our experimental results show that the text and image features have complementary information, and our fused architecture significantly outperforms the individual modalities without any complex training methods.","We evaluate our approach on three different datasets, namely, EMOTIC, CAER-S, and BoLD, and achieve state-of-the-art or comparable accuracy across all datasets and metrics compared to much more complex approaches.","The code will be made publicly available on github: https://github.com/NickyFot/EmoCommonSense.git"],"url":"http://arxiv.org/abs/2404.07078v1","category":"cs.CV"}
{"created":"2024-04-10 15:07:26","title":"Parametric topological entropy on orbits of arbitrary multivalued maps in compact Hausdorff spaces","abstract":"The Adler-Konheim-McAndrew type definitions and the Bowen-Dinaburg-Hood type definitions of parametric topological entropy will be considered on orbits and coincidence orbits of nonautonomous multivalued maps in compact Hausdorff spaces. Their mutual relationship and their link to various further types of definitions like those of (parametric) preimage entropy, will be investigated. In this way, several recent results of the other authors will be generalized and extended into a new setting.","sentences":["The Adler-Konheim-McAndrew type definitions and the Bowen-Dinaburg-Hood type definitions of parametric topological entropy will be considered on orbits and coincidence orbits of nonautonomous multivalued maps in compact Hausdorff spaces.","Their mutual relationship and their link to various further types of definitions like those of (parametric) preimage entropy, will be investigated.","In this way, several recent results of the other authors will be generalized and extended into a new setting."],"url":"http://arxiv.org/abs/2404.07076v1","category":"math.DS"}
{"created":"2024-04-10 15:05:44","title":"Constraints on Null Energy Condition Violation from Advanced LIGO and Advanced Virgo's First Three Observing Runs","abstract":"The null energy condition (NEC) is a cornerstone of general relativity, and its violation could leave observable imprints in the cosmic gravitational wave spectrum. Theoretical models suggest that NEC violations during inflation can amplify the primordial tensor power spectrum, leading to distinct features in the stochastic gravitational wave background (SGWB). In this work, we search for these NEC-violating signatures in the SGWB using data from Advanced LIGO and Advanced Virgo's first three observing runs. Our analysis reveals no statistically significant evidence of such signals, allowing us to place stringent upper limits on the tensor power spectrum amplitude, $P_{T,2}$, during the second inflationary stage. Specifically, we find that $P_{T,2} \\lesssim 0.15$ at a $95\\%$ confidence level. Notably, this upper limit is consistent with constraints derived from pulsar timing array observations, reinforcing the hypothesis that NEC violations during inflation could explain the signal detected by pulsar timing arrays. Our findings contribute to a deeper understanding of the early Universe and highlight the potential of current and future gravitational wave experiments in probing the physics of inflation and NEC violations.","sentences":["The null energy condition (NEC) is a cornerstone of general relativity, and its violation could leave observable imprints in the cosmic gravitational wave spectrum.","Theoretical models suggest that NEC violations during inflation can amplify the primordial tensor power spectrum, leading to distinct features in the stochastic gravitational wave background (SGWB).","In this work, we search for these NEC-violating signatures in the SGWB using data from Advanced LIGO and Advanced Virgo's first three observing runs.","Our analysis reveals no statistically significant evidence of such signals, allowing us to place stringent upper limits on the tensor power spectrum amplitude, $P_{T,2}$, during the second inflationary stage.","Specifically, we find that $P_{T,2} \\lesssim 0.15$ at a $95\\%$ confidence level.","Notably, this upper limit is consistent with constraints derived from pulsar timing array observations, reinforcing the hypothesis that NEC violations during inflation could explain the signal detected by pulsar timing arrays.","Our findings contribute to a deeper understanding of the early Universe and highlight the potential of current and future gravitational wave experiments in probing the physics of inflation and NEC violations."],"url":"http://arxiv.org/abs/2404.07075v1","category":"gr-qc"}
{"created":"2024-04-10 15:03:30","title":"Data-driven quasiconformal morphodynamic flows","abstract":"Temporal imaging of biological epithelial structures yields shape data at discrete time points, leading to a natural question: how can we reconstruct the most likely path of growth patterns consistent with these discrete observations? We present a physically plausible framework to solve this inverse problem by creating a framework that generalizes quasiconformal maps to quasiconformal flows. By allowing for the spatio-temporal variation of the shear and dilatation fields during the growth process, subject to regulatory mechanisms, we are led to a type of generalized Ricci flow. When guided by observational data associated with surface shape as a function of time, this leads to a constrained optimization problem. Deploying our data-driven algorithmic approach to the shape of insect wings, leaves and even sculpted faces, we show how optimal quasiconformal flows allow us to characterize the morphogenesis of a range of surfaces.","sentences":["Temporal imaging of biological epithelial structures yields shape data at discrete time points, leading to a natural question: how can we reconstruct the most likely path of growth patterns consistent with these discrete observations?","We present a physically plausible framework to solve this inverse problem by creating a framework that generalizes quasiconformal maps to quasiconformal flows.","By allowing for the spatio-temporal variation of the shear and dilatation fields during the growth process, subject to regulatory mechanisms, we are led to a type of generalized Ricci flow.","When guided by observational data associated with surface shape as a function of time, this leads to a constrained optimization problem.","Deploying our data-driven algorithmic approach to the shape of insect wings, leaves and even sculpted faces, we show how optimal quasiconformal flows allow us to characterize the morphogenesis of a range of surfaces."],"url":"http://arxiv.org/abs/2404.07073v1","category":"cs.CG"}
{"created":"2024-04-10 15:02:26","title":"Implicit Multi-Spectral Transformer: An Lightweight and Effective Visible to Infrared Image Translation Model","abstract":"In the field of computer vision, visible light images often exhibit low contrast in low-light conditions, presenting a significant challenge. While infrared imagery provides a potential solution, its utilization entails high costs and practical limitations. Recent advancements in deep learning, particularly the deployment of Generative Adversarial Networks (GANs), have facilitated the transformation of visible light images to infrared images. However, these methods often experience unstable training phases and may produce suboptimal outputs. To address these issues, we propose a novel end-to-end Transformer-based model that efficiently converts visible light images into high-fidelity infrared images. Initially, the Texture Mapping Module and Color Perception Adapter collaborate to extract texture and color features from the visible light image. The Dynamic Fusion Aggregation Module subsequently integrates these features. Finally, the transformation into an infrared image is refined through the synergistic action of the Color Perception Adapter and the Enhanced Perception Attention mechanism. Comprehensive benchmarking experiments confirm that our model outperforms existing methods, producing infrared images of markedly superior quality, both qualitatively and quantitatively. Furthermore, the proposed model enables more effective downstream applications for infrared images than other methods.","sentences":["In the field of computer vision, visible light images often exhibit low contrast in low-light conditions, presenting a significant challenge.","While infrared imagery provides a potential solution, its utilization entails high costs and practical limitations.","Recent advancements in deep learning, particularly the deployment of Generative Adversarial Networks (GANs), have facilitated the transformation of visible light images to infrared images.","However, these methods often experience unstable training phases and may produce suboptimal outputs.","To address these issues, we propose a novel end-to-end Transformer-based model that efficiently converts visible light images into high-fidelity infrared images.","Initially, the Texture Mapping Module and Color Perception Adapter collaborate to extract texture and color features from the visible light image.","The Dynamic Fusion Aggregation Module subsequently integrates these features.","Finally, the transformation into an infrared image is refined through the synergistic action of the Color Perception Adapter and the Enhanced Perception Attention mechanism.","Comprehensive benchmarking experiments confirm that our model outperforms existing methods, producing infrared images of markedly superior quality, both qualitatively and quantitatively.","Furthermore, the proposed model enables more effective downstream applications for infrared images than other methods."],"url":"http://arxiv.org/abs/2404.07072v1","category":"cs.CV"}
{"created":"2024-04-10 15:01:17","title":"Model independent cosmographic constraints from DESI 2024","abstract":"In this study, we explore model independent constraints on the universe kinematics up to the snap and jerk hierarchical terms. To do so, we consider the latest Baryon Acoustic Oscillation (BAO) release provided by the DESI collaboration, tackling the $r_d$ parameter to span within the range $[144,152]$ Mpc, with fixed step, $\\delta r_d=2$ Mpc, aligning with Planck and DESI results. Thus, employing Monte Carlo Markov chain analyses, we place stringent constraints on the cosmographic series, incorporating three combinations of data catalogs: the first BAO with observational Hubble data, the second BAO with type Ia supernovae, and the last including all three data sets. Our results conclusively constrain the cosmographic series, say the deceleration $q_0$, the jerk $j_0$, and the snap $s_0$ parameters, at the $2$--$\\sigma$ level, showcasing a significant departure on $j_0$ even at $1$--$\\sigma$ confidence level, albeit being compatible with the $\\Lambda$CDM paradigm on $q_0$ and $s_0$, at $2$--$\\sigma$ level. Analogously, the $h_0$ tension appears alleviated in the second hierarchy, say including snap. Finally, a direct comparison with the $\\Lambda$CDM, $w$CDM models and the Chevallier-Polarski-Linder parametrization is reported, definitively favoring the wCDM scenario.","sentences":["In this study, we explore model independent constraints on the universe kinematics up to the snap and jerk hierarchical terms.","To do so, we consider the latest Baryon Acoustic Oscillation (BAO) release provided by the DESI collaboration, tackling the $r_d$ parameter to span within the range $[144,152]$ Mpc, with fixed step, $\\delta r_d=2$ Mpc, aligning with Planck and DESI results.","Thus, employing Monte Carlo Markov chain analyses, we place stringent constraints on the cosmographic series, incorporating three combinations of data catalogs: the first BAO with observational Hubble data, the second BAO with type Ia supernovae, and the last including all three data sets.","Our results conclusively constrain the cosmographic series, say the deceleration $q_0$, the jerk $j_0$, and the snap $s_0$ parameters, at the $2$--$\\sigma$ level, showcasing a significant departure on $j_0$ even at $1$--$\\sigma$ confidence level, albeit being compatible with the $\\Lambda$CDM paradigm on $q_0$ and $s_0$, at $2$--$\\sigma$ level.","Analogously, the $h_0$ tension appears alleviated in the second hierarchy, say including snap.","Finally, a direct comparison with the $\\Lambda$CDM, $w$CDM models and the Chevallier-Polarski-Linder parametrization is reported, definitively favoring the wCDM scenario."],"url":"http://arxiv.org/abs/2404.07070v1","category":"astro-ph.CO"}
{"created":"2024-04-10 15:00:28","title":"Entanglement entropy in the ground state of non-interacting massless Dirac fermions in dimension one","abstract":"We present a novel proof of a formula of Casini and Huerta for the entanglement entropy of the ground state of non-interacting massless Dirac fermions in dimension one localized to (a union of) intervals and generalize it to the case of R\\'enyi entropies. At first, we prove that these entropies are well-defined for non-intersecting intervals. This is accomplished by an inequality of Alexander V.~Sobolev. Then we compute this entropy using a trace formula for Wiener--Hopf operators by Harold Widom. For intersecting intervals, we discuss an extended entropy formula of Casini and Huerta and support this with a proof for polynomial test functions (instead of entropy).","sentences":["We present a novel proof of a formula of Casini and Huerta for the entanglement entropy of the ground state of non-interacting massless Dirac fermions in dimension one localized to (a union of) intervals and generalize it to the case of R\\'enyi entropies.","At first, we prove that these entropies are well-defined for non-intersecting intervals.","This is accomplished by an inequality of Alexander V.~Sobolev.","Then we compute this entropy using a trace formula for Wiener--Hopf operators by Harold Widom.","For intersecting intervals, we discuss an extended entropy formula of Casini and Huerta and support this with a proof for polynomial test functions (instead of entropy)."],"url":"http://arxiv.org/abs/2404.07068v1","category":"math-ph"}
{"created":"2024-04-10 14:57:46","title":"One-dimensional, geometrically stratified semi-empirical models of the quiet-Sun photosphere and lower chromosphere","abstract":"One-dimensional, semi-empirical models of the solar atmosphere are widely employed in numerous contexts within solar physics, ranging from the determination of element abundances and atomic parameters to studies of the solar irradiance and from Stokes inversions to coronal extrapolations. These models provide the physical parameters (i.e. temperature, gas pressure, etc.) in the solar atmosphere as a function of the continuum optical depth $\\tau_{\\rm c}$. The transformation to the geometrical $z$ scale (i.e. vertical coordinate) is provided via vertical hydrostatic equilibrium. Our aim is to provide updated, one-dimensional, semi-empirical models of the solar atmosphere as a function of $z,$ but employing the more general case of three-dimensional magneto-hydrostatic equilibrium (MHS) instead of vertical hydrostatic equilibrium (HE). We employed a recently developed Stokes inversion code that, along with non-local thermodynamic equilibrium effects, considers MHS instead of HE. This code is applied to spatially and temporally resolved spectropolarimetric observations of the quiet Sun obtained with the CRISP instrument attached to the Swedish Solar Telescope. We provide average models for granules, intergranules, dark magnetic elements, and overall quiet-Sun as a function of both $\\tau_{\\rm c}$ and $z$ from the photosphere to the lower chromosphere. We demonstrate that, in these quiet-Sun models, the effect of considering MHS instead of HE is negligible. However, employing MHS increases the consistency of the inversion results before averaging. We surmise that in regions with stronger magnetic fields (i.e. pores, sunspots, network) the benefits of employing the magneto-hydrostatic approximation will be much more palpable.","sentences":["One-dimensional, semi-empirical models of the solar atmosphere are widely employed in numerous contexts within solar physics, ranging from the determination of element abundances and atomic parameters to studies of the solar irradiance and from Stokes inversions to coronal extrapolations.","These models provide the physical parameters (i.e. temperature, gas pressure, etc.) in the solar atmosphere as a function of the continuum optical depth","$\\tau_{\\rm c}$.","The transformation to the geometrical $z$ scale (i.e. vertical coordinate) is provided via vertical hydrostatic equilibrium.","Our aim is to provide updated, one-dimensional, semi-empirical models of the solar atmosphere as a function of $z,$ but employing the more general case of three-dimensional magneto-hydrostatic equilibrium (MHS) instead of vertical hydrostatic equilibrium (HE).","We employed a recently developed Stokes inversion code that, along with non-local thermodynamic equilibrium effects, considers MHS instead of HE.","This code is applied to spatially and temporally resolved spectropolarimetric observations of the quiet Sun obtained with the CRISP instrument attached to the Swedish Solar Telescope.","We provide average models for granules, intergranules, dark magnetic elements, and overall quiet-Sun as a function of both $\\tau_{\\rm c}$ and $z$ from the photosphere to the lower chromosphere.","We demonstrate that, in these quiet-Sun models, the effect of considering MHS instead of HE is negligible.","However, employing MHS increases the consistency of the inversion results before averaging.","We surmise that in regions with stronger magnetic fields (i.e. pores, sunspots, network) the benefits of employing the magneto-hydrostatic approximation will be much more palpable."],"url":"http://arxiv.org/abs/2404.07067v1","category":"astro-ph.SR"}
{"created":"2024-04-10 14:56:40","title":"Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?","abstract":"This paper studies the phenomenon that different concepts are learned in different layers of large language models, i.e. more difficult concepts are fully acquired with deeper layers. We define the difficulty of concepts by the level of abstraction, and here it is crudely categorized by factual, emotional, and inferential. Each category contains a spectrum of tasks, arranged from simple to complex. For example, within the factual dimension, tasks range from lie detection to categorizing mathematical problems. We employ a probing technique to extract representations from different layers of the model and apply these to classification tasks. Our findings reveal that models tend to efficiently classify simpler tasks, indicating that these concepts are learned in shallower layers. Conversely, more complex tasks may only be discernible at deeper layers, if at all. This paper explores the implications of these findings for our understanding of model learning processes and internal representations. Our implementation is available at \\url{https://github.com/Luckfort/CD}.","sentences":["This paper studies the phenomenon that different concepts are learned in different layers of large language models, i.e. more difficult concepts are fully acquired with deeper layers.","We define the difficulty of concepts by the level of abstraction, and here it is crudely categorized by factual, emotional, and inferential.","Each category contains a spectrum of tasks, arranged from simple to complex.","For example, within the factual dimension, tasks range from lie detection to categorizing mathematical problems.","We employ a probing technique to extract representations from different layers of the model and apply these to classification tasks.","Our findings reveal that models tend to efficiently classify simpler tasks, indicating that these concepts are learned in shallower layers.","Conversely, more complex tasks may only be discernible at deeper layers, if at all.","This paper explores the implications of these findings for our understanding of model learning processes and internal representations.","Our implementation is available at \\url{https://github.com/Luckfort/CD}."],"url":"http://arxiv.org/abs/2404.07066v1","category":"cs.CL"}
{"created":"2024-04-10 14:52:35","title":"LaPlaSS: Latent Space Planning for Stochastic Systems","abstract":"Autonomous mobile agents often operate in hazardous environments, necessitating an awareness of safety. These agents can have non-linear, stochastic dynamics that must be considered during planning to guarantee bounded risk. Most state of the art methods require closed-form dynamics to verify plan correctness and safety however modern robotic systems often have dynamics that are learned from data. Thus, there is a need to perform efficient trajectory planning with guarantees on risk for agents without known dynamics models. We propose a \"generate-and-test\" approach to risk-bounded planning in which a planner generates a candidate trajectory using an approximate linear dynamics model and a validator assesses the risk of the trajectory, computing additional safety constraints for the planner if the candidate does not satisfy the desired risk bound. To acquire the approximate model, we use a variational autoencoder to learn a latent linear dynamics model and encode the planning problem into the latent space to generate the candidate trajectory. The VAE also serves to sample trajectories around the candidate to use in the validator. We demonstrate that our algorithm, LaPlaSS, is able to generate trajectory plans with bounded risk for a real-world agent with learned dynamics and is an order of magnitude more efficient than the state of the art.","sentences":["Autonomous mobile agents often operate in hazardous environments, necessitating an awareness of safety.","These agents can have non-linear, stochastic dynamics that must be considered during planning to guarantee bounded risk.","Most state of the art methods require closed-form dynamics to verify plan correctness and safety however modern robotic systems often have dynamics that are learned from data.","Thus, there is a need to perform efficient trajectory planning with guarantees on risk for agents without known dynamics models.","We propose a \"generate-and-test\" approach to risk-bounded planning in which a planner generates a candidate trajectory using an approximate linear dynamics model and a validator assesses the risk of the trajectory, computing additional safety constraints for the planner if the candidate does not satisfy the desired risk bound.","To acquire the approximate model, we use a variational autoencoder to learn a latent linear dynamics model and encode the planning problem into the latent space to generate the candidate trajectory.","The VAE also serves to sample trajectories around the candidate to use in the validator.","We demonstrate that our algorithm, LaPlaSS, is able to generate trajectory plans with bounded risk for a real-world agent with learned dynamics and is an order of magnitude more efficient than the state of the art."],"url":"http://arxiv.org/abs/2404.07063v1","category":"cs.RO"}
{"created":"2024-04-10 14:50:10","title":"Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study","abstract":"We present an empirical study of groundedness in long-form question answering (LFQA) by retrieval-augmented large language models (LLMs). In particular, we evaluate whether every generated sentence is grounded in the retrieved documents or the model's pre-training data. Across 3 datasets and 4 model families, our findings reveal that a significant fraction of generated sentences are consistently ungrounded, even when those sentences contain correct ground-truth answers. Additionally, we examine the impacts of factors such as model size, decoding strategy, and instruction tuning on groundedness. Our results show that while larger models tend to ground their outputs more effectively, a significant portion of correct answers remains compromised by hallucinations. This study provides novel insights into the groundedness challenges in LFQA and underscores the necessity for more robust mechanisms in LLMs to mitigate the generation of ungrounded content.","sentences":["We present an empirical study of groundedness in long-form question answering (LFQA) by retrieval-augmented large language models (LLMs).","In particular, we evaluate whether every generated sentence is grounded in the retrieved documents or the model's pre-training data.","Across 3 datasets and 4 model families, our findings reveal that a significant fraction of generated sentences are consistently ungrounded, even when those sentences contain correct ground-truth answers.","Additionally, we examine the impacts of factors such as model size, decoding strategy, and instruction tuning on groundedness.","Our results show that while larger models tend to ground their outputs more effectively, a significant portion of correct answers remains compromised by hallucinations.","This study provides novel insights into the groundedness challenges in LFQA and underscores the necessity for more robust mechanisms in LLMs to mitigate the generation of ungrounded content."],"url":"http://arxiv.org/abs/2404.07060v1","category":"cs.CL"}
{"created":"2024-04-10 14:46:40","title":"Inner-extremal regular black holes from pure gravity","abstract":"Recently it was shown that essentially all regular black hole models constructed so far can be obtained as solutions of vacuum gravity equations, upon considering an infinite series of quasi-topological higher curvature corrections. Here we show that such a construction can be upgraded to yield regular black holes with vanishing inner horizon surface gravity. In four dimensions, such a condition is necessary for the absence of classical instabilities associated with mass inflation on the inner horizon.","sentences":["Recently it was shown that essentially all regular black hole models constructed so far can be obtained as solutions of vacuum gravity equations, upon considering an infinite series of quasi-topological higher curvature corrections.","Here we show that such a construction can be upgraded to yield regular black holes with vanishing inner horizon surface gravity.","In four dimensions, such a condition is necessary for the absence of classical instabilities associated with mass inflation on the inner horizon."],"url":"http://arxiv.org/abs/2404.07058v1","category":"gr-qc"}
{"created":"2024-04-10 14:46:14","title":"Generalized Straight-Line Programs","abstract":"It was recently proved that any Straight-Line Program (SLP) generating a given string can be transformed in linear time into an equivalent balanced SLP of the same asymptotic size. We generalize this proof to a general class of grammars we call Generalized SLPs (GSLPs), which allow rules of the form $A \\rightarrow x$ where $x$ is any Turing-complete representation (of size $|x|$) of a sequence of symbols (potentially much longer than $|x|$). We then specialize GSLPs to so-called Iterated SLPs (ISLPs), which allow rules of the form $A \\rightarrow \\Pi_{i=k_1}^{k_2} B_1^{i^{c_1}}\\cdots B_t^{i^{c_t}}$ of size $2t+2$. We prove that ISLPs break, for some text families, the measure $\\delta$ based on substring complexity, a lower bound for most measures and compressors exploiting repetitiveness. Further, ISLPs can extract any substring of length $\\lambda$, from the represented text $T[1.. n]$, in time $O(\\lambda + \\log^2 n\\log\\log n)$. This is the first compressed representation for repetitive texts breaking $\\delta$ while, at the same time, supporting direct access to arbitrary text symbols in polylogarithmic time. We also show how to compute some substring queries, like range minima and next/previous smaller value, in time $O(\\log^2 n \\log\\log n)$. Finally, we further specialize the grammars to Run-Length SLPs (RLSLPs), which restrict the rules allowed by ISLPs to the form $A \\rightarrow B^t$. Apart from inheriting all the previous results with the term $\\log^2 n \\log\\log n$ reduced to the near-optimal $\\log n$, we show that RLSLPs can exploit balance to efficiently compute a wide class of substring queries we call ``composable'' -- i.e., $f(X \\cdot Y)$ can be obtained from $f(X)$ and $f(Y)$...","sentences":["It was recently proved that any Straight-Line Program (SLP) generating a given string can be transformed in linear time into an equivalent balanced SLP of the same asymptotic size.","We generalize this proof to a general class of grammars we call Generalized SLPs (GSLPs), which allow rules of the form $A \\rightarrow x$ where $x$ is any Turing-complete representation (of size $|x|$) of a sequence of symbols (potentially much longer than $|x|$).","We then specialize GSLPs to so-called Iterated SLPs (ISLPs), which allow rules of the form $A \\rightarrow \\Pi_{i=k_1}^{k_2} B_1^{i^{c_1}}\\cdots B_t^{i^{c_t}}$ of size $2t+2$. We prove that ISLPs break, for some text families, the measure $\\delta$ based on substring complexity, a lower bound for most measures and compressors exploiting repetitiveness.","Further, ISLPs can extract any substring of length $\\lambda$, from the represented text $T[1.. n]$, in time $O(\\lambda + \\log^2 n\\log\\log","n)$. This is the first compressed representation for repetitive texts breaking $\\delta$ while, at the same time, supporting direct access to arbitrary text symbols in polylogarithmic time.","We also show how to compute some substring queries, like range minima and next/previous smaller value, in time $O(\\log^2 n \\log\\log n)$.","Finally, we further specialize the grammars to Run-Length SLPs (RLSLPs), which restrict the rules allowed by ISLPs to the form $A \\rightarrow B^t$. Apart from inheriting all the previous results with the term $\\log^2 n \\log\\log n$ reduced to the near-optimal $\\log n$, we show that RLSLPs can exploit balance to efficiently compute a wide class of substring queries we call ``composable'' -- i.e., $f(X \\cdot Y)$ can be obtained from $f(X)$ and $f(Y)$..."],"url":"http://arxiv.org/abs/2404.07057v1","category":"cs.DS"}
{"created":"2024-04-10 14:45:56","title":"Quantum Isotropic Universe in RQM Analogy: the Cosmological Horizon","abstract":"We investigate the quantum dynamics of the isotropic Universe in the presence of a free massless scalar field, playing the role of a physical clock. The Hilbert space is constructed via a direct analogy between the Wheeler-DeWitt equation in the minisuperspace and a relativistic scalar one in physical space. In particular, we show how the introduction of a \"turning point\" in the Universe evolution allows to overcome an intrinsic ambiguity in representing the expanding and collapsing Universe. In this way, the positive and negative frequencies are simply identified with time reversed states. The main subject of the present analysis is the construction of a horizon operator, whose quantum behavior is investigated when Polymer Quantum Mechanics is implemented to describe the asymptotic evolution near the initial singularity. The reason of this choice is motivated by the intrinsic spreading of localized wavepackets when the polymer dispersion relation governs the quantum dynamics. The evidence that the mean value of the quantum horizon operator follows its semiclassical behavior (corrected for polymerization) is a clear indication that a concept of causality can be restored also in the quantum cosmological picture.","sentences":["We investigate the quantum dynamics of the isotropic Universe in the presence of a free massless scalar field, playing the role of a physical clock.","The Hilbert space is constructed via a direct analogy between the Wheeler-DeWitt equation in the minisuperspace and a relativistic scalar one in physical space.","In particular, we show how the introduction of a \"turning point\" in the Universe evolution allows to overcome an intrinsic ambiguity in representing the expanding and collapsing Universe.","In this way, the positive and negative frequencies are simply identified with time reversed states.","The main subject of the present analysis is the construction of a horizon operator, whose quantum behavior is investigated when Polymer Quantum Mechanics is implemented to describe the asymptotic evolution near the initial singularity.","The reason of this choice is motivated by the intrinsic spreading of localized wavepackets when the polymer dispersion relation governs the quantum dynamics.","The evidence that the mean value of the quantum horizon operator follows its semiclassical behavior (corrected for polymerization) is a clear indication that a concept of causality can be restored also in the quantum cosmological picture."],"url":"http://arxiv.org/abs/2404.07056v1","category":"gr-qc"}
{"created":"2024-04-10 14:45:21","title":"Observational features of reflection asymmetric black holes","abstract":"The Kerr spacetime is symmetric with respect to a well-defined equatorial plane. When testing the equatorial reflection symmetry of an isolated black hole, one is at the same time testing the Kerr hypothesis in General Relativity. In this work, we investigate the possible observational features when a Keplerian disk is surrounding a rotating black hole without reflection symmetry. When such symmetry is broken, generically, the photon trajectories around the black hole and the Keplerian orbits on the accretion disk are distorted vertically away from the equatorial plane by an amount that depends on their distance to the black hole. In the reflection asymmetric spacetime we are considering, these two kinds of orbits are distorted in opposite directions. Interestingly, while the size and shape of black hole shadows closely resemble those of Kerr black holes, distinct observational characteristics can emerge in the disk image and emission line profiles. When observing the disk edge-on, a pronounced concave shape may appear along its innermost edge on the incoming side. Furthermore, distinctive horn-like features might be observed on the spectral line profile at the blue-shifted side. These special features can serve as compelling indicators of the reflection asymmetry present in rotating black holes.","sentences":["The Kerr spacetime is symmetric with respect to a well-defined equatorial plane.","When testing the equatorial reflection symmetry of an isolated black hole, one is at the same time testing the Kerr hypothesis in General Relativity.","In this work, we investigate the possible observational features when a Keplerian disk is surrounding a rotating black hole without reflection symmetry.","When such symmetry is broken, generically, the photon trajectories around the black hole and the Keplerian orbits on the accretion disk are distorted vertically away from the equatorial plane by an amount that depends on their distance to the black hole.","In the reflection asymmetric spacetime we are considering, these two kinds of orbits are distorted in opposite directions.","Interestingly, while the size and shape of black hole shadows closely resemble those of Kerr black holes, distinct observational characteristics can emerge in the disk image and emission line profiles.","When observing the disk edge-on, a pronounced concave shape may appear along its innermost edge on the incoming side.","Furthermore, distinctive horn-like features might be observed on the spectral line profile at the blue-shifted side.","These special features can serve as compelling indicators of the reflection asymmetry present in rotating black holes."],"url":"http://arxiv.org/abs/2404.07055v1","category":"gr-qc"}
{"created":"2024-04-10 14:44:48","title":"Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation","abstract":"Metaphors, although occasionally unperceived, are ubiquitous in our everyday language. Thus, it is crucial for Language Models to be able to grasp the underlying meaning of this kind of figurative language. In this work, we present Meta4XNLI, a novel parallel dataset for the tasks of metaphor detection and interpretation that contains metaphor annotations in both Spanish and English. We investigate language models' metaphor identification and understanding abilities through a series of monolingual and cross-lingual experiments by leveraging our proposed corpus. In order to comprehend how these non-literal expressions affect models' performance, we look over the results and perform an error analysis. Additionally, parallel data offers many potential opportunities to investigate metaphor transferability between these languages and the impact of translation on the development of multilingual annotated resources.","sentences":["Metaphors, although occasionally unperceived, are ubiquitous in our everyday language.","Thus, it is crucial for Language Models to be able to grasp the underlying meaning of this kind of figurative language.","In this work, we present Meta4XNLI, a novel parallel dataset for the tasks of metaphor detection and interpretation that contains metaphor annotations in both Spanish and English.","We investigate language models' metaphor identification and understanding abilities through a series of monolingual and cross-lingual experiments by leveraging our proposed corpus.","In order to comprehend how these non-literal expressions affect models' performance, we look over the results and perform an error analysis.","Additionally, parallel data offers many potential opportunities to investigate metaphor transferability between these languages and the impact of translation on the development of multilingual annotated resources."],"url":"http://arxiv.org/abs/2404.07053v1","category":"cs.CL"}
{"created":"2024-04-10 14:37:31","title":"Four-fifths laws in electron and Hall magnetohydrodynamic fluids: Energy, Magnetic helicity and Generalized helicity","abstract":"This paper examines the Kolmogorov type laws of conserved quantities in the electron and Hall magnetohydrodynamic fluids. Inspired by Eyink's longitudinal structure functions and recent progress in classical MHD equations, we derive four-fifths laws for energy, magnetic helicity and generalized helicity in these systems.","sentences":["This paper examines the Kolmogorov type laws of conserved quantities in the electron and Hall magnetohydrodynamic fluids.","Inspired by Eyink's longitudinal structure functions and recent progress in classical MHD equations, we derive four-fifths laws for energy, magnetic helicity and generalized helicity in these systems."],"url":"http://arxiv.org/abs/2404.07047v1","category":"math.AP"}
{"created":"2024-04-10 14:36:35","title":"Comparison of decision trees with Local Interpretable Model-Agnostic Explanations (LIME) technique and multi-linear regression for explaining support vector regression model in terms of root mean square error (RMSE) values","abstract":"In this work the decision trees are used for explanation of support vector regression model. The decision trees act as a global technique as well as a local technique. They are compared against the popular technique of LIME which is a local explanatory technique and with multi linear regression. It is observed that decision trees give a lower RMSE value when fitted to support vector regression as compared to LIME in 87% of the runs over 5 datasets. The comparison of results is statistically significant. Multi linear regression also gives a lower RMSE value when fitted to support vector regression model as compared to LIME in 73% of the runs over 5 datasets but the comparison of results is not statistically significant. Also, when used as a local explanatory technique, decision trees give better performance than LIME and the comparison of results is statistically significant.","sentences":["In this work the decision trees are used for explanation of support vector regression model.","The decision trees act as a global technique as well as a local technique.","They are compared against the popular technique of LIME which is a local explanatory technique and with multi linear regression.","It is observed that decision trees give a lower RMSE value when fitted to support vector regression as compared to LIME in 87% of the runs over 5 datasets.","The comparison of results is statistically significant.","Multi linear regression also gives a lower RMSE value when fitted to support vector regression model as compared to LIME in 73% of the runs over 5 datasets but the comparison of results is not statistically significant.","Also, when used as a local explanatory technique, decision trees give better performance than LIME and the comparison of results is statistically significant."],"url":"http://arxiv.org/abs/2404.07046v1","category":"cs.LG"}
{"created":"2024-04-10 14:35:22","title":"Identification of Fine-grained Systematic Errors via Controlled Scene Generation","abstract":"Many safety-critical applications, especially in autonomous driving, require reliable object detectors. They can be very effectively assisted by a method to search for and identify potential failures and systematic errors before these detectors are deployed. Systematic errors are characterized by combinations of attributes such as object location, scale, orientation, and color, as well as the composition of their respective backgrounds. To identify them, one must rely on something other than real images from a test set because they do not account for very rare but possible combinations of attributes. To overcome this limitation, we propose a pipeline for generating realistic synthetic scenes with fine-grained control, allowing the creation of complex scenes with multiple objects. Our approach, BEV2EGO, allows for a realistic generation of the complete scene with road-contingent control that maps 2D bird's-eye view (BEV) scene configurations to a first-person view (EGO). In addition, we propose a benchmark for controlled scene generation to select the most appropriate generative outpainting model for BEV2EGO. We further use it to perform a systematic analysis of multiple state-of-the-art object detection models and discover differences between them.","sentences":["Many safety-critical applications, especially in autonomous driving, require reliable object detectors.","They can be very effectively assisted by a method to search for and identify potential failures and systematic errors before these detectors are deployed.","Systematic errors are characterized by combinations of attributes such as object location, scale, orientation, and color, as well as the composition of their respective backgrounds.","To identify them, one must rely on something other than real images from a test set because they do not account for very rare but possible combinations of attributes.","To overcome this limitation, we propose a pipeline for generating realistic synthetic scenes with fine-grained control, allowing the creation of complex scenes with multiple objects.","Our approach, BEV2EGO, allows for a realistic generation of the complete scene with road-contingent control that maps 2D bird's-eye view (BEV) scene configurations to a first-person view (EGO).","In addition, we propose a benchmark for controlled scene generation to select the most appropriate generative outpainting model for BEV2EGO.","We further use it to perform a systematic analysis of multiple state-of-the-art object detection models and discover differences between them."],"url":"http://arxiv.org/abs/2404.07045v1","category":"cs.CV"}
{"created":"2024-04-10 14:34:19","title":"On the Performance of IRS-Assisted SSK and RPM over Rician Fading Channels","abstract":"This paper presents the index modulation, that is, the space-shift keying (SSK) and reflection phase modulation (RPM) schemes for intelligent reflecting surface (IRS)-assisted wireless network. IRS simultaneously reflects the incoming information signal from the base station and explicitly encodes the local information bits in the reflection phase shift of IRS elements. The phase shift of the IRS elements is employed according to local data from the RPM constellation. A joint detection using a maximum-likelihood (ML) decoder is performed for the SSK and RPM symbols over a realistic fading scenario modeled as the Rician fading channel. The pairwise error probability over Rician fading channels is derived and utilized to determine the average bit error rate. In addition, the ergodic capacity of the presented system is derived. The derived analytical results are verified and are in exact agreement with Monte-Carlo simulations.","sentences":["This paper presents the index modulation, that is, the space-shift keying (SSK) and reflection phase modulation (RPM) schemes for intelligent reflecting surface (IRS)-assisted wireless network.","IRS simultaneously reflects the incoming information signal from the base station and explicitly encodes the local information bits in the reflection phase shift of IRS elements.","The phase shift of the IRS elements is employed according to local data from the RPM constellation.","A joint detection using a maximum-likelihood (ML) decoder is performed for the SSK and RPM symbols over a realistic fading scenario modeled as the Rician fading channel.","The pairwise error probability over Rician fading channels is derived and utilized to determine the average bit error rate.","In addition, the ergodic capacity of the presented system is derived.","The derived analytical results are verified and are in exact agreement with Monte-Carlo simulations."],"url":"http://arxiv.org/abs/2404.07044v1","category":"cs.IT"}
{"created":"2024-04-10 14:32:01","title":"A note on spectral theory of integral-functional Volterra operators","abstract":"A concise overview of the spectral theory of integral-functional operators is provided. In the context of analysis, a technique is described for deriving solutions to equations involving operators in a closed form. A constructive theorem has been established, outlining a procedure for determining the eigenvalues and eigenfunctions of these operators. Based on this foundation, an analytical approach for generating solutions to a Volterra-type integro-functional inhomogeneous equation is proposed.","sentences":["A concise overview of the spectral theory of integral-functional operators is provided.","In the context of analysis, a technique is described for deriving solutions to equations involving operators in a closed form.","A constructive theorem has been established, outlining a procedure for determining the eigenvalues and eigenfunctions of these operators.","Based on this foundation, an analytical approach for generating solutions to a Volterra-type integro-functional inhomogeneous equation is proposed."],"url":"http://arxiv.org/abs/2404.07041v1","category":"math.DS"}
{"created":"2024-04-10 14:31:25","title":"The Restricted Picard Functor","abstract":"We prove in significant generality the (almost-)representability of the Picard functor when restricted to smooth test schemes.","sentences":["We prove in significant generality the (almost-)representability of the Picard functor when restricted to smooth test schemes."],"url":"http://arxiv.org/abs/2404.07040v1","category":"math.AG"}
{"created":"2024-04-10 14:29:52","title":"Gravitational mode mixing around black holes in scalar-tensor theories with parity-violating terms","abstract":"We investigate black holes and gravitational perturbations when both the scalar Gauss-Bonnet and dynamical Chern-Simons gravity sectors coexist in addition to the Einstein-Hilbert term, and both sectors are coupled to a single canonically normalized scalar field. The presence of the scalar Gauss-Bonnet gravity sector allows the scalar field to possess a non-vanishing background solution, resulting in additional couplings between odd and even-type gravitational perturbations arising from the dynamical Chern-Simons gravity sector. We illustrate the impact of these even-odd gravitational couplings in gravitational perturbations around a static spherically symmetric black hole. Although the couplings between the odd and even-type gravitational perturbations are known to appear in purely tensorial gravity theories with higher curvature corrections, we demonstrate it in scalar-tensor theories.","sentences":["We investigate black holes and gravitational perturbations when both the scalar Gauss-Bonnet and dynamical Chern-Simons gravity sectors coexist in addition to the Einstein-Hilbert term, and both sectors are coupled to a single canonically normalized scalar field.","The presence of the scalar Gauss-Bonnet gravity sector allows the scalar field to possess a non-vanishing background solution, resulting in additional couplings between odd and even-type gravitational perturbations arising from the dynamical Chern-Simons gravity sector.","We illustrate the impact of these even-odd gravitational couplings in gravitational perturbations around a static spherically symmetric black hole.","Although the couplings between the odd and even-type gravitational perturbations are known to appear in purely tensorial gravity theories with higher curvature corrections, we demonstrate it in scalar-tensor theories."],"url":"http://arxiv.org/abs/2404.07039v1","category":"gr-qc"}
{"created":"2024-04-10 14:29:20","title":"Achieving High Polarization of Photons Emitted by Unpolarized Electrons in Ultrastrong Laser Fields","abstract":"Nonlinear Compton scattering driven by ultraintense lasers presents a promising avenue for enhancing the photon energy, brilliance, and setup compactness of $\\gamma$-ray sources. However, a significant challenge lies in achieving a high polarization degree with commonly generated unpolarized electrons, thus addressing a longstanding puzzle in the field. Here we investigate the polarization dynamics of photons emitted by an unpolarized electron beam interacting with a counter-propagating ultraintense laser pulse numerically, and propose a novel method to generate highly polarized $\\gamma$ rays via nonlinear Compton scattering with the aid of vacuum dichroism effect. Our simulations reveal that high-brilliance $\\gamma$ rays with polarization beyond 90\\% are feasible in a single-shot interaction, rivaling the highest achieved by any $\\gamma$-ray sources to date, based on a developed Monte Carlo method incorporating polarization-resolved tree processes of nonlinear Compton scattering and Breit-Wheeler pair production and one-loop vacuum polarization. This generation method showcases an extraordinary ultra-high polarization degree and a user-friendly all-optical experimental setup, while harnessing the high photon energy and brilliance characteristic of nonlinear Compton scattering sources, thus making it of great potential for experimental applications.","sentences":["Nonlinear Compton scattering driven by ultraintense lasers presents a promising avenue for enhancing the photon energy, brilliance, and setup compactness of $\\gamma$-ray sources.","However, a significant challenge lies in achieving a high polarization degree with commonly generated unpolarized electrons, thus addressing a longstanding puzzle in the field.","Here we investigate the polarization dynamics of photons emitted by an unpolarized electron beam interacting with a counter-propagating ultraintense laser pulse numerically, and propose a novel method to generate highly polarized $\\gamma$ rays via nonlinear Compton scattering with the aid of vacuum dichroism effect.","Our simulations reveal that high-brilliance $\\gamma$ rays with polarization beyond 90\\% are feasible in a single-shot interaction, rivaling the highest achieved by any $\\gamma$-ray sources to date, based on a developed Monte Carlo method incorporating polarization-resolved tree processes of nonlinear Compton scattering and Breit-Wheeler pair production and one-loop vacuum polarization.","This generation method showcases an extraordinary ultra-high polarization degree and a user-friendly all-optical experimental setup, while harnessing the high photon energy and brilliance characteristic of nonlinear Compton scattering sources, thus making it of great potential for experimental applications."],"url":"http://arxiv.org/abs/2404.07038v2","category":"physics.plasm-ph"}
{"created":"2024-04-10 14:24:10","title":"ORacle: Large Vision-Language Models for Knowledge-Guided Holistic OR Domain Modeling","abstract":"Every day, countless surgeries are performed worldwide, each within the distinct settings of operating rooms (ORs) that vary not only in their setups but also in the personnel, tools, and equipment used. This inherent diversity poses a substantial challenge for achieving a holistic understanding of the OR, as it requires models to generalize beyond their initial training datasets. To reduce this gap, we introduce ORacle, an advanced vision-language model designed for holistic OR domain modeling, which incorporates multi-view and temporal capabilities and can leverage external knowledge during inference, enabling it to adapt to previously unseen surgical scenarios. This capability is further enhanced by our novel data augmentation framework, which significantly diversifies the training dataset, ensuring ORacle's proficiency in applying the provided knowledge effectively. In rigorous testing, in scene graph generation, and downstream tasks on the 4D-OR dataset, ORacle not only demonstrates state-of-the-art performance but does so requiring less data than existing models. Furthermore, its adaptability is displayed through its ability to interpret unseen views, actions, and appearances of tools and equipment. This demonstrates ORacle's potential to significantly enhance the scalability and affordability of OR domain modeling and opens a pathway for future advancements in surgical data science. We will release our code and data upon acceptance.","sentences":["Every day, countless surgeries are performed worldwide, each within the distinct settings of operating rooms (ORs) that vary not only in their setups but also in the personnel, tools, and equipment used.","This inherent diversity poses a substantial challenge for achieving a holistic understanding of the OR, as it requires models to generalize beyond their initial training datasets.","To reduce this gap, we introduce ORacle, an advanced vision-language model designed for holistic OR domain modeling, which incorporates multi-view and temporal capabilities and can leverage external knowledge during inference, enabling it to adapt to previously unseen surgical scenarios.","This capability is further enhanced by our novel data augmentation framework, which significantly diversifies the training dataset, ensuring ORacle's proficiency in applying the provided knowledge effectively.","In rigorous testing, in scene graph generation, and downstream tasks on the 4D-OR dataset, ORacle not only demonstrates state-of-the-art performance but does so requiring less data than existing models.","Furthermore, its adaptability is displayed through its ability to interpret unseen views, actions, and appearances of tools and equipment.","This demonstrates ORacle's potential to significantly enhance the scalability and affordability of OR domain modeling and opens a pathway for future advancements in surgical data science.","We will release our code and data upon acceptance."],"url":"http://arxiv.org/abs/2404.07031v1","category":"cs.CV"}
{"created":"2024-04-10 14:23:47","title":"Exploring Repetitiveness Measures for Two-Dimensional Strings","abstract":"Detecting and measuring repetitiveness of strings is a problem that has been extensively studied in data compression and text indexing. However, when the data are structured in a non-linear way, like in the context of two-dimensional strings, inherent redundancy offers a rich source for compression, yet systematic studies on repetitiveness measures are still lacking. In the paper we introduce extensions of repetitiveness measures to general two-dimensional strings. In particular, we propose a new extension of the measures $\\delta$ and $\\gamma$, diverging from previous square based definitions proposed in [Carfagna and Manzini, SPIRE 2023]. We further consider generalizations of macro schemes and straight line programs for the 2D setting and show that, in contrast to what happens on strings, 2D macro schemes and 2D SLPs can be both asymptotically smaller than $\\delta$ and $\\gamma$. The results of the paper can be easily extended to $d$-dimensional strings with $d > 2$.","sentences":["Detecting and measuring repetitiveness of strings is a problem that has been extensively studied in data compression and text indexing.","However, when the data are structured in a non-linear way, like in the context of two-dimensional strings, inherent redundancy offers a rich source for compression, yet systematic studies on repetitiveness measures are still lacking.","In the paper we introduce extensions of repetitiveness measures to general two-dimensional strings.","In particular, we propose a new extension of the measures $\\delta$ and $\\gamma$, diverging from previous square based definitions proposed in [Carfagna and Manzini, SPIRE 2023].","We further consider generalizations of macro schemes and straight line programs for the 2D setting and show that, in contrast to what happens on strings, 2D macro schemes and 2D SLPs can be both asymptotically smaller than $\\delta$ and $\\gamma$. The results of the paper can be easily extended to $d$-dimensional strings with $d > 2$."],"url":"http://arxiv.org/abs/2404.07030v1","category":"cs.DS"}
{"created":"2024-04-10 14:22:16","title":"Diffusion-based inpainting of incomplete Euclidean distance matrices of trajectories generated by a fractional Brownian motion","abstract":"Fractional Brownian trajectories (fBm) feature both randomness and strong scale-free correlations, challenging generative models to reproduce the intrinsic memory characterizing the underlying process. Here we test a diffusion probabilistic model on a specific dataset of corrupted images corresponding to incomplete Euclidean distance matrices of fBm at various memory exponents $H$. Our dataset implies uniqueness of the data imputation in the regime of low missing ratio, where the remaining partial graph is rigid, providing the ground truth for the inpainting. We find that the conditional diffusion generation stably reproduces the statistics of missing fBm-distributed distances for different values of $H$ exponent. Furthermore, while diffusion models have been recently shown to remember samples from the training database, we show that diffusion-based inpainting behaves qualitatively different from the database search with the increasing database size. Finally, we apply our fBm-trained diffusion model with $H=1/3$ for completion of chromosome distance matrices obtained in single-cell microscopy experiments, showing its superiority over the standard bioinformatics algorithms. Our source code is available on GitHub at https://github.com/alobashev/diffusion_fbm.","sentences":["Fractional Brownian trajectories (fBm) feature both randomness and strong scale-free correlations, challenging generative models to reproduce the intrinsic memory characterizing the underlying process.","Here we test a diffusion probabilistic model on a specific dataset of corrupted images corresponding to incomplete Euclidean distance matrices of fBm at various memory exponents $H$. Our dataset implies uniqueness of the data imputation in the regime of low missing ratio, where the remaining partial graph is rigid, providing the ground truth for the inpainting.","We find that the conditional diffusion generation stably reproduces the statistics of missing fBm-distributed distances for different values of $H$ exponent.","Furthermore, while diffusion models have been recently shown to remember samples from the training database, we show that diffusion-based inpainting behaves qualitatively different from the database search with the increasing database size.","Finally, we apply our fBm-trained diffusion model with $H=1/3$ for completion of chromosome distance matrices obtained in single-cell microscopy experiments, showing its superiority over the standard bioinformatics algorithms.","Our source code is available on GitHub at https://github.com/alobashev/diffusion_fbm."],"url":"http://arxiv.org/abs/2404.07029v1","category":"cs.CV"}
{"created":"2024-04-10 14:20:52","title":"Optimal Communication Complexity of Chained Index","abstract":"We study the CHAIN communication problem introduced by Cormode et al. [ICALP 2019]. It is a generalization of the well-studied INDEX problem. For $k\\geq 1$, in CHAIN$_{n,k}$, there are $k$ instances of INDEX, all with the same answer. They are shared between $k+1$ players as follows. Player 1 has the first string $X^1 \\in \\{0,1\\}^n$, player 2 has the first index $\\sigma^1 \\in [n]$ and the second string $X^2 \\in \\{0,1\\}^n$, player 3 has the second index $\\sigma^2 \\in [n]$ along with the third string $X^3 \\in \\{0,1\\}^n$, and so on. Player $k+1$ has the last index $\\sigma^k \\in [n]$. The communication is one way from each player to the next, starting from player 1 to player 2, then from player 2 to player 3 and so on. Player $k+1$, after receiving the message from player $k$, has to output a single bit which is the answer to all $k$ instances of INDEX.   It was proved that the CHAIN$_{n,k}$ problem requires $\\Omega(n/k^2)$ communication by Cormode et al., and they used it to prove streaming lower bounds for approximation of maximum independent sets. Subsequently, it was used by Feldman et al. [STOC 2020] to prove lower bounds for streaming submodular maximization. However, these works do not get optimal bounds on the communication complexity of CHAIN$_{n,k}$, and in fact, it was conjectured by Cormode et al. that $\\Omega(n)$ bits are necessary, for any $k$.   As our main result, we prove the optimal lower bound of $\\Omega(n)$ for CHAIN$_{n,k}$. This settles the open conjecture of Cormode et al. in the affirmative. The key technique is to use information theoretic tools to analyze protocols over the Jensen-Shannon divergence measure, as opposed to total variation distance. As a corollary, we get an improved lower bound for approximation of maximum independent set in vertex arrival streams through a reduction from CHAIN directly.","sentences":["We study the CHAIN communication problem introduced by Cormode et al.","[ICALP 2019].","It is a generalization of the well-studied INDEX problem.","For $k\\geq 1$, in CHAIN$_{n,k}$, there are $k$ instances of INDEX, all with the same answer.","They are shared between $k+1$ players as follows.","Player 1 has the first string $X^1 \\in \\{0,1\\}^n$, player 2 has the first index $\\sigma^1","\\in [n]$ and the second string $X^2 \\in \\{0,1\\}^n$, player 3 has the second index $\\sigma^2 \\in","[n]$ along with the third string $X^3 \\in \\{0,1\\}^n$, and so on.","Player $k+1$ has the last index $\\sigma^k \\in","[n]$. The communication is one way from each player to the next, starting from player 1 to player 2, then from player 2 to player 3 and so on.","Player $k+1$, after receiving the message from player $k$, has to output a single bit which is the answer to all $k$ instances of INDEX.   ","It was proved that the CHAIN$_{n,k}$ problem requires $\\Omega(n/k^2)$ communication by Cormode et al., and they used it to prove streaming lower bounds for approximation of maximum independent sets.","Subsequently, it was used by Feldman et al.","[STOC 2020] to prove lower bounds for streaming submodular maximization.","However, these works do not get optimal bounds on the communication complexity of CHAIN$_{n,k}$, and in fact, it was conjectured by Cormode et al. that $\\Omega(n)$ bits are necessary, for any $k$.   ","As our main result, we prove the optimal lower bound of $\\Omega(n)$ for CHAIN$_{n,k}$. This settles the open conjecture of Cormode et al. in the affirmative.","The key technique is to use information theoretic tools to analyze protocols over the Jensen-Shannon divergence measure, as opposed to total variation distance.","As a corollary, we get an improved lower bound for approximation of maximum independent set in vertex arrival streams through a reduction from CHAIN directly."],"url":"http://arxiv.org/abs/2404.07026v1","category":"cs.CC"}
{"created":"2024-04-10 14:18:20","title":"Anisotropy ansatz for the axisymmetric Jeans equations","abstract":"The Jeans equations do not form a closed system, and to solve them a parametrization relating the velocity moments is often adopted. For axisymmetric models, a phenomenological choice (the \"$b$-ansatz\") is widely used for the relation between the vertical ($\\sigma_z^2$) and radial ($\\sigma_R^2$) components of the velocity dispersion tensor, thus breaking their identity present in two-integral systems. However, the way in which the ansatz affects the resulting kinematical fields can be quite complicated, so that the analysis of these fields is usually performed only after numerically computing them. We present here a general procedure to study the properties of the ansatz-dependent fields $\\overline{v_{\\varphi}^2}$, $\\Delta = \\overline{v_{\\varphi}^2} - \\sigma_z^2$ and $\\Delta_R = \\overline{v_{\\varphi}^2} - \\sigma_R^2$. Specifically, the effects of the $b$-ansatz can be determined before solving the Jeans equations once the behaviour over the ($R,z$)-plane of three easy-to-build ansatz-independent functions is known. The procedure also constrains the ansatz to exclude unphysical results (as a negative $\\overline{v_{\\varphi}^2}$). The method is illustrated by discussing the cases of three well-known galaxy models: the Miyamoto & Nagai and Satoh disks, and the Binney logarithmic halo, for which the regions and the constraints on the ansatz values can be determined analytically; a two-component (Miyamoto & Nagai plus logarithmic halo) model is also discussed.","sentences":["The Jeans equations do not form a closed system, and to solve them a parametrization relating the velocity moments is often adopted.","For axisymmetric models, a phenomenological choice (the \"$b$-ansatz\") is widely used for the relation between the vertical ($\\sigma_z^2$) and radial ($\\sigma_R^2$) components of the velocity dispersion tensor, thus breaking their identity present in two-integral systems.","However, the way in which the ansatz affects the resulting kinematical fields can be quite complicated, so that the analysis of these fields is usually performed only after numerically computing them.","We present here a general procedure to study the properties of the ansatz-dependent fields $\\overline{v_{\\varphi}^2}$, $\\Delta = \\overline{v_{\\varphi}^2} - \\sigma_z^2$ and $\\Delta_R = \\overline{v_{\\varphi}^2} - \\sigma_R^2$. Specifically, the effects of the $b$-ansatz can be determined before solving the Jeans equations once the behaviour over the ($R,z$)-plane of three easy-to-build ansatz-independent functions is known.","The procedure also constrains the ansatz to exclude unphysical results (as a negative $\\overline{v_{\\varphi}^2}$).","The method is illustrated by discussing the cases of three well-known galaxy models: the Miyamoto & Nagai and Satoh disks, and the Binney logarithmic halo, for which the regions and the constraints on the ansatz values can be determined analytically; a two-component (Miyamoto & Nagai plus logarithmic halo) model is also discussed."],"url":"http://arxiv.org/abs/2404.07023v1","category":"astro-ph.GA"}
{"created":"2024-04-10 14:09:46","title":"Learned Finite-Time Consensus for Distributed Optimization","abstract":"Most algorithms for decentralized learning employ a consensus or diffusion mechanism to drive agents to a common solution of a global optimization problem. Generally this takes the form of linear averaging, at a rate of contraction determined by the mixing rate of the underlying network topology. For very sparse graphs this can yield a bottleneck, slowing down the convergence of the learning algorithm. We show that a sequence of matrices achieving finite-time consensus can be learned for unknown graph topologies in a decentralized manner by solving a constrained matrix factorization problem. We demonstrate numerically the benefit of the resulting scheme in both structured and unstructured graphs.","sentences":["Most algorithms for decentralized learning employ a consensus or diffusion mechanism to drive agents to a common solution of a global optimization problem.","Generally this takes the form of linear averaging, at a rate of contraction determined by the mixing rate of the underlying network topology.","For very sparse graphs this can yield a bottleneck, slowing down the convergence of the learning algorithm.","We show that a sequence of matrices achieving finite-time consensus can be learned for unknown graph topologies in a decentralized manner by solving a constrained matrix factorization problem.","We demonstrate numerically the benefit of the resulting scheme in both structured and unstructured graphs."],"url":"http://arxiv.org/abs/2404.07018v1","category":"math.OC"}
{"created":"2024-04-10 14:05:44","title":"Improving Language Model Reasoning with Self-motivated Learning","abstract":"Large-scale high-quality training data is important for improving the performance of models. After trained with data that has rationales (reasoning steps), models gain reasoning capability. However, the dataset with high-quality rationales is relatively scarce due to the high annotation cost. To address this issue, we propose \\textit{Self-motivated Learning} framework. The framework motivates the model itself to automatically generate rationales on existing datasets. Based on the inherent rank from correctness across multiple rationales, the model learns to generate better rationales, leading to higher reasoning capability. Specifically, we train a reward model with the rank to evaluate the quality of rationales, and improve the performance of reasoning through reinforcement learning. Experiment results of Llama2 7B on multiple reasoning datasets show that our method significantly improves the reasoning ability of models, even outperforming text-davinci-002 in some datasets.","sentences":["Large-scale high-quality training data is important for improving the performance of models.","After trained with data that has rationales (reasoning steps), models gain reasoning capability.","However, the dataset with high-quality rationales is relatively scarce due to the high annotation cost.","To address this issue, we propose \\textit{Self-motivated Learning} framework.","The framework motivates the model itself to automatically generate rationales on existing datasets.","Based on the inherent rank from correctness across multiple rationales, the model learns to generate better rationales, leading to higher reasoning capability.","Specifically, we train a reward model with the rank to evaluate the quality of rationales, and improve the performance of reasoning through reinforcement learning.","Experiment results of Llama2 7B on multiple reasoning datasets show that our method significantly improves the reasoning ability of models, even outperforming text-davinci-002 in some datasets."],"url":"http://arxiv.org/abs/2404.07017v1","category":"cs.CL"}
{"created":"2024-04-10 13:58:29","title":"Zero-one Laws for a Control Problem with Random Action Sets","abstract":"In many control problems there is only limited information about the actions that will be available at future stages. We introduce a framework where the Controller chooses actions $a_{0}, a_{1}, \\ldots$, one at a time. Her goal is to maximize the probability that the infinite sequence $(a_{0}, a_{1}, \\ldots)$ is an element of a given subset $G$ of $\\mathbb{N}^{\\mathbb{N}}$. The set $G$, called the goal, is assumed to be a Borel tail set. The Controller's choices are restricted: having taken a sequence $h_{t} = (a_{0}, \\ldots, a_{t-1})$ of actions prior to stage $t \\in \\mathbb{N}$, she must choose an action $a_{t}$ at stage $t$ from a non-empty, finite subset $A(h_{t})$ of $\\mathbb{N}$. The set $A(h_{t})$ is chosen from a distribution $p_{t}$, independently over all $t \\in \\mathbb{N}$ and all $h_{t} \\in \\mathbb{N}^{t}$. We consider several information structures defined by how far ahead into the future the Controller knows what actions will be available.   In the special case where all the action sets are singletons (and thus the Controller is a dummy), Kolmogorov's 0-1 law says that the probability for the goal to be reached is 0 or 1. We construct a number of counterexamples to show that in general the value of the control problem can be strictly between 0 and 1, and derive several sufficient conditions for the 0-1 ``law\" to hold.","sentences":["In many control problems there is only limited information about the actions that will be available at future stages.","We introduce a framework where the Controller chooses actions $a_{0}, a_{1}, \\ldots$, one at a time.","Her goal is to maximize the probability that the infinite sequence $(a_{0}, a_{1}, \\ldots)$ is an element of a given subset $G$ of $\\mathbb{N}^{\\mathbb{N}}$. The set $G$, called the goal, is assumed to be a Borel tail set.","The Controller's choices are restricted: having taken a sequence $h_{t} = (a_{0}, \\ldots, a_{t-1})$ of actions prior to stage $t \\in \\mathbb{N}$, she must choose an action $a_{t}$ at stage $t$ from a non-empty, finite subset $A(h_{t})$ of $\\mathbb{N}$. The set $A(h_{t})$ is chosen from a distribution $p_{t}$, independently over all $t \\in \\mathbb{N}$ and","all $h_{t} \\in \\mathbb{N}^{t}$.","We consider several information structures defined by how far ahead into the future the Controller knows what actions will be available.   ","In the special case where all the action sets are singletons (and thus the Controller is a dummy), Kolmogorov's 0-1 law says that the probability for the goal to be reached is 0 or 1.","We construct a number of counterexamples to show that in general the value of the control problem can be strictly between 0 and 1, and derive several sufficient conditions for the 0-1 ``law\" to hold."],"url":"http://arxiv.org/abs/2404.07012v1","category":"math.OC"}
{"created":"2024-04-10 13:53:33","title":"An asymptotically optimal algorithm for generating bin cardinalities","abstract":"In the balls-into-bins setting, $n$ balls are thrown uniformly at random into $n$ bins. The na\\\"{i}ve way to generate the final load vector takes $\\Theta(n)$ time. However, it is well-known that this load vector has with high probability bin cardinalities of size $\\Theta(\\frac{\\log n}{\\log \\log n})$. Here, we present an algorithm in the RAM model that generates the bin cardinalities of the final load vector in the optimal $\\Theta(\\frac{\\log n}{\\log \\log n})$ time in expectation and with high probability.   Further, the algorithm that we present is still optimal for any $m \\in [n, n \\log n]$ balls and can also be used as a building block to efficiently simulate more involved load balancing algorithms. In particular, for the Two-Choice algorithm, which samples two bins in each step and allocates to the least-loaded of the two, we obtain roughly a quadratic speed-up over the na\\\"{i}ve simulation.","sentences":["In the balls-into-bins setting, $n$ balls are thrown uniformly at random into $n$ bins.","The na\\\"{i}ve way to generate the final load vector takes $\\Theta(n)$ time.","However, it is well-known that this load vector has with high probability bin cardinalities of size $\\Theta(\\frac{\\log n}{\\log \\log n})$.","Here, we present an algorithm in the RAM model that generates the bin cardinalities of the final load vector in the optimal $\\Theta(\\frac{\\log n}{\\log \\log n})$ time in expectation and with high probability.   ","Further, the algorithm that we present is still optimal for any $m \\in [n, n \\log n]$ balls and can also be used as a building block to efficiently simulate more involved load balancing algorithms.","In particular, for the Two-Choice algorithm, which samples two bins in each step and allocates to the least-loaded of the two, we obtain roughly a quadratic speed-up over the na\\\"{i}ve simulation."],"url":"http://arxiv.org/abs/2404.07011v1","category":"cs.DS"}
{"created":"2024-04-10 13:52:45","title":"Gaining or losing perspective for convex multivariate functions on box domains","abstract":"MINLO (mixed-integer nonlinear optimization) formulations of the disjunction between the origin and a polytope via a binary indicator variable is broadly used in nonlinear combinatorial optimization for modeling a fixed cost associated with carrying out a group of activities and a convex cost function associated with the levels of the activities. The perspective relaxation of such models is often used to solve to global optimality in a branch-and-bound context, but it typically requires suitable conic solvers and is not compatible with general-purpose NLP software in the presence of other classes of constraints. This motivates the investigation of when simpler but weaker relaxations may be adequate. Comparing the volume (i.e., Lebesgue measure) of the relaxations as a measure of tightness, we lift some of the results related to the simplex case to the box case. In order to compare the volumes of different relaxations in the box case, it is necessary to find an appropriate concave upper bound that preserves the convexity and is minimal, which is more difficult than in the simplex case. To address the challenge beyond the simplex case, the triangulation approach is used.","sentences":["MINLO (mixed-integer nonlinear optimization) formulations of the disjunction between the origin and a polytope via a binary indicator variable is broadly used in nonlinear combinatorial optimization for modeling a fixed cost associated with carrying out a group of activities and a convex cost function associated with the levels of the activities.","The perspective relaxation of such models is often used to solve to global optimality in a branch-and-bound context, but it typically requires suitable conic solvers and is not compatible with general-purpose NLP software in the presence of other classes of constraints.","This motivates the investigation of when simpler but weaker relaxations may be adequate.","Comparing the volume (i.e., Lebesgue measure) of the relaxations as a measure of tightness, we lift some of the results related to the simplex case to the box case.","In order to compare the volumes of different relaxations in the box case, it is necessary to find an appropriate concave upper bound that preserves the convexity and is minimal, which is more difficult than in the simplex case.","To address the challenge beyond the simplex case, the triangulation approach is used."],"url":"http://arxiv.org/abs/2404.07010v1","category":"math.OC"}
{"created":"2024-04-10 13:47:22","title":"Knowledge graphs for empirical concept retrieval","abstract":"Concept-based explainable AI is promising as a tool to improve the understanding of complex models at the premises of a given user, viz.\\ as a tool for personalized explainability. An important class of concept-based explainability methods is constructed with empirically defined concepts, indirectly defined through a set of positive and negative examples, as in the TCAV approach (Kim et al., 2018). While it is appealing to the user to avoid formal definitions of concepts and their operationalization, it can be challenging to establish relevant concept datasets. Here, we address this challenge using general knowledge graphs (such as, e.g., Wikidata or WordNet) for comprehensive concept definition and present a workflow for user-driven data collection in both text and image domains. The concepts derived from knowledge graphs are defined interactively, providing an opportunity for personalization and ensuring that the concepts reflect the user's intentions. We test the retrieved concept datasets on two concept-based explainability methods, namely concept activation vectors (CAVs) and concept activation regions (CARs) (Crabbe and van der Schaar, 2022). We show that CAVs and CARs based on these empirical concept datasets provide robust and accurate explanations. Importantly, we also find good alignment between the models' representations of concepts and the structure of knowledge graphs, i.e., human representations. This supports our conclusion that knowledge graph-based concepts are relevant for XAI.","sentences":["Concept-based explainable AI is promising as a tool to improve the understanding of complex models at the premises of a given user, viz.\\ as a tool for personalized explainability.","An important class of concept-based explainability methods is constructed with empirically defined concepts, indirectly defined through a set of positive and negative examples, as in the TCAV approach (Kim et al., 2018).","While it is appealing to the user to avoid formal definitions of concepts and their operationalization, it can be challenging to establish relevant concept datasets.","Here, we address this challenge using general knowledge graphs (such as, e.g., Wikidata or WordNet) for comprehensive concept definition and present a workflow for user-driven data collection in both text and image domains.","The concepts derived from knowledge graphs are defined interactively, providing an opportunity for personalization and ensuring that the concepts reflect the user's intentions.","We test the retrieved concept datasets on two concept-based explainability methods, namely concept activation vectors (CAVs) and concept activation regions (CARs) (Crabbe and van der Schaar, 2022).","We show that CAVs and CARs based on these empirical concept datasets provide robust and accurate explanations.","Importantly, we also find good alignment between the models' representations of concepts and the structure of knowledge graphs, i.e., human representations.","This supports our conclusion that knowledge graph-based concepts are relevant for XAI."],"url":"http://arxiv.org/abs/2404.07008v1","category":"cs.LG"}
{"created":"2024-04-10 13:40:29","title":"WordDecipher: Enhancing Digital Workspace Communication with Explainable AI for Non-native English Speakers","abstract":"Non-native English speakers (NNES) face challenges in digital workspace communication (e.g., emails, Slack messages), often inadvertently translating expressions from their native languages, which can lead to awkward or incorrect usage. Current AI-assisted writing tools are equipped with fluency enhancement and rewriting suggestions; however, NNES may struggle to grasp the subtleties among various expressions, making it challenging to choose the one that accurately reflects their intent. Such challenges are exacerbated in high-stake text-based communications, where the absence of non-verbal cues heightens the risk of misinterpretation. By leveraging the latest advancements in large language models (LLM) and word embeddings, we propose WordDecipher, an explainable AI-assisted writing tool to enhance digital workspace communication for NNES. WordDecipher not only identifies the perceived social intentions detected in users' writing, but also generates rewriting suggestions aligned with users' intended messages, either numerically or by inferring from users' writing in their native language. Then, WordDecipher provides an overview of nuances to help NNES make selections. Through a usage scenario, we demonstrate how WordDecipher can significantly enhance an NNES's ability to communicate her request, showcasing its potential to transform workspace communication for NNES.","sentences":["Non-native English speakers (NNES) face challenges in digital workspace communication (e.g., emails, Slack messages), often inadvertently translating expressions from their native languages, which can lead to awkward or incorrect usage.","Current AI-assisted writing tools are equipped with fluency enhancement and rewriting suggestions; however, NNES may struggle to grasp the subtleties among various expressions, making it challenging to choose the one that accurately reflects their intent.","Such challenges are exacerbated in high-stake text-based communications, where the absence of non-verbal cues heightens the risk of misinterpretation.","By leveraging the latest advancements in large language models (LLM) and word embeddings, we propose WordDecipher, an explainable AI-assisted writing tool to enhance digital workspace communication for NNES.","WordDecipher not only identifies the perceived social intentions detected in users' writing, but also generates rewriting suggestions aligned with users' intended messages, either numerically or by inferring from users' writing in their native language.","Then, WordDecipher provides an overview of nuances to help NNES make selections.","Through a usage scenario, we demonstrate how WordDecipher can significantly enhance an NNES's ability to communicate her request, showcasing its potential to transform workspace communication for NNES."],"url":"http://arxiv.org/abs/2404.07005v1","category":"cs.HC"}
{"created":"2024-04-10 13:35:48","title":"An adaptive acceleration scheme for phase-field fatigue computations","abstract":"Phase-field models of fatigue are capable of reproducing the main phenomenology of fatigue behavior. However, phase-field computations in the high-cycle fatigue regime are prohibitively expensive, due to the need to resolve spatially the small length scale inherent to phase-field models and temporally the loading history for several millions of cycles. As a remedy, we propose a fully adaptive acceleration scheme based on the cycle jump technique, where the cycle-by-cycle resolution of an appropriately determined number of cycles is skipped while predicting the local system evolution during the jump. The novelty of our approach is a cycle-jump criterion to determine the appropriate cycle-jump size based on a target increment of a global variable which monitors the advancement of fatigue. We propose the definition and meaning of this variable for three general stages of the fatigue life. In comparison to existing acceleration techniques, our approach needs no parameters and bounds for the cycle-jump size, and it works independently of the material, specimen or loading conditions. Since one of the monitoring variables is the fatigue crack length, we introduce an accurate, flexible and efficient method for its computation, which overcomes the issues of conventional crack tip tracking algorithms and enables the consideration of several cracks evolving at the same time. The performance of the proposed acceleration scheme is demonstrated with representative numerical examples, which show a speedup reaching four orders of magnitude in the high-cycle fatigue regime with consistently high accuracy.","sentences":["Phase-field models of fatigue are capable of reproducing the main phenomenology of fatigue behavior.","However, phase-field computations in the high-cycle fatigue regime are prohibitively expensive, due to the need to resolve spatially the small length scale inherent to phase-field models and temporally the loading history for several millions of cycles.","As a remedy, we propose a fully adaptive acceleration scheme based on the cycle jump technique, where the cycle-by-cycle resolution of an appropriately determined number of cycles is skipped while predicting the local system evolution during the jump.","The novelty of our approach is a cycle-jump criterion to determine the appropriate cycle-jump size based on a target increment of a global variable which monitors the advancement of fatigue.","We propose the definition and meaning of this variable for three general stages of the fatigue life.","In comparison to existing acceleration techniques, our approach needs no parameters and bounds for the cycle-jump size, and it works independently of the material, specimen or loading conditions.","Since one of the monitoring variables is the fatigue crack length, we introduce an accurate, flexible and efficient method for its computation, which overcomes the issues of conventional crack tip tracking algorithms and enables the consideration of several cracks evolving at the same time.","The performance of the proposed acceleration scheme is demonstrated with representative numerical examples, which show a speedup reaching four orders of magnitude in the high-cycle fatigue regime with consistently high accuracy."],"url":"http://arxiv.org/abs/2404.07003v1","category":"cs.CE"}
{"created":"2024-04-10 13:31:07","title":"Event Grounded Criminal Court View Generation withCooperative (Large) Language Models","abstract":"With the development of legal intelligence, Criminal Court View Generation has attracted much attention as a crucial task of legal intelligence, which aims to generate concise and coherent texts that summarize case facts and provide explanations for verdicts. Existing researches explore the key information in case facts to yield the court views. Most of them employ a coarse-grained approach that partitions the facts into broad segments (e.g., verdict-related sentences) to make predictions. However, this approach fails to capture the complex details present in the case facts, such as various criminal elements and legal events. To this end, in this paper, we propose an Event Grounded Generation (EGG) method for criminal court view generation with cooperative (Large) Language Models, which introduces the fine-grained event information into the generation. Specifically, we first design a LLMs-based extraction method that can extract events in case facts without massive annotated events. Then, we incorporate the extracted events into court view generation by merging case facts and events. Besides, considering the computational burden posed by the use of LLMs in the extraction phase of EGG, we propose a LLMs-free EGG method that can eliminate the requirement for event extraction using LLMs in the inference phase. Extensive experimental results on a real-world dataset clearly validate the effectiveness of our proposed method.","sentences":["With the development of legal intelligence, Criminal Court View Generation has attracted much attention as a crucial task of legal intelligence, which aims to generate concise and coherent texts that summarize case facts and provide explanations for verdicts.","Existing researches explore the key information in case facts to yield the court views.","Most of them employ a coarse-grained approach that partitions the facts into broad segments (e.g., verdict-related sentences) to make predictions.","However, this approach fails to capture the complex details present in the case facts, such as various criminal elements and legal events.","To this end, in this paper, we propose an Event Grounded Generation (EGG) method for criminal court view generation with cooperative (Large) Language Models, which introduces the fine-grained event information into the generation.","Specifically, we first design a LLMs-based extraction method that can extract events in case facts without massive annotated events.","Then, we incorporate the extracted events into court view generation by merging case facts and events.","Besides, considering the computational burden posed by the use of LLMs in the extraction phase of EGG, we propose a LLMs-free EGG method that can eliminate the requirement for event extraction using LLMs in the inference phase.","Extensive experimental results on a real-world dataset clearly validate the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2404.07001v1","category":"cs.CL"}
{"created":"2024-04-10 13:24:27","title":"Agent-driven Generative Semantic Communication for Remote Surveillance","abstract":"In the era of 6G, featuring compelling visions of intelligent transportation system, digital twins, remote surveillance is poised to become a ubiquitous practice. The substantial data volume and frequent updates present challenges in wireless networks. To address this, we propose a novel agent-driven generative semantic communication (A-GSC) framework based on reinforcement learning. In contrast to the existing research on semantic communication (SemCom), which mainly focuses on semantic compression or semantic sampling, we seamlessly cascade both together by jointly considering the intrinsic attributes of source information and the contextual information regarding the task. Notably, the introduction of the generative artificial intelligence (GAI) enables the independent design of semantic encoders and decoders. In this work, we develop an agent-assisted semantic encoder leveraging the knowledge based soft actor-critic algorithm, which can track the semantic changes, channel condition, and sampling intervals, so as to perform adaptive semantic sampling. Accordingly, we design a semantic decoder with both predictive and generative capabilities, which consists of two tailored modules. Moreover, the effectiveness of the designed models has been verified based on the dataset generated from CDNet2014, and the performance gain of the overall A-GSC framework in both energy saving and reconstruction accuracy have been demonstrated.","sentences":["In the era of 6G, featuring compelling visions of intelligent transportation system, digital twins, remote surveillance is poised to become a ubiquitous practice.","The substantial data volume and frequent updates present challenges in wireless networks.","To address this, we propose a novel agent-driven generative semantic communication (A-GSC) framework based on reinforcement learning.","In contrast to the existing research on semantic communication (SemCom), which mainly focuses on semantic compression or semantic sampling, we seamlessly cascade both together by jointly considering the intrinsic attributes of source information and the contextual information regarding the task.","Notably, the introduction of the generative artificial intelligence (GAI) enables the independent design of semantic encoders and decoders.","In this work, we develop an agent-assisted semantic encoder leveraging the knowledge based soft actor-critic algorithm, which can track the semantic changes, channel condition, and sampling intervals, so as to perform adaptive semantic sampling.","Accordingly, we design a semantic decoder with both predictive and generative capabilities, which consists of two tailored modules.","Moreover, the effectiveness of the designed models has been verified based on the dataset generated from CDNet2014, and the performance gain of the overall A-GSC framework in both energy saving and reconstruction accuracy have been demonstrated."],"url":"http://arxiv.org/abs/2404.06997v1","category":"cs.NI"}
{"created":"2024-04-10 13:19:56","title":"XNLIeu: a dataset for cross-lingual NLI in Basque","abstract":"XNLI is a popular Natural Language Inference (NLI) benchmark widely used to evaluate cross-lingual Natural Language Understanding (NLU) capabilities across languages. In this paper, we expand XNLI to include Basque, a low-resource language that can greatly benefit from transfer-learning approaches. The new dataset, dubbed XNLIeu, has been developed by first machine-translating the English XNLI corpus into Basque, followed by a manual post-edition step. We have conducted a series of experiments using mono- and multilingual LLMs to assess a) the effect of professional post-edition on the MT system; b) the best cross-lingual strategy for NLI in Basque; and c) whether the choice of the best cross-lingual strategy is influenced by the fact that the dataset is built by translation. The results show that post-edition is necessary and that the translate-train cross-lingual strategy obtains better results overall, although the gain is lower when tested in a dataset that has been built natively from scratch. Our code and datasets are publicly available under open licenses.","sentences":["XNLI is a popular Natural Language Inference (NLI) benchmark widely used to evaluate cross-lingual Natural Language Understanding (NLU) capabilities across languages.","In this paper, we expand XNLI to include Basque, a low-resource language that can greatly benefit from transfer-learning approaches.","The new dataset, dubbed XNLIeu, has been developed by first machine-translating the English XNLI corpus into Basque, followed by a manual post-edition step.","We have conducted a series of experiments using mono- and multilingual LLMs to assess a) the effect of professional post-edition on the MT system; b) the best cross-lingual strategy for NLI in Basque; and c) whether the choice of the best cross-lingual strategy is influenced by the fact that the dataset is built by translation.","The results show that post-edition is necessary and that the translate-train cross-lingual strategy obtains better results overall, although the gain is lower when tested in a dataset that has been built natively from scratch.","Our code and datasets are publicly available under open licenses."],"url":"http://arxiv.org/abs/2404.06996v1","category":"cs.CL"}
{"created":"2024-04-10 13:10:52","title":"Ray-driven Spectral CT Reconstruction Based on Neural Base-Material Fields","abstract":"In spectral CT reconstruction, the basis materials decomposition involves solving a large-scale nonlinear system of integral equations, which is highly ill-posed mathematically. This paper proposes a model that parameterizes the attenuation coefficients of the object using a neural field representation, thereby avoiding the complex calculations of pixel-driven projection coefficient matrices during the discretization process of line integrals. It introduces a lightweight discretization method for line integrals based on a ray-driven neural field, enhancing the accuracy of the integral approximation during the discretization process. The basis materials are represented as continuous vector-valued implicit functions to establish a neural field parameterization model for the basis materials. The auto-differentiation framework of deep learning is then used to solve the implicit continuous function of the neural base-material fields. This method is not limited by the spatial resolution of reconstructed images, and the network has compact and regular properties. Experimental validation shows that our method performs exceptionally well in addressing the spectral CT reconstruction. Additionally, it fulfils the requirements for the generation of high-resolution reconstruction images.","sentences":["In spectral CT reconstruction, the basis materials decomposition involves solving a large-scale nonlinear system of integral equations, which is highly ill-posed mathematically.","This paper proposes a model that parameterizes the attenuation coefficients of the object using a neural field representation, thereby avoiding the complex calculations of pixel-driven projection coefficient matrices during the discretization process of line integrals.","It introduces a lightweight discretization method for line integrals based on a ray-driven neural field, enhancing the accuracy of the integral approximation during the discretization process.","The basis materials are represented as continuous vector-valued implicit functions to establish a neural field parameterization model for the basis materials.","The auto-differentiation framework of deep learning is then used to solve the implicit continuous function of the neural base-material fields.","This method is not limited by the spatial resolution of reconstructed images, and the network has compact and regular properties.","Experimental validation shows that our method performs exceptionally well in addressing the spectral CT reconstruction.","Additionally, it fulfils the requirements for the generation of high-resolution reconstruction images."],"url":"http://arxiv.org/abs/2404.06991v1","category":"eess.IV"}
{"created":"2024-04-10 13:09:53","title":"Emergence of large-scale mechanical spiral waves in bacterial living matter","abstract":"Propagating spiral waves have been discovered in various chemical, biological and physical systems. Spiral waves in multicellular organisms are often associated with essential living functions. Although certain eukaryotic microorganisms have long been known to generate spiral waves, evidence of spiral wave pattern has not been lacking in the bacterial world. Here we report the discovery of a unique form of propagating spiral waves in dense bacterial populations where cells engage in cyclic force-generating processes driven by type-IV pilus motility. Specifically, we discovered that synchronization of pilus activity in the bacterial living matter leads to large-scale spatiotemporal regulation of tension force in the form of propagating spiral waves. Theoretical modelling reveals that the spiral tension waves result from nonreciprocity in cell-cell interactions. Our findings reveal a novel mechanism of large-scale force regulation in bacterial world and may shed light on the emergent mechanics of biofilms and microbiomes. Pilus-driven bacterial living matter also provides a mechanical active medium for studying electrical or chemical spiral waves in living systems.","sentences":["Propagating spiral waves have been discovered in various chemical, biological and physical systems.","Spiral waves in multicellular organisms are often associated with essential living functions.","Although certain eukaryotic microorganisms have long been known to generate spiral waves, evidence of spiral wave pattern has not been lacking in the bacterial world.","Here we report the discovery of a unique form of propagating spiral waves in dense bacterial populations where cells engage in cyclic force-generating processes driven by type-IV pilus motility.","Specifically, we discovered that synchronization of pilus activity in the bacterial living matter leads to large-scale spatiotemporal regulation of tension force in the form of propagating spiral waves.","Theoretical modelling reveals that the spiral tension waves result from nonreciprocity in cell-cell interactions.","Our findings reveal a novel mechanism of large-scale force regulation in bacterial world and may shed light on the emergent mechanics of biofilms and microbiomes.","Pilus-driven bacterial living matter also provides a mechanical active medium for studying electrical or chemical spiral waves in living systems."],"url":"http://arxiv.org/abs/2404.06990v1","category":"physics.bio-ph"}
{"created":"2024-04-10 13:03:42","title":"Constraining the UV with the electroweak effective action","abstract":"By considering an arbitrary bare action describing BSM physics, we use the Barvinsky-Vilkovisky resummation to obtain the most general non-local electroweak effective action at second order in the field strength. We also include the contribution of the functional measure to the effective action, which is found to modify the Higgs potential by shifting its vacuum value. The resulting effective action provides one-loop corrections to the $W$ and $Z$ boson masses, ultimately leading to the most general expression for the $\\rho$ parameter at one-loop. The functional measure plays a pivotal role as it allows the parameterization of $\\rho$ in inverse powers of the scale of new physics, while containing non-local form factors. The comparison of $\\rho$ with the latest data leads to several constraints on the UV particle spectra of BSM models.","sentences":["By considering an arbitrary bare action describing BSM physics, we use the Barvinsky-Vilkovisky resummation to obtain the most general non-local electroweak effective action at second order in the field strength.","We also include the contribution of the functional measure to the effective action, which is found to modify the Higgs potential by shifting its vacuum value.","The resulting effective action provides one-loop corrections to the $W$ and $Z$ boson masses, ultimately leading to the most general expression for the $\\rho$ parameter at one-loop.","The functional measure plays a pivotal role as it allows the parameterization of $\\rho$ in inverse powers of the scale of new physics, while containing non-local form factors.","The comparison of $\\rho$ with the latest data leads to several constraints on the UV particle spectra of BSM models."],"url":"http://arxiv.org/abs/2404.06987v1","category":"hep-th"}
{"created":"2024-04-10 13:01:52","title":"Friedmann equations of the fractal apparent horizon","abstract":"From a fractal perspective, the entropy bound of gravitational systems undergoes changes. Furthermore, in the cosmological setting, the conservation law of a perfect fluid is also altered in such systems, affecting spatial elements like volume, area, and radius. By applying the first law of thermodynamics and deriving the Friedmann equations, we can gain insight into the evolution of such a fractal cosmos. However, observations continue to necessitate the existence of a dark energy source. To address this, in this article, we have created a novel fractal $\\Lambda$CDM cosmological model and determined the fractal cosmological observables. We show that the spatial fractal dimension is two, and the age of the Universe is 13.91 Gyr, by fitting the model's parameters to cosmological data.","sentences":["From a fractal perspective, the entropy bound of gravitational systems undergoes changes.","Furthermore, in the cosmological setting, the conservation law of a perfect fluid is also altered in such systems, affecting spatial elements like volume, area, and radius.","By applying the first law of thermodynamics and deriving the Friedmann equations, we can gain insight into the evolution of such a fractal cosmos.","However, observations continue to necessitate the existence of a dark energy source.","To address this, in this article, we have created a novel fractal $\\Lambda$CDM cosmological model and determined the fractal cosmological observables.","We show that the spatial fractal dimension is two, and the age of the Universe is 13.91 Gyr, by fitting the model's parameters to cosmological data."],"url":"http://arxiv.org/abs/2404.06986v1","category":"gr-qc"}
{"created":"2024-04-10 13:00:46","title":"Algebraic Proofs of Path Disconnectedness using Time-Dependent Barrier Functions","abstract":"Two subsets of a given set are path-disconnected if they lie in different connected components of the larger set. Verification of path-disconnectedness is essential in proving the infeasibility of motion planning and trajectory optimization algorithms. We formulate path-disconnectedness as the infeasibility of a single-integrator control task to move between an initial set and a target set in a sufficiently long time horizon. This control-infeasibility task is certified through the generation of a time-dependent barrier function that separates the initial and final sets. The existence of a time-dependent barrier function is a necessary and sufficient condition for path-disconnectedness under compactness conditions. Numerically, the search for a polynomial barrier function is formulated using the moment-sum-of-squares hierarchy of semidefinite programs. The barrier function proves path-disconnectedness at a sufficiently large polynomial degree. The computational complexity of these semidefinite programs can be reduced by elimination of the control variables. Disconnectedness proofs are synthesized for example systems.","sentences":["Two subsets of a given set are path-disconnected if they lie in different connected components of the larger set.","Verification of path-disconnectedness is essential in proving the infeasibility of motion planning and trajectory optimization algorithms.","We formulate path-disconnectedness as the infeasibility of a single-integrator control task to move between an initial set and a target set in a sufficiently long time horizon.","This control-infeasibility task is certified through the generation of a time-dependent barrier function that separates the initial and final sets.","The existence of a time-dependent barrier function is a necessary and sufficient condition for path-disconnectedness under compactness conditions.","Numerically, the search for a polynomial barrier function is formulated using the moment-sum-of-squares hierarchy of semidefinite programs.","The barrier function proves path-disconnectedness at a sufficiently large polynomial degree.","The computational complexity of these semidefinite programs can be reduced by elimination of the control variables.","Disconnectedness proofs are synthesized for example systems."],"url":"http://arxiv.org/abs/2404.06985v1","category":"math.OC"}
{"created":"2024-04-10 12:52:22","title":"Arakelov-Green's functions for dynamical systems on projective varieties","abstract":"We introduce functions associated to polarized dynamical systems that generalize averages of the dynamical Arakelov-Green's functions for rational functions due to Baker and Rumely. For a polarized dynamical system $X\\to X$ over a product formula field, we prove an Elkies-style lower bound for these functions evaluated on the adelic points of $X$. As an application, we prove a Lehmer-type lower bound on the canonical height of a non-torsion point $P$ on an abelian variety $A/K$, where $K$ is a product formula field having perfect residue fields at its completions (for instance, $K$ may be a number field or the function field of a curve over $\\mathbb{C}$ or $\\mathbb{F}_p$). For $A$ of dimension $g$, the lower bound has the form \\[\\widehat{h}(P)\\ge\\frac{C}{D^{2g+3}(\\log D)^{2g}},\\] where $C=C(A,K,\\widehat{h})>0$, $D=[K(P):K]\\ge2$, and $P\\in A(\\bar{K})\\setminus A(\\bar{K})_{\\text{tors}}$ is not contained in a torsion translate of an abelian subvariety of $A$ having everywhere potential good reduction.","sentences":["We introduce functions associated to polarized dynamical systems that generalize averages of the dynamical Arakelov-Green's functions for rational functions due to Baker and Rumely.","For a polarized dynamical system $X\\to X$ over a product formula field, we prove an Elkies-style lower bound for these functions evaluated on the adelic points of $X$. As an application, we prove a Lehmer-type lower bound on the canonical height of a non-torsion point $P$ on an abelian variety $A/K$, where $K$ is a product formula field having perfect residue fields at its completions (for instance, $K$ may be a number field or the function field of a curve over $\\mathbb{C}$ or $\\mathbb{F}_p$).","For $A$ of dimension $g$, the lower bound has the form \\[\\widehat{h}(P)\\ge\\frac{C}{D^{2g+3}(\\log D)^{2g}},\\] where $C=C(A,K,\\widehat{h})>0$, $D=[K(P):K]\\ge2$, and $P\\in A(\\bar{K})\\setminus A(\\bar{K})_{\\text{tors}}$ is not contained in a torsion translate of an abelian subvariety of $A$ having everywhere potential good reduction."],"url":"http://arxiv.org/abs/2404.06981v1","category":"math.NT"}
{"created":"2024-04-10 12:49:31","title":"Tidal Love numbers and approximate universal relations for fermion soliton stars","abstract":"Fermion soliton stars are a consistent model of exotic compact objects which involve a nonlinear interaction between a real scalar field and fermions through a Yukawa term. This interaction results in an effective fermion mass that depends upon the vacuum structure in the scalar potential. In this work we investigate the tidal deformations of fermion soliton stars and compute the corresponding tidal Love numbers for different model parameters. Furthermore, we discuss the existence of approximate universal relations for the electric and magnetic tidal deformabilities of these stars, and compare them with other solutions of general relativity, such as neutron stars or boson stars. These relations for fermion soliton stars are less universal than for neutron stars, but they are sufficiently different from the ordinary neutron star case that a measurement of the electric and magnetic tidal Love numbers (as potentially achievable by next-generation gravitational wave detectors) can be used to disentangle these families of compact objects. Finally, we discuss the conditions for tidal disruption of fermion soliton stars in a binary system and estimate the detectability of the electromagnetic signal associated with such tidal disruption events.","sentences":["Fermion soliton stars are a consistent model of exotic compact objects which involve a nonlinear interaction between a real scalar field and fermions through a Yukawa term.","This interaction results in an effective fermion mass that depends upon the vacuum structure in the scalar potential.","In this work we investigate the tidal deformations of fermion soliton stars and compute the corresponding tidal Love numbers for different model parameters.","Furthermore, we discuss the existence of approximate universal relations for the electric and magnetic tidal deformabilities of these stars, and compare them with other solutions of general relativity, such as neutron stars or boson stars.","These relations for fermion soliton stars are less universal than for neutron stars, but they are sufficiently different from the ordinary neutron star case that a measurement of the electric and magnetic tidal Love numbers (as potentially achievable by next-generation gravitational wave detectors) can be used to disentangle these families of compact objects.","Finally, we discuss the conditions for tidal disruption of fermion soliton stars in a binary system and estimate the detectability of the electromagnetic signal associated with such tidal disruption events."],"url":"http://arxiv.org/abs/2404.06979v1","category":"gr-qc"}
{"created":"2024-04-10 12:45:27","title":"Accurate Tennis Court Line Detection on Amateur Recorded Matches","abstract":"Typically, tennis court line detection is done by running Hough-Line-Detection to find straight lines in the image, and then computing a transformation matrix from the detected lines to create the final court structure. We propose numerous improvements and enhancements to this algorithm, including using pretrained State-of-the-Art shadow-removal and object-detection ML models to make our line-detection more robust. Compared to the original algorithm, our method can accurately detect lines on amateur, dirty courts. When combined with a robust ball-tracking system, our method will enable accurate, automatic refereeing for amateur and professional tennis matches alike.","sentences":["Typically, tennis court line detection is done by running Hough-Line-Detection to find straight lines in the image, and then computing a transformation matrix from the detected lines to create the final court structure.","We propose numerous improvements and enhancements to this algorithm, including using pretrained State-of-the-Art shadow-removal and object-detection ML models to make our line-detection more robust.","Compared to the original algorithm, our method can accurately detect lines on amateur, dirty courts.","When combined with a robust ball-tracking system, our method will enable accurate, automatic refereeing for amateur and professional tennis matches alike."],"url":"http://arxiv.org/abs/2404.06977v1","category":"cs.CV"}
{"created":"2024-04-10 12:32:18","title":"Toward industrial use of continual learning : new metrics proposal for class incremental learning","abstract":"In this paper, we investigate continual learning performance metrics used in class incremental learning strategies for continual learning (CL) using some high performing methods. We investigate especially mean task accuracy. First, we show that it lacks of expressiveness through some simple experiments to capture performance. We show that monitoring average tasks performance is over optimistic and can lead to misleading conclusions for future real life industrial uses. Then, we propose first a simple metric, Minimal Incremental Class Accuracy (MICA) which gives a fair and more useful evaluation of different continual learning methods. Moreover, in order to provide a simple way to easily compare different methods performance in continual learning, we derive another single scalar metric that take into account the learning performance variation as well as our newly introduced metric.","sentences":["In this paper, we investigate continual learning performance metrics used in class incremental learning strategies for continual learning (CL) using some high performing methods.","We investigate especially mean task accuracy.","First, we show that it lacks of expressiveness through some simple experiments to capture performance.","We show that monitoring average tasks performance is over optimistic and can lead to misleading conclusions for future real life industrial uses.","Then, we propose first a simple metric, Minimal Incremental Class Accuracy (MICA) which gives a fair and more useful evaluation of different continual learning methods.","Moreover, in order to provide a simple way to easily compare different methods performance in continual learning, we derive another single scalar metric that take into account the learning performance variation as well as our newly introduced metric."],"url":"http://arxiv.org/abs/2404.06972v1","category":"cs.LG"}
{"created":"2024-04-10 12:31:43","title":"TrajPRed: Trajectory Prediction with Region-based Relation Learning","abstract":"Forecasting human trajectories in traffic scenes is critical for safety within mixed or fully autonomous systems. Human future trajectories are driven by two major stimuli, social interactions, and stochastic goals. Thus, reliable forecasting needs to capture these two stimuli. Edge-based relation modeling represents social interactions using pairwise correlations from precise individual states. Nevertheless, edge-based relations can be vulnerable under perturbations. To alleviate these issues, we propose a region-based relation learning paradigm that models social interactions via region-wise dynamics of joint states, i.e., the changes in the density of crowds. In particular, region-wise agent joint information is encoded within convolutional feature grids. Social relations are modeled by relating the temporal changes of local joint information from a global perspective. We show that region-based relations are less susceptible to perturbations. In order to account for the stochastic individual goals, we exploit a conditional variational autoencoder to realize multi-goal estimation and diverse future prediction. Specifically, we perform variational inference via the latent distribution, which is conditioned on the correlation between input states and associated target goals. Sampling from the latent distribution enables the framework to reliably capture the stochastic behavior in test data. We integrate multi-goal estimation and region-based relation learning to model the two stimuli, social interactions, and stochastic goals, in a prediction framework. We evaluate our framework on the ETH-UCY dataset and Stanford Drone Dataset (SDD). We show that the diverse prediction better fits the ground truth when incorporating the relation module. Our framework outperforms the state-of-the-art models on SDD by $27.61\\%$/$18.20\\%$ of ADE/FDE metrics.","sentences":["Forecasting human trajectories in traffic scenes is critical for safety within mixed or fully autonomous systems.","Human future trajectories are driven by two major stimuli, social interactions, and stochastic goals.","Thus, reliable forecasting needs to capture these two stimuli.","Edge-based relation modeling represents social interactions using pairwise correlations from precise individual states.","Nevertheless, edge-based relations can be vulnerable under perturbations.","To alleviate these issues, we propose a region-based relation learning paradigm that models social interactions via region-wise dynamics of joint states, i.e., the changes in the density of crowds.","In particular, region-wise agent joint information is encoded within convolutional feature grids.","Social relations are modeled by relating the temporal changes of local joint information from a global perspective.","We show that region-based relations are less susceptible to perturbations.","In order to account for the stochastic individual goals, we exploit a conditional variational autoencoder to realize multi-goal estimation and diverse future prediction.","Specifically, we perform variational inference via the latent distribution, which is conditioned on the correlation between input states and associated target goals.","Sampling from the latent distribution enables the framework to reliably capture the stochastic behavior in test data.","We integrate multi-goal estimation and region-based relation learning to model the two stimuli, social interactions, and stochastic goals, in a prediction framework.","We evaluate our framework on the ETH-UCY dataset and Stanford Drone Dataset (SDD).","We show that the diverse prediction better fits the ground truth when incorporating the relation module.","Our framework outperforms the state-of-the-art models on SDD by $27.61\\%$/$18.20\\%$ of ADE/FDE metrics."],"url":"http://arxiv.org/abs/2404.06971v1","category":"cs.CV"}
{"created":"2024-04-10 12:31:09","title":"Hybrid Multi-stage Decoding for Few-shot NER with Entity-aware Contrastive Learning","abstract":"Few-shot named entity recognition can identify new types of named entities based on a few labeled examples. Previous methods employing token-level or span-level metric learning suffer from the computational burden and a large number of negative sample spans. In this paper, we propose the Hybrid Multi-stage Decoding for Few-shot NER with Entity-aware Contrastive Learning (MsFNER), which splits the general NER into two stages: entity-span detection and entity classification. There are 3 processes for introducing MsFNER: training, finetuning, and inference. In the training process, we train and get the best entity-span detection model and the entity classification model separately on the source domain using meta-learning, where we create a contrastive learning module to enhance entity representations for entity classification. During finetuning, we finetune the both models on the support dataset of target domain. In the inference process, for the unlabeled data, we first detect the entity-spans, then the entity-spans are jointly determined by the entity classification model and the KNN. We conduct experiments on the open FewNERD dataset and the results demonstrate the advance of MsFNER.","sentences":["Few-shot named entity recognition can identify new types of named entities based on a few labeled examples.","Previous methods employing token-level or span-level metric learning suffer from the computational burden and a large number of negative sample spans.","In this paper, we propose the Hybrid Multi-stage Decoding for Few-shot NER with Entity-aware Contrastive Learning (MsFNER), which splits the general NER into two stages: entity-span detection and entity classification.","There are 3 processes for introducing MsFNER: training, finetuning, and inference.","In the training process, we train and get the best entity-span detection model and the entity classification model separately on the source domain using meta-learning, where we create a contrastive learning module to enhance entity representations for entity classification.","During finetuning, we finetune the both models on the support dataset of target domain.","In the inference process, for the unlabeled data, we first detect the entity-spans, then the entity-spans are jointly determined by the entity classification model and the KNN.","We conduct experiments on the open FewNERD dataset and the results demonstrate the advance of MsFNER."],"url":"http://arxiv.org/abs/2404.06970v1","category":"cs.CL"}
{"created":"2024-04-10 12:29:05","title":"FiP: a Fixed-Point Approach for Causal Generative Modeling","abstract":"Modeling true world data-generating processes lies at the heart of empirical science. Structural Causal Models (SCMs) and their associated Directed Acyclic Graphs (DAGs) provide an increasingly popular answer to such problems by defining the causal generative process that transforms random noise into observations. However, learning them from observational data poses an ill-posed and NP-hard inverse problem in general. In this work, we propose a new and equivalent formalism that do not require DAGs to describe them, viewed as fixed-point problems on the causally ordered variables, and show three important cases where they can be uniquely recovered given the topological ordering (TO). To the best of our knowledge, we obtain the most general recovery results when the TO is known. Based on our theoretical findings, we design a two-stage causal generative model that first infers the causal order from observations in a zero-shot manner, thus by-passing the search, and then learns the generative fixed-point SCM on the ordered variables. To infer TOs from observations, we propose to amortize the learning of TOs on generated datasets by sequentially predicting the leaves of graphs seen during training. To learn fixed-point SCMs, we design a transformer-based architecture that exploits a new attention mechanism enabling the modeling of causal structures, and show that this parameterization is consistent with our formalism. Finally, we conduct an extensive evaluation of each method individually, and show that when combined, our model outperforms various baselines on generated out-of-distribution problems.","sentences":["Modeling true world data-generating processes lies at the heart of empirical science.","Structural Causal Models (SCMs) and their associated Directed Acyclic Graphs (DAGs) provide an increasingly popular answer to such problems by defining the causal generative process that transforms random noise into observations.","However, learning them from observational data poses an ill-posed and NP-hard inverse problem in general.","In this work, we propose a new and equivalent formalism that do not require DAGs to describe them, viewed as fixed-point problems on the causally ordered variables, and show three important cases where they can be uniquely recovered given the topological ordering (TO).","To the best of our knowledge, we obtain the most general recovery results when the TO is known.","Based on our theoretical findings, we design a two-stage causal generative model that first infers the causal order from observations in a zero-shot manner, thus by-passing the search, and then learns the generative fixed-point SCM on the ordered variables.","To infer TOs from observations, we propose to amortize the learning of TOs on generated datasets by sequentially predicting the leaves of graphs seen during training.","To learn fixed-point SCMs, we design a transformer-based architecture that exploits a new attention mechanism enabling the modeling of causal structures, and show that this parameterization is consistent with our formalism.","Finally, we conduct an extensive evaluation of each method individually, and show that when combined, our model outperforms various baselines on generated out-of-distribution problems."],"url":"http://arxiv.org/abs/2404.06969v1","category":"cs.LG"}
{"created":"2024-04-10 12:25:49","title":"Multiple imputation for longitudinal data: A tutorial","abstract":"Longitudinal studies are frequently used in medical research and involve collecting repeated measures on individuals over time. Observations from the same individual are invariably correlated and thus an analytic approach that accounts for this clustering by individual is required. While almost all research suffers from missing data, this can be particularly problematic in longitudinal studies as participation often becomes harder to maintain over time. Multiple imputation (MI) is widely used to handle missing data in such studies. When using MI, it is important that the imputation model is compatible with the proposed analysis model. In a longitudinal analysis, this implies that the clustering considered in the analysis model should be reflected in the imputation process. Several MI approaches have been proposed to impute incomplete longitudinal data, such as treating repeated measurements of the same variable as distinct variables or using generalized linear mixed imputation models. However, the uptake of these methods has been limited, as they require additional data manipulation and use of advanced imputation procedures. In this tutorial, we review the available MI approaches that can be used for handling incomplete longitudinal data, including where individuals are clustered within higher-level clusters. We illustrate implementation with replicable R and Stata code using a case study from the Childhood to Adolescence Transition Study.","sentences":["Longitudinal studies are frequently used in medical research and involve collecting repeated measures on individuals over time.","Observations from the same individual are invariably correlated and thus an analytic approach that accounts for this clustering by individual is required.","While almost all research suffers from missing data, this can be particularly problematic in longitudinal studies as participation often becomes harder to maintain over time.","Multiple imputation (MI) is widely used to handle missing data in such studies.","When using MI, it is important that the imputation model is compatible with the proposed analysis model.","In a longitudinal analysis, this implies that the clustering considered in the analysis model should be reflected in the imputation process.","Several MI approaches have been proposed to impute incomplete longitudinal data, such as treating repeated measurements of the same variable as distinct variables or using generalized linear mixed imputation models.","However, the uptake of these methods has been limited, as they require additional data manipulation and use of advanced imputation procedures.","In this tutorial, we review the available MI approaches that can be used for handling incomplete longitudinal data, including where individuals are clustered within higher-level clusters.","We illustrate implementation with replicable R and Stata code using a case study from the Childhood to Adolescence Transition Study."],"url":"http://arxiv.org/abs/2404.06967v1","category":"stat.ME"}
{"created":"2024-04-10 12:24:05","title":"Are EEG Sequences Time Series? EEG Classification with Time Series Models and Joint Subject Training","abstract":"As with most other data domains, EEG data analysis relies on rich domain-specific preprocessing. Beyond such preprocessing, machine learners would hope to deal with such data as with any other time series data. For EEG classification many models have been developed with layer types and architectures we typically do not see in time series classification. Furthermore, typically separate models for each individual subject are learned, not one model for all of them. In this paper, we systematically study the differences between EEG classification models and generic time series classification models. We describe three different model setups to deal with EEG data from different subjects, subject-specific models (most EEG literature), subject-agnostic models and subject-conditional models. In experiments on three datasets, we demonstrate that off-the-shelf time series classification models trained per subject perform close to EEG classification models, but that do not quite reach the performance of domain-specific modeling. Additionally, we combine time-series models with subject embeddings to train one joint subject-conditional classifier on all subjects. The resulting models are competitive with dedicated EEG models in 2 out of 3 datasets, even outperforming all EEG methods on one of them.","sentences":["As with most other data domains, EEG data analysis relies on rich domain-specific preprocessing.","Beyond such preprocessing, machine learners would hope to deal with such data as with any other time series data.","For EEG classification many models have been developed with layer types and architectures we typically do not see in time series classification.","Furthermore, typically separate models for each individual subject are learned, not one model for all of them.","In this paper, we systematically study the differences between EEG classification models and generic time series classification models.","We describe three different model setups to deal with EEG data from different subjects, subject-specific models (most EEG literature), subject-agnostic models and subject-conditional models.","In experiments on three datasets, we demonstrate that off-the-shelf time series classification models trained per subject perform close to EEG classification models, but that do not quite reach the performance of domain-specific modeling.","Additionally, we combine time-series models with subject embeddings to train one joint subject-conditional classifier on all subjects.","The resulting models are competitive with dedicated EEG models in 2 out of 3 datasets, even outperforming all EEG methods on one of them."],"url":"http://arxiv.org/abs/2404.06966v1","category":"cs.LG"}
{"created":"2024-04-10 12:22:03","title":"Advancing Real-time Pandemic Forecasting Using Large Language Models: A COVID-19 Case Study","abstract":"Forecasting the short-term spread of an ongoing disease outbreak is a formidable challenge due to the complexity of contributing factors, some of which can be characterized through interlinked, multi-modality variables such as epidemiological time series data, viral biology, population demographics, and the intersection of public policy and human behavior. Existing forecasting model frameworks struggle with the multifaceted nature of relevant data and robust results translation, which hinders their performances and the provision of actionable insights for public health decision-makers. Our work introduces PandemicLLM, a novel framework with multi-modal Large Language Models (LLMs) that reformulates real-time forecasting of disease spread as a text reasoning problem, with the ability to incorporate real-time, complex, non-numerical information that previously unattainable in traditional forecasting models. This approach, through a unique AI-human cooperative prompt design and time series representation learning, encodes multi-modal data for LLMs. The model is applied to the COVID-19 pandemic, and trained to utilize textual public health policies, genomic surveillance, spatial, and epidemiological time series data, and is subsequently tested across all 50 states of the U.S. Empirically, PandemicLLM is shown to be a high-performing pandemic forecasting framework that effectively captures the impact of emerging variants and can provide timely and accurate predictions. The proposed PandemicLLM opens avenues for incorporating various pandemic-related data in heterogeneous formats and exhibits performance benefits over existing models. This study illuminates the potential of adapting LLMs and representation learning to enhance pandemic forecasting, illustrating how AI innovations can strengthen pandemic responses and crisis management in the future.","sentences":["Forecasting the short-term spread of an ongoing disease outbreak is a formidable challenge due to the complexity of contributing factors, some of which can be characterized through interlinked, multi-modality variables such as epidemiological time series data, viral biology, population demographics, and the intersection of public policy and human behavior.","Existing forecasting model frameworks struggle with the multifaceted nature of relevant data and robust results translation, which hinders their performances and the provision of actionable insights for public health decision-makers.","Our work introduces PandemicLLM, a novel framework with multi-modal Large Language Models (LLMs) that reformulates real-time forecasting of disease spread as a text reasoning problem, with the ability to incorporate real-time, complex, non-numerical information that previously unattainable in traditional forecasting models.","This approach, through a unique AI-human cooperative prompt design and time series representation learning, encodes multi-modal data for LLMs.","The model is applied to the COVID-19 pandemic, and trained to utilize textual public health policies, genomic surveillance, spatial, and epidemiological time series data, and is subsequently tested across all 50 states of the U.S. Empirically, PandemicLLM is shown to be a high-performing pandemic forecasting framework that effectively captures the impact of emerging variants and can provide timely and accurate predictions.","The proposed PandemicLLM opens avenues for incorporating various pandemic-related data in heterogeneous formats and exhibits performance benefits over existing models.","This study illuminates the potential of adapting LLMs and representation learning to enhance pandemic forecasting, illustrating how AI innovations can strengthen pandemic responses and crisis management in the future."],"url":"http://arxiv.org/abs/2404.06962v1","category":"cs.LG"}
{"created":"2024-04-10 12:18:22","title":"Regular inclusions of simple unital $C^*$-algebras","abstract":"We prove that an inclusion $\\mathcal{B} \\subset \\mathcal{A}$ of simple unital $C^*$-algebras with a finite-index conditional expectation is regular if and only if there exists a finite group $G$ that admits a cocycle action $(\\alpha,\\sigma)$ on the intermediate $C^*$-subalgebra $\\mathcal{C}$ generated by $\\mathcal{B}$ and its centralizer $\\mathcal{C}_\\mathcal{A}(\\mathcal{B})$ such that $\\mathcal{B}$ is outerly $\\alpha$-invariant and $(\\mathcal{B} \\subset \\mathcal{A}) \\cong ( \\mathcal{B} \\subset \\mathcal{C}\\rtimes^r_{\\alpha, \\sigma} G)$. Prior to this characterization, we prove the existence of two-sided and unitary quasi-bases for the minimal conditional expectation of any such inclusion, and also show that such an inclusion has integer Watatani index and depth at most $2$.","sentences":["We prove that an inclusion $\\mathcal{B} \\subset \\mathcal{A}$ of simple unital $C^*$-algebras with a finite-index conditional expectation is regular if and only if there exists a finite group $G$ that admits a cocycle action $(\\alpha,\\sigma)$ on the intermediate $C^*$-subalgebra $\\mathcal{C}$ generated by $\\mathcal{B}$ and its centralizer $\\mathcal{C}_\\mathcal{A}(\\mathcal{B})$ such that $\\mathcal{B}$ is outerly $\\alpha$-invariant and $(\\mathcal{B} \\subset \\mathcal{A})","\\cong ( \\mathcal{B} \\subset \\mathcal{C}\\rtimes^r_{\\alpha, \\sigma} G)$. Prior to this characterization, we prove the existence of two-sided and unitary quasi-bases for the minimal conditional expectation of any such inclusion, and also show that such an inclusion has integer Watatani index and depth at most $2$."],"url":"http://arxiv.org/abs/2404.06959v1","category":"math.OA"}
{"created":"2024-04-10 12:17:25","title":"Adversarial purification for no-reference image-quality metrics: applicability study and new methods","abstract":"Recently, the area of adversarial attacks on image quality metrics has begun to be explored, whereas the area of defences remains under-researched. In this study, we aim to cover that case and check the transferability of adversarial purification defences from image classifiers to IQA methods. In this paper, we apply several widespread attacks on IQA models and examine the success of the defences against them. The purification methodologies covered different preprocessing techniques, including geometrical transformations, compression, denoising, and modern neural network-based methods. Also, we address the challenge of assessing the efficacy of a defensive methodology by proposing ways to estimate output visual quality and the success of neutralizing attacks. Defences were tested against attack on three IQA metrics -- Linearity, MetaIQA and SPAQ. The code for attacks and defences is available at: (link is hidden for a blind review).","sentences":["Recently, the area of adversarial attacks on image quality metrics has begun to be explored, whereas the area of defences remains under-researched.","In this study, we aim to cover that case and check the transferability of adversarial purification defences from image classifiers to IQA methods.","In this paper, we apply several widespread attacks on IQA models and examine the success of the defences against them.","The purification methodologies covered different preprocessing techniques, including geometrical transformations, compression, denoising, and modern neural network-based methods.","Also, we address the challenge of assessing the efficacy of a defensive methodology by proposing ways to estimate output visual quality and the success of neutralizing attacks.","Defences were tested against attack on three IQA metrics -- Linearity, MetaIQA and SPAQ.","The code for attacks and defences is available at: (link is hidden for a blind review)."],"url":"http://arxiv.org/abs/2404.06957v1","category":"cs.CV"}
{"created":"2024-04-10 12:12:50","title":"Untangling Critical Interaction with AI in Students Written Assessment","abstract":"Artificial Intelligence (AI) has become a ubiquitous part of society, but a key challenge exists in ensuring that humans are equipped with the required critical thinking and AI literacy skills to interact with machines effectively by understanding their capabilities and limitations. These skills are particularly important for learners to develop in the age of generative AI where AI tools can demonstrate complex knowledge and ability previously thought to be uniquely human. To activate effective human-AI partnerships in writing, this paper provides a first step toward conceptualizing the notion of critical learner interaction with AI. Using both theoretical models and empirical data, our preliminary findings suggest a general lack of Deep interaction with AI during the writing process. We believe that the outcomes can lead to better task and tool design in the future for learners to develop deep, critical thinking when interacting with AI.","sentences":["Artificial Intelligence (AI) has become a ubiquitous part of society, but a key challenge exists in ensuring that humans are equipped with the required critical thinking and AI literacy skills to interact with machines effectively by understanding their capabilities and limitations.","These skills are particularly important for learners to develop in the age of generative AI where AI tools can demonstrate complex knowledge and ability previously thought to be uniquely human.","To activate effective human-AI partnerships in writing, this paper provides a first step toward conceptualizing the notion of critical learner interaction with AI.","Using both theoretical models and empirical data, our preliminary findings suggest a general lack of Deep interaction with AI during the writing process.","We believe that the outcomes can lead to better task and tool design in the future for learners to develop deep, critical thinking when interacting with AI."],"url":"http://arxiv.org/abs/2404.06955v1","category":"cs.HC"}
{"created":"2024-04-10 12:12:07","title":"Accelerating Inference in Large Language Models with a Unified Layer Skipping Strategy","abstract":"Recently, dynamic computation methods have shown notable acceleration for Large Language Models (LLMs) by skipping several layers of computations through elaborate heuristics or additional predictors. However, in the decoding process of existing approaches, different samples are assigned different computational budgets, which cannot guarantee a stable and precise acceleration effect. Furthermore, existing approaches generally skip multiple contiguous layers at the bottom or top of the layers, leading to a drastic change in the model's layer-wise representations, and thus a consequent performance degeneration. Therefore, we propose a Unified Layer Skipping strategy, which selects the number of layers to skip computation based solely on the target speedup ratio, and then skips the corresponding number of intermediate layer computations in a balanced manner. Since the Unified Layer Skipping strategy is independent of input samples, it naturally supports popular acceleration techniques such as batch decoding and KV caching, thus demonstrating more practicality for real-world applications. Experimental results on two common tasks, i.e., machine translation and text summarization, indicate that given a target speedup ratio, the Unified Layer Skipping strategy significantly enhances both the inference performance and the actual model throughput over existing dynamic approaches.","sentences":["Recently, dynamic computation methods have shown notable acceleration for Large Language Models (LLMs) by skipping several layers of computations through elaborate heuristics or additional predictors.","However, in the decoding process of existing approaches, different samples are assigned different computational budgets, which cannot guarantee a stable and precise acceleration effect.","Furthermore, existing approaches generally skip multiple contiguous layers at the bottom or top of the layers, leading to a drastic change in the model's layer-wise representations, and thus a consequent performance degeneration.","Therefore, we propose a Unified Layer Skipping strategy, which selects the number of layers to skip computation based solely on the target speedup ratio, and then skips the corresponding number of intermediate layer computations in a balanced manner.","Since the Unified Layer Skipping strategy is independent of input samples, it naturally supports popular acceleration techniques such as batch decoding and KV caching, thus demonstrating more practicality for real-world applications.","Experimental results on two common tasks, i.e., machine translation and text summarization, indicate that given a target speedup ratio, the Unified Layer Skipping strategy significantly enhances both the inference performance and the actual model throughput over existing dynamic approaches."],"url":"http://arxiv.org/abs/2404.06954v1","category":"cs.CL"}
{"created":"2024-04-10 12:03:51","title":"Perfectly Secure Key Agreement Over a Full Duplex Wireless Channel","abstract":"Secret key generation (SKG) between authenticated devices is a pivotal task for secure communications. Diffie-Hellman (DH) is de-facto standard but not post-quantum secure. In this paper, we shall invent and analyze a new security primitive that is specifically designed for WPAN. For WPAN, wireless channel-based SKG has been proposed but was not widely deployed due to its critical dependence on the channel's entropy which is uncontrollable. We formulate a different approach: We still exploit channel properties but mainly hinge on the reciprocity of the wireless channel and not on the channel's entropy. The radio advantage comes from the use of full duplex communication. We show that in this situation both legitimate parties can agree on a common secret key even without ever probing the channel at all. At the core is a new bisparse blind deconvolution scheme for which we prove correctness and information-theoretic, i.e. perfect, security. We show that, ultimately, a secret key can be extracted and give a lower bound for the number of secret key bits which is then verified by experiments.","sentences":["Secret key generation (SKG) between authenticated devices is a pivotal task for secure communications.","Diffie-Hellman (DH) is de-facto standard but not post-quantum secure.","In this paper, we shall invent and analyze a new security primitive that is specifically designed for WPAN.","For WPAN, wireless channel-based SKG has been proposed but was not widely deployed due to its critical dependence on the channel's entropy which is uncontrollable.","We formulate a different approach: We still exploit channel properties but mainly hinge on the reciprocity of the wireless channel and not on the channel's entropy.","The radio advantage comes from the use of full duplex communication.","We show that in this situation both legitimate parties can agree on a common secret key even without ever probing the channel at all.","At the core is a new bisparse blind deconvolution scheme for which we prove correctness and information-theoretic, i.e. perfect, security.","We show that, ultimately, a secret key can be extracted and give a lower bound for the number of secret key bits which is then verified by experiments."],"url":"http://arxiv.org/abs/2404.06952v1","category":"cs.IT"}
{"created":"2024-04-10 11:56:01","title":"MetaCheckGPT -- A Multi-task Hallucination Detection Using LLM Uncertainty and Meta-models","abstract":"This paper presents our winning solution for the SemEval-2024 Task 6 competition. We propose a meta-regressor framework of large language models (LLMs) for model evaluation and integration that achieves the highest scores on the leader board. Our approach leverages uncertainty signals present in a diverse basket of LLMs to detect hallucinations more robustly.","sentences":["This paper presents our winning solution for the SemEval-2024 Task 6 competition.","We propose a meta-regressor framework of large language models (LLMs) for model evaluation and integration that achieves the highest scores on the leader board.","Our approach leverages uncertainty signals present in a diverse basket of LLMs to detect hallucinations more robustly."],"url":"http://arxiv.org/abs/2404.06948v1","category":"cs.CL"}
{"created":"2024-04-10 11:55:33","title":"A Survey on the Integration of Generative AI for Critical Thinking in Mobile Networks","abstract":"In the near future, mobile networks are expected to broaden their services and coverage to accommodate a larger user base and diverse user needs. Thus, they will increasingly rely on artificial intelligence (AI) to manage network operation and control costs, undertaking complex decision-making roles. This shift will necessitate the application of techniques that incorporate critical thinking abilities, including reasoning and planning. Symbolic AI techniques already facilitate critical thinking based on existing knowledge. Yet, their use in telecommunications is hindered by the high cost of mostly manual curation of this knowledge and high computational complexity of reasoning tasks. At the same time, there is a spurt of innovations in industries such as telecommunications due to Generative AI (GenAI) technologies, operating independently of human-curated knowledge. However, their capacity for critical thinking remains uncertain. This paper aims to address this gap by examining the current status of GenAI algorithms with critical thinking capabilities and investigating their potential applications in telecom networks. Specifically, the aim of this study is to offer an introduction to the potential utilization of GenAI for critical thinking techniques in mobile networks, while also establishing a foundation for future research.","sentences":["In the near future, mobile networks are expected to broaden their services and coverage to accommodate a larger user base and diverse user needs.","Thus, they will increasingly rely on artificial intelligence (AI) to manage network operation and control costs, undertaking complex decision-making roles.","This shift will necessitate the application of techniques that incorporate critical thinking abilities, including reasoning and planning.","Symbolic AI techniques already facilitate critical thinking based on existing knowledge.","Yet, their use in telecommunications is hindered by the high cost of mostly manual curation of this knowledge and high computational complexity of reasoning tasks.","At the same time, there is a spurt of innovations in industries such as telecommunications due to Generative AI (GenAI) technologies, operating independently of human-curated knowledge.","However, their capacity for critical thinking remains uncertain.","This paper aims to address this gap by examining the current status of GenAI algorithms with critical thinking capabilities and investigating their potential applications in telecom networks.","Specifically, the aim of this study is to offer an introduction to the potential utilization of GenAI for critical thinking techniques in mobile networks, while also establishing a foundation for future research."],"url":"http://arxiv.org/abs/2404.06946v1","category":"cs.AI"}
{"created":"2024-04-10 11:51:20","title":"Automorphisms of the generalized cluster complex","abstract":"It is proved that the generalized cluster complex defined by Fomin and Reading has a dihedral symmetry. Together with diagram symmetries, they generate its automorphism group. A consequence is a simple explicit formula for the order of this automorphism group.","sentences":["It is proved that the generalized cluster complex defined by Fomin and Reading has a dihedral symmetry.","Together with diagram symmetries, they generate its automorphism group.","A consequence is a simple explicit formula for the order of this automorphism group."],"url":"http://arxiv.org/abs/2404.06945v1","category":"math.CO"}
{"created":"2024-04-10 11:49:10","title":"Log-Lipschitz and H\u00f6lder regularity imply smoothness for complex analytic sets","abstract":"In this paper, we prove metric analogues, in any dimension and in any co-dimension, of the famous Theorem of Mumford on smoothness of normal surfaces and the beautiful Theorem of Ramanujam that gives a topological characterization of $\\mathbb{C}^2$ as an algebraic surface. For instance, we prove that a complex analytic set that is log-Lipschitz regular at 0 (i.e., a complex analytic set that has a neighbourhood of the origin which bi-log-Lipschitz homeomorphic to an Euclidean ball) must be smooth at 0. We prove even more, we prove that if a complex analytic set $X$ such that, for each $0<\\alpha<1$, $(X,0)$ and $(\\mathbb{R}^k,0)$ are bi-$\\alpha$-H\\\"older homeomorphic, then $X$ must be smooth at 0. These results generalize the Lipschitz Regularity Theorem, which says that a Lipschitz regular complex analytic set must be smooth. Global versions of these results are also presented here and, in particular, we obtain a characterization of an affine linear subspace as a pure-dimensional entire complex analytic set.","sentences":["In this paper, we prove metric analogues, in any dimension and in any co-dimension, of the famous Theorem of Mumford on smoothness of normal surfaces and the beautiful Theorem of Ramanujam that gives a topological characterization of $\\mathbb{C}^2$ as an algebraic surface.","For instance, we prove that a complex analytic set that is log-Lipschitz regular at 0 (i.e., a complex analytic set that has a neighbourhood of the origin which bi-log-Lipschitz homeomorphic to an Euclidean ball) must be smooth at 0.","We prove even more, we prove that if a complex analytic set $X$ such that, for each $0<\\alpha<1$, $(X,0)$ and $(\\mathbb{R}^k,0)$ are bi-$\\alpha$-H\\\"older homeomorphic, then $X$ must be smooth at 0.","These results generalize the Lipschitz Regularity Theorem, which says that a Lipschitz regular complex analytic set must be smooth.","Global versions of these results are also presented here and, in particular, we obtain a characterization of an affine linear subspace as a pure-dimensional entire complex analytic set."],"url":"http://arxiv.org/abs/2404.06943v1","category":"math.AG"}
{"created":"2024-04-10 11:45:31","title":"Robotic Learning for Adaptive Informative Path Planning","abstract":"Adaptive informative path planning (AIPP) is important to many robotics applications, enabling mobile robots to efficiently collect useful data about initially unknown environments. In addition, learning-based methods are increasingly used in robotics to enhance adaptability, versatility, and robustness across diverse and complex tasks. Our survey explores research on applying robotic learning to AIPP, bridging the gap between these two research fields. We begin by providing a unified mathematical framework for general AIPP problems. Next, we establish two complementary taxonomies of current work from the perspectives of (i) learning algorithms and (ii) robotic applications. We explore synergies, recent trends, and highlight the benefits of learning-based methods in AIPP frameworks. Finally, we discuss key challenges and promising future directions to enable more generally applicable and robust robotic data-gathering systems through learning. We provide a comprehensive catalogue of papers reviewed in our survey, including publicly available repositories, to facilitate future studies in the field.","sentences":["Adaptive informative path planning (AIPP) is important to many robotics applications, enabling mobile robots to efficiently collect useful data about initially unknown environments.","In addition, learning-based methods are increasingly used in robotics to enhance adaptability, versatility, and robustness across diverse and complex tasks.","Our survey explores research on applying robotic learning to AIPP, bridging the gap between these two research fields.","We begin by providing a unified mathematical framework for general AIPP problems.","Next, we establish two complementary taxonomies of current work from the perspectives of (i) learning algorithms and (ii) robotic applications.","We explore synergies, recent trends, and highlight the benefits of learning-based methods in AIPP frameworks.","Finally, we discuss key challenges and promising future directions to enable more generally applicable and robust robotic data-gathering systems through learning.","We provide a comprehensive catalogue of papers reviewed in our survey, including publicly available repositories, to facilitate future studies in the field."],"url":"http://arxiv.org/abs/2404.06940v1","category":"cs.RO"}
{"created":"2024-04-10 11:43:26","title":"Fast System Technology Co-Optimization Framework for Emerging Technology Based on Graph Neural Networks","abstract":"This paper proposes a fast system technology co-optimization (STCO) framework that optimizes power, performance, and area (PPA) for next-generation IC design, addressing the challenges and opportunities presented by novel materials and device architectures. We focus on accelerating the technology level of STCO using AI techniques, by employing graph neural network (GNN)-based approaches for both TCAD simulation and cell library characterization, which are interconnected through a unified compact model, collectively achieving over a 100X speedup over traditional methods. These advancements enable comprehensive STCO iterations with runtime speedups ranging from 1.9X to 14.1X and supports both emerging and traditional technologies.","sentences":["This paper proposes a fast system technology co-optimization (STCO) framework that optimizes power, performance, and area (PPA) for next-generation IC design, addressing the challenges and opportunities presented by novel materials and device architectures.","We focus on accelerating the technology level of STCO using AI techniques, by employing graph neural network (GNN)-based approaches for both TCAD simulation and cell library characterization, which are interconnected through a unified compact model, collectively achieving over a 100X speedup over traditional methods.","These advancements enable comprehensive STCO iterations with runtime speedups ranging from 1.9X to 14.1X and supports both emerging and traditional technologies."],"url":"http://arxiv.org/abs/2404.06939v1","category":"cs.ET"}
{"created":"2024-04-10 11:40:02","title":"Efficient and Generic Point Model for Lossless Point Cloud Attribute Compression","abstract":"The past several years have witnessed the emergence of learned point cloud compression (PCC) techniques. However, current learning-based lossless point cloud attribute compression (PCAC) methods either suffer from high computational complexity or deteriorated compression performance. Moreover, the significant variations in point cloud scale and sparsity encountered in real-world applications make developing an all-in-one neural model a challenging task. In this paper, we propose PoLoPCAC, an efficient and generic lossless PCAC method that achieves high compression efficiency and strong generalizability simultaneously. We formulate lossless PCAC as the task of inferring explicit distributions of attributes from group-wise autoregressive priors. A progressive random grouping strategy is first devised to efficiently resolve the point cloud into groups, and then the attributes of each group are modeled sequentially from accumulated antecedents. A locality-aware attention mechanism is utilized to exploit prior knowledge from context windows in parallel. Since our method directly operates on points, it can naturally avoids distortion caused by voxelization, and can be executed on point clouds with arbitrary scale and density. Experiments show that our method can be instantly deployed once trained on a Synthetic 2k-ShapeNet dataset while enjoying continuous bit-rate reduction over the latest G-PCCv23 on various datasets (ShapeNet, ScanNet, MVUB, 8iVFB). Meanwhile, our method reports shorter coding time than G-PCCv23 on the majority of sequences with a lightweight model size (2.6MB), which is highly attractive for practical applications. Dataset, code and trained model are available at https://github.com/I2-Multimedia-Lab/PoLoPCAC.","sentences":["The past several years have witnessed the emergence of learned point cloud compression (PCC) techniques.","However, current learning-based lossless point cloud attribute compression (PCAC) methods either suffer from high computational complexity or deteriorated compression performance.","Moreover, the significant variations in point cloud scale and sparsity encountered in real-world applications make developing an all-in-one neural model a challenging task.","In this paper, we propose PoLoPCAC, an efficient and generic lossless PCAC method that achieves high compression efficiency and strong generalizability simultaneously.","We formulate lossless PCAC as the task of inferring explicit distributions of attributes from group-wise autoregressive priors.","A progressive random grouping strategy is first devised to efficiently resolve the point cloud into groups, and then the attributes of each group are modeled sequentially from accumulated antecedents.","A locality-aware attention mechanism is utilized to exploit prior knowledge from context windows in parallel.","Since our method directly operates on points, it can naturally avoids distortion caused by voxelization, and can be executed on point clouds with arbitrary scale and density.","Experiments show that our method can be instantly deployed once trained on a Synthetic 2k-ShapeNet dataset while enjoying continuous bit-rate reduction over the latest G-PCCv23 on various datasets (ShapeNet, ScanNet, MVUB, 8iVFB).","Meanwhile, our method reports shorter coding time than G-PCCv23 on the majority of sequences with a lightweight model size (2.6MB), which is highly attractive for practical applications.","Dataset, code and trained model are available at https://github.com/I2-Multimedia-Lab/PoLoPCAC."],"url":"http://arxiv.org/abs/2404.06936v1","category":"cs.CV"}
{"created":"2024-04-10 11:35:39","title":"Homogeneous spacetime with shear viscosity","abstract":"We study the homogeneous and anisotropic evolution of spacetime driven by perfect fluid with shear viscosity. We obtain exact solutions by considering the simplest form of the equation of state wherein the pressure and the shear stress are proportional to the energy density individually. The solutions represent Bianchi type-I cosmology of which the special case becomes Bianchi type-VII. We find that the initial singularity can be removed only for the Bianchi type-VII.","sentences":["We study the homogeneous and anisotropic evolution of spacetime driven by perfect fluid with shear viscosity.","We obtain exact solutions by considering the simplest form of the equation of state wherein the pressure and the shear stress are proportional to the energy density individually.","The solutions represent Bianchi type-I cosmology of which the special case becomes Bianchi type-VII.","We find that the initial singularity can be removed only for the Bianchi type-VII."],"url":"http://arxiv.org/abs/2404.06934v1","category":"gr-qc"}
{"created":"2024-04-10 11:27:01","title":"Gravitational Wave Memory Imprints on the CMB from Populations of Massive Black Hole Mergers","abstract":"Aims: To showcase and characterise the rich phenomenology of temperature fluctuation patterns that are imprinted on the CMB by the gravitational wave memory (GWM) of massive black hole (BH) mergers. Methods: We analyse both individual binaries as well as populations of binaries, distributed in local cosmological boxes at a given redshift. Results: The magnitude of the temperature fluctuations scales primarily as a function of binary total mass and pattern angular scale, and accumulates as a random-walk process when populations of mergers are considered. Fluctuations of order $\\sim 10^{-10}$ K are easily reached across scales of $\\sim 1'$ to $\\sim 1^{\\circ}$ for realistic volumetric merger rates of 10$^{-3}$ Mpc$^{-3}$ Gyr$^{-1}$, as appropriate for massive galaxies at $z=1$. We determine numerically that GWM temperature fluctuations result in a universal power spectrum with a scaling of $P(k)\\propto k^{-2.7}$. Conclusion: While not detectable given the limitations of current all-sky CMB surveys, our work explicitly shows how every black hole merger in the Universe left us its unique faint signature.","sentences":["Aims: To showcase and characterise the rich phenomenology of temperature fluctuation patterns that are imprinted on the CMB by the gravitational wave memory (GWM) of massive black hole (BH) mergers.","Methods: We analyse both individual binaries as well as populations of binaries, distributed in local cosmological boxes at a given redshift.","Results: The magnitude of the temperature fluctuations scales primarily as a function of binary total mass and pattern angular scale, and accumulates as a random-walk process when populations of mergers are considered.","Fluctuations of order $\\sim 10^{-10}$ K are easily reached across scales of $\\sim 1'$ to $\\sim 1^{\\circ}$ for realistic volumetric merger rates of 10$^{-3}$ Mpc$^{-3}$ Gyr$^{-1}$, as appropriate for massive galaxies at $z=1$. We determine numerically that GWM temperature fluctuations result in a universal power spectrum with a scaling of $P(k)\\propto k^{-2.7}$. Conclusion: While not detectable given the limitations of current all-sky CMB surveys, our work explicitly shows how every black hole merger in the Universe left us its unique faint signature."],"url":"http://arxiv.org/abs/2404.06927v1","category":"astro-ph.CO"}
{"created":"2024-04-10 11:17:33","title":"GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications","abstract":"Large Language Models (LLMs) are evolving beyond their classical role of providing information within dialogue systems to actively engaging with tools and performing actions on real-world applications and services. Today, humans verify the correctness and appropriateness of the LLM-generated outputs (e.g., code, functions, or actions) before putting them into real-world execution. This poses significant challenges as code comprehension is well known to be notoriously difficult. In this paper, we study how humans can efficiently collaborate with, delegate to, and supervise autonomous LLMs in the future. We argue that in many cases, \"post-facto validation\" - verifying the correctness of a proposed action after seeing the output - is much easier than the aforementioned \"pre-facto validation\" setting. The core concept behind enabling a post-facto validation system is the integration of an intuitive undo feature, and establishing a damage confinement for the LLM-generated actions as effective strategies to mitigate the associated risks. Using this, a human can now either revert the effect of an LLM-generated output or be confident that the potential risk is bounded. We believe this is critical to unlock the potential for LLM agents to interact with applications and services with limited (post-facto) human involvement. We describe the design and implementation of our open-source runtime for executing LLM actions, Gorilla Execution Engine (GoEX), and present open research questions towards realizing the goal of LLMs and applications interacting with each other with minimal human supervision. We release GoEX at https://github.com/ShishirPatil/gorilla/.","sentences":["Large Language Models (LLMs) are evolving beyond their classical role of providing information within dialogue systems to actively engaging with tools and performing actions on real-world applications and services.","Today, humans verify the correctness and appropriateness of the LLM-generated outputs (e.g., code, functions, or actions) before putting them into real-world execution.","This poses significant challenges as code comprehension is well known to be notoriously difficult.","In this paper, we study how humans can efficiently collaborate with, delegate to, and supervise autonomous LLMs in the future.","We argue that in many cases, \"post-facto validation\" - verifying the correctness of a proposed action after seeing the output - is much easier than the aforementioned \"pre-facto validation\" setting.","The core concept behind enabling a post-facto validation system is the integration of an intuitive undo feature, and establishing a damage confinement for the LLM-generated actions as effective strategies to mitigate the associated risks.","Using this, a human can now either revert the effect of an LLM-generated output or be confident that the potential risk is bounded.","We believe this is critical to unlock the potential for LLM agents to interact with applications and services with limited (post-facto) human involvement.","We describe the design and implementation of our open-source runtime for executing LLM actions, Gorilla Execution Engine (GoEX), and present open research questions towards realizing the goal of LLMs and applications interacting with each other with minimal human supervision.","We release GoEX at https://github.com/ShishirPatil/gorilla/."],"url":"http://arxiv.org/abs/2404.06921v1","category":"cs.CL"}
{"created":"2024-04-10 11:10:50","title":"HRVDA: High-Resolution Visual Document Assistant","abstract":"Leveraging vast training data, multimodal large language models (MLLMs) have demonstrated formidable general visual comprehension capabilities and achieved remarkable performance across various tasks. However, their performance in visual document understanding still leaves much room for improvement. This discrepancy is primarily attributed to the fact that visual document understanding is a fine-grained prediction task. In natural scenes, MLLMs typically use low-resolution images, leading to a substantial loss of visual information. Furthermore, general-purpose MLLMs do not excel in handling document-oriented instructions. In this paper, we propose a High-Resolution Visual Document Assistant (HRVDA), which bridges the gap between MLLMs and visual document understanding. This model employs a content filtering mechanism and an instruction filtering module to separately filter out the content-agnostic visual tokens and instruction-agnostic visual tokens, thereby achieving efficient model training and inference for high-resolution images. In addition, we construct a document-oriented visual instruction tuning dataset and apply a multi-stage training strategy to enhance the model's document modeling capabilities. Extensive experiments demonstrate that our model achieves state-of-the-art performance across multiple document understanding datasets, while maintaining training efficiency and inference speed comparable to low-resolution models.","sentences":["Leveraging vast training data, multimodal large language models (MLLMs) have demonstrated formidable general visual comprehension capabilities and achieved remarkable performance across various tasks.","However, their performance in visual document understanding still leaves much room for improvement.","This discrepancy is primarily attributed to the fact that visual document understanding is a fine-grained prediction task.","In natural scenes, MLLMs typically use low-resolution images, leading to a substantial loss of visual information.","Furthermore, general-purpose MLLMs do not excel in handling document-oriented instructions.","In this paper, we propose a High-Resolution Visual Document Assistant (HRVDA), which bridges the gap between MLLMs and visual document understanding.","This model employs a content filtering mechanism and an instruction filtering module to separately filter out the content-agnostic visual tokens and instruction-agnostic visual tokens, thereby achieving efficient model training and inference for high-resolution images.","In addition, we construct a document-oriented visual instruction tuning dataset and apply a multi-stage training strategy to enhance the model's document modeling capabilities.","Extensive experiments demonstrate that our model achieves state-of-the-art performance across multiple document understanding datasets, while maintaining training efficiency and inference speed comparable to low-resolution models."],"url":"http://arxiv.org/abs/2404.06918v1","category":"cs.CV"}
{"created":"2024-04-10 11:09:04","title":"Ordering kinetics with long-range interactions: interpolating between voter and Ising models","abstract":"We study the ordering kinetics of a generalization of the voter model with long-range interactions, the $p$-voter model, in one dimension. It is defined in terms of boolean variables $S_{i}$, agents or spins, located on sites $i$ of a lattice, each of which takes in an elementary move the state of the majority of $p$ other agents at distances $r$ chosen with probability $P(r)\\propto r^{-\\alpha}$. For $p=2$ the model can be exactly mapped onto the case with $p=1$, which amounts to the voter model with long-range interactions decaying algebraically. For $3\\le p<\\infty$, instead, the dynamics falls into the universality class of the one-dimensional Ising model with long-ranged coupling constant $J(r)=P(r)$ quenched to small finite temperatures. In the limit $p\\to \\infty$, a crossover to the (different) behavior of the long-range Ising model quenched to zero temperature is observed.","sentences":["We study the ordering kinetics of a generalization of the voter model with long-range interactions, the $p$-voter model, in one dimension.","It is defined in terms of boolean variables $S_{i}$, agents or spins, located on sites $i$ of a lattice, each of which takes in an elementary move the state of the majority of $p$ other agents at distances $r$ chosen with probability $P(r)\\propto r^{-\\alpha}$. For $p=2$ the model can be exactly mapped onto the case with $p=1$, which amounts to the voter model with long-range interactions decaying algebraically.","For $3\\le p<\\infty$, instead, the dynamics falls into the universality class of the one-dimensional Ising model with long-ranged coupling constant $J(r)=P(r)$ quenched to small finite temperatures.","In the limit $p\\to \\infty$, a crossover to the (different) behavior of the long-range Ising model quenched to zero temperature is observed."],"url":"http://arxiv.org/abs/2404.06917v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-10 11:06:29","title":"Sparse Global Matching for Video Frame Interpolation with Large Motion","abstract":"Large motion poses a critical challenge in Video Frame Interpolation (VFI) task. Existing methods are often constrained by limited receptive fields, resulting in sub-optimal performance when handling scenarios with large motion. In this paper, we introduce a new pipeline for VFI, which can effectively integrate global-level information to alleviate issues associated with large motion. Specifically, we first estimate a pair of initial intermediate flows using a high-resolution feature map for extracting local details. Then, we incorporate a sparse global matching branch to compensate for flow estimation, which consists of identifying flaws in initial flows and generating sparse flow compensation with a global receptive field. Finally, we adaptively merge the initial flow estimation with global flow compensation, yielding a more accurate intermediate flow. To evaluate the effectiveness of our method in handling large motion, we carefully curate a more challenging subset from commonly used benchmarks. Our method demonstrates the state-of-the-art performance on these VFI subsets with large motion.","sentences":["Large motion poses a critical challenge in Video Frame Interpolation (VFI) task.","Existing methods are often constrained by limited receptive fields, resulting in sub-optimal performance when handling scenarios with large motion.","In this paper, we introduce a new pipeline for VFI, which can effectively integrate global-level information to alleviate issues associated with large motion.","Specifically, we first estimate a pair of initial intermediate flows using a high-resolution feature map for extracting local details.","Then, we incorporate a sparse global matching branch to compensate for flow estimation, which consists of identifying flaws in initial flows and generating sparse flow compensation with a global receptive field.","Finally, we adaptively merge the initial flow estimation with global flow compensation, yielding a more accurate intermediate flow.","To evaluate the effectiveness of our method in handling large motion, we carefully curate a more challenging subset from commonly used benchmarks.","Our method demonstrates the state-of-the-art performance on these VFI subsets with large motion."],"url":"http://arxiv.org/abs/2404.06913v1","category":"cs.CV"}
{"created":"2024-04-10 11:03:57","title":"GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism","abstract":"Pretrained Language Models (PLMs) benefit from external knowledge stored in graph structures for various downstream tasks. However, bridging the modality gap between graph structures and text remains a significant challenge. Traditional methods like linearizing graphs for PLMs lose vital graph connectivity, whereas Graph Neural Networks (GNNs) require cumbersome processes for integration into PLMs. In this work, we propose a novel graph-guided self-attention mechanism, GraSAME. GraSAME seamlessly incorporates token-level structural information into PLMs without necessitating additional alignment or concatenation efforts. As an end-to-end, lightweight multimodal module, GraSAME follows a multi-task learning strategy and effectively bridges the gap between graph and textual modalities, facilitating dynamic interactions between GNNs and PLMs. Our experiments on the graph-to-text generation task demonstrate that GraSAME outperforms baseline models and achieves results comparable to state-of-the-art (SOTA) models on WebNLG datasets. Furthermore, compared to SOTA models, GraSAME eliminates the need for extra pre-training tasks to adjust graph inputs and reduces the number of trainable parameters by over 100 million.","sentences":["Pretrained Language Models (PLMs) benefit from external knowledge stored in graph structures for various downstream tasks.","However, bridging the modality gap between graph structures and text remains a significant challenge.","Traditional methods like linearizing graphs for PLMs lose vital graph connectivity, whereas Graph Neural Networks (GNNs) require cumbersome processes for integration into PLMs.","In this work, we propose a novel graph-guided self-attention mechanism, GraSAME.","GraSAME seamlessly incorporates token-level structural information into PLMs without necessitating additional alignment or concatenation efforts.","As an end-to-end, lightweight multimodal module, GraSAME follows a multi-task learning strategy and effectively bridges the gap between graph and textual modalities, facilitating dynamic interactions between GNNs and PLMs.","Our experiments on the graph-to-text generation task demonstrate that GraSAME outperforms baseline models and achieves results comparable to state-of-the-art (SOTA) models on WebNLG datasets.","Furthermore, compared to SOTA models, GraSAME eliminates the need for extra pre-training tasks to adjust graph inputs and reduces the number of trainable parameters by over 100 million."],"url":"http://arxiv.org/abs/2404.06911v1","category":"cs.CL"}
{"created":"2024-04-10 11:03:17","title":"Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation","abstract":"Despite the successes of large language models (LLMs), they exhibit significant drawbacks, particularly when processing long contexts. Their inference cost scales quadratically with respect to sequence length, making it expensive for deployment in some real-world text processing applications, such as retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the \"distraction phenomenon,\" where irrelevant context in the prompt degrades output quality. To address these drawbacks, we propose a novel RAG prompting methodology, superposition prompting, which can be directly applied to pre-trained transformer-based LLMs without the need for fine-tuning. At a high level, superposition prompting allows the LLM to process input documents in parallel prompt paths, discarding paths once they are deemed irrelevant. We demonstrate the capability of our method to simultaneously enhance time efficiency across a variety of question-answering benchmarks using multiple pre-trained LLMs. Furthermore, our technique significantly improves accuracy when the retrieved context is large relative the context the model was trained on. For example, our approach facilitates an 93x reduction in compute time while improving accuracy by 43\\% on the NaturalQuestions-Open dataset with the MPT-7B instruction-tuned model over naive RAG.","sentences":["Despite the successes of large language models (LLMs), they exhibit significant drawbacks, particularly when processing long contexts.","Their inference cost scales quadratically with respect to sequence length, making it expensive for deployment in some real-world text processing applications, such as retrieval-augmented generation (RAG).","Additionally, LLMs also exhibit the \"distraction phenomenon,\" where irrelevant context in the prompt degrades output quality.","To address these drawbacks, we propose a novel RAG prompting methodology, superposition prompting, which can be directly applied to pre-trained transformer-based LLMs without the need for fine-tuning.","At a high level, superposition prompting allows the LLM to process input documents in parallel prompt paths, discarding paths once they are deemed irrelevant.","We demonstrate the capability of our method to simultaneously enhance time efficiency across a variety of question-answering benchmarks using multiple pre-trained LLMs.","Furthermore, our technique significantly improves accuracy when the retrieved context is large relative the context the model was trained on.","For example, our approach facilitates an 93x reduction in compute time while improving accuracy by 43\\% on the NaturalQuestions-Open dataset with the MPT-7B instruction-tuned model over naive RAG."],"url":"http://arxiv.org/abs/2404.06910v1","category":"cs.CL"}
{"created":"2024-04-10 11:00:48","title":"Twisted Adiabatic Limit for Complex Structures","abstract":"Given a complex manifold $X$ and a smooth positive function $\\eta$ thereon, we perturb the standard differential operator $d=\\partial + \\bar\\partial$ acting on differential forms to a first-order differential operator $D_\\eta$ whose principal part is $\\eta\\partial + \\bar\\partial$. The role of the zero-th order part is to force the integrability property $D_\\eta^2=0$ that leads to a cohomology isomorphic to the de Rham cohomology of $X$, while the components of types $(0,\\,1)$ and $(1,\\,0)$ of $D_\\eta$ induce cohomologies isomorphic to the Dolbeault and conjugate-Dolbeault cohomologies. We compute Bochner-Kodaira-Nakano-type formulae for the Laplacians induced by these operators and a given Hermitian metric on $X$. The computations throw up curvature-like operators of order one that can be made (semi-)positive under appropriate assumptions on the function $\\eta$. As applications, we obtain vanishing results for certain harmonic spaces on complete, non-compact, manifolds and for the Dolbeault cohomology of compact complex manifolds that carry certain types of functions $\\eta$. This study continues and generalises the one of the operators $d_h=h\\partial + \\bar\\partial$ that we introduced and investigated recently for a positive constant $h$ that was then let to converge to $0$ and, more generally, for constants $h\\in\\C$. The operators $d_h$ had, in turn, been adapted to complex structures from the well-known adiabatic limit construction for Riemannian foliations. Allowing now for possibly non-constant functions $\\eta$ creates positivity in the curvature-like operator that stands one in good stead for various kinds of applications.","sentences":["Given a complex manifold $X$ and a smooth positive function $\\eta$ thereon, we perturb the standard differential operator $d=\\partial + \\bar\\partial$ acting on differential forms to a first-order differential operator $D_\\eta$ whose principal part is $\\eta\\partial + \\bar\\partial$.","The role of the zero-th order part is to force the integrability property $D_\\eta^2=0$ that leads to a cohomology isomorphic to the de Rham cohomology of $X$, while the components of types $(0,\\,1)$ and $(1,\\,0)$ of $D_\\eta$ induce cohomologies isomorphic to the Dolbeault and conjugate-Dolbeault cohomologies.","We compute Bochner-Kodaira-Nakano-type formulae for the Laplacians induced by these operators and a given Hermitian metric on $X$. The computations throw up curvature-like operators of order one that can be made (semi-)positive under appropriate assumptions on the function $\\eta$. As applications, we obtain vanishing results for certain harmonic spaces on complete, non-compact, manifolds and for the Dolbeault cohomology of compact complex manifolds that carry certain types of functions $\\eta$.","This study continues and generalises the one of the operators $d_h=h\\partial + \\bar\\partial$ that we introduced and investigated recently for a positive constant $h$ that was then let to converge to $0$ and, more generally, for constants $h\\in\\C$. The operators $d_h$ had, in turn, been adapted to complex structures from the well-known adiabatic limit construction for Riemannian foliations.","Allowing now for possibly non-constant functions $\\eta$ creates positivity in the curvature-like operator that stands one in good stead for various kinds of applications."],"url":"http://arxiv.org/abs/2404.06908v1","category":"math.DG"}
{"created":"2024-04-10 10:46:59","title":"DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting","abstract":"The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360$^{\\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\\circ}$ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary \"flat\" (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360$^{\\circ}$ perspective, providing an enhanced immersive experience over existing techniques. Project website at: http://dreamscene360.github.io/","sentences":["The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets.","We present a text-to-3D 360$^{\\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\\circ}$ scenes for in-the-wild environments in a matter of minutes.","Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image.","This image acts as a preliminary \"flat\" (2D) scene representation.","Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration.","To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud.","This point cloud serves as the initial state for the centroids of 3D Gaussians.","In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations.","These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions.","In summary, our method offers a globally consistent 3D scene within a 360$^{\\circ}$ perspective, providing an enhanced immersive experience over existing techniques.","Project website at: http://dreamscene360.github.io/"],"url":"http://arxiv.org/abs/2404.06903v1","category":"cs.CV"}
{"created":"2024-04-10 10:46:43","title":"Spatiotemporal Analysis of Shared Situation Awareness among Connected Vehicles","abstract":"Shared situation awareness (SSA) has been garnering explosive interest in various applications for intelligent transportation systems (ITS). In addition, the delay-constrained nature of supporting vehicular networks makes it critical to precisely analyze the performance of a SSA procedure. Extending the relevant literature, this paper provides an analysis framework that evaluates the performance of SSA in spatial and temporal aspects simultaneously. Specifically, this paper provides a closed-form probability distribution for the length of time taken for constitution of a SSA among a group of connected vehicles. This paper extends the calculation to investigation of feasibility of SSA in supporting various types of safety messages defined by the SAE J2735.","sentences":["Shared situation awareness (SSA) has been garnering explosive interest in various applications for intelligent transportation systems (ITS).","In addition, the delay-constrained nature of supporting vehicular networks makes it critical to precisely analyze the performance of a SSA procedure.","Extending the relevant literature, this paper provides an analysis framework that evaluates the performance of SSA in spatial and temporal aspects simultaneously.","Specifically, this paper provides a closed-form probability distribution for the length of time taken for constitution of a SSA among a group of connected vehicles.","This paper extends the calculation to investigation of feasibility of SSA in supporting various types of safety messages defined by the SAE J2735."],"url":"http://arxiv.org/abs/2404.06902v1","category":"eess.SY"}
{"created":"2024-04-10 10:40:53","title":"Precession and Split of Tilted, Geometrically Thin Accretion Disk: an Analytical Study","abstract":"It has been observed that many relativistic jets display a kind of cork-screw-like precession. Numerical simulations has suggested that such kind of precession may originate from the precession of the disk. In this work, we introduce an analytical model to describe the precession and split of a tilted, geometrically thin disk. We consider the Lense-Thirring effect from the central (primary) black hole (BH) and the gravitational effect from the companion (secondary) BH far away from the center, both of which could induce the precession of the accretion disk around the spin axis of central black hole. We propose the splitting conditions that when the rate of viscous diffusion cannot catch up with the dynamical frequency at a certain layer of fluid, the disk would split into two parts which precess independently. We presume that the precessions of the inner and outer disks are in accord with the rotation and precession of jet, respectively. By matching the frequencies of the disks to the observed frequencies of jet in the cork-screw-like precession and considering the splitting condition, we are allowed to read four parameters, the innermost radius ($r_{\\rm in}$), the outermost radius ($r_{\\rm out}$) of the disk, the initial splitting radius ($r_{\\rm sp,0}$), and the inflow speed magnitude($\\beta$), of the disk. We apply this model to OJ 287. Moreover, considering the inward shrinking of the disks, we find the time variation of the precession angle of jet. This time variation presents a unique feature of our model, which could be distinguishable in the future observation.","sentences":["It has been observed that many relativistic jets display a kind of cork-screw-like precession.","Numerical simulations has suggested that such kind of precession may originate from the precession of the disk.","In this work, we introduce an analytical model to describe the precession and split of a tilted, geometrically thin disk.","We consider the Lense-Thirring effect from the central (primary) black hole (BH) and the gravitational effect from the companion (secondary) BH far away from the center, both of which could induce the precession of the accretion disk around the spin axis of central black hole.","We propose the splitting conditions that when the rate of viscous diffusion cannot catch up with the dynamical frequency at a certain layer of fluid, the disk would split into two parts which precess independently.","We presume that the precessions of the inner and outer disks are in accord with the rotation and precession of jet, respectively.","By matching the frequencies of the disks to the observed frequencies of jet in the cork-screw-like precession and considering the splitting condition, we are allowed to read four parameters, the innermost radius ($r_{\\rm in}$), the outermost radius ($r_{\\rm out}$) of the disk, the initial splitting radius ($r_{\\rm sp,0}$), and the inflow speed magnitude($\\beta$), of the disk.","We apply this model to OJ 287.","Moreover, considering the inward shrinking of the disks, we find the time variation of the precession angle of jet.","This time variation presents a unique feature of our model, which could be distinguishable in the future observation."],"url":"http://arxiv.org/abs/2404.06898v1","category":"astro-ph.HE"}
{"created":"2024-04-10 10:40:16","title":"Unravelling the Band Structure and Orbital Character of a $\u03c0$-Conjugated 2D Graphdiyne-Based Organometallic Network","abstract":"Graphdiyne-based carbon systems generate intriguing layered sp-sp$^2$ organometallic lattices, characterized by flexible acetylenic groups connecting planar carbon units through metal centers. At their thinnest limit, they can result in two-dimensional (2D) organometallic networks exhibiting unique quantum properties and even confining the surface states of the substrate, which is of great importance for fundamental studies. In this work, we present the on-surface synthesis of a highly crystalline 2D organometallic network grown on Ag(111). The electronic structure of this mixed honeycomb-kagome arrangement - investigated by angle-resolved photoemission spectroscopy and scanning tunneling spectroscopy - reveals a strong electronic conjugation within the network, leading to the formation of two intense electronic band-manifolds. In comparison to theoretical density functional theory calculations, we observe that these bands exhibit a well-defined orbital character that can be associated with distinct regions of the sp-sp$^2$ monomers. Moreover, we find that the halogen by-products resulting from the network formation locally affect the pore-confined states, causing a significant energy shift. This work contributes to the understanding of the growth and electronic structure of graphdiyne-like 2D networks, providing insights into the development of novel carbon materials beyond graphene with tailored properties.","sentences":["Graphdiyne-based carbon systems generate intriguing layered sp-sp$^2$ organometallic lattices, characterized by flexible acetylenic groups connecting planar carbon units through metal centers.","At their thinnest limit, they can result in two-dimensional (2D) organometallic networks exhibiting unique quantum properties and even confining the surface states of the substrate, which is of great importance for fundamental studies.","In this work, we present the on-surface synthesis of a highly crystalline 2D organometallic network grown on Ag(111).","The electronic structure of this mixed honeycomb-kagome arrangement - investigated by angle-resolved photoemission spectroscopy and scanning tunneling spectroscopy - reveals a strong electronic conjugation within the network, leading to the formation of two intense electronic band-manifolds.","In comparison to theoretical density functional theory calculations, we observe that these bands exhibit a well-defined orbital character that can be associated with distinct regions of the sp-sp$^2$ monomers.","Moreover, we find that the halogen by-products resulting from the network formation locally affect the pore-confined states, causing a significant energy shift.","This work contributes to the understanding of the growth and electronic structure of graphdiyne-like 2D networks, providing insights into the development of novel carbon materials beyond graphene with tailored properties."],"url":"http://arxiv.org/abs/2404.06896v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-10 10:13:37","title":"Research on Detection of Floating Objects in River and Lake Based on AI Intelligent Image Recognition","abstract":"With the rapid advancement of artificial intelligence technology, AI-enabled image recognition has emerged as a potent tool for addressing challenges in traditional environmental monitoring. This study focuses on the detection of floating objects in river and lake environments, exploring an innovative approach based on deep learning. By intricately analyzing the technical pathways for detecting static and dynamic features and considering the characteristics of river and lake debris, a comprehensive image acquisition and processing workflow has been developed. The study highlights the application and performance comparison of three mainstream deep learning models -SSD, Faster-RCNN, and YOLOv5- in debris identification. Additionally, a detection system for floating objects has been designed and implemented, encompassing both hardware platform construction and software framework development. Through rigorous experimental validation, the proposed system has demonstrated its ability to significantly enhance the accuracy and efficiency of debris detection, thus offering a new technological avenue for water quality monitoring in rivers and lakes","sentences":["With the rapid advancement of artificial intelligence technology, AI-enabled image recognition has emerged as a potent tool for addressing challenges in traditional environmental monitoring.","This study focuses on the detection of floating objects in river and lake environments, exploring an innovative approach based on deep learning.","By intricately analyzing the technical pathways for detecting static and dynamic features and considering the characteristics of river and lake debris, a comprehensive image acquisition and processing workflow has been developed.","The study highlights the application and performance comparison of three mainstream deep learning models -SSD, Faster-RCNN, and YOLOv5- in debris identification.","Additionally, a detection system for floating objects has been designed and implemented, encompassing both hardware platform construction and software framework development.","Through rigorous experimental validation, the proposed system has demonstrated its ability to significantly enhance the accuracy and efficiency of debris detection, thus offering a new technological avenue for water quality monitoring in rivers and lakes"],"url":"http://arxiv.org/abs/2404.06883v1","category":"cs.CV"}
{"created":"2024-04-10 10:08:42","title":"Joint Active And Passive IRS Aided Wireless Communication: Elements Allocation and Achievable Rate","abstract":"Equipping reflecting elements at the active intelligent reflecting surface (AIRS) enhances signal amplification capability but meanwhile incurs non-negligible amplification noise, which thus challenges the determination of elements allocation for maximizing achievable rate in multi-cooperative AIRS and passive IRS (PIRS) jointly aided wireless communication system. To tackle this issue, we consider the downlink communication from a single-antenna transmitter (Tx) to a single-antenna receiver (Rx), which aided by a pair of AIRS and PIRS with two different deployment orders. Specifically, we target to determine the number of AIRS/PIRS elements over both transmission orders under given deployment budget for the achievable rate maximization. Our analysis illustrates that the PIRS should be allocated more elements than the AIRS for achieving optimized rate and linear signal-to-noise ratio (SNR) scaling orders are attained in both schemes. Simulation results are provided to evaluate the proposed algorithm and compare the rate performance of the AIRS and PIRS jointly aided wireless system with various benchmark systems.","sentences":["Equipping reflecting elements at the active intelligent reflecting surface (AIRS) enhances signal amplification capability but meanwhile incurs non-negligible amplification noise, which thus challenges the determination of elements allocation for maximizing achievable rate in multi-cooperative AIRS and passive IRS (PIRS) jointly aided wireless communication system.","To tackle this issue, we consider the downlink communication from a single-antenna transmitter (Tx) to a single-antenna receiver (Rx), which aided by a pair of AIRS and PIRS with two different deployment orders.","Specifically, we target to determine the number of AIRS/PIRS elements over both transmission orders under given deployment budget for the achievable rate maximization.","Our analysis illustrates that the PIRS should be allocated more elements than the AIRS for achieving optimized rate and linear signal-to-noise ratio (SNR) scaling orders are attained in both schemes.","Simulation results are provided to evaluate the proposed algorithm and compare the rate performance of the AIRS and PIRS jointly aided wireless system with various benchmark systems."],"url":"http://arxiv.org/abs/2404.06880v2","category":"cs.IT"}
{"created":"2024-04-10 10:03:43","title":"Magnetic Configuration of Active Regions Associated with GLE Events","abstract":"Charged particles, generated in solar flares, sometimes can get extremely high energy, above the 500 MeV level, and produce abrupt ground level enhancements (GLEs) on the ground-based detectors of cosmic rays. The initial flares are strong eruptions and they should be originated from active regions (ARs). A list of GLE events and associated flares was initially available, and our aim here was to find the hosting AR for each GLE event. Moreover, we aimed to classify the revealed ARs using the magneto-morphological classification (MMC: \\citealp{2021MNRAS.507.3698A}). We have shown that in 94\\% of cases such ARs belong to the most complex morphological classes, namely, $\\beta \\gamma$, $\\beta \\delta$, $\\gamma \\delta$, $\\beta \\gamma \\delta$ classes by the Hale classification and B2, B3 classes by the MMC. We also found that the GLE-associated ARs are the ARs with the total unsigned magnetic flux much stronger than the common ARs of the same complexity. The set of GLE-related ARs only partially overlaps with the set of SARs (super-active regions). These ARs seem to be a manifestation of nonlinearities in the regular process of the global mean-field dynamo, the key ingredient to keep fluctuations and to create critical conditions in different aspects of the solar activity.","sentences":["Charged particles, generated in solar flares, sometimes can get extremely high energy, above the 500 MeV level, and produce abrupt ground level enhancements (GLEs) on the ground-based detectors of cosmic rays.","The initial flares are strong eruptions and they should be originated from active regions (ARs).","A list of GLE events and associated flares was initially available, and our aim here was to find the hosting AR for each GLE event.","Moreover, we aimed to classify the revealed ARs using the magneto-morphological classification (MMC: \\citealp{2021MNRAS.507.3698A}).","We have shown that in 94\\% of cases such ARs belong to the most complex morphological classes, namely, $\\beta \\gamma$, $\\beta \\delta$, $\\gamma \\delta$, $\\beta \\gamma \\delta$ classes by the Hale classification and B2, B3 classes by the MMC.","We also found that the GLE-associated ARs are the ARs with the total unsigned magnetic flux much stronger than the common ARs of the same complexity.","The set of GLE-related ARs only partially overlaps with the set of SARs (super-active regions).","These ARs seem to be a manifestation of nonlinearities in the regular process of the global mean-field dynamo, the key ingredient to keep fluctuations and to create critical conditions in different aspects of the solar activity."],"url":"http://arxiv.org/abs/2404.06877v1","category":"astro-ph.SR"}
{"created":"2024-04-10 09:48:42","title":"Braided coproduct, antipode and adjoint action for $U_q(sl_2)$","abstract":"Motivated by our attempts to construct an analogue of the Dirac operator in the setting of $U_q(\\mathfrak{sl}_n)$, we write down explicitly the braided coproduct, antipode, and adjoint action for quantum algebra $U_q(\\mathfrak{sl}_2)$. The braided adjoint action is seen to coincide with the ordinary quantum adjoint action, which also follows from the general results of S. Majid.","sentences":["Motivated by our attempts to construct an analogue of the Dirac operator in the setting of $U_q(\\mathfrak{sl}_n)$, we write down explicitly the braided coproduct, antipode, and adjoint action for quantum algebra $U_q(\\mathfrak{sl}_2)$. The braided adjoint action is seen to coincide with the ordinary quantum adjoint action, which also follows from the general results of S. Majid."],"url":"http://arxiv.org/abs/2404.06871v1","category":"math.QA"}
{"created":"2024-04-10 09:47:55","title":"Energetics of Buchdahl stars and the magnetic Penrose process","abstract":"Buchdahl star is the most compact object without an event horizon and is an excellent candidate for a black hole mimicker. Unlike black holes, rotating Buchdahl star can be over-extremal with respect to the black hole, sustaining a larger spin. We show that it can also develop an ergosphere above the threshold spin $\\beta > 1/\\sqrt{2}$, which allows extraction of its rotational energy. Electromagnetic field around Buchdahl star is also expected to differ from that of black hole in both strength and topology. In this paper, we explore the energetics of Buchdahl star focusing on the magnetic Penrose process in the two magnetic field configurations, i.e., uniform and dipole. Below the threshold spin, Buchdahl star is expected to be quiet, while above the threshold it can be much more efficient than the black hole if a dipolar magnetic field is developed on its surface.","sentences":["Buchdahl star is the most compact object without an event horizon and is an excellent candidate for a black hole mimicker.","Unlike black holes, rotating Buchdahl star can be over-extremal with respect to the black hole, sustaining a larger spin.","We show that it can also develop an ergosphere above the threshold spin $\\beta > 1/\\sqrt{2}$, which allows extraction of its rotational energy.","Electromagnetic field around Buchdahl star is also expected to differ from that of black hole in both strength and topology.","In this paper, we explore the energetics of Buchdahl star focusing on the magnetic Penrose process in the two magnetic field configurations, i.e., uniform and dipole.","Below the threshold spin, Buchdahl star is expected to be quiet, while above the threshold it can be much more efficient than the black hole if a dipolar magnetic field is developed on its surface."],"url":"http://arxiv.org/abs/2404.06870v1","category":"gr-qc"}
{"created":"2024-04-10 09:47:34","title":"SleepPPG-Net2: Deep learning generalization for sleep staging from photoplethysmography","abstract":"Background: Sleep staging is a fundamental component in the diagnosis of sleep disorders and the management of sleep health. Traditionally, this analysis is conducted in clinical settings and involves a time-consuming scoring procedure. Recent data-driven algorithms for sleep staging, using the photoplethysmogram (PPG) time series, have shown high performance on local test sets but lower performance on external datasets due to data drift. Methods: This study aimed to develop a generalizable deep learning model for the task of four class (wake, light, deep, and rapid eye movement (REM)) sleep staging from raw PPG physiological time-series. Six sleep datasets, totaling 2,574 patients recordings, were used. In order to create a more generalizable representation, we developed and evaluated a deep learning model called SleepPPG-Net2, which employs a multi-source domain training approach.SleepPPG-Net2 was benchmarked against two state-of-the-art models. Results: SleepPPG-Net2 showed consistently higher performance over benchmark approaches, with generalization performance (Cohen's kappa) improving by up to 19%. Performance disparities were observed in relation to age, sex, and sleep apnea severity. Conclusion: SleepPPG-Net2 sets a new standard for staging sleep from raw PPG time-series.","sentences":["Background:","Sleep staging is a fundamental component in the diagnosis of sleep disorders and the management of sleep health.","Traditionally, this analysis is conducted in clinical settings and involves a time-consuming scoring procedure.","Recent data-driven algorithms for sleep staging, using the photoplethysmogram (PPG) time series, have shown high performance on local test sets but lower performance on external datasets due to data drift.","Methods:","This study aimed to develop a generalizable deep learning model for the task of four class (wake, light, deep, and rapid eye movement (REM)) sleep staging from raw PPG physiological time-series.","Six sleep datasets, totaling 2,574 patients recordings, were used.","In order to create a more generalizable representation, we developed and evaluated a deep learning model called SleepPPG-Net2, which employs a multi-source domain training approach.","SleepPPG-Net2 was benchmarked against two state-of-the-art models.","Results: SleepPPG-Net2 showed consistently higher performance over benchmark approaches, with generalization performance (Cohen's kappa) improving by up to 19%.","Performance disparities were observed in relation to age, sex, and sleep apnea severity.","Conclusion: SleepPPG-Net2 sets a new standard for staging sleep from raw PPG time-series."],"url":"http://arxiv.org/abs/2404.06869v1","category":"cs.LG"}
{"created":"2024-04-10 09:45:02","title":"Fine color guidance in diffusion models and its application to image compression at extremely low bitrates","abstract":"This study addresses the challenge of, without training or fine-tuning, controlling the global color aspect of images generated with a diffusion model. We rewrite the guidance equations to ensure that the outputs are closer to a known color map, and this without hindering the quality of the generation. Our method leads to new guidance equations. We show in the color guidance context that, the scaling of the guidance should not decrease but remains high throughout the diffusion process. In a second contribution, our guidance is applied in a compression framework, we combine both semantic and general color information on the image to decode the images at low cost. We show that our method is effective at improving fidelity and realism of compressed images at extremely low bit rates, when compared to other classical or more semantic oriented approaches.","sentences":["This study addresses the challenge of, without training or fine-tuning, controlling the global color aspect of images generated with a diffusion model.","We rewrite the guidance equations to ensure that the outputs are closer to a known color map, and this without hindering the quality of the generation.","Our method leads to new guidance equations.","We show in the color guidance context that, the scaling of the guidance should not decrease but remains high throughout the diffusion process.","In a second contribution, our guidance is applied in a compression framework, we combine both semantic and general color information on the image to decode the images at low cost.","We show that our method is effective at improving fidelity and realism of compressed images at extremely low bit rates, when compared to other classical or more semantic oriented approaches."],"url":"http://arxiv.org/abs/2404.06865v1","category":"cs.CV"}
{"created":"2024-04-10 09:43:03","title":"Hydrodynamic simulations of WD-WD mergers and the origin of RCB stars","abstract":"We study the properties of double white dwarf (DWD) mergers by performing hydrodynamic simulations using the new and improved adaptive mesh refinement code Octo-Tiger. We follow the orbital evolution of DWD systems of mass ratio q=0.7 for tens of orbits until and after the merger to investigate them as a possible origin for R Coronae Borealis (RCB) type stars. We reproduce previous results, finding that during the merger, the Helium WD donor star is tidally disrupted within 20-80 minutes since the beginning of the simulation onto the accretor Carbon-Oxygen WD, creating a high temperature shell around the accretor. We investigate the possible Helium burning in this shell and the merged object's general structure. Specifically, we are interested in the amount of Oxygen-16 dredged-up to the hot shell and the amount of Oxygen-18 produced. This is critical as the discovery of very low Oxygen-16 to Oxygen-18 ratios in RCB stars pointed out the merger scenario as a favorable explanation for their origin. A small amount of hydrogen in the donor may help keep the Oxygen-16 to Oxygen-18 ratios within observational bounds, even if moderate dredge-up from the accretor occurs. In addition, we perform a resolution study to reconcile the difference found in the amount of Oxygen-16 dredge-up between smoothed-particle hydrodynamics and grid-based simulations.","sentences":["We study the properties of double white dwarf (DWD) mergers by performing hydrodynamic simulations using the new and improved adaptive mesh refinement code Octo-Tiger.","We follow the orbital evolution of DWD systems of mass ratio q=0.7 for tens of orbits until and after the merger to investigate them as a possible origin for R Coronae Borealis (RCB) type stars.","We reproduce previous results, finding that during the merger, the Helium WD donor star is tidally disrupted within 20-80 minutes since the beginning of the simulation onto the accretor Carbon-Oxygen WD, creating a high temperature shell around the accretor.","We investigate the possible Helium burning in this shell and the merged object's general structure.","Specifically, we are interested in the amount of Oxygen-16 dredged-up to the hot shell and the amount of Oxygen-18 produced.","This is critical as the discovery of very low Oxygen-16 to Oxygen-18 ratios in RCB stars pointed out the merger scenario as a favorable explanation for their origin.","A small amount of hydrogen in the donor may help keep the Oxygen-16 to Oxygen-18 ratios within observational bounds, even if moderate dredge-up from the accretor occurs.","In addition, we perform a resolution study to reconcile the difference found in the amount of Oxygen-16 dredge-up between smoothed-particle hydrodynamics and grid-based simulations."],"url":"http://arxiv.org/abs/2404.06864v1","category":"astro-ph.SR"}
{"created":"2024-04-10 09:40:56","title":"RESSCAL3D: Resolution Scalable 3D Semantic Segmentation of Point Clouds","abstract":"While deep learning-based methods have demonstrated outstanding results in numerous domains, some important functionalities are missing. Resolution scalability is one of them. In this work, we introduce a novel architecture, dubbed RESSCAL3D, providing resolution-scalable 3D semantic segmentation of point clouds. In contrast to existing works, the proposed method does not require the whole point cloud to be available to start inference. Once a low-resolution version of the input point cloud is available, first semantic predictions can be generated in an extremely fast manner. This enables early decision-making in subsequent processing steps. As additional points become available, these are processed in parallel. To improve performance, features from previously computed scales are employed as prior knowledge at the current scale. Our experiments show that RESSCAL3D is 31-62% faster than the non-scalable baseline while keeping a limited impact on performance. To the best of our knowledge, the proposed method is the first to propose a resolution-scalable approach for 3D semantic segmentation of point clouds based on deep learning.","sentences":["While deep learning-based methods have demonstrated outstanding results in numerous domains, some important functionalities are missing.","Resolution scalability is one of them.","In this work, we introduce a novel architecture, dubbed RESSCAL3D, providing resolution-scalable 3D semantic segmentation of point clouds.","In contrast to existing works, the proposed method does not require the whole point cloud to be available to start inference.","Once a low-resolution version of the input point cloud is available, first semantic predictions can be generated in an extremely fast manner.","This enables early decision-making in subsequent processing steps.","As additional points become available, these are processed in parallel.","To improve performance, features from previously computed scales are employed as prior knowledge at the current scale.","Our experiments show that RESSCAL3D is 31-62% faster than the non-scalable baseline while keeping a limited impact on performance.","To the best of our knowledge, the proposed method is the first to propose a resolution-scalable approach for 3D semantic segmentation of point clouds based on deep learning."],"url":"http://arxiv.org/abs/2404.06863v1","category":"cs.CV"}
{"created":"2024-04-10 09:38:28","title":"Electron acceleration and X-ray generation from near-critical-density carbon nanotube foams driven by moderately relativistic lasers","abstract":"Direct laser acceleration of electrons in near-critical-density (NCD) carbon nanotube foams (CNFs) has its advantages in the high-efficiency generation of relativistic electrons and broadband X-rays. Here, we report the first simultaneous measurement on the spectra of laser-driven electrons and X-rays from CNFs at moderately relativistic intensities of around 5\\times{10}^{19}\\ W/cm^2.\\ The density and thickness of the CNFs were scanned in the experiments, indicating the optimized electrons temperature of 5.5 MeV and X-ray critical energy of 5 keV. Two-dimensional (2D) particle-in-cell (PIC) simulations confirm that the electrons, with a temperature significantly higher than the pondermotive scale, are directly accelerated by the laser along the NCD plasma channel, while the bright X-rays are emitted by these electrons through betatron radiation or Thomson backscattering inside the channel. The simultaneously generated electrons and X-rays, automatically synchronized with the femtosecond laser driver, are suitable for applications such as bi-modal radiography.","sentences":["Direct laser acceleration of electrons in near-critical-density (NCD) carbon nanotube foams (CNFs) has its advantages in the high-efficiency generation of relativistic electrons and broadband X-rays.","Here, we report the first simultaneous measurement on the spectra of laser-driven electrons and X-rays from CNFs at moderately relativistic intensities of around 5\\times{10}^{19}\\ W/cm^2.\\ The density and thickness of the CNFs were scanned in the experiments, indicating the optimized electrons temperature of 5.5 MeV and X-ray critical energy of 5 keV. Two-dimensional (2D) particle-in-cell (PIC) simulations confirm that the electrons, with a temperature significantly higher than the pondermotive scale, are directly accelerated by the laser along the NCD plasma channel, while the bright X-rays are emitted by these electrons through betatron radiation or Thomson backscattering inside the channel.","The simultaneously generated electrons and X-rays, automatically synchronized with the femtosecond laser driver, are suitable for applications such as bi-modal radiography."],"url":"http://arxiv.org/abs/2404.06862v1","category":"physics.plasm-ph"}
{"created":"2024-04-10 09:35:36","title":"Multi-Label Continual Learning for the Medical Domain: A Novel Benchmark","abstract":"Multi-label image classification in dynamic environments is a problem that poses significant challenges. Previous studies have primarily focused on scenarios such as Domain Incremental Learning and Class Incremental Learning, which do not fully capture the complexity of real-world applications. In this paper, we study the problem of classification of medical imaging in the scenario termed New Instances and New Classes, which combines the challenges of both new class arrivals and domain shifts in a single framework. Unlike traditional scenarios, it reflects the realistic nature of CL in domains such as medical imaging, where updates may introduce both new classes and changes in domain characteristics. To address the unique challenges posed by this complex scenario, we introduce a novel approach called Pseudo-Label Replay. This method aims to mitigate forgetting while adapting to new classes and domain shifts by combining the advantages of the Replay and Pseudo-Label methods and solving their limitations in the proposed scenario. We evaluate our proposed approach on a challenging benchmark consisting of two datasets, seven tasks, and nineteen classes, modeling a realistic Continual Learning scenario. Our experimental findings demonstrate the effectiveness of Pseudo-Label Replay in addressing the challenges posed by the complex scenario proposed. Our method surpasses existing approaches, exhibiting superior performance while showing minimal forgetting.","sentences":["Multi-label image classification in dynamic environments is a problem that poses significant challenges.","Previous studies have primarily focused on scenarios such as Domain Incremental Learning and Class Incremental Learning, which do not fully capture the complexity of real-world applications.","In this paper, we study the problem of classification of medical imaging in the scenario termed New Instances and New Classes, which combines the challenges of both new class arrivals and domain shifts in a single framework.","Unlike traditional scenarios, it reflects the realistic nature of CL in domains such as medical imaging, where updates may introduce both new classes and changes in domain characteristics.","To address the unique challenges posed by this complex scenario, we introduce a novel approach called Pseudo-Label Replay.","This method aims to mitigate forgetting while adapting to new classes and domain shifts by combining the advantages of the Replay and Pseudo-Label methods and solving their limitations in the proposed scenario.","We evaluate our proposed approach on a challenging benchmark consisting of two datasets, seven tasks, and nineteen classes, modeling a realistic Continual Learning scenario.","Our experimental findings demonstrate the effectiveness of Pseudo-Label Replay in addressing the challenges posed by the complex scenario proposed.","Our method surpasses existing approaches, exhibiting superior performance while showing minimal forgetting."],"url":"http://arxiv.org/abs/2404.06859v2","category":"cs.CV"}
{"created":"2024-04-10 09:28:54","title":"Beyond Random Inputs: A Novel ML-Based Hardware Fuzzing","abstract":"Modern computing systems heavily rely on hardware as the root of trust. However, their increasing complexity has given rise to security-critical vulnerabilities that cross-layer at-tacks can exploit. Traditional hardware vulnerability detection methods, such as random regression and formal verification, have limitations. Random regression, while scalable, is slow in exploring hardware, and formal verification techniques are often concerned with manual effort and state explosions. Hardware fuzzing has emerged as an effective approach to exploring and detecting security vulnerabilities in large-scale designs like modern processors. They outperform traditional methods regarding coverage, scalability, and efficiency. However, state-of-the-art fuzzers struggle to achieve comprehensive coverage of intricate hardware designs within a practical timeframe, often falling short of a 70% coverage threshold. We propose a novel ML-based hardware fuzzer, ChatFuzz, to address this challenge. Ourapproach leverages LLMs like ChatGPT to understand processor language, focusing on machine codes and generating assembly code sequences. RL is integrated to guide the input generation process by rewarding the inputs using code coverage metrics. We use the open-source RISCV-based RocketCore processor as our testbed. ChatFuzz achieves condition coverage rate of 75% in just 52 minutes compared to a state-of-the-art fuzzer, which requires a lengthy 30-hour window to reach a similar condition coverage. Furthermore, our fuzzer can attain 80% coverage when provided with a limited pool of 10 simulation instances/licenses within a 130-hour window. During this time, it conducted a total of 199K test cases, of which 6K produced discrepancies with the processor's golden model. Our analysis identified more than 10 unique mismatches, including two new bugs in the RocketCore and discrepancies from the RISC-V ISA Simulator.","sentences":["Modern computing systems heavily rely on hardware as the root of trust.","However, their increasing complexity has given rise to security-critical vulnerabilities that cross-layer at-tacks can exploit.","Traditional hardware vulnerability detection methods, such as random regression and formal verification, have limitations.","Random regression, while scalable, is slow in exploring hardware, and formal verification techniques are often concerned with manual effort and state explosions.","Hardware fuzzing has emerged as an effective approach to exploring and detecting security vulnerabilities in large-scale designs like modern processors.","They outperform traditional methods regarding coverage, scalability, and efficiency.","However, state-of-the-art fuzzers struggle to achieve comprehensive coverage of intricate hardware designs within a practical timeframe, often falling short of a 70% coverage threshold.","We propose a novel ML-based hardware fuzzer, ChatFuzz, to address this challenge.","Ourapproach leverages LLMs like ChatGPT to understand processor language, focusing on machine codes and generating assembly code sequences.","RL is integrated to guide the input generation process by rewarding the inputs using code coverage metrics.","We use the open-source RISCV-based RocketCore processor as our testbed.","ChatFuzz achieves condition coverage rate of 75% in just 52 minutes compared to a state-of-the-art fuzzer, which requires a lengthy 30-hour window to reach a similar condition coverage.","Furthermore, our fuzzer can attain 80% coverage when provided with a limited pool of 10 simulation instances/licenses within a 130-hour window.","During this time, it conducted a total of 199K test cases, of which 6K produced discrepancies with the processor's golden model.","Our analysis identified more than 10 unique mismatches, including two new bugs in the RocketCore and discrepancies from the RISC-V ISA Simulator."],"url":"http://arxiv.org/abs/2404.06856v1","category":"cs.SE"}
{"created":"2024-04-10 09:28:14","title":"Control-DAG: Constrained Decoding for Non-Autoregressive Directed Acyclic T5 using Weighted Finite State Automata","abstract":"The Directed Acyclic Transformer is a fast non-autoregressive (NAR) model that performs well in Neural Machine Translation. Two issues prevent its application to general Natural Language Generation (NLG) tasks: frequent Out-Of-Vocabulary (OOV) errors and the inability to faithfully generate entity names. We introduce Control-DAG, a constrained decoding algorithm for our Directed Acyclic T5 (DA-T5) model which offers lexical, vocabulary and length control. We show that Control-DAG significantly enhances DA-T5 on the Schema Guided Dialogue and the DART datasets, establishing strong NAR results for Task-Oriented Dialogue and Data-to-Text NLG.","sentences":["The Directed Acyclic Transformer is a fast non-autoregressive (NAR) model that performs well in Neural Machine Translation.","Two issues prevent its application to general Natural Language Generation (NLG) tasks: frequent Out-Of-Vocabulary (OOV) errors and the inability to faithfully generate entity names.","We introduce Control-DAG, a constrained decoding algorithm for our Directed Acyclic T5 (DA-T5) model which offers lexical, vocabulary and length control.","We show that Control-DAG significantly enhances DA-T5 on the Schema Guided Dialogue and the DART datasets, establishing strong NAR results for Task-Oriented Dialogue and Data-to-Text NLG."],"url":"http://arxiv.org/abs/2404.06854v1","category":"cs.CL"}
{"created":"2024-04-10 09:25:56","title":"Revealing mechanism of pore defect formation in laser directed energy deposition of aluminum alloy via in-situ synchrotron X-ray imaging","abstract":"Laser metal additive manufacturing technology is capable of producing components with complex geometries and compositions that cannot be realized by conventional manufacturing methods. However, a large number of pores generated during the additive manufacturing process greatly affect the mechanical properties of the additively manufactured parts, and the mechanism of such pore generation has not been revealed by direct observation clearly. Here, we report the mechanism of pore generation in the laser direct energy deposition process as revealed by {\\it in-situ} high-speed high-resolution synchrotron X-ray imaging. We found that dissolution and re-precipitation of external gases and precipitation of metal vapors are the two main mechanisms of pore formation. We further explored the effects of different process parameters on the generation of pores and optimized the process to suppress pore generation. This work provides important insights into the formation of porosity defects during laser metal additive manufacturing, and can provide guidance for related process optimization.","sentences":["Laser metal additive manufacturing technology is capable of producing components with complex geometries and compositions that cannot be realized by conventional manufacturing methods.","However, a large number of pores generated during the additive manufacturing process greatly affect the mechanical properties of the additively manufactured parts, and the mechanism of such pore generation has not been revealed by direct observation clearly.","Here, we report the mechanism of pore generation in the laser direct energy deposition process as revealed by {\\it in-situ} high-speed high-resolution synchrotron X-ray imaging.","We found that dissolution and re-precipitation of external gases and precipitation of metal vapors are the two main mechanisms of pore formation.","We further explored the effects of different process parameters on the generation of pores and optimized the process to suppress pore generation.","This work provides important insights into the formation of porosity defects during laser metal additive manufacturing, and can provide guidance for related process optimization."],"url":"http://arxiv.org/abs/2404.06853v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-10 09:24:54","title":"UDiFF: Generating Conditional Unsigned Distance Fields with Optimal Wavelet Diffusion","abstract":"Diffusion models have shown remarkable results for image generation, editing and inpainting. Recent works explore diffusion models for 3D shape generation with neural implicit functions, i.e., signed distance function and occupancy function. However, they are limited to shapes with closed surfaces, which prevents them from generating diverse 3D real-world contents containing open surfaces. In this work, we present UDiFF, a 3D diffusion model for unsigned distance fields (UDFs) which is capable to generate textured 3D shapes with open surfaces from text conditions or unconditionally. Our key idea is to generate UDFs in spatial-frequency domain with an optimal wavelet transformation, which produces a compact representation space for UDF generation. Specifically, instead of selecting an appropriate wavelet transformation which requires expensive manual efforts and still leads to large information loss, we propose a data-driven approach to learn the optimal wavelet transformation for UDFs. We evaluate UDiFF to show our advantages by numerical and visual comparisons with the latest methods on widely used benchmarks. Page: https://weiqi-zhang.github.io/UDiFF.","sentences":["Diffusion models have shown remarkable results for image generation, editing and inpainting.","Recent works explore diffusion models for 3D shape generation with neural implicit functions, i.e., signed distance function and occupancy function.","However, they are limited to shapes with closed surfaces, which prevents them from generating diverse 3D real-world contents containing open surfaces.","In this work, we present UDiFF, a 3D diffusion model for unsigned distance fields (UDFs) which is capable to generate textured 3D shapes with open surfaces from text conditions or unconditionally.","Our key idea is to generate UDFs in spatial-frequency domain with an optimal wavelet transformation, which produces a compact representation space for UDF generation.","Specifically, instead of selecting an appropriate wavelet transformation which requires expensive manual efforts and still leads to large information loss, we propose a data-driven approach to learn the optimal wavelet transformation for UDFs.","We evaluate UDiFF to show our advantages by numerical and visual comparisons with the latest methods on widely used benchmarks.","Page: https://weiqi-zhang.github.io/UDiFF."],"url":"http://arxiv.org/abs/2404.06851v1","category":"cs.CV"}
{"created":"2024-04-10 09:20:06","title":"From naive trees to Random Forests: A general approach for proofing consistency of tree-based methods","abstract":"Tree-based methods such as Random Forests are learning algorithms that have become an integral part of the statistical toolbox. The last decade has shed some light on theoretical properties such as their consistency for regression tasks. However, the usual proofs assume normal error terms as well as an additive regression function and are rather technical. We overcome these issues by introducing a simple and catchy technique for proofing consistency under quite general assumptions. To this end, we introduce a new class of naive trees, which do the subspacing completely at random and independent of the data. We then give a direct proof of their consistency. Using them to bound the error of more complex tree-based approaches such as univariate and multivariate CARTs, Extra Randomized Trees, or Random Forests, we deduce the consistency of all of them. Since naive trees appear to be too simple for actual application, we further analyze their finite sample properties in a simulation and small benchmark study. We find a slow convergence speed and a rather poor predictive performance. Based on these results, we finally discuss to what extent consistency proofs help to justify the application of complex learning algorithms.","sentences":["Tree-based methods such as Random Forests are learning algorithms that have become an integral part of the statistical toolbox.","The last decade has shed some light on theoretical properties such as their consistency for regression tasks.","However, the usual proofs assume normal error terms as well as an additive regression function and are rather technical.","We overcome these issues by introducing a simple and catchy technique for proofing consistency under quite general assumptions.","To this end, we introduce a new class of naive trees, which do the subspacing completely at random and independent of the data.","We then give a direct proof of their consistency.","Using them to bound the error of more complex tree-based approaches such as univariate and multivariate CARTs, Extra Randomized Trees, or Random Forests, we deduce the consistency of all of them.","Since naive trees appear to be too simple for actual application, we further analyze their finite sample properties in a simulation and small benchmark study.","We find a slow convergence speed and a rather poor predictive performance.","Based on these results, we finally discuss to what extent consistency proofs help to justify the application of complex learning algorithms."],"url":"http://arxiv.org/abs/2404.06850v1","category":"math.ST"}
{"created":"2024-04-10 09:20:02","title":"Higher Order Lipschitz Sandwich Theorems","abstract":"We investigate the consequence of two Lip$(\\gamma)$ functions, in the sense of Stein, being close throughout a subset of their domain. A particular consequence of our results is the following. Given $K_0 > \\varepsilon > 0$ and $\\gamma > \\eta > 0$ there is a constant $\\delta = \\delta(\\gamma,\\eta,\\varepsilon,K_0) > 0$ for which the following is true. Let $\\Sigma \\subset \\mathbb{R}^d$ be closed and $f , h : \\Sigma \\to \\mathbb{R}$ be Lip$(\\gamma)$ functions whose Lip$(\\gamma)$ norms are both bounded above by $K_0$. Suppose $B \\subset \\Sigma$ is closed and that $f$ and $h$ coincide throughout $B$. Then over the set of points in $\\Sigma$ whose distance to $B$ is at most $\\delta$ we have that the Lip$(\\eta)$ norm of the difference $f-h$ is bounded above by $\\varepsilon$. More generally, we establish that this phenomenon remains valid in a less restrictive Banach space setting under the weaker hypothesis that the two Lip$(\\gamma)$ functions $f$ and $h$ are only close in a pointwise sense throughout the closed subset $B$. We require only that the subset $\\Sigma$ be closed; in particular, the case that $\\Sigma$ is finite is covered by our results. The restriction that $\\eta < \\gamma$ is sharp in the sense that our result is false for $\\eta := \\gamma$.","sentences":["We investigate the consequence of two Lip$(\\gamma)$ functions, in the sense of Stein, being close throughout a subset of their domain.","A particular consequence of our results is the following.","Given $K_0 >","\\varepsilon > 0$ and $\\gamma > \\eta > 0$ there is a constant $\\delta = \\delta(\\gamma,\\eta,\\varepsilon,K_0) > 0$ for which the following is true.","Let $\\Sigma \\subset \\mathbb{R}^d$ be closed and $f , h : \\Sigma \\to \\mathbb{R}$ be Lip$(\\gamma)$ functions whose Lip$(\\gamma)$ norms are both bounded above by $K_0$. Suppose $B \\subset \\Sigma$ is closed and that $f$ and $h$ coincide throughout $B$. Then over the set of points in $\\Sigma$ whose distance to $B$ is at most $\\delta$ we have that the Lip$(\\eta)$ norm of the difference $f-h$ is bounded above by $\\varepsilon$. More generally, we establish that this phenomenon remains valid in a less restrictive Banach space setting under the weaker hypothesis that the two Lip$(\\gamma)$ functions $f$ and $h$ are only close in a pointwise sense throughout the closed subset $B$. We require only that the subset $\\Sigma$ be closed; in particular, the case that $\\Sigma$ is finite is covered by our results.","The restriction that $\\eta < \\gamma$ is sharp in the sense that our result is false for $\\eta := \\gamma$."],"url":"http://arxiv.org/abs/2404.06849v1","category":"math.CA"}
{"created":"2024-04-10 09:18:01","title":"Quadratically Regularized Optimal Transport: Existence and Multiplicity of Potentials","abstract":"The optimal transport problem with quadratic regularization is useful when sparse couplings are desired. The density of the optimal coupling is described by two functions called potentials; equivalently, potentials can be defined as a solution of the dual problem. We prove the existence of potentials for a general square-integrable cost. Potentials are not necessarily unique, a phenomenon directly related to sparsity of the optimal support. For discrete problems, we describe the family of all potentials based on the connected components of the support, for a graph-theoretic notion of connectedness. On the other hand, we show that continuous problems have unique potentials under standard regularity assumptions, regardless of sparsity. Using potentials, we prove that the optimal support is indeed sparse for small regularization parameter in a continuous setting with quadratic cost, which seems to be the first theoretical guarantee for sparsity in this context.","sentences":["The optimal transport problem with quadratic regularization is useful when sparse couplings are desired.","The density of the optimal coupling is described by two functions called potentials; equivalently, potentials can be defined as a solution of the dual problem.","We prove the existence of potentials for a general square-integrable cost.","Potentials are not necessarily unique, a phenomenon directly related to sparsity of the optimal support.","For discrete problems, we describe the family of all potentials based on the connected components of the support, for a graph-theoretic notion of connectedness.","On the other hand, we show that continuous problems have unique potentials under standard regularity assumptions, regardless of sparsity.","Using potentials, we prove that the optimal support is indeed sparse for small regularization parameter in a continuous setting with quadratic cost, which seems to be the first theoretical guarantee for sparsity in this context."],"url":"http://arxiv.org/abs/2404.06847v1","category":"math.OC"}
{"created":"2024-04-10 09:17:22","title":"Register Your Forests: Decision Tree Ensemble Optimization by Explicit CPU Register Allocation","abstract":"Bringing high-level machine learning models to efficient and well-suited machine implementations often invokes a bunch of tools, e.g.~code generators, compilers, and optimizers. Along such tool chains, abstractions have to be applied. This leads to not optimally used CPU registers. This is a shortcoming, especially in resource constrained embedded setups. In this work, we present a code generation approach for decision tree ensembles, which produces machine assembly code within a single conversion step directly from the high-level model representation. Specifically, we develop various approaches to effectively allocate registers for the inference of decision tree ensembles. Extensive evaluations of the proposed method are conducted in comparison to the basic realization of C code from the high-level machine learning model and succeeding compilation. The results show that the performance of decision tree ensemble inference can be significantly improved (by up to $\\approx1.6\\times$), if the methods are applied carefully to the appropriate scenario.","sentences":["Bringing high-level machine learning models to efficient and well-suited machine implementations often invokes a bunch of tools, e.g.~code generators, compilers, and optimizers.","Along such tool chains, abstractions have to be applied.","This leads to not optimally used CPU registers.","This is a shortcoming, especially in resource constrained embedded setups.","In this work, we present a code generation approach for decision tree ensembles, which produces machine assembly code within a single conversion step directly from the high-level model representation.","Specifically, we develop various approaches to effectively allocate registers for the inference of decision tree ensembles.","Extensive evaluations of the proposed method are conducted in comparison to the basic realization of C code from the high-level machine learning model and succeeding compilation.","The results show that the performance of decision tree ensemble inference can be significantly improved (by up to $\\approx1.6\\times$), if the methods are applied carefully to the appropriate scenario."],"url":"http://arxiv.org/abs/2404.06846v1","category":"cs.LG"}
{"created":"2024-04-10 09:14:28","title":"MoCha-Stereo: Motif Channel Attention Network for Stereo Matching","abstract":"Learning-based stereo matching techniques have made significant progress. However, existing methods inevitably lose geometrical structure information during the feature channel generation process, resulting in edge detail mismatches. In this paper, the Motif Cha}nnel Attention Stereo Matching Network (MoCha-Stereo) is designed to address this problem. We provide the Motif Channel Correlation Volume (MCCV) to determine more accurate edge matching costs. MCCV is achieved by projecting motif channels, which capture common geometric structures in feature channels, onto feature maps and cost volumes. In addition, edge variations in %potential feature channels of the reconstruction error map also affect details matching, we propose the Reconstruction Error Motif Penalty (REMP) module to further refine the full-resolution disparity estimation. REMP integrates the frequency information of typical channel features from the reconstruction error. MoCha-Stereo ranks 1st on the KITTI-2015 and KITTI-2012 Reflective leaderboards. Our structure also shows excellent performance in Multi-View Stereo. Code is avaliable at https://github.com/ZYangChen/MoCha-Stereo.","sentences":["Learning-based stereo matching techniques have made significant progress.","However, existing methods inevitably lose geometrical structure information during the feature channel generation process, resulting in edge detail mismatches.","In this paper, the Motif Cha}nnel Attention Stereo Matching Network (MoCha-Stereo) is designed to address this problem.","We provide the Motif Channel Correlation Volume (MCCV) to determine more accurate edge matching costs.","MCCV is achieved by projecting motif channels, which capture common geometric structures in feature channels, onto feature maps and cost volumes.","In addition, edge variations in %potential feature channels of the reconstruction error map also affect details matching, we propose the Reconstruction Error Motif Penalty (REMP) module to further refine the full-resolution disparity estimation.","REMP integrates the frequency information of typical channel features from the reconstruction error.","MoCha-Stereo ranks 1st on the KITTI-2015 and KITTI-2012 Reflective leaderboards.","Our structure also shows excellent performance in Multi-View Stereo.","Code is avaliable at https://github.com/ZYangChen/MoCha-Stereo."],"url":"http://arxiv.org/abs/2404.06842v1","category":"cs.CV"}
{"created":"2024-04-10 09:06:10","title":"Generalized homomorphisms and KK with extra structure","abstract":"We develop the approach via quasihomomorphisms and the universal algebra $qA$ to Kasparov's $KK$-theory, so as to cover versions of $KK$ such as $KK^{nuc}$, $KK^G$ and ideal related $KK$-theory.","sentences":["We develop the approach via quasihomomorphisms and the universal algebra $qA$ to Kasparov's $KK$-theory, so as to cover versions of $KK$ such as $KK^{nuc}$, $KK^G$ and ideal related $KK$-theory."],"url":"http://arxiv.org/abs/2404.06840v1","category":"math.KT"}
{"created":"2024-04-10 09:05:59","title":"On the radially deformed Fourier transform","abstract":"In this paper we consider the kernel of the radially deformed Fourier transform introduced in the context of Clifford analysis in [10]. By adapting the Laplace transform method from [4], we obtain the Laplace domain expressions of the kernel for the cases of $m=2$ and $m > 2$ when $1+c=\\frac{1}{n}, n\\in \\mathbb{N}_0\\backslash\\{1\\}$ with $n$ odd. Moreover, we show that the expressions can be simplified using the Poisson kernel and the generating function of the Gegenbauer polynomials. As a consequence, the inverse formulas are used to get the integral expressions of the kernel in terms of Mittag-Leffler functions.","sentences":["In this paper we consider the kernel of the radially deformed Fourier transform introduced in the context of Clifford analysis in [10].","By adapting the Laplace transform method from [4], we obtain the Laplace domain expressions of the kernel for the cases of $m=2$ and $m > 2$ when $1+c=\\frac{1}{n}, n\\in \\mathbb{N}_0\\backslash\\{1\\}$ with $n$ odd.","Moreover, we show that the expressions can be simplified using the Poisson kernel and the generating function of the Gegenbauer polynomials.","As a consequence, the inverse formulas are used to get the integral expressions of the kernel in terms of Mittag-Leffler functions."],"url":"http://arxiv.org/abs/2404.06839v1","category":"math.CA"}
{"created":"2024-04-10 08:59:01","title":"Sensitivity analysis for publication bias in meta-analysis of sparse data based on exact likelihood","abstract":"Meta-analysis is a powerful tool to synthesize findings from multiple studies. The normal-normal random-effects model is widely used accounting for between-study heterogeneity. However, meta-analysis of sparse data, which may arise when the event rate is low for binary or count outcomes, poses a challenge to the normal-normal random-effects model in the accuracy and stability in inference since the normal approximation in the within-study likelihood may not be good. To reduce bias arising from data sparsity, the generalized linear mixed model can be used by replacing the approximate normal within-study likelihood with an exact likelihood. Publication bias is one of the most serious threats in meta-analysis. Several objective sensitivity analysis methods for evaluating potential impacts of selective publication are available for the normal-normal random-effects model. We propose a sensitivity analysis method by extending the likelihood-based sensitivity analysis with the $t$-statistic selection function of Copas to several generalized linear mixed-effects models. Through applications of our proposed method to several real-world meta-analyses and simulation studies, the proposed method was proven to outperform the likelihood-based sensitivity analysis based on the normal-normal model. The proposed method would give a useful guidance to address publication bias in meta-analysis of sparse data.","sentences":["Meta-analysis is a powerful tool to synthesize findings from multiple studies.","The normal-normal random-effects model is widely used accounting for between-study heterogeneity.","However, meta-analysis of sparse data, which may arise when the event rate is low for binary or count outcomes, poses a challenge to the normal-normal random-effects model in the accuracy and stability in inference since the normal approximation in the within-study likelihood may not be good.","To reduce bias arising from data sparsity, the generalized linear mixed model can be used by replacing the approximate normal within-study likelihood with an exact likelihood.","Publication bias is one of the most serious threats in meta-analysis.","Several objective sensitivity analysis methods for evaluating potential impacts of selective publication are available for the normal-normal random-effects model.","We propose a sensitivity analysis method by extending the likelihood-based sensitivity analysis with the $t$-statistic selection function of Copas to several generalized linear mixed-effects models.","Through applications of our proposed method to several real-world meta-analyses and simulation studies, the proposed method was proven to outperform the likelihood-based sensitivity analysis based on the normal-normal model.","The proposed method would give a useful guidance to address publication bias in meta-analysis of sparse data."],"url":"http://arxiv.org/abs/2404.06837v1","category":"stat.ME"}
{"created":"2024-04-10 08:47:57","title":"Optimal Regret with Limited Adaptivity for Generalized Linear Contextual Bandits","abstract":"We study the generalized linear contextual bandit problem within the requirements of limited adaptivity. In this paper, we present two algorithms, \\texttt{B-GLinCB} and \\texttt{RS-GLinCB}, that address, respectively, two prevalent limited adaptivity models: batch learning with stochastic contexts and rare policy switches with adversarial contexts. For both these models, we establish essentially tight regret bounds. Notably, in the obtained bounds, we manage to eliminate a dependence on a key parameter $\\kappa$, which captures the non-linearity of the underlying reward model. For our batch learning algorithm \\texttt{B-GLinCB}, with $\\Omega\\left( \\log{\\log T} \\right)$ batches, the regret scales as $\\tilde{O}(\\sqrt{T})$. Further, we establish that our rarely switching algorithm \\texttt{RS-GLinCB} updates its policy at most $\\tilde{O}(\\log^2 T)$ times and achieves a regret of $\\tilde{O}(\\sqrt{T})$. Our approach for removing the dependence on $\\kappa$ for generalized linear contextual bandits might be of independent interest.","sentences":["We study the generalized linear contextual bandit problem within the requirements of limited adaptivity.","In this paper, we present two algorithms, \\texttt{B-GLinCB} and \\texttt{RS-GLinCB}, that address, respectively, two prevalent limited adaptivity models: batch learning with stochastic contexts and rare policy switches with adversarial contexts.","For both these models, we establish essentially tight regret bounds.","Notably, in the obtained bounds, we manage to eliminate a dependence on a key parameter $\\kappa$, which captures the non-linearity of the underlying reward model.","For our batch learning algorithm \\texttt{B-GLinCB}, with $\\Omega\\left( \\log{\\log T} \\right)$ batches, the regret scales as $\\tilde{O}(\\sqrt{T})$. Further, we establish that our rarely switching algorithm \\texttt{RS-GLinCB} updates its policy at most $\\tilde{O}(\\log^2 T)$ times and achieves a regret of $\\tilde{O}(\\sqrt{T})$. Our approach for removing the dependence on $\\kappa$ for generalized linear contextual bandits might be of independent interest."],"url":"http://arxiv.org/abs/2404.06831v1","category":"cs.LG"}
{"created":"2024-04-10 08:38:09","title":"EMF Mitigation via 5G and 6G MAC Scheduling","abstract":"High antenna directivity allows for high throughput transmission but also increases the exposure to electromagnetic field (EMF) of the end-users. Health regulations impose limitations on the incident power density, that generate a negative impact on network performance. In this work we focus at the slot-by-slot operations of a cellular Medium Access Control (MAC) scheduler to constrain the short-term EMF exposure upon real-time resource allocation, minimizing the impacts on network performance. We assume that the long-term EMF exposure is controlled by a proper outer-loop technique, that is not the object of this paper. Due to the minimal computational complexity allowed in MAC scheduling, existing solutions allowing practical implementation are few and focused at sub-optimal approaches curbing radio resource allocation. Our contribution is the derivation of a computationally efficient water-filling solution to allocate power and - then - resources, with a feasible integration of the necessary algorithms in the operations of a 5G MAC scheduler. We finally evaluate our proposal versus the prior art approaches with system level simulations with realistic modeling of physical and MAC level cellular procedures. We conclude that our proposal can control EMF with considerable less impact on network performance, making it a standout candidate for 5G and future 6G MAC scheduler implementations.","sentences":["High antenna directivity allows for high throughput transmission but also increases the exposure to electromagnetic field (EMF) of the end-users.","Health regulations impose limitations on the incident power density, that generate a negative impact on network performance.","In this work we focus at the slot-by-slot operations of a cellular Medium Access Control (MAC) scheduler to constrain the short-term EMF exposure upon real-time resource allocation, minimizing the impacts on network performance.","We assume that the long-term EMF exposure is controlled by a proper outer-loop technique, that is not the object of this paper.","Due to the minimal computational complexity allowed in MAC scheduling, existing solutions allowing practical implementation are few and focused at sub-optimal approaches curbing radio resource allocation.","Our contribution is the derivation of a computationally efficient water-filling solution to allocate power and - then - resources, with a feasible integration of the necessary algorithms in the operations of a 5G MAC scheduler.","We finally evaluate our proposal versus the prior art approaches with system level simulations with realistic modeling of physical and MAC level cellular procedures.","We conclude that our proposal can control EMF with considerable less impact on network performance, making it a standout candidate for 5G and future 6G MAC scheduler implementations."],"url":"http://arxiv.org/abs/2404.06830v1","category":"cs.NI"}
{"created":"2024-04-10 08:32:29","title":"Proposed modified computational model for the amoeba-inspired combinatorial optimization machine","abstract":"A single-celled amoeba can solve the traveling salesman problem through its shape-changing dynamics. In this paper, we examine roles of several elements in a previously proposed computational model of the solution-search process of amoeba and three modifications towards enhancing the solution-search preformance. We find that appropriate modifications can indeed significantly improve the quality of solutions. It is also found that a condition associated with the volume conservation can also be modified in contrast to the naive belief that it is indispensable for the solution-search ability of amoeba. A proposed modified model shows much better performance.","sentences":["A single-celled amoeba can solve the traveling salesman problem through its shape-changing dynamics.","In this paper, we examine roles of several elements in a previously proposed computational model of the solution-search process of amoeba and three modifications towards enhancing the solution-search preformance.","We find that appropriate modifications can indeed significantly improve the quality of solutions.","It is also found that a condition associated with the volume conservation can also be modified in contrast to the naive belief that it is indispensable for the solution-search ability of amoeba.","A proposed modified model shows much better performance."],"url":"http://arxiv.org/abs/2404.06828v1","category":"cs.NE"}
{"created":"2024-04-10 08:27:46","title":"A Global Stochastic Maximum Principle for Mean-Field Forward-Backward Stochastic Control Systems with Quadratic Generators","abstract":"Our paper is devoted to the study of Peng's stochastic maximum principle (SMP) for a stochastic control problem composed of a controlled forward stochastic differential equation (SDE) as dynamics and a controlled backward SDE which defines the cost functional. Our studies combine the difficulties which come, on one hand, from the fact that the coefficients of both the SDE and the backward SDE are of mean-field type (i.e., they do not only depend on the control process and the solution processes but also on their law), and on the other hand, from the fact that the coefficient of the BSDE is of quadratic growth in $Z$. Our SMP is novel, it extends in a by far non trivial way existing results on SMP.","sentences":["Our paper is devoted to the study of Peng's stochastic maximum principle (SMP) for a stochastic control problem composed of a controlled forward stochastic differential equation (SDE) as dynamics and a controlled backward SDE which defines the cost functional.","Our studies combine the difficulties which come, on one hand, from the fact that the coefficients of both the SDE and the backward SDE are of mean-field type (i.e., they do not only depend on the control process and the solution processes but also on their law), and on the other hand, from the fact that the coefficient of the BSDE is of quadratic growth in $Z$. Our SMP is novel, it extends in a by far non trivial way existing results on SMP."],"url":"http://arxiv.org/abs/2404.06826v1","category":"math.OC"}
{"created":"2024-04-10 08:13:21","title":"A proposal for a revised meta-architecture of intelligent tutoring systems to foster explainability and transparency for educators","abstract":"This contribution draws attention to implications connected with meta-architectural design decisions for intelligent tutoring systems in the context of formative assessments. As a first result of addressing this issue, this contribution presents a meta-architectural system design that includes the role of educators.","sentences":["This contribution draws attention to implications connected with meta-architectural design decisions for intelligent tutoring systems in the context of formative assessments.","As a first result of addressing this issue, this contribution presents a meta-architectural system design that includes the role of educators."],"url":"http://arxiv.org/abs/2404.06820v1","category":"cs.HC"}
{"created":"2024-04-10 07:56:26","title":"Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation","abstract":"The rapid development of large language models has led to the widespread adoption of Retrieval-Augmented Generation (RAG), which integrates external knowledge to alleviate knowledge bottlenecks and mitigate hallucinations. However, the existing RAG paradigm inevitably suffers from the impact of flawed information introduced during the retrieval phrase, thereby diminishing the reliability and correctness of the generated outcomes. In this paper, we propose Credibility-aware Generation (CAG), a universally applicable framework designed to mitigate the impact of flawed information in RAG. At its core, CAG aims to equip models with the ability to discern and process information based on its credibility. To this end, we propose an innovative data transformation framework that generates data based on credibility, thereby effectively endowing models with the capability of CAG. Furthermore, to accurately evaluate the models' capabilities of CAG, we construct a comprehensive benchmark covering three critical real-world scenarios. Experimental results demonstrate that our model can effectively understand and utilize credibility for generation, significantly outperform other models with retrieval augmentation, and exhibit resilience against the disruption caused by noisy documents, thereby maintaining robust performance. Moreover, our model supports customized credibility, offering a wide range of potential applications.","sentences":["The rapid development of large language models has led to the widespread adoption of Retrieval-Augmented Generation (RAG), which integrates external knowledge to alleviate knowledge bottlenecks and mitigate hallucinations.","However, the existing RAG paradigm inevitably suffers from the impact of flawed information introduced during the retrieval phrase, thereby diminishing the reliability and correctness of the generated outcomes.","In this paper, we propose Credibility-aware Generation (CAG), a universally applicable framework designed to mitigate the impact of flawed information in RAG.","At its core, CAG aims to equip models with the ability to discern and process information based on its credibility.","To this end, we propose an innovative data transformation framework that generates data based on credibility, thereby effectively endowing models with the capability of CAG.","Furthermore, to accurately evaluate the models' capabilities of CAG, we construct a comprehensive benchmark covering three critical real-world scenarios.","Experimental results demonstrate that our model can effectively understand and utilize credibility for generation, significantly outperform other models with retrieval augmentation, and exhibit resilience against the disruption caused by noisy documents, thereby maintaining robust performance.","Moreover, our model supports customized credibility, offering a wide range of potential applications."],"url":"http://arxiv.org/abs/2404.06809v1","category":"cs.CL"}
{"created":"2024-04-10 07:55:10","title":"Formation-Controlled Dimensionality Reduction","abstract":"Dimensionality reduction represents the process of generating a low dimensional representation of high dimensional data. Motivated by the formation control of mobile agents, we propose a nonlinear dynamical system for dimensionality reduction. The system consists of two parts; the control of neighbor points, addressing local structures, and the control of remote points, accounting for global structures. We also include a brief mathematical observation of the model and its numerical procedure. Numerical experiments are performed on both synthetic and real datasets and comparisons with existing models demonstrate the soundness and effectiveness of the proposed model.","sentences":["Dimensionality reduction represents the process of generating a low dimensional representation of high dimensional data.","Motivated by the formation control of mobile agents, we propose a nonlinear dynamical system for dimensionality reduction.","The system consists of two parts; the control of neighbor points, addressing local structures, and the control of remote points, accounting for global structures.","We also include a brief mathematical observation of the model and its numerical procedure.","Numerical experiments are performed on both synthetic and real datasets and comparisons with existing models demonstrate the soundness and effectiveness of the proposed model."],"url":"http://arxiv.org/abs/2404.06808v1","category":"cs.LG"}
{"created":"2024-04-10 07:49:48","title":"A new way to evaluate G-Wishart normalising constants via Fourier analysis","abstract":"The G-Wishart distribution is an essential component for the Bayesian analysis of Gaussian graphical models as the conjugate prior for the precision matrix. Evaluating the marginal likelihood of such models usually requires computing high-dimensional integrals to determine the G-Wishart normalising constant. Closed-form results are known for decomposable or chordal graphs, while an explicit representation as a formal series expansion has been derived recently for general graphs. The nested infinite sums, however, do not lend themselves to computation, remaining of limited practical value. Borrowing techniques from random matrix theory and Fourier analysis, we provide novel exact results well suited to the numerical evaluation of the normalising constant for a large class of graphs beyond chordal graphs. Furthermore, they open new possibilities for developing more efficient sampling schemes for Bayesian inference of Gaussian graphical models.","sentences":["The G-Wishart distribution is an essential component for the Bayesian analysis of Gaussian graphical models as the conjugate prior for the precision matrix.","Evaluating the marginal likelihood of such models usually requires computing high-dimensional integrals to determine the G-Wishart normalising constant.","Closed-form results are known for decomposable or chordal graphs, while an explicit representation as a formal series expansion has been derived recently for general graphs.","The nested infinite sums, however, do not lend themselves to computation, remaining of limited practical value.","Borrowing techniques from random matrix theory and Fourier analysis, we provide novel exact results well suited to the numerical evaluation of the normalising constant for a large class of graphs beyond chordal graphs.","Furthermore, they open new possibilities for developing more efficient sampling schemes for Bayesian inference of Gaussian graphical models."],"url":"http://arxiv.org/abs/2404.06803v1","category":"stat.ME"}
{"created":"2024-04-10 07:45:06","title":"A general class of iterative splitting methods for solving linear systems","abstract":"Recently Ahmadi et al. (2021) and Tagliaferro (2022) proposed some iterative methods for the numerical solution of linear systems which, under the classical hypothesis of strict diagonal dominance, typically converge faster than the Jacobi method, but slower than the forward/backward Gauss-Seidel one. In this paper we introduce a general class of iterative methods, based on suitable splittings of the matrix that defines the system, which include all of the methods mentioned above and have the same cost per iteration in a sequential computation environment. We also introduce a partial order relation in the set of the splittings and, partly theoretically and partly on the basis of a certain number of examples, we show that such partial order is typically connected to the speed of convergence of the corresponding methods. We pay particular attention to the case of linear systems for which the Jacobi iteration matrix is nonnegative, in which case we give a rigorous proof of the correspondence between the partial order relation and the magnitude of the spectral radius of the iteration matrices. Within the considered general class, some new specific promising methods are proposed as well.","sentences":["Recently Ahmadi et al. (2021) and Tagliaferro (2022) proposed some iterative methods for the numerical solution of linear systems which, under the classical hypothesis of strict diagonal dominance, typically converge faster than the Jacobi method, but slower than the forward/backward Gauss-Seidel one.","In this paper we introduce a general class of iterative methods, based on suitable splittings of the matrix that defines the system, which include all of the methods mentioned above and have the same cost per iteration in a sequential computation environment.","We also introduce a partial order relation in the set of the splittings and, partly theoretically and partly on the basis of a certain number of examples, we show that such partial order is typically connected to the speed of convergence of the corresponding methods.","We pay particular attention to the case of linear systems for which the Jacobi iteration matrix is nonnegative, in which case we give a rigorous proof of the correspondence between the partial order relation and the magnitude of the spectral radius of the iteration matrices.","Within the considered general class, some new specific promising methods are proposed as well."],"url":"http://arxiv.org/abs/2404.06800v1","category":"math.NA"}
{"created":"2024-04-10 07:41:35","title":"MedRG: Medical Report Grounding with Multi-modal Large Language Model","abstract":"Medical Report Grounding is pivotal in identifying the most relevant regions in medical images based on a given phrase query, a critical aspect in medical image analysis and radiological diagnosis. However, prevailing visual grounding approaches necessitate the manual extraction of key phrases from medical reports, imposing substantial burdens on both system efficiency and physicians. In this paper, we introduce a novel framework, Medical Report Grounding (MedRG), an end-to-end solution for utilizing a multi-modal Large Language Model to predict key phrase by incorporating a unique token, BOX, into the vocabulary to serve as an embedding for unlocking detection capabilities. Subsequently, the vision encoder-decoder jointly decodes the hidden embedding and the input medical image, generating the corresponding grounding box. The experimental results validate the effectiveness of MedRG, surpassing the performance of the existing state-of-the-art medical phrase grounding methods. This study represents a pioneering exploration of the medical report grounding task, marking the first-ever endeavor in this domain.","sentences":["Medical Report Grounding is pivotal in identifying the most relevant regions in medical images based on a given phrase query, a critical aspect in medical image analysis and radiological diagnosis.","However, prevailing visual grounding approaches necessitate the manual extraction of key phrases from medical reports, imposing substantial burdens on both system efficiency and physicians.","In this paper, we introduce a novel framework, Medical Report Grounding (MedRG), an end-to-end solution for utilizing a multi-modal Large Language Model to predict key phrase by incorporating a unique token, BOX, into the vocabulary to serve as an embedding for unlocking detection capabilities.","Subsequently, the vision encoder-decoder jointly decodes the hidden embedding and the input medical image, generating the corresponding grounding box.","The experimental results validate the effectiveness of MedRG, surpassing the performance of the existing state-of-the-art medical phrase grounding methods.","This study represents a pioneering exploration of the medical report grounding task, marking the first-ever endeavor in this domain."],"url":"http://arxiv.org/abs/2404.06798v1","category":"cs.CV"}
{"created":"2024-04-10 07:35:45","title":"Constraining Cosmological Physics with DESI BAO Observations","abstract":"The DESI year one observations can help probe new physics on cosmological scales. In light of the latest DESI BAO measurements, we constrain four popular cosmological scenarios including inflation, modified gravity, annihilating dark matter and interacting dark energy. Using a data combination of BICEP/Keck array, cosmic microwave background and DESI, we obtain the $1\\sigma$ and $2\\sigma$ constraints on the tensor-to-scalar ratio $r_{0.05}= 0.0176^{+0.0070}_{-0.0130}$ and $r_{0.05}=0.018^{+0.020}_{-0.017}$. Using the combination of cosmic microwave background and DESI, we find a $2.4\\sigma$ evidence for gravitational theories beyond the general relativity, shrinks the dark matter annihilation cross-section by $12\\%$ relative to cosmic microwave background, and obtain a $1.6\\sigma$ hint of the positive interaction between dark matter and dark energy indicating that energy may be transferred from dark matter to dark energy in the dark sector of the universe. Future DESI observations could go a step further to explore the nature of inflation, dark matter and dark energy, and test the validity of general relativity on cosmological scales.","sentences":["The DESI year one observations can help probe new physics on cosmological scales.","In light of the latest DESI BAO measurements, we constrain four popular cosmological scenarios including inflation, modified gravity, annihilating dark matter and interacting dark energy.","Using a data combination of BICEP/Keck array, cosmic microwave background and DESI, we obtain the $1\\sigma$ and $2\\sigma$ constraints on the tensor-to-scalar ratio $r_{0.05}= 0.0176^{+0.0070}_{-0.0130}$ and $r_{0.05}=0.018^{+0.020}_{-0.017}$. Using the combination of cosmic microwave background and DESI, we find a $2.4\\sigma$ evidence for gravitational theories beyond the general relativity, shrinks the dark matter annihilation cross-section by $12\\%$ relative to cosmic microwave background, and obtain a $1.6\\sigma$ hint of the positive interaction between dark matter and dark energy indicating that energy may be transferred from dark matter to dark energy in the dark sector of the universe.","Future DESI observations could go a step further to explore the nature of inflation, dark matter and dark energy, and test the validity of general relativity on cosmological scales."],"url":"http://arxiv.org/abs/2404.06796v1","category":"astro-ph.CO"}
{"created":"2024-04-10 07:06:51","title":"Data-driven modeling of unsteady flow based on deep operator network","abstract":"Time-dependent flow fields are typically generated by a computational fluid dynamics (CFD) method, which is an extremely time-consuming process. However, the latent relationship between the flow fields is governed by the Navier-Stokes equations and can be described by an operator. We therefore train a deep operator network, or simply DeepONet, to learn the temporal evolution between flow snapshots. Once properly trained, given a few consecutive snapshots as input, the network has a great potential to generate the next snapshot accurately and quickly. Using the output as a new input, the network iterates the process, generating a series of successive snapshots with little wall time. Specifically, we consider 2D flow around a circular cylinder at Reynolds number 1000, and prepare a set of high-fidelity data using a high-order spectral/hp element method as ground truth. Although the flow fields are periodic, there are many small-scale features in the wake flow that are difficult to generate accurately. Furthermore, any discrepancy between the prediction and the ground truth for the first snapshots can easily accumulate during the iterative process, which eventually amplifies the overall deviations. Therefore, we propose two alternative techniques to improve the training of DeepONet. The first one enhances the feature extraction of the network by harnessing the \"multi-head non-local block\". The second one refines the network parameters by leveraging the local smooth optimization technique. Both techniques prove to be highly effective in reducing the cumulative errors and our results outperform those of the dynamic mode decomposition method.","sentences":["Time-dependent flow fields are typically generated by a computational fluid dynamics (CFD) method, which is an extremely time-consuming process.","However, the latent relationship between the flow fields is governed by the Navier-Stokes equations and can be described by an operator.","We therefore train a deep operator network, or simply DeepONet, to learn the temporal evolution between flow snapshots.","Once properly trained, given a few consecutive snapshots as input, the network has a great potential to generate the next snapshot accurately and quickly.","Using the output as a new input, the network iterates the process, generating a series of successive snapshots with little wall time.","Specifically, we consider 2D flow around a circular cylinder at Reynolds number 1000, and prepare a set of high-fidelity data using a high-order spectral/hp element method as ground truth.","Although the flow fields are periodic, there are many small-scale features in the wake flow that are difficult to generate accurately.","Furthermore, any discrepancy between the prediction and the ground truth for the first snapshots can easily accumulate during the iterative process, which eventually amplifies the overall deviations.","Therefore, we propose two alternative techniques to improve the training of DeepONet.","The first one enhances the feature extraction of the network by harnessing the \"multi-head non-local block\".","The second one refines the network parameters by leveraging the local smooth optimization technique.","Both techniques prove to be highly effective in reducing the cumulative errors and our results outperform those of the dynamic mode decomposition method."],"url":"http://arxiv.org/abs/2404.06791v1","category":"physics.flu-dyn"}
{"created":"2024-04-10 07:04:18","title":"Future stability of perfect fluids with extreme tilt and linear equation of state $p=c_s^2\u03c1$ for the Einstein-Euler system with positive cosmological constant: The range $\\frac{1}{3}<c_s^2<\\frac{3}{7}$","abstract":"We study the future stability of cosmological fluids, in spacetimes with an accelerated expansion, which exhibit extreme tilt behavior, ie. their fluid velocity becoming asymptotically null at timelike infinity. It has been predicted in the article \\cite{LEUW} that the latter behavior is dominant for sound speeds beyond radiation $c_s=1/\\sqrt{3}$, hence, bifurcating off of the stable orthogonal fluid behavior modeled by the classical FLRW family of solutions, for $c_s^2\\in[0,\\frac{1}{3}]$. First, we construct homogeneous solutions to the Einstein-Euler system with the latter behavior, in $\\mathbb{S}^3$ spatial topology, for sound speeds $c_s^2\\in(\\frac{1}{3},1)$. Then, we study their future dynamics and prove a global stability result in the restricted range $c_s^2\\in(\\frac{1}{3},\\frac{3}{7})$. In particular, we show that extreme tilt behavior persists to sufficiently small perturbations of the homogeneous backgrounds, without any symmetry assumptions or analyticity. Our method is based on a bootstrap argument, in weighted Sobolev spaces, capturing the exponential decay of suitable renormalized variables. Extreme tilt behavior is associated with a degeneracy in the top order energy estimates that we derive, which allows us to complete our bootstrap argument only in the aforementioned restricted range of sound speeds. Interestingly, this is a degeneracy that does not appear in the study of formal series expansions. Moreover, for the Euler equations on a fixed FLRW background, our estimates can be improved to treat the entire beyond radiation interval $c_s^2\\in(\\frac{1}{3},1)$, a result already obtained in \\cite{MO}. The latter indicates that the former issue is related to the general inhomogeneous geometry of the perturbed metric in the coupled to Einstein case.","sentences":["We study the future stability of cosmological fluids, in spacetimes with an accelerated expansion, which exhibit extreme tilt behavior, ie.","their fluid velocity becoming asymptotically null at timelike infinity.","It has been predicted in the article \\cite{LEUW} that the latter behavior is dominant for sound speeds beyond radiation $c_s=1/\\sqrt{3}$, hence, bifurcating off of the stable orthogonal fluid behavior modeled by the classical FLRW family of solutions, for $c_s^2\\in[0,\\frac{1}{3}]$. First, we construct homogeneous solutions to the Einstein-Euler system with the latter behavior, in $\\mathbb{S}^3$ spatial topology, for sound speeds $c_s^2\\in(\\frac{1}{3},1)$. Then, we study their future dynamics and prove a global stability result in the restricted range $c_s^2\\in(\\frac{1}{3},\\frac{3}{7})$. In particular, we show that extreme tilt behavior persists to sufficiently small perturbations of the homogeneous backgrounds, without any symmetry assumptions or analyticity.","Our method is based on a bootstrap argument, in weighted Sobolev spaces, capturing the exponential decay of suitable renormalized variables.","Extreme tilt behavior is associated with a degeneracy in the top order energy estimates that we derive, which allows us to complete our bootstrap argument only in the aforementioned restricted range of sound speeds.","Interestingly, this is a degeneracy that does not appear in the study of formal series expansions.","Moreover, for the Euler equations on a fixed FLRW background, our estimates can be improved to treat the entire beyond radiation interval $c_s^2\\in(\\frac{1}{3},1)$, a result already obtained in \\cite{MO}.","The latter indicates that the former issue is related to the general inhomogeneous geometry of the perturbed metric in the coupled to Einstein case."],"url":"http://arxiv.org/abs/2404.06789v1","category":"math.AP"}
{"created":"2024-04-10 06:58:58","title":"Private Wasserstein Distance with Random Noises","abstract":"Wasserstein distance is a principle measure of data divergence from a distributional standpoint. However, its application becomes challenging in the context of data privacy, where sharing raw data is restricted. Prior attempts have employed techniques like Differential Privacy or Federated optimization to approximate Wasserstein distance. Nevertheless, these approaches often lack accuracy and robustness against potential attack. In this study, we investigate the underlying triangular properties within the Wasserstein space, leading to a straightforward solution named TriangleWad. This approach enables the computation of Wasserstein distance between datasets stored across different entities. Notably, TriangleWad is 20 times faster, making raw data information truly invisible, enhancing resilience against attacks, and without sacrificing estimation accuracy. Through comprehensive experimentation across various tasks involving both image and text data, we demonstrate its superior performance and generalizations.","sentences":["Wasserstein distance is a principle measure of data divergence from a distributional standpoint.","However, its application becomes challenging in the context of data privacy, where sharing raw data is restricted.","Prior attempts have employed techniques like Differential Privacy or Federated optimization to approximate Wasserstein distance.","Nevertheless, these approaches often lack accuracy and robustness against potential attack.","In this study, we investigate the underlying triangular properties within the Wasserstein space, leading to a straightforward solution named TriangleWad.","This approach enables the computation of Wasserstein distance between datasets stored across different entities.","Notably, TriangleWad is 20 times faster, making raw data information truly invisible, enhancing resilience against attacks, and without sacrificing estimation accuracy.","Through comprehensive experimentation across various tasks involving both image and text data, we demonstrate its superior performance and generalizations."],"url":"http://arxiv.org/abs/2404.06787v1","category":"cs.LG"}
{"created":"2024-04-10 06:54:33","title":"Gravitational production of massive scalars in the context of inflation","abstract":"We set up a formalism for calculating the energy density generated in a quantized massive scalar field in the course of the drastic change in spacetime geometry at the end of the inflationary era. The calculation relies on the notion of adiabatic vacuum. The Bogolubov coefficients are computed by employing the sudden approximation. After obtaining a general formula, we calculate explicitly the energy density generated in a particle species with $m/H \\ll1$, where $m$ is the particle mass and $H$ is the Hubble constant during the inflationary epoch. We find the contribution of the long-wavelength modes to be $\\propto H^5/m$. If such particles are very weakly interacting, they can come to dominate the total energy density in the Universe. Other cosmological implications are also discussed.","sentences":["We set up a formalism for calculating the energy density generated in a quantized massive scalar field in the course of the drastic change in spacetime geometry at the end of the inflationary era.","The calculation relies on the notion of adiabatic vacuum.","The Bogolubov coefficients are computed by employing the sudden approximation.","After obtaining a general formula, we calculate explicitly the energy density generated in a particle species with $m/H \\ll1$, where $m$ is the particle mass and $H$ is the Hubble constant during the inflationary epoch.","We find the contribution of the long-wavelength modes to be $\\propto H^5/m$.","If such particles are very weakly interacting, they can come to dominate the total energy density in the Universe.","Other cosmological implications are also discussed."],"url":"http://arxiv.org/abs/2404.06785v1","category":"astro-ph.CO"}
{"created":"2024-04-10 06:41:30","title":"Urban Architect: Steerable 3D Urban Scene Generation with Layout Prior","abstract":"Text-to-3D generation has achieved remarkable success via large-scale text-to-image diffusion models. Nevertheless, there is no paradigm for scaling up the methodology to urban scale. Urban scenes, characterized by numerous elements, intricate arrangement relationships, and vast scale, present a formidable barrier to the interpretability of ambiguous textual descriptions for effective model optimization. In this work, we surmount the limitations by introducing a compositional 3D layout representation into text-to-3D paradigm, serving as an additional prior. It comprises a set of semantic primitives with simple geometric structures and explicit arrangement relationships, complementing textual descriptions and enabling steerable generation. Upon this, we propose two modifications -- (1) We introduce Layout-Guided Variational Score Distillation to address model optimization inadequacies. It conditions the score distillation sampling process with geometric and semantic constraints of 3D layouts. (2) To handle the unbounded nature of urban scenes, we represent 3D scene with a Scalable Hash Grid structure, incrementally adapting to the growing scale of urban scenes. Extensive experiments substantiate the capability of our framework to scale text-to-3D generation to large-scale urban scenes that cover over 1000m driving distance for the first time. We also present various scene editing demonstrations, showing the powers of steerable urban scene generation. Website: https://urbanarchitect.github.io.","sentences":["Text-to-3D generation has achieved remarkable success via large-scale text-to-image diffusion models.","Nevertheless, there is no paradigm for scaling up the methodology to urban scale.","Urban scenes, characterized by numerous elements, intricate arrangement relationships, and vast scale, present a formidable barrier to the interpretability of ambiguous textual descriptions for effective model optimization.","In this work, we surmount the limitations by introducing a compositional 3D layout representation into text-to-3D paradigm, serving as an additional prior.","It comprises a set of semantic primitives with simple geometric structures and explicit arrangement relationships, complementing textual descriptions and enabling steerable generation.","Upon this, we propose two modifications -- (1) We introduce Layout-Guided Variational Score Distillation to address model optimization inadequacies.","It conditions the score distillation sampling process with geometric and semantic constraints of 3D layouts.","(2) To handle the unbounded nature of urban scenes, we represent 3D scene with a Scalable Hash Grid structure, incrementally adapting to the growing scale of urban scenes.","Extensive experiments substantiate the capability of our framework to scale text-to-3D generation to large-scale urban scenes that cover over 1000m driving distance for the first time.","We also present various scene editing demonstrations, showing the powers of steerable urban scene generation.","Website: https://urbanarchitect.github.io."],"url":"http://arxiv.org/abs/2404.06780v1","category":"cs.CV"}
{"created":"2024-04-10 06:39:18","title":"Efficient and Scalable Chinese Vector Font Generation via Component Composition","abstract":"Chinese vector font generation is challenging due to the complex structure and huge amount of Chinese characters. Recent advances remain limited to generating a small set of characters with simple structure. In this work, we first observe that most Chinese characters can be disassembled into frequently-reused components. Therefore, we introduce the first efficient and scalable Chinese vector font generation approach via component composition, allowing generating numerous vector characters from a small set of components. To achieve this, we collect a large-scale dataset that contains over \\textit{90K} Chinese characters with their components and layout information. Upon the dataset, we propose a simple yet effective framework based on spatial transformer networks (STN) and multiple losses tailored to font characteristics to learn the affine transformation of the components, which can be directly applied to the B\\'ezier curves, resulting in Chinese characters in vector format. Our qualitative and quantitative experiments have demonstrated that our method significantly surpasses the state-of-the-art vector font generation methods in generating large-scale complex Chinese characters in both font generation and zero-shot font extension.","sentences":["Chinese vector font generation is challenging due to the complex structure and huge amount of Chinese characters.","Recent advances remain limited to generating a small set of characters with simple structure.","In this work, we first observe that most Chinese characters can be disassembled into frequently-reused components.","Therefore, we introduce the first efficient and scalable Chinese vector font generation approach via component composition, allowing generating numerous vector characters from a small set of components.","To achieve this, we collect a large-scale dataset that contains over \\textit{90K} Chinese characters with their components and layout information.","Upon the dataset, we propose a simple yet effective framework based on spatial transformer networks (STN) and multiple losses tailored to font characteristics to learn the affine transformation of the components, which can be directly applied to the B\\'ezier curves, resulting in Chinese characters in vector format.","Our qualitative and quantitative experiments have demonstrated that our method significantly surpasses the state-of-the-art vector font generation methods in generating large-scale complex Chinese characters in both font generation and zero-shot font extension."],"url":"http://arxiv.org/abs/2404.06779v1","category":"cs.CV"}
{"created":"2024-04-10 06:36:48","title":"Responsible Federated Learning in Smart Transportation: Outlooks and Challenges","abstract":"Integrating artificial intelligence (AI) and federated learning (FL) in smart transportation has raised critical issues regarding their responsible use. Ensuring responsible AI is paramount for the stability and sustainability of intelligent transportation systems. Despite its importance, research on the responsible application of AI and FL in this domain remains nascent, with a paucity of in-depth investigations into their confluence. Our study analyzes the roles of FL in smart transportation, as well as the promoting effect of responsible AI on distributed smart transportation. Lastly, we discuss the challenges of developing and implementing responsible FL in smart transportation and propose potential solutions. By integrating responsible AI and federated learning, intelligent transportation systems are expected to achieve a higher degree of intelligence, personalization, safety, and transparency.","sentences":["Integrating artificial intelligence (AI) and federated learning (FL) in smart transportation has raised critical issues regarding their responsible use.","Ensuring responsible AI is paramount for the stability and sustainability of intelligent transportation systems.","Despite its importance, research on the responsible application of AI and FL in this domain remains nascent, with a paucity of in-depth investigations into their confluence.","Our study analyzes the roles of FL in smart transportation, as well as the promoting effect of responsible AI on distributed smart transportation.","Lastly, we discuss the challenges of developing and implementing responsible FL in smart transportation and propose potential solutions.","By integrating responsible AI and federated learning, intelligent transportation systems are expected to achieve a higher degree of intelligence, personalization, safety, and transparency."],"url":"http://arxiv.org/abs/2404.06777v1","category":"cs.NI"}
{"created":"2024-04-10 06:35:25","title":"Logit Calibration and Feature Contrast for Robust Federated Learning on Non-IID Data","abstract":"Federated learning (FL) is a privacy-preserving distributed framework for collaborative model training on devices in edge networks. However, challenges arise due to vulnerability to adversarial examples (AEs) and the non-independent and identically distributed (non-IID) nature of data distribution among devices, hindering the deployment of adversarially robust and accurate learning models at the edge. While adversarial training (AT) is commonly acknowledged as an effective defense strategy against adversarial attacks in centralized training, we shed light on the adverse effects of directly applying AT in FL that can severely compromise accuracy, especially in non-IID challenges. Given this limitation, this paper proposes FatCC, which incorporates local logit \\underline{C}alibration and global feature \\underline{C}ontrast into the vanilla federated adversarial training (\\underline{FAT}) process from both logit and feature perspectives. This approach can effectively enhance the federated system's robust accuracy (RA) and clean accuracy (CA). First, we propose logit calibration, where the logits are calibrated during local adversarial updates, thereby improving adversarial robustness. Second, FatCC introduces feature contrast, which involves a global alignment term that aligns each local representation with unbiased global features, thus further enhancing robustness and accuracy in federated adversarial environments. Extensive experiments across multiple datasets demonstrate that FatCC achieves comparable or superior performance gains in both CA and RA compared to other baselines.","sentences":["Federated learning (FL) is a privacy-preserving distributed framework for collaborative model training on devices in edge networks.","However, challenges arise due to vulnerability to adversarial examples (AEs) and the non-independent and identically distributed (non-IID) nature of data distribution among devices, hindering the deployment of adversarially robust and accurate learning models at the edge.","While adversarial training (AT) is commonly acknowledged as an effective defense strategy against adversarial attacks in centralized training, we shed light on the adverse effects of directly applying AT in FL that can severely compromise accuracy, especially in non-IID challenges.","Given this limitation, this paper proposes FatCC, which incorporates local logit \\underline{C}alibration and global feature \\underline{C}ontrast into the vanilla federated adversarial training (\\underline{FAT}) process from both logit and feature perspectives.","This approach can effectively enhance the federated system's robust accuracy (RA) and clean accuracy (CA).","First, we propose logit calibration, where the logits are calibrated during local adversarial updates, thereby improving adversarial robustness.","Second, FatCC introduces feature contrast, which involves a global alignment term that aligns each local representation with unbiased global features, thus further enhancing robustness and accuracy in federated adversarial environments.","Extensive experiments across multiple datasets demonstrate that FatCC achieves comparable or superior performance gains in both CA and RA compared to other baselines."],"url":"http://arxiv.org/abs/2404.06776v1","category":"cs.LG"}
{"created":"2024-04-10 06:31:58","title":"Probabilistic channel simulation using coherence","abstract":"Channel simulation using coherence, which refers to realizing a target channel with coherent states and free operations, is a fundamental problem in quantum resource theory. The limitations of the accuracy of deterministic channel simulation motivate us to consider the more general probabilistic framework. In this work, we show the relation between the maximal success probability and the accuracy of channel simulation with free operations. When the chosen free operation is the maximally incoherent operation (MIO), we provide an efficiently computable semidefinite program (SDP) to calculate the maximal success probability and derive the analytic expression of success probability for some special cases. When the chosen free operation is dephasing-covariant incoherent operations (DIO), it is proved that if the target channel is not a resource nonactivating channel, then one cannot simulate it exactly both deterministically and probabilistically. The SDP for maximal success probability of simulating channel by DIO is also given correspondingly.","sentences":["Channel simulation using coherence, which refers to realizing a target channel with coherent states and free operations, is a fundamental problem in quantum resource theory.","The limitations of the accuracy of deterministic channel simulation motivate us to consider the more general probabilistic framework.","In this work, we show the relation between the maximal success probability and the accuracy of channel simulation with free operations.","When the chosen free operation is the maximally incoherent operation (MIO), we provide an efficiently computable semidefinite program (SDP) to calculate the maximal success probability and derive the analytic expression of success probability for some special cases.","When the chosen free operation is dephasing-covariant incoherent operations (DIO), it is proved that if the target channel is not a resource nonactivating channel, then one cannot simulate it exactly both deterministically and probabilistically.","The SDP for maximal success probability of simulating channel by DIO is also given correspondingly."],"url":"http://arxiv.org/abs/2404.06775v1","category":"quant-ph"}
{"created":"2024-04-10 06:30:47","title":"Scalar Field Dark Matter Around Charged Black Holes","abstract":"In this paper, we investigate the behavior of a massive scalar field dark matter scenarios in the large mass limit around a central Reissner-Nordstr\\\"{o}m black hole. This study is motivated by observations from the Event Horizon Telescope collaboration, which does not exclude the possibility of the existence of such black holes. Through these inquiries, we uncover that the electric charge may significantly impact the scalar field profile and the density profile in the vecinty of the black hole. For the maximum electric charge allowed by the constraints of the Event Horizon Telescope, the maximum accretion rate decreases by $\\thicksim$ 50 \\% compared to the Schwarszchild case for marginally bound orbits. The maximum accretion rate of the massive scalar field is approximately $\\dot M_{\\text{SFDM}} \\thicksim 10^{-8} M_{\\odot} \\;\\text{yr}^{-1}$, which is significantly lower than the typical baryonic accretion rate commonly found in the literature. This implies that the scalar cloud located at the center of galaxies may have survived untill present times.","sentences":["In this paper, we investigate the behavior of a massive scalar field dark matter scenarios in the large mass limit around a central Reissner-Nordstr\\\"{o}m black hole.","This study is motivated by observations from the Event Horizon Telescope collaboration, which does not exclude the possibility of the existence of such black holes.","Through these inquiries, we uncover that the electric charge may significantly impact the scalar field profile and the density profile in the vecinty of the black hole.","For the maximum electric charge allowed by the constraints of the Event Horizon Telescope, the maximum accretion rate decreases by $\\thicksim$ 50 \\% compared to the Schwarszchild case for marginally bound orbits.","The maximum accretion rate of the massive scalar field is approximately $\\dot M_{\\text{SFDM}} \\thicksim 10^{-8} M_{\\odot} \\;\\text{yr}^{-1}$, which is significantly lower than the typical baryonic accretion rate commonly found in the literature.","This implies that the scalar cloud located at the center of galaxies may have survived untill present times."],"url":"http://arxiv.org/abs/2404.06774v1","category":"astro-ph.CO"}
{"created":"2024-04-10 06:25:26","title":"Stellar Populations of AGN-host Dwarf Galaxies Selected with Different Methods","abstract":"In this paper we investigate the stellar populations and star formation histories of 235 active galactic nuclei (AGN)-host dwarf galaxies, consisting of four samples identified separately with different methods (i.e., radio, X-ray, mid-IR and variability), utilizing the synthesis code STARLIGHT and spectra from the Sloan Digital Sky Survey (SDSS) Data Release 8. Our results show that the variability sample is the oldest, while the mid-IR sample is the youngest, for which the luminosity at 4020 \\AA\\ is dominated ($>50\\%$) by the young population ($t<10^8$ yr). The light-weighted mean stellar age of the whole sample is in general about 0.7 dex younger than the optical sample studie in Cai et al. We compare the population results between fitting models with and without a power-law (PL) component and find that the neglect of a PL component would lead to an under- and over-estimation by 0.2 and 0.1 dex for the light- and mass-weighted mean stellar age, respectively, for our sample of dwarf galaxies, which has a mean fractional contribution of $\\sim$16\\% from the AGN. In addition, we obtain further evidence for a possible suppression of star formation in the host galaxy by the central AGN. We also find that there exists an anti-correlation between the extinction-corrected [O\\,{\\sc iii}] luminosity and light-weighted mean stellar age, confirming our previous finding that there is a physical connection between AGN and star-forming activies in AGN-host dwarfs.","sentences":["In this paper we investigate the stellar populations and star formation histories of 235 active galactic nuclei (AGN)-host dwarf galaxies, consisting of four samples identified separately with different methods (i.e., radio, X-ray, mid-IR and variability), utilizing the synthesis code STARLIGHT and spectra from the Sloan Digital Sky Survey (SDSS) Data Release 8.","Our results show that the variability sample is the oldest, while the mid-IR sample is the youngest, for which the luminosity at 4020 \\AA\\ is dominated ($>50\\%$) by the young population ($t<10^8$ yr).","The light-weighted mean stellar age of the whole sample is in general about 0.7 dex younger than the optical sample studie in Cai et al.","We compare the population results between fitting models with and without a power-law (PL) component and find that the neglect of a PL component would lead to an under- and over-estimation by 0.2 and 0.1 dex for the light- and mass-weighted mean stellar age, respectively, for our sample of dwarf galaxies, which has a mean fractional contribution of $\\sim$16\\% from the AGN.","In addition, we obtain further evidence for a possible suppression of star formation in the host galaxy by the central AGN.","We also find that there exists an anti-correlation between the extinction-corrected [O\\,{\\sc iii}] luminosity and light-weighted mean stellar age, confirming our previous finding that there is a physical connection between AGN and star-forming activies in AGN-host dwarfs."],"url":"http://arxiv.org/abs/2404.06771v1","category":"astro-ph.GA"}
{"created":"2024-04-10 06:24:07","title":"Vibrational ADAPT-VQE: Critical points leads to problematic convergence","abstract":"Quantum chemistry is one of the most promising applications for which quantum computing is expected to have significant impact. Despite considerable research in the field of electronic structure, calculating the vibrational properties of molecules on quantum computers remain a relatively unexplored field. In this work, we develop a vibrational ADAPT-VQE (vADAPT-VQE) formalism based on an infinite product representation (IPR) of anti-Hermitian excitation operators of the Full Vibrational Configuration Interaction (FVCI) wavefunction which allows for preparing eigenstates of vibrational Hamiltonians on quantum computers. In order to establish the vADAPT- VQE algorithm using the IPR, we study the exactness of disentangled Unitary Vibrational Coupled Cluster (dUVCC) theory and show that dUVCC can formally represent the FVCI wavefunction in an infinite expansion. To investigate the performance of the vADAPT-VQE algorithm, we numerically study whether the vADAPT-VQE algorithm generates a sequence of operators which may represent the FVCI wavefunction. Our numerical results indicate frequent appearance of critical points in the wavefunction preparation using vADAPT-VQE. These results imply that one may encounter diminishing usefulness when preparing vibrational wavefunctions on quantum computers using vADAPT-VQE and that additional studies are required to find methods that can circumvent this behavior.","sentences":["Quantum chemistry is one of the most promising applications for which quantum computing is expected to have significant impact.","Despite considerable research in the field of electronic structure, calculating the vibrational properties of molecules on quantum computers remain a relatively unexplored field.","In this work, we develop a vibrational ADAPT-VQE (vADAPT-VQE) formalism based on an infinite product representation (IPR) of anti-Hermitian excitation operators of the Full Vibrational Configuration Interaction (FVCI) wavefunction which allows for preparing eigenstates of vibrational Hamiltonians on quantum computers.","In order to establish the vADAPT- VQE algorithm using the IPR, we study the exactness of disentangled Unitary Vibrational Coupled Cluster (dUVCC) theory and show that dUVCC can formally represent the FVCI wavefunction in an infinite expansion.","To investigate the performance of the vADAPT-VQE algorithm, we numerically study whether the vADAPT-VQE algorithm generates a sequence of operators which may represent the FVCI wavefunction.","Our numerical results indicate frequent appearance of critical points in the wavefunction preparation using vADAPT-VQE.","These results imply that one may encounter diminishing usefulness when preparing vibrational wavefunctions on quantum computers using vADAPT-VQE and that additional studies are required to find methods that can circumvent this behavior."],"url":"http://arxiv.org/abs/2404.06770v1","category":"quant-ph"}
{"created":"2024-04-10 06:19:19","title":"Solving the Food-Energy-Water Nexus Problem via Intelligent Optimization Algorithms","abstract":"The application of evolutionary algorithms (EAs) to multi-objective optimization problems has been widespread. However, the EA research community has not paid much attention to large-scale multi-objective optimization problems arising from real-world applications. Especially, Food-Energy-Water systems are intricately linked among food, energy and water that impact each other. They usually involve a huge number of decision variables and many conflicting objectives to be optimized. Solving their related optimization problems is essentially important to sustain the high-quality life of human beings. Their solution space size expands exponentially with the number of decision variables. Searching in such a vast space is challenging because of such large numbers of decision variables and objective functions. In recent years, a number of large-scale many-objectives optimization evolutionary algorithms have been proposed. In this paper, we solve a Food-Energy-Water optimization problem by using the state-of-art intelligent optimization methods and compare their performance. Our results conclude that the algorithm based on an inverse model outperforms the others. This work should be highly useful for practitioners to select the most suitable method for their particular large-scale engineering optimization problems.","sentences":["The application of evolutionary algorithms (EAs) to multi-objective optimization problems has been widespread.","However, the EA research community has not paid much attention to large-scale multi-objective optimization problems arising from real-world applications.","Especially, Food-Energy-Water systems are intricately linked among food, energy and water that impact each other.","They usually involve a huge number of decision variables and many conflicting objectives to be optimized.","Solving their related optimization problems is essentially important to sustain the high-quality life of human beings.","Their solution space size expands exponentially with the number of decision variables.","Searching in such a vast space is challenging because of such large numbers of decision variables and objective functions.","In recent years, a number of large-scale many-objectives optimization evolutionary algorithms have been proposed.","In this paper, we solve a Food-Energy-Water optimization problem by using the state-of-art intelligent optimization methods and compare their performance.","Our results conclude that the algorithm based on an inverse model outperforms the others.","This work should be highly useful for practitioners to select the most suitable method for their particular large-scale engineering optimization problems."],"url":"http://arxiv.org/abs/2404.06769v1","category":"cs.NE"}
{"created":"2024-04-10 06:13:45","title":"Harnessing the Power of AI-Generated Content for Semantic Communication","abstract":"Semantic Communication (SemCom) is envisaged as the next-generation paradigm to address challenges stemming from the conflicts between the increasing volume of transmission data and the scarcity of spectrum resources. However, existing SemCom systems face drawbacks, such as low explainability, modality rigidity, and inadequate reconstruction functionality. Recognizing the transformative capabilities of AI-generated content (AIGC) technologies in content generation, this paper explores a pioneering approach by integrating them into SemCom to address the aforementioned challenges. We employ a three-layer model to illustrate the proposed AIGC-assisted SemCom (AIGC-SCM) architecture, emphasizing its clear deviation from existing SemCom. Grounded in this model, we investigate various AIGC technologies with the potential to augment SemCom's performance. In alignment with SemCom's goal of conveying semantic meanings, we also introduce the new evaluation methods for our AIGC-SCM system. Subsequently, we explore communication scenarios where our proposed AIGC-SCM can realize its potential. For practical implementation, we construct a detailed integration workflow and conduct a case study in a virtual reality image transmission scenario. The results demonstrate our ability to maintain a high degree of alignment between the reconstructed content and the original source information, while substantially minimizing the data volume required for transmission. These findings pave the way for further enhancements in communication efficiency and the improvement of Quality of Service. At last, we present future directions for AIGC-SCM studies.","sentences":["Semantic Communication (SemCom) is envisaged as the next-generation paradigm to address challenges stemming from the conflicts between the increasing volume of transmission data and the scarcity of spectrum resources.","However, existing SemCom systems face drawbacks, such as low explainability, modality rigidity, and inadequate reconstruction functionality.","Recognizing the transformative capabilities of AI-generated content (AIGC) technologies in content generation, this paper explores a pioneering approach by integrating them into SemCom to address the aforementioned challenges.","We employ a three-layer model to illustrate the proposed AIGC-assisted SemCom (AIGC-SCM) architecture, emphasizing its clear deviation from existing SemCom.","Grounded in this model, we investigate various AIGC technologies with the potential to augment SemCom's performance.","In alignment with SemCom's goal of conveying semantic meanings, we also introduce the new evaluation methods for our AIGC-SCM system.","Subsequently, we explore communication scenarios where our proposed AIGC-SCM can realize its potential.","For practical implementation, we construct a detailed integration workflow and conduct a case study in a virtual reality image transmission scenario.","The results demonstrate our ability to maintain a high degree of alignment between the reconstructed content and the original source information, while substantially minimizing the data volume required for transmission.","These findings pave the way for further enhancements in communication efficiency and the improvement of Quality of Service.","At last, we present future directions for AIGC-SCM studies."],"url":"http://arxiv.org/abs/2404.06765v1","category":"eess.SP"}
{"created":"2024-04-10 06:03:13","title":"Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems","abstract":"Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced learning experience. The emergence of large language models (LLMs) further enables better human-machine interaction, and facilitates the development of conversational ITSs in various disciplines such as math and language learning. In dialogic teaching, recognizing and adapting to individual characteristics can significantly enhance student engagement and learning efficiency. However, characterizing and simulating student's persona remain challenging in training and evaluating conversational ITSs. In this work, we propose a framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects, and leverage LLMs for personality-aware student simulation in a language learning scenario. We further enhance the framework with multi-aspect validation, and conduct extensive analysis from both teacher and student perspectives. Our experimental results show that state-of-the-art LLMs can produce diverse student responses according to the given language ability and personality traits, and trigger teacher's adaptive scaffolding strategies.","sentences":["Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced learning experience.","The emergence of large language models (LLMs) further enables better human-machine interaction, and facilitates the development of conversational ITSs in various disciplines such as math and language learning.","In dialogic teaching, recognizing and adapting to individual characteristics can significantly enhance student engagement and learning efficiency.","However, characterizing and simulating student's persona remain challenging in training and evaluating conversational ITSs.","In this work, we propose a framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects, and leverage LLMs for personality-aware student simulation in a language learning scenario.","We further enhance the framework with multi-aspect validation, and conduct extensive analysis from both teacher and student perspectives.","Our experimental results show that state-of-the-art LLMs can produce diverse student responses according to the given language ability and personality traits, and trigger teacher's adaptive scaffolding strategies."],"url":"http://arxiv.org/abs/2404.06762v1","category":"cs.CL"}
{"created":"2024-04-10 05:56:46","title":"DiffusionDialog: A Diffusion Model for Diverse Dialog Generation with Latent Space","abstract":"In real-life conversations, the content is diverse, and there exists the one-to-many problem that requires diverse generation. Previous studies attempted to introduce discrete or Gaussian-based continuous latent variables to address the one-to-many problem, but the diversity is limited. Recently, diffusion models have made breakthroughs in computer vision, and some attempts have been made in natural language processing. In this paper, we propose DiffusionDialog, a novel approach to enhance the diversity of dialogue generation with the help of diffusion model. In our approach, we introduce continuous latent variables into the diffusion model. The problem of using latent variables in the dialog task is how to build both an effective prior of the latent space and an inferring process to obtain the proper latent given the context. By combining the encoder and latent-based diffusion model, we encode the response's latent representation in a continuous space as the prior, instead of fixed Gaussian distribution or simply discrete ones. We then infer the latent by denoising step by step with the diffusion model. The experimental results show that our model greatly enhances the diversity of dialog responses while maintaining coherence. Furthermore, in further analysis, we find that our diffusion model achieves high inference efficiency, which is the main challenge of applying diffusion models in natural language processing.","sentences":["In real-life conversations, the content is diverse, and there exists the one-to-many problem that requires diverse generation.","Previous studies attempted to introduce discrete or Gaussian-based continuous latent variables to address the one-to-many problem, but the diversity is limited.","Recently, diffusion models have made breakthroughs in computer vision, and some attempts have been made in natural language processing.","In this paper, we propose DiffusionDialog, a novel approach to enhance the diversity of dialogue generation with the help of diffusion model.","In our approach, we introduce continuous latent variables into the diffusion model.","The problem of using latent variables in the dialog task is how to build both an effective prior of the latent space and an inferring process to obtain the proper latent given the context.","By combining the encoder and latent-based diffusion model, we encode the response's latent representation in a continuous space as the prior, instead of fixed Gaussian distribution or simply discrete ones.","We then infer the latent by denoising step by step with the diffusion model.","The experimental results show that our model greatly enhances the diversity of dialog responses while maintaining coherence.","Furthermore, in further analysis, we find that our diffusion model achieves high inference efficiency, which is the main challenge of applying diffusion models in natural language processing."],"url":"http://arxiv.org/abs/2404.06760v1","category":"cs.CL"}
{"created":"2024-04-10 05:53:25","title":"Language Generation in the Limit","abstract":"Although current large language models are complex, the most basic specifications of the underlying language generation problem itself are simple to state: given a finite set of training samples from an unknown language, produce valid new strings from the language that don't already appear in the training data. Here we ask what we can conclude about language generation using only this specification, without further assumptions. In particular, suppose that an adversary enumerates the strings of an unknown target language L that is known only to come from one of a possibly infinite list of candidates. A computational agent is trying to learn to generate from this language; we say that the agent generates from L in the limit if after some finite point in the enumeration of L, the agent is able to produce new elements that come exclusively from L and that have not yet been presented by the adversary. Our main result is that there is an agent that is able to generate in the limit for every countable list of candidate languages. This contrasts dramatically with negative results due to Gold and Angluin in a well-studied model of language learning where the goal is to identify an unknown language from samples; the difference between these results suggests that identifying a language is a fundamentally different problem than generating from it.","sentences":["Although current large language models are complex, the most basic specifications of the underlying language generation problem itself are simple to state: given a finite set of training samples from an unknown language, produce valid new strings from the language that don't already appear in the training data.","Here we ask what we can conclude about language generation using only this specification, without further assumptions.","In particular, suppose that an adversary enumerates the strings of an unknown target language L that is known only to come from one of a possibly infinite list of candidates.","A computational agent is trying to learn to generate from this language; we say that the agent generates from L in the limit if after some finite point in the enumeration of L, the agent is able to produce new elements that come exclusively from L and that have not yet been presented by the adversary.","Our main result is that there is an agent that is able to generate in the limit for every countable list of candidate languages.","This contrasts dramatically with negative results due to Gold and Angluin in a well-studied model of language learning where the goal is to identify an unknown language from samples; the difference between these results suggests that identifying a language is a fundamentally different problem than generating from it."],"url":"http://arxiv.org/abs/2404.06757v1","category":"cs.DS"}
{"created":"2024-04-10 05:44:28","title":"CrimeAlarm: Towards Intensive Intent Dynamics in Fine-grained Crime Prediction","abstract":"Granularity and accuracy are two crucial factors for crime event prediction. Within fine-grained event classification, multiple criminal intents may alternately exhibit in preceding sequential events, and progress differently in next. Such intensive intent dynamics makes training models hard to capture unobserved intents, and thus leads to sub-optimal generalization performance, especially in the intertwining of numerous potential events. To capture comprehensive criminal intents, this paper proposes a fine-grained sequential crime prediction framework, CrimeAlarm, that equips with a novel mutual distillation strategy inspired by curriculum learning. During the early training phase, spot-shared criminal intents are captured through high-confidence sequence samples. In the later phase, spot-specific intents are gradually learned by increasing the contribution of low-confidence sequences. Meanwhile, the output probability distributions are reciprocally learned between prediction networks to model unobserved criminal intents. Extensive experiments show that CrimeAlarm outperforms state-of-the-art methods in terms of NDCG@5, with improvements of 4.51% for the NYC16 and 7.73% for the CHI18 in accuracy measures.","sentences":["Granularity and accuracy are two crucial factors for crime event prediction.","Within fine-grained event classification, multiple criminal intents may alternately exhibit in preceding sequential events, and progress differently in next.","Such intensive intent dynamics makes training models hard to capture unobserved intents, and thus leads to sub-optimal generalization performance, especially in the intertwining of numerous potential events.","To capture comprehensive criminal intents, this paper proposes a fine-grained sequential crime prediction framework, CrimeAlarm, that equips with a novel mutual distillation strategy inspired by curriculum learning.","During the early training phase, spot-shared criminal intents are captured through high-confidence sequence samples.","In the later phase, spot-specific intents are gradually learned by increasing the contribution of low-confidence sequences.","Meanwhile, the output probability distributions are reciprocally learned between prediction networks to model unobserved criminal intents.","Extensive experiments show that CrimeAlarm outperforms state-of-the-art methods in terms of NDCG@5, with improvements of 4.51% for the NYC16 and 7.73% for the CHI18 in accuracy measures."],"url":"http://arxiv.org/abs/2404.06756v1","category":"cs.LG"}
{"created":"2024-04-10 05:34:07","title":"Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Generative Agents","abstract":"Some have criticised Generative AI Systems for replicating the familiar pathologies of already widely-deployed AI systems. Other critics highlight how they foreshadow vastly more powerful future systems, which might threaten humanity's survival. The first group says there is nothing new here; the other looks through the present to a perhaps distant horizon. In this paper, I instead pay attention to what makes these particular systems distinctive: both their remarkable scientific achievement, and the most likely and consequential ways in which they will change society over the next five to ten years. In particular, I explore the potential societal impacts and normative questions raised by the looming prospect of 'Generative Agents', in which multimodal large language models (LLMs) form the executive centre of complex, tool-using AI systems that can take unsupervised sequences of actions towards some goal.","sentences":["Some have criticised Generative AI Systems for replicating the familiar pathologies of already widely-deployed AI systems.","Other critics highlight how they foreshadow vastly more powerful future systems, which might threaten humanity's survival.","The first group says there is nothing new here; the other looks through the present to a perhaps distant horizon.","In this paper, I instead pay attention to what makes these particular systems distinctive: both their remarkable scientific achievement, and the most likely and consequential ways in which they will change society over the next five to ten years.","In particular, I explore the potential societal impacts and normative questions raised by the looming prospect of 'Generative Agents', in which multimodal large language models (LLMs) form the executive centre of complex, tool-using AI systems that can take unsupervised sequences of actions towards some goal."],"url":"http://arxiv.org/abs/2404.06750v1","category":"cs.CY"}
{"created":"2024-04-10 05:27:03","title":"Combination of Site-Wide and Real-Time Optimization for the Control of Systems of Flexible Energy Resources","abstract":"The rapid expansion of renewable energy sources introduces significant volatility and unpredictability to the energy supply chain, challenging the stability and reliability of the power grid. This work presents a method that enhances existing static optimization models by converting them into dynamic models suitable for the real-time optimization of flexible energy resources. By adapting static models for real-time application, the proposed two-stage optimization strategy allows for flexible adjustments of operational plans, facilitating the seamless integration of renewable energy sources. This approach not only ensures grid reliability but also improves economic efficiency by optimizing resource utilization. The effectiveness of this method is demonstrated through a case study involving a system of electrolyzers, showcasing significant advantages over traditional static optimization methods in aligning energy consumption with renewable energy generation.","sentences":["The rapid expansion of renewable energy sources introduces significant volatility and unpredictability to the energy supply chain, challenging the stability and reliability of the power grid.","This work presents a method that enhances existing static optimization models by converting them into dynamic models suitable for the real-time optimization of flexible energy resources.","By adapting static models for real-time application, the proposed two-stage optimization strategy allows for flexible adjustments of operational plans, facilitating the seamless integration of renewable energy sources.","This approach not only ensures grid reliability but also improves economic efficiency by optimizing resource utilization.","The effectiveness of this method is demonstrated through a case study involving a system of electrolyzers, showcasing significant advantages over traditional static optimization methods in aligning energy consumption with renewable energy generation."],"url":"http://arxiv.org/abs/2404.06748v1","category":"math.OC"}
{"created":"2024-04-10 05:12:31","title":"Direct transition from a fractional quantum anomalous Hall state to a smectic state with the same Hall conductance","abstract":"The recent developments in twisted MoTe_2 and rhombohedral multilayer graphene have generated widespread attention to the general features of fractional quantum anomalous Hall (FQAH) states, including their possible coexistence with and transition to various symmetry breaking charge ordered states. These attentions are pushing forward our knowledge of the relation between the topological order in FQAH states and the Landau-type of symmetry breaking order such as the 1D smectic electronic liquid crystal and 2D charge-density-wave (CDW) solid. Although the transitions from topological states to symmetry breaking states with trivial topology have been discussed, the road from one topological ordered state to another with the same Hall conductance coexisting with Landau order has not been found. Here we show the intriguing evidence that the FQAH to FQAH Smectic (FQAHS) transition is robustly realizable in the archetypal correlated flat Chern-band model at filling {\\nu} = 2/3. This transition is novel in that: i) the FQAHS acquires the same fractional Hall conductance as FQAH, which cannot be explained by mean-field band folding. The formation of smectic order can be viewed as perturbation around the transition point, and thus, do not destroy or change the original topology; ii) the charge excitation remains gapped across the transition although the neutral gap is closed at transition point; and iii) the transition is triggered by the softening of roton mode with the same wave vector as the smectic order. Our discovery opens countless new possibilities, both theoretical and experimental, in the fast-growing field of robust fractional Chern insulators.","sentences":["The recent developments in twisted MoTe_2 and rhombohedral multilayer graphene have generated widespread attention to the general features of fractional quantum anomalous Hall (FQAH) states, including their possible coexistence with and transition to various symmetry breaking charge ordered states.","These attentions are pushing forward our knowledge of the relation between the topological order in FQAH states and the Landau-type of symmetry breaking order such as the 1D smectic electronic liquid crystal and 2D charge-density-wave (CDW) solid.","Although the transitions from topological states to symmetry breaking states with trivial topology have been discussed, the road from one topological ordered state to another with the same Hall conductance coexisting with Landau order has not been found.","Here we show the intriguing evidence that the FQAH to FQAH Smectic (FQAHS) transition is robustly realizable in the archetypal correlated flat Chern-band model at filling {\\nu} = 2/3.","This transition is novel in that: i) the FQAHS acquires the same fractional Hall conductance as FQAH, which cannot be explained by mean-field band folding.","The formation of smectic order can be viewed as perturbation around the transition point, and thus, do not destroy or change the original topology; ii) the charge excitation remains gapped across the transition although the neutral gap is closed at transition point; and iii) the transition is triggered by the softening of roton mode with the same wave vector as the smectic order.","Our discovery opens countless new possibilities, both theoretical and experimental, in the fast-growing field of robust fractional Chern insulators."],"url":"http://arxiv.org/abs/2404.06745v1","category":"cond-mat.str-el"}
{"created":"2024-04-10 05:00:35","title":"Transferable and Efficient Non-Factual Content Detection via Probe Training with Offline Consistency Checking","abstract":"Detecting non-factual content is a longstanding goal to increase the trustworthiness of large language models (LLMs) generations. Current factuality probes, trained using humanannotated labels, exhibit limited transferability to out-of-distribution content, while online selfconsistency checking imposes extensive computation burden due to the necessity of generating multiple outputs. This paper proposes PINOSE, which trains a probing model on offline self-consistency checking results, thereby circumventing the need for human-annotated data and achieving transferability across diverse data distributions. As the consistency check process is offline, PINOSE reduces the computational burden of generating multiple responses by online consistency verification. Additionally, it examines various aspects of internal states prior to response decoding, contributing to more effective detection of factual inaccuracies. Experiment results on both factuality detection and question answering benchmarks show that PINOSE achieves surpassing results than existing factuality detection methods. Our code and datasets are publicly available on this anonymized repository.","sentences":["Detecting non-factual content is a longstanding goal to increase the trustworthiness of large language models (LLMs) generations.","Current factuality probes, trained using humanannotated labels, exhibit limited transferability to out-of-distribution content, while online selfconsistency checking imposes extensive computation burden due to the necessity of generating multiple outputs.","This paper proposes PINOSE, which trains a probing model on offline self-consistency checking results, thereby circumventing the need for human-annotated data and achieving transferability across diverse data distributions.","As the consistency check process is offline, PINOSE reduces the computational burden of generating multiple responses by online consistency verification.","Additionally, it examines various aspects of internal states prior to response decoding, contributing to more effective detection of factual inaccuracies.","Experiment results on both factuality detection and question answering benchmarks show that PINOSE achieves surpassing results than existing factuality detection methods.","Our code and datasets are publicly available on this anonymized repository."],"url":"http://arxiv.org/abs/2404.06742v1","category":"cs.CL"}
{"created":"2024-04-10 04:56:17","title":"Partition-based distributed extended Kalman filter for large-scale nonlinear processes with application to chemical and wastewater treatment processes","abstract":"In this paper, we address a partition-based distributed state estimation problem for large-scale general nonlinear processes by proposing a Kalman-based approach. First, we formulate a linear full-information estimation design within a distributed framework as the basis for developing our approach. Second, the analytical solution to the local optimization problems associated with the formulated distributed full-information design is established, in the form of a recursive distributed Kalman filter algorithm. Then, the linear distributed Kalman filter is extended to the nonlinear context by incorporating successive linearization of nonlinear subsystem models, and the proposed distributed extended Kalman filter approach is formulated. We conduct rigorous analysis and prove the stability of the estimation error dynamics provided by the proposed method for general nonlinear processes consisting of interconnected subsystems. A chemical process example is used to illustrate the effectiveness of the proposed method and to justify the validity of the theoretical findings. In addition, the proposed method is applied to a wastewater treatment process for estimating the full state of the process with 145 state variables.","sentences":["In this paper, we address a partition-based distributed state estimation problem for large-scale general nonlinear processes by proposing a Kalman-based approach.","First, we formulate a linear full-information estimation design within a distributed framework as the basis for developing our approach.","Second, the analytical solution to the local optimization problems associated with the formulated distributed full-information design is established, in the form of a recursive distributed Kalman filter algorithm.","Then, the linear distributed Kalman filter is extended to the nonlinear context by incorporating successive linearization of nonlinear subsystem models, and the proposed distributed extended Kalman filter approach is formulated.","We conduct rigorous analysis and prove the stability of the estimation error dynamics provided by the proposed method for general nonlinear processes consisting of interconnected subsystems.","A chemical process example is used to illustrate the effectiveness of the proposed method and to justify the validity of the theoretical findings.","In addition, the proposed method is applied to a wastewater treatment process for estimating the full state of the process with 145 state variables."],"url":"http://arxiv.org/abs/2404.06738v1","category":"eess.SY"}
{"created":"2024-04-10 04:55:57","title":"Disguised Copyright Infringement of Latent Diffusion Model","abstract":"Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools. In this paper, we provide a better understanding of such disguised copyright infringement by uncovering the disguises generation algorithm, the revelation of the disguises, and importantly, how to detect them to augment the existing toolbox. Additionally, we introduce a broader notion of acknowledgment for comprehending such indirect access.","sentences":["Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase.","The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement.","We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it.","Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools.","In this paper, we provide a better understanding of such disguised copyright infringement by uncovering the disguises generation algorithm, the revelation of the disguises, and importantly, how to detect them to augment the existing toolbox.","Additionally, we introduce a broader notion of acknowledgment for comprehending such indirect access."],"url":"http://arxiv.org/abs/2404.06737v1","category":"cs.LG"}
{"created":"2024-04-10 04:40:48","title":"CryinGAN: Design and evaluation of point-cloud-based generative adversarial networks using disordered materials $-$ application to Li$_3$ScCl$_6$-LiCoO$_2$ battery interfaces","abstract":"Generative models have received significant attention in recent years for materials science applications, particularly in the area of inverse design for materials discovery. While current efforts have mainly focused on bulk materials with relatively small unit cells, the possibility of generative models for more complex, disordered materials would significantly extent the current capabilities of generative modeling. Generative models would also benefit from better ways to assess their performance. They are typically evaluated on the new, unverified materials being generated, which give limited metrics for evaluation. In this work, we design and evaluate models intended for disordered interface structure generation. Using a disordered Li$_3$ScCl$_6$-LiCoO$_2$ battery interface, we tested different point-cloud-based generative adversarial network architectures that further include bond distance information in the discriminator, rather than only atomic coordinates. By working with a fixed material system, we evaluated the model performance through direct comparisons between training and generated structures. The best performing architecture, Crystal Interface Generative Adversarial Network (CryinGAN), uses two separate 1D convolutional discriminators, one that accepts coordinates and another that accepts bond distances as input. We demonstrate that CryinGAN is able to successfully generate low-interface-energy structures for systems with > 250 atoms, in which the generated interfaces are structurally similar to the training structures. This study highlights the capabilities of a relatively simple generative model in generating large disordered materials, and discusses the limitations of the point cloud representation. Insights are provided to help guide the development of future generative models that are useful to not just disordered, but also ordered materials.","sentences":["Generative models have received significant attention in recent years for materials science applications, particularly in the area of inverse design for materials discovery.","While current efforts have mainly focused on bulk materials with relatively small unit cells, the possibility of generative models for more complex, disordered materials would significantly extent the current capabilities of generative modeling.","Generative models would also benefit from better ways to assess their performance.","They are typically evaluated on the new, unverified materials being generated, which give limited metrics for evaluation.","In this work, we design and evaluate models intended for disordered interface structure generation.","Using a disordered Li$_3$ScCl$_6$-LiCoO$_2$ battery interface, we tested different point-cloud-based generative adversarial network architectures that further include bond distance information in the discriminator, rather than only atomic coordinates.","By working with a fixed material system, we evaluated the model performance through direct comparisons between training and generated structures.","The best performing architecture, Crystal Interface Generative Adversarial Network (CryinGAN), uses two separate 1D convolutional discriminators, one that accepts coordinates and another that accepts bond distances as input.","We demonstrate that CryinGAN is able to successfully generate low-interface-energy structures for systems with > 250 atoms, in which the generated interfaces are structurally similar to the training structures.","This study highlights the capabilities of a relatively simple generative model in generating large disordered materials, and discusses the limitations of the point cloud representation.","Insights are provided to help guide the development of future generative models that are useful to not just disordered, but also ordered materials."],"url":"http://arxiv.org/abs/2404.06734v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-10 04:38:17","title":"Incremental XAI: Memorable Understanding of AI with Incremental Explanations","abstract":"Many explainable AI (XAI) techniques strive for interpretability by providing concise salient information, such as sparse linear factors. However, users either only see inaccurate global explanations, or highly-varying local explanations. We propose to provide more detailed explanations by leveraging the human cognitive capacity to accumulate knowledge by incrementally receiving more details. Focusing on linear factor explanations (factors $\\times$ values = outcome), we introduce Incremental XAI to automatically partition explanations for general and atypical instances by providing Base + Incremental factors to help users read and remember more faithful explanations. Memorability is improved by reusing base factors and reducing the number of factors shown in atypical cases. In modeling, formative, and summative user studies, we evaluated the faithfulness, memorability and understandability of Incremental XAI against baseline explanation methods. This work contributes towards more usable explanation that users can better ingrain to facilitate intuitive engagement with AI.","sentences":["Many explainable AI (XAI) techniques strive for interpretability by providing concise salient information, such as sparse linear factors.","However, users either only see inaccurate global explanations, or highly-varying local explanations.","We propose to provide more detailed explanations by leveraging the human cognitive capacity to accumulate knowledge by incrementally receiving more details.","Focusing on linear factor explanations (factors $\\times$ values = outcome), we introduce Incremental XAI to automatically partition explanations for general and atypical instances by providing Base + Incremental factors to help users read and remember more faithful explanations.","Memorability is improved by reusing base factors and reducing the number of factors shown in atypical cases.","In modeling, formative, and summative user studies, we evaluated the faithfulness, memorability and understandability of Incremental XAI against baseline explanation methods.","This work contributes towards more usable explanation that users can better ingrain to facilitate intuitive engagement with AI."],"url":"http://arxiv.org/abs/2404.06733v1","category":"cs.HC"}
{"created":"2024-04-10 04:36:24","title":"Enhancing Safety in Mixed Traffic: Learning-Based Modeling and Efficient Control of Autonomous and Human-Driven Vehicles","abstract":"With the increasing presence of autonomous vehicles (AVs) on public roads, developing robust control strategies to navigate the uncertainty of human-driven vehicles (HVs) is crucial. This paper introduces an advanced method for modeling HV behavior, combining a first-principles model with Gaussian process (GP) learning to enhance velocity prediction accuracy and provide a measurable uncertainty. We validated this innovative HV model using real-world data from field experiments and applied it to develop a GP-enhanced model predictive control (GP-MPC) strategy. This strategy aims to improve safety in mixed vehicle platoons by integrating uncertainty assessment into distance constraints. Comparative simulation studies with a conventional model predictive control (MPC) approach demonstrated that our GP-MPC strategy ensures more reliable safe distancing and fosters efficient vehicular dynamics, achieving notably higher speeds within the platoon. By incorporating a sparse GP technique in HV modeling and adopting a dynamic GP prediction within the MPC framework, we significantly reduced the computation time of GP-MPC, marking it only 4.6% higher than that of the conventional MPC. This represents a substantial improvement, making the process about 100 times faster than our preliminary work without these approximations. Our findings underscore the effectiveness of learning-based HV modeling in enhancing both safety and operational efficiency in mixed-traffic environments, paving the way for more harmonious AV-HV interactions.","sentences":["With the increasing presence of autonomous vehicles (AVs) on public roads, developing robust control strategies to navigate the uncertainty of human-driven vehicles (HVs) is crucial.","This paper introduces an advanced method for modeling HV behavior, combining a first-principles model with Gaussian process (GP) learning to enhance velocity prediction accuracy and provide a measurable uncertainty.","We validated this innovative HV model using real-world data from field experiments and applied it to develop a GP-enhanced model predictive control (GP-MPC) strategy.","This strategy aims to improve safety in mixed vehicle platoons by integrating uncertainty assessment into distance constraints.","Comparative simulation studies with a conventional model predictive control (MPC) approach demonstrated that our GP-MPC strategy ensures more reliable safe distancing and fosters efficient vehicular dynamics, achieving notably higher speeds within the platoon.","By incorporating a sparse GP technique in HV modeling and adopting a dynamic GP prediction within the MPC framework, we significantly reduced the computation time of GP-MPC, marking it only 4.6% higher than that of the conventional MPC.","This represents a substantial improvement, making the process about 100 times faster than our preliminary work without these approximations.","Our findings underscore the effectiveness of learning-based HV modeling in enhancing both safety and operational efficiency in mixed-traffic environments, paving the way for more harmonious AV-HV interactions."],"url":"http://arxiv.org/abs/2404.06732v1","category":"cs.RO"}
{"created":"2024-04-10 16:50:07","title":"Exploring Physiological Responses in Virtual Reality-based Interventions for Autism Spectrum Disorder: A Data-Driven Investigation","abstract":"Virtual Reality (VR) has emerged as a promising tool for enhancing social skills and emotional well-being in individuals with Autism Spectrum Disorder (ASD). Through a technical exploration, this study employs a multiplayer serious gaming environment within VR, engaging 34 individuals diagnosed with ASD and employing high-precision biosensors for a comprehensive view of the participants' arousal and responses during the VR sessions. Participants were subjected to a series of 3 virtual scenarios designed in collaboration with stakeholders and clinical experts to promote socio-cognitive skills and emotional regulation in a controlled and structured virtual environment. We combined the framework with wearable non-invasive sensors for bio-signal acquisition, focusing on the collection of heart rate variability, and respiratory patterns to monitor participants behaviors. Further, behavioral assessments were conducted using observation and semi-structured interviews, with the data analyzed in conjunction with physiological measures to identify correlations and explore digital-intervention efficacy. Preliminary analysis revealed significant correlations between physiological responses and behavioral outcomes, indicating the potential of physiological feedback to enhance VR-based interventions for ASD. The study demonstrated the feasibility of using real-time data to adapt virtual scenarios, suggesting a promising avenue to support personalized therapy. The integration of quantitative physiological feedback into digital platforms represents a forward step in the personalized intervention for ASD. By leveraging real-time data to adjust therapeutic content, this approach promises to enhance the efficacy and engagement of digital-based therapies.","sentences":["Virtual Reality (VR) has emerged as a promising tool for enhancing social skills and emotional well-being in individuals with Autism Spectrum Disorder (ASD).","Through a technical exploration, this study employs a multiplayer serious gaming environment within VR, engaging 34 individuals diagnosed with ASD and employing high-precision biosensors for a comprehensive view of the participants' arousal and responses during the VR sessions.","Participants were subjected to a series of 3 virtual scenarios designed in collaboration with stakeholders and clinical experts to promote socio-cognitive skills and emotional regulation in a controlled and structured virtual environment.","We combined the framework with wearable non-invasive sensors for bio-signal acquisition, focusing on the collection of heart rate variability, and respiratory patterns to monitor participants behaviors.","Further, behavioral assessments were conducted using observation and semi-structured interviews, with the data analyzed in conjunction with physiological measures to identify correlations and explore digital-intervention efficacy.","Preliminary analysis revealed significant correlations between physiological responses and behavioral outcomes, indicating the potential of physiological feedback to enhance VR-based interventions for ASD.","The study demonstrated the feasibility of using real-time data to adapt virtual scenarios, suggesting a promising avenue to support personalized therapy.","The integration of quantitative physiological feedback into digital platforms represents a forward step in the personalized intervention for ASD.","By leveraging real-time data to adjust therapeutic content, this approach promises to enhance the efficacy and engagement of digital-based therapies."],"url":"http://arxiv.org/abs/2404.07159v1","category":"cs.HC"}
{"created":"2024-04-10 15:52:00","title":"\"My toxic trait is thinking I'll remember this\": gaps in the learner experience of video tutorials for feature-rich software","abstract":"Video tutorials are a popular medium for informal and formal learning. However, when learners attempt to view and follow along with these tutorials, they encounter what we call gaps, that is, issues that can prevent learning. We examine the gaps encountered by users of video tutorials for feature-rich software, such as spreadsheets. We develop a theory and taxonomy of such gaps, identifying how they act as barriers to learning, by collecting and analyzing 360 viewer comments from 90 Microsoft Excel video tutorials published by 43 creators across YouTube, TikTok, and Instagram. We conducted contextual interviews with 8 highly influential tutorial creators to investigate the gaps their viewers experience and how they address them. Further, we obtain insights into their creative process and frustrations when creating video tutorials. Finally, we present creators with two designs that aim to address gaps identified in the comment analysis for feedback and alternative design ideas.","sentences":["Video tutorials are a popular medium for informal and formal learning.","However, when learners attempt to view and follow along with these tutorials, they encounter what we call gaps, that is, issues that can prevent learning.","We examine the gaps encountered by users of video tutorials for feature-rich software, such as spreadsheets.","We develop a theory and taxonomy of such gaps, identifying how they act as barriers to learning, by collecting and analyzing 360 viewer comments from 90 Microsoft Excel video tutorials published by 43 creators across YouTube, TikTok, and Instagram.","We conducted contextual interviews with 8 highly influential tutorial creators to investigate the gaps their viewers experience and how they address them.","Further, we obtain insights into their creative process and frustrations when creating video tutorials.","Finally, we present creators with two designs that aim to address gaps identified in the comment analysis for feedback and alternative design ideas."],"url":"http://arxiv.org/abs/2404.07114v1","category":"cs.HC"}
{"created":"2024-04-10 14:45:04","title":"Quantum Mechanics of Open Systems in Non-Inertial Motion","abstract":"The study of quantum mechanics in non-inertial reference frames, particularly in the context of open systems, introduces several intriguing phenomena and challenges. This paper presents a comprehensive framework for analyzing the quantum mechanics of open systems undergoing noninertial motion. Our methodology leverages the concept of dissipatons, statistical quasi-particles that capture collective dissipative effects from the environment. We demonstrate that our approach offers a natural understanding of the intricate dynamics among non-inertial effects, decoherence, dissipation, and system-bath entanglement. Specifically, we conduct demonstrations focusing on the Lamb shift phenomenon within a rotating ring cavity. Through theoretical exposition and practical applications, our framework elucidates the profound interplay between open quantum dynamics and non-inertial motion, paving the way for advancements in quantum information processing and sensing technologies.","sentences":["The study of quantum mechanics in non-inertial reference frames, particularly in the context of open systems, introduces several intriguing phenomena and challenges.","This paper presents a comprehensive framework for analyzing the quantum mechanics of open systems undergoing noninertial motion.","Our methodology leverages the concept of dissipatons, statistical quasi-particles that capture collective dissipative effects from the environment.","We demonstrate that our approach offers a natural understanding of the intricate dynamics among non-inertial effects, decoherence, dissipation, and system-bath entanglement.","Specifically, we conduct demonstrations focusing on the Lamb shift phenomenon within a rotating ring cavity.","Through theoretical exposition and practical applications, our framework elucidates the profound interplay between open quantum dynamics and non-inertial motion, paving the way for advancements in quantum information processing and sensing technologies."],"url":"http://arxiv.org/abs/2404.07054v1","category":"quant-ph"}
{"created":"2024-04-10 14:21:22","title":"Propensity of water self-ions at air(oil)-water interfaces revealed by deep potential molecular dynamics with enhanced sampling","abstract":"The preference of water self-ions (hydronium and hydroxide) near air/oil-water interfaces is one of the hottest topics in water research due to its importance for understanding properties, phenomena, and reactions of interfaces. In this work, we performed enhanced-sampling molecular dynamics based on state-of-the-art neural network potentials with M06-2X accuracy to investigate the propensity of hydronium and hydroxide ions at air/oil-water interfaces, which can simultaneously describe well the water autoionization process forming these ions, recombination of ions, and ionic distribution along the normal distance to the interface by employing a set of appropriate Voronoi collective variables. The results support a stable ionic double-layer distribution near the interface for both air-water and oil-water interface systems. Hydronium tends to reside in the topmost layer of the interface, while hydroxide with a slightly stronger interfacial stabilization free energy is enriched in the deeper interfacial layer. This double-layer distribution may help to understand the longstanding controversy about the interfacial acid-base nature.","sentences":["The preference of water self-ions (hydronium and hydroxide) near air/oil-water interfaces is one of the hottest topics in water research due to its importance for understanding properties, phenomena, and reactions of interfaces.","In this work, we performed enhanced-sampling molecular dynamics based on state-of-the-art neural network potentials with M06-2X accuracy to investigate the propensity of hydronium and hydroxide ions at air/oil-water interfaces, which can simultaneously describe well the water autoionization process forming these ions, recombination of ions, and ionic distribution along the normal distance to the interface by employing a set of appropriate Voronoi collective variables.","The results support a stable ionic double-layer distribution near the interface for both air-water and oil-water interface systems.","Hydronium tends to reside in the topmost layer of the interface, while hydroxide with a slightly stronger interfacial stabilization free energy is enriched in the deeper interfacial layer.","This double-layer distribution may help to understand the longstanding controversy about the interfacial acid-base nature."],"url":"http://arxiv.org/abs/2404.07027v1","category":"physics.chem-ph"}
{"created":"2024-04-10 13:40:29","title":"Linked open data per la valorizzazione di collezioni culturali: il dataset mythLOD","abstract":"The formal representation of cultural metadata has always been a challenge, considering both the heterogeneity of cultural objects and the need to document the interpretive act exercised by experts. This article provides an overview of the revalorization of the digital collection Mythologiae in Linked Open Data format. The research aims to explore the data of a collection of artworks (Mythologiae) by promoting the potential of the Semantic Web, focusing particularly on the formal representation of the association of cultural objects with literary sources, as realized by experts, also documenting their interpretations. The workflow consisted of defining the data model, cleaning and disambiguating the data, converting it (from tabular structure to graph), and conducting testing activities (particularly expert domain review of the dataset through competency questions and data visualizations). The result is the mythLOD platform, which presents the dataset and detailed research documentation. Additionally, the platform hosts two data visualization spaces (the online catalogue and a data storytelling experiment on the case study of the Aeneid) that enrich the project documentation as user-friendly test units for the dataset and constitute an additional project documentation tool and exploration of the collection.","sentences":["The formal representation of cultural metadata has always been a challenge, considering both the heterogeneity of cultural objects and the need to document the interpretive act exercised by experts.","This article provides an overview of the revalorization of the digital collection Mythologiae in Linked Open Data format.","The research aims to explore the data of a collection of artworks (Mythologiae) by promoting the potential of the Semantic Web, focusing particularly on the formal representation of the association of cultural objects with literary sources, as realized by experts, also documenting their interpretations.","The workflow consisted of defining the data model, cleaning and disambiguating the data, converting it (from tabular structure to graph), and conducting testing activities (particularly expert domain review of the dataset through competency questions and data visualizations).","The result is the mythLOD platform, which presents the dataset and detailed research documentation.","Additionally, the platform hosts two data visualization spaces (the online catalogue and a data storytelling experiment on the case study of the Aeneid) that enrich the project documentation as user-friendly test units for the dataset and constitute an additional project documentation tool and exploration of the collection."],"url":"http://arxiv.org/abs/2404.07006v1","category":"cs.DL"}
{"created":"2024-04-10 12:53:27","title":"Search for a light muon-philic $Z'$ with the NA64-$e$ experiment at CERN","abstract":"The inclusion of an additional U(1) gauge $L_{\\mu} - L_{\\tau}$ symmetry would release the tension between the measured and the predicted value of the anomalous muon magnetic moment: this paradigm assumes the existence of a new, light $Z'$ vector boson, with dominant coupling to ${\\mu}$ and ${\\tau}$ and interacting with electrons via a loop mechanism. The $L_{\\mu} - L_{\\tau}$ model can also explain the Dark Matter relic abundance, by assuming that the $Z'$ boson acts as a \"portal\" to a new Dark Sector of particles in Nature, not charged under known interactions. In this work we present the results of the $Z'$ search performed by the NA64-$e$ experiment at CERN SPS, that collected $ \\sim 9 \\times 10^{11}$ 100-GeV electrons impinging on an active thick target. Despite the suppressed $Z'$ production yield with an electron beam, the limits sets by NA64-$e$ are competitive with other experimental searches, and partially exclude the $g-2$ preferred model parameter values for $Z'$ masses lighter than 2 MeV. This result proves the complementarity of this search with NA64-${\\mu}$, the parallel effort of the NA64 collaboration with a muon beam.","sentences":["The inclusion of an additional U(1) gauge $L_{\\mu} - L_{\\tau}$ symmetry would release the tension between the measured and the predicted value of the anomalous muon magnetic moment: this paradigm assumes the existence of a new, light $Z'$ vector boson, with dominant coupling to ${\\mu}$ and ${\\tau}$ and interacting with electrons via a loop mechanism.","The $L_{\\mu} - L_{\\tau}$ model can also explain the Dark Matter relic abundance, by assuming that the $Z'$ boson acts as a \"portal\" to a new Dark Sector of particles in Nature, not charged under known interactions.","In this work we present the results of the $Z'$ search performed by the NA64-$e$ experiment at CERN SPS, that collected $ \\sim 9 \\times 10^{11}$ 100-GeV electrons impinging on an active thick target.","Despite the suppressed $Z'$ production yield with an electron beam, the limits sets by NA64-$e$ are competitive with other experimental searches, and partially exclude the $g-2$ preferred model parameter values for $Z'$ masses lighter than 2 MeV.","This result proves the complementarity of this search with NA64-${\\mu}$, the parallel effort of the NA64 collaboration with a muon beam."],"url":"http://arxiv.org/abs/2404.06982v1","category":"hep-ex"}
{"created":"2024-04-10 12:42:28","title":"Quati: A Brazilian Portuguese Information Retrieval Dataset from Native Speakers","abstract":"Despite Portuguese being one of the most spoken languages in the world, there is a lack of high-quality information retrieval datasets in that language. We present Quati, a dataset specifically designed for the Brazilian Portuguese language. It comprises a collection of queries formulated by native speakers and a curated set of documents sourced from a selection of high-quality Brazilian Portuguese websites. These websites are frequented more likely by real users compared to those randomly scraped, ensuring a more representative and relevant corpus. To label the query-document pairs, we use a state-of-the-art LLM, which shows inter-annotator agreement levels comparable to human performance in our assessments. We provide a detailed description of our annotation methodology to enable others to create similar datasets for other languages, providing a cost-effective way of creating high-quality IR datasets with an arbitrary number of labeled documents per query. Finally, we evaluate a diverse range of open-source and commercial retrievers to serve as baseline systems. Quati is publicly available at https://huggingface.co/datasets/unicamp-dl/quati and all scripts at https://github.com/unicamp-dl/quati .","sentences":["Despite Portuguese being one of the most spoken languages in the world, there is a lack of high-quality information retrieval datasets in that language.","We present Quati, a dataset specifically designed for the Brazilian Portuguese language.","It comprises a collection of queries formulated by native speakers and a curated set of documents sourced from a selection of high-quality Brazilian Portuguese websites.","These websites are frequented more likely by real users compared to those randomly scraped, ensuring a more representative and relevant corpus.","To label the query-document pairs, we use a state-of-the-art LLM, which shows inter-annotator agreement levels comparable to human performance in our assessments.","We provide a detailed description of our annotation methodology to enable others to create similar datasets for other languages, providing a cost-effective way of creating high-quality IR datasets with an arbitrary number of labeled documents per query.","Finally, we evaluate a diverse range of open-source and commercial retrievers to serve as baseline systems.","Quati is publicly available at https://huggingface.co/datasets/unicamp-dl/quati and all scripts at https://github.com/unicamp-dl/quati ."],"url":"http://arxiv.org/abs/2404.06976v1","category":"cs.IR"}
{"created":"2024-04-10 12:37:53","title":"Embedding Economic Incentives in Social Networks Shape the Diffusion of Digital Technological Innovation","abstract":"The digital innovation accompanied by explicit economic incentives have fundamentally changed the process of innovation diffusion. As a representative of digital innovation, NFTs provide a decentralized and secure way to authenticate and trade digital assets, offering the potential for new revenue streams in the digital space. However, current researches about NFTs mainly focus on their transaction networks and community culture, leaving the interplay among diffusion dynamics, economic dynamics, and social constraints on Twitter. By collecting and analyzing NFTs-related tweet dataset, the motivations of retweeters, the information mechanisms behind emojis, and the networked-based diffusion dynamics is systematically investigated. Results indicate that Retweeting is fueled by Freemint and trading information, with the higher economic incentives as a major motivation and some potential organizational tendencies. The diffusion of NFT is primarily driven by a 'Ringed-layered' information mechanism involving individual promoters and speculators. Both the frequency and presentation of content contribute positively to the growth of the retweet network. This study contributes to the innovation diffusion theory with economic incentives embedded.","sentences":["The digital innovation accompanied by explicit economic incentives have fundamentally changed the process of innovation diffusion.","As a representative of digital innovation, NFTs provide a decentralized and secure way to authenticate and trade digital assets, offering the potential for new revenue streams in the digital space.","However, current researches about NFTs mainly focus on their transaction networks and community culture, leaving the interplay among diffusion dynamics, economic dynamics, and social constraints on Twitter.","By collecting and analyzing NFTs-related tweet dataset, the motivations of retweeters, the information mechanisms behind emojis, and the networked-based diffusion dynamics is systematically investigated.","Results indicate that Retweeting is fueled by Freemint and trading information, with the higher economic incentives as a major motivation and some potential organizational tendencies.","The diffusion of NFT is primarily driven by a 'Ringed-layered' information mechanism involving individual promoters and speculators.","Both the frequency and presentation of content contribute positively to the growth of the retweet network.","This study contributes to the innovation diffusion theory with economic incentives embedded."],"url":"http://arxiv.org/abs/2404.06973v1","category":"cs.SI"}
{"created":"2024-04-10 12:22:32","title":"Charles Translator: A Machine Translation System between Ukrainian and Czech","abstract":"We present Charles Translator, a machine translation system between Ukrainian and Czech, developed as part of a society-wide effort to mitigate the impact of the Russian-Ukrainian war on individuals and society. The system was developed in the spring of 2022 with the help of many language data providers in order to quickly meet the demand for such a service, which was not available at the time in the required quality. The translator was later implemented as an online web interface and as an Android app with speech input, both featuring Cyrillic-Latin script transliteration. The system translates directly, compared to other available systems that use English as a pivot, and thus take advantage of the typological similarity of the two languages. It uses the block back-translation method, which allows for efficient use of monolingual training data. The paper describes the development process, including data collection and implementation, evaluation, mentions several use cases, and outlines possibilities for the further development of the system for educational purposes.","sentences":["We present Charles Translator, a machine translation system between Ukrainian and Czech, developed as part of a society-wide effort to mitigate the impact of the Russian-Ukrainian war on individuals and society.","The system was developed in the spring of 2022 with the help of many language data providers in order to quickly meet the demand for such a service, which was not available at the time in the required quality.","The translator was later implemented as an online web interface and as an Android app with speech input, both featuring Cyrillic-Latin script transliteration.","The system translates directly, compared to other available systems that use English as a pivot, and thus take advantage of the typological similarity of the two languages.","It uses the block back-translation method, which allows for efficient use of monolingual training data.","The paper describes the development process, including data collection and implementation, evaluation, mentions several use cases, and outlines possibilities for the further development of the system for educational purposes."],"url":"http://arxiv.org/abs/2404.06964v1","category":"cs.CL"}
{"created":"2024-04-10 09:25:18","title":"Research Artifacts in Software Engineering Publications: Status and Trends","abstract":"The Software Engineering (SE) community has been embracing the open science policy and encouraging researchers to disclose artifacts in their publications. However, the status and trends of artifact practice and quality remain unclear, lacking insights on further improvement. In this paper, we present an empirical study to characterize the research artifacts in SE publications. Specifically, we manually collect 1,487 artifacts from all 2,196 papers published in top-tier SE conferences (ASE, FSE, ICSE, and ISSTA) from 2017 to 2022. We investigate the common practices (e.g., URL location and format, storage websites), maintenance activities (e.g., last update time and URL validity), popularity (e.g., the number of stars on GitHub and characteristics), and quality (e.g., documentation and code smell) of these artifacts. Based on our analysis, we reveal a rise in publications providing artifacts. The usage of Zenodo for sharing artifacts has significantly increased. However, artifacts stored in GitHub tend to receive few stars, indicating a limited influence on real-world SE applications. We summarize the results and provide suggestions to different stakeholders in conjunction with current guidelines.","sentences":["The Software Engineering (SE) community has been embracing the open science policy and encouraging researchers to disclose artifacts in their publications.","However, the status and trends of artifact practice and quality remain unclear, lacking insights on further improvement.","In this paper, we present an empirical study to characterize the research artifacts in SE publications.","Specifically, we manually collect 1,487 artifacts from all 2,196 papers published in top-tier SE conferences (ASE, FSE, ICSE, and ISSTA) from 2017 to 2022.","We investigate the common practices (e.g., URL location and format, storage websites), maintenance activities (e.g., last update time and URL validity), popularity (e.g., the number of stars on GitHub and characteristics), and quality (e.g., documentation and code smell) of these artifacts.","Based on our analysis, we reveal a rise in publications providing artifacts.","The usage of Zenodo for sharing artifacts has significantly increased.","However, artifacts stored in GitHub tend to receive few stars, indicating a limited influence on real-world SE applications.","We summarize the results and provide suggestions to different stakeholders in conjunction with current guidelines."],"url":"http://arxiv.org/abs/2404.06852v1","category":"cs.SE"}
{"created":"2024-04-10 09:09:03","title":"Projection method for quasiperiodic elliptic equations and application to quasiperiodic homogenization","abstract":"In this study, our main objective is to address the challenge of solving elliptic equations with quasiperiodic coefficients. To achieve accurate and efficient computation, we introduce the projection method, which enables the embedding of quasiperiodic systems into higher-dimensional periodic systems. To enhance the computational efficiency, we propose a compressed storage strategy for the stiffness matrix, reducing memory requirements while preserving accuracy. Furthermore, we design a diagonal preconditioner to efficiently solve the resulting high-dimensional linear system by reducing the condition number of the stiffness matrix. These techniques collectively contribute to the computational effectiveness of our proposed approach. We demonstrate the effectiveness and accuracy of our approach through a series of numerical examples. Moreover, we apply our method to achieve a highly accurate computation of the homogenized coefficients for a quasiperiodic multiscale elliptic equation.","sentences":["In this study, our main objective is to address the challenge of solving elliptic equations with quasiperiodic coefficients.","To achieve accurate and efficient computation, we introduce the projection method, which enables the embedding of quasiperiodic systems into higher-dimensional periodic systems.","To enhance the computational efficiency, we propose a compressed storage strategy for the stiffness matrix, reducing memory requirements while preserving accuracy.","Furthermore, we design a diagonal preconditioner to efficiently solve the resulting high-dimensional linear system by reducing the condition number of the stiffness matrix.","These techniques collectively contribute to the computational effectiveness of our proposed approach.","We demonstrate the effectiveness and accuracy of our approach through a series of numerical examples.","Moreover, we apply our method to achieve a highly accurate computation of the homogenized coefficients for a quasiperiodic multiscale elliptic equation."],"url":"http://arxiv.org/abs/2404.06841v1","category":"math.NA"}
{"created":"2024-04-10 08:31:40","title":"Impact of Extensions on Browser Performance: An Empirical Study on Google Chrome","abstract":"Web browsers have been used widely by users to conduct various online activities, such as information seeking or online shopping. To improve user experience and extend the functionality of browsers, practitioners provide mechanisms to allow users to install third-party-provided plugins (i.e., extensions) on their browsers. However, little is known about the performance implications caused by such extensions. In this paper, we conduct an empirical study to understand the impact of extensions on the user-perceived performance (i.e., energy consumption and page load time) of Google Chrome, the most popular browser. We study a total of 72 representative extensions from 11 categories (e.g., Developer Tools and Sports). We observe that browser performance can be negatively impacted by the use of extensions, even when the extensions are used in unintended circumstances (e.g., when logging into an extension is not granted but required, or when an extension is not used for designated websites). We also identify a set of factors that significantly influence the performance impact of extensions, such as code complexity and privacy practices (i.e., collection of user data) adopted by the extensions. Based on our empirical observations, we provide recommendations for developers and users to mitigate the performance impact of browser extensions, such as conducting performance testing and optimization for unintended usage scenarios of extensions, or adhering to proper usage practices of extensions (e.g., logging into an extension when required).","sentences":["Web browsers have been used widely by users to conduct various online activities, such as information seeking or online shopping.","To improve user experience and extend the functionality of browsers, practitioners provide mechanisms to allow users to install third-party-provided plugins (i.e., extensions) on their browsers.","However, little is known about the performance implications caused by such extensions.","In this paper, we conduct an empirical study to understand the impact of extensions on the user-perceived performance (i.e., energy consumption and page load time) of Google Chrome, the most popular browser.","We study a total of 72 representative extensions from 11 categories (e.g., Developer Tools and Sports).","We observe that browser performance can be negatively impacted by the use of extensions, even when the extensions are used in unintended circumstances (e.g., when logging into an extension is not granted but required, or when an extension is not used for designated websites).","We also identify a set of factors that significantly influence the performance impact of extensions, such as code complexity and privacy practices (i.e., collection of user data) adopted by the extensions.","Based on our empirical observations, we provide recommendations for developers and users to mitigate the performance impact of browser extensions, such as conducting performance testing and optimization for unintended usage scenarios of extensions, or adhering to proper usage practices of extensions (e.g., logging into an extension when required)."],"url":"http://arxiv.org/abs/2404.06827v1","category":"cs.PF"}
{"created":"2024-04-10 06:45:02","title":"On the effect of a large cloud of rigid particles on the motion of an incompressible non--Newtonian fluid","abstract":"We show that the collective effect of $N$ rigid bodies $(\\mathcal{S}_{n,N})_{n=1}^N$ of diameters $(r_{n,N})_{n=1}^N$ immersed in an incompressible non--Newtonian fluid is negligible in the asymptotic limit $N \\to \\infty$ as long as their total packing volume $\\sum_{n=1}^N r_{n,N}^d$, $d=2,3$ tends to zero exponentially -- $\\sum_{n=1}^N r_{n,N}^d \\approx A^{-N}$ -- for a certain constant $A > 1$. The result is rather surprising and in a sharp contrast with the associated homogenization problem, where the same number of obstacles can completely stop the fluid motion in the case of shear thickening viscosity. A large class of non--Newtonian fluids is included, for which the viscous stress is a subdifferential of a convex potential.","sentences":["We show that the collective effect of $N$ rigid bodies $(\\mathcal{S}_{n,N})_{n=1}^N$ of diameters $(r_{n,N})_{n=1}^N$ immersed in an incompressible non--Newtonian fluid is negligible in the asymptotic limit $N \\to \\infty$ as long as their total packing volume $\\sum_{n=1}^N r_{n,N}^d$, $d=2,3$ tends to zero exponentially -- $\\sum_{n=1}^N r_{n,N}^d \\approx A^{-N}$ -- for a certain constant $A > 1$.","The result is rather surprising and in a sharp contrast with the associated homogenization problem, where the same number of obstacles can completely stop the fluid motion in the case of shear thickening viscosity.","A large class of non--Newtonian fluids is included, for which the viscous stress is a subdifferential of a convex potential."],"url":"http://arxiv.org/abs/2404.06782v1","category":"math.AP"}
{"created":"2024-04-10 06:15:32","title":"A Lagrangian meshfree model for solidification of liquid thin-films","abstract":"In this paper, a new method to model solidification of thin liquid films is proposed. \\blue{This method is targeted at applications like aircraft icing and tablet coating where the formation of liquid films from impinging droplets on a surface form a critical part of the physics of the process.} The proposed model takes into account the (i) unsteadiness in temperature distribution, (ii) heat transfer at the interface between the solid and the surface, (iii) volumetric expansion/contraction and (iv) the liquid thin-film behaviour, each of which are either partly or fully ignored in existing models. The liquid thin-film, modeled using the Discrete Droplet Method (DDM), is represented as a collection of discrete droplets that are tracked in a Lagrangian sense. The height of the liquid film is estimated as a summation of Gaussian kernel functions associated with each droplet. At each droplet location, a solid height is also computed. The evolution of the solid height is governed by the Stefan problem. The flow of the liquid thin-film is solved just as in the case of DDM, while also taking into consideration the shape of the solidified region lying beneath the droplet. The results presented in this work show the reliability of the proposed model in simulating solidification of thin-films and its applicability to complex problems such as ice-formation on aircraft wings. The model has been verified for canonical problems that have analytical solutions. For the more complex problems of icing, the results of the model are compared with data from literature, without considering a background air flow. The comparison can be improved by coupling this model with suitable air flow solvers, as shown in the final test case.","sentences":["In this paper, a new method to model solidification of thin liquid films is proposed.","\\blue{This method is targeted at applications like aircraft icing and tablet coating where the formation of liquid films from impinging droplets on a surface form a critical part of the physics of the process.}","The proposed model takes into account the (i) unsteadiness in temperature distribution, (ii) heat transfer at the interface between the solid and the surface, (iii) volumetric expansion/contraction and (iv) the liquid thin-film behaviour, each of which are either partly or fully ignored in existing models.","The liquid thin-film, modeled using the Discrete Droplet Method (DDM), is represented as a collection of discrete droplets that are tracked in a Lagrangian sense.","The height of the liquid film is estimated as a summation of Gaussian kernel functions associated with each droplet.","At each droplet location, a solid height is also computed.","The evolution of the solid height is governed by the Stefan problem.","The flow of the liquid thin-film is solved just as in the case of DDM, while also taking into consideration the shape of the solidified region lying beneath the droplet.","The results presented in this work show the reliability of the proposed model in simulating solidification of thin-films and its applicability to complex problems such as ice-formation on aircraft wings.","The model has been verified for canonical problems that have analytical solutions.","For the more complex problems of icing, the results of the model are compared with data from literature, without considering a background air flow.","The comparison can be improved by coupling this model with suitable air flow solvers, as shown in the final test case."],"url":"http://arxiv.org/abs/2404.06766v1","category":"physics.flu-dyn"}
{"created":"2024-04-10 04:35:54","title":"Accuracy of a Large Language Model in Distinguishing Anti- And Pro-vaccination Messages on Social Media: The Case of Human Papillomavirus Vaccination","abstract":"Objective. Vaccination has engendered a spectrum of public opinions, with social media acting as a crucial platform for health-related discussions. The emergence of artificial intelligence technologies, such as large language models (LLMs), offers a novel opportunity to efficiently investigate public discourses. This research assesses the accuracy of ChatGPT, a widely used and freely available service built upon an LLM, for sentiment analysis to discern different stances toward Human Papillomavirus (HPV) vaccination. Methods. Messages related to HPV vaccination were collected from social media supporting different message formats: Facebook (long format) and Twitter (short format). A selection of 1,000 human-evaluated messages was input into the LLM, which generated multiple response instances containing its classification results. Accuracy was measured for each message as the level of concurrence between human and machine decisions, ranging between 0 and 1. Results. Average accuracy was notably high when 20 response instances were used to determine the machine decision of each message: .882 (SE = .021) and .750 (SE = .029) for anti- and pro-vaccination long-form; .773 (SE = .027) and .723 (SE = .029) for anti- and pro-vaccination short-form, respectively. Using only three or even one instance did not lead to a severe decrease in accuracy. However, for long-form messages, the language model exhibited significantly lower accuracy in categorizing pro-vaccination messages than anti-vaccination ones. Conclusions. ChatGPT shows potential in analyzing public opinions on HPV vaccination using social media content. However, understanding the characteristics and limitations of a language model within specific public health contexts remains imperative.","sentences":["Objective.","Vaccination has engendered a spectrum of public opinions, with social media acting as a crucial platform for health-related discussions.","The emergence of artificial intelligence technologies, such as large language models (LLMs), offers a novel opportunity to efficiently investigate public discourses.","This research assesses the accuracy of ChatGPT, a widely used and freely available service built upon an LLM, for sentiment analysis to discern different stances toward Human Papillomavirus (HPV) vaccination.","Methods.","Messages related to HPV vaccination were collected from social media supporting different message formats: Facebook (long format) and Twitter (short format).","A selection of 1,000 human-evaluated messages was input into the LLM, which generated multiple response instances containing its classification results.","Accuracy was measured for each message as the level of concurrence between human and machine decisions, ranging between 0 and 1.","Results.","Average accuracy was notably high when 20 response instances were used to determine the machine decision of each message: .882 (SE = .021) and .750 (SE = .029) for anti- and pro-vaccination long-form; .773 (SE = .027) and .723 (SE = .029) for anti- and pro-vaccination short-form, respectively.","Using only three or even one instance did not lead to a severe decrease in accuracy.","However, for long-form messages, the language model exhibited significantly lower accuracy in categorizing pro-vaccination messages than anti-vaccination ones.","Conclusions.","ChatGPT shows potential in analyzing public opinions on HPV vaccination using social media content.","However, understanding the characteristics and limitations of a language model within specific public health contexts remains imperative."],"url":"http://arxiv.org/abs/2404.06731v1","category":"cs.CY"}
{"created":"2024-04-10 04:25:41","title":"A Data Efficient Framework for Learning Local Heuristics","abstract":"With the advent of machine learning, there have been several recent attempts to learn effective and generalizable heuristics. Local Heuristic A* (LoHA*) is one recent method that instead of learning the entire heuristic estimate, learns a \"local\" residual heuristic that estimates the cost to escape a region (Veerapaneni et al 2023). LoHA*, like other supervised learning methods, collects a dataset of target values by querying an oracle on many planning problems (in this case, local planning problems). This data collection process can become slow as the size of the local region increases or if the domain requires expensive collision checks. Our main insight is that when an A* search solves a start-goal planning problem it inherently ends up solving multiple local planning problems. We exploit this observation to propose an efficient data collection framework that does <1/10th the amount of work (measured by expansions) to collect the same amount of data in comparison to baselines. This idea also enables us to run LoHA* in an online manner where we can iteratively collect data and improve our model while solving relevant start-goal tasks. We demonstrate the performance of our data collection and online framework on a 4D $(x, y, \\theta, v)$ navigation domain.","sentences":["With the advent of machine learning, there have been several recent attempts to learn effective and generalizable heuristics.","Local Heuristic A* (LoHA*) is one recent method that instead of learning the entire heuristic estimate, learns a \"local\" residual heuristic that estimates the cost to escape a region (Veerapaneni et al 2023).","LoHA*, like other supervised learning methods, collects a dataset of target values by querying an oracle on many planning problems (in this case, local planning problems).","This data collection process can become slow as the size of the local region increases or if the domain requires expensive collision checks.","Our main insight is that when an A* search solves a start-goal planning problem it inherently ends up solving multiple local planning problems.","We exploit this observation to propose an efficient data collection framework that does <1/10th the amount of work (measured by expansions) to collect the same amount of data in comparison to baselines.","This idea also enables us to run LoHA*","in an online manner where we can iteratively collect data and improve our model while solving relevant start-goal tasks.","We demonstrate the performance of our data collection and online framework on a 4D $(x, y, \\theta, v)$ navigation domain."],"url":"http://arxiv.org/abs/2404.06728v1","category":"cs.RO"}
{"created":"2024-04-10 04:20:32","title":"Best-in-class modeling: A novel strategy to discover constitutive models for soft matter systems","abstract":"The ability to automatically discover interpretable mathematical models from data could forever change how we model soft matter systems. For convex discovery problems with a unique global minimum, model discovery is well-established. It uses a classical top-down approach that first calculates a dense parameter vector, and then sparsifies the vector by gradually removing terms. For non-convex discovery problems with multiple local minima, this strategy is infeasible since the initial parameter vector is generally non-unique. Here we propose a novel bottom-up approach that starts with a sparse single-term vector, and then densifies the vector by systematically adding terms. Along the way, we discover models of gradually increasing complexity, a strategy that we call best-in-class modeling. To identify successful candidate terms, we reverse-engineer a library of sixteen functional building blocks that integrate a century of knowledge in material modeling with recent trends in machine learning and artificial intelligence. Yet, instead of solving the discrete combinatorial problem with 65,536 possible combinations of terms, best-in-class modeling starts with the best one-term model and iteratively repeats adding terms, until the objective function meets a user-defined convergence criterion. Strikingly, we achieve good convergence with only one or two terms. We illustrate the best-in-class one- and two-term models for a variety of soft matter systems including rubber, brain, artificial meat, skin, and arteries. Our discovered models display distinct and unexpected features for each family of materials, and suggest that best-in-class modeling is an efficient, robust, and easy-to-use strategy to discover the mechanical signatures of traditional and unconventional soft materials. We anticipate that our technology will generalize naturally to other classes of natural and man-made soft matter.","sentences":["The ability to automatically discover interpretable mathematical models from data could forever change how we model soft matter systems.","For convex discovery problems with a unique global minimum, model discovery is well-established.","It uses a classical top-down approach that first calculates a dense parameter vector, and then sparsifies the vector by gradually removing terms.","For non-convex discovery problems with multiple local minima, this strategy is infeasible since the initial parameter vector is generally non-unique.","Here we propose a novel bottom-up approach that starts with a sparse single-term vector, and then densifies the vector by systematically adding terms.","Along the way, we discover models of gradually increasing complexity, a strategy that we call best-in-class modeling.","To identify successful candidate terms, we reverse-engineer a library of sixteen functional building blocks that integrate a century of knowledge in material modeling with recent trends in machine learning and artificial intelligence.","Yet, instead of solving the discrete combinatorial problem with 65,536 possible combinations of terms, best-in-class modeling starts with the best one-term model and iteratively repeats adding terms, until the objective function meets a user-defined convergence criterion.","Strikingly, we achieve good convergence with only one or two terms.","We illustrate the best-in-class one- and two-term models for a variety of soft matter systems including rubber, brain, artificial meat, skin, and arteries.","Our discovered models display distinct and unexpected features for each family of materials, and suggest that best-in-class modeling is an efficient, robust, and easy-to-use strategy to discover the mechanical signatures of traditional and unconventional soft materials.","We anticipate that our technology will generalize naturally to other classes of natural and man-made soft matter."],"url":"http://arxiv.org/abs/2404.06725v1","category":"cond-mat.soft"}
{"created":"2024-04-10 04:19:36","title":"Fuel-optimal powered descent guidance for lunar pinpoint landing using neural networks","abstract":"This paper presents a Neural Networks (NNs) based approach for designing the Fuel-Optimal Powered Descent Guidance (FOPDG) for lunar pinpoint landing. According to Pontryagin's Minimum Principle, the optimality conditions are first derived. To generate the dataset of optimal trajectories for training NNs, we formulate a parameterized system, which allows for generating each optimal trajectory by a simple propagation without using any optimization method. Then, a dataset containing the optimal state and optimal thrust vector pairs can be readily collected. Since it is challenging for NNs to approximate bang-bang (or discontinuous) type of optimal thrust magnitude, we introduce a regularisation function to the switching function so that the regularized switching function approximated by a simple NN can be used to represent the optimal thrust magnitude. Meanwhile, another two well-trained NNs are used to predict the thrust steering angle and time of flight given a flight state. Finally, numerical simulations show that the proposed method is capable of generating the FOPDG that steers the lunar lander to the desired landing site with acceptable landing errors.","sentences":["This paper presents a Neural Networks (NNs) based approach for designing the Fuel-Optimal Powered Descent Guidance (FOPDG) for lunar pinpoint landing.","According to Pontryagin's Minimum Principle, the optimality conditions are first derived.","To generate the dataset of optimal trajectories for training NNs, we formulate a parameterized system, which allows for generating each optimal trajectory by a simple propagation without using any optimization method.","Then, a dataset containing the optimal state and optimal thrust vector pairs can be readily collected.","Since it is challenging for NNs to approximate bang-bang (or discontinuous) type of optimal thrust magnitude, we introduce a regularisation function to the switching function so that the regularized switching function approximated by a simple NN can be used to represent the optimal thrust magnitude.","Meanwhile, another two well-trained NNs are used to predict the thrust steering angle and time of flight given a flight state.","Finally, numerical simulations show that the proposed method is capable of generating the FOPDG that steers the lunar lander to the desired landing site with acceptable landing errors."],"url":"http://arxiv.org/abs/2404.06722v1","category":"math.OC"}
{"created":"2024-04-10 04:18:26","title":"Poisoning Prevention in Federated Learning and Differential Privacy via Stateful Proofs of Execution","abstract":"The rise in IoT-driven distributed data analytics, coupled with increasing privacy concerns, has led to a demand for effective privacy-preserving and federated data collection/model training mechanisms. In response, approaches such as Federated Learning (FL) and Local Differential Privacy (LDP) have been proposed and attracted much attention over the past few years. However, they still share the common limitation of being vulnerable to poisoning attacks wherein adversaries compromising edge devices feed forged (a.k.a. poisoned) data to aggregation back-ends, undermining the integrity of FL/LDP results.   In this work, we propose a system-level approach to remedy this issue based on a novel security notion of Proofs of Stateful Execution (PoSX) for IoT/embedded devices' software. To realize the PoSX concept, we design SLAPP: a System-Level Approach for Poisoning Prevention. SLAPP leverages commodity security features of embedded devices - in particular ARM TrustZoneM security extensions - to verifiably bind raw sensed data to their correct usage as part of FL/LDP edge device routines. As a consequence, it offers robust security guarantees against poisoning. Our evaluation, based on real-world prototypes featuring multiple cryptographic primitives and data collection schemes, showcases SLAPP's security and low overhead.","sentences":["The rise in IoT-driven distributed data analytics, coupled with increasing privacy concerns, has led to a demand for effective privacy-preserving and federated data collection/model training mechanisms.","In response, approaches such as Federated Learning (FL) and Local Differential Privacy (LDP) have been proposed and attracted much attention over the past few years.","However, they still share the common limitation of being vulnerable to poisoning attacks wherein adversaries compromising edge devices feed forged (a.k.a. poisoned) data to aggregation back-ends, undermining the integrity of FL/LDP results.   ","In this work, we propose a system-level approach to remedy this issue based on a novel security notion of Proofs of Stateful Execution (PoSX) for IoT/embedded devices' software.","To realize the PoSX concept, we design SLAPP: a System-Level Approach for Poisoning Prevention.","SLAPP leverages commodity security features of embedded devices - in particular ARM TrustZoneM security extensions - to verifiably bind raw sensed data to their correct usage as part of FL/LDP edge device routines.","As a consequence, it offers robust security guarantees against poisoning.","Our evaluation, based on real-world prototypes featuring multiple cryptographic primitives and data collection schemes, showcases SLAPP's security and low overhead."],"url":"http://arxiv.org/abs/2404.06721v1","category":"cs.CR"}
{"created":"2024-04-10 04:10:57","title":"Measurement of the Born cross section for $e^{+}e^{-}\\to \u03b7h_c $ at center-of-mass energies between 4.1 and 4.6\\,GeV","abstract":"We measure the Born cross section for the reaction $e^{+}e^{-} \\rightarrow \\eta h_c$ from $\\sqrt{s} = 4.129$ to $4.600$~GeV using data sets collected by the BESIII detector running at the BEPCII collider. A resonant structure in the cross section line shape near 4.200~GeV is observed with a statistical significance of 7$\\sigma$. The parameters of this resonance are measured to be \\MeasMass\\ and \\MeasWidth, where the first uncertainties are statistical and the second systematic.","sentences":["We measure the Born cross section for the reaction $e^{+}e^{-} \\rightarrow \\eta h_c$ from $\\sqrt{s} = 4.129$ to $4.600$~GeV using data sets collected by the BESIII detector running at the BEPCII collider.","A resonant structure in the cross section line shape near 4.200~GeV is observed with a statistical significance of 7$\\sigma$.","The parameters of this resonance are measured to be \\MeasMass\\ and \\MeasWidth, where the first uncertainties are statistical and the second systematic."],"url":"http://arxiv.org/abs/2404.06718v1","category":"hep-ex"}
{"created":"2024-04-10 04:04:05","title":"Racial/Ethnic Categories in AI and Algorithmic Fairness: Why They Matter and What They Represent","abstract":"Racial diversity has become increasingly discussed within the AI and algorithmic fairness literature, yet little attention is focused on justifying the choices of racial categories and understanding how people are racialized into these chosen racial categories. Even less attention is given to how racial categories shift and how the racialization process changes depending on the context of a dataset or model. An unclear understanding of \\textit{who} comprises the racial categories chosen and \\textit{how} people are racialized into these categories can lead to varying interpretations of these categories. These varying interpretations can lead to harm when the understanding of racial categories and the racialization process is misaligned from the actual racialization process and racial categories used. Harm can also arise if the racialization process and racial categories used are irrelevant or do not exist in the context they are applied.   In this paper, we make two contributions. First, we demonstrate how racial categories with unclear assumptions and little justification can lead to varying datasets that poorly represent groups obfuscated or unrepresented by the given racial categories and models that perform poorly on these groups. Second, we develop a framework, CIRCSheets, for documenting the choices and assumptions in choosing racial categories and the process of racialization into these categories to facilitate transparency in understanding the processes and assumptions made by dataset or model developers when selecting or using these racial categories.","sentences":["Racial diversity has become increasingly discussed within the AI and algorithmic fairness literature, yet little attention is focused on justifying the choices of racial categories and understanding how people are racialized into these chosen racial categories.","Even less attention is given to how racial categories shift and how the racialization process changes depending on the context of a dataset or model.","An unclear understanding of \\textit{who} comprises the racial categories chosen and \\textit{how} people are racialized into these categories can lead to varying interpretations of these categories.","These varying interpretations can lead to harm when the understanding of racial categories and the racialization process is misaligned from the actual racialization process and racial categories used.","Harm can also arise if the racialization process and racial categories used are irrelevant or do not exist in the context they are applied.   ","In this paper, we make two contributions.","First, we demonstrate how racial categories with unclear assumptions and little justification can lead to varying datasets that poorly represent groups obfuscated or unrepresented by the given racial categories and models that perform poorly on these groups.","Second, we develop a framework, CIRCSheets, for documenting the choices and assumptions in choosing racial categories and the process of racialization into these categories to facilitate transparency in understanding the processes and assumptions made by dataset or model developers when selecting or using these racial categories."],"url":"http://arxiv.org/abs/2404.06717v1","category":"cs.CY"}
{"created":"2024-04-10 03:46:03","title":"Llama-VITS: Enhancing TTS Synthesis with Semantic Awareness","abstract":"Recent advancements in Natural Language Processing (NLP) have seen Large-scale Language Models (LLMs) excel at producing high-quality text for various purposes. Notably, in Text-To-Speech (TTS) systems, the integration of BERT for semantic token generation has underscored the importance of semantic content in producing coherent speech outputs. Despite this, the specific utility of LLMs in enhancing TTS synthesis remains considerably limited. This research introduces an innovative approach, Llama-VITS, which enhances TTS synthesis by enriching the semantic content of text using LLM. Llama-VITS integrates semantic embeddings from Llama2 with the VITS model, a leading end-to-end TTS framework. By leveraging Llama2 for the primary speech synthesis process, our experiments demonstrate that Llama-VITS matches the naturalness of the original VITS (ORI-VITS) and those incorporate BERT (BERT-VITS), on the LJSpeech dataset, a substantial collection of neutral, clear speech. Moreover, our method significantly enhances emotive expressiveness on the EmoV_DB_bea_sem dataset, a curated selection of emotionally consistent speech from the EmoV_DB dataset, highlighting its potential to generate emotive speech.","sentences":["Recent advancements in Natural Language Processing (NLP) have seen Large-scale Language Models (LLMs) excel at producing high-quality text for various purposes.","Notably, in Text-To-Speech (TTS) systems, the integration of BERT for semantic token generation has underscored the importance of semantic content in producing coherent speech outputs.","Despite this, the specific utility of LLMs in enhancing TTS synthesis remains considerably limited.","This research introduces an innovative approach, Llama-VITS, which enhances TTS synthesis by enriching the semantic content of text using LLM.","Llama-VITS integrates semantic embeddings from Llama2 with the VITS model, a leading end-to-end TTS framework.","By leveraging Llama2 for the primary speech synthesis process, our experiments demonstrate that Llama-VITS matches the naturalness of the original VITS (ORI-VITS) and those incorporate BERT (BERT-VITS), on the LJSpeech dataset, a substantial collection of neutral, clear speech.","Moreover, our method significantly enhances emotive expressiveness on the EmoV_DB_bea_sem dataset, a curated selection of emotionally consistent speech from the EmoV_DB dataset, highlighting its potential to generate emotive speech."],"url":"http://arxiv.org/abs/2404.06714v1","category":"cs.CL"}
{"created":"2024-04-10 03:31:32","title":"SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera","abstract":"One of the most critical factors in achieving sharp Novel View Synthesis (NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) is the quality of the training images. However, Conventional RGB cameras are susceptible to motion blur. In contrast, neuromorphic cameras like event and spike cameras inherently capture more comprehensive temporal information, which can provide a sharp representation of the scene as additional training data. Recent methods have explored the integration of event cameras to improve the quality of NVS. The event-RGB approaches have some limitations, such as high training costs and the inability to work effectively in the background. Instead, our study introduces a new method that uses the spike camera to overcome these limitations. By considering texture reconstruction from spike streams as ground truth, we design the Texture from Spike (TfS) loss. Since the spike camera relies on temporal integration instead of temporal differentiation used by event cameras, our proposed TfS loss maintains manageable training costs. It handles foreground objects with backgrounds simultaneously. We also provide a real-world dataset captured with our spike-RGB camera system to facilitate future research endeavors. We conduct extensive experiments using synthetic and real-world datasets to demonstrate that our design can enhance novel view synthesis across NeRF and 3DGS. The code and dataset will be made available for public access.","sentences":["One of the most critical factors in achieving sharp Novel View Synthesis (NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) is the quality of the training images.","However, Conventional RGB cameras are susceptible to motion blur.","In contrast, neuromorphic cameras like event and spike cameras inherently capture more comprehensive temporal information, which can provide a sharp representation of the scene as additional training data.","Recent methods have explored the integration of event cameras to improve the quality of NVS.","The event-RGB approaches have some limitations, such as high training costs and the inability to work effectively in the background.","Instead, our study introduces a new method that uses the spike camera to overcome these limitations.","By considering texture reconstruction from spike streams as ground truth, we design the Texture from Spike (TfS) loss.","Since the spike camera relies on temporal integration instead of temporal differentiation used by event cameras, our proposed TfS loss maintains manageable training costs.","It handles foreground objects with backgrounds simultaneously.","We also provide a real-world dataset captured with our spike-RGB camera system to facilitate future research endeavors.","We conduct extensive experiments using synthetic and real-world datasets to demonstrate that our design can enhance novel view synthesis across NeRF and 3DGS.","The code and dataset will be made available for public access."],"url":"http://arxiv.org/abs/2404.06710v1","category":"cs.CV"}
{"created":"2024-04-10 03:20:33","title":"Convolution-based Probability Gradient Loss for Semantic Segmentation","abstract":"In this paper, we introduce a novel Convolution-based Probability Gradient (CPG) loss for semantic segmentation. It employs convolution kernels similar to the Sobel operator, capable of computing the gradient of pixel intensity in an image. This enables the computation of gradients for both ground-truth and predicted category-wise probabilities. It enhances network performance by maximizing the similarity between these two probability gradients. Moreover, to specifically enhance accuracy near the object's boundary, we extract the object boundary based on the ground-truth probability gradient and exclusively apply the CPG loss to pixels belonging to boundaries. CPG loss proves to be highly convenient and effective. It establishes pixel relationships through convolution, calculating errors from a distinct dimension compared to pixel-wise loss functions such as cross-entropy loss. We conduct qualitative and quantitative analyses to evaluate the impact of the CPG loss on three well-established networks (DeepLabv3-Resnet50, HRNetV2-OCR, and LRASPP_MobileNet_V3_Large) across three standard segmentation datasets (Cityscapes, COCO-Stuff, ADE20K). Our extensive experimental results consistently and significantly demonstrate that the CPG loss enhances the mean Intersection over Union.","sentences":["In this paper, we introduce a novel Convolution-based Probability Gradient (CPG) loss for semantic segmentation.","It employs convolution kernels similar to the Sobel operator, capable of computing the gradient of pixel intensity in an image.","This enables the computation of gradients for both ground-truth and predicted category-wise probabilities.","It enhances network performance by maximizing the similarity between these two probability gradients.","Moreover, to specifically enhance accuracy near the object's boundary, we extract the object boundary based on the ground-truth probability gradient and exclusively apply the CPG loss to pixels belonging to boundaries.","CPG loss proves to be highly convenient and effective.","It establishes pixel relationships through convolution, calculating errors from a distinct dimension compared to pixel-wise loss functions such as cross-entropy loss.","We conduct qualitative and quantitative analyses to evaluate the impact of the CPG loss on three well-established networks (DeepLabv3-Resnet50, HRNetV2-OCR, and LRASPP_MobileNet_V3_Large) across three standard segmentation datasets (Cityscapes, COCO-Stuff, ADE20K).","Our extensive experimental results consistently and significantly demonstrate that the CPG loss enhances the mean Intersection over Union."],"url":"http://arxiv.org/abs/2404.06704v1","category":"cs.CV"}
{"created":"2024-04-10 02:54:18","title":"How to Craft Backdoors with Unlabeled Data Alone?","abstract":"Relying only on unlabeled data, Self-supervised learning (SSL) can learn rich features in an economical and scalable way. As the drive-horse for building foundation models, SSL has received a lot of attention recently with wide applications, which also raises security concerns where backdoor attack is a major type of threat: if the released dataset is maliciously poisoned, backdoored SSL models can behave badly when triggers are injected to test samples. The goal of this work is to investigate this potential risk. We notice that existing backdoors all require a considerable amount of \\emph{labeled} data that may not be available for SSL. To circumvent this limitation, we explore a more restrictive setting called no-label backdoors, where we only have access to the unlabeled data alone, where the key challenge is how to select the proper poison set without using label information. We propose two strategies for poison selection: clustering-based selection using pseudolabels, and contrastive selection derived from the mutual information principle. Experiments on CIFAR-10 and ImageNet-100 show that both no-label backdoors are effective on many SSL methods and outperform random poisoning by a large margin. Code will be available at https://github.com/PKU-ML/nlb.","sentences":["Relying only on unlabeled data, Self-supervised learning (SSL) can learn rich features in an economical and scalable way.","As the drive-horse for building foundation models, SSL has received a lot of attention recently with wide applications, which also raises security concerns where backdoor attack is a major type of threat: if the released dataset is maliciously poisoned, backdoored SSL models can behave badly when triggers are injected to test samples.","The goal of this work is to investigate this potential risk.","We notice that existing backdoors all require a considerable amount of \\emph{labeled} data that may not be available for SSL.","To circumvent this limitation, we explore a more restrictive setting called no-label backdoors, where we only have access to the unlabeled data alone, where the key challenge is how to select the proper poison set without using label information.","We propose two strategies for poison selection: clustering-based selection using pseudolabels, and contrastive selection derived from the mutual information principle.","Experiments on CIFAR-10 and ImageNet-100 show that both no-label backdoors are effective on many SSL methods and outperform random poisoning by a large margin.","Code will be available at https://github.com/PKU-ML/nlb."],"url":"http://arxiv.org/abs/2404.06694v1","category":"cs.LG"}
{"created":"2024-04-10 02:32:58","title":"CoVoMix: Advancing Zero-Shot Speech Generation for Human-like Multi-talker Conversations","abstract":"Recent advancements in zero-shot text-to-speech (TTS) modeling have led to significant strides in generating high-fidelity and diverse speech. However, dialogue generation, along with achieving human-like naturalness in speech, continues to be a challenge in the field. In this paper, we introduce CoVoMix: Conversational Voice Mixture Generation, a novel model for zero-shot, human-like, multi-speaker, multi-round dialogue speech generation. CoVoMix is capable of first converting dialogue text into multiple streams of discrete tokens, with each token stream representing semantic information for individual talkers. These token streams are then fed into a flow-matching based acoustic model to generate mixed mel-spectrograms. Finally, the speech waveforms are produced using a HiFi-GAN model. Furthermore, we devise a comprehensive set of metrics for measuring the effectiveness of dialogue modeling and generation. Our experimental results show that CoVoMix can generate dialogues that are not only human-like in their naturalness and coherence but also involve multiple talkers engaging in multiple rounds of conversation. These dialogues, generated within a single channel, are characterized by seamless speech transitions, including overlapping speech, and appropriate paralinguistic behaviors such as laughter. Audio samples are available at https://aka.ms/covomix.","sentences":["Recent advancements in zero-shot text-to-speech (TTS) modeling have led to significant strides in generating high-fidelity and diverse speech.","However, dialogue generation, along with achieving human-like naturalness in speech, continues to be a challenge in the field.","In this paper, we introduce CoVoMix: Conversational Voice Mixture Generation, a novel model for zero-shot, human-like, multi-speaker, multi-round dialogue speech generation.","CoVoMix is capable of first converting dialogue text into multiple streams of discrete tokens, with each token stream representing semantic information for individual talkers.","These token streams are then fed into a flow-matching based acoustic model to generate mixed mel-spectrograms.","Finally, the speech waveforms are produced using a HiFi-GAN model.","Furthermore, we devise a comprehensive set of metrics for measuring the effectiveness of dialogue modeling and generation.","Our experimental results show that CoVoMix can generate dialogues that are not only human-like in their naturalness and coherence but also involve multiple talkers engaging in multiple rounds of conversation.","These dialogues, generated within a single channel, are characterized by seamless speech transitions, including overlapping speech, and appropriate paralinguistic behaviors such as laughter.","Audio samples are available at https://aka.ms/covomix."],"url":"http://arxiv.org/abs/2404.06690v1","category":"eess.AS"}
{"created":"2024-04-10 02:02:34","title":"Causal Unit Selection using Tractable Arithmetic Circuits","abstract":"The unit selection problem aims to find objects, called units, that optimize a causal objective function which describes the objects' behavior in a causal context (e.g., selecting customers who are about to churn but would most likely change their mind if encouraged). While early studies focused mainly on bounding a specific class of counterfactual objective functions using data, more recent work allows one to find optimal units exactly by reducing the causal objective to a classical objective on a meta-model, and then applying a variant of the classical Variable Elimination (VE) algorithm to the meta-model -- assuming a fully specified causal model is available. In practice, however, finding optimal units using this approach can be very expensive because the used VE algorithm must be exponential in the constrained treewidth of the meta-model, which is larger and denser than the original model. We address this computational challenge by introducing a new approach for unit selection that is not necessarily limited by the constrained treewidth. This is done through compiling the meta-model into a special class of tractable arithmetic circuits that allows the computation of optimal units in time linear in the circuit size. We finally present empirical results on random causal models that show order-of-magnitude speedups based on the proposed method for solving unit selection.","sentences":["The unit selection problem aims to find objects, called units, that optimize a causal objective function which describes the objects' behavior in a causal context (e.g., selecting customers who are about to churn but would most likely change their mind if encouraged).","While early studies focused mainly on bounding a specific class of counterfactual objective functions using data, more recent work allows one to find optimal units exactly by reducing the causal objective to a classical objective on a meta-model, and then applying a variant of the classical Variable Elimination (VE) algorithm to the meta-model -- assuming a fully specified causal model is available.","In practice, however, finding optimal units using this approach can be very expensive because the used VE algorithm must be exponential in the constrained treewidth of the meta-model, which is larger and denser than the original model.","We address this computational challenge by introducing a new approach for unit selection that is not necessarily limited by the constrained treewidth.","This is done through compiling the meta-model into a special class of tractable arithmetic circuits that allows the computation of optimal units in time linear in the circuit size.","We finally present empirical results on random causal models that show order-of-magnitude speedups based on the proposed method for solving unit selection."],"url":"http://arxiv.org/abs/2404.06681v1","category":"cs.AI"}
{"created":"2024-04-10 02:00:24","title":"Neural Optimizer Equation, Decay Function, and Learning Rate Schedule Joint Evolution","abstract":"A major contributor to the quality of a deep learning model is the selection of the optimizer. We propose a new dual-joint search space in the realm of neural optimizer search (NOS), along with an integrity check, to automate the process of finding deep learning optimizers. Our dual-joint search space simultaneously allows for the optimization of not only the update equation, but also internal decay functions and learning rate schedules for optimizers. We search the space using our proposed mutation-only, particle-based genetic algorithm able to be massively parallelized for our domain-specific problem. We evaluate our candidate optimizers on the CIFAR-10 dataset using a small ConvNet. To assess generalization, the final optimizers were then transferred to large-scale image classification on CIFAR- 100 and TinyImageNet, while also being fine-tuned on Flowers102, Cars196, and Caltech101 using EfficientNetV2Small. We found multiple optimizers, learning rate schedules, and Adam variants that outperformed Adam, as well as other standard deep learning optimizers, across the image classification tasks.","sentences":["A major contributor to the quality of a deep learning model is the selection of the optimizer.","We propose a new dual-joint search space in the realm of neural optimizer search (NOS), along with an integrity check, to automate the process of finding deep learning optimizers.","Our dual-joint search space simultaneously allows for the optimization of not only the update equation, but also internal decay functions and learning rate schedules for optimizers.","We search the space using our proposed mutation-only, particle-based genetic algorithm able to be massively parallelized for our domain-specific problem.","We evaluate our candidate optimizers on the CIFAR-10 dataset using a small ConvNet.","To assess generalization, the final optimizers were then transferred to large-scale image classification on CIFAR- 100 and TinyImageNet, while also being fine-tuned on Flowers102, Cars196, and Caltech101 using EfficientNetV2Small.","We found multiple optimizers, learning rate schedules, and Adam variants that outperformed Adam, as well as other standard deep learning optimizers, across the image classification tasks."],"url":"http://arxiv.org/abs/2404.06679v1","category":"cs.NE"}
{"created":"2024-04-10 01:33:08","title":"VoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving Zero-Shot Voice Editing","abstract":"We present VoiceShop, a novel speech-to-speech framework that can modify multiple attributes of speech, such as age, gender, accent, and speech style, in a single forward pass while preserving the input speaker's timbre. Previous works have been constrained to specialized models that can only edit these attributes individually and suffer from the following pitfalls: the magnitude of the conversion effect is weak, there is no zero-shot capability for out-of-distribution speakers, or the synthesized outputs exhibit timbre leakage which changes the speaker's perceived identity. Our work proposes solutions for each of these issues in a simple modular framework based on a conditional diffusion backbone model with optional normalizing flow-based and sequence-to-sequence speaker attribute-editing modules, whose components can be combined or removed during inference to meet a wide array of tasks without additional model finetuning. Audio samples are available at https://voiceshopai.github.io","sentences":["We present VoiceShop, a novel speech-to-speech framework that can modify multiple attributes of speech, such as age, gender, accent, and speech style, in a single forward pass while preserving the input speaker's timbre.","Previous works have been constrained to specialized models that can only edit these attributes individually and suffer from the following pitfalls: the magnitude of the conversion effect is weak, there is no zero-shot capability for out-of-distribution speakers, or the synthesized outputs exhibit timbre leakage which changes the speaker's perceived identity.","Our work proposes solutions for each of these issues in a simple modular framework based on a conditional diffusion backbone model with optional normalizing flow-based and sequence-to-sequence speaker attribute-editing modules, whose components can be combined or removed during inference to meet a wide array of tasks without additional model finetuning.","Audio samples are available at https://voiceshopai.github.io"],"url":"http://arxiv.org/abs/2404.06674v1","category":"cs.SD"}
{"created":"2024-04-10 01:21:09","title":"Simple arithmetic operation in latent space can generate a novel three dimensional graph metamaterials","abstract":"Recent advancements in artificial intelligence (AI)-based design strategies for metamaterials have revolutionized the creation of customizable architectures spanning nano- to macro-scale dimensions, achieving unprecedented mechanical behaviors that surpass the inherent properties of the constituent materials. However, the growing complexity of these methods poses challenges in generating diverse metamaterials without substantial human and computational resources, hindering widespread adoption. Addressing this, our study introduces an innovative design strategy capable of generating various three-dimensional graph metamaterials using simple arithmetic operations within the latent space. By seamlessly integrating hidden representations of disentangled latent space and latent diffusion processes, our approach provides a comprehensive understanding of complex design spaces, generating diverse graph metamaterials through arithmetic operations. This methodology stands as a versatile tool for creating structures ranging from repetitive lattice structures to functionally graded mechanical metamaterials. It also serves as an inverse design strategy for diverse lattice structures, including crystalline structures and those made of trabecular bone. We believe that this methodology represents a foundational step in advancing our comprehension of the intricate latent design space, offering the potential to establish a unified model for various traditional generative models in the realm of mechanical metamaterials.","sentences":["Recent advancements in artificial intelligence (AI)-based design strategies for metamaterials have revolutionized the creation of customizable architectures spanning nano- to macro-scale dimensions, achieving unprecedented mechanical behaviors that surpass the inherent properties of the constituent materials.","However, the growing complexity of these methods poses challenges in generating diverse metamaterials without substantial human and computational resources, hindering widespread adoption.","Addressing this, our study introduces an innovative design strategy capable of generating various three-dimensional graph metamaterials using simple arithmetic operations within the latent space.","By seamlessly integrating hidden representations of disentangled latent space and latent diffusion processes, our approach provides a comprehensive understanding of complex design spaces, generating diverse graph metamaterials through arithmetic operations.","This methodology stands as a versatile tool for creating structures ranging from repetitive lattice structures to functionally graded mechanical metamaterials.","It also serves as an inverse design strategy for diverse lattice structures, including crystalline structures and those made of trabecular bone.","We believe that this methodology represents a foundational step in advancing our comprehension of the intricate latent design space, offering the potential to establish a unified model for various traditional generative models in the realm of mechanical metamaterials."],"url":"http://arxiv.org/abs/2404.06671v1","category":"physics.app-ph"}
{"created":"2024-04-10 01:12:20","title":"On Bounds for Greedy Schemes in String Optimization based on Greedy Curvatures","abstract":"We consider the celebrated bound introduced by Conforti and Cornu\\'ejols (1984) for greedy schemes in submodular optimization. The bound assumes a submodular function defined on a collection of sets forming a matroid and is based on greedy curvature. We show that the bound holds for a very general class of string problems that includes maximizing submodular functions over set matroids as a special case. We also derive a bound that is computable in the sense that they depend only on quantities along the greedy trajectory. We prove that our bound is superior to the greedy curvature bound of Conforti and Cornu\\'ejols. In addition, our bound holds under a condition that is weaker than submodularity.","sentences":["We consider the celebrated bound introduced by Conforti and Cornu\\'ejols (1984) for greedy schemes in submodular optimization.","The bound assumes a submodular function defined on a collection of sets forming a matroid and is based on greedy curvature.","We show that the bound holds for a very general class of string problems that includes maximizing submodular functions over set matroids as a special case.","We also derive a bound that is computable in the sense that they depend only on quantities along the greedy trajectory.","We prove that our bound is superior to the greedy curvature bound of Conforti and Cornu\\'ejols.","In addition, our bound holds under a condition that is weaker than submodularity."],"url":"http://arxiv.org/abs/2404.06669v1","category":"eess.SY"}
{"created":"2024-04-10 00:52:54","title":"Forecasting the Future with Future Technologies: Advancements in Large Meteorological Models","abstract":"The field of meteorological forecasting has undergone a significant transformation with the integration of large models, especially those employing deep learning techniques. This paper reviews the advancements and applications of these models in weather prediction, emphasizing their role in transforming traditional forecasting methods. Models like FourCastNet, Pangu-Weather, GraphCast, ClimaX, and FengWu have made notable contributions by providing accurate, high-resolution forecasts, surpassing the capabilities of traditional Numerical Weather Prediction (NWP) models. These models utilize advanced neural network architectures, such as Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and Transformers, to process diverse meteorological data, enhancing predictive accuracy across various time scales and spatial resolutions. The paper addresses challenges in this domain, including data acquisition and computational demands, and explores future opportunities for model optimization and hardware advancements. It underscores the integration of artificial intelligence with conventional meteorological techniques, promising improved weather prediction accuracy and a significant contribution to addressing climate-related challenges. This synergy positions large models as pivotal in the evolving landscape of meteorological forecasting.","sentences":["The field of meteorological forecasting has undergone a significant transformation with the integration of large models, especially those employing deep learning techniques.","This paper reviews the advancements and applications of these models in weather prediction, emphasizing their role in transforming traditional forecasting methods.","Models like FourCastNet, Pangu-Weather, GraphCast, ClimaX, and FengWu have made notable contributions by providing accurate, high-resolution forecasts, surpassing the capabilities of traditional Numerical Weather Prediction (NWP) models.","These models utilize advanced neural network architectures, such as Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and Transformers, to process diverse meteorological data, enhancing predictive accuracy across various time scales and spatial resolutions.","The paper addresses challenges in this domain, including data acquisition and computational demands, and explores future opportunities for model optimization and hardware advancements.","It underscores the integration of artificial intelligence with conventional meteorological techniques, promising improved weather prediction accuracy and a significant contribution to addressing climate-related challenges.","This synergy positions large models as pivotal in the evolving landscape of meteorological forecasting."],"url":"http://arxiv.org/abs/2404.06668v1","category":"cs.LG"}
{"created":"2024-04-10 00:26:08","title":"SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models","abstract":"Text-to-image (T2I) models, such as Stable Diffusion, have exhibited remarkable performance in generating high-quality images from text descriptions in recent years. However, text-to-image models may be tricked into generating not-safe-for-work (NSFW) content, particularly in sexual scenarios. Existing countermeasures mostly focus on filtering inappropriate inputs and outputs, or suppressing improper text embeddings, which can block explicit NSFW-related content (e.g., naked or sexy) but may still be vulnerable to adversarial prompts inputs that appear innocent but are ill-intended. In this paper, we present SafeGen, a framework to mitigate unsafe content generation by text-to-image models in a text-agnostic manner. The key idea is to eliminate unsafe visual representations from the model regardless of the text input. In this way, the text-to-image model is resistant to adversarial prompts since unsafe visual representations are obstructed from within. Extensive experiments conducted on four datasets demonstrate SafeGen's effectiveness in mitigating unsafe content generation while preserving the high-fidelity of benign images. SafeGen outperforms eight state-of-the-art baseline methods and achieves 99.1% sexual content removal performance. Furthermore, our constructed benchmark of adversarial prompts provides a basis for future development and evaluation of anti-NSFW-generation methods.","sentences":["Text-to-image (T2I) models, such as Stable Diffusion, have exhibited remarkable performance in generating high-quality images from text descriptions in recent years.","However, text-to-image models may be tricked into generating not-safe-for-work (NSFW) content, particularly in sexual scenarios.","Existing countermeasures mostly focus on filtering inappropriate inputs and outputs, or suppressing improper text embeddings, which can block explicit NSFW-related content (e.g., naked or sexy) but may still be vulnerable to adversarial prompts inputs that appear innocent but are ill-intended.","In this paper, we present SafeGen, a framework to mitigate unsafe content generation by text-to-image models in a text-agnostic manner.","The key idea is to eliminate unsafe visual representations from the model regardless of the text input.","In this way, the text-to-image model is resistant to adversarial prompts since unsafe visual representations are obstructed from within.","Extensive experiments conducted on four datasets demonstrate SafeGen's effectiveness in mitigating unsafe content generation while preserving the high-fidelity of benign images.","SafeGen outperforms eight state-of-the-art baseline methods and achieves 99.1% sexual content removal performance.","Furthermore, our constructed benchmark of adversarial prompts provides a basis for future development and evaluation of anti-NSFW-generation methods."],"url":"http://arxiv.org/abs/2404.06666v1","category":"cs.CV"}
{"created":"2024-04-10 00:25:09","title":"CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging LLMs' (Lack of) Multicultural Knowledge","abstract":"Frontier large language models (LLMs) are developed by researchers and practitioners with skewed cultural backgrounds and on datasets with skewed sources. However, LLMs' (lack of) multicultural knowledge cannot be effectively assessed with current methods for developing benchmarks. Existing multicultural evaluations primarily rely on expensive and restricted human annotations or potentially outdated internet resources. Thus, they struggle to capture the intricacy, dynamics, and diversity of cultural norms. LLM-generated benchmarks are promising, yet risk propagating the same biases they are meant to measure. To synergize the creativity and expert cultural knowledge of human annotators and the scalability and standardizability of LLM-based automation, we introduce CulturalTeaming, an interactive red-teaming system that leverages human-AI collaboration to build truly challenging evaluation dataset for assessing the multicultural knowledge of LLMs, while improving annotators' capabilities and experiences. Our study reveals that CulturalTeaming's various modes of AI assistance support annotators in creating cultural questions, that modern LLMs fail at, in a gamified manner. Importantly, the increased level of AI assistance (e.g., LLM-generated revision hints) empowers users to create more difficult questions with enhanced perceived creativity of themselves, shedding light on the promises of involving heavier AI assistance in modern evaluation dataset creation procedures. Through a series of 1-hour workshop sessions, we gather CULTURALBENCH-V0.1, a compact yet high-quality evaluation dataset with users' red-teaming attempts, that different families of modern LLMs perform with accuracy ranging from 37.7% to 72.2%, revealing a notable gap in LLMs' multicultural proficiency.","sentences":["Frontier large language models (LLMs) are developed by researchers and practitioners with skewed cultural backgrounds and on datasets with skewed sources.","However, LLMs' (lack of) multicultural knowledge cannot be effectively assessed with current methods for developing benchmarks.","Existing multicultural evaluations primarily rely on expensive and restricted human annotations or potentially outdated internet resources.","Thus, they struggle to capture the intricacy, dynamics, and diversity of cultural norms.","LLM-generated benchmarks are promising, yet risk propagating the same biases they are meant to measure.","To synergize the creativity and expert cultural knowledge of human annotators and the scalability and standardizability of LLM-based automation, we introduce CulturalTeaming, an interactive red-teaming system that leverages human-AI collaboration to build truly challenging evaluation dataset for assessing the multicultural knowledge of LLMs, while improving annotators' capabilities and experiences.","Our study reveals that CulturalTeaming's various modes of AI assistance support annotators in creating cultural questions, that modern LLMs fail at, in a gamified manner.","Importantly, the increased level of AI assistance (e.g., LLM-generated revision hints) empowers users to create more difficult questions with enhanced perceived creativity of themselves, shedding light on the promises of involving heavier AI assistance in modern evaluation dataset creation procedures.","Through a series of 1-hour workshop sessions, we gather CULTURALBENCH-V0.1, a compact yet high-quality evaluation dataset with users' red-teaming attempts, that different families of modern LLMs perform with accuracy ranging from 37.7% to 72.2%, revealing a notable gap in LLMs' multicultural proficiency."],"url":"http://arxiv.org/abs/2404.06664v1","category":"cs.CL"}
{"created":"2024-04-10 00:11:03","title":"Multi-modal Document Presentation Attack Detection With Forensics Trace Disentanglement","abstract":"Document Presentation Attack Detection (DPAD) is an important measure in protecting the authenticity of a document image. However, recent DPAD methods demand additional resources, such as manual effort in collecting additional data or knowing the parameters of acquisition devices. This work proposes a DPAD method based on multi-modal disentangled traces (MMDT) without the above drawbacks. We first disentangle the recaptured traces by a self-supervised disentanglement and synthesis network to enhance the generalization capacity in document images with different contents and layouts. Then, unlike the existing DPAD approaches that rely only on data in the RGB domain, we propose to explicitly employ the disentangled recaptured traces as new modalities in the transformer backbone through adaptive multi-modal adapters to fuse RGB/trace features efficiently. Visualization of the disentangled traces confirms the effectiveness of the proposed method in different document contents. Extensive experiments on three benchmark datasets demonstrate the superiority of our MMDT method on representing forensic traces of recapturing distortion.","sentences":["Document Presentation Attack Detection (DPAD) is an important measure in protecting the authenticity of a document image.","However, recent DPAD methods demand additional resources, such as manual effort in collecting additional data or knowing the parameters of acquisition devices.","This work proposes a DPAD method based on multi-modal disentangled traces (MMDT) without the above drawbacks.","We first disentangle the recaptured traces by a self-supervised disentanglement and synthesis network to enhance the generalization capacity in document images with different contents and layouts.","Then, unlike the existing DPAD approaches that rely only on data in the RGB domain, we propose to explicitly employ the disentangled recaptured traces as new modalities in the transformer backbone through adaptive multi-modal adapters to fuse RGB/trace features efficiently.","Visualization of the disentangled traces confirms the effectiveness of the proposed method in different document contents.","Extensive experiments on three benchmark datasets demonstrate the superiority of our MMDT method on representing forensic traces of recapturing distortion."],"url":"http://arxiv.org/abs/2404.06663v1","category":"cs.CV"}
{"created":"2024-04-09 22:59:07","title":"A Frequency-Domain Beamforming Procedure for Extracting Rayleigh Wave Attenuation Coefficients and Small-Strain Damping Ratio from 2D Ambient Noise Array Measurements","abstract":"The small-strain damping ratio plays a crucial role in assessing the response of soil deposits to earthquake-induced ground motions and general dynamic loading. The damping ratio can theoretically be inverted for after extracting frequency-dependent Rayleigh wave attenuation coefficients from wavefields collected during surface wave testing. However, determining reliable estimates of in-situ attenuation coefficients is much more challenging than achieving robust phase velocity dispersion data, which are commonly measured using both active-source and ambient-wavefield surface wave methods. This paper introduces a new methodology for estimating frequency-dependent attenuation coefficients through the analysis of ambient noise wavefield data recorded by two-dimensional (2D) arrays of surface seismic sensors for the subsequent evaluation of the small-strain damping ratio. The approach relies on the application of an attenuation-specific wavefield conversion and frequency-domain beamforming. Numerical simulations are employed to verify the proposed approach and inform best practices for its application. Finally, the practical efficacy of the proposed approach is showcased through its application to field data collected at a deep, soft soil site in Logan, Utah, USA, where phase velocity and attenuation coefficients are extracted from surface wave data and then simultaneously inverted to develop deep shear wave velocity and damping ratio profiles.","sentences":["The small-strain damping ratio plays a crucial role in assessing the response of soil deposits to earthquake-induced ground motions and general dynamic loading.","The damping ratio can theoretically be inverted for after extracting frequency-dependent Rayleigh wave attenuation coefficients from wavefields collected during surface wave testing.","However, determining reliable estimates of in-situ attenuation coefficients is much more challenging than achieving robust phase velocity dispersion data, which are commonly measured using both active-source and ambient-wavefield surface wave methods.","This paper introduces a new methodology for estimating frequency-dependent attenuation coefficients through the analysis of ambient noise wavefield data recorded by two-dimensional (2D) arrays of surface seismic sensors for the subsequent evaluation of the small-strain damping ratio.","The approach relies on the application of an attenuation-specific wavefield conversion and frequency-domain beamforming.","Numerical simulations are employed to verify the proposed approach and inform best practices for its application.","Finally, the practical efficacy of the proposed approach is showcased through its application to field data collected at a deep, soft soil site in Logan, Utah, USA, where phase velocity and attenuation coefficients are extracted from surface wave data and then simultaneously inverted to develop deep shear wave velocity and damping ratio profiles."],"url":"http://arxiv.org/abs/2404.06650v1","category":"physics.geo-ph"}
{"created":"2024-04-09 22:55:06","title":"From Protoscience to Epistemic Monoculture: How Benchmarking Set the Stage for the Deep Learning Revolution","abstract":"Over the past decade, AI research has focused heavily on building ever-larger deep learning models. This approach has simultaneously unlocked incredible achievements in science and technology, and hindered AI from overcoming long-standing limitations with respect to explainability, ethical harms, and environmental efficiency. Drawing on qualitative interviews and computational analyses, our three-part history of AI research traces the creation of this \"epistemic monoculture\" back to a radical reconceptualization of scientific progress that began in the late 1980s. In the first era of AI research (1950s-late 1980s), researchers and patrons approached AI as a \"basic\" science that would advance through autonomous exploration and organic assessments of progress (e.g., peer-review, theoretical consensus). The failure of this approach led to a retrenchment of funding in the 1980s. Amid this \"AI Winter,\" an intervention by the U.S. government reoriented the field towards measurable progress on tasks of military and commercial interest. A new evaluation system called \"benchmarking\" provided an objective way to quantify progress on tasks by focusing exclusively on increasing predictive accuracy on example datasets. Distilling science down to verifiable metrics clarified the roles of scientists, allowed the field to rapidly integrate talent, and provided clear signals of significance and progress. But history has also revealed a tradeoff to this streamlined approach to science: the consolidation around external interests and inherent conservatism of benchmarking has disincentivized exploration beyond scaling monoculture. In the discussion, we explain how AI's monoculture offers a compelling challenge to the belief that basic, exploration-driven research is needed for scientific progress. Implications for the spread of AI monoculture to other sciences in the era of generative AI are also discussed.","sentences":["Over the past decade, AI research has focused heavily on building ever-larger deep learning models.","This approach has simultaneously unlocked incredible achievements in science and technology, and hindered AI from overcoming long-standing limitations with respect to explainability, ethical harms, and environmental efficiency.","Drawing on qualitative interviews and computational analyses, our three-part history of AI research traces the creation of this \"epistemic monoculture\" back to a radical reconceptualization of scientific progress that began in the late 1980s.","In the first era of AI research (1950s-late 1980s), researchers and patrons approached AI as a \"basic\" science that would advance through autonomous exploration and organic assessments of progress (e.g., peer-review, theoretical consensus).","The failure of this approach led to a retrenchment of funding in the 1980s.","Amid this \"AI Winter,\" an intervention by the U.S. government reoriented the field towards measurable progress on tasks of military and commercial interest.","A new evaluation system called \"benchmarking\" provided an objective way to quantify progress on tasks by focusing exclusively on increasing predictive accuracy on example datasets.","Distilling science down to verifiable metrics clarified the roles of scientists, allowed the field to rapidly integrate talent, and provided clear signals of significance and progress.","But history has also revealed a tradeoff to this streamlined approach to science: the consolidation around external interests and inherent conservatism of benchmarking has disincentivized exploration beyond scaling monoculture.","In the discussion, we explain how AI's monoculture offers a compelling challenge to the belief that basic, exploration-driven research is needed for scientific progress.","Implications for the spread of AI monoculture to other sciences in the era of generative AI are also discussed."],"url":"http://arxiv.org/abs/2404.06647v2","category":"cs.CY"}
{"created":"2024-04-09 22:47:25","title":"GenCHiP: Generating Robot Policy Code for High-Precision and Contact-Rich Manipulation Tasks","abstract":"Large Language Models (LLMs) have been successful at generating robot policy code, but so far these results have been limited to high-level tasks that do not require precise movement. It is an open question how well such approaches work for tasks that require reasoning over contact forces and working within tight success tolerances. We find that, with the right action space, LLMs are capable of successfully generating policies for a variety of contact-rich and high-precision manipulation tasks, even under noisy conditions, such as perceptual errors or grasping inaccuracies. Specifically, we reparameterize the action space to include compliance with constraints on the interaction forces and stiffnesses involved in reaching a target pose. We validate this approach on subtasks derived from the Functional Manipulation Benchmark (FMB) and NIST Task Board Benchmarks. Exposing this action space alongside methods for estimating object poses improves policy generation with an LLM by greater than 3x and 4x when compared to non-compliant action spaces","sentences":["Large Language Models (LLMs) have been successful at generating robot policy code, but so far these results have been limited to high-level tasks that do not require precise movement.","It is an open question how well such approaches work for tasks that require reasoning over contact forces and working within tight success tolerances.","We find that, with the right action space, LLMs are capable of successfully generating policies for a variety of contact-rich and high-precision manipulation tasks, even under noisy conditions, such as perceptual errors or grasping inaccuracies.","Specifically, we reparameterize the action space to include compliance with constraints on the interaction forces and stiffnesses involved in reaching a target pose.","We validate this approach on subtasks derived from the Functional Manipulation Benchmark (FMB) and NIST Task Board Benchmarks.","Exposing this action space alongside methods for estimating object poses improves policy generation with an LLM by greater than 3x and 4x when compared to non-compliant action spaces"],"url":"http://arxiv.org/abs/2404.06645v1","category":"cs.RO"}
{"created":"2024-04-09 22:38:13","title":"Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language?","abstract":"Evaluating Large Language Models (LLMs) is challenging due to their generative nature, necessitating precise evaluation methodologies. Additionally, non-English LLM evaluation lags behind English, resulting in the absence or weakness of LLMs for many languages. In response to this necessity, we introduce Khayyam Challenge (also known as PersianMMLU), a meticulously curated collection comprising 20,192 four-choice questions sourced from 38 diverse tasks extracted from Persian examinations, spanning a wide spectrum of subjects, complexities, and ages. The primary objective of the Khayyam Challenge is to facilitate the rigorous evaluation of LLMs that support the Persian language. Distinctive features of the Khayyam Challenge are (i) its comprehensive coverage of various topics, including literary comprehension, mathematics, sciences, logic, intelligence testing, etc., aimed at assessing different facets of LLMs such as language comprehension, reasoning, and information retrieval across various educational stages, from lower primary school to upper secondary school (ii) its inclusion of rich metadata such as human response rates, difficulty levels, and descriptive answers (iii) its utilization of new data to avoid data contamination issues prevalent in existing frameworks (iv) its use of original, non-translated data tailored for Persian speakers, ensuring the framework is free from translation challenges and errors while encompassing cultural nuances (v) its inherent scalability for future data updates and evaluations without requiring special human effort. Previous works lacked an evaluation framework that combined all of these features into a single comprehensive benchmark. Furthermore, we evaluate a wide range of existing LLMs that support the Persian language, with statistical analyses and interpretations of their outputs.","sentences":["Evaluating Large Language Models (LLMs) is challenging due to their generative nature, necessitating precise evaluation methodologies.","Additionally, non-English LLM evaluation lags behind English, resulting in the absence or weakness of LLMs for many languages.","In response to this necessity, we introduce Khayyam Challenge (also known as PersianMMLU), a meticulously curated collection comprising 20,192 four-choice questions sourced from 38 diverse tasks extracted from Persian examinations, spanning a wide spectrum of subjects, complexities, and ages.","The primary objective of the Khayyam Challenge is to facilitate the rigorous evaluation of LLMs that support the Persian language.","Distinctive features of the Khayyam Challenge are (i) its comprehensive coverage of various topics, including literary comprehension, mathematics, sciences, logic, intelligence testing, etc., aimed at assessing different facets of LLMs such as language comprehension, reasoning, and information retrieval across various educational stages, from lower primary school to upper secondary school (ii) its inclusion of rich metadata such as human response rates, difficulty levels, and descriptive answers (iii) its utilization of new data to avoid data contamination issues prevalent in existing frameworks (iv) its use of original, non-translated data tailored for Persian speakers, ensuring the framework is free from translation challenges and errors while encompassing cultural nuances (v) its inherent scalability for future data updates and evaluations without requiring special human effort.","Previous works lacked an evaluation framework that combined all of these features into a single comprehensive benchmark.","Furthermore, we evaluate a wide range of existing LLMs that support the Persian language, with statistical analyses and interpretations of their outputs."],"url":"http://arxiv.org/abs/2404.06644v1","category":"cs.CL"}
{"created":"2024-04-09 22:31:10","title":"Federated learning model for predicting major postoperative complications","abstract":"Background: The accurate prediction of postoperative complication risk using Electronic Health Records (EHR) and artificial intelligence shows great potential. Training a robust artificial intelligence model typically requires large-scale and diverse datasets. In reality, collecting medical data often encounters challenges surrounding privacy protection. Methods: This retrospective cohort study includes adult patients who were admitted to UFH Gainesville (GNV) (n = 79,850) and Jacksonville (JAX) (n = 28,636) for any type of inpatient surgical procedure. Using perioperative and intraoperative features, we developed federated learning models to predict nine major postoperative complications (i.e., prolonged intensive care unit stay and mechanical ventilation). We compared federated learning models with local learning models trained on a single site and central learning models trained on pooled dataset from two centers. Results: Our federated learning models achieved the area under the receiver operating characteristics curve (AUROC) values ranged from 0.81 for wound complications to 0.92 for prolonged ICU stay at UFH GNV center. At UFH JAX center, these values ranged from 0.73-0.74 for wound complications to 0.92-0.93 for hospital mortality. Federated learning models achieved comparable AUROC performance to central learning models, except for prolonged ICU stay, where the performance of federated learning models was slightly higher than central learning models at UFH GNV center, but slightly lower at UFH JAX center. In addition, our federated learning model obtained comparable performance to the best local learning model at each center, demonstrating strong generalizability. Conclusion: Federated learning is shown to be a useful tool to train robust and generalizable models from large scale data across multiple institutions where data protection barriers are high.","sentences":["Background: The accurate prediction of postoperative complication risk using Electronic Health Records (EHR) and artificial intelligence shows great potential.","Training a robust artificial intelligence model typically requires large-scale and diverse datasets.","In reality, collecting medical data often encounters challenges surrounding privacy protection.","Methods: This retrospective cohort study includes adult patients who were admitted to UFH Gainesville (GNV) (n = 79,850) and Jacksonville (JAX) (n = 28,636) for any type of inpatient surgical procedure.","Using perioperative and intraoperative features, we developed federated learning models to predict nine major postoperative complications (i.e., prolonged intensive care unit stay and mechanical ventilation).","We compared federated learning models with local learning models trained on a single site and central learning models trained on pooled dataset from two centers.","Results: Our federated learning models achieved the area under the receiver operating characteristics curve (AUROC) values ranged from 0.81 for wound complications to 0.92 for prolonged ICU stay at UFH GNV center.","At UFH JAX center, these values ranged from 0.73-0.74 for wound complications to 0.92-0.93 for hospital mortality.","Federated learning models achieved comparable AUROC performance to central learning models, except for prolonged ICU stay, where the performance of federated learning models was slightly higher than central learning models at UFH GNV center, but slightly lower at UFH JAX center.","In addition, our federated learning model obtained comparable performance to the best local learning model at each center, demonstrating strong generalizability.","Conclusion:","Federated learning is shown to be a useful tool to train robust and generalizable models from large scale data across multiple institutions where data protection barriers are high."],"url":"http://arxiv.org/abs/2404.06641v1","category":"cs.LG"}
{"created":"2024-04-09 21:53:53","title":"Evolving Loss Functions for Specific Image Augmentation Techniques","abstract":"Previous work in Neural Loss Function Search (NLFS) has shown a lack of correlation between smaller surrogate functions and large convolutional neural networks with massive regularization. We expand upon this research by revealing another disparity that exists, correlation between different types of image augmentation techniques. We show that different loss functions can perform well on certain image augmentation techniques, while performing poorly on others. We exploit this disparity by performing an evolutionary search on five types of image augmentation techniques in the hopes of finding image augmentation specific loss functions. The best loss functions from each evolution were then taken and transferred to WideResNet-28-10 on CIFAR-10 and CIFAR-100 across each of the five image augmentation techniques. The best from that were then taken and evaluated by fine-tuning EfficientNetV2Small on the CARS, Oxford-Flowers, and Caltech datasets across each of the five image augmentation techniques. Multiple loss functions were found that outperformed cross-entropy across multiple experiments. In the end, we found a single loss function, which we called the inverse bessel logarithm loss, that was able to outperform cross-entropy across the majority of experiments.","sentences":["Previous work in Neural Loss Function Search (NLFS) has shown a lack of correlation between smaller surrogate functions and large convolutional neural networks with massive regularization.","We expand upon this research by revealing another disparity that exists, correlation between different types of image augmentation techniques.","We show that different loss functions can perform well on certain image augmentation techniques, while performing poorly on others.","We exploit this disparity by performing an evolutionary search on five types of image augmentation techniques in the hopes of finding image augmentation specific loss functions.","The best loss functions from each evolution were then taken and transferred to WideResNet-28-10 on CIFAR-10 and CIFAR-100 across each of the five image augmentation techniques.","The best from that were then taken and evaluated by fine-tuning EfficientNetV2Small on the CARS, Oxford-Flowers, and Caltech datasets across each of the five image augmentation techniques.","Multiple loss functions were found that outperformed cross-entropy across multiple experiments.","In the end, we found a single loss function, which we called the inverse bessel logarithm loss, that was able to outperform cross-entropy across the majority of experiments."],"url":"http://arxiv.org/abs/2404.06633v1","category":"cs.NE"}
{"created":"2024-04-09 21:46:14","title":"Counting Objects in a Robotic Hand","abstract":"A robot performing multi-object grasping needs to sense the number of objects in the hand after grasping. The count plays an important role in determining the robot's next move and the outcome and efficiency of the whole pick-place process. This paper presents a data-driven contrastive learning-based counting classifier with a modified loss function as a simple and effective approach for object counting despite significant occlusion challenges caused by robotic fingers and objects. The model was validated against other models with three different common shapes (spheres, cylinders, and cubes) in simulation and in a real setup. The proposed contrastive learning-based counting approach achieved above 96\\% accuracy for all three objects in the real setup.","sentences":["A robot performing multi-object grasping needs to sense the number of objects in the hand after grasping.","The count plays an important role in determining the robot's next move and the outcome and efficiency of the whole pick-place process.","This paper presents a data-driven contrastive learning-based counting classifier with a modified loss function as a simple and effective approach for object counting despite significant occlusion challenges caused by robotic fingers and objects.","The model was validated against other models with three different common shapes (spheres, cylinders, and cubes) in simulation and in a real setup.","The proposed contrastive learning-based counting approach achieved above 96\\% accuracy for all three objects in the real setup."],"url":"http://arxiv.org/abs/2404.06631v1","category":"cs.RO"}
{"created":"2024-04-09 21:22:09","title":"Why is soccer so popular: Understanding underdog achievement and randomness in team ball sports","abstract":"In this paper, we examine team ball sports to investigate how the likelihood of weaker teams winning against stronger ones, referred to as underdog achievement, is influenced by inherent randomness factors that affect match outcomes in such sports. To address our research question, we collected data on match scores and computed corresponding team rankings from major international competitions for 12 popular team ball sports: basketball, cricket, field hockey, futsal, handball, ice hockey, lacrosse, roller hockey, rugby, soccer, volleyball, and water polo. Then, we developed an underdog achievement score to identify the sports with the highest occurrences of weaker teams prevailing over stronger ones, and we designed a randomness model consisting of factors that contribute to unexpected match outcomes within each sport. Our findings indicate that soccer is among the sports in which a weaker team is most likely to win. Through principal component analysis (PCA) and correlation analysis, we demonstrate that our randomness model can explain such a phenomenon, showing that the underdog achievement can be attributed to numerous factors that can randomly influence match outcomes.","sentences":["In this paper, we examine team ball sports to investigate how the likelihood of weaker teams winning against stronger ones, referred to as underdog achievement, is influenced by inherent randomness factors that affect match outcomes in such sports.","To address our research question, we collected data on match scores and computed corresponding team rankings from major international competitions for 12 popular team ball sports: basketball, cricket, field hockey, futsal, handball, ice hockey, lacrosse, roller hockey, rugby, soccer, volleyball, and water polo.","Then, we developed an underdog achievement score to identify the sports with the highest occurrences of weaker teams prevailing over stronger ones, and we designed a randomness model consisting of factors that contribute to unexpected match outcomes within each sport.","Our findings indicate that soccer is among the sports in which a weaker team is most likely to win.","Through principal component analysis (PCA) and correlation analysis, we demonstrate that our randomness model can explain such a phenomenon, showing that the underdog achievement can be attributed to numerous factors that can randomly influence match outcomes."],"url":"http://arxiv.org/abs/2404.06626v1","category":"stat.AP"}
{"created":"2024-04-09 21:07:41","title":"No evidence for anisotropy in galaxy spin directions","abstract":"Modern cosmology rests on the cosmological principle, that on large enough scales the Universe is both homogeneous and isotropic. A corollary is that galaxies' spin vectors should be isotropically distributed on the sky. This has been challenged by multiple authors for over a decade, with claims to have detected a statistically significant dipole pattern of spins. We collect all publicly available datasets with spin classifications (binary clockwise/anticlockwise), and analyse them for large-angle anisotropies ($\\ell \\le 2$). We perform each inference in both a Bayesian and frequentist fashion, the former establishing posterior probabilities on the multipole parameters and the latter calculating $p$-values for rejection of the null hypothesis of isotropy (i.e. no power at $\\ell>0$). All analysis indicate consistency with isotropy to within $3\\sigma$. We isolate the differences with contrary claims in the ad hoc or biased statistics that they employ.","sentences":["Modern cosmology rests on the cosmological principle",", that on large enough scales the Universe is both homogeneous and isotropic.","A corollary is that galaxies' spin vectors should be isotropically distributed on the sky.","This has been challenged by multiple authors for over a decade, with claims to have detected a statistically significant dipole pattern of spins.","We collect all publicly available datasets with spin classifications (binary clockwise/anticlockwise), and analyse them for large-angle anisotropies ($\\ell \\le 2$).","We perform each inference in both a Bayesian and frequentist fashion, the former establishing posterior probabilities on the multipole parameters and the latter calculating $p$-values for rejection of the null hypothesis of isotropy (i.e. no power at $\\ell>0$).","All analysis indicate consistency with isotropy to within $3\\sigma$. We isolate the differences with contrary claims in the ad hoc or biased statistics that they employ."],"url":"http://arxiv.org/abs/2404.06617v1","category":"astro-ph.GA"}
{"created":"2024-04-09 20:40:00","title":"GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation","abstract":"The Embodied AI community has made significant strides in visual navigation tasks, exploring targets from 3D coordinates, objects, language descriptions, and images. However, these navigation models often handle only a single input modality as the target. With the progress achieved so far, it is time to move towards universal navigation models capable of handling various goal types, enabling more effective user interaction with robots. To facilitate this goal, we propose GOAT-Bench, a benchmark for the universal navigation task referred to as GO to AnyThing (GOAT). In this task, the agent is directed to navigate to a sequence of targets specified by the category name, language description, or image in an open-vocabulary fashion. We benchmark monolithic RL and modular methods on the GOAT task, analyzing their performance across modalities, the role of explicit and implicit scene memories, their robustness to noise in goal specifications, and the impact of memory in lifelong scenarios.","sentences":["The Embodied AI community has made significant strides in visual navigation tasks, exploring targets from 3D coordinates, objects, language descriptions, and images.","However, these navigation models often handle only a single input modality as the target.","With the progress achieved so far, it is time to move towards universal navigation models capable of handling various goal types, enabling more effective user interaction with robots.","To facilitate this goal, we propose GOAT-Bench, a benchmark for the universal navigation task referred to as GO to AnyThing (GOAT).","In this task, the agent is directed to navigate to a sequence of targets specified by the category name, language description, or image in an open-vocabulary fashion.","We benchmark monolithic RL and modular methods on the GOAT task, analyzing their performance across modalities, the role of explicit and implicit scene memories, their robustness to noise in goal specifications, and the impact of memory in lifelong scenarios."],"url":"http://arxiv.org/abs/2404.06609v1","category":"cs.AI"}
{"created":"2024-04-09 20:12:44","title":"Resource Management in RIS-Assisted Rate Splitting Multiple Access for Next Generation (xG) Wireless Communications: Models, State-of-the-Art, and Future Directions","abstract":"Next generation wireless networks require more stringent performance levels.   New technologies such as Reconfigurable intelligent surfaces (RISs) and rate-splitting multiple access (RSMA) are candidates for meeting some of the performance requirements, including higher user rates at reduced costs.   RSMA provides a new way of mixing the messages of multiple users, and the RIS provides a controllable wireless environment.   This paper provides a comprehensive survey on the various aspects of the synergy between reconfigurable intelligent surfaces (RISs) and rate splitting multiple access (RSMA) for next-generation (xG) wireless communications systems.   In particular, the paper studies more than 60 articles considering over 20 different system models where the RIS-aided RSMA system shows performance advantage (in terms of sum-rate or outage probability) over traditional RSMA models.   These models include reflective RIS, simultaneously transmitting and reflecting surfaces (STAR-RIS), as well as transmissive surfaces.   The state-of-the-art resource management methods for RIS-assisted RSMA communications employ traditional optimization techniques and/or machine learning techniques.   We outline major research challenges and multiple future research directions.","sentences":["Next generation wireless networks require more stringent performance levels.   ","New technologies such as Reconfigurable intelligent surfaces (RISs) and rate-splitting multiple access (RSMA) are candidates for meeting some of the performance requirements, including higher user rates at reduced costs.   ","RSMA provides a new way of mixing the messages of multiple users, and the RIS provides a controllable wireless environment.   ","This paper provides a comprehensive survey on the various aspects of the synergy between reconfigurable intelligent surfaces (RISs) and rate splitting multiple access (RSMA) for next-generation (xG) wireless communications systems.   ","In particular, the paper studies more than 60 articles considering over 20 different system models where the RIS-aided RSMA system shows performance advantage (in terms of sum-rate or outage probability) over traditional RSMA models.   ","These models include reflective RIS, simultaneously transmitting and reflecting surfaces (STAR-RIS), as well as transmissive surfaces.   ","The state-of-the-art resource management methods for RIS-assisted RSMA communications employ traditional optimization techniques and/or machine learning techniques.   ","We outline major research challenges and multiple future research directions."],"url":"http://arxiv.org/abs/2404.06604v1","category":"cs.NI"}
{"created":"2024-04-09 20:06:25","title":"FMDA-OT: Federated Multi-source Domain Adaptation Through Optimal Transport","abstract":"Multi-source Domain Adaptation (MDA) aims to adapt models trained on multiple labeled source domains to an unlabeled target domain. In this paper, we introduce our approach as a collaborative MDA framework, which comprises two adaptation phases. Firstly, we conduct domain adaptation for each source individually with the target, utilizing optimal transport. Then, in the second phase, which constitutes the final part of the framework, we design the architecture of centralized federated learning to collaborate the N models representing the N sources. This architecture offers the advantage of using the sources without accessing their data, thus resolving data privacy issues inherent in domain adaptation. Additionally, during this phase, the server guides and fine-tunes the adaptation using a small number of pseudo-labeled samples available in the target domain, referred to as the target validation subset of the dataset.","sentences":["Multi-source Domain Adaptation (MDA) aims to adapt models trained on multiple labeled source domains to an unlabeled target domain.","In this paper, we introduce our approach as a collaborative MDA framework, which comprises two adaptation phases.","Firstly, we conduct domain adaptation for each source individually with the target, utilizing optimal transport.","Then, in the second phase, which constitutes the final part of the framework, we design the architecture of centralized federated learning to collaborate the N models representing the N sources.","This architecture offers the advantage of using the sources without accessing their data, thus resolving data privacy issues inherent in domain adaptation.","Additionally, during this phase, the server guides and fine-tunes the adaptation using a small number of pseudo-labeled samples available in the target domain, referred to as the target validation subset of the dataset."],"url":"http://arxiv.org/abs/2404.06599v1","category":"cs.LG"}
{"created":"2024-04-09 19:49:01","title":"Spatially Optimized Compact Deep Metric Learning Model for Similarity Search","abstract":"Spatial optimization is often overlooked in many computer vision tasks. Filters should be able to recognize the features of an object regardless of where it is in the image. Similarity search is a crucial task where spatial features decide an important output. The capacity of convolution to capture visual patterns across various locations is limited. In contrast to convolution, the involution kernel is dynamically created at each pixel based on the pixel value and parameters that have been learned. This study demonstrates that utilizing a single layer of involution feature extractor alongside a compact convolution model significantly enhances the performance of similarity search. Additionally, we improve predictions by using the GELU activation function rather than the ReLU. The negligible amount of weight parameters in involution with a compact model with better performance makes the model very useful in real-world implementations. Our proposed model is below 1 megabyte in size. We have experimented with our proposed methodology and other models on CIFAR-10, FashionMNIST, and MNIST datasets. Our proposed method outperforms across all three datasets.","sentences":["Spatial optimization is often overlooked in many computer vision tasks.","Filters should be able to recognize the features of an object regardless of where it is in the image.","Similarity search is a crucial task where spatial features decide an important output.","The capacity of convolution to capture visual patterns across various locations is limited.","In contrast to convolution, the involution kernel is dynamically created at each pixel based on the pixel value and parameters that have been learned.","This study demonstrates that utilizing a single layer of involution feature extractor alongside a compact convolution model significantly enhances the performance of similarity search.","Additionally, we improve predictions by using the GELU activation function rather than the ReLU.","The negligible amount of weight parameters in involution with a compact model with better performance makes the model very useful in real-world implementations.","Our proposed model is below 1 megabyte in size.","We have experimented with our proposed methodology and other models on CIFAR-10, FashionMNIST, and MNIST datasets.","Our proposed method outperforms across all three datasets."],"url":"http://arxiv.org/abs/2404.06593v1","category":"cs.CV"}
{"created":"2024-04-09 19:02:12","title":"Less is More for Improving Automatic Evaluation of Factual Consistency","abstract":"Assessing the factual consistency of automatically generated texts in relation to source context is crucial for developing reliable natural language generation applications. Recent literature proposes AlignScore which uses a unified alignment model to evaluate factual consistency and substantially outperforms previous methods across many benchmark tasks. In this paper, we take a closer look of datasets used in AlignScore and uncover an unexpected finding: utilizing a smaller number of data points can actually improve performance. We process the original AlignScore training dataset to remove noise, augment with robustness-enhanced samples, and utilize a subset comprising 10\\% of the data to train an improved factual consistency evaluation model, we call LIM-RA (Less Is More for Robust AlignScore). LIM-RA demonstrates superior performance, consistently outperforming AlignScore and other strong baselines like ChatGPT across four benchmarks (two utilizing traditional natural language generation datasets and two focused on large language model outputs). Our experiments show that LIM-RA achieves the highest score on 24 of the 33 test datasets, while staying competitive on the rest, establishing the new state-of-the-art benchmarks.","sentences":["Assessing the factual consistency of automatically generated texts in relation to source context is crucial for developing reliable natural language generation applications.","Recent literature proposes AlignScore which uses a unified alignment model to evaluate factual consistency and substantially outperforms previous methods across many benchmark tasks.","In this paper, we take a closer look of datasets used in AlignScore and uncover an unexpected finding: utilizing a smaller number of data points can actually improve performance.","We process the original AlignScore training dataset to remove noise, augment with robustness-enhanced samples, and utilize a subset comprising 10\\% of the data to train an improved factual consistency evaluation model, we call LIM-RA (Less Is More for Robust AlignScore).","LIM-RA demonstrates superior performance, consistently outperforming AlignScore and other strong baselines like ChatGPT across four benchmarks (two utilizing traditional natural language generation datasets and two focused on large language model outputs).","Our experiments show that LIM-RA achieves the highest score on 24 of the 33 test datasets, while staying competitive on the rest, establishing the new state-of-the-art benchmarks."],"url":"http://arxiv.org/abs/2404.06579v1","category":"cs.CL"}
{"created":"2024-04-09 18:46:46","title":"Building A Knowledge Graph to Enrich ChatGPT Responses in Manufacturing Service Discovery","abstract":"Sourcing and identification of new manufacturing partners is crucial for manufacturing system integrators to enhance agility and reduce risk through supply chain diversification in the global economy. The advent of advanced large language models has captured significant interest, due to their ability to generate comprehensive and articulate responses across a wide range of knowledge domains. However, the system often falls short in accuracy and completeness when responding to domain-specific inquiries, particularly in areas like manufacturing service discovery. This research explores the potential of leveraging Knowledge Graphs in conjunction with ChatGPT to streamline the process for prospective clients in identifying small manufacturing enterprises. In this study, we propose a method that integrates bottom-up ontology with advanced machine learning models to develop a Manufacturing Service Knowledge Graph from an array of structured and unstructured data sources, including the digital footprints of small-scale manufacturers throughout North America. The Knowledge Graph and the learned graph embedding vectors are leveraged to tackle intricate queries within the digital supply chain network, responding with enhanced reliability and greater interpretability. The approach highlighted is scalable to millions of entities that can be distributed to form a global Manufacturing Service Knowledge Network Graph that can potentially interconnect multiple types of Knowledge Graphs that span industry sectors, geopolitical boundaries, and business domains. The dataset developed for this study, now publicly accessible, encompasses more than 13,000 manufacturers' weblinks, manufacturing services, certifications, and location entity types.","sentences":["Sourcing and identification of new manufacturing partners is crucial for manufacturing system integrators to enhance agility and reduce risk through supply chain diversification in the global economy.","The advent of advanced large language models has captured significant interest, due to their ability to generate comprehensive and articulate responses across a wide range of knowledge domains.","However, the system often falls short in accuracy and completeness when responding to domain-specific inquiries, particularly in areas like manufacturing service discovery.","This research explores the potential of leveraging Knowledge Graphs in conjunction with ChatGPT to streamline the process for prospective clients in identifying small manufacturing enterprises.","In this study, we propose a method that integrates bottom-up ontology with advanced machine learning models to develop a Manufacturing Service Knowledge Graph from an array of structured and unstructured data sources, including the digital footprints of small-scale manufacturers throughout North America.","The Knowledge Graph and the learned graph embedding vectors are leveraged to tackle intricate queries within the digital supply chain network, responding with enhanced reliability and greater interpretability.","The approach highlighted is scalable to millions of entities that can be distributed to form a global Manufacturing Service Knowledge Network Graph that can potentially interconnect multiple types of Knowledge Graphs that span industry sectors, geopolitical boundaries, and business domains.","The dataset developed for this study, now publicly accessible, encompasses more than 13,000 manufacturers' weblinks, manufacturing services, certifications, and location entity types."],"url":"http://arxiv.org/abs/2404.06571v1","category":"cs.AI"}
{"created":"2024-04-09 18:25:21","title":"Learning Strategies For Successful Crowd Navigation","abstract":"Teaching autonomous mobile robots to successfully navigate human crowds is a challenging task. Not only does it require planning, but it requires maintaining social norms which may differ from one context to another. Here we focus on crowd navigation, using a neural network to learn specific strategies in-situ with a robot. This allows us to take into account human behavior and reactions toward a real robot as well as learn strategies that are specific to various scenarios in that context. A CNN takes a top-down image of the scene as input and outputs the next action for the robot to take in terms of speed and angle. Here we present the method, experimental results, and quantitatively evaluate our approach.","sentences":["Teaching autonomous mobile robots to successfully navigate human crowds is a challenging task.","Not only does it require planning, but it requires maintaining social norms which may differ from one context to another.","Here we focus on crowd navigation, using a neural network to learn specific strategies in-situ with a robot.","This allows us to take into account human behavior and reactions toward a real robot as well as learn strategies that are specific to various scenarios in that context.","A CNN takes a top-down image of the scene as input and outputs the next action for the robot to take in terms of speed and angle.","Here we present the method, experimental results, and quantitatively evaluate our approach."],"url":"http://arxiv.org/abs/2404.06561v1","category":"cs.RO"}
{"created":"2024-04-09 18:00:25","title":"Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation","abstract":"Open-vocabulary semantic segmentation aims at segmenting arbitrary categories expressed in textual form. Previous works have trained over large amounts of image-caption pairs to enforce pixel-level multimodal alignments. However, captions provide global information about the semantics of a given image but lack direct localization of individual concepts. Further, training on large-scale datasets inevitably brings significant computational costs. In this paper, we propose FreeDA, a training-free diffusion-augmented method for open-vocabulary semantic segmentation, which leverages the ability of diffusion models to visually localize generated concepts and local-global similarities to match class-agnostic regions with semantic classes. Our approach involves an offline stage in which textual-visual reference embeddings are collected, starting from a large set of captions and leveraging visual and semantic contexts. At test time, these are queried to support the visual matching process, which is carried out by jointly considering class-agnostic regions and global semantic similarities. Extensive analyses demonstrate that FreeDA achieves state-of-the-art performance on five datasets, surpassing previous methods by more than 7.0 average points in terms of mIoU and without requiring any training.","sentences":["Open-vocabulary semantic segmentation aims at segmenting arbitrary categories expressed in textual form.","Previous works have trained over large amounts of image-caption pairs to enforce pixel-level multimodal alignments.","However, captions provide global information about the semantics of a given image but lack direct localization of individual concepts.","Further, training on large-scale datasets inevitably brings significant computational costs.","In this paper, we propose FreeDA, a training-free diffusion-augmented method for open-vocabulary semantic segmentation, which leverages the ability of diffusion models to visually localize generated concepts and local-global similarities to match class-agnostic regions with semantic classes.","Our approach involves an offline stage in which textual-visual reference embeddings are collected, starting from a large set of captions and leveraging visual and semantic contexts.","At test time, these are queried to support the visual matching process, which is carried out by jointly considering class-agnostic regions and global semantic similarities.","Extensive analyses demonstrate that FreeDA achieves state-of-the-art performance on five datasets, surpassing previous methods by more than 7.0 average points in terms of mIoU and without requiring any training."],"url":"http://arxiv.org/abs/2404.06542v1","category":"cs.CV"}
{"created":"2024-04-09 17:59:31","title":"MoReVQA: Exploring Modular Reasoning Models for Video Question Answering","abstract":"This paper addresses the task of video question answering (videoQA) via a decomposed multi-stage, modular reasoning framework. Previous modular methods have shown promise with a single planning stage ungrounded in visual content. However, through a simple and effective baseline, we find that such systems can lead to brittle behavior in practice for challenging videoQA settings. Thus, unlike traditional single-stage planning methods, we propose a multi-stage system consisting of an event parser, a grounding stage, and a final reasoning stage in conjunction with an external memory. All stages are training-free, and performed using few-shot prompting of large models, creating interpretable intermediate outputs at each stage. By decomposing the underlying planning and task complexity, our method, MoReVQA, improves over prior work on standard videoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) with state-of-the-art results, and extensions to related tasks (grounded videoQA, paragraph captioning).","sentences":["This paper addresses the task of video question answering (videoQA) via a decomposed multi-stage, modular reasoning framework.","Previous modular methods have shown promise with a single planning stage ungrounded in visual content.","However, through a simple and effective baseline, we find that such systems can lead to brittle behavior in practice for challenging videoQA settings.","Thus, unlike traditional single-stage planning methods, we propose a multi-stage system consisting of an event parser, a grounding stage, and a final reasoning stage in conjunction with an external memory.","All stages are training-free, and performed using few-shot prompting of large models, creating interpretable intermediate outputs at each stage.","By decomposing the underlying planning and task complexity, our method, MoReVQA, improves over prior work on standard videoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) with state-of-the-art results, and extensions to related tasks (grounded videoQA, paragraph captioning)."],"url":"http://arxiv.org/abs/2404.06511v1","category":"cs.CV"}
{"created":"2024-04-09 17:59:04","title":"Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?","abstract":"Enhancing semantic grounding abilities in Vision-Language Models (VLMs) often involves collecting domain-specific training data, refining the network architectures, or modifying the training recipes. In this work, we venture into an orthogonal direction and explore whether VLMs can improve their semantic grounding by \"receiving\" feedback, without requiring in-domain data, fine-tuning, or modifications to the network architectures. We systematically analyze this hypothesis using a feedback mechanism composed of a binary signal. We find that if prompted appropriately, VLMs can utilize feedback both in a single step and iteratively, showcasing the potential of feedback as an alternative technique to improve grounding in internet-scale VLMs. Furthermore, VLMs, like LLMs, struggle to self-correct errors out-of-the-box. However, we find that this issue can be mitigated via a binary verification mechanism. Finally, we explore the potential and limitations of amalgamating these findings and applying them iteratively to automatically enhance VLMs' grounding performance, showing grounding accuracy consistently improves using automated feedback across all models in all settings investigated. Overall, our iterative framework improves semantic grounding in VLMs by more than 15 accuracy points under noise-free feedback and up to 5 accuracy points under a simple automated binary verification mechanism. The project website is hosted at https://andrewliao11.github.io/vlms_feedback","sentences":["Enhancing semantic grounding abilities in Vision-Language Models (VLMs) often involves collecting domain-specific training data, refining the network architectures, or modifying the training recipes.","In this work, we venture into an orthogonal direction and explore whether VLMs can improve their semantic grounding by \"receiving\" feedback, without requiring in-domain data, fine-tuning, or modifications to the network architectures.","We systematically analyze this hypothesis using a feedback mechanism composed of a binary signal.","We find that if prompted appropriately, VLMs can utilize feedback both in a single step and iteratively, showcasing the potential of feedback as an alternative technique to improve grounding in internet-scale VLMs.","Furthermore, VLMs, like LLMs, struggle to self-correct errors out-of-the-box.","However, we find that this issue can be mitigated via a binary verification mechanism.","Finally, we explore the potential and limitations of amalgamating these findings and applying them iteratively to automatically enhance VLMs' grounding performance, showing grounding accuracy consistently improves using automated feedback across all models in all settings investigated.","Overall, our iterative framework improves semantic grounding in VLMs by more than 15 accuracy points under noise-free feedback and up to 5 accuracy points under a simple automated binary verification mechanism.","The project website is hosted at https://andrewliao11.github.io/vlms_feedback"],"url":"http://arxiv.org/abs/2404.06510v1","category":"cs.CV"}
{"created":"2024-04-09 17:45:25","title":"Graph Reinforcement Learning for Combinatorial Optimization: A Survey and Unifying Perspective","abstract":"Graphs are a natural representation for systems based on relations between connected entities. Combinatorial optimization problems, which arise when considering an objective function related to a process of interest on discrete structures, are often challenging due to the rapid growth of the solution space. The trial-and-error paradigm of Reinforcement Learning has recently emerged as a promising alternative to traditional methods, such as exact algorithms and (meta)heuristics, for discovering better decision-making strategies in a variety of disciplines including chemistry, computer science, and statistics. Despite the fact that they arose in markedly different fields, these techniques share significant commonalities. Therefore, we set out to synthesize this work in a unifying perspective that we term Graph Reinforcement Learning, interpreting it as a constructive decision-making method for graph problems. After covering the relevant technical background, we review works along the dividing line of whether the goal is to optimize graph structure given a process of interest, or to optimize the outcome of the process itself under fixed graph structure. Finally, we discuss the common challenges facing the field and open research questions. In contrast with other surveys, the present work focuses on non-canonical graph problems for which performant algorithms are typically not known and Reinforcement Learning is able to provide efficient and effective solutions.","sentences":["Graphs are a natural representation for systems based on relations between connected entities.","Combinatorial optimization problems, which arise when considering an objective function related to a process of interest on discrete structures, are often challenging due to the rapid growth of the solution space.","The trial-and-error paradigm of Reinforcement Learning has recently emerged as a promising alternative to traditional methods, such as exact algorithms and (meta)heuristics, for discovering better decision-making strategies in a variety of disciplines including chemistry, computer science, and statistics.","Despite the fact that they arose in markedly different fields, these techniques share significant commonalities.","Therefore, we set out to synthesize this work in a unifying perspective that we term Graph Reinforcement Learning, interpreting it as a constructive decision-making method for graph problems.","After covering the relevant technical background, we review works along the dividing line of whether the goal is to optimize graph structure given a process of interest, or to optimize the outcome of the process itself under fixed graph structure.","Finally, we discuss the common challenges facing the field and open research questions.","In contrast with other surveys, the present work focuses on non-canonical graph problems for which performant algorithms are typically not known and Reinforcement Learning is able to provide efficient and effective solutions."],"url":"http://arxiv.org/abs/2404.06492v1","category":"cs.LG"}
{"created":"2024-04-09 17:42:59","title":"Pitfalls of Conversational LLMs on News Debiasing","abstract":"This paper addresses debiasing in news editing and evaluates the effectiveness of conversational Large Language Models in this task. We designed an evaluation checklist tailored to news editors' perspectives, obtained generated texts from three popular conversational models using a subset of a publicly available dataset in media bias, and evaluated the texts according to the designed checklist. Furthermore, we examined the models as evaluator for checking the quality of debiased model outputs. Our findings indicate that none of the LLMs are perfect in debiasing. Notably, some models, including ChatGPT, introduced unnecessary changes that may impact the author's style and create misinformation. Lastly, we show that the models do not perform as proficiently as domain experts in evaluating the quality of debiased outputs.","sentences":["This paper addresses debiasing in news editing and evaluates the effectiveness of conversational Large Language Models in this task.","We designed an evaluation checklist tailored to news editors' perspectives, obtained generated texts from three popular conversational models using a subset of a publicly available dataset in media bias, and evaluated the texts according to the designed checklist.","Furthermore, we examined the models as evaluator for checking the quality of debiased model outputs.","Our findings indicate that none of the LLMs are perfect in debiasing.","Notably, some models, including ChatGPT, introduced unnecessary changes that may impact the author's style and create misinformation.","Lastly, we show that the models do not perform as proficiently as domain experts in evaluating the quality of debiased outputs."],"url":"http://arxiv.org/abs/2404.06488v1","category":"cs.CL"}
{"created":"2024-04-09 17:35:11","title":"Public-private funding models in open source software development: A case study on scikit-learn","abstract":"Governments are increasingly allocating funding for open source software (OSS) development to address concerns related to software security, digital sovereignty, and national competitiveness in science and innovation, amongst others. While announcements of governmental funding are generally well-received by OSS developers, we still have a limited understanding of OSS developers evaluate the relative benefits and drawbacks of such funding compared to other types of funding. This paper explores this question through a case study on scikit-learn, a Python library for machine learning, whose funding model combines research grants, commercial sponsorship, community donations, and a 32 million euro grant from the France's artificial intelligence strategy. Through 25 interviews with scikit-learn's maintainers and funders, this study makes two key contributions to research and practice. First, the study illustrates how the maintainers have weaved public and private funding into their project to ensure the continued provision of scikit-learn as a digital public good, as well as the importance of diversified funding and governance protocols for funding to safeguard the community ethos of the project. Second, it offers practical recommendations to various stakeholders. For OSS developer communities, it illustrates the benefits of a diversified funding model in balancing the merits and drawbacks of different funding sources. For companies, it serves as a reminder that sponsoring developers or OSS projects can significantly support OSS maintainers, who often struggle with limited resources and towering workloads. For governments, it emphasises the importance of funding the maintenance of existing OSS in addition to or exclusively funding the development of new OSS libraries or features. The paper concludes with suggestions for future research directions.","sentences":["Governments are increasingly allocating funding for open source software (OSS) development to address concerns related to software security, digital sovereignty, and national competitiveness in science and innovation, amongst others.","While announcements of governmental funding are generally well-received by OSS developers, we still have a limited understanding of OSS developers evaluate the relative benefits and drawbacks of such funding compared to other types of funding.","This paper explores this question through a case study on scikit-learn, a Python library for machine learning, whose funding model combines research grants, commercial sponsorship, community donations, and a 32 million euro grant from the France's artificial intelligence strategy.","Through 25 interviews with scikit-learn's maintainers and funders, this study makes two key contributions to research and practice.","First, the study illustrates how the maintainers have weaved public and private funding into their project to ensure the continued provision of scikit-learn as a digital public good, as well as the importance of diversified funding and governance protocols for funding to safeguard the community ethos of the project.","Second, it offers practical recommendations to various stakeholders.","For OSS developer communities, it illustrates the benefits of a diversified funding model in balancing the merits and drawbacks of different funding sources.","For companies, it serves as a reminder that sponsoring developers or OSS projects can significantly support OSS maintainers, who often struggle with limited resources and towering workloads.","For governments, it emphasises the importance of funding the maintenance of existing OSS in addition to or exclusively funding the development of new OSS libraries or features.","The paper concludes with suggestions for future research directions."],"url":"http://arxiv.org/abs/2404.06484v2","category":"cs.SE"}
{"created":"2024-04-09 17:30:48","title":"Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks","abstract":"Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs' capability to handle extremely long documents. As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models' long-text capabilities has become increasingly important. Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks. These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges. Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs' long context capabilities. These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval. The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings. Our code is available at https://github.com/open-compass/Ada-LEval.","sentences":["Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs' capability to handle extremely long documents.","As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models' long-text capabilities has become increasingly important.","Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks.","These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges.","Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve.","In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs.","Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs' long context capabilities.","These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens.","We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval.","The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings.","Our code is available at https://github.com/open-compass/Ada-LEval."],"url":"http://arxiv.org/abs/2404.06480v2","category":"cs.CL"}
{"created":"2024-04-09 17:30:18","title":"Text-Based Reasoning About Vector Graphics","abstract":"While large multimodal models excel in broad vision-language benchmarks, they often struggle with tasks requiring precise perception of low-level visual details, such as comparing line lengths or solving simple mazes. In particular, this failure mode persists in question-answering tasks about vector graphics -- images composed purely of 2D objects and shapes. To address this challenge, we propose the Visually Descriptive Language Model (VDLM), which performs text-based reasoning about vector graphics. VDLM leverages Scalable Vector Graphics (SVG) for a more precise visual description and first uses an off-the-shelf raster-to-SVG algorithm for encoding. Since existing language models cannot understand raw SVGs in a zero-shot setting, VDLM then bridges SVG with pretrained language models through a newly introduced intermediate symbolic representation, Primal Visual Description (PVD), comprising primitive attributes (e.g., shape, position, measurement) with their corresponding predicted values. PVD is task-agnostic and represents visual primitives that are universal across all vector graphics. It can be learned with procedurally generated (SVG, PVD) pairs and also enables the direct use of LLMs for generalization to complex reasoning tasks. By casting an image to a text-based representation, we can leverage the power of language models to learn alignment from SVG to visual primitives and generalize to unseen question-answering tasks. Empirical results show that VDLM achieves stronger zero-shot performance compared to state-of-the-art LMMs, such as GPT-4V, in various low-level multimodal perception and reasoning tasks on vector graphics. We additionally present extensive analyses on VDLM's performance, demonstrating that our framework offers better interpretability due to its disentangled perception and reasoning processes. Project page: https://mikewangwzhl.github.io/VDLM/","sentences":["While large multimodal models excel in broad vision-language benchmarks, they often struggle with tasks requiring precise perception of low-level visual details, such as comparing line lengths or solving simple mazes.","In particular, this failure mode persists in question-answering tasks about vector graphics -- images composed purely of 2D objects and shapes.","To address this challenge, we propose the Visually Descriptive Language Model (VDLM), which performs text-based reasoning about vector graphics.","VDLM leverages Scalable Vector Graphics (SVG) for a more precise visual description and first uses an off-the-shelf raster-to-SVG algorithm for encoding.","Since existing language models cannot understand raw SVGs in a zero-shot setting, VDLM then bridges SVG with pretrained language models through a newly introduced intermediate symbolic representation, Primal Visual Description (PVD), comprising primitive attributes (e.g., shape, position, measurement) with their corresponding predicted values.","PVD is task-agnostic and represents visual primitives that are universal across all vector graphics.","It can be learned with procedurally generated (SVG, PVD) pairs and also enables the direct use of LLMs for generalization to complex reasoning tasks.","By casting an image to a text-based representation, we can leverage the power of language models to learn alignment from SVG to visual primitives and generalize to unseen question-answering tasks.","Empirical results show that VDLM achieves stronger zero-shot performance compared to state-of-the-art LMMs, such as GPT-4V, in various low-level multimodal perception and reasoning tasks on vector graphics.","We additionally present extensive analyses on VDLM's performance, demonstrating that our framework offers better interpretability due to its disentangled perception and reasoning processes.","Project page: https://mikewangwzhl.github.io/VDLM/"],"url":"http://arxiv.org/abs/2404.06479v2","category":"cs.CL"}
{"created":"2024-04-09 17:28:07","title":"Mechanised Hypersafety Proofs about Structured Data: Extended Version","abstract":"Arrays are a fundamental abstraction to represent collections of data. It is often possible to exploit structural properties of the data stored in an array (e.g., repetition or sparsity) to develop a specialised representation optimised for space efficiency. Formally reasoning about correctness of manipulations with such structured data is challenging, as they are often composed of multiple loops with non-trivial invariants.   In this work, we observe that specifications for structured data manipulations can be phrased as hypersafety properties, i.e., predicates that relate traces of $k$ programs. To turn this observation into an effective verification methodology, we developed the Logic for Graceful Tensor Manipulation (LGTM), a new Hoare-style relational separation logic for specifying and verifying computations over structured data. The key enabling idea of LGTM is that of parametrised hypersafety specifications that allow the number $k$ of the program components to depend on the program variables. We implemented LGTM as a foundational embedding into Coq, mechanising its rules, meta-theory, and the proof of soundness. Furthermore, we developed a library of domain-specific tactics that automate computer-aided hypersafety reasoning, resulting in pleasantly short proof scripts that enjoy a high degree of reuse. We argue for the effectiveness of relational reasoning about structured data in LGTM by specifying and mechanically proving correctness of 13 case studies including computations on compressed arrays and efficient operations over multiple kinds of sparse tensors.","sentences":["Arrays are a fundamental abstraction to represent collections of data.","It is often possible to exploit structural properties of the data stored in an array (e.g., repetition or sparsity) to develop a specialised representation optimised for space efficiency.","Formally reasoning about correctness of manipulations with such structured data is challenging, as they are often composed of multiple loops with non-trivial invariants.   ","In this work, we observe that specifications for structured data manipulations can be phrased as hypersafety properties, i.e., predicates that relate traces of $k$ programs.","To turn this observation into an effective verification methodology, we developed the Logic for Graceful Tensor Manipulation (LGTM), a new Hoare-style relational separation logic for specifying and verifying computations over structured data.","The key enabling idea of LGTM is that of parametrised hypersafety specifications that allow the number $k$ of the program components to depend on the program variables.","We implemented LGTM as a foundational embedding into Coq, mechanising its rules, meta-theory, and the proof of soundness.","Furthermore, we developed a library of domain-specific tactics that automate computer-aided hypersafety reasoning, resulting in pleasantly short proof scripts that enjoy a high degree of reuse.","We argue for the effectiveness of relational reasoning about structured data in LGTM by specifying and mechanically proving correctness of 13 case studies including computations on compressed arrays and efficient operations over multiple kinds of sparse tensors."],"url":"http://arxiv.org/abs/2404.06477v1","category":"cs.PL"}
{"created":"2024-04-09 17:25:47","title":"Autonomous Evaluation and Refinement of Digital Agents","abstract":"We show that domain-general automatic evaluators can significantly improve the performance of agents for web navigation and device control. We experiment with multiple evaluation models that trade off between inference cost, modularity of design, and accuracy. We validate the performance of these models in several popular benchmarks for digital agents, finding between 74.4 and 92.9% agreement with oracle evaluation metrics. Finally, we use these evaluators to improve the performance of existing agents via fine-tuning and inference-time guidance. Without any additional supervision, we improve state-of-the-art performance by 29% on the popular benchmark WebArena, and achieve a 75% relative improvement in a challenging domain transfer scenario.","sentences":["We show that domain-general automatic evaluators can significantly improve the performance of agents for web navigation and device control.","We experiment with multiple evaluation models that trade off between inference cost, modularity of design, and accuracy.","We validate the performance of these models in several popular benchmarks for digital agents, finding between 74.4 and 92.9% agreement with oracle evaluation metrics.","Finally, we use these evaluators to improve the performance of existing agents via fine-tuning and inference-time guidance.","Without any additional supervision, we improve state-of-the-art performance by 29% on the popular benchmark WebArena, and achieve a 75% relative improvement in a challenging domain transfer scenario."],"url":"http://arxiv.org/abs/2404.06474v2","category":"cs.AI"}
{"created":"2024-04-09 17:24:25","title":"High-skilled Human Workers in Non-Routine Jobs are Susceptible to AI Automation but Wage Benefits Differ between Occupations","abstract":"Artificial Intelligence (AI) will change human work by taking over specific job tasks, but there is a debate which tasks are susceptible to automation, and whether AI will augment or replace workers and affect wages. By combining data on job tasks with a measure of AI susceptibility, we show that more highly skilled workers are more susceptible to AI automation, and that analytical non-routine tasks are at risk to be impacted by AI. Moreover, we observe that wage growth premiums for the lowest and the highest required skill level appear unrelated to AI susceptibility and that workers in occupations with many routine tasks saw higher wage growth if their work was more strongly susceptible to AI. Our findings imply that AI has the potential to affect human workers differently than canonical economic theories about the impact of technology on work these theories predict.","sentences":["Artificial Intelligence (AI) will change human work by taking over specific job tasks, but there is a debate which tasks are susceptible to automation, and whether AI will augment or replace workers and affect wages.","By combining data on job tasks with a measure of AI susceptibility, we show that more highly skilled workers are more susceptible to AI automation, and that analytical non-routine tasks are at risk to be impacted by AI.","Moreover, we observe that wage growth premiums for the lowest and the highest required skill level appear unrelated to AI susceptibility and that workers in occupations with many routine tasks saw higher wage growth if their work was more strongly susceptible to AI.","Our findings imply that AI has the potential to affect human workers differently than canonical economic theories about the impact of technology on work these theories predict."],"url":"http://arxiv.org/abs/2404.06472v1","category":"econ.GN"}
{"created":"2024-04-09 17:17:48","title":"Learning State-Invariant Representations of Objects from Image Collections with State, Pose, and Viewpoint Changes","abstract":"We add one more invariance - state invariance - to the more commonly used other invariances for learning object representations for recognition and retrieval. By state invariance, we mean robust with respect to changes in the structural form of the object, such as when an umbrella is folded, or when an item of clothing is tossed on the floor. Since humans generally have no difficulty in recognizing objects despite such state changes, we are naturally faced with the question of whether it is possible to devise a neural architecture with similar abilities. To that end, we present a novel dataset, ObjectsWithStateChange, that captures state and pose variations in the object images recorded from arbitrary viewpoints. We believe that this dataset will facilitate research in fine-grained object recognition and retrieval of objects that are capable of state changes. The goal of such research would be to train models capable of generating object embeddings that remain invariant to state changes while also staying invariant to transformations induced by changes in viewpoint, pose, illumination, etc. To demonstrate the usefulness of the ObjectsWithStateChange dataset, we also propose a curriculum learning strategy that uses the similarity relationships in the learned embedding space after each epoch to guide the training process. The model learns discriminative features by comparing visually similar objects within and across different categories, encouraging it to differentiate between objects that may be challenging to distinguish due to changes in their state. We believe that this strategy enhances the model's ability to capture discriminative features for fine-grained tasks that may involve objects with state changes, leading to performance improvements on object-level tasks not only on our new dataset, but also on two other challenging multi-view datasets such as ModelNet40 and ObjectPI.","sentences":["We add one more invariance - state invariance - to the more commonly used other invariances for learning object representations for recognition and retrieval.","By state invariance, we mean robust with respect to changes in the structural form of the object, such as when an umbrella is folded, or when an item of clothing is tossed on the floor.","Since humans generally have no difficulty in recognizing objects despite such state changes, we are naturally faced with the question of whether it is possible to devise a neural architecture with similar abilities.","To that end, we present a novel dataset, ObjectsWithStateChange, that captures state and pose variations in the object images recorded from arbitrary viewpoints.","We believe that this dataset will facilitate research in fine-grained object recognition and retrieval of objects that are capable of state changes.","The goal of such research would be to train models capable of generating object embeddings that remain invariant to state changes while also staying invariant to transformations induced by changes in viewpoint, pose, illumination, etc.","To demonstrate the usefulness of the ObjectsWithStateChange dataset, we also propose a curriculum learning strategy that uses the similarity relationships in the learned embedding space after each epoch to guide the training process.","The model learns discriminative features by comparing visually similar objects within and across different categories, encouraging it to differentiate between objects that may be challenging to distinguish due to changes in their state.","We believe that this strategy enhances the model's ability to capture discriminative features for fine-grained tasks that may involve objects with state changes, leading to performance improvements on object-level tasks not only on our new dataset, but also on two other challenging multi-view datasets such as ModelNet40 and ObjectPI."],"url":"http://arxiv.org/abs/2404.06470v1","category":"cs.CV"}
{"created":"2024-04-09 17:12:16","title":"Emergent Braitenberg-style Behaviours for Navigating the ViZDoom `My Way Home' Labyrinth","abstract":"The navigation of complex labyrinths with tens of rooms under visual partially observable state is typically addressed using recurrent deep reinforcement learning architectures. In this work, we show that navigation can be achieved through the emergent evolution of a simple Braitentberg-style heuristic that structures the interaction between agent and labyrinth, i.e. complex behaviour from simple heuristics. To do so, the approach of tangled program graphs is assumed in which programs cooperatively coevolve to develop a modular indexing scheme that only employs 0.8\\% of the state space. We attribute this simplicity to several biases implicit in the representation, such as the use of pixel indexing as opposed to deploying a convolutional kernel or image processing operators.","sentences":["The navigation of complex labyrinths with tens of rooms under visual partially observable state is typically addressed using recurrent deep reinforcement learning architectures.","In this work, we show that navigation can be achieved through the emergent evolution of a simple Braitentberg-style heuristic that structures the interaction between agent and labyrinth, i.e. complex behaviour from simple heuristics.","To do so, the approach of tangled program graphs is assumed in which programs cooperatively coevolve to develop a modular indexing scheme that only employs 0.8\\% of the state space.","We attribute this simplicity to several biases implicit in the representation, such as the use of pixel indexing as opposed to deploying a convolutional kernel or image processing operators."],"url":"http://arxiv.org/abs/2404.06529v1","category":"cs.NE"}
{"created":"2024-04-09 16:55:23","title":"A comparative analysis of deep learning models for lung segmentation on X-ray images","abstract":"Robust and highly accurate lung segmentation in X-rays is crucial in medical imaging. This study evaluates deep learning solutions for this task, ranking existing methods and analyzing their performance under diverse image modifications. Out of 61 analyzed papers, only nine offered implementation or pre-trained models, enabling assessment of three prominent methods: Lung VAE, TransResUNet, and CE-Net. The analysis revealed that CE-Net performs best, demonstrating the highest values in dice similarity coefficient and intersection over union metric.","sentences":["Robust and highly accurate lung segmentation in X-rays is crucial in medical imaging.","This study evaluates deep learning solutions for this task, ranking existing methods and analyzing their performance under diverse image modifications.","Out of 61 analyzed papers, only nine offered implementation or pre-trained models, enabling assessment of three prominent methods: Lung VAE, TransResUNet, and CE-Net.","The analysis revealed that CE-Net performs best, demonstrating the highest values in dice similarity coefficient and intersection over union metric."],"url":"http://arxiv.org/abs/2404.06455v1","category":"eess.IV"}
{"created":"2024-04-09 16:54:19","title":"PURE: Turning Polysemantic Neurons Into Pure Features by Identifying Relevant Circuits","abstract":"The field of mechanistic interpretability aims to study the role of individual neurons in Deep Neural Networks. Single neurons, however, have the capability to act polysemantically and encode for multiple (unrelated) features, which renders their interpretation difficult. We present a method for disentangling polysemanticity of any Deep Neural Network by decomposing a polysemantic neuron into multiple monosemantic \"virtual\" neurons. This is achieved by identifying the relevant sub-graph (\"circuit\") for each \"pure\" feature. We demonstrate how our approach allows us to find and disentangle various polysemantic units of ResNet models trained on ImageNet. While evaluating feature visualizations using CLIP, our method effectively disentangles representations, improving upon methods based on neuron activations. Our code is available at https://github.com/maxdreyer/PURE.","sentences":["The field of mechanistic interpretability aims to study the role of individual neurons in Deep Neural Networks.","Single neurons, however, have the capability to act polysemantically and encode for multiple (unrelated) features, which renders their interpretation difficult.","We present a method for disentangling polysemanticity of any Deep Neural Network by decomposing a polysemantic neuron into multiple monosemantic \"virtual\" neurons.","This is achieved by identifying the relevant sub-graph (\"circuit\") for each \"pure\" feature.","We demonstrate how our approach allows us to find and disentangle various polysemantic units of ResNet models trained on ImageNet.","While evaluating feature visualizations using CLIP, our method effectively disentangles representations, improving upon methods based on neuron activations.","Our code is available at https://github.com/maxdreyer/PURE."],"url":"http://arxiv.org/abs/2404.06453v1","category":"cs.CV"}
{"created":"2024-04-09 16:50:30","title":"Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of Large Language Models","abstract":"Recently, there has been a surge in the development of advanced intelligent generative content (AIGC), especially large language models (LLMs). However, for many downstream tasks, it is necessary to fine-tune LLMs using private data. While federated learning offers a promising privacy-preserving solution to LLM fine-tuning, the substantial size of an LLM, combined with high computational and communication demands, makes it hard to apply to downstream tasks. More importantly, private edge servers often possess varying computing and network resources in real-world scenarios, introducing additional complexities to LLM fine-tuning. To tackle these problems, we design and implement an automated federated pipeline, named FedPipe, to fine-tune LLMs with minimal training cost but without adding any inference latency. FedPipe firstly identifies the weights to be fine-tuned based on their contributions to the LLM training. It then configures a low-rank adapter for each selected weight to train local low-rank adapters on an edge server, and aggregate local adapters of all edge servers to fine-tune the whole LLM. Finally, it appropriately quantizes the parameters of LLM to reduce memory space according to the requirements of edge servers. Extensive experiments demonstrate that FedPipe expedites the model training and achieves higher accuracy than state-of-the-art benchmarks.","sentences":["Recently, there has been a surge in the development of advanced intelligent generative content (AIGC), especially large language models (LLMs).","However, for many downstream tasks, it is necessary to fine-tune LLMs using private data.","While federated learning offers a promising privacy-preserving solution to LLM fine-tuning, the substantial size of an LLM, combined with high computational and communication demands, makes it hard to apply to downstream tasks.","More importantly, private edge servers often possess varying computing and network resources in real-world scenarios, introducing additional complexities to LLM fine-tuning.","To tackle these problems, we design and implement an automated federated pipeline, named FedPipe, to fine-tune LLMs with minimal training cost but without adding any inference latency.","FedPipe firstly identifies the weights to be fine-tuned based on their contributions to the LLM training.","It then configures a low-rank adapter for each selected weight to train local low-rank adapters on an edge server, and aggregate local adapters of all edge servers to fine-tune the whole LLM.","Finally, it appropriately quantizes the parameters of LLM to reduce memory space according to the requirements of edge servers.","Extensive experiments demonstrate that FedPipe expedites the model training and achieves higher accuracy than state-of-the-art benchmarks."],"url":"http://arxiv.org/abs/2404.06448v1","category":"cs.LG"}
{"created":"2024-04-09 16:25:02","title":"Missing Pieces: How Framing Uncertainty Impacts Longitudinal Trust in AI Decision Aids -- A Gig Driver Case Study","abstract":"Decision aids based on artificial intelligence (AI) are becoming increasingly common. When such systems are deployed in environments with inherent uncertainty, following AI-recommended decisions may lead to a wide range of outcomes. In this work, we investigate how the framing of uncertainty in outcomes impacts users' longitudinal trust in AI decision aids, which is crucial to ensuring that these systems achieve their intended purposes. More specifically, we use gig driving as a representative domain to address the question: how does exposing uncertainty at different levels of granularity affect the evolution of users' trust and their willingness to rely on recommended decisions? We report on a longitudinal mixed-methods study $(n = 51)$ where we measured the trust of gig drivers as they interacted with an AI-based schedule recommendation tool. Statistically significant quantitative results indicate that participants' trust in and willingness to rely on the tool for planning depended on the perceived accuracy of the tool's estimates; that providing ranged estimates improved trust; and that increasing prediction granularity and using hedging language improved willingness to rely on the tool even when trust was low. Additionally, we report on interviews with participants which revealed a diversity of experiences with the tool, suggesting that AI systems must build trust by going beyond general designs to calibrate the expectations of individual users.","sentences":["Decision aids based on artificial intelligence (AI) are becoming increasingly common.","When such systems are deployed in environments with inherent uncertainty, following AI-recommended decisions may lead to a wide range of outcomes.","In this work, we investigate how the framing of uncertainty in outcomes impacts users' longitudinal trust in AI decision aids, which is crucial to ensuring that these systems achieve their intended purposes.","More specifically, we use gig driving as a representative domain to address the question: how does exposing uncertainty at different levels of granularity affect the evolution of users' trust and their willingness to rely on recommended decisions?","We report on a longitudinal mixed-methods study $(n = 51)$ where we measured the trust of gig drivers as they interacted with an AI-based schedule recommendation tool.","Statistically significant quantitative results indicate that participants' trust in and willingness to rely on the tool for planning depended on the perceived accuracy of the tool's estimates; that providing ranged estimates improved trust; and that increasing prediction granularity and using hedging language improved willingness to rely on the tool even when trust was low.","Additionally, we report on interviews with participants which revealed a diversity of experiences with the tool, suggesting that AI systems must build trust by going beyond general designs to calibrate the expectations of individual users."],"url":"http://arxiv.org/abs/2404.06432v1","category":"cs.HC"}
{"created":"2024-04-09 16:23:01","title":"pfl-research: simulation framework for accelerating research in Private Federated Learning","abstract":"Federated learning (FL) is an emerging machine learning (ML) training paradigm where clients own their data and collaborate to train a global model, without revealing any data to the server and other participants. Researchers commonly perform experiments in a simulation environment to quickly iterate on ideas. However, existing open-source tools do not offer the efficiency required to simulate FL on larger and more realistic FL datasets. We introduce pfl-research, a fast, modular, and easy-to-use Python framework for simulating FL. It supports TensorFlow, PyTorch, and non-neural network models, and is tightly integrated with state-of-the-art privacy algorithms. We study the speed of open-source FL frameworks and show that pfl-research is 7-72$\\times$ faster than alternative open-source frameworks on common cross-device setups. Such speedup will significantly boost the productivity of the FL research community and enable testing hypotheses on realistic FL datasets that were previously too resource intensive. We release a suite of benchmarks that evaluates an algorithm's overall performance on a diverse set of realistic scenarios. The code is available on GitHub at https://github.com/apple/pfl-research.","sentences":["Federated learning (FL) is an emerging machine learning (ML) training paradigm where clients own their data and collaborate to train a global model, without revealing any data to the server and other participants.","Researchers commonly perform experiments in a simulation environment to quickly iterate on ideas.","However, existing open-source tools do not offer the efficiency required to simulate FL on larger and more realistic FL datasets.","We introduce pfl-research, a fast, modular, and easy-to-use Python framework for simulating FL.","It supports TensorFlow, PyTorch, and non-neural network models, and is tightly integrated with state-of-the-art privacy algorithms.","We study the speed of open-source FL frameworks and show that pfl-research is 7-72$\\times$ faster than alternative open-source frameworks on common cross-device setups.","Such speedup will significantly boost the productivity of the FL research community and enable testing hypotheses on realistic FL datasets that were previously too resource intensive.","We release a suite of benchmarks that evaluates an algorithm's overall performance on a diverse set of realistic scenarios.","The code is available on GitHub at https://github.com/apple/pfl-research."],"url":"http://arxiv.org/abs/2404.06430v1","category":"cs.LG"}
{"created":"2024-04-09 16:20:03","title":"Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion","abstract":"Benefiting from the rapid development of 2D diffusion models, 3D content creation has made significant progress recently. One promising solution involves the fine-tuning of pre-trained 2D diffusion models to harness their capacity for producing multi-view images, which are then lifted into accurate 3D models via methods like fast-NeRFs or large reconstruction models. However, as inconsistency still exists and limited generated resolution, the generation results of such methods still lack intricate textures and complex geometries. To solve this problem, we propose Magic-Boost, a multi-view conditioned diffusion model that significantly refines coarse generative results through a brief period of SDS optimization ($\\sim15$min). Compared to the previous text or single image based diffusion models, Magic-Boost exhibits a robust capability to generate images with high consistency from pseudo synthesized multi-view images. It provides precise SDS guidance that well aligns with the identity of the input images, enriching the local detail in both geometry and texture of the initial generative results. Extensive experiments show Magic-Boost greatly enhances the coarse inputs and generates high-quality 3D assets with rich geometric and textural details. (Project Page: https://magic-research.github.io/magic-boost/)","sentences":["Benefiting from the rapid development of 2D diffusion models, 3D content creation has made significant progress recently.","One promising solution involves the fine-tuning of pre-trained 2D diffusion models to harness their capacity for producing multi-view images, which are then lifted into accurate 3D models via methods like fast-NeRFs or large reconstruction models.","However, as inconsistency still exists and limited generated resolution, the generation results of such methods still lack intricate textures and complex geometries.","To solve this problem, we propose Magic-Boost, a multi-view conditioned diffusion model that significantly refines coarse generative results through a brief period of SDS optimization ($\\sim15$min).","Compared to the previous text or single image based diffusion models, Magic-Boost exhibits a robust capability to generate images with high consistency from pseudo synthesized multi-view images.","It provides precise SDS guidance that well aligns with the identity of the input images, enriching the local detail in both geometry and texture of the initial generative results.","Extensive experiments show Magic-Boost greatly enhances the coarse inputs and generates high-quality 3D assets with rich geometric and textural details.","(Project Page: https://magic-research.github.io/magic-boost/)"],"url":"http://arxiv.org/abs/2404.06429v1","category":"cs.CV"}
{"created":"2024-04-09 16:17:48","title":"Quantum stochastic thermodynamics in the mesoscopic-leads formulation","abstract":"We introduce a numerical method to sample the distributions of charge, heat, and entropy production in open quantum systems coupled strongly to macroscopic reservoirs, with both temporal and energy resolution and beyond the linear-response regime. Our method exploits the mesoscopic-leads formulation, where macroscopic reservoirs are modeled by a finite collection of modes that are continuously damped toward thermal equilibrium by an appropriate Gorini-Kossakowski-Sudarshan-Lindblad master equation. Focussing on non-interacting fermionic systems, we access the time-resolved full counting statistics through a trajectory unraveling of the master equation. We show that the integral fluctuation theorems for the total entropy production, as well as the martingale and uncertainty entropy production, hold. Furthermore, we investigate the fluctuations of the dissipated heat in finite-time information erasure. Conceptually, our approach extends the continuous-time trajectory description of quantum stochastic thermodynamics beyond the regime of weak system-environment coupling.","sentences":["We introduce a numerical method to sample the distributions of charge, heat, and entropy production in open quantum systems coupled strongly to macroscopic reservoirs, with both temporal and energy resolution and beyond the linear-response regime.","Our method exploits the mesoscopic-leads formulation, where macroscopic reservoirs are modeled by a finite collection of modes that are continuously damped toward thermal equilibrium by an appropriate Gorini-Kossakowski-Sudarshan-Lindblad master equation.","Focussing on non-interacting fermionic systems, we access the time-resolved full counting statistics through a trajectory unraveling of the master equation.","We show that the integral fluctuation theorems for the total entropy production, as well as the martingale and uncertainty entropy production, hold.","Furthermore, we investigate the fluctuations of the dissipated heat in finite-time information erasure.","Conceptually, our approach extends the continuous-time trajectory description of quantum stochastic thermodynamics beyond the regime of weak system-environment coupling."],"url":"http://arxiv.org/abs/2404.06426v1","category":"quant-ph"}
{"created":"2024-04-09 16:14:03","title":"Deep Reinforcement Learning-Based Approach for a Single Vehicle Persistent Surveillance Problem with Fuel Constraints","abstract":"This article presents a deep reinforcement learning-based approach to tackle a persistent surveillance mission requiring a single unmanned aerial vehicle initially stationed at a depot with fuel or time-of-flight constraints to repeatedly visit a set of targets with equal priority. Owing to the vehicle's fuel or time-of-flight constraints, the vehicle must be regularly refueled, or its battery must be recharged at the depot. The objective of the problem is to determine an optimal sequence of visits to the targets that minimizes the maximum time elapsed between successive visits to any target while ensuring that the vehicle never runs out of fuel or charge. We present a deep reinforcement learning algorithm to solve this problem and present the results of numerical experiments that corroborate the effectiveness of this approach in comparison with common-sense greedy heuristics.","sentences":["This article presents a deep reinforcement learning-based approach to tackle a persistent surveillance mission requiring a single unmanned aerial vehicle initially stationed at a depot with fuel or time-of-flight constraints to repeatedly visit a set of targets with equal priority.","Owing to the vehicle's fuel or time-of-flight constraints, the vehicle must be regularly refueled, or its battery must be recharged at the depot.","The objective of the problem is to determine an optimal sequence of visits to the targets that minimizes the maximum time elapsed between successive visits to any target while ensuring that the vehicle never runs out of fuel or charge.","We present a deep reinforcement learning algorithm to solve this problem and present the results of numerical experiments that corroborate the effectiveness of this approach in comparison with common-sense greedy heuristics."],"url":"http://arxiv.org/abs/2404.06423v1","category":"cs.RO"}
{"created":"2024-04-09 16:07:35","title":"Studying the Impact of Latent Representations in Implicit Neural Networks for Scientific Continuous Field Reconstruction","abstract":"Learning a continuous and reliable representation of physical fields from sparse sampling is challenging and it affects diverse scientific disciplines. In a recent work, we present a novel model called MMGN (Multiplicative and Modulated Gabor Network) with implicit neural networks. In this work, we design additional studies leveraging explainability methods to complement the previous experiments and further enhance the understanding of latent representations generated by the model. The adopted methods are general enough to be leveraged for any latent space inspection. Preliminary results demonstrate the contextual information incorporated in the latent representations and their impact on the model performance. As a work in progress, we will continue to verify our findings and develop novel explainability approaches.","sentences":["Learning a continuous and reliable representation of physical fields from sparse sampling is challenging and it affects diverse scientific disciplines.","In a recent work, we present a novel model called MMGN (Multiplicative and Modulated Gabor Network) with implicit neural networks.","In this work, we design additional studies leveraging explainability methods to complement the previous experiments and further enhance the understanding of latent representations generated by the model.","The adopted methods are general enough to be leveraged for any latent space inspection.","Preliminary results demonstrate the contextual information incorporated in the latent representations and their impact on the model performance.","As a work in progress, we will continue to verify our findings and develop novel explainability approaches."],"url":"http://arxiv.org/abs/2404.06418v1","category":"cs.LG"}
{"created":"2024-04-09 16:01:24","title":"AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents","abstract":"The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks. As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress. However, existing benchmarks are often narrow and simply compute overall task success. To face these issues, we propose AgentQuest -- a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task. We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase. Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https://github.com/nec-research/agentquest.","sentences":["The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks.","As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress.","However, existing benchmarks are often narrow and simply compute overall task success.","To face these issues, we propose AgentQuest -- a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task.","We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase.","Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https://github.com/nec-research/agentquest."],"url":"http://arxiv.org/abs/2404.06411v1","category":"cs.AI"}
{"created":"2024-04-09 15:54:16","title":"Take a Look at it! Rethinking How to Evaluate Language Model Jailbreak","abstract":"Large language models (LLMs) have become increasingly integrated with various applications. To ensure that LLMs do not generate unsafe responses, they are aligned with safeguards that specify what content is restricted. However, such alignment can be bypassed to produce prohibited content using a technique commonly referred to as jailbreak. Different systems have been proposed to perform the jailbreak automatically. These systems rely on evaluation methods to determine whether a jailbreak attempt is successful. However, our analysis reveals that current jailbreak evaluation methods have two limitations. (1) Their objectives lack clarity and do not align with the goal of identifying unsafe responses. (2) They oversimplify the jailbreak result as a binary outcome, successful or not.   In this paper, we propose three metrics, safeguard violation, informativeness, and relative truthfulness, to evaluate language model jailbreak. Additionally, we demonstrate how these metrics correlate with the goal of different malicious actors. To compute these metrics, we introduce a multifaceted approach that extends the natural language generation evaluation method after preprocessing the response. We evaluate our metrics on a benchmark dataset produced from three malicious intent datasets and three jailbreak systems. The benchmark dataset is labeled by three annotators. We compare our multifaceted approach with three existing jailbreak evaluation methods. Experiments demonstrate that our multifaceted evaluation outperforms existing methods, with F1 scores improving on average by 17% compared to existing baselines. Our findings motivate the need to move away from the binary view of the jailbreak problem and incorporate a more comprehensive evaluation to ensure the safety of the language model.","sentences":["Large language models (LLMs) have become increasingly integrated with various applications.","To ensure that LLMs do not generate unsafe responses, they are aligned with safeguards that specify what content is restricted.","However, such alignment can be bypassed to produce prohibited content using a technique commonly referred to as jailbreak.","Different systems have been proposed to perform the jailbreak automatically.","These systems rely on evaluation methods to determine whether a jailbreak attempt is successful.","However, our analysis reveals that current jailbreak evaluation methods have two limitations.","(1) Their objectives lack clarity and do not align with the goal of identifying unsafe responses.","(2) They oversimplify the jailbreak result as a binary outcome, successful or not.   ","In this paper, we propose three metrics, safeguard violation, informativeness, and relative truthfulness, to evaluate language model jailbreak.","Additionally, we demonstrate how these metrics correlate with the goal of different malicious actors.","To compute these metrics, we introduce a multifaceted approach that extends the natural language generation evaluation method after preprocessing the response.","We evaluate our metrics on a benchmark dataset produced from three malicious intent datasets and three jailbreak systems.","The benchmark dataset is labeled by three annotators.","We compare our multifaceted approach with three existing jailbreak evaluation methods.","Experiments demonstrate that our multifaceted evaluation outperforms existing methods, with F1 scores improving on average by 17% compared to existing baselines.","Our findings motivate the need to move away from the binary view of the jailbreak problem and incorporate a more comprehensive evaluation to ensure the safety of the language model."],"url":"http://arxiv.org/abs/2404.06407v1","category":"cs.CL"}
{"created":"2024-04-09 15:54:00","title":"Wu's Method can Boost Symbolic AI to Rival Silver Medalists and AlphaGeometry to Outperform Gold Medalists at IMO Geometry","abstract":"Proving geometric theorems constitutes a hallmark of visual reasoning combining both intuitive and logical skills. Therefore, automated theorem proving of Olympiad-level geometry problems is considered a notable milestone in human-level automated reasoning. The introduction of AlphaGeometry, a neuro-symbolic model trained with 100 million synthetic samples, marked a major breakthrough. It solved 25 of 30 International Mathematical Olympiad (IMO) problems whereas the reported baseline based on Wu's method solved only ten. In this note, we revisit the IMO-AG-30 Challenge introduced with AlphaGeometry, and find that Wu's method is surprisingly strong. Wu's method alone can solve 15 problems, and some of them are not solved by any of the other methods. This leads to two key findings: (i) Combining Wu's method with the classic synthetic methods of deductive databases and angle, ratio, and distance chasing solves 21 out of 30 methods by just using a CPU-only laptop with a time limit of 5 minutes per problem. Essentially, this classic method solves just 4 problems less than AlphaGeometry and establishes the first fully symbolic baseline strong enough to rival the performance of an IMO silver medalist. (ii) Wu's method even solves 2 of the 5 problems that AlphaGeometry failed to solve. Thus, by combining AlphaGeometry with Wu's method we set a new state-of-the-art for automated theorem proving on IMO-AG-30, solving 27 out of 30 problems, the first AI method which outperforms an IMO gold medalist.","sentences":["Proving geometric theorems constitutes a hallmark of visual reasoning combining both intuitive and logical skills.","Therefore, automated theorem proving of Olympiad-level geometry problems is considered a notable milestone in human-level automated reasoning.","The introduction of AlphaGeometry, a neuro-symbolic model trained with 100 million synthetic samples, marked a major breakthrough.","It solved 25 of 30 International Mathematical Olympiad (IMO) problems whereas the reported baseline based on Wu's method solved only ten.","In this note, we revisit the IMO-AG-30 Challenge introduced with AlphaGeometry, and find that Wu's method is surprisingly strong.","Wu's method alone can solve 15 problems, and some of them are not solved by any of the other methods.","This leads to two key findings: (i) Combining Wu's method with the classic synthetic methods of deductive databases and angle, ratio, and distance chasing solves 21 out of 30 methods by just using a CPU-only laptop with a time limit of 5 minutes per problem.","Essentially, this classic method solves just 4 problems less than AlphaGeometry and establishes the first fully symbolic baseline strong enough to rival the performance of an IMO silver medalist.","(ii) Wu's method even solves 2 of the 5 problems that AlphaGeometry failed to solve.","Thus, by combining AlphaGeometry with Wu's method we set a new state-of-the-art for automated theorem proving on IMO-AG-30, solving 27 out of 30 problems, the first AI method which outperforms an IMO gold medalist."],"url":"http://arxiv.org/abs/2404.06405v1","category":"cs.AI"}
{"created":"2024-04-09 15:53:06","title":"Apprentices to Research Assistants: Advancing Research with Large Language Models","abstract":"Large Language Models (LLMs) have emerged as powerful tools in various research domains. This article examines their potential through a literature review and firsthand experimentation. While LLMs offer benefits like cost-effectiveness and efficiency, challenges such as prompt tuning, biases, and subjectivity must be addressed. The study presents insights from experiments utilizing LLMs for qualitative analysis, highlighting successes and limitations. Additionally, it discusses strategies for mitigating challenges, such as prompt optimization techniques and leveraging human expertise. This study aligns with the 'LLMs as Research Tools' workshop's focus on integrating LLMs into HCI data work critically and ethically. By addressing both opportunities and challenges, our work contributes to the ongoing dialogue on their responsible application in research.","sentences":["Large Language Models (LLMs) have emerged as powerful tools in various research domains.","This article examines their potential through a literature review and firsthand experimentation.","While LLMs offer benefits like cost-effectiveness and efficiency, challenges such as prompt tuning, biases, and subjectivity must be addressed.","The study presents insights from experiments utilizing LLMs for qualitative analysis, highlighting successes and limitations.","Additionally, it discusses strategies for mitigating challenges, such as prompt optimization techniques and leveraging human expertise.","This study aligns with the 'LLMs as Research Tools' workshop's focus on integrating LLMs into HCI data work critically and ethically.","By addressing both opportunities and challenges, our work contributes to the ongoing dialogue on their responsible application in research."],"url":"http://arxiv.org/abs/2404.06404v1","category":"cs.HC"}
{"created":"2024-04-09 15:53:02","title":"Online Learning of Decision Trees with Thompson Sampling","abstract":"Decision Trees are prominent prediction models for interpretable Machine Learning. They have been thoroughly researched, mostly in the batch setting with a fixed labelled dataset, leading to popular algorithms such as C4.5, ID3 and CART. Unfortunately, these methods are of heuristic nature, they rely on greedy splits offering no guarantees of global optimality and often leading to unnecessarily complex and hard-to-interpret Decision Trees. Recent breakthroughs addressed this suboptimality issue in the batch setting, but no such work has considered the online setting with data arriving in a stream. To this end, we devise a new Monte Carlo Tree Search algorithm, Thompson Sampling Decision Trees (TSDT), able to produce optimal Decision Trees in an online setting. We analyse our algorithm and prove its almost sure convergence to the optimal tree. Furthermore, we conduct extensive experiments to validate our findings empirically. The proposed TSDT outperforms existing algorithms on several benchmarks, all while presenting the practical advantage of being tailored to the online setting.","sentences":["Decision Trees are prominent prediction models for interpretable Machine Learning.","They have been thoroughly researched, mostly in the batch setting with a fixed labelled dataset, leading to popular algorithms such as C4.5, ID3 and CART.","Unfortunately, these methods are of heuristic nature, they rely on greedy splits offering no guarantees of global optimality and often leading to unnecessarily complex and hard-to-interpret Decision Trees.","Recent breakthroughs addressed this suboptimality issue in the batch setting, but no such work has considered the online setting with data arriving in a stream.","To this end, we devise a new Monte Carlo Tree Search algorithm, Thompson Sampling Decision Trees (TSDT), able to produce optimal Decision Trees in an online setting.","We analyse our algorithm and prove its almost sure convergence to the optimal tree.","Furthermore, we conduct extensive experiments to validate our findings empirically.","The proposed TSDT outperforms existing algorithms on several benchmarks, all while presenting the practical advantage of being tailored to the online setting."],"url":"http://arxiv.org/abs/2404.06403v1","category":"cs.LG"}
{"created":"2024-04-09 15:35:52","title":"MuPT: A Generative Symbolic Music Pretrained Transformer","abstract":"In this paper, we explore the application of Large Language Models (LLMs) to the pre-training of music. While the prevalent use of MIDI in music modeling is well-established, our findings suggest that LLMs are inherently more compatible with ABC Notation, which aligns more closely with their design and strengths, thereby enhancing the model's performance in musical composition. To address the challenges associated with misaligned measures from different tracks during generation, we propose the development of a Synchronized Multi-Track ABC Notation (SMT-ABC Notation), which aims to preserve coherence across multiple musical tracks. Our contributions include a series of models capable of handling up to 8192 tokens, covering 90% of the symbolic music data in our training set. Furthermore, we explore the implications of the Symbolic Music Scaling Law (SMS Law) on model performance. The results indicate a promising direction for future research in music generation, offering extensive resources for community-led research through our open-source contributions.","sentences":["In this paper, we explore the application of Large Language Models (LLMs) to the pre-training of music.","While the prevalent use of MIDI in music modeling is well-established, our findings suggest that LLMs are inherently more compatible with ABC Notation, which aligns more closely with their design and strengths, thereby enhancing the model's performance in musical composition.","To address the challenges associated with misaligned measures from different tracks during generation, we propose the development of a Synchronized Multi-Track ABC Notation (SMT-ABC Notation), which aims to preserve coherence across multiple musical tracks.","Our contributions include a series of models capable of handling up to 8192 tokens, covering 90% of the symbolic music data in our training set.","Furthermore, we explore the implications of the Symbolic Music Scaling Law (SMS Law) on model performance.","The results indicate a promising direction for future research in music generation, offering extensive resources for community-led research through our open-source contributions."],"url":"http://arxiv.org/abs/2404.06393v2","category":"cs.SD"}
{"created":"2024-04-09 15:35:41","title":"Event Extraction in Basque: Typologically motivated Cross-Lingual Transfer-Learning Analysis","abstract":"Cross-lingual transfer-learning is widely used in Event Extraction for low-resource languages and involves a Multilingual Language Model that is trained in a source language and applied to the target language. This paper studies whether the typological similarity between source and target languages impacts the performance of cross-lingual transfer, an under-explored topic. We first focus on Basque as the target language, which is an ideal target language because it is typologically different from surrounding languages. Our experiments on three Event Extraction tasks show that the shared linguistic characteristic between source and target languages does have an impact on transfer quality. Further analysis of 72 language pairs reveals that for tasks that involve token classification such as entity and event trigger identification, common writing script and morphological features produce higher quality cross-lingual transfer. In contrast, for tasks involving structural prediction like argument extraction, common word order is the most relevant feature. In addition, we show that when increasing the training size, not all the languages scale in the same way in the cross-lingual setting. To perform the experiments we introduce EusIE, an event extraction dataset for Basque, which follows the Multilingual Event Extraction dataset (MEE). The dataset and code are publicly available.","sentences":["Cross-lingual transfer-learning is widely used in Event Extraction for low-resource languages and involves a Multilingual Language Model that is trained in a source language and applied to the target language.","This paper studies whether the typological similarity between source and target languages impacts the performance of cross-lingual transfer, an under-explored topic.","We first focus on Basque as the target language, which is an ideal target language because it is typologically different from surrounding languages.","Our experiments on three Event Extraction tasks show that the shared linguistic characteristic between source and target languages does have an impact on transfer quality.","Further analysis of 72 language pairs reveals that for tasks that involve token classification such as entity and event trigger identification, common writing script and morphological features produce higher quality cross-lingual transfer.","In contrast, for tasks involving structural prediction like argument extraction, common word order is the most relevant feature.","In addition, we show that when increasing the training size, not all the languages scale in the same way in the cross-lingual setting.","To perform the experiments we introduce EusIE, an event extraction dataset for Basque, which follows the Multilingual Event Extraction dataset (MEE).","The dataset and code are publicly available."],"url":"http://arxiv.org/abs/2404.06392v1","category":"cs.CL"}
{"created":"2024-04-09 15:06:25","title":"Enhancing Decision Analysis with a Large Language Model: pyDecision a Comprehensive Library of MCDA Methods in Python","abstract":"Purpose: Multicriteria decision analysis (MCDA) has become increasingly essential for decision-making in complex environments. In response to this need, the pyDecision library, implemented in Python and available at https://bit.ly/3tLFGtH, has been developed to provide a comprehensive and accessible collection of MCDA methods. Methods: The pyDecision offers 70 MCDA methods, including AHP, TOPSIS, and the PROMETHEE and ELECTRE families. Beyond offering a vast range of techniques, the library provides visualization tools for more intuitive results interpretation. In addition to these features, pyDecision has integrated ChatGPT, an advanced Large Language Model, where decision-makers can use ChatGPT to discuss and compare the outcomes of different methods, providing a more interactive and intuitive understanding of the solutions. Findings: Large Language Models are undeniably potent but can sometimes be a double-edged sword. Its answers may be misleading without rigorous verification of its outputs, especially for researchers lacking deep domain expertise. It's imperative to approach its insights with a discerning eye and a solid foundation in the relevant field. Originality: With the integration of MCDA methods and ChatGPT, pyDecision is a significant contribution to the scientific community, as it is an invaluable resource for researchers, practitioners, and decision-makers navigating complex decision-making problems and seeking the most appropriate solutions based on MCDA methods.","sentences":["Purpose:","Multicriteria decision analysis (MCDA) has become increasingly essential for decision-making in complex environments.","In response to this need, the pyDecision library, implemented in Python and available at https://bit.ly/3tLFGtH, has been developed to provide a comprehensive and accessible collection of MCDA methods.","Methods: The pyDecision offers 70 MCDA methods, including AHP, TOPSIS, and the PROMETHEE and ELECTRE families.","Beyond offering a vast range of techniques, the library provides visualization tools for more intuitive results interpretation.","In addition to these features, pyDecision has integrated ChatGPT, an advanced Large Language Model, where decision-makers can use ChatGPT to discuss and compare the outcomes of different methods, providing a more interactive and intuitive understanding of the solutions.","Findings:","Large Language Models are undeniably potent but can sometimes be a double-edged sword.","Its answers may be misleading without rigorous verification of its outputs, especially for researchers lacking deep domain expertise.","It's imperative to approach its insights with a discerning eye and a solid foundation in the relevant field.","Originality: With the integration of MCDA methods and ChatGPT, pyDecision is a significant contribution to the scientific community, as it is an invaluable resource for researchers, practitioners, and decision-makers navigating complex decision-making problems and seeking the most appropriate solutions based on MCDA methods."],"url":"http://arxiv.org/abs/2404.06370v1","category":"cs.AI"}
{"created":"2024-04-09 15:05:48","title":"VISION2UI: A Real-World Dataset with Layout for Code Generation from UI Designs","abstract":"Automatically generating UI code from webpage design visions can significantly alleviate the burden of developers, enabling beginner developers or designers to directly generate Web pages from design diagrams. Currently, prior research has accomplished the objective of generating UI code from rudimentary design visions or sketches through designing deep neural networks. Inspired by the groundbreaking advancements achieved by Multimodal Large Language Models (MLLMs), the automatic generation of UI code from high-fidelity design images is now emerging as a viable possibility. Nevertheless, our investigation reveals that existing MLLMs are hampered by the scarcity of authentic, high-quality, and large-scale datasets, leading to unsatisfactory performance in automated UI code generation. To mitigate this gap, we present a novel dataset, termed VISION2UI, extracted from real-world scenarios, augmented with comprehensive layout information, tailored specifically for finetuning MLLMs in UI code generation. Specifically, this dataset is derived through a series of operations, encompassing collecting, cleaning, and filtering of the open-source Common Crawl dataset. In order to uphold its quality, a neural scorer trained on labeled samples is utilized to refine the data, retaining higher-quality instances. Ultimately, this process yields a dataset comprising 2,000 (Much more is coming soon) parallel samples encompassing design visions and UI code. The dataset is available at https://huggingface.co/datasets/xcodemind/vision2ui.","sentences":["Automatically generating UI code from webpage design visions can significantly alleviate the burden of developers, enabling beginner developers or designers to directly generate Web pages from design diagrams.","Currently, prior research has accomplished the objective of generating UI code from rudimentary design visions or sketches through designing deep neural networks.","Inspired by the groundbreaking advancements achieved by Multimodal Large Language Models (MLLMs), the automatic generation of UI code from high-fidelity design images is now emerging as a viable possibility.","Nevertheless, our investigation reveals that existing MLLMs are hampered by the scarcity of authentic, high-quality, and large-scale datasets, leading to unsatisfactory performance in automated UI code generation.","To mitigate this gap, we present a novel dataset, termed VISION2UI, extracted from real-world scenarios, augmented with comprehensive layout information, tailored specifically for finetuning MLLMs in UI code generation.","Specifically, this dataset is derived through a series of operations, encompassing collecting, cleaning, and filtering of the open-source Common Crawl dataset.","In order to uphold its quality, a neural scorer trained on labeled samples is utilized to refine the data, retaining higher-quality instances.","Ultimately, this process yields a dataset comprising 2,000 (Much more is coming soon) parallel samples encompassing design visions and UI code.","The dataset is available at https://huggingface.co/datasets/xcodemind/vision2ui."],"url":"http://arxiv.org/abs/2404.06369v1","category":"cs.CV"}
{"created":"2024-04-09 14:56:34","title":"Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero shot Medical Image Segmentation","abstract":"The Segment Anything Model (SAM) and CLIP are remarkable vision foundation models (VFMs). SAM, a prompt driven segmentation model, excels in segmentation tasks across diverse domains, while CLIP is renowned for its zero shot recognition capabilities. However, their unified potential has not yet been explored in medical image segmentation. To adapt SAM to medical imaging, existing methods primarily rely on tuning strategies that require extensive data or prior prompts tailored to the specific task, making it particularly challenging when only a limited number of data samples are available. This work presents an in depth exploration of integrating SAM and CLIP into a unified framework for medical image segmentation. Specifically, we propose a simple unified framework, SaLIP, for organ segmentation. Initially, SAM is used for part based segmentation within the image, followed by CLIP to retrieve the mask corresponding to the region of interest (ROI) from the pool of SAM generated masks. Finally, SAM is prompted by the retrieved ROI to segment a specific organ. Thus, SaLIP is training and fine tuning free and does not rely on domain expertise or labeled data for prompt engineering. Our method shows substantial enhancements in zero shot segmentation, showcasing notable improvements in DICE scores across diverse segmentation tasks like brain (63.46%), lung (50.11%), and fetal head (30.82%), when compared to un prompted SAM. Code and text prompts will be available online.","sentences":["The Segment Anything Model (SAM) and CLIP are remarkable vision foundation models (VFMs).","SAM, a prompt driven segmentation model, excels in segmentation tasks across diverse domains, while CLIP is renowned for its zero shot recognition capabilities.","However, their unified potential has not yet been explored in medical image segmentation.","To adapt SAM to medical imaging, existing methods primarily rely on tuning strategies that require extensive data or prior prompts tailored to the specific task, making it particularly challenging when only a limited number of data samples are available.","This work presents an in depth exploration of integrating SAM and CLIP into a unified framework for medical image segmentation.","Specifically, we propose a simple unified framework, SaLIP, for organ segmentation.","Initially, SAM is used for part based segmentation within the image, followed by CLIP to retrieve the mask corresponding to the region of interest (ROI) from the pool of SAM generated masks.","Finally, SAM is prompted by the retrieved ROI to segment a specific organ.","Thus, SaLIP is training and fine tuning free and does not rely on domain expertise or labeled data for prompt engineering.","Our method shows substantial enhancements in zero shot segmentation, showcasing notable improvements in DICE scores across diverse segmentation tasks like brain (63.46%), lung (50.11%), and fetal head (30.82%), when compared to un prompted SAM.","Code and text prompts will be available online."],"url":"http://arxiv.org/abs/2404.06362v1","category":"cs.CV"}
{"created":"2024-04-09 14:46:48","title":"Policy-Guided Diffusion","abstract":"In many real-world settings, agents must learn from an offline dataset gathered by some prior behavior policy. Such a setting naturally leads to distribution shift between the behavior policy and the target policy being trained - requiring policy conservatism to avoid instability and overestimation bias. Autoregressive world models offer a different solution to this by generating synthetic, on-policy experience. However, in practice, model rollouts must be severely truncated to avoid compounding error. As an alternative, we propose policy-guided diffusion. Our method uses diffusion models to generate entire trajectories under the behavior distribution, applying guidance from the target policy to move synthetic experience further on-policy. We show that policy-guided diffusion models a regularized form of the target distribution that balances action likelihood under both the target and behavior policies, leading to plausible trajectories with high target policy probability, while retaining a lower dynamics error than an offline world model baseline. Using synthetic experience from policy-guided diffusion as a drop-in substitute for real data, we demonstrate significant improvements in performance across a range of standard offline reinforcement learning algorithms and environments. Our approach provides an effective alternative to autoregressive offline world models, opening the door to the controllable generation of synthetic training data.","sentences":["In many real-world settings, agents must learn from an offline dataset gathered by some prior behavior policy.","Such a setting naturally leads to distribution shift between the behavior policy and the target policy being trained - requiring policy conservatism to avoid instability and overestimation bias.","Autoregressive world models offer a different solution to this by generating synthetic, on-policy experience.","However, in practice, model rollouts must be severely truncated to avoid compounding error.","As an alternative, we propose policy-guided diffusion.","Our method uses diffusion models to generate entire trajectories under the behavior distribution, applying guidance from the target policy to move synthetic experience further on-policy.","We show that policy-guided diffusion models a regularized form of the target distribution that balances action likelihood under both the target and behavior policies, leading to plausible trajectories with high target policy probability, while retaining a lower dynamics error than an offline world model baseline.","Using synthetic experience from policy-guided diffusion as a drop-in substitute for real data, we demonstrate significant improvements in performance across a range of standard offline reinforcement learning algorithms and environments.","Our approach provides an effective alternative to autoregressive offline world models, opening the door to the controllable generation of synthetic training data."],"url":"http://arxiv.org/abs/2404.06356v1","category":"cs.LG"}
{"created":"2024-04-09 14:44:12","title":"High Noise Scheduling is a Must","abstract":"Consistency models possess high capabilities for image generation, advancing sampling steps to a single step through their advanced techniques. Current advancements move one step forward consistency training techniques and eliminates the limitation of distillation training. Even though the proposed curriculum and noise scheduling in improved training techniques yield better results than basic consistency models, it lacks well balanced noise distribution and its consistency between curriculum. In this study, it is investigated the balance between high and low noise levels in noise distribution and offered polynomial noise distribution to maintain the stability. This proposed polynomial noise distribution is also supported with a predefined Karras noises to prevent unique noise levels arises with Karras noise generation algorithm. Furthermore, by elimination of learned noisy steps with a curriculum based on sinusoidal function increase the performance of the model in denoising. To make a fair comparison with the latest released consistency model training techniques, experiments are conducted with same hyper-parameters except curriculum and noise distribution. The models utilized during experiments are determined with low depth to prove the robustness of our proposed technique. The results show that the polynomial noise distribution outperforms the model trained with log-normal noise distribution, yielding a 33.54 FID score after 100,000 training steps with constant discretization steps. Additionally, the implementation of a sinusoidal-based curriculum enhances denoising performance, resulting in a FID score of 30.48.","sentences":["Consistency models possess high capabilities for image generation, advancing sampling steps to a single step through their advanced techniques.","Current advancements move one step forward consistency training techniques and eliminates the limitation of distillation training.","Even though the proposed curriculum and noise scheduling in improved training techniques yield better results than basic consistency models, it lacks well balanced noise distribution and its consistency between curriculum.","In this study, it is investigated the balance between high and low noise levels in noise distribution and offered polynomial noise distribution to maintain the stability.","This proposed polynomial noise distribution is also supported with a predefined Karras noises to prevent unique noise levels arises with Karras noise generation algorithm.","Furthermore, by elimination of learned noisy steps with a curriculum based on sinusoidal function increase the performance of the model in denoising.","To make a fair comparison with the latest released consistency model training techniques, experiments are conducted with same hyper-parameters except curriculum and noise distribution.","The models utilized during experiments are determined with low depth to prove the robustness of our proposed technique.","The results show that the polynomial noise distribution outperforms the model trained with log-normal noise distribution, yielding a 33.54 FID score after 100,000 training steps with constant discretization steps.","Additionally, the implementation of a sinusoidal-based curriculum enhances denoising performance, resulting in a FID score of 30.48."],"url":"http://arxiv.org/abs/2404.06353v1","category":"cs.LG"}
{"created":"2024-04-09 14:33:16","title":"AgentsCoDriver: Large Language Model Empowered Collaborative Driving with Lifelong Learning","abstract":"Connected and autonomous driving is developing rapidly in recent years. However, current autonomous driving systems, which are primarily based on data-driven approaches, exhibit deficiencies in interpretability, generalization, and continuing learning capabilities. In addition, the single-vehicle autonomous driving systems lack of the ability of collaboration and negotiation with other vehicles, which is crucial for the safety and efficiency of autonomous driving systems. In order to address these issues, we leverage large language models (LLMs) to develop a novel framework, AgentsCoDriver, to enable multiple vehicles to conduct collaborative driving. AgentsCoDriver consists of five modules: observation module, reasoning engine, cognitive memory module, reinforcement reflection module, and communication module. It can accumulate knowledge, lessons, and experiences over time by continuously interacting with the environment, thereby making itself capable of lifelong learning. In addition, by leveraging the communication module, different agents can exchange information and realize negotiation and collaboration in complex traffic environments. Extensive experiments are conducted and show the superiority of AgentsCoDriver.","sentences":["Connected and autonomous driving is developing rapidly in recent years.","However, current autonomous driving systems, which are primarily based on data-driven approaches, exhibit deficiencies in interpretability, generalization, and continuing learning capabilities.","In addition, the single-vehicle autonomous driving systems lack of the ability of collaboration and negotiation with other vehicles, which is crucial for the safety and efficiency of autonomous driving systems.","In order to address these issues, we leverage large language models (LLMs) to develop a novel framework, AgentsCoDriver, to enable multiple vehicles to conduct collaborative driving.","AgentsCoDriver consists of five modules: observation module, reasoning engine, cognitive memory module, reinforcement reflection module, and communication module.","It can accumulate knowledge, lessons, and experiences over time by continuously interacting with the environment, thereby making itself capable of lifelong learning.","In addition, by leveraging the communication module, different agents can exchange information and realize negotiation and collaboration in complex traffic environments.","Extensive experiments are conducted and show the superiority of AgentsCoDriver."],"url":"http://arxiv.org/abs/2404.06345v1","category":"cs.AI"}
{"created":"2024-04-09 14:08:47","title":"Generative Pre-Trained Transformer for Symbolic Regression Base In-Context Reinforcement Learning","abstract":"The mathematical formula is the human language to describe nature and is the essence of scientific research. Finding mathematical formulas from observational data is a major demand of scientific research and a major challenge of artificial intelligence. This area is called symbolic regression. Originally symbolic regression was often formulated as a combinatorial optimization problem and solved using GP or reinforcement learning algorithms. These two kinds of algorithms have strong noise robustness ability and good Versatility. However, inference time usually takes a long time, so the search efficiency is relatively low. Later, based on large-scale pre-training data proposed, such methods use a large number of synthetic data points and expression pairs to train a Generative Pre-Trained Transformer(GPT). Then this GPT can only need to perform one forward propagation to obtain the results, the advantage is that the inference speed is very fast. However, its performance is very dependent on the training data and performs poorly on data outside the training set, which leads to poor noise robustness and Versatility of such methods. So, can we combine the advantages of the above two categories of SR algorithms? In this paper, we propose \\textbf{FormulaGPT}, which trains a GPT using massive sparse reward learning histories of reinforcement learning-based SR algorithms as training data. After training, the SR algorithm based on reinforcement learning is distilled into a Transformer. When new test data comes, FormulaGPT can directly generate a \"reinforcement learning process\" and automatically update the learning policy in context. Tested on more than ten datasets including SRBench, formulaGPT achieves the state-of-the-art performance in fitting ability compared with four baselines. In addition, it achieves satisfactory results in noise robustness, versatility, and inference efficiency.","sentences":["The mathematical formula is the human language to describe nature and is the essence of scientific research.","Finding mathematical formulas from observational data is a major demand of scientific research and a major challenge of artificial intelligence.","This area is called symbolic regression.","Originally symbolic regression was often formulated as a combinatorial optimization problem and solved using GP or reinforcement learning algorithms.","These two kinds of algorithms have strong noise robustness ability and good Versatility.","However, inference time usually takes a long time, so the search efficiency is relatively low.","Later, based on large-scale pre-training data proposed, such methods use a large number of synthetic data points and expression pairs to train a Generative Pre-Trained Transformer(GPT).","Then this GPT can only need to perform one forward propagation to obtain the results, the advantage is that the inference speed is very fast.","However, its performance is very dependent on the training data and performs poorly on data outside the training set, which leads to poor noise robustness and Versatility of such methods.","So, can we combine the advantages of the above two categories of SR algorithms?","In this paper, we propose \\textbf{FormulaGPT}, which trains a GPT using massive sparse reward learning histories of reinforcement learning-based SR algorithms as training data.","After training, the SR algorithm based on reinforcement learning is distilled into a Transformer.","When new test data comes, FormulaGPT can directly generate a \"reinforcement learning process\" and automatically update the learning policy in context.","Tested on more than ten datasets including SRBench, formulaGPT achieves the state-of-the-art performance in fitting ability compared with four baselines.","In addition, it achieves satisfactory results in noise robustness, versatility, and inference efficiency."],"url":"http://arxiv.org/abs/2404.06330v1","category":"cs.LG"}
{"created":"2024-04-09 14:04:26","title":"What is the $\\textit{intrinsic}$ dimension of your binary data? -- and how to compute it quickly","abstract":"Dimensionality is an important aspect for analyzing and understanding (high-dimensional) data. In their 2006 ICDM paper Tatti et al. answered the question for a (interpretable) dimension of binary data tables by introducing a normalized correlation dimension. In the present work we revisit their results and contrast them with a concept based notion of intrinsic dimension (ID) recently introduced for geometric data sets. To do this, we present a novel approximation for this ID that is based on computing concepts only up to a certain support value. We demonstrate and evaluate our approximation using all available datasets from Tatti et al., which have between 469 and 41271 extrinsic dimensions.","sentences":["Dimensionality is an important aspect for analyzing and understanding (high-dimensional) data.","In their 2006 ICDM paper Tatti et al. answered the question for a (interpretable) dimension of binary data tables by introducing a normalized correlation dimension.","In the present work we revisit their results and contrast them with a concept based notion of intrinsic dimension (ID) recently introduced for geometric data sets.","To do this, we present a novel approximation for this ID that is based on computing concepts only up to a certain support value.","We demonstrate and evaluate our approximation using all available datasets from Tatti et al., which have between 469 and 41271 extrinsic dimensions."],"url":"http://arxiv.org/abs/2404.06326v1","category":"cs.LG"}
{"created":"2024-04-09 14:03:38","title":"Automatically Learning HTN Methods from Landmarks","abstract":"Hierarchical Task Network (HTN) planning usually requires a domain engineer to provide manual input about how to decompose a planning problem. Even HTN-MAKER, a well-known method-learning algorithm, requires a domain engineer to annotate the tasks with information about what to learn. We introduce CURRICULAMA, an HTN method learning algorithm that completely automates the learning process. It uses landmark analysis to compose annotated tasks and leverages curriculum learning to order the learning of methods from simpler to more complex. This eliminates the need for manual input, resolving a core issue with HTN-MAKER. We prove CURRICULAMA's soundness, and show experimentally that it has a substantially similar convergence rate in learning a complete set of methods to HTN-MAKER.","sentences":["Hierarchical Task Network (HTN) planning usually requires a domain engineer to provide manual input about how to decompose a planning problem.","Even HTN-MAKER, a well-known method-learning algorithm, requires a domain engineer to annotate the tasks with information about what to learn.","We introduce CURRICULAMA, an HTN method learning algorithm that completely automates the learning process.","It uses landmark analysis to compose annotated tasks and leverages curriculum learning to order the learning of methods from simpler to more complex.","This eliminates the need for manual input, resolving a core issue with HTN-MAKER.","We prove CURRICULAMA's soundness, and show experimentally that it has a substantially similar convergence rate in learning a complete set of methods to HTN-MAKER."],"url":"http://arxiv.org/abs/2404.06325v1","category":"cs.AI"}
{"created":"2024-04-09 14:03:04","title":"Dynamic D2D-Assisted Federated Learning over O-RAN: Performance Analysis, MAC Scheduler, and Asymmetric User Selection","abstract":"Existing studies on federated learning (FL) are mostly focused on system orchestration for static snapshots of the network and making static control decisions (e.g., spectrum allocation). However, real-world wireless networks are susceptible to temporal variations of wireless channel capacity and users' datasets. In this paper, we incorporate multi-granular system dynamics (MSDs) into FL, including (M1) dynamic wireless channel capacity, captured by a set of discrete-time events, called $\\mathscr{D}$-Events, and (M2) dynamic datasets of users. The latter is characterized by (M2-a) modeling the dynamics of user's dataset size via an ordinary differential equation and (M2-b) introducing dynamic model drift}, formulated via a partial differential inequality} drawing concrete analytical connections between the dynamics of users' datasets and FL accuracy. We then conduct FL orchestration under MSDs by introducing dynamic cooperative FL with dedicated MAC schedulers (DCLM), exploiting the unique features of open radio access network (O-RAN). DCLM proposes (i) a hierarchical device-to-device (D2D)-assisted model training, (ii) dynamic control decisions through dedicated O-RAN MAC schedulers, and (iii) asymmetric user selection. We provide extensive theoretical analysis to study the convergence of DCLM. We then optimize the degrees of freedom (e.g., user selection and spectrum allocation) in DCLM through a highly non-convex optimization problem. We develop a systematic approach to obtain the solution for this problem, opening the door to solving a broad variety of network-aware FL optimization problems. We show the efficiency of DCLM via numerical simulations and provide a series of future directions.","sentences":["Existing studies on federated learning (FL) are mostly focused on system orchestration for static snapshots of the network and making static control decisions (e.g., spectrum allocation).","However, real-world wireless networks are susceptible to temporal variations of wireless channel capacity and users' datasets.","In this paper, we incorporate multi-granular system dynamics (MSDs) into FL, including (M1) dynamic wireless channel capacity, captured by a set of discrete-time events, called $\\mathscr{D}$-Events, and (M2) dynamic datasets of users.","The latter is characterized by (M2-a) modeling the dynamics of user's dataset size via an ordinary differential equation and (M2-b) introducing dynamic model drift}, formulated via a partial differential inequality} drawing concrete analytical connections between the dynamics of users' datasets and FL accuracy.","We then conduct FL orchestration under MSDs by introducing dynamic cooperative FL with dedicated MAC schedulers (DCLM), exploiting the unique features of open radio access network (O-RAN).","DCLM proposes (i) a hierarchical device-to-device (D2D)-assisted model training, (ii) dynamic control decisions through dedicated O-RAN MAC schedulers, and (iii) asymmetric user selection.","We provide extensive theoretical analysis to study the convergence of DCLM.","We then optimize the degrees of freedom (e.g., user selection and spectrum allocation) in DCLM through a highly non-convex optimization problem.","We develop a systematic approach to obtain the solution for this problem, opening the door to solving a broad variety of network-aware FL optimization problems.","We show the efficiency of DCLM via numerical simulations and provide a series of future directions."],"url":"http://arxiv.org/abs/2404.06324v1","category":"cs.NI"}
{"created":"2024-04-09 13:52:54","title":"Towards the wall-crossing of locally $\\mathbb{Q}_p$-analytic representations of $\\mathrm{GL}_n(K)$ for a $p$-adic field $K$","abstract":"Let $K$ be a finite extension of $\\mathbb{Q}_p$. We study the locally $\\mathbb{Q}_p$-analytic representations $\\pi$ of $\\mathrm{GL}_n(K)$ of integral weights that appear in spaces of $p$-adic automorphic representations. We conjecture that the translation of $\\pi$ to the singular block has an internal structure which is compatible with certain algebraic representations of $\\mathrm{GL}_n$, analogously to the mod $p$ local-global compatibility conjecture of Breuil-Herzig-Hu-Morra-Schraen. We next make some conjectures and speculations on the wall-crossings of $\\pi$. In particular, when $\\pi$ is associated to a two dimensional de Rham Galois representation, we make conjectures and speculations on the relation between the Hodge filtrations of $\\rho$ and the wall-crossings of $\\pi$, which have a flavour of the Breuil-Strauch conjecture. We collect some results towards the conjectures and speculations.","sentences":["Let $K$ be a finite extension of $\\mathbb{Q}_p$. We study the locally $\\mathbb{Q}_p$-analytic representations $\\pi$ of $\\mathrm{GL}_n(K)$ of integral weights that appear in spaces of $p$-adic automorphic representations.","We conjecture that the translation of $\\pi$ to the singular block has an internal structure which is compatible with certain algebraic representations of $\\mathrm{GL}_n$, analogously to the mod $p$ local-global compatibility conjecture of Breuil-Herzig-Hu-Morra-Schraen.","We next make some conjectures and speculations on the wall-crossings of $\\pi$. In particular, when $\\pi$ is associated to a two dimensional de Rham Galois representation, we make conjectures and speculations on the relation between the Hodge filtrations of $\\rho$ and the wall-crossings of $\\pi$, which have a flavour of the Breuil-Strauch conjecture.","We collect some results towards the conjectures and speculations."],"url":"http://arxiv.org/abs/2404.06315v1","category":"math.NT"}
{"created":"2024-04-09 13:18:44","title":"Computer-Assisted Global Analysis for Vibro-Impact Dynamics: A Reduced Smooth Maps Approach","abstract":"We present a novel approach for studying the global dynamics of a vibro-impact pair, that is, a ball moving in a harmonically forced capsule. Motivated by a specific context of vibro-impact energy harvesting, we develop the method with broader non-smooth systems in mind. The seeming complications of the impacts of the ball with the capsule are exploited as useful non-smooth features in selecting appropriate return maps. This choice yields a computationally efficient framework for constructing return maps on short-time realizations from the state space of possible initial conditions rather than via long-time simulations often used to generate more traditional maps. The different dynamics in sub-regions in the state space yield a small collection of reduced polynomial approximations. Combined into a piecewise composite map, these capture transient and attracting behaviors and reproduce bifurcation sequences of the full system. Further ``separable'' reductions of the composite map provide insight into both transient and global dynamics. This composite map is valuable for cobweb analysis, which opens the door to computer-assisted global analysis and is realized via conservative auxiliary maps based on the extreme bounds of the maps in each subregion. We study the global dynamics of energetically favorable states and illustrate the potential of this approach in broader classes of dynamics.","sentences":["We present a novel approach for studying the global dynamics of a vibro-impact pair, that is, a ball moving in a harmonically forced capsule.","Motivated by a specific context of vibro-impact energy harvesting, we develop the method with broader non-smooth systems in mind.","The seeming complications of the impacts of the ball with the capsule are exploited as useful non-smooth features in selecting appropriate return maps.","This choice yields a computationally efficient framework for constructing return maps on short-time realizations from the state space of possible initial conditions rather than via long-time simulations often used to generate more traditional maps.","The different dynamics in sub-regions in the state space yield a small collection of reduced polynomial approximations.","Combined into a piecewise composite map, these capture transient and attracting behaviors and reproduce bifurcation sequences of the full system.","Further ``separable'' reductions of the composite map provide insight into both transient and global dynamics.","This composite map is valuable for cobweb analysis, which opens the door to computer-assisted global analysis and is realized via conservative auxiliary maps based on the extreme bounds of the maps in each subregion.","We study the global dynamics of energetically favorable states and illustrate the potential of this approach in broader classes of dynamics."],"url":"http://arxiv.org/abs/2404.06291v1","category":"math.DS"}
{"created":"2024-04-09 13:17:28","title":"Exploring the True Potential: Evaluating the Black-box Optimization Capability of Large Language Models","abstract":"Large language models (LLMs) have gained widespread popularity and demonstrated exceptional performance not only in natural language processing (NLP) tasks but also in non-linguistic domains. Their potential as artificial general intelligence extends beyond NLP, showcasing promising capabilities in diverse optimization scenarios. Despite this rising trend, whether the integration of LLMs into these black-box optimization problems is genuinely beneficial remains unexplored. This paper endeavors to tackle this issue by offering deeper insights into the potential of LLMs in optimization tasks through a comprehensive investigation. Our approach involves a comprehensive evaluation, covering both discrete and continuous optimization problems, aiming to assess the efficacy and distinctive characteristics that LLMs bring to the realm of optimization. Our findings reveal both the limitations and advantages of LLMs in optimization. On one hand, despite consuming the significant power required to run the model, LLMs exhibit subpar performance and lack desirable properties in pure numerical tasks, primarily due to a mismatch between the problem domain and their processing capabilities. On the other hand, although LLMs may not be ideal for traditional numerical optimization, their potential in broader optimization contexts remains promising. LLMs exhibit the ability to solve problems in non-numerical domains and can leverage heuristics from the prompt to enhance their performance. To the best of our knowledge, this work presents the first systematic evaluation of LLMs for numerical optimization, offering a progressive, wide-coverage, and behavioral analysis. Our findings pave the way for a deeper understanding of LLMs' role in optimization and guide future application in diverse scenarios for LLMs.","sentences":["Large language models (LLMs) have gained widespread popularity and demonstrated exceptional performance not only in natural language processing (NLP) tasks but also in non-linguistic domains.","Their potential as artificial general intelligence extends beyond NLP, showcasing promising capabilities in diverse optimization scenarios.","Despite this rising trend, whether the integration of LLMs into these black-box optimization problems is genuinely beneficial remains unexplored.","This paper endeavors to tackle this issue by offering deeper insights into the potential of LLMs in optimization tasks through a comprehensive investigation.","Our approach involves a comprehensive evaluation, covering both discrete and continuous optimization problems, aiming to assess the efficacy and distinctive characteristics that LLMs bring to the realm of optimization.","Our findings reveal both the limitations and advantages of LLMs in optimization.","On one hand, despite consuming the significant power required to run the model, LLMs exhibit subpar performance and lack desirable properties in pure numerical tasks, primarily due to a mismatch between the problem domain and their processing capabilities.","On the other hand, although LLMs may not be ideal for traditional numerical optimization, their potential in broader optimization contexts remains promising.","LLMs exhibit the ability to solve problems in non-numerical domains and can leverage heuristics from the prompt to enhance their performance.","To the best of our knowledge, this work presents the first systematic evaluation of LLMs for numerical optimization, offering a progressive, wide-coverage, and behavioral analysis.","Our findings pave the way for a deeper understanding of LLMs' role in optimization and guide future application in diverse scenarios for LLMs."],"url":"http://arxiv.org/abs/2404.06290v1","category":"cs.NE"}
{"created":"2024-04-09 13:15:23","title":"Statistical Modelling of Driving Scenarios in Road Traffic using Fleet Data of Production Vehicles","abstract":"Ensuring the safety of road vehicles at an acceptable level requires the absence of any unreasonable risk arising from all potential hazards linked to the intended au-tomated driving function and its implementation. The assurance that there are no unreasonable risks stemming from hazardous behaviours associated to functional insufficiencies is denoted as safety of intended functionality (SOTIF), a concept outlined in the ISO 21448 standard. In this context, the acquisition of real driving data is considered essential for the verification and validation. For this purpose, we are currently developing a method with which data collect-ed representatively from production vehicles can be modelled into a knowledge-based system in the future. A system that represents the probabilities of occur-rence of concrete driving scenarios over the statistical population of road traffic and makes them usable. The method includes the qualitative and quantitative ab-straction of the drives recorded by the sensors in the vehicles, the possibility of subsequent wireless transmission of the abstracted data from the vehicles and the derivation of the distributions and correlations of scenario parameters. This paper provides a summary of the research project and outlines its central idea. To this end, among other things, the needs for statistical information and da-ta from road traffic are elaborated from ISO 21448, the current state of research is addressed, and methodical aspects are discussed.","sentences":["Ensuring the safety of road vehicles at an acceptable level requires the absence of any unreasonable risk arising from all potential hazards linked to the intended au-tomated driving function and its implementation.","The assurance that there are no unreasonable risks stemming from hazardous behaviours associated to functional insufficiencies is denoted as safety of intended functionality (SOTIF), a concept outlined in the ISO 21448 standard.","In this context, the acquisition of real driving data is considered essential for the verification and validation.","For this purpose, we are currently developing a method with which data collect-ed representatively from production vehicles can be modelled into a knowledge-based system in the future.","A system that represents the probabilities of occur-rence of concrete driving scenarios over the statistical population of road traffic and makes them usable.","The method includes the qualitative and quantitative ab-straction of the drives recorded by the sensors in the vehicles, the possibility of subsequent wireless transmission of the abstracted data from the vehicles and the derivation of the distributions and correlations of scenario parameters.","This paper provides a summary of the research project and outlines its central idea.","To this end, among other things, the needs for statistical information and da-ta from road traffic are elaborated from ISO 21448, the current state of research is addressed, and methodical aspects are discussed."],"url":"http://arxiv.org/abs/2404.06288v1","category":"cs.RO"}
{"created":"2024-04-09 13:12:44","title":"NR-V2X Quality of Service Prediction Through Machine Learning with Nested Cross-Validation Scheme","abstract":"The proliferation of connected vehicles and the advent of New Radio (NR) technologies have ushered in a new era of intelligent transportation systems. Ensuring reliable and lowlatency communication between vehicles and their surrounding environment is of utmost importance for the success of these systems. This paper presents a novel approach to predict Quality of Service (QoS) in Vehicle-to-Everything (V2X) communications through nested cross-validation. Our methodology employs several machine learning (ML) methods to predict some QoS metrics, such as packet delivery ratio (PDR), and throughput, in NR-based V2X scenarios. In ML employment, nested cross-validation approach, unlike conventional cross-validation approach, prevents information leakage from parameter selection into hyperparameter selection, and this results in getting more robust results in terms of overfitting. The study utilizes real-world NR-V2X datasets to train and validate the proposed ML methods. Through extensive experiments, we demonstrate the efficacy of our approach in accurately predicting QoS parameters, even in dynamic and challenging vehicular environments. In summary, our research contributes to the advancement of NR-based V2X communication systems by introducing employment of ML methods with a novel approach for QoS prediction. The combination of accurate predictions through nested cross-validation not only enhances the reliability of communication in connected vehicles' landscape but also has a supportive role for stakeholders to make informed decisions for the optimization and management of vehicular networks.","sentences":["The proliferation of connected vehicles and the advent of New Radio (NR) technologies have ushered in a new era of intelligent transportation systems.","Ensuring reliable and lowlatency communication between vehicles and their surrounding environment is of utmost importance for the success of these systems.","This paper presents a novel approach to predict Quality of Service (QoS) in Vehicle-to-Everything (V2X) communications through nested cross-validation.","Our methodology employs several machine learning (ML) methods to predict some QoS metrics, such as packet delivery ratio (PDR), and throughput, in NR-based V2X scenarios.","In ML employment, nested cross-validation approach, unlike conventional cross-validation approach, prevents information leakage from parameter selection into hyperparameter selection, and this results in getting more robust results in terms of overfitting.","The study utilizes real-world NR-V2X datasets to train and validate the proposed ML methods.","Through extensive experiments, we demonstrate the efficacy of our approach in accurately predicting QoS parameters, even in dynamic and challenging vehicular environments.","In summary, our research contributes to the advancement of NR-based V2X communication systems by introducing employment of ML methods with a novel approach for QoS prediction.","The combination of accurate predictions through nested cross-validation not only enhances the reliability of communication in connected vehicles' landscape but also has a supportive role for stakeholders to make informed decisions for the optimization and management of vehicular networks."],"url":"http://arxiv.org/abs/2404.06286v1","category":"cs.IT"}
{"created":"2024-04-09 13:11:27","title":"Coarse-grained quantum state tomography with optimal POVM construction","abstract":"Constructing an integrated large-scale qubit system of realistic size requires addressing the challenge of physical crowding among qubits. This constraint poses an issue of coarse-grained (CG) measurement, wherein information from the multi-qubit system is collectively gathered. In this work, we introduce a novel approach to reconstruct the target density matrix from a comprehensive set of Positive Operator-Valued Measures (POVM) using a Parameterized Quantum Circuit (PQC) under the constraint of CG measurement. We improve the robustness and stability of CG quantum state tomography (QST) by optimizing the POVM set to achieve a generalized symmetric informationally complete (GSIC) POVM through maximization of the von Neumann entropy. This optimized construction of CG-POVMs is scalable to an N-qubit system. We further discuss a more efficient construction of N-qubit CG-QST without exponential increases in two-qubit gates or circuit depth per measurement. Our scheme offers a viable pathway towards a detector-efficient large-scale solid-state embedded qubit platform by reconstructing crucial quantum information from collective measurements.","sentences":["Constructing an integrated large-scale qubit system of realistic size requires addressing the challenge of physical crowding among qubits.","This constraint poses an issue of coarse-grained (CG) measurement, wherein information from the multi-qubit system is collectively gathered.","In this work, we introduce a novel approach to reconstruct the target density matrix from a comprehensive set of Positive Operator-Valued Measures (POVM) using a Parameterized Quantum Circuit (PQC) under the constraint of CG measurement.","We improve the robustness and stability of CG quantum state tomography (QST) by optimizing the POVM set to achieve a generalized symmetric informationally complete (GSIC) POVM through maximization of the von Neumann entropy.","This optimized construction of CG-POVMs is scalable to an N-qubit system.","We further discuss a more efficient construction of N-qubit CG-QST without exponential increases in two-qubit gates or circuit depth per measurement.","Our scheme offers a viable pathway towards a detector-efficient large-scale solid-state embedded qubit platform by reconstructing crucial quantum information from collective measurements."],"url":"http://arxiv.org/abs/2404.06285v1","category":"quant-ph"}
{"created":"2024-04-09 13:02:33","title":"NoiseNCA: Noisy Seed Improves Spatio-Temporal Continuity of Neural Cellular Automata","abstract":"Neural Cellular Automata (NCA) is a class of Cellular Automata where the update rule is parameterized by a neural network that can be trained using gradient descent. In this paper, we focus on NCA models used for texture synthesis, where the update rule is inspired by partial differential equations (PDEs) describing reaction-diffusion systems. To train the NCA model, the spatio-termporal domain is discretized, and Euler integration is used to numerically simulate the PDE. However, whether a trained NCA truly learns the continuous dynamic described by the corresponding PDE or merely overfits the discretization used in training remains an open question. We study NCA models at the limit where space-time discretization approaches continuity. We find that existing NCA models tend to overfit the training discretization, especially in the proximity of the initial condition, also called \"seed\". To address this, we propose a solution that utilizes uniform noise as the initial condition. We demonstrate the effectiveness of our approach in preserving the consistency of NCA dynamics across a wide range of spatio-temporal granularities. Our improved NCA model enables two new test-time interactions by allowing continuous control over the speed of pattern formation and the scale of the synthesized patterns. We demonstrate this new NCA feature in our interactive online demo. Our work reveals that NCA models can learn continuous dynamics and opens new venues for NCA research from a dynamical systems' perspective.","sentences":["Neural Cellular Automata (NCA) is a class of Cellular Automata where the update rule is parameterized by a neural network that can be trained using gradient descent.","In this paper, we focus on NCA models used for texture synthesis, where the update rule is inspired by partial differential equations (PDEs) describing reaction-diffusion systems.","To train the NCA model, the spatio-termporal domain is discretized, and Euler integration is used to numerically simulate the PDE.","However, whether a trained NCA truly learns the continuous dynamic described by the corresponding PDE or merely overfits the discretization used in training remains an open question.","We study NCA models at the limit where space-time discretization approaches continuity.","We find that existing NCA models tend to overfit the training discretization, especially in the proximity of the initial condition, also called \"seed\".","To address this, we propose a solution that utilizes uniform noise as the initial condition.","We demonstrate the effectiveness of our approach in preserving the consistency of NCA dynamics across a wide range of spatio-temporal granularities.","Our improved NCA model enables two new test-time interactions by allowing continuous control over the speed of pattern formation and the scale of the synthesized patterns.","We demonstrate this new NCA feature in our interactive online demo.","Our work reveals that NCA models can learn continuous dynamics and opens new venues for NCA research from a dynamical systems' perspective."],"url":"http://arxiv.org/abs/2404.06279v1","category":"cs.CV"}
{"created":"2024-04-09 13:02:22","title":"Dimensionality Reduction in Sentence Transformer Vector Databases with Fast Fourier Transform","abstract":"Dimensionality reduction in vector databases is pivotal for streamlining AI data management, enabling efficient storage, faster computation, and improved model performance. This paper explores the benefits of reducing vector database dimensions, with a focus on computational efficiency and overcoming the curse of dimensionality. We introduce a novel application of Fast Fourier Transform (FFT) to dimensionality reduction, a method previously underexploited in this context. By demonstrating its utility across various AI domains, including Retrieval-Augmented Generation (RAG) models and image processing, this FFT-based approach promises to improve data retrieval processes and enhance the efficiency and scalability of AI solutions. The incorporation of FFT may not only optimize operations in real-time processing and recommendation systems but also extend to advanced image processing techniques, where dimensionality reduction can significantly improve performance and analysis efficiency. This paper advocates for the broader adoption of FFT in vector database management, marking a significant stride towards addressing the challenges of data volume and complexity in AI research and applications. Unlike many existing approaches, we directly handle the embedding vectors produced by the model after processing a test input.","sentences":["Dimensionality reduction in vector databases is pivotal for streamlining AI data management, enabling efficient storage, faster computation, and improved model performance.","This paper explores the benefits of reducing vector database dimensions, with a focus on computational efficiency and overcoming the curse of dimensionality.","We introduce a novel application of Fast Fourier Transform (FFT) to dimensionality reduction, a method previously underexploited in this context.","By demonstrating its utility across various AI domains, including Retrieval-Augmented Generation (RAG) models and image processing, this FFT-based approach promises to improve data retrieval processes and enhance the efficiency and scalability of AI solutions.","The incorporation of FFT may not only optimize operations in real-time processing and recommendation systems but also extend to advanced image processing techniques, where dimensionality reduction can significantly improve performance and analysis efficiency.","This paper advocates for the broader adoption of FFT in vector database management, marking a significant stride towards addressing the challenges of data volume and complexity in AI research and applications.","Unlike many existing approaches, we directly handle the embedding vectors produced by the model after processing a test input."],"url":"http://arxiv.org/abs/2404.06278v1","category":"cs.DB"}
{"created":"2024-04-09 12:47:30","title":"3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis","abstract":"In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   The project is available at https://npucvr.github.io/GaGS/","sentences":["In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis.","Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry.","Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction.","Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation.","Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation.","To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation.","In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction.","Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   ","The project is available at https://npucvr.github.io/GaGS/"],"url":"http://arxiv.org/abs/2404.06270v1","category":"cs.CV"}
{"created":"2024-04-09 12:45:32","title":"Kostka polynomials of $G(\\ell,1,m)$","abstract":"For each integer $n, m, \\ell \\ge 1$ such that $n > m$, we exhibit an equivalence between the category of polynomial modules over a paraholic subalgebra $\\mathfrak p$ of an affine Lie algebra of $\\mathfrak{gl}(n\\ell)$ and the module category of the smash product algebra $A$ of the complex reflection group $G(\\ell,1,m)$ with $\\mathbb C [X_1,\\ldots,X_m]$. Then, we transfer the collection of $\\mathfrak p$-modules considered in [Feigin-Makedonskyi-Khoroshkhin, arXiv:2311.12673] to $A$. Applying the Lusztig-Shoji algorithm [Shoji, Invent. Math. 74 (1983)] (or rather its homological variant [K. Ann. Sci. ENS 48(5) (2015)]), we conclude that the multiplicity counts of these modules yield the Kostka polynomials attached to limit symbols in the sense of [Shoji, ASPM 40 (2004)]. This particularly settles a conjecture of Shoji [loc. cit. \\S 3.13] and answers a question in [Shoji, Sci. China Math. 61 (2018)].","sentences":["For each integer $n, m, \\ell \\ge 1$ such that $n > m$, we exhibit an equivalence between the category of polynomial modules over a paraholic subalgebra $\\mathfrak p$ of an affine Lie algebra of $\\mathfrak{gl}(n\\ell)$ and the module category of the smash product algebra $A$ of the complex reflection group $G(\\ell,1,m)$ with $\\mathbb C","[X_1,\\ldots,X_m]$.","Then, we transfer the collection of $\\mathfrak p$-modules considered in [Feigin-Makedonskyi-Khoroshkhin, arXiv:2311.12673] to $A$.","Applying the Lusztig-Shoji algorithm","[Shoji, Invent.","Math. 74 (1983)] (or rather its homological variant [K. Ann.","Sci.","ENS 48(5) (2015)]), we conclude that the multiplicity counts of these modules yield the Kostka polynomials attached to limit symbols in the sense of [Shoji, ASPM 40 (2004)].","This particularly settles a conjecture of Shoji [loc. cit.","\\S 3.13] and answers a question in [Shoji, Sci.","China Math. 61 (2018)]."],"url":"http://arxiv.org/abs/2404.06268v1","category":"math.RT"}
{"created":"2024-04-09 12:45:17","title":"PGTNet: A Process Graph Transformer Network for Remaining Time Prediction of Business Process Instances","abstract":"We present PGTNet, an approach that transforms event logs into graph datasets and leverages graph-oriented data for training Process Graph Transformer Networks to predict the remaining time of business process instances. PGTNet consistently outperforms state-of-the-art deep learning approaches across a diverse range of 20 publicly available real-world event logs. Notably, our approach is most promising for highly complex processes, where existing deep learning approaches encounter difficulties stemming from their limited ability to learn control-flow relationships among process activities and capture long-range dependencies. PGTNet addresses these challenges, while also being able to consider multiple process perspectives during the learning process.","sentences":["We present PGTNet, an approach that transforms event logs into graph datasets and leverages graph-oriented data for training Process Graph Transformer Networks to predict the remaining time of business process instances.","PGTNet consistently outperforms state-of-the-art deep learning approaches across a diverse range of 20 publicly available real-world event logs.","Notably, our approach is most promising for highly complex processes, where existing deep learning approaches encounter difficulties stemming from their limited ability to learn control-flow relationships among process activities and capture long-range dependencies.","PGTNet addresses these challenges, while also being able to consider multiple process perspectives during the learning process."],"url":"http://arxiv.org/abs/2404.06267v1","category":"cs.LG"}
{"created":"2024-04-09 12:34:28","title":"Playing to Vision Foundation Model's Strengths in Stereo Matching","abstract":"Stereo matching has become a key technique for 3D environment perception in intelligent vehicles. For a considerable time, convolutional neural networks (CNNs) have remained the mainstream choice for feature extraction in this domain. Nonetheless, there is a growing consensus that the existing paradigm should evolve towards vision foundation models (VFM), particularly those developed based on vision Transformers (ViTs) and pre-trained through self-supervision on extensive, unlabeled datasets. While VFMs are adept at extracting informative, general-purpose visual features, specifically for dense prediction tasks, their performance often lacks in geometric vision tasks. This study serves as the first exploration of a viable approach for adapting VFMs to stereo matching. Our ViT adapter, referred to as ViTAS, is constructed upon three types of modules: spatial differentiation, patch attention fusion, and cross-attention. The first module initializes feature pyramids, while the latter two aggregate stereo and multi-scale contextual information into fine-grained features, respectively. ViTAStereo, which combines ViTAS with cost volume-based stereo matching back-end processes, achieves the top rank on the KITTI Stereo 2012 dataset and outperforms the second-best network StereoBase by approximately 7.9% in terms of the percentage of error pixels, with a tolerance of 3 pixels. Additional experiments across diverse scenarios further demonstrate its superior generalizability compared to all other state-of-the-art approaches. We believe this new paradigm will pave the way for the next generation of stereo matching networks.","sentences":["Stereo matching has become a key technique for 3D environment perception in intelligent vehicles.","For a considerable time, convolutional neural networks (CNNs) have remained the mainstream choice for feature extraction in this domain.","Nonetheless, there is a growing consensus that the existing paradigm should evolve towards vision foundation models (VFM), particularly those developed based on vision Transformers (ViTs) and pre-trained through self-supervision on extensive, unlabeled datasets.","While VFMs are adept at extracting informative, general-purpose visual features, specifically for dense prediction tasks, their performance often lacks in geometric vision tasks.","This study serves as the first exploration of a viable approach for adapting VFMs to stereo matching.","Our ViT adapter, referred to as ViTAS, is constructed upon three types of modules: spatial differentiation, patch attention fusion, and cross-attention.","The first module initializes feature pyramids, while the latter two aggregate stereo and multi-scale contextual information into fine-grained features, respectively. ViTAStereo, which combines ViTAS with cost volume-based stereo matching back-end processes, achieves the top rank on the KITTI Stereo 2012 dataset and outperforms the second-best network StereoBase by approximately 7.9% in terms of the percentage of error pixels, with a tolerance of 3 pixels.","Additional experiments across diverse scenarios further demonstrate its superior generalizability compared to all other state-of-the-art approaches.","We believe this new paradigm will pave the way for the next generation of stereo matching networks."],"url":"http://arxiv.org/abs/2404.06261v1","category":"cs.CV"}
{"created":"2024-04-09 12:29:16","title":"Label-Efficient 3D Object Detection For Road-Side Units","abstract":"Occlusion presents a significant challenge for safety-critical applications such as autonomous driving. Collaborative perception has recently attracted a large research interest thanks to the ability to enhance the perception of autonomous vehicles via deep information fusion with intelligent roadside units (RSU), thus minimizing the impact of occlusion. While significant advancement has been made, the data-hungry nature of these methods creates a major hurdle for their real-world deployment, particularly due to the need for annotated RSU data. Manually annotating the vast amount of RSU data required for training is prohibitively expensive, given the sheer number of intersections and the effort involved in annotating point clouds. We address this challenge by devising a label-efficient object detection method for RSU based on unsupervised object discovery. Our paper introduces two new modules: one for object discovery based on a spatial-temporal aggregation of point clouds, and another for refinement. Furthermore, we demonstrate that fine-tuning on a small portion of annotated data allows our object discovery models to narrow the performance gap with, or even surpass, fully supervised models. Extensive experiments are carried out in simulated and real-world datasets to evaluate our method.","sentences":["Occlusion presents a significant challenge for safety-critical applications such as autonomous driving.","Collaborative perception has recently attracted a large research interest thanks to the ability to enhance the perception of autonomous vehicles via deep information fusion with intelligent roadside units (RSU), thus minimizing the impact of occlusion.","While significant advancement has been made, the data-hungry nature of these methods creates a major hurdle for their real-world deployment, particularly due to the need for annotated RSU data.","Manually annotating the vast amount of RSU data required for training is prohibitively expensive, given the sheer number of intersections and the effort involved in annotating point clouds.","We address this challenge by devising a label-efficient object detection method for RSU based on unsupervised object discovery.","Our paper introduces two new modules: one for object discovery based on a spatial-temporal aggregation of point clouds, and another for refinement.","Furthermore, we demonstrate that fine-tuning on a small portion of annotated data allows our object discovery models to narrow the performance gap with, or even surpass, fully supervised models.","Extensive experiments are carried out in simulated and real-world datasets to evaluate our method."],"url":"http://arxiv.org/abs/2404.06256v1","category":"cs.CV"}
{"created":"2024-04-09 12:11:25","title":"GHNeRF: Learning Generalizable Human Features with Efficient Neural Radiance Fields","abstract":"Recent advances in Neural Radiance Fields (NeRF) have demonstrated promising results in 3D scene representations, including 3D human representations. However, these representations often lack crucial information on the underlying human pose and structure, which is crucial for AR/VR applications and games. In this paper, we introduce a novel approach, termed GHNeRF, designed to address these limitations by learning 2D/3D joint locations of human subjects with NeRF representation. GHNeRF uses a pre-trained 2D encoder streamlined to extract essential human features from 2D images, which are then incorporated into the NeRF framework in order to encode human biomechanic features. This allows our network to simultaneously learn biomechanic features, such as joint locations, along with human geometry and texture. To assess the effectiveness of our method, we conduct a comprehensive comparison with state-of-the-art human NeRF techniques and joint estimation algorithms. Our results show that GHNeRF can achieve state-of-the-art results in near real-time.","sentences":["Recent advances in Neural Radiance Fields (NeRF) have demonstrated promising results in 3D scene representations, including 3D human representations.","However, these representations often lack crucial information on the underlying human pose and structure, which is crucial for AR/VR applications and games.","In this paper, we introduce a novel approach, termed GHNeRF, designed to address these limitations by learning 2D/3D joint locations of human subjects with NeRF representation.","GHNeRF uses a pre-trained 2D encoder streamlined to extract essential human features from 2D images, which are then incorporated into the NeRF framework in order to encode human biomechanic features.","This allows our network to simultaneously learn biomechanic features, such as joint locations, along with human geometry and texture.","To assess the effectiveness of our method, we conduct a comprehensive comparison with state-of-the-art human NeRF techniques and joint estimation algorithms.","Our results show that GHNeRF can achieve state-of-the-art results in near real-time."],"url":"http://arxiv.org/abs/2404.06246v1","category":"cs.CV"}
{"created":"2024-04-09 12:09:56","title":"ActNetFormer: Transformer-ResNet Hybrid Method for Semi-Supervised Action Recognition in Videos","abstract":"Human action or activity recognition in videos is a fundamental task in computer vision with applications in surveillance and monitoring, self-driving cars, sports analytics, human-robot interaction and many more. Traditional supervised methods require large annotated datasets for training, which are expensive and time-consuming to acquire. This work proposes a novel approach using Cross-Architecture Pseudo-Labeling with contrastive learning for semi-supervised action recognition. Our framework leverages both labeled and unlabelled data to robustly learn action representations in videos, combining pseudo-labeling with contrastive learning for effective learning from both types of samples. We introduce a novel cross-architecture approach where 3D Convolutional Neural Networks (3D CNNs) and video transformers (VIT) are utilised to capture different aspects of action representations; hence we call it ActNetFormer. The 3D CNNs excel at capturing spatial features and local dependencies in the temporal domain, while VIT excels at capturing long-range dependencies across frames. By integrating these complementary architectures within the ActNetFormer framework, our approach can effectively capture both local and global contextual information of an action. This comprehensive representation learning enables the model to achieve better performance in semi-supervised action recognition tasks by leveraging the strengths of each of these architectures. Experimental results on standard action recognition datasets demonstrate that our approach performs better than the existing methods, achieving state-of-the-art performance with only a fraction of labeled data. The official website of this work is available at: https://github.com/rana2149/ActNetFormer.","sentences":["Human action or activity recognition in videos is a fundamental task in computer vision with applications in surveillance and monitoring, self-driving cars, sports analytics, human-robot interaction and many more.","Traditional supervised methods require large annotated datasets for training, which are expensive and time-consuming to acquire.","This work proposes a novel approach using Cross-Architecture Pseudo-Labeling with contrastive learning for semi-supervised action recognition.","Our framework leverages both labeled and unlabelled data to robustly learn action representations in videos, combining pseudo-labeling with contrastive learning for effective learning from both types of samples.","We introduce a novel cross-architecture approach where 3D Convolutional Neural Networks (3D CNNs) and video transformers (VIT) are utilised to capture different aspects of action representations; hence we call it ActNetFormer.","The 3D CNNs excel at capturing spatial features and local dependencies in the temporal domain, while VIT excels at capturing long-range dependencies across frames.","By integrating these complementary architectures within the ActNetFormer framework, our approach can effectively capture both local and global contextual information of an action.","This comprehensive representation learning enables the model to achieve better performance in semi-supervised action recognition tasks by leveraging the strengths of each of these architectures.","Experimental results on standard action recognition datasets demonstrate that our approach performs better than the existing methods, achieving state-of-the-art performance with only a fraction of labeled data.","The official website of this work is available at: https://github.com/rana2149/ActNetFormer."],"url":"http://arxiv.org/abs/2404.06243v1","category":"cs.CV"}
{"created":"2024-04-09 12:08:51","title":"Redshift drift in a universe with structure III: Numerical relativity","abstract":"Measurements of the cosmic redshift drift - the change in redshift of a source over time - will enable independent detection of cosmological expansion thanks to the immense precision soon reached by new facilities such as the Square Kilometer Array Observatory and the Extremely Large Telescope. We conduct the first ever redshift drift computation in fully relativistic cosmological simulations, with the simulations performed with the Einstein Toolkit. We compute the redshift drift over the full skies of 50 synthetic observers in the simulation. We compare all-sky averages for each observer - and across all observers - to the Einstein-de Sitter (EdS) model which represents the large-scale spatially-averaged spacetime of the simulation. We find that at $z\\approx0.2$ the mean redshift drift across the sky for all observers deviates from the EdS prediction at the percent level, reducing to $\\sim0.1\\%$ by $z\\approx 1$. However, fluctuations in the redshift drift across the sky are $\\sim 10-30\\%$ at $z\\approx 0.1$ and a few percent at $z\\approx 0.5$. Such fluctuations are large enough to potentially exceed the expected precision of upcoming redshift drift measurements. Additionally, we find that along 0.48% of the light rays the redshift drift becomes temporarily positive at very low redshift of $z\\lesssim 0.02$. This occurs despite our simulation data being based on a matter-dominated model universe. By including a cosmological constant, we expect a slower growth of structures than in the leading-order EdS space-time, and this may reduce the anisotropy over the observers' skies, although we generally expect our results to hold as order-of-magnitude estimates. Redshift drift is arguably one of the most important measurements to be made by next-generation telescopes. Our results collectively serve as preparation for interpreting such a measurement in the presence of realistic cosmic structures.","sentences":["Measurements of the cosmic redshift drift - the change in redshift of a source over time - will enable independent detection of cosmological expansion thanks to the immense precision soon reached by new facilities such as the Square Kilometer Array Observatory and the Extremely Large Telescope.","We conduct the first ever redshift drift computation in fully relativistic cosmological simulations, with the simulations performed with the Einstein Toolkit.","We compute the redshift drift over the full skies of 50 synthetic observers in the simulation.","We compare all-sky averages for each observer - and across all observers - to the Einstein-de Sitter (EdS) model which represents the large-scale spatially-averaged spacetime of the simulation.","We find that at $z\\approx0.2$ the mean redshift drift across the sky for all observers deviates from the EdS prediction at the percent level, reducing to $\\sim0.1\\%$ by $z\\approx 1$.","However, fluctuations in the redshift drift across the sky are $\\sim 10-30\\%$ at $z\\approx 0.1$ and a few percent at $z\\approx 0.5$.","Such fluctuations are large enough to potentially exceed the expected precision of upcoming redshift drift measurements.","Additionally, we find that along 0.48% of the light rays the redshift drift becomes temporarily positive at very low redshift of $z\\lesssim 0.02$.","This occurs despite our simulation data being based on a matter-dominated model universe.","By including a cosmological constant, we expect a slower growth of structures than in the leading-order EdS space-time, and this may reduce the anisotropy over the observers' skies, although we generally expect our results to hold as order-of-magnitude estimates.","Redshift drift is arguably one of the most important measurements to be made by next-generation telescopes.","Our results collectively serve as preparation for interpreting such a measurement in the presence of realistic cosmic structures."],"url":"http://arxiv.org/abs/2404.06242v1","category":"astro-ph.CO"}
{"created":"2024-04-09 11:40:37","title":"Towards Autonomous Driving with Small-Scale Cars: A Survey of Recent Development","abstract":"While engaging with the unfolding revolution in autonomous driving, a challenge presents itself, how can we effectively raise awareness within society about this transformative trend? While full-scale autonomous driving vehicles often come with a hefty price tag, the emergence of small-scale car platforms offers a compelling alternative. These platforms not only serve as valuable educational tools for the broader public and young generations but also function as robust research platforms, contributing significantly to the ongoing advancements in autonomous driving technology. This survey outlines various small-scale car platforms, categorizing them and detailing the research advancements accomplished through their usage. The conclusion provides proposals for promising future directions in the field.","sentences":["While engaging with the unfolding revolution in autonomous driving, a challenge presents itself, how can we effectively raise awareness within society about this transformative trend?","While full-scale autonomous driving vehicles often come with a hefty price tag, the emergence of small-scale car platforms offers a compelling alternative.","These platforms not only serve as valuable educational tools for the broader public and young generations but also function as robust research platforms, contributing significantly to the ongoing advancements in autonomous driving technology.","This survey outlines various small-scale car platforms, categorizing them and detailing the research advancements accomplished through their usage.","The conclusion provides proposals for promising future directions in the field."],"url":"http://arxiv.org/abs/2404.06229v1","category":"cs.RO"}
{"created":"2024-04-09 11:36:08","title":"Multimodal Road Network Generation Based on Large Language Model","abstract":"With the increasing popularity of ChatGPT, large language models (LLMs) have demonstrated their capabilities in communication and reasoning, promising for transportation sector intelligentization. However, they still face challenges in domain-specific knowledge. This paper aims to leverage LLMs' reasoning and recognition abilities to replace traditional user interfaces and create an \"intelligent operating system\" for transportation simulation software, exploring their potential with transportation modeling and simulation. We introduce Network Generation AI (NGAI), integrating LLMs with road network modeling plugins, validated through experiments for accuracy and robustness. NGAI's effective use has reduced modeling costs, revolutionized transportation simulations, optimized user steps, and proposed a novel approach for LLM integration in the transportation field.","sentences":["With the increasing popularity of ChatGPT, large language models (LLMs) have demonstrated their capabilities in communication and reasoning, promising for transportation sector intelligentization.","However, they still face challenges in domain-specific knowledge.","This paper aims to leverage LLMs' reasoning and recognition abilities to replace traditional user interfaces and create an \"intelligent operating system\" for transportation simulation software, exploring their potential with transportation modeling and simulation.","We introduce Network Generation AI (NGAI), integrating LLMs with road network modeling plugins, validated through experiments for accuracy and robustness.","NGAI's effective use has reduced modeling costs, revolutionized transportation simulations, optimized user steps, and proposed a novel approach for LLM integration in the transportation field."],"url":"http://arxiv.org/abs/2404.06227v1","category":"cs.HC"}
{"created":"2024-04-09 11:26:59","title":"Low-Cost Generation and Evaluation of Dictionary Example Sentences","abstract":"Dictionary example sentences play an important role in illustrating word definitions and usage, but manually creating quality sentences is challenging. Prior works have demonstrated that language models can be trained to generate example sentences. However, they relied on costly customized models and word sense datasets for generation and evaluation of their work. Rapid advancements in foundational models present the opportunity to create low-cost, zero-shot methods for the generation and evaluation of dictionary example sentences. We introduce a new automatic evaluation metric called OxfordEval that measures the win-rate of generated sentences against existing Oxford Dictionary sentences. OxfordEval shows high alignment with human judgments, enabling large-scale automated quality evaluation. We experiment with various LLMs and configurations to generate dictionary sentences across word classes. We complement this with a novel approach of using masked language models to identify and select sentences that best exemplify word meaning. The eventual model, FM-MLM, achieves over 85.1% win rate against Oxford baseline sentences according to OxfordEval, compared to 39.8% win rate for prior model-generated sentences.","sentences":["Dictionary example sentences play an important role in illustrating word definitions and usage, but manually creating quality sentences is challenging.","Prior works have demonstrated that language models can be trained to generate example sentences.","However, they relied on costly customized models and word sense datasets for generation and evaluation of their work.","Rapid advancements in foundational models present the opportunity to create low-cost, zero-shot methods for the generation and evaluation of dictionary example sentences.","We introduce a new automatic evaluation metric called OxfordEval that measures the win-rate of generated sentences against existing Oxford Dictionary sentences.","OxfordEval shows high alignment with human judgments, enabling large-scale automated quality evaluation.","We experiment with various LLMs and configurations to generate dictionary sentences across word classes.","We complement this with a novel approach of using masked language models to identify and select sentences that best exemplify word meaning.","The eventual model, FM-MLM, achieves over 85.1% win rate against Oxford baseline sentences according to OxfordEval, compared to 39.8% win rate for prior model-generated sentences."],"url":"http://arxiv.org/abs/2404.06224v1","category":"cs.CL"}
{"created":"2024-04-09 11:00:19","title":"OmniFusion Technical Report","abstract":"Last year, multimodal architectures served up a revolution in AI-based approaches and solutions, extending the capabilities of large language models (LLM). We propose an \\textit{OmniFusion} model based on a pretrained LLM and adapters for visual modality. We evaluated and compared several architecture design principles for better text and visual data coupling: MLP and transformer adapters, various CLIP ViT-based encoders (SigLIP, InternVIT, etc.), and their fusing approach, image encoding method (whole image or tiles encoding) and two 7B LLMs (the proprietary one and open-source Mistral). Experiments on 8 visual-language benchmarks show the top score for the best OmniFusion setup in terms of different VQA tasks in comparison with open-source LLaVA-like solutions: VizWiz, Pope, MM-Vet, ScienceQA, MMBench, TextVQA, VQAv2, MMMU. We also propose a variety of situations, where OmniFusion provides highly-detailed answers in different domains: housekeeping, sightseeing, culture, medicine, handwritten and scanned equations recognition, etc. Mistral-based OmniFusion model is an open-source solution with weights, training and inference scripts available at https://github.com/AIRI-Institute/OmniFusion.","sentences":["Last year, multimodal architectures served up a revolution in AI-based approaches and solutions, extending the capabilities of large language models (LLM).","We propose an \\textit{OmniFusion} model based on a pretrained LLM and adapters for visual modality.","We evaluated and compared several architecture design principles for better text and visual data coupling: MLP and transformer adapters, various CLIP ViT-based encoders (SigLIP, InternVIT, etc.), and their fusing approach, image encoding method (whole image or tiles encoding) and two 7B LLMs (the proprietary one and open-source Mistral).","Experiments on 8 visual-language benchmarks show the top score for the best OmniFusion setup in terms of different VQA tasks in comparison with open-source LLaVA-like solutions: VizWiz, Pope, MM-Vet, ScienceQA, MMBench, TextVQA, VQAv2, MMMU.","We also propose a variety of situations, where OmniFusion provides highly-detailed answers in different domains: housekeeping, sightseeing, culture, medicine, handwritten and scanned equations recognition, etc. Mistral-based OmniFusion model is an open-source solution with weights, training and inference scripts available at https://github.com/AIRI-Institute/OmniFusion."],"url":"http://arxiv.org/abs/2404.06212v1","category":"cs.CV"}
{"created":"2024-04-09 11:00:11","title":"Unified Physical-Digital Attack Detection Challenge","abstract":"Face Anti-Spoofing (FAS) is crucial to safeguard Face Recognition (FR) Systems. In real-world scenarios, FRs are confronted with both physical and digital attacks. However, existing algorithms often address only one type of attack at a time, which poses significant limitations in real-world scenarios where FR systems face hybrid physical-digital threats. To facilitate the research of Unified Attack Detection (UAD) algorithms, a large-scale UniAttackData dataset has been collected. UniAttackData is the largest public dataset for Unified Attack Detection, with a total of 28,706 videos, where each unique identity encompasses all advanced attack types. Based on this dataset, we organized a Unified Physical-Digital Face Attack Detection Challenge to boost the research in Unified Attack Detections. It attracted 136 teams for the development phase, with 13 qualifying for the final round. The results re-verified by the organizing team were used for the final ranking. This paper comprehensively reviews the challenge, detailing the dataset introduction, protocol definition, evaluation criteria, and a summary of published results. Finally, we focus on the detailed analysis of the highest-performing algorithms and offer potential directions for unified physical-digital attack detection inspired by this competition. Challenge Website: https://sites.google.com/view/face-anti-spoofing-challenge/welcome/challengecvpr2024.","sentences":["Face Anti-Spoofing (FAS) is crucial to safeguard Face Recognition (FR) Systems.","In real-world scenarios, FRs are confronted with both physical and digital attacks.","However, existing algorithms often address only one type of attack at a time, which poses significant limitations in real-world scenarios where FR systems face hybrid physical-digital threats.","To facilitate the research of Unified Attack Detection (UAD) algorithms, a large-scale UniAttackData dataset has been collected.","UniAttackData is the largest public dataset for Unified Attack Detection, with a total of 28,706 videos, where each unique identity encompasses all advanced attack types.","Based on this dataset, we organized a Unified Physical-Digital Face Attack Detection Challenge to boost the research in Unified Attack Detections.","It attracted 136 teams for the development phase, with 13 qualifying for the final round.","The results re-verified by the organizing team were used for the final ranking.","This paper comprehensively reviews the challenge, detailing the dataset introduction, protocol definition, evaluation criteria, and a summary of published results.","Finally, we focus on the detailed analysis of the highest-performing algorithms and offer potential directions for unified physical-digital attack detection inspired by this competition.","Challenge Website: https://sites.google.com/view/face-anti-spoofing-challenge/welcome/challengecvpr2024."],"url":"http://arxiv.org/abs/2404.06211v1","category":"cs.CV"}
{"created":"2024-04-09 10:58:21","title":"Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models","abstract":"While many have shown how Large Language Models (LLMs) can be applied to a diverse set of tasks, the critical issues of data contamination and memorization are often glossed over. In this work, we address this concern for tabular data. Specifically, we introduce a variety of different techniques to assess whether a language model has seen a tabular dataset during training. This investigation reveals that LLMs have memorized many popular tabular datasets verbatim. We then compare the few-shot learning performance of LLMs on datasets that were seen during training to the performance on datasets released after training. We find that LLMs perform better on datasets seen during training, indicating that memorization leads to overfitting. At the same time, LLMs show non-trivial performance on novel datasets and are surprisingly robust to data transformations. We then investigate the in-context statistical learning abilities of LLMs. Without fine-tuning, we find them to be limited. This suggests that much of the few-shot performance on novel datasets is due to the LLM's world knowledge. Overall, our results highlight the importance of testing whether an LLM has seen an evaluation dataset during pre-training. We make the exposure tests we developed available as the tabmemcheck Python package at https://github.com/interpretml/LLM-Tabular-Memorization-Checker","sentences":["While many have shown how Large Language Models (LLMs) can be applied to a diverse set of tasks, the critical issues of data contamination and memorization are often glossed over.","In this work, we address this concern for tabular data.","Specifically, we introduce a variety of different techniques to assess whether a language model has seen a tabular dataset during training.","This investigation reveals that LLMs have memorized many popular tabular datasets verbatim.","We then compare the few-shot learning performance of LLMs on datasets that were seen during training to the performance on datasets released after training.","We find that LLMs perform better on datasets seen during training, indicating that memorization leads to overfitting.","At the same time, LLMs show non-trivial performance on novel datasets and are surprisingly robust to data transformations.","We then investigate the in-context statistical learning abilities of LLMs.","Without fine-tuning, we find them to be limited.","This suggests that much of the few-shot performance on novel datasets is due to the LLM's world knowledge.","Overall, our results highlight the importance of testing whether an LLM has seen an evaluation dataset during pre-training.","We make the exposure tests we developed available as the tabmemcheck Python package at https://github.com/interpretml/LLM-Tabular-Memorization-Checker"],"url":"http://arxiv.org/abs/2404.06209v1","category":"cs.LG"}
{"created":"2024-04-09 10:52:35","title":"Precise measurements of $W$- and $Z$-boson transverse momentum spectra with the ATLAS detector using $pp$ collisions at $\\sqrt{s} = 5.02$ TeV and $13$ TeV","abstract":"This paper describes measurements of the transverse momentum spectra of $W$ and $Z$ bosons produced in proton-proton collisions at centre-of-mass energies of $\\sqrt{s}=5.02$ TeV and $\\sqrt{s}=13$ TeV with the ATLAS experiment at the Large Hadron Collider. Measurements are performed in the electron and muon channels, $W \\to \\ell\\nu$ and $Z \\to \\ell\\ell$ ($\\ell=e$ or $\\mu$), and for $W$ events further separated by charge. The data were collected in 2017 and 2018, in dedicated runs with reduced instantaneous luminosity, and correspond to $255$ pb$^{-1}$ and $338$ pb$^{-1}$ at $\\sqrt{s}=5.02$ TeV and $13$ TeV, respectively. These conditions optimise the reconstruction of the $W$-boson transverse momentum. The distributions observed in the electron and muon channels are unfolded, combined, and compared to QCD calculations based on parton shower Monte Carlo event generators and analytical resummation. The description of the transverse momentum distributions by Monte Carlo event generators is imperfect and shows significant differences largely common to $W^-$, $W^+$ and $Z$ production. The agreement is better at $\\sqrt{s}=5.02$ TeV, especially for predictions that were tuned to $Z$ production data at $\\sqrt{s}=7$ TeV. Higher-order, resummed predictions based on DYTurbo generally match the data best across the spectra. Distribution ratios are also presented and test the understanding of differences between the production processes.","sentences":["This paper describes measurements of the transverse momentum spectra of $W$ and $Z$ bosons produced in proton-proton collisions at centre-of-mass energies of $\\sqrt{s}=5.02$ TeV and $\\sqrt{s}=13$ TeV with the ATLAS experiment at the Large Hadron Collider.","Measurements are performed in the electron and muon channels, $W \\to \\ell\\nu$ and $Z \\to \\ell\\ell$ ($\\ell=e$ or $\\mu$), and for $W$ events further separated by charge.","The data were collected in 2017 and 2018, in dedicated runs with reduced instantaneous luminosity, and correspond to $255$ pb$^{-1}$ and $338$ pb$^{-1}$ at $\\sqrt{s}=5.02$ TeV and $13$ TeV, respectively.","These conditions optimise the reconstruction of the $W$-boson transverse momentum.","The distributions observed in the electron and muon channels are unfolded, combined, and compared to QCD calculations based on parton shower Monte Carlo event generators and analytical resummation.","The description of the transverse momentum distributions by Monte Carlo event generators is imperfect and shows significant differences largely common to $W^-$, $W^+$ and $Z$ production.","The agreement is better at $\\sqrt{s}=5.02$ TeV, especially for predictions that were tuned to $Z$ production data at $\\sqrt{s}=7$ TeV. Higher-order, resummed predictions based on DYTurbo generally match the data best across the spectra.","Distribution ratios are also presented and test the understanding of differences between the production processes."],"url":"http://arxiv.org/abs/2404.06204v1","category":"hep-ex"}
{"created":"2024-04-09 10:47:02","title":"Open-Source AI-based SE Tools: Opportunities and Challenges of Collaborative Software Learning","abstract":"Large Language Models (LLMs) have become instrumental in advancing software engineering (SE) tasks, showcasing their efficacy in code understanding and beyond. Like traditional SE tools, open-source collaboration is key in realising the excellent products. However, with AI models, the essential need is in data. The collaboration of these AI-based SE models hinges on maximising the sources of high-quality data. However, data especially of high quality, often holds commercial or sensitive value, making it less accessible for open-source AI-based SE projects. This reality presents a significant barrier to the development and enhancement of AI-based SE tools within the software engineering community. Therefore, researchers need to find solutions for enabling open-source AI-based SE models to tap into resources by different organisations. Addressing this challenge, our position paper investigates one solution to facilitate access to diverse organizational resources for open-source AI models, ensuring privacy and commercial sensitivities are respected. We introduce a governance framework centered on federated learning (FL), designed to foster the joint development and maintenance of open-source AI code models while safeguarding data privacy and security. Additionally, we present guidelines for developers on AI-based SE tool collaboration, covering data requirements, model architecture, updating strategies, and version control. Given the significant influence of data characteristics on FL, our research examines the effect of code data heterogeneity on FL performance.","sentences":["Large Language Models (LLMs) have become instrumental in advancing software engineering (SE) tasks, showcasing their efficacy in code understanding and beyond.","Like traditional SE tools, open-source collaboration is key in realising the excellent products.","However, with AI models, the essential need is in data.","The collaboration of these AI-based SE models hinges on maximising the sources of high-quality data.","However, data especially of high quality, often holds commercial or sensitive value, making it less accessible for open-source AI-based SE projects.","This reality presents a significant barrier to the development and enhancement of AI-based SE tools within the software engineering community.","Therefore, researchers need to find solutions for enabling open-source AI-based SE models to tap into resources by different organisations.","Addressing this challenge, our position paper investigates one solution to facilitate access to diverse organizational resources for open-source AI models, ensuring privacy and commercial sensitivities are respected.","We introduce a governance framework centered on federated learning (FL), designed to foster the joint development and maintenance of open-source AI code models while safeguarding data privacy and security.","Additionally, we present guidelines for developers on AI-based SE tool collaboration, covering data requirements, model architecture, updating strategies, and version control.","Given the significant influence of data characteristics on FL, our research examines the effect of code data heterogeneity on FL performance."],"url":"http://arxiv.org/abs/2404.06201v1","category":"cs.SE"}
{"created":"2024-04-09 10:15:18","title":"Diverse Randomized Value Functions: A Provably Pessimistic Approach for Offline Reinforcement Learning","abstract":"Offline Reinforcement Learning (RL) faces distributional shift and unreliable value estimation, especially for out-of-distribution (OOD) actions. To address this, existing uncertainty-based methods penalize the value function with uncertainty quantification and demand numerous ensemble networks, posing computational challenges and suboptimal outcomes. In this paper, we introduce a novel strategy employing diverse randomized value functions to estimate the posterior distribution of $Q$-values. It provides robust uncertainty quantification and estimates lower confidence bounds (LCB) of $Q$-values. By applying moderate value penalties for OOD actions, our method fosters a provably pessimistic approach. We also emphasize on diversity within randomized value functions and enhance efficiency by introducing a diversity regularization method, reducing the requisite number of networks. These modules lead to reliable value estimation and efficient policy learning from offline data. Theoretical analysis shows that our method recovers the provably efficient LCB-penalty under linear MDP assumptions. Extensive empirical results also demonstrate that our proposed method significantly outperforms baseline methods in terms of performance and parametric efficiency.","sentences":["Offline Reinforcement Learning (RL) faces distributional shift and unreliable value estimation, especially for out-of-distribution (OOD) actions.","To address this, existing uncertainty-based methods penalize the value function with uncertainty quantification and demand numerous ensemble networks, posing computational challenges and suboptimal outcomes.","In this paper, we introduce a novel strategy employing diverse randomized value functions to estimate the posterior distribution of $Q$-values.","It provides robust uncertainty quantification and estimates lower confidence bounds (LCB) of $Q$-values.","By applying moderate value penalties for OOD actions, our method fosters a provably pessimistic approach.","We also emphasize on diversity within randomized value functions and enhance efficiency by introducing a diversity regularization method, reducing the requisite number of networks.","These modules lead to reliable value estimation and efficient policy learning from offline data.","Theoretical analysis shows that our method recovers the provably efficient LCB-penalty under linear MDP assumptions.","Extensive empirical results also demonstrate that our proposed method significantly outperforms baseline methods in terms of performance and parametric efficiency."],"url":"http://arxiv.org/abs/2404.06188v1","category":"cs.LG"}
{"created":"2024-04-09 10:12:34","title":"Clue-Instruct: Text-Based Clue Generation for Educational Crossword Puzzles","abstract":"Crossword puzzles are popular linguistic games often used as tools to engage students in learning. Educational crosswords are characterized by less cryptic and more factual clues that distinguish them from traditional crossword puzzles. Despite there exist several publicly available clue-answer pair databases for traditional crosswords, educational clue-answer pairs datasets are missing. In this article, we propose a methodology to build educational clue generation datasets that can be used to instruct Large Language Models (LLMs). By gathering from Wikipedia pages informative content associated with relevant keywords, we use Large Language Models to automatically generate pedagogical clues related to the given input keyword and its context. With such an approach, we created clue-instruct, a dataset containing 44,075 unique examples with text-keyword pairs associated with three distinct crossword clues. We used clue-instruct to instruct different LLMs to generate educational clues from a given input content and keyword. Both human and automatic evaluations confirmed the quality of the generated clues, thus validating the effectiveness of our approach.","sentences":["Crossword puzzles are popular linguistic games often used as tools to engage students in learning.","Educational crosswords are characterized by less cryptic and more factual clues that distinguish them from traditional crossword puzzles.","Despite there exist several publicly available clue-answer pair databases for traditional crosswords, educational clue-answer pairs datasets are missing.","In this article, we propose a methodology to build educational clue generation datasets that can be used to instruct Large Language Models (LLMs).","By gathering from Wikipedia pages informative content associated with relevant keywords, we use Large Language Models to automatically generate pedagogical clues related to the given input keyword and its context.","With such an approach, we created clue-instruct, a dataset containing 44,075 unique examples with text-keyword pairs associated with three distinct crossword clues.","We used clue-instruct to instruct different LLMs to generate educational clues from a given input content and keyword.","Both human and automatic evaluations confirmed the quality of the generated clues, thus validating the effectiveness of our approach."],"url":"http://arxiv.org/abs/2404.06186v1","category":"cs.CL"}
{"created":"2024-04-09 10:06:43","title":"Streamlined Transmission: A Semantic-Aware XR Deployment Framework Enhanced by Generative AI","abstract":"In the era of 6G, featuring compelling visions of digital twins and metaverses, Extended Reality (XR) has emerged as a vital conduit connecting the digital and physical realms, garnering widespread interest. Ensuring a fully immersive wireless XR experience stands as a paramount technical necessity, demanding the liberation of XR from the confines of wired connections. In this paper, we first introduce the technologies applied in the wireless XR domain, delve into their benefits and limitations, and highlight the ongoing challenges. We then propose a novel deployment framework for a broad XR pipeline, termed \"GeSa-XRF\", inspired by the core philosophy of Semantic Communication (SemCom) which shifts the concern from \"how\" to transmit to \"what\" to transmit. Particularly, the framework comprises three stages: data collection, data analysis, and data delivery. In each stage, we integrate semantic awareness to achieve streamlined transmission and employ Generative Artificial Intelligence (GAI) to achieve collaborative refinements. For the data collection of multi-modal data with differentiated data volumes and heterogeneous latency requirements, we propose a novel SemCom paradigm based on multi-modal fusion and separation and a GAI-based robust superposition scheme. To perform a comprehensive data analysis, we employ multi-task learning to perform the prediction of field of view and personalized attention and discuss the possible preprocessing approaches assisted by GAI. Lastly, for the data delivery stage, we present a semantic-aware multicast-based delivery strategy aimed at reducing pixel level redundant transmissions and introduce the GAI collaborative refinement approach. The performance gain of the proposed GeSa-XRF is preliminarily demonstrated through a case study.","sentences":["In the era of 6G, featuring compelling visions of digital twins and metaverses, Extended Reality (XR) has emerged as a vital conduit connecting the digital and physical realms, garnering widespread interest.","Ensuring a fully immersive wireless XR experience stands as a paramount technical necessity, demanding the liberation of XR from the confines of wired connections.","In this paper, we first introduce the technologies applied in the wireless XR domain, delve into their benefits and limitations, and highlight the ongoing challenges.","We then propose a novel deployment framework for a broad XR pipeline, termed \"GeSa-XRF\", inspired by the core philosophy of Semantic Communication (SemCom) which shifts the concern from \"how\" to transmit to \"what\" to transmit.","Particularly, the framework comprises three stages: data collection, data analysis, and data delivery.","In each stage, we integrate semantic awareness to achieve streamlined transmission and employ Generative Artificial Intelligence (GAI) to achieve collaborative refinements.","For the data collection of multi-modal data with differentiated data volumes and heterogeneous latency requirements, we propose a novel SemCom paradigm based on multi-modal fusion and separation and a GAI-based robust superposition scheme.","To perform a comprehensive data analysis, we employ multi-task learning to perform the prediction of field of view and personalized attention and discuss the possible preprocessing approaches assisted by GAI.","Lastly, for the data delivery stage, we present a semantic-aware multicast-based delivery strategy aimed at reducing pixel level redundant transmissions and introduce the GAI collaborative refinement approach.","The performance gain of the proposed GeSa-XRF is preliminarily demonstrated through a case study."],"url":"http://arxiv.org/abs/2404.06182v1","category":"cs.NI"}
{"created":"2024-04-09 10:04:06","title":"EPL: Evidential Prototype Learning for Semi-supervised Medical Image Segmentation","abstract":"Although current semi-supervised medical segmentation methods can achieve decent performance, they are still affected by the uncertainty in unlabeled data and model predictions, and there is currently a lack of effective strategies that can explore the uncertain aspects of both simultaneously. To address the aforementioned issues, we propose Evidential Prototype Learning (EPL), which utilizes an extended probabilistic framework to effectively fuse voxel probability predictions from different sources and achieves prototype fusion utilization of labeled and unlabeled data under a generalized evidential framework, leveraging voxel-level dual uncertainty masking. The uncertainty not only enables the model to self-correct predictions but also improves the guided learning process with pseudo-labels and is able to feed back into the construction of hidden features. The method proposed in this paper has been experimented on LA, Pancreas-CT and TBAD datasets, achieving the state-of-the-art performance in three different labeled ratios, which strongly demonstrates the effectiveness of our strategy.","sentences":["Although current semi-supervised medical segmentation methods can achieve decent performance, they are still affected by the uncertainty in unlabeled data and model predictions, and there is currently a lack of effective strategies that can explore the uncertain aspects of both simultaneously.","To address the aforementioned issues, we propose Evidential Prototype Learning (EPL), which utilizes an extended probabilistic framework to effectively fuse voxel probability predictions from different sources and achieves prototype fusion utilization of labeled and unlabeled data under a generalized evidential framework, leveraging voxel-level dual uncertainty masking.","The uncertainty not only enables the model to self-correct predictions but also improves the guided learning process with pseudo-labels and is able to feed back into the construction of hidden features.","The method proposed in this paper has been experimented on LA, Pancreas-CT and TBAD datasets, achieving the state-of-the-art performance in three different labeled ratios, which strongly demonstrates the effectiveness of our strategy."],"url":"http://arxiv.org/abs/2404.06181v1","category":"cs.CV"}
{"created":"2024-04-09 09:58:10","title":"Uncertainty-aware Evidential Fusion-based Learning for Semi-supervised Medical Image Segmentation","abstract":"Although the existing uncertainty-based semi-supervised medical segmentation methods have achieved excellent performance, they usually only consider a single uncertainty evaluation, which often fails to solve the problem related to credibility completely. Therefore, based on the framework of evidential deep learning, this paper integrates the evidential predictive results in the cross-region of mixed and original samples to reallocate the confidence degree and uncertainty measure of each voxel, which is realized by emphasizing uncertain information of probability assignments fusion rule of traditional evidence theory. Furthermore, we design a voxel-level asymptotic learning strategy by introducing information entropy to combine with the fused uncertainty measure to estimate voxel prediction more precisely. The model will gradually pay attention to the prediction results with high uncertainty in the learning process, to learn the features that are difficult to master. The experimental results on LA, Pancreas-CT, ACDC and TBAD datasets demonstrate the superior performance of our proposed method in comparison with the existing state of the arts.","sentences":["Although the existing uncertainty-based semi-supervised medical segmentation methods have achieved excellent performance, they usually only consider a single uncertainty evaluation, which often fails to solve the problem related to credibility completely.","Therefore, based on the framework of evidential deep learning, this paper integrates the evidential predictive results in the cross-region of mixed and original samples to reallocate the confidence degree and uncertainty measure of each voxel, which is realized by emphasizing uncertain information of probability assignments fusion rule of traditional evidence theory.","Furthermore, we design a voxel-level asymptotic learning strategy by introducing information entropy to combine with the fused uncertainty measure to estimate voxel prediction more precisely.","The model will gradually pay attention to the prediction results with high uncertainty in the learning process, to learn the features that are difficult to master.","The experimental results on LA, Pancreas-CT, ACDC and TBAD datasets demonstrate the superior performance of our proposed method in comparison with the existing state of the arts."],"url":"http://arxiv.org/abs/2404.06177v1","category":"cs.CV"}
{"created":"2024-04-09 09:55:27","title":"Fundamental interactions in self-organized critical dynamics on higher-order networks","abstract":"In functionally complex systems, higher-order connectivity is often revealed in the underlying geometry of networked units. Furthermore, such systems often show signatures of self-organized criticality, a specific type of non-equilibrium collective behaviour associated with an attractor of internal dynamics with long-range correlations and scale invariance, which ensures the robust functioning of complex systems, such as the brain. Here, we highlight the intertwining of features of higher-order geometry and self-organized critical dynamics as a plausible mechanism for the emergence of new properties on a larger scale, representing the central paradigm of the physical notion of complexity. Considering the time scale of the structural evolution with the known separation of the time scale in self-organized criticality, i.e., internal dynamics and external driving, we distinguish three classes of geometries that can shape the self-organized dynamics on them differently. We provide an overview of current trends in the study of collective dynamics phenomena, such as the synchronization of phase oscillators and discrete spin dynamics with higher-order couplings embedded in the faces of simplicial complexes. For a representative example of self-organized critical behaviour induced by higher-order structures, we present a more detailed analysis of the dynamics of field-driven spin reversal on the hysteresis loops in simplicial complexes composed of triangles. These numerical results suggest that two fundamental interactions representing the edge-embedded and triangle-embedded couplings must be taken into account in theoretical models to describe the influence of higher-order geometry on critical dynamics.","sentences":["In functionally complex systems, higher-order connectivity is often revealed in the underlying geometry of networked units.","Furthermore, such systems often show signatures of self-organized criticality, a specific type of non-equilibrium collective behaviour associated with an attractor of internal dynamics with long-range correlations and scale invariance, which ensures the robust functioning of complex systems, such as the brain.","Here, we highlight the intertwining of features of higher-order geometry and self-organized critical dynamics as a plausible mechanism for the emergence of new properties on a larger scale, representing the central paradigm of the physical notion of complexity.","Considering the time scale of the structural evolution with the known separation of the time scale in self-organized criticality, i.e., internal dynamics and external driving, we distinguish three classes of geometries that can shape the self-organized dynamics on them differently.","We provide an overview of current trends in the study of collective dynamics phenomena, such as the synchronization of phase oscillators and discrete spin dynamics with higher-order couplings embedded in the faces of simplicial complexes.","For a representative example of self-organized critical behaviour induced by higher-order structures, we present a more detailed analysis of the dynamics of field-driven spin reversal on the hysteresis loops in simplicial complexes composed of triangles.","These numerical results suggest that two fundamental interactions representing the edge-embedded and triangle-embedded couplings must be taken into account in theoretical models to describe the influence of higher-order geometry on critical dynamics."],"url":"http://arxiv.org/abs/2404.06175v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-09 09:50:02","title":"Intelligence and Motion Models of Continuum Robots: an Overview","abstract":"Many technical solutions are bio-inspired. Octopus-inspired robotic arms belong to continuum robots which are used in minimally invasive surgery or for technical system restoration in areas difficult-toaccess. Continuum robot missions are bounded with their motions, whereby the motion of the robots is controlled by humans via wireless communication. In case of a lost connection, robot autonomy is required. Distributed control and distributed decision-making mechanisms based on artificial intelligence approaches can be a promising solution to achieve autonomy of technical systems and to increase their resilience. However these methods are not well investigated yet. Octopuses are the living example of natural distributed intelligence but their learning and decision-making mechanisms are also not fully investigated and understood yet. Our major interest is investigating mechanisms of Distributed Artificial Intelligence as a basis for improving resilience of complex systems. We decided to use a physical continuum robot prototype that is able to perform some basic movements for our research. The idea is to research how a technical system can be empowered to combine movements into sequences of motions by itself. For the experimental investigations a suitable physical prototype has to be selected, its motion control has to be implemented and automated. In this paper, we give an overview combining different fields of research, such as Distributed Artificial Intelligence and continuum robots based on 98 publications. We provide a detailed description of the basic motion control models of continuum robots based on the literature reviewed, discuss different aspects of autonomy and give an overview of physical prototypes of continuum robots.","sentences":["Many technical solutions are bio-inspired.","Octopus-inspired robotic arms belong to continuum robots which are used in minimally invasive surgery or for technical system restoration in areas difficult-toaccess.","Continuum robot missions are bounded with their motions, whereby the motion of the robots is controlled by humans via wireless communication.","In case of a lost connection, robot autonomy is required.","Distributed control and distributed decision-making mechanisms based on artificial intelligence approaches can be a promising solution to achieve autonomy of technical systems and to increase their resilience.","However these methods are not well investigated yet.","Octopuses are the living example of natural distributed intelligence but their learning and decision-making mechanisms are also not fully investigated and understood yet.","Our major interest is investigating mechanisms of Distributed Artificial Intelligence as a basis for improving resilience of complex systems.","We decided to use a physical continuum robot prototype that is able to perform some basic movements for our research.","The idea is to research how a technical system can be empowered to combine movements into sequences of motions by itself.","For the experimental investigations a suitable physical prototype has to be selected, its motion control has to be implemented and automated.","In this paper, we give an overview combining different fields of research, such as Distributed Artificial Intelligence and continuum robots based on 98 publications.","We provide a detailed description of the basic motion control models of continuum robots based on the literature reviewed, discuss different aspects of autonomy and give an overview of physical prototypes of continuum robots."],"url":"http://arxiv.org/abs/2404.06171v1","category":"cs.RO"}
{"created":"2024-04-09 09:49:57","title":"CLIP-Embed-KD: Computationally Efficient Knowledge Distillation Using Embeddings as Teachers","abstract":"Contrastive Language-Image Pre-training (CLIP) has been shown to improve zero-shot generalization capabilities of language and vision models. In this paper, we extend CLIP for efficient knowledge distillation, by utilizing embeddings as teachers. Typical knowledge distillation frameworks require running forward passes through a teacher model, which is often prohibitive in the case of billion or trillion parameter teachers. In these cases, using only the embeddings of the teacher models to guide the distillation can yield significant computational savings. Our preliminary findings show that CLIP-based knowledge distillation with embeddings can outperform full scale knowledge distillation using $9\\times$ less memory and $8\\times$ less training time. Code available at: https://github.com/lnairGT/CLIP-Distillation/","sentences":["Contrastive Language-Image Pre-training (CLIP) has been shown to improve zero-shot generalization capabilities of language and vision models.","In this paper, we extend CLIP for efficient knowledge distillation, by utilizing embeddings as teachers.","Typical knowledge distillation frameworks require running forward passes through a teacher model, which is often prohibitive in the case of billion or trillion parameter teachers.","In these cases, using only the embeddings of the teacher models to guide the distillation can yield significant computational savings.","Our preliminary findings show that CLIP-based knowledge distillation with embeddings can outperform full scale knowledge distillation using $9\\times$ less memory and $8\\times$ less training time.","Code available at: https://github.com/lnairGT/CLIP-Distillation/"],"url":"http://arxiv.org/abs/2404.06170v1","category":"cs.LG"}
{"created":"2024-04-09 09:49:08","title":"Protection of Guizhou Miao Batik Culture Based on Knowledge Graph and Deep Learning","abstract":"In the globalization trend, China's cultural heritage is in danger of gradually disappearing. The protection and inheritance of these precious cultural resources has become a critical task. This paper focuses on the Miao batik culture in Guizhou Province, China, and explores the application of knowledge graphs, natural language processing, and deep learning techniques in the promotion and protection of batik culture. We propose a dual-channel mechanism that integrates semantic and visual information, aiming to connect batik pattern features with cultural connotations. First, we use natural language processing techniques to automatically extract batik-related entities and relationships from the literature, and construct and visualize a structured batik pattern knowledge graph. Based on this knowledge graph, users can textually search and understand the images, meanings, taboos, and other cultural information of specific patterns. Second, for the batik pattern classification, we propose an improved ResNet34 model. By embedding average pooling and convolutional operations into the residual blocks and introducing long-range residual connections, the classification performance is enhanced. By inputting pattern images into this model, their subjects can be accurately identified, and then the underlying cultural connotations can be understood. Experimental results show that our model outperforms other mainstream models in evaluation metrics such as accuracy, precision, recall, and F1-score, achieving 99.0%, 99.0%, 98.9%, and 99.0%, respectively. This research provides new ideas for the digital protection of batik culture and demonstrates the great potential of artificial intelligence technology in cultural heritage protection.","sentences":["In the globalization trend, China's cultural heritage is in danger of gradually disappearing.","The protection and inheritance of these precious cultural resources has become a critical task.","This paper focuses on the Miao batik culture in Guizhou Province, China, and explores the application of knowledge graphs, natural language processing, and deep learning techniques in the promotion and protection of batik culture.","We propose a dual-channel mechanism that integrates semantic and visual information, aiming to connect batik pattern features with cultural connotations.","First, we use natural language processing techniques to automatically extract batik-related entities and relationships from the literature, and construct and visualize a structured batik pattern knowledge graph.","Based on this knowledge graph, users can textually search and understand the images, meanings, taboos, and other cultural information of specific patterns.","Second, for the batik pattern classification, we propose an improved ResNet34 model.","By embedding average pooling and convolutional operations into the residual blocks and introducing long-range residual connections, the classification performance is enhanced.","By inputting pattern images into this model, their subjects can be accurately identified, and then the underlying cultural connotations can be understood.","Experimental results show that our model outperforms other mainstream models in evaluation metrics such as accuracy, precision, recall, and F1-score, achieving 99.0%, 99.0%, 98.9%, and 99.0%, respectively.","This research provides new ideas for the digital protection of batik culture and demonstrates the great potential of artificial intelligence technology in cultural heritage protection."],"url":"http://arxiv.org/abs/2404.06168v1","category":"stat.AP"}
{"created":"2024-04-09 09:46:17","title":"scCDCG: Efficient Deep Structural Clustering for single-cell RNA-seq via Deep Cut-informed Graph Embedding","abstract":"Single-cell RNA sequencing (scRNA-seq) is essential for unraveling cellular heterogeneity and diversity, offering invaluable insights for bioinformatics advancements. Despite its potential, traditional clustering methods in scRNA-seq data analysis often neglect the structural information embedded in gene expression profiles, crucial for understanding cellular correlations and dependencies. Existing strategies, including graph neural networks, face challenges in handling the inefficiency due to scRNA-seq data's intrinsic high-dimension and high-sparsity. Addressing these limitations, we introduce scCDCG (single-cell RNA-seq Clustering via Deep Cut-informed Graph), a novel framework designed for efficient and accurate clustering of scRNA-seq data that simultaneously utilizes intercellular high-order structural information. scCDCG comprises three main components: (i) A graph embedding module utilizing deep cut-informed techniques, which effectively captures intercellular high-order structural information, overcoming the over-smoothing and inefficiency issues prevalent in prior graph neural network methods. (ii) A self-supervised learning module guided by optimal transport, tailored to accommodate the unique complexities of scRNA-seq data, specifically its high-dimension and high-sparsity. (iii) An autoencoder-based feature learning module that simplifies model complexity through effective dimension reduction and feature extraction. Our extensive experiments on 6 datasets demonstrate scCDCG's superior performance and efficiency compared to 7 established models, underscoring scCDCG's potential as a transformative tool in scRNA-seq data analysis. Our code is available at: https://github.com/XPgogogo/scCDCG.","sentences":["Single-cell RNA sequencing (scRNA-seq) is essential for unraveling cellular heterogeneity and diversity, offering invaluable insights for bioinformatics advancements.","Despite its potential, traditional clustering methods in scRNA-seq data analysis often neglect the structural information embedded in gene expression profiles, crucial for understanding cellular correlations and dependencies.","Existing strategies, including graph neural networks, face challenges in handling the inefficiency due to scRNA-seq data's intrinsic high-dimension and high-sparsity.","Addressing these limitations, we introduce scCDCG (single-cell RNA-seq Clustering via Deep Cut-informed Graph), a novel framework designed for efficient and accurate clustering of scRNA-seq data that simultaneously utilizes intercellular high-order structural information.","scCDCG comprises three main components: (i) A graph embedding module utilizing deep cut-informed techniques, which effectively captures intercellular high-order structural information, overcoming the over-smoothing and inefficiency issues prevalent in prior graph neural network methods.","(ii) A self-supervised learning module guided by optimal transport, tailored to accommodate the unique complexities of scRNA-seq data, specifically its high-dimension and high-sparsity.","(iii) An autoencoder-based feature learning module that simplifies model complexity through effective dimension reduction and feature extraction.","Our extensive experiments on 6 datasets demonstrate scCDCG's superior performance and efficiency compared to 7 established models, underscoring scCDCG's potential as a transformative tool in scRNA-seq data analysis.","Our code is available at: https://github.com/XPgogogo/scCDCG."],"url":"http://arxiv.org/abs/2404.06167v1","category":"cs.LG"}
{"created":"2024-04-09 09:42:18","title":"Enhanced Radar Perception via Multi-Task Learning: Towards Refined Data for Sensor Fusion Applications","abstract":"Radar and camera fusion yields robustness in perception tasks by leveraging the strength of both sensors. The typical extracted radar point cloud is 2D without height information due to insufficient antennas along the elevation axis, which challenges the network performance. This work introduces a learning-based approach to infer the height of radar points associated with 3D objects. A novel robust regression loss is introduced to address the sparse target challenge. In addition, a multi-task training strategy is employed, emphasizing important features. The average radar absolute height error decreases from 1.69 to 0.25 meters compared to the state-of-the-art height extension method. The estimated target height values are used to preprocess and enrich radar data for downstream perception tasks. Integrating this refined radar information further enhances the performance of existing radar camera fusion models for object detection and depth estimation tasks.","sentences":["Radar and camera fusion yields robustness in perception tasks by leveraging the strength of both sensors.","The typical extracted radar point cloud is 2D without height information due to insufficient antennas along the elevation axis, which challenges the network performance.","This work introduces a learning-based approach to infer the height of radar points associated with 3D objects.","A novel robust regression loss is introduced to address the sparse target challenge.","In addition, a multi-task training strategy is employed, emphasizing important features.","The average radar absolute height error decreases from 1.69 to 0.25 meters compared to the state-of-the-art height extension method.","The estimated target height values are used to preprocess and enrich radar data for downstream perception tasks.","Integrating this refined radar information further enhances the performance of existing radar camera fusion models for object detection and depth estimation tasks."],"url":"http://arxiv.org/abs/2404.06165v1","category":"cs.CV"}
{"created":"2024-04-09 09:34:25","title":"Characterizing Multimodal Long-form Summarization: A Case Study on Financial Reports","abstract":"As large language models (LLMs) expand the power of natural language processing to handle long inputs, rigorous and systematic analyses are necessary to understand their abilities and behavior. A salient application is summarization, due to its ubiquity and controversy (e.g., researchers have declared the death of summarization). In this paper, we use financial report summarization as a case study because financial reports not only are long but also use numbers and tables extensively. We propose a computational framework for characterizing multimodal long-form summarization and investigate the behavior of Claude 2.0/2.1, GPT-4/3.5, and Command. We find that GPT-3.5 and Command fail to perform this summarization task meaningfully. For Claude 2 and GPT-4, we analyze the extractiveness of the summary and identify a position bias in LLMs. This position bias disappears after shuffling the input for Claude, which suggests that Claude has the ability to recognize important information. We also conduct a comprehensive investigation on the use of numeric data in LLM-generated summaries and offer a taxonomy of numeric hallucination. We employ prompt engineering to improve GPT-4's use of numbers with limited success. Overall, our analyses highlight the strong capability of Claude 2 in handling long multimodal inputs compared to GPT-4.","sentences":["As large language models (LLMs) expand the power of natural language processing to handle long inputs, rigorous and systematic analyses are necessary to understand their abilities and behavior.","A salient application is summarization, due to its ubiquity and controversy (e.g., researchers have declared the death of summarization).","In this paper, we use financial report summarization as a case study because financial reports not only are long but also use numbers and tables extensively.","We propose a computational framework for characterizing multimodal long-form summarization and investigate the behavior of Claude 2.0/2.1, GPT-4/3.5, and Command.","We find that GPT-3.5 and Command fail to perform this summarization task meaningfully.","For Claude 2 and GPT-4, we analyze the extractiveness of the summary and identify a position bias in LLMs.","This position bias disappears after shuffling the input for Claude, which suggests that Claude has the ability to recognize important information.","We also conduct a comprehensive investigation on the use of numeric data in LLM-generated summaries and offer a taxonomy of numeric hallucination.","We employ prompt engineering to improve GPT-4's use of numbers with limited success.","Overall, our analyses highlight the strong capability of Claude 2 in handling long multimodal inputs compared to GPT-4."],"url":"http://arxiv.org/abs/2404.06162v1","category":"cs.CL"}
{"created":"2024-04-09 09:32:00","title":"Distributed Artificial Intelligence as a Means to Achieve Self-X-Functions for Increasing Resilience: the First Steps","abstract":"Using sensors as a means to achieve self-awareness and artificial intelligence for decision-making, may be a way to make complex systems self-adaptive, autonomous and resilient. Investigating the combination of distributed artificial intelligence methods and bio-inspired robotics can provide results that will be helpful for implementing autonomy of such robots and other complex systems. In this paper, we describe Distributed Artificial Intelligence application area, the most common examples of continuum robots and provide a description of our first steps towards implementing distributed control.","sentences":["Using sensors as a means to achieve self-awareness and artificial intelligence for decision-making, may be a way to make complex systems self-adaptive, autonomous and resilient.","Investigating the combination of distributed artificial intelligence methods and bio-inspired robotics can provide results that will be helpful for implementing autonomy of such robots and other complex systems.","In this paper, we describe Distributed Artificial Intelligence application area, the most common examples of continuum robots and provide a description of our first steps towards implementing distributed control."],"url":"http://arxiv.org/abs/2404.06159v1","category":"cs.RO"}
{"created":"2024-04-09 09:29:42","title":"A data-driven approach to UIO-based fault diagnosis","abstract":"In this paper we propose a data-driven approach to the design of a residual generator, based on a dead-beat unknown-input observer, for linear time-invariant discrete-time state-space models, whose state equation is affected both by disturbances and by actuator faults. We first review the modelbased conditions for the existence of such a residual generator, and then prove that under suitable assumptions on the collected historical data, we are both able to determine if the problem is solvable and to identify the matrices of a possible residual generator. We propose an algorithm that, based only on the collected data (and not on the system description), is able to perform both tasks. An illustrating example and some remarks on limitations and possible extensions of the current results conclude the paper.","sentences":["In this paper we propose a data-driven approach to the design of a residual generator, based on a dead-beat unknown-input observer, for linear time-invariant discrete-time state-space models, whose state equation is affected both by disturbances and by actuator faults.","We first review the modelbased conditions for the existence of such a residual generator, and then prove that under suitable assumptions on the collected historical data, we are both able to determine if the problem is solvable and to identify the matrices of a possible residual generator.","We propose an algorithm that, based only on the collected data (and not on the system description), is able to perform both tasks.","An illustrating example and some remarks on limitations and possible extensions of the current results conclude the paper."],"url":"http://arxiv.org/abs/2404.06158v1","category":"eess.SY"}
{"created":"2024-04-09 09:28:05","title":"Efficient and Robust Point Cloud Registration via Heuristics-guided Parameter Search","abstract":"Estimating the rigid transformation with 6 degrees of freedom based on a putative 3D correspondence set is a crucial procedure in point cloud registration. Existing correspondence identification methods usually lead to large outlier ratios ($>$ 95 $\\%$ is common), underscoring the significance of robust registration methods. Many researchers turn to parameter search-based strategies (e.g., Branch-and-Bround) for robust registration. Although related methods show high robustness, their efficiency is limited to the high-dimensional search space. This paper proposes a heuristics-guided parameter search strategy to accelerate the search while maintaining high robustness. We first sample some correspondences (i.e., heuristics) and then just need to sequentially search the feasible regions that make each sample an inlier. Our strategy largely reduces the search space and can guarantee accuracy with only a few inlier samples, therefore enjoying an excellent trade-off between efficiency and robustness. Since directly parameterizing the 6-dimensional nonlinear feasible region for efficient search is intractable, we construct a three-stage decomposition pipeline to reparameterize the feasible region, resulting in three lower-dimensional sub-problems that are easily solvable via our strategy. Besides reducing the searching dimension, our decomposition enables the leverage of 1-dimensional interval stabbing at all three stages for searching acceleration. Moreover, we propose a valid sampling strategy to guarantee our sampling effectiveness, and a compatibility verification setup to further accelerate our search. Extensive experiments on both simulated and real-world datasets demonstrate that our approach exhibits comparable robustness with state-of-the-art methods while achieving a significant efficiency boost.","sentences":["Estimating the rigid transformation with 6 degrees of freedom based on a putative 3D correspondence set is a crucial procedure in point cloud registration.","Existing correspondence identification methods usually lead to large outlier ratios ($>$ 95 $\\%$ is common), underscoring the significance of robust registration methods.","Many researchers turn to parameter search-based strategies (e.g., Branch-and-Bround) for robust registration.","Although related methods show high robustness, their efficiency is limited to the high-dimensional search space.","This paper proposes a heuristics-guided parameter search strategy to accelerate the search while maintaining high robustness.","We first sample some correspondences (i.e., heuristics) and then just need to sequentially search the feasible regions that make each sample an inlier.","Our strategy largely reduces the search space and can guarantee accuracy with only a few inlier samples, therefore enjoying an excellent trade-off between efficiency and robustness.","Since directly parameterizing the 6-dimensional nonlinear feasible region for efficient search is intractable, we construct a three-stage decomposition pipeline to reparameterize the feasible region, resulting in three lower-dimensional sub-problems that are easily solvable via our strategy.","Besides reducing the searching dimension, our decomposition enables the leverage of 1-dimensional interval stabbing at all three stages for searching acceleration.","Moreover, we propose a valid sampling strategy to guarantee our sampling effectiveness, and a compatibility verification setup to further accelerate our search.","Extensive experiments on both simulated and real-world datasets demonstrate that our approach exhibits comparable robustness with state-of-the-art methods while achieving a significant efficiency boost."],"url":"http://arxiv.org/abs/2404.06155v1","category":"cs.CV"}
{"created":"2024-04-09 09:23:04","title":"HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields","abstract":"In recent advancements in novel view synthesis, generalizable Neural Radiance Fields (NeRF) based methods applied to human subjects have shown remarkable results in generating novel views from few images. However, this generalization ability cannot capture the underlying structural features of the skeleton shared across all instances. Building upon this, we introduce HFNeRF: a novel generalizable human feature NeRF aimed at generating human biomechanic features using a pre-trained image encoder. While previous human NeRF methods have shown promising results in the generation of photorealistic virtual avatars, such methods lack underlying human structure or biomechanic features such as skeleton or joint information that are crucial for downstream applications including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D pre-trained foundation models toward learning human features in 3D using neural rendering, and then volume rendering towards generating 2D feature maps. We evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as features. The proposed method is fully differentiable, allowing to successfully learn color, geometry, and human skeleton in a simultaneous manner. This paper presents preliminary results of HFNeRF, illustrating its potential in generating realistic virtual avatars with biomechanic features using NeRF.","sentences":["In recent advancements in novel view synthesis, generalizable Neural Radiance Fields (NeRF) based methods applied to human subjects have shown remarkable results in generating novel views from few images.","However, this generalization ability cannot capture the underlying structural features of the skeleton shared across all instances.","Building upon this, we introduce HFNeRF: a novel generalizable human feature NeRF aimed at generating human biomechanic features using a pre-trained image encoder.","While previous human NeRF methods have shown promising results in the generation of photorealistic virtual avatars, such methods lack underlying human structure or biomechanic features such as skeleton or joint information that are crucial for downstream applications including Augmented Reality (AR)/Virtual Reality (VR).","HFNeRF leverages 2D pre-trained foundation models toward learning human features in 3D using neural rendering, and then volume rendering towards generating 2D feature maps.","We evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as features.","The proposed method is fully differentiable, allowing to successfully learn color, geometry, and human skeleton in a simultaneous manner.","This paper presents preliminary results of HFNeRF, illustrating its potential in generating realistic virtual avatars with biomechanic features using NeRF."],"url":"http://arxiv.org/abs/2404.06152v1","category":"cs.CV"}
{"created":"2024-04-09 09:09:36","title":"Differential Privacy for Anomaly Detection: Analyzing the Trade-off Between Privacy and Explainability","abstract":"Anomaly detection (AD), also referred to as outlier detection, is a statistical process aimed at identifying observations within a dataset that significantly deviate from the expected pattern of the majority of the data. Such a process finds wide application in various fields, such as finance and healthcare. While the primary objective of AD is to yield high detection accuracy, the requirements of explainability and privacy are also paramount. The first ensures the transparency of the AD process, while the second guarantees that no sensitive information is leaked to untrusted parties. In this work, we exploit the trade-off of applying Explainable AI (XAI) through SHapley Additive exPlanations (SHAP) and differential privacy (DP). We perform AD with different models and on various datasets, and we thoroughly evaluate the cost of privacy in terms of decreased accuracy and explainability. Our results show that the enforcement of privacy through DP has a significant impact on detection accuracy and explainability, which depends on both the dataset and the considered AD model. We further show that the visual interpretation of explanations is also influenced by the choice of the AD algorithm.","sentences":["Anomaly detection (AD), also referred to as outlier detection, is a statistical process aimed at identifying observations within a dataset that significantly deviate from the expected pattern of the majority of the data.","Such a process finds wide application in various fields, such as finance and healthcare.","While the primary objective of AD is to yield high detection accuracy, the requirements of explainability and privacy are also paramount.","The first ensures the transparency of the AD process, while the second guarantees that no sensitive information is leaked to untrusted parties.","In this work, we exploit the trade-off of applying Explainable AI (XAI) through SHapley Additive exPlanations (SHAP) and differential privacy (DP).","We perform AD with different models and on various datasets, and we thoroughly evaluate the cost of privacy in terms of decreased accuracy and explainability.","Our results show that the enforcement of privacy through DP has a significant impact on detection accuracy and explainability, which depends on both the dataset and the considered AD model.","We further show that the visual interpretation of explanations is also influenced by the choice of the AD algorithm."],"url":"http://arxiv.org/abs/2404.06144v1","category":"cs.LG"}
{"created":"2024-04-09 09:04:30","title":"Cendol: Open Instruction-tuned Generative Large Language Models for Indonesian Languages","abstract":"Large language models (LLMs) show remarkable human-like capability in various domains and languages. However, a notable quality gap arises in low-resource languages, e.g., Indonesian indigenous languages, rendering them ineffective and inefficient in such linguistic contexts. To bridge this quality gap, we introduce Cendol, a collection of Indonesian LLMs encompassing both decoder-only and encoder-decoder architectures across a range of model sizes. We highlight Cendol's effectiveness across a diverse array of tasks, attaining 20% improvement, and demonstrate its capability to generalize to unseen tasks and indigenous languages of Indonesia. Furthermore, Cendol models showcase improved human favorability despite their limitations in capturing indigenous knowledge and cultural values in Indonesia. In addition, we discuss the shortcomings of parameter-efficient tunings, such as LoRA, for language adaptation. Alternatively, we propose the usage of vocabulary adaptation to enhance efficiency. Lastly, we evaluate the safety of Cendol and showcase that safety in pre-training in one language such as English is transferable to low-resource languages, such as Indonesian, even without RLHF and safety fine-tuning.","sentences":["Large language models (LLMs) show remarkable human-like capability in various domains and languages.","However, a notable quality gap arises in low-resource languages, e.g., Indonesian indigenous languages, rendering them ineffective and inefficient in such linguistic contexts.","To bridge this quality gap, we introduce Cendol, a collection of Indonesian LLMs encompassing both decoder-only and encoder-decoder architectures across a range of model sizes.","We highlight Cendol's effectiveness across a diverse array of tasks, attaining 20% improvement, and demonstrate its capability to generalize to unseen tasks and indigenous languages of Indonesia.","Furthermore, Cendol models showcase improved human favorability despite their limitations in capturing indigenous knowledge and cultural values in Indonesia.","In addition, we discuss the shortcomings of parameter-efficient tunings, such as LoRA, for language adaptation.","Alternatively, we propose the usage of vocabulary adaptation to enhance efficiency.","Lastly, we evaluate the safety of Cendol and showcase that safety in pre-training in one language such as English is transferable to low-resource languages, such as Indonesian, even without RLHF and safety fine-tuning."],"url":"http://arxiv.org/abs/2404.06138v1","category":"cs.CL"}
{"created":"2024-04-09 09:03:44","title":"SmurfCat at SemEval-2024 Task 6: Leveraging Synthetic Data for Hallucination Detection","abstract":"In this paper, we present our novel systems developed for the SemEval-2024 hallucination detection task. Our investigation spans a range of strategies to compare model predictions with reference standards, encompassing diverse baselines, the refinement of pre-trained encoders through supervised learning, and an ensemble approaches utilizing several high-performing models. Through these explorations, we introduce three distinct methods that exhibit strong performance metrics. To amplify our training data, we generate additional training samples from unlabelled training subset. Furthermore, we provide a detailed comparative analysis of our approaches. Notably, our premier method achieved a commendable 9th place in the competition's model-agnostic track and 17th place in model-aware track, highlighting its effectiveness and potential.","sentences":["In this paper, we present our novel systems developed for the SemEval-2024 hallucination detection task.","Our investigation spans a range of strategies to compare model predictions with reference standards, encompassing diverse baselines, the refinement of pre-trained encoders through supervised learning, and an ensemble approaches utilizing several high-performing models.","Through these explorations, we introduce three distinct methods that exhibit strong performance metrics.","To amplify our training data, we generate additional training samples from unlabelled training subset.","Furthermore, we provide a detailed comparative analysis of our approaches.","Notably, our premier method achieved a commendable 9th place in the competition's model-agnostic track and 17th place in model-aware track, highlighting its effectiveness and potential."],"url":"http://arxiv.org/abs/2404.06137v1","category":"cs.CL"}
{"created":"2024-04-09 08:51:05","title":"FLEX: FLEXible Federated Learning Framework","abstract":"In the realm of Artificial Intelligence (AI), the need for privacy and security in data processing has become paramount. As AI applications continue to expand, the collection and handling of sensitive data raise concerns about individual privacy protection. Federated Learning (FL) emerges as a promising solution to address these challenges by enabling decentralized model training on local devices, thus preserving data privacy. This paper introduces FLEX: a FLEXible Federated Learning Framework designed to provide maximum flexibility in FL research experiments. By offering customizable features for data distribution, privacy parameters, and communication strategies, FLEX empowers researchers to innovate and develop novel FL techniques. The framework also includes libraries for specific FL implementations including: (1) anomalies, (2) blockchain, (3) adversarial attacks and defences, (4) natural language processing and (5) decision trees, enhancing its versatility and applicability in various domains. Overall, FLEX represents a significant advancement in FL research, facilitating the development of robust and efficient FL applications.","sentences":["In the realm of Artificial Intelligence (AI), the need for privacy and security in data processing has become paramount.","As AI applications continue to expand, the collection and handling of sensitive data raise concerns about individual privacy protection.","Federated Learning (FL) emerges as a promising solution to address these challenges by enabling decentralized model training on local devices, thus preserving data privacy.","This paper introduces FLEX: a FLEXible Federated Learning Framework designed to provide maximum flexibility in FL research experiments.","By offering customizable features for data distribution, privacy parameters, and communication strategies, FLEX empowers researchers to innovate and develop novel FL techniques.","The framework also includes libraries for specific FL implementations including: (1) anomalies, (2) blockchain, (3) adversarial attacks and defences, (4) natural language processing and (5) decision trees, enhancing its versatility and applicability in various domains.","Overall, FLEX represents a significant advancement in FL research, facilitating the development of robust and efficient FL applications."],"url":"http://arxiv.org/abs/2404.06127v1","category":"cs.CR"}
{"created":"2024-04-09 08:49:01","title":"Hierarchical Insights: Exploiting Structural Similarities for Reliable 3D Semantic Segmentation","abstract":"Safety-critical applications like autonomous driving call for robust 3D environment perception algorithms which can withstand highly diverse and ambiguous surroundings. The predictive performance of any classification model strongly depends on the underlying dataset and the prior knowledge conveyed by the annotated labels. While the labels provide a basis for the learning process, they usually fail to represent inherent relations between the classes - representations, which are a natural element of the human perception system. We propose a training strategy which enables a 3D LiDAR semantic segmentation model to learn structural relationships between the different classes through abstraction. We achieve this by implicitly modeling those relationships through a learning rule for hierarchical multi-label classification (HMC). With a detailed analysis we show, how this training strategy not only improves the model's confidence calibration, but also preserves additional information for downstream tasks like fusion, prediction and planning.","sentences":["Safety-critical applications like autonomous driving call for robust 3D environment perception algorithms which can withstand highly diverse and ambiguous surroundings.","The predictive performance of any classification model strongly depends on the underlying dataset and the prior knowledge conveyed by the annotated labels.","While the labels provide a basis for the learning process, they usually fail to represent inherent relations between the classes - representations, which are a natural element of the human perception system.","We propose a training strategy which enables a 3D LiDAR semantic segmentation model to learn structural relationships between the different classes through abstraction.","We achieve this by implicitly modeling those relationships through a learning rule for hierarchical multi-label classification (HMC).","With a detailed analysis we show, how this training strategy not only improves the model's confidence calibration, but also preserves additional information for downstream tasks like fusion, prediction and planning."],"url":"http://arxiv.org/abs/2404.06124v1","category":"cs.CV"}
{"created":"2024-04-10 17:53:33","title":"Laser driven melt pool resonances through dynamically oscillating energy inputs","abstract":"Spatially selective melting of metal materials by laser irradiation allows for the precise welding as well as the 3D printing of complex metal parts. However, the simple scanning of a conventional Gaussian beam typically results in a melt track with randomly distributed surface features due to the complex and dynamic behavior of the melt pool. In this study, the implications of utilizing a dynamically oscillating energy input on driving melt track fluctuations is investigated. Specifically, the laser intensity and/or intensity distribution is sinusoidally modulated at different scan speeds, and the effect of modulation frequency on the resulting surface features of the melt track is examined. The formation of periodically oriented surface features indicates an evident frequency coupling between the melt pool and the modulation frequency. Moreover, such a frequency coupling becomes most prominent under a specific modulation frequency, suggesting resonant behavior. The insights provided in this study will enable the development of novel methods, allowing for the control and/or mitigation of inherent fluctuations in the melt pool through laser-driven resonances.","sentences":["Spatially selective melting of metal materials by laser irradiation allows for the precise welding as well as the 3D printing of complex metal parts.","However, the simple scanning of a conventional Gaussian beam typically results in a melt track with randomly distributed surface features due to the complex and dynamic behavior of the melt pool.","In this study, the implications of utilizing a dynamically oscillating energy input on driving melt track fluctuations is investigated.","Specifically, the laser intensity and/or intensity distribution is sinusoidally modulated at different scan speeds, and the effect of modulation frequency on the resulting surface features of the melt track is examined.","The formation of periodically oriented surface features indicates an evident frequency coupling between the melt pool and the modulation frequency.","Moreover, such a frequency coupling becomes most prominent under a specific modulation frequency, suggesting resonant behavior.","The insights provided in this study will enable the development of novel methods, allowing for the control and/or mitigation of inherent fluctuations in the melt pool through laser-driven resonances."],"url":"http://arxiv.org/abs/2404.07195v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-10 17:31:49","title":"BAMBOO: a predictive and transferable machine learning force field framework for liquid electrolyte development","abstract":"Despite the widespread applications of machine learning force field (MLFF) on solids and small molecules, there is a notable gap in applying MLFF to complex liquid electrolytes. In this work, we introduce BAMBOO (ByteDance AI Molecular Simulation Booster), a novel framework for molecular dynamics (MD) simulations, with a demonstration of its capabilities in the context of liquid electrolytes for lithium batteries. We design a physics-inspired graph equivariant transformer architecture as the backbone of BAMBOO to learn from quantum mechanical simulations. Additionally, we pioneer an ensemble knowledge distillation approach and apply it on MLFFs to improve the stability of MD simulations. Finally, we propose the density alignment algorithm to align BAMBOO with experimental measurements. BAMBOO demonstrates state-of-the-art accuracy in predicting key electrolyte properties such as density, viscosity, and ionic conductivity across various solvents and salt combinations. Our current model, trained on more than 15 chemical species, achieves the average density error of 0.01 g/cm^3 on various compositions compared with experimental data. Moreover, our model demonstrates transferability to molecules not included in the quantum mechanical dataset. We envision this work as paving the way to a ''universal MLFF'' capable of simulating properties of common organic liquids.","sentences":["Despite the widespread applications of machine learning force field (MLFF) on solids and small molecules, there is a notable gap in applying MLFF to complex liquid electrolytes.","In this work, we introduce BAMBOO (ByteDance AI Molecular Simulation Booster), a novel framework for molecular dynamics (MD) simulations, with a demonstration of its capabilities in the context of liquid electrolytes for lithium batteries.","We design a physics-inspired graph equivariant transformer architecture as the backbone of BAMBOO to learn from quantum mechanical simulations.","Additionally, we pioneer an ensemble knowledge distillation approach and apply it on MLFFs to improve the stability of MD simulations.","Finally, we propose the density alignment algorithm to align BAMBOO with experimental measurements.","BAMBOO demonstrates state-of-the-art accuracy in predicting key electrolyte properties such as density, viscosity, and ionic conductivity across various solvents and salt combinations.","Our current model, trained on more than 15 chemical species, achieves the average density error of 0.01 g/cm^3 on various compositions compared with experimental data.","Moreover, our model demonstrates transferability to molecules not included in the quantum mechanical dataset.","We envision this work as paving the way to a ''universal MLFF'' capable of simulating properties of common organic liquids."],"url":"http://arxiv.org/abs/2404.07181v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-10 17:29:12","title":"Machine learning-based similarity measure to forecast M&A from patent data","abstract":"Defining and finalizing Mergers and Acquisitions (M&A) requires complex human skills, which makes it very hard to automatically find the best partner or predict which firms will make a deal. In this work, we propose the MASS algorithm, a specifically designed measure of similarity between companies and we apply it to patenting activity data to forecast M&A deals. MASS is based on an extreme simplification of tree-based machine learning algorithms and naturally incorporates intuitive criteria for deals; as such, it is fully interpretable and explainable. By applying MASS to the Zephyr and Crunchbase datasets, we show that it outperforms LightGCN, a \"black box\" graph convolutional network algorithm. When similar companies have disjoint patenting activities, on the contrary, LightGCN turns out to be the most effective algorithm. This study provides a simple and powerful tool to model and predict M&A deals, offering valuable insights to managers and practitioners for informed decision-making.","sentences":["Defining and finalizing Mergers and Acquisitions (M&A) requires complex human skills, which makes it very hard to automatically find the best partner or predict which firms will make a deal.","In this work, we propose the MASS algorithm, a specifically designed measure of similarity between companies and we apply it to patenting activity data to forecast M&A deals.","MASS is based on an extreme simplification of tree-based machine learning algorithms and naturally incorporates intuitive criteria for deals; as such, it is fully interpretable and explainable.","By applying MASS to the Zephyr and Crunchbase datasets, we show that it outperforms LightGCN, a \"black box\" graph convolutional network algorithm.","When similar companies have disjoint patenting activities, on the contrary, LightGCN turns out to be the most effective algorithm.","This study provides a simple and powerful tool to model and predict M&A deals, offering valuable insights to managers and practitioners for informed decision-making."],"url":"http://arxiv.org/abs/2404.07179v1","category":"physics.soc-ph"}
{"created":"2024-04-10 17:17:05","title":"Ground state-based quantum feature maps","abstract":"We introduce a quantum data embedding protocol based on the preparation of a ground state of a parameterized Hamiltonian. We analyze the corresponding quantum feature map, recasting it as an adiabatic state preparation procedure with Trotterized evolution. We compare the properties of underlying quantum models with ubiquitous Fourier-type quantum models, and show that ground state embeddings can be described effectively by a spectrum with degree that grows rapidly with the number of qubits, corresponding to a large model capacity. We observe that the spectrum contains massive frequency degeneracies, and the weighting coefficients for the modes are highly structured, thus limiting model expressivity. Our results provide a step towards understanding models based on quantum data, and contribute to fundamental knowledge needed for building efficient quantum machine learning (QML) protocols. As non-trivial embeddings are crucial for designing QML protocols that cannot be simulated classically, our findings guide the search for high-capacity quantum models that can largely outperform classical models.","sentences":["We introduce a quantum data embedding protocol based on the preparation of a ground state of a parameterized Hamiltonian.","We analyze the corresponding quantum feature map, recasting it as an adiabatic state preparation procedure with Trotterized evolution.","We compare the properties of underlying quantum models with ubiquitous Fourier-type quantum models, and show that ground state embeddings can be described effectively by a spectrum with degree that grows rapidly with the number of qubits, corresponding to a large model capacity.","We observe that the spectrum contains massive frequency degeneracies, and the weighting coefficients for the modes are highly structured, thus limiting model expressivity.","Our results provide a step towards understanding models based on quantum data, and contribute to fundamental knowledge needed for building efficient quantum machine learning (QML) protocols.","As non-trivial embeddings are crucial for designing QML protocols that cannot be simulated classically, our findings guide the search for high-capacity quantum models that can largely outperform classical models."],"url":"http://arxiv.org/abs/2404.07174v1","category":"quant-ph"}
{"created":"2024-04-10 17:08:07","title":"Unlocking Quantum Optimization: A Use Case Study on NISQ Systems","abstract":"The major advances in quantum computing over the last few decades have sparked great interest in applying it to solve the most challenging computational problems in a wide variety of areas. One of the most pronounced domains here are optimization problems and a number of algorithmic approaches have been proposed for their solution. For the current noisy intermediate-scale quantum (NISQ) computers the quantum approximate optimization algorithm (QAOA), the variational quantum eigensolver (VQE), and quantum annealing (QA) are the central algorithms for this problem class. The two former can be executed on digital gate-model quantum computers, whereas the latter requires a quantum annealer. Across all hardware architectures and manufactures, the quantum computers available today share the property of being too error-prone to reliably execute involved quantum circuits as they typically arise from quantum optimization algorithms. In order to characterize the limits of existing quantum computers, many component and system level benchmarks have been proposed. However, owing to the complex nature of the errors in quantum systems these benchmark fail to provide predictive power beyond simple quantum circuits and small examples. Application oriented benchmarks have been proposed to remedy this problem, but both, results from real quantum systems as well as use cases beyond constructed academic examples, remain very rare. This paper addresses precisely this gap by considering two industrial relevant use cases: one in the realm of optimizing charging schedules for electric vehicles, the other concerned with the optimization of truck routes. Our central contribution are systematic series of examples derived from these uses cases that we execute on different processors of the gate-based quantum computers of IBM as well as on the quantum annealer of D-Wave.","sentences":["The major advances in quantum computing over the last few decades have sparked great interest in applying it to solve the most challenging computational problems in a wide variety of areas.","One of the most pronounced domains here are optimization problems and a number of algorithmic approaches have been proposed for their solution.","For the current noisy intermediate-scale quantum (NISQ) computers the quantum approximate optimization algorithm (QAOA), the variational quantum eigensolver (VQE), and quantum annealing (QA) are the central algorithms for this problem class.","The two former can be executed on digital gate-model quantum computers, whereas the latter requires a quantum annealer.","Across all hardware architectures and manufactures, the quantum computers available today share the property of being too error-prone to reliably execute involved quantum circuits as they typically arise from quantum optimization algorithms.","In order to characterize the limits of existing quantum computers, many component and system level benchmarks have been proposed.","However, owing to the complex nature of the errors in quantum systems these benchmark fail to provide predictive power beyond simple quantum circuits and small examples.","Application oriented benchmarks have been proposed to remedy this problem, but both, results from real quantum systems as well as use cases beyond constructed academic examples, remain very rare.","This paper addresses precisely this gap by considering two industrial relevant use cases: one in the realm of optimizing charging schedules for electric vehicles, the other concerned with the optimization of truck routes.","Our central contribution are systematic series of examples derived from these uses cases that we execute on different processors of the gate-based quantum computers of IBM as well as on the quantum annealer of D-Wave."],"url":"http://arxiv.org/abs/2404.07171v1","category":"quant-ph"}
{"created":"2024-04-10 17:02:09","title":"Defect-engineering hexagonal boron nitride using low-energy Ar+ irradiation","abstract":"Monolayer hexagonal boron nitride (hBN) has recently become the focus of intense research as a material to host quantum emitters. Although it is well known that such emission is associated with point defects, so far no conclusive correlation between the spectra and specific defects has been demonstrated. Here, we prepare atomically clean suspended hBN samples and subject them to low-energy ion irradiation. The samples are characterized before and after irradiation via automated scanning transmission electron microscopy imaging to assess the defect concentrations and distributions. We find an intrinsic defect concentration of ca. 0.03/nm2 (with ca. 55% boron and 8% nitrogen single vacancies, 20% double vacancies and 16% more complex vacancy structures). To be able to differentiate between these and irradiation-induced defects, we create a significantly higher (but still moderate) concentration of defects with the ions (0.30/nm2), and now find ca. 55% boron and 12% nitrogen single vacancies, 14% double vacancies, and 18% more complex vacancy structures. The results demonstrate that already the simplest irradiation provides selectivity for the defect types, and open the way for future experiments to explore changing the selectivity by modifying the irradiation parameters.","sentences":["Monolayer hexagonal boron nitride (hBN) has recently become the focus of intense research as a material to host quantum emitters.","Although it is well known that such emission is associated with point defects, so far no conclusive correlation between the spectra and specific defects has been demonstrated.","Here, we prepare atomically clean suspended hBN samples and subject them to low-energy ion irradiation.","The samples are characterized before and after irradiation via automated scanning transmission electron microscopy imaging to assess the defect concentrations and distributions.","We find an intrinsic defect concentration of ca.","0.03/nm2 (with ca.","55% boron and 8% nitrogen single vacancies, 20% double vacancies and 16% more complex vacancy structures).","To be able to differentiate between these and irradiation-induced defects, we create a significantly higher (but still moderate) concentration of defects with the ions (0.30/nm2), and now find ca.","55% boron and 12% nitrogen single vacancies, 14% double vacancies, and 18% more complex vacancy structures.","The results demonstrate that already the simplest irradiation provides selectivity for the defect types, and open the way for future experiments to explore changing the selectivity by modifying the irradiation parameters."],"url":"http://arxiv.org/abs/2404.07166v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-10 17:02:00","title":"Pressure-tuned many-body phases through $\u0393$-K valleytronics in moir\u00e9 bilayer WSe$_2$","abstract":"Recent experiments in twisted bilayer transition-metal dichalcogenides have revealed a variety of strongly correlated phenomena. To theoretically explore their origin, we combine here ab initio calculations with correlated model approaches to describe and study many-body effects in twisted bilayer WSe$_2$ under pressure. We find that the interlayer distance is a key factor for the electronic structure, as it tunes the relative energetic positions between the K and the $\\Gamma$ valleys of the valence band maximum of the untwisted bilayer. As a result, applying uniaxial pressure to a twisted bilayer induces a charge-transfer from the K valley to the flat bands in the $\\Gamma$ valley. Upon Wannierizing moir\\'e bands from both valleys, we establish the relevant tight-binding model parameters and calculate the effective interaction strengths using the constrained random phase approximation. With this, we approximate the interacting pressure-doping phase diagram of WSe$_2$ moir\\'e bilayers using self-consistent mean field theory. Our results establish twisted bilayer WSe$_2$ as a platform that allows the direct pressure-tuning of different correlated phases, ranging from Mott insulators, charge-valley-transfer insulators to Kondo lattice-like systems.","sentences":["Recent experiments in twisted bilayer transition-metal dichalcogenides have revealed a variety of strongly correlated phenomena.","To theoretically explore their origin, we combine here ab initio calculations with correlated model approaches to describe and study many-body effects in twisted bilayer WSe$_2$ under pressure.","We find that the interlayer distance is a key factor for the electronic structure, as it tunes the relative energetic positions between the K and the $\\Gamma$ valleys of the valence band maximum of the untwisted bilayer.","As a result, applying uniaxial pressure to a twisted bilayer induces a charge-transfer from the K valley to the flat bands in the $\\Gamma$ valley.","Upon Wannierizing moir\\'e bands from both valleys, we establish the relevant tight-binding model parameters and calculate the effective interaction strengths using the constrained random phase approximation.","With this, we approximate the interacting pressure-doping phase diagram of WSe$_2$ moir\\'e bilayers using self-consistent mean field theory.","Our results establish twisted bilayer WSe$_2$ as a platform that allows the direct pressure-tuning of different correlated phases, ranging from Mott insulators, charge-valley-transfer insulators to Kondo lattice-like systems."],"url":"http://arxiv.org/abs/2404.07165v1","category":"cond-mat.str-el"}
{"created":"2024-04-10 16:50:59","title":"Polluting White Dwarfs with Oort Cloud Comets","abstract":"Observations point to old white dwarfs (WDs) accreting metals at a relatively constant rate over 8~Gyrs. Exo-Oort clouds around WDs have been proposed as potential reservoirs of materials, with galactic tide as a mechanism to deliver distant comets to the WD's Roche limit. In this work, we characterise the dynamics of comets around a WD with a companion having semi-major axes on the orders of 10 - 100 AU. We develop simulation techniques capable of integrating a large number ($10^8$) of objects over a 1 Gyr timescale. Our simulations include galactic tide and are capable of resolving close-interactions with a massive companion. Through simulations, we study the accretion rate of exo-Oort cloud comets into a WD's Roche limit. We also characterise the dynamics of precession and scattering induced on a comet by a massive companion. We find that (i) WD pollution by an exo-Oort cloud can be sustained over a Gyr timescale, (ii) an exo-Oort cloud with structure like our own Solar System's is capable of delivering materials into an isolated WD with pollution rate $\\sim 10^8 \\mathrm{~g~s^{-1}}$, (iii) adding a planetary-mass companion reduces the pollution rate to $\\sim 10^7 \\mathrm{~g~s^{-1}}$, and (iv) if the companion is stellar-mass, with $M_p \\gtrsim 0.1 M_\\odot$, the pollution rate reduces to $\\sim 3 \\times 10^5 \\mathrm{~g~s^{-1}}$ due to a combination of precession induced on a comet by the companion, a strong scattering barrier, and low-likelihood of direct collisions of comets with the companion.","sentences":["Observations point to old white dwarfs (WDs) accreting metals at a relatively constant rate over 8~Gyrs.","Exo-Oort clouds around WDs have been proposed as potential reservoirs of materials, with galactic tide as a mechanism to deliver distant comets to the WD's Roche limit.","In this work, we characterise the dynamics of comets around a WD with a companion having semi-major axes on the orders of 10 - 100 AU.","We develop simulation techniques capable of integrating a large number ($10^8$) of objects over a 1 Gyr timescale.","Our simulations include galactic tide and are capable of resolving close-interactions with a massive companion.","Through simulations, we study the accretion rate of exo-Oort cloud comets into a WD's Roche limit.","We also characterise the dynamics of precession and scattering induced on a comet by a massive companion.","We find that (i) WD pollution by an exo-Oort cloud can be sustained over a Gyr timescale, (ii) an exo-Oort cloud with structure like our own Solar System's is capable of delivering materials into an isolated WD with pollution rate $\\sim 10^8","\\mathrm{~g~s^{-1}}$, (iii) adding a planetary-mass companion reduces the pollution rate to $\\sim 10^7","\\mathrm{~g~s^{-1}}$, and (iv) if the companion is stellar-mass, with $M_p \\gtrsim 0.1 M_\\odot$, the pollution rate reduces to $\\sim 3 \\times 10^5","\\mathrm{~g~s^{-1}}$ due to a combination of precession induced on a comet by the companion, a strong scattering barrier, and low-likelihood of direct collisions of comets with the companion."],"url":"http://arxiv.org/abs/2404.07160v1","category":"astro-ph.EP"}
{"created":"2024-04-10 16:47:04","title":"Local probe of bulk and edge states in a fractional Chern insulator","abstract":"Fractional quantum Hall effect (FQHE) is a prime example of topological quantum many-body phenomena, arising from the interplay between strong electron correlation, topological order, and time reversal symmetry breaking. Recently, a lattice analog of FQHE at zero magnetic field has been observed, confirming the existence of a zero-field fractional Chern insulator (FCI). Despite this, the bulk-edge correspondence -- a hallmark of FCI featuring an insulating bulk with conductive edges -- has not been directly observed. In fact, this correspondence has not been visualized in any system for fractional states due to experimental challenges. Here we report the imaging of FCI edge states in twisted MoTe2 by employing a newly developed modality of microwave-impedance microscopy. By tuning the carrier density, we observe the system evolving between metallic and FCI states, the latter of which exhibits insulating bulk and conductive edges as expected from bulk-boundary correspondence. We also observe the evolution of edge states across the topological phase transition from an incompressible Chern insulator state to a metal and finally to a putative charge ordered insulating state as a function of interlayer electric field. The local measurement further reveals tantalizing prospects of neighboring domains with different fractional orders. These findings pave the way for research into topologically protected 1D interfaces between various anyonic states at zero magnetic field, such as topological entanglement entropy, Halperin-Laughlin interfaces, and the creation of non-abelian anyons.","sentences":["Fractional quantum Hall effect (FQHE) is a prime example of topological quantum many-body phenomena, arising from the interplay between strong electron correlation, topological order, and time reversal symmetry breaking.","Recently, a lattice analog of FQHE at zero magnetic field has been observed, confirming the existence of a zero-field fractional Chern insulator (FCI).","Despite this, the bulk-edge correspondence -- a hallmark of FCI featuring an insulating bulk with conductive edges -- has not been directly observed.","In fact, this correspondence has not been visualized in any system for fractional states due to experimental challenges.","Here we report the imaging of FCI edge states in twisted MoTe2 by employing a newly developed modality of microwave-impedance microscopy.","By tuning the carrier density, we observe the system evolving between metallic and FCI states, the latter of which exhibits insulating bulk and conductive edges as expected from bulk-boundary correspondence.","We also observe the evolution of edge states across the topological phase transition from an incompressible Chern insulator state to a metal and finally to a putative charge ordered insulating state as a function of interlayer electric field.","The local measurement further reveals tantalizing prospects of neighboring domains with different fractional orders.","These findings pave the way for research into topologically protected 1D interfaces between various anyonic states at zero magnetic field, such as topological entanglement entropy, Halperin-Laughlin interfaces, and the creation of non-abelian anyons."],"url":"http://arxiv.org/abs/2404.07157v1","category":"cond-mat.str-el"}
{"created":"2024-04-10 16:35:36","title":"Logarithmic-Depth Quantum Circuits for Hamming Weight Projections","abstract":"A pure state of fixed Hamming weight is a superposition of computational basis states such that each bitstring in the superposition has the same number of ones. Given a Hilbert space of the form $\\mathcal{H} = (\\mathbb{C}_2)^{\\otimes n}$, or an $n$-qubit system, the identity operator can be decomposed as a sum of projectors onto subspaces of fixed Hamming weight. In this work, we propose several quantum algorithms that realize a coherent Hamming weight projective measurement on an input pure state, meaning that the post-measurement state of the algorithm is the projection of the input state onto the corresponding subspace of fixed Hamming weight. We analyze a depth-width trade-off for the corresponding quantum circuits, allowing for a depth reduction of the circuits at the cost of more control qubits. For an $n$-qubit input, the depth-optimal algorithm uses $O(n)$ control qubits and the corresponding circuit has depth $O(\\log (n))$, assuming that we have the ability to perform qubit resets. Furthermore, the proposed algorithm construction uses only one- and two-qubit gates.","sentences":["A pure state of fixed Hamming weight is a superposition of computational basis states such that each bitstring in the superposition has the same number of ones.","Given a Hilbert space of the form $\\mathcal{H} = (\\mathbb{C}_2)^{\\otimes n}$, or an $n$-qubit system, the identity operator can be decomposed as a sum of projectors onto subspaces of fixed Hamming weight.","In this work, we propose several quantum algorithms that realize a coherent Hamming weight projective measurement on an input pure state, meaning that the post-measurement state of the algorithm is the projection of the input state onto the corresponding subspace of fixed Hamming weight.","We analyze a depth-width trade-off for the corresponding quantum circuits, allowing for a depth reduction of the circuits at the cost of more control qubits.","For an $n$-qubit input, the depth-optimal algorithm uses $O(n)$ control qubits and the corresponding circuit has depth $O(\\log (n))$, assuming that we have the ability to perform qubit resets.","Furthermore, the proposed algorithm construction uses only one-","and two-qubit gates."],"url":"http://arxiv.org/abs/2404.07151v1","category":"quant-ph"}
{"created":"2024-04-10 16:33:24","title":"Tianyu: search for the second solar system and explore the dynamic universe","abstract":"Giant planets like Jupiter and Saturn, play important roles in the formation and habitability of Earth-like planets. The detection of solar system analogs that have multiple cold giant planets is essential for our understanding of planet habitability and planet formation. Although transit surveys such as Kepler and TESS have discovered thousands of exoplanets, these missions are not sensitive to long period planets due to their limited observation baseline. The Tianyu project, comprising two 1-meter telescopes (Tianyu-I and II), is designed to detect transiting cold giant planets in order to find solar system analogs. Featuring a large field of view and equipped with a high-speed CMOS camera, Tianyu-I will perform a high-precision photometric survey of about 100 million stars, measuring light curves at hour-long cadence. The candidates found by Tianyu-I will be confirmed by Tianyu-II and other surveys and follow-up facilities through multi-band photometry, spectroscopy, and high resolution imaging. Tianyu telescopes will be situated at an elevation about 4000 meters in Lenghu, China. With a photometric precision of 1% for stars with V < 18 mag, Tianyu is expected to find more than 300 transiting exoplanets, including about 12 cold giant planets, over five years. A five-year survey of Tianyu would discover 1-2 solar system analogs. Moreover, Tianyu is also designed for non-exoplanetary exploration, incorporating multiple survey modes covering timescales from sub-seconds to months, with a particular emphasis on events occurring within the sub-second to hour range. It excels in observing areas such as infant supernovae, rare variable stars and binaries, tidal disruption events, Be stars, cometary activities, and interstellar objects. These discoveries not only enhance our comprehension of the universe but also offer compelling opportunities for public engagement in scientific exploration.","sentences":["Giant planets like Jupiter and Saturn, play important roles in the formation and habitability of Earth-like planets.","The detection of solar system analogs that have multiple cold giant planets is essential for our understanding of planet habitability and planet formation.","Although transit surveys such as Kepler and TESS have discovered thousands of exoplanets, these missions are not sensitive to long period planets due to their limited observation baseline.","The Tianyu project, comprising two 1-meter telescopes (Tianyu-I and II), is designed to detect transiting cold giant planets in order to find solar system analogs.","Featuring a large field of view and equipped with a high-speed CMOS camera, Tianyu-I will perform a high-precision photometric survey of about 100 million stars, measuring light curves at hour-long cadence.","The candidates found by Tianyu-I will be confirmed by Tianyu-II and other surveys and follow-up facilities through multi-band photometry, spectroscopy, and high resolution imaging.","Tianyu telescopes will be situated at an elevation about 4000 meters in Lenghu, China.","With a photometric precision of 1% for stars with V < 18 mag, Tianyu is expected to find more than 300 transiting exoplanets, including about 12 cold giant planets, over five years.","A five-year survey of Tianyu would discover 1-2 solar system analogs.","Moreover, Tianyu is also designed for non-exoplanetary exploration, incorporating multiple survey modes covering timescales from sub-seconds to months, with a particular emphasis on events occurring within the sub-second to hour range.","It excels in observing areas such as infant supernovae, rare variable stars and binaries, tidal disruption events, Be stars, cometary activities, and interstellar objects.","These discoveries not only enhance our comprehension of the universe but also offer compelling opportunities for public engagement in scientific exploration."],"url":"http://arxiv.org/abs/2404.07149v2","category":"astro-ph.IM"}
{"created":"2024-04-10 16:17:41","title":"Characterising directed and undirected metrics of high-order interdependence","abstract":"Systems of interest for theoretical or experimental work often exhibit high-order interactions, corresponding to statistical interdependencies in groups of variables that cannot be reduced to dependencies in subsets of them. While still under active development, the framework of partial information decomposition (PID) has emerged as the dominant approach to conceptualise and calculate high-order interdependencies. PID approaches can be grouped in two types: directed approaches that divide variables into sources and targets, and undirected approaches that treat all variables equally. Directed and undirected approaches are usually employed to investigate different scenarios, and hence little is known about how these two types of approaches may relate to each other, or if their corresponding quantities are linked in some way. In this paper we investigate the relationship between the redundancy-synergy index (RSI) and the O-information, which are practical metrics of directed and undirected high-order interdependencies, respectively. Our results reveal tight links between these two quantities, and provide interpretations of them in terms of likelihood ratios in a hypothesis testing setting, as well as in terms of projections in information geometry.","sentences":["Systems of interest for theoretical or experimental work often exhibit high-order interactions, corresponding to statistical interdependencies in groups of variables that cannot be reduced to dependencies in subsets of them.","While still under active development, the framework of partial information decomposition (PID) has emerged as the dominant approach to conceptualise and calculate high-order interdependencies.","PID approaches can be grouped in two types: directed approaches that divide variables into sources and targets, and undirected approaches that treat all variables equally.","Directed and undirected approaches are usually employed to investigate different scenarios, and hence little is known about how these two types of approaches may relate to each other, or if their corresponding quantities are linked in some way.","In this paper we investigate the relationship between the redundancy-synergy index (RSI) and the O-information, which are practical metrics of directed and undirected high-order interdependencies, respectively.","Our results reveal tight links between these two quantities, and provide interpretations of them in terms of likelihood ratios in a hypothesis testing setting, as well as in terms of projections in information geometry."],"url":"http://arxiv.org/abs/2404.07140v1","category":"cs.IT"}
{"created":"2024-04-10 16:10:59","title":"Investigating Ocean Circulation Dynamics Through Data Assimilation: A Mathematical Study Using the Stommel Box Model with Rapid Oscillatory Forcings","abstract":"We investigate ocean circulation changes through the lens of data assimilation using a reduced-order model. Our primary interest lies in the Stommel box model which reveals itself to be one of the most practicable models that has the ability of reproducing the meridional overturning circulation. The Stommel box model has at most two regimes: TH (temperature driven circulation with sinking near the north pole) and SA (salinity driven with sinking near the equator). Currently, the meridional overturning is in the TH regime. Using box-averaged Met Office EN4 ocean temperature and salinity data, our goal is to provide a probability that a future regime change occurs and establish how this probability depends on the uncertainties in initial conditions, parameters and forcings. We will achieve this using data assimilation tools and DAPPER within the Stommel box model with fast oscillatory regimes.","sentences":["We investigate ocean circulation changes through the lens of data assimilation using a reduced-order model.","Our primary interest lies in the Stommel box model which reveals itself to be one of the most practicable models that has the ability of reproducing the meridional overturning circulation.","The Stommel box model has at most two regimes: TH (temperature driven circulation with sinking near the north pole) and SA (salinity driven with sinking near the equator).","Currently, the meridional overturning is in the TH regime.","Using box-averaged Met Office EN4 ocean temperature and salinity data, our goal is to provide a probability that a future regime change occurs and establish how this probability depends on the uncertainties in initial conditions, parameters and forcings.","We will achieve this using data assimilation tools and DAPPER within the Stommel box model with fast oscillatory regimes."],"url":"http://arxiv.org/abs/2404.07134v1","category":"math.DS"}
{"created":"2024-04-10 16:10:34","title":"Dynamic Mode Decomposition with Non-uniform Sampling","abstract":"Dynamic Mode Decomposition (DMD) and its extensions (EDMD) have been at the forefront of data-based approaches to Koopman operators. Most (E)DMD algorithms assume that the entire state is sampled at a uniform sampling rate. In this paper, we provide an algorithm where the entire state is not uniformly sampled, with individual components of the states measured at individual (but known) sampling rates. We propose a two-step DMD algorithm where the first step performs Hankel DMD on individual state components to estimate them at specified time instants. With the entire state reconstructed at the same time instants, we compute the (E)DMD for the system with the estimated data in the second step.","sentences":["Dynamic Mode Decomposition (DMD) and its extensions (EDMD) have been at the forefront of data-based approaches to Koopman operators.","Most (E)DMD algorithms assume that the entire state is sampled at a uniform sampling rate.","In this paper, we provide an algorithm where the entire state is not uniformly sampled, with individual components of the states measured at individual (but known) sampling rates.","We propose a two-step DMD algorithm where the first step performs Hankel DMD on individual state components to estimate them at specified time instants.","With the entire state reconstructed at the same time instants, we compute the (E)DMD for the system with the estimated data in the second step."],"url":"http://arxiv.org/abs/2404.07133v1","category":"eess.SY"}
{"created":"2024-04-10 16:04:49","title":"Strengthening Lasserre's Hierarchy in Real and Complex Polynomial Optimization","abstract":"We establish a connection between multiplication operators and shift operators. Moreover, we derive positive semidefinite conditions of finite rank moment sequences and use these conditions to strengthen Lasserre's hierarchy for real and complex polynomial optimization. Integration of the strengthening technique with sparsity is considered. Extensive numerical experiments show that our strengthening technique can significantly improve the bound (especially for complex polynomial optimization) and allows to achieve global optimality at lower relaxation orders, thus providing substantial computational savings.","sentences":["We establish a connection between multiplication operators and shift operators.","Moreover, we derive positive semidefinite conditions of finite rank moment sequences and use these conditions to strengthen Lasserre's hierarchy for real and complex polynomial optimization.","Integration of the strengthening technique with sparsity is considered.","Extensive numerical experiments show that our strengthening technique can significantly improve the bound (especially for complex polynomial optimization) and allows to achieve global optimality at lower relaxation orders, thus providing substantial computational savings."],"url":"http://arxiv.org/abs/2404.07125v1","category":"math.OC"}
{"created":"2024-04-10 16:01:37","title":"Driver Attention Tracking and Analysis","abstract":"We propose a novel method to estimate a driver's points-of-gaze using a pair of ordinary cameras mounted on the windshield and dashboard of a car. This is a challenging problem due to the dynamics of traffic environments with 3D scenes of unknown depths. This problem is further complicated by the volatile distance between the driver and the camera system. To tackle these challenges, we develop a novel convolutional network that simultaneously analyzes the image of the scene and the image of the driver's face. This network has a camera calibration module that can compute an embedding vector that represents the spatial configuration between the driver and the camera system. This calibration module improves the overall network's performance, which can be jointly trained end to end.   We also address the lack of annotated data for training and evaluation by introducing a large-scale driving dataset with point-of-gaze annotations. This is an in situ dataset of real driving sessions in an urban city, containing synchronized images of the driving scene as well as the face and gaze of the driver. Experiments on this dataset show that the proposed method outperforms various baseline methods, having the mean prediction error of 29.69 pixels, which is relatively small compared to the $1280{\\times}720$ resolution of the scene camera.","sentences":["We propose a novel method to estimate a driver's points-of-gaze using a pair of ordinary cameras mounted on the windshield and dashboard of a car.","This is a challenging problem due to the dynamics of traffic environments with 3D scenes of unknown depths.","This problem is further complicated by the volatile distance between the driver and the camera system.","To tackle these challenges, we develop a novel convolutional network that simultaneously analyzes the image of the scene and the image of the driver's face.","This network has a camera calibration module that can compute an embedding vector that represents the spatial configuration between the driver and the camera system.","This calibration module improves the overall network's performance, which can be jointly trained end to end.   ","We also address the lack of annotated data for training and evaluation by introducing a large-scale driving dataset with point-of-gaze annotations.","This is an in situ dataset of real driving sessions in an urban city, containing synchronized images of the driving scene as well as the face and gaze of the driver.","Experiments on this dataset show that the proposed method outperforms various baseline methods, having the mean prediction error of 29.69 pixels, which is relatively small compared to the $1280{\\times}720$ resolution of the scene camera."],"url":"http://arxiv.org/abs/2404.07122v1","category":"cs.CV"}
{"created":"2024-04-10 15:59:05","title":"TEMPUS, a Timepix4-based system for event-based X-rays detection","abstract":"A readout system based on the Timepix4 ASIC, is being developed for photon science. The TEMPUS detector can be operated in two distinct modes: a photon counting mode, which allows for conventional full-frame readout at rates up to 40 kfps; and an event-driven time-stamping mode, which allows excellent time resolution in the nanosecond regime in measurements with moderate X-ray flux. In this paper, we introduce the initial prototype, a single-chip system, and present the first results obtained at PETRA III and ESRF.","sentences":["A readout system based on the Timepix4 ASIC, is being developed for photon science.","The TEMPUS detector can be operated in two distinct modes: a photon counting mode, which allows for conventional full-frame readout at rates up to 40 kfps; and an event-driven time-stamping mode, which allows excellent time resolution in the nanosecond regime in measurements with moderate X-ray flux.","In this paper, we introduce the initial prototype, a single-chip system, and present the first results obtained at PETRA III and ESRF."],"url":"http://arxiv.org/abs/2404.07120v1","category":"physics.ins-det"}
{"created":"2024-04-10 15:57:31","title":"Open reaction-diffusion systems: bridging probabilistic theory across scales","abstract":"Reaction-diffusion processes are the foundational model for a diverse range of complex systems, ranging from biochemical reactions to social agent-based phenomena. The underlying dynamics of these systems occur at the individual particle/agent level, and in realistic applications, they often display interaction with their environment through energy or material exchange with a reservoir. This requires intricate mathematical considerations, especially in the case of material exchange since the varying number of particles/agents results in ``on-the-fly'' modification of the system dimension. In this work, we first overview the probabilistic description of reaction-diffusion processes at the particle level, which readily handles varying numbers of particles. We then extend this model to consistently incorporate interactions with macroscopic material reservoirs. Based on the resulting expressions, we bridge the probabilistic description with macroscopic concentration-based descriptions for linear and nonlinear reaction-diffusion systems, as well as for an archetypal open reaction-diffusion system. This establishes a methodological workflow to bridge particle-based probabilistic descriptions with macroscopic concentration-based descriptions of reaction-diffusion in open settings, laying the foundations for a multiscale theoretical framework upon which to construct theory and simulation schemes that are consistent across scales.","sentences":["Reaction-diffusion processes are the foundational model for a diverse range of complex systems, ranging from biochemical reactions to social agent-based phenomena.","The underlying dynamics of these systems occur at the individual particle/agent level, and in realistic applications, they often display interaction with their environment through energy or material exchange with a reservoir.","This requires intricate mathematical considerations, especially in the case of material exchange since the varying number of particles/agents results in ``on-the-fly'' modification of the system dimension.","In this work, we first overview the probabilistic description of reaction-diffusion processes at the particle level, which readily handles varying numbers of particles.","We then extend this model to consistently incorporate interactions with macroscopic material reservoirs.","Based on the resulting expressions, we bridge the probabilistic description with macroscopic concentration-based descriptions for linear and nonlinear reaction-diffusion systems, as well as for an archetypal open reaction-diffusion system.","This establishes a methodological workflow to bridge particle-based probabilistic descriptions with macroscopic concentration-based descriptions of reaction-diffusion in open settings, laying the foundations for a multiscale theoretical framework upon which to construct theory and simulation schemes that are consistent across scales."],"url":"http://arxiv.org/abs/2404.07119v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-10 15:44:02","title":"Matrix product states and first quantization","abstract":"Common wisdom says that the entanglement of fermionic systems can be low in the second quantization formalism but is extremely large in the first quantization. Hence Matrix Product State (MPS) methods based on moderate entanglement have been overwhelmingly formulated in second quantization. Here we introduce a first-quantized MPS approach to simulate quantum many-body systems. We show that by reformulating the way the fermionic anti-symmetry is handled, we arrive at MPS with a level of entanglement comparable to the usual one found in second quantization. We demonstrate our scheme on the one-dimensional $t$-$V$ model (spinless fermions with nearest neighbour density-density interaction) for both ground state and time evolution. For time evolution, we find that the entanglement entropy in first quantization is significantly smaller than in its second quantization counterpart.","sentences":["Common wisdom says that the entanglement of fermionic systems can be low in the second quantization formalism but is extremely large in the first quantization.","Hence Matrix Product State (MPS) methods based on moderate entanglement have been overwhelmingly formulated in second quantization.","Here we introduce a first-quantized MPS approach to simulate quantum many-body systems.","We show that by reformulating the way the fermionic anti-symmetry is handled, we arrive at MPS with a level of entanglement comparable to the usual one found in second quantization.","We demonstrate our scheme on the one-dimensional $t$-$V$ model (spinless fermions with nearest neighbour density-density interaction) for both ground state and time evolution.","For time evolution, we find that the entanglement entropy in first quantization is significantly smaller than in its second quantization counterpart."],"url":"http://arxiv.org/abs/2404.07105v1","category":"quant-ph"}
{"created":"2024-04-10 15:41:26","title":"Temperature stabilization of a lab space at $10\\,\\mathrm{mK}$-level over a day","abstract":"Temperature fluctuations over long time scales ($\\gtrsim 1\\,\\mathrm{h}$) are an insidious problem for precision measurements. In optical laboratories, the primary effect of temperature fluctuations is drifts in optical circuits over spatial scales of a few meters and temporal scales extending beyond a few minutes. We present a lab-scale environment temperature control system approaching $10\\, \\mathrm{mK}$-level temperature instability across a lab for integration times above an hour and extending to a few days. This is achieved by passive isolation of the laboratory space from the building walls using a circulating air gap and an active control system feeding back to heating coils at the outlet of the laboratory HVAC unit. The latter achieves 20 dB suppression of temperature fluctuations across the lab, approaching the limit set by statistical coherence of the temperature field.","sentences":["Temperature fluctuations over long time scales ($\\gtrsim 1\\,\\mathrm{h}$) are an insidious problem for precision measurements.","In optical laboratories, the primary effect of temperature fluctuations is drifts in optical circuits over spatial scales of a few meters and temporal scales extending beyond a few minutes.","We present a lab-scale environment temperature control system approaching $10\\, \\mathrm{mK}$-level temperature instability across a lab for integration times above an hour and extending to a few days.","This is achieved by passive isolation of the laboratory space from the building walls using a circulating air gap and an active control system feeding back to heating coils at the outlet of the laboratory HVAC unit.","The latter achieves 20 dB suppression of temperature fluctuations across the lab, approaching the limit set by statistical coherence of the temperature field."],"url":"http://arxiv.org/abs/2404.07101v1","category":"physics.ins-det"}
{"created":"2024-04-10 15:41:16","title":"A New Statistic for Testing Covariance Equality in High-Dimensional Gaussian Low-Rank Models","abstract":"In this paper, we consider the problem of testing equality of the covariance matrices of L complex Gaussian multivariate time series of dimension $M$ . We study the special case where each of the L covariance matrices is modeled as a rank K perturbation of the identity matrix, corresponding to a signal plus noise model. A new test statistic based on the estimates of the eigenvalues of the different covariance matrices is proposed. In particular, we show that this statistic is consistent and with controlled type I error in the high-dimensional asymptotic regime where the sample sizes $N_1,\\ldots,N_L$ of each time series and the dimension $M$ both converge to infinity at the same rate, while $K$ and $L$ are kept fixed. We also provide some simulations on synthetic and real data (SAR images) which demonstrate significant improvements over some classical methods such as the GLRT, or other alternative methods relevant for the high-dimensional regime and the low-rank model.","sentences":["In this paper, we consider the problem of testing equality of the covariance matrices of L complex Gaussian multivariate time series of dimension $M$ .","We study the special case where each of the L covariance matrices is modeled as a rank K perturbation of the identity matrix, corresponding to a signal plus noise model.","A new test statistic based on the estimates of the eigenvalues of the different covariance matrices is proposed.","In particular, we show that this statistic is consistent and with controlled type I error in the high-dimensional asymptotic regime where the sample sizes $N_1,\\ldots,N_L$ of each time series and the dimension $M$ both converge to infinity at the same rate, while $K$ and $L$ are kept fixed.","We also provide some simulations on synthetic and real data (SAR images) which demonstrate significant improvements over some classical methods such as the GLRT, or other alternative methods relevant for the high-dimensional regime and the low-rank model."],"url":"http://arxiv.org/abs/2404.07100v1","category":"math.ST"}
{"created":"2024-04-10 15:26:54","title":"Non-equilibrium microbial dynamics unveil a new macroecological pattern beyond Taylor's law","abstract":"We introduce a comprehensive analytical benchmark, relying on Fokker-Planck formalism, to study microbial dynamics in presence of both biotic and abiotic forces. In equilibrium, we observe a balance between the two kinds of forces, leading to no correlations between species abundances. This implies that real microbiomes, where correlations have been observed, operate out of equilibrium. Therefore, we analyze non-equilibrium dynamics, presenting an ansatz for an approximate solution that embodies the complex interplay of forces in the system. This solution is consistent with Taylor's law as a coarse-grained approximation of the relation between species abundance and variance, but implies subtler effects, predicting unobserved structure beyond Taylor's law. Motivated by this theoretical prediction, we refine the analysis of existing metagenomic data, unveiling a novel universal macroecological pattern. Finally, we speculate on the physical origin of Taylor's law: building upon an analogy with Brownian motion theory, we propose that Taylor's law emerges as a Fluctuation-Growth relation resulting from equipartition of environmental resources among microbial species.","sentences":["We introduce a comprehensive analytical benchmark, relying on Fokker-Planck formalism, to study microbial dynamics in presence of both biotic and abiotic forces.","In equilibrium, we observe a balance between the two kinds of forces, leading to no correlations between species abundances.","This implies that real microbiomes, where correlations have been observed, operate out of equilibrium.","Therefore, we analyze non-equilibrium dynamics, presenting an ansatz for an approximate solution that embodies the complex interplay of forces in the system.","This solution is consistent with Taylor's law as a coarse-grained approximation of the relation between species abundance and variance, but implies subtler effects, predicting unobserved structure beyond Taylor's law.","Motivated by this theoretical prediction, we refine the analysis of existing metagenomic data, unveiling a novel universal macroecological pattern.","Finally, we speculate on the physical origin of Taylor's law: building upon an analogy with Brownian motion theory, we propose that Taylor's law emerges as a Fluctuation-Growth relation resulting from equipartition of environmental resources among microbial species."],"url":"http://arxiv.org/abs/2404.07090v1","category":"q-bio.PE"}
{"created":"2024-04-10 15:24:12","title":"Probing phase transitions with correlations in configuration space: a Monte Carlo study on lattice models","abstract":"While phases and phase transitions are mostly characterized by order parameters or physical quantities in real space, we propose that the correlation in Hilbert space is closely connected to phase transitions. Specifically, this correlation is quantified by the 1-norm distance between configurations, and the distribution of distances can be obtained from a small fraction of configurations in the exponentially large Hilbert space, with the help of importance sampling procedures of the Monte Carlo method. Considering the obtained distances as a data set, its distribution varies substantially in different phases, and the numerical results further suggest a universal critical behavior for the uncertainty and participation entropy extracted from it. For various classical spin models with different types of phases and phase transitions, the finite-size analysis based on these quantities successfully catches the phase transitions with accurate critical points. Moreover, in all cases for different systems and phase transitions, the critical exponent from the uncertainty of the distances is found to numerically equal the anomalous dimension that determines the decay of the correlation in real space. This implies a deeper connection between the correlation in the real space and configuration space, which deserves further investigation. In our proposal, the definition of distance covers various lattice models with different local degrees of freedom, e.g., two levels for Ising-like models, discrete multi-levels for $q$-state clock models, and continuous local levels for the $XY$ model, and the way handling the distances is very simple and robust. Our proposal provides an alternative way of understanding the complex phases and phase transitions in complicated systems where the order parameter is hard to compute or define.","sentences":["While phases and phase transitions are mostly characterized by order parameters or physical quantities in real space, we propose that the correlation in Hilbert space is closely connected to phase transitions.","Specifically, this correlation is quantified by the 1-norm distance between configurations, and the distribution of distances can be obtained from a small fraction of configurations in the exponentially large Hilbert space, with the help of importance sampling procedures of the Monte Carlo method.","Considering the obtained distances as a data set, its distribution varies substantially in different phases, and the numerical results further suggest a universal critical behavior for the uncertainty and participation entropy extracted from it.","For various classical spin models with different types of phases and phase transitions, the finite-size analysis based on these quantities successfully catches the phase transitions with accurate critical points.","Moreover, in all cases for different systems and phase transitions, the critical exponent from the uncertainty of the distances is found to numerically equal the anomalous dimension that determines the decay of the correlation in real space.","This implies a deeper connection between the correlation in the real space and configuration space, which deserves further investigation.","In our proposal, the definition of distance covers various lattice models with different local degrees of freedom, e.g., two levels for Ising-like models, discrete multi-levels for $q$-state clock models, and continuous local levels for the $XY$ model, and the way handling the distances is very simple and robust.","Our proposal provides an alternative way of understanding the complex phases and phase transitions in complicated systems where the order parameter is hard to compute or define."],"url":"http://arxiv.org/abs/2404.07087v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-10 15:10:31","title":"All-optical scanning vector magnetometry based on fine and hyperfine interactions in spin-$\\frac{3}{2}$ centers in silicon carbide","abstract":"The possibility of using axial spin color centers with $S=3/2$, oriented along the hexagonal $c$ axis in a silicon carbide (SiC) wafer, has been demonstrated for all-optical measurement of projection of the external magnetic field coinciding with the $c$ axis of the crystal, and the polar and azimuthal angles of the external measured magnetic field at room and significantly higher temperatures. A distinctive feature of spin centers in SiC, in which optically induced spin alignment is carried out, is the presence of a unique system of spin levels in a magnetic field, caused by fine structure interaction and hyperfine interaction with the $^{29}$Si nuclei and there is a wide range of level anticrossings (LACs), leading to an exceptionally strong change in photoluminescence in the region of LAC and the dependence of the LAC spectrum on the orientation of the external measured magnetic field, which made it possible to develop the all-optical vector magnetometry method. Such measurements do not require microwave radiation, it is possible to use single spin center for the all-optical vector magnetometry. The proposed magnetometer is based on an external magnetic field cancellation scheme, which leads to a LAC spectrum observed in a zero external magnetic field, called its reference spectrum, by maintaining a local region of zero magnetic field at the site of optical excitation of spin centers. 4H-SiC plate is placed on the scanning stage of a confocal microscope inside the Helmholtz coils. Sensitivity to a constant magnetic field for $z$ component of the magnetic field ${B}_{z}$ is better than $0.1\\mu$T$/\\sqrt{\\text{Hz}}$ in a volume of $1\\times10^{-8}$mm$^{3}$ at room temperature. The sensitivity of determining the angles polar and azimuthal is determined by the sensitivity to determining the perpendicular component of the magnetic field, which is better than $5~\\mu$T$/\\sqrt{\\text{Hz}}$.","sentences":["The possibility of using axial spin color centers with $S=3/2$, oriented along the hexagonal $c$ axis in a silicon carbide (SiC) wafer, has been demonstrated for all-optical measurement of projection of the external magnetic field coinciding with the $c$ axis of the crystal, and the polar and azimuthal angles of the external measured magnetic field at room and significantly higher temperatures.","A distinctive feature of spin centers in SiC, in which optically induced spin alignment is carried out, is the presence of a unique system of spin levels in a magnetic field, caused by fine structure interaction and hyperfine interaction with the $^{29}$Si nuclei and there is a wide range of level anticrossings (LACs), leading to an exceptionally strong change in photoluminescence in the region of LAC and the dependence of the LAC spectrum on the orientation of the external measured magnetic field, which made it possible to develop the all-optical vector magnetometry method.","Such measurements do not require microwave radiation, it is possible to use single spin center for the all-optical vector magnetometry.","The proposed magnetometer is based on an external magnetic field cancellation scheme, which leads to a LAC spectrum observed in a zero external magnetic field, called its reference spectrum, by maintaining a local region of zero magnetic field at the site of optical excitation of spin centers.","4H-SiC plate is placed on the scanning stage of a confocal microscope inside the Helmholtz coils.","Sensitivity to a constant magnetic field for $z$ component of the magnetic field ${B}_{z}$ is better than $0.1\\mu$T$/\\sqrt{\\text{Hz}}$ in a volume of $1\\times10^{-8}$mm$^{3}$ at room temperature.","The sensitivity of determining the angles polar and azimuthal is determined by the sensitivity to determining the perpendicular component of the magnetic field, which is better than $5~\\mu$T$/\\sqrt{\\text{Hz}}$."],"url":"http://arxiv.org/abs/2404.07080v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-10 14:52:54","title":"A random matrix model for the density of states of jammed soft spheres with applied stress","abstract":"We investigate the addition of applied stress to a random block matrix model introduced by Parisi to study the Hessian matrix of soft spheres near the jamming point. In the infinite dimensional limit the applied stress translates the spectral distribution to the left, leading to a stability constraint. With negative stress, as in the case of a random network of stretched elastic springs, the spectral distribution is translated to the right, and the density of states has a peak before the plateau.","sentences":["We investigate the addition of applied stress to a random block matrix model introduced by Parisi to study the Hessian matrix of soft spheres near the jamming point.","In the infinite dimensional limit the applied stress translates the spectral distribution to the left, leading to a stability constraint.","With negative stress, as in the case of a random network of stretched elastic springs, the spectral distribution is translated to the right, and the density of states has a peak before the plateau."],"url":"http://arxiv.org/abs/2404.07064v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-10 14:51:15","title":"Accessibility and Ergodicity of Partially Hyperbolic Diffeomorphisms without Periodic Points","abstract":"We prove that every $C^2$ conservative partially hyperbolic diffeomorphism of a closed 3-manifold without periodic points is ergodic, which gives an affirmative answer to the Ergodicity Conjecture by Hertz-Hertz-Ures in the absence of periodic points. We also show that a partially hyperbolic diffeomorphism of a closed 3-manifold $M$ with no periodic points is accessible if the non-wandering set is all of $M$ and the fundamental group $\\pi_1(M)$ is not virtually solvable.","sentences":["We prove that every $C^2$ conservative partially hyperbolic diffeomorphism of a closed 3-manifold without periodic points is ergodic, which gives an affirmative answer to the Ergodicity Conjecture by Hertz-Hertz-Ures in the absence of periodic points.","We also show that a partially hyperbolic diffeomorphism of a closed 3-manifold $M$ with no periodic points is accessible if the non-wandering set is all of $M$ and the fundamental group $\\pi_1(M)$ is not virtually solvable."],"url":"http://arxiv.org/abs/2404.07062v1","category":"math.DS"}
{"created":"2024-04-10 14:42:40","title":"Efficient Post-Quantum Secured Blind Computation","abstract":"In the medium term, quantum computing must tackle two key challenges: fault tolerance and security. Fault tolerance will be solved with sufficiently high quality experiments on large numbers of qubits, but the scale and complexity of these devices means that a cloud-based access model is likely to dominate. How can we risk evaluating valuable computations on an untrusted server? Here we detail a verifiable circuit-based model that only requires classical communication between parties. The server is blind to the details of the computation, which is computationally secure.","sentences":["In the medium term, quantum computing must tackle two key challenges: fault tolerance and security.","Fault tolerance will be solved with sufficiently high quality experiments on large numbers of qubits, but the scale and complexity of these devices means that a cloud-based access model is likely to dominate.","How can we risk evaluating valuable computations on an untrusted server?","Here we detail a verifiable circuit-based model that only requires classical communication between parties.","The server is blind to the details of the computation, which is computationally secure."],"url":"http://arxiv.org/abs/2404.07052v1","category":"quant-ph"}
{"created":"2024-04-10 14:38:58","title":"Towards Learning Stochastic Population Models by Gradient Descent","abstract":"Increasing effort is put into the development of methods for learning mechanistic models from data. This task entails not only the accurate estimation of parameters, but also a suitable model structure. Recent work on the discovery of dynamical systems formulates this problem as a linear equation system. Here, we explore several simulation-based optimization approaches, which allow much greater freedom in the objective formulation and weaker conditions on the available data. We show that even for relatively small stochastic population models, simultaneous estimation of parameters and structure poses major challenges for optimization procedures. Particularly, we investigate the application of the local stochastic gradient descent method, commonly used for training machine learning models. We demonstrate accurate estimation of models but find that enforcing the inference of parsimonious, interpretable models drastically increases the difficulty. We give an outlook on how this challenge can be overcome.","sentences":["Increasing effort is put into the development of methods for learning mechanistic models from data.","This task entails not only the accurate estimation of parameters, but also a suitable model structure.","Recent work on the discovery of dynamical systems formulates this problem as a linear equation system.","Here, we explore several simulation-based optimization approaches, which allow much greater freedom in the objective formulation and weaker conditions on the available data.","We show that even for relatively small stochastic population models, simultaneous estimation of parameters and structure poses major challenges for optimization procedures.","Particularly, we investigate the application of the local stochastic gradient descent method, commonly used for training machine learning models.","We demonstrate accurate estimation of models but find that enforcing the inference of parsimonious, interpretable models drastically increases the difficulty.","We give an outlook on how this challenge can be overcome."],"url":"http://arxiv.org/abs/2404.07049v1","category":"cs.LG"}
{"created":"2024-04-10 14:38:15","title":"Uncovering quantum characteristics of incipient evolutions at the photosynthetic oxygen evolving complex","abstract":"Water oxidation of photosynthesis at the oxygen evolving complex (OEC) is driven by the polarization field induced by the photoelectric hole. By highlighting the role of the polarization field in reshaping the spin-orbit coupling deduced from the Dirac quantum mechanics, we reveal in this work the characteristics and underlying mechanism in the relatively simpler OEC evolutions within the states S0 - S2 prior to the water oxidation. The characteristic shifts of the density of states (DOS) of the electron donor Mn atom are observed in the vicinity of the Fermi surface to occur with the spin flips of Mn atoms and the change of the Mn oxidation states during the electron transfer. Notably, the spin flips of Mn atoms point to the resulting spin configuration of the next states. It is found that the electron transfer tend to stabilize the catalyst OEC itself, whereas the proton transfer pushes the evolution forward by preparing a new electron donor. Meanwhile, it shows that the Mn-O bonds around the candidate Mn atom of the electron donor undergo characteristic changes in the bond lengths during the electron transfer. These concomitant phenomena uncovered in first-principle calculations characterize the essential equilibrium of the OEC between the state evolution and stability that forms a ground of the dynamic OEC cycles.","sentences":["Water oxidation of photosynthesis at the oxygen evolving complex (OEC) is driven by the polarization field induced by the photoelectric hole.","By highlighting the role of the polarization field in reshaping the spin-orbit coupling deduced from the Dirac quantum mechanics, we reveal in this work the characteristics and underlying mechanism in the relatively simpler OEC evolutions within the states S0 - S2 prior to the water oxidation.","The characteristic shifts of the density of states (DOS) of the electron donor Mn atom are observed in the vicinity of the Fermi surface to occur with the spin flips of Mn atoms and the change of the Mn oxidation states during the electron transfer.","Notably, the spin flips of Mn atoms point to the resulting spin configuration of the next states.","It is found that the electron transfer tend to stabilize the catalyst OEC itself, whereas the proton transfer pushes the evolution forward by preparing a new electron donor.","Meanwhile, it shows that the Mn-O bonds around the candidate Mn atom of the electron donor undergo characteristic changes in the bond lengths during the electron transfer.","These concomitant phenomena uncovered in first-principle calculations characterize the essential equilibrium of the OEC between the state evolution and stability that forms a ground of the dynamic OEC cycles."],"url":"http://arxiv.org/abs/2404.07048v1","category":"physics.bio-ph"}
{"created":"2024-04-10 14:33:13","title":"Normalization flow in the presence of a resonance","abstract":"Following arXiv:2303.02992, we develop an approach to the Hamiltonian theory of normal forms based on continuous averaging. We concentrate on the case of normal forms near an elliptic singular point, but unlike arXiv:2303.02992 we do not assume that frequences of the linearized system are nonresonant. We study analytic properties of the normalization procedure. In particular we show that in the case of a codimension one resonance an analytic Hamiltonian function may be reduced to a normal form up to an exponentially small reminder with explicit estimates of the reminder and the analyticity domain.","sentences":["Following arXiv:2303.02992, we develop an approach to the Hamiltonian theory of normal forms based on continuous averaging.","We concentrate on the case of normal forms near an elliptic singular point, but unlike arXiv:2303.02992 we do not assume that frequences of the linearized system are nonresonant.","We study analytic properties of the normalization procedure.","In particular we show that in the case of a codimension one resonance an analytic Hamiltonian function may be reduced to a normal form up to an exponentially small reminder with explicit estimates of the reminder and the analyticity domain."],"url":"http://arxiv.org/abs/2404.07043v1","category":"math.DS"}
{"created":"2024-04-10 14:29:03","title":"Computing the $D$-base and $D$-relation in finite closure systems","abstract":"Implicational bases (IBs) are a common representation of finite closure systems and lattices, along with meet-irreducible elements. They appear in a wide variety of fields ranging from logic and databases to Knowledge Space Theory. Different IBs can represent the same closure system. Therefore, several IBs have been studied, such as the canonical and canonical direct bases. In this paper, we investigate the $D$-base, a refinement of the canonical direct base. It is connected with the $D$-relation, an essential tool in the study of free lattices. The $D$-base demonstrates desirable algorithmic properties, and together with the $D$-relation, it conveys essential properties of the underlying closure system. Hence, computing the $D$-base and the $D$-relation of a closure system from another representation is crucial to enjoy its benefits. However, complexity results for this task are lacking. In this paper, we give algorithms and hardness results for the computation of the $D$-base and $D$-relation. Specifically, we establish the $NP$-completeness of finding the $D$-relation from an arbitrary IB; we give an output-quasi-polynomial time algorithm to compute the $D$-base from meet-irreducible elements; and we obtain a polynomial-delay algorithm computing the $D$-base from an arbitrary IB. These results complete the picture regarding the complexity of identifying the $D$-base and $D$-relation of a closure system.","sentences":["Implicational bases (IBs) are a common representation of finite closure systems and lattices, along with meet-irreducible elements.","They appear in a wide variety of fields ranging from logic and databases to Knowledge Space Theory.","Different IBs can represent the same closure system.","Therefore, several IBs have been studied, such as the canonical and canonical direct bases.","In this paper, we investigate the $D$-base, a refinement of the canonical direct base.","It is connected with the $D$-relation, an essential tool in the study of free lattices.","The $D$-base demonstrates desirable algorithmic properties, and together with the $D$-relation, it conveys essential properties of the underlying closure system.","Hence, computing the $D$-base and the $D$-relation of a closure system from another representation is crucial to enjoy its benefits.","However, complexity results for this task are lacking.","In this paper, we give algorithms and hardness results for the computation of the $D$-base and $D$-relation.","Specifically, we establish the $NP$-completeness of finding the $D$-relation from an arbitrary IB; we give an output-quasi-polynomial time algorithm to compute the $D$-base from meet-irreducible elements; and we obtain a polynomial-delay algorithm computing the $D$-base from an arbitrary IB.","These results complete the picture regarding the complexity of identifying the $D$-base and $D$-relation of a closure system."],"url":"http://arxiv.org/abs/2404.07037v1","category":"cs.DS"}
{"created":"2024-04-10 14:26:33","title":"Relative Occurrence Rate Between Hot and Cold Jupiters as an Indicator to Probe Planet Migration","abstract":"We propose a second-order statistic parameter $\\varepsilon$, the relative occurrence rate between hot and cold Jupiters ($\\varepsilon=\\eta_{\\rm HJ}/\\eta_{\\rm CJ}$), to probe the migration of gas giants. Since the planet occurrence rate is the combined outcome of the formation and migration processes, a joint analysis of hot and cold Jupiter frequency may shed light on the dynamical evolution of giant planet systems. We first investigate the behavior of $\\varepsilon$ as the stellar mass changes observationally. Based on the occurrence rate measurements of hot Jupiters ($\\eta_{\\rm HJ}$) from the TESS survey and cold Jupiters ($\\eta_{\\rm CJ}$) from the CLS survey, we find a tentative trend (97% confidence) that $\\varepsilon$ drops when the stellar mass rises from $0.8$ to $1.4\\ M_\\odot$, which can be explained by different giant planet growth and disk migration timescales around different stars. We carry out planetesimal and pebble accretion simulations, both of which could reproduce the results of $\\eta_{\\rm HJ}$, $\\eta_{\\rm CJ}$ and $\\varepsilon$. Our findings indicate that the classical core accretion + disk migration model can explain the observed decreasing trend of $\\varepsilon$. We propose two ways to increase the significance of the trend and verify the anti-correlation. Future works are required to better constrain $\\varepsilon$, especially for M dwarfs and for more massive stars.","sentences":["We propose a second-order statistic parameter $\\varepsilon$, the relative occurrence rate between hot and cold Jupiters ($\\varepsilon=\\eta_{\\rm HJ}/\\eta_{\\rm CJ}$), to probe the migration of gas giants.","Since the planet occurrence rate is the combined outcome of the formation and migration processes, a joint analysis of hot and cold Jupiter frequency may shed light on the dynamical evolution of giant planet systems.","We first investigate the behavior of $\\varepsilon$ as the stellar mass changes observationally.","Based on the occurrence rate measurements of hot Jupiters ($\\eta_{\\rm HJ}$) from the TESS survey and cold Jupiters ($\\eta_{\\rm CJ}$) from the CLS survey, we find a tentative trend (97% confidence) that $\\varepsilon$ drops when the stellar mass rises from $0.8$ to $1.4\\ M_\\odot$, which can be explained by different giant planet growth and disk migration timescales around different stars.","We carry out planetesimal and pebble accretion simulations, both of which could reproduce the results of $\\eta_{\\rm HJ}$, $\\eta_{\\rm CJ}$ and $\\varepsilon$. Our findings indicate that the classical core accretion + disk migration model can explain the observed decreasing trend of $\\varepsilon$. We propose two ways to increase the significance of the trend and verify the anti-correlation.","Future works are required to better constrain $\\varepsilon$, especially for M dwarfs and for more massive stars."],"url":"http://arxiv.org/abs/2404.07033v1","category":"astro-ph.EP"}
{"created":"2024-04-10 14:18:47","title":"Secrecy Enhancement for UAV-enabled Integrated Sensing and Communication System","abstract":"In this correspondence, we propose an unmanned aerial vehicle (UAV)-enabled integrated sensing and communication (ISAC) system, where a full-duplex UAV equipped with uniform planar array (UPA) is adopted as a base station for the multiuser downlink communications, while sensing and jamming a passive ground eavesdropper. The goal of this work is to maximize the sum secrecy rate of ground users subject to the constraints of sensing accuracy and UAV's operational capability by jointly optimizing the transceiver beamforming and UAV's trajectory. To this end, we develop the algorithmic solution based on block coordinate descent (BCD) and semidefinite programming (SDP) relaxation techniques, whose performance is verified via simulations indicating its efficacy in improving communication security with the sufficient mission period.","sentences":["In this correspondence, we propose an unmanned aerial vehicle (UAV)-enabled integrated sensing and communication (ISAC) system, where a full-duplex UAV equipped with uniform planar array (UPA) is adopted as a base station for the multiuser downlink communications, while sensing and jamming a passive ground eavesdropper.","The goal of this work is to maximize the sum secrecy rate of ground users subject to the constraints of sensing accuracy and UAV's operational capability by jointly optimizing the transceiver beamforming and UAV's trajectory.","To this end, we develop the algorithmic solution based on block coordinate descent (BCD) and semidefinite programming (SDP) relaxation techniques, whose performance is verified via simulations indicating its efficacy in improving communication security with the sufficient mission period."],"url":"http://arxiv.org/abs/2404.07024v1","category":"eess.SP"}
{"created":"2024-04-10 14:16:44","title":"Non-Degenerate One-Time Pad and the integrity of perfectly secret messages","abstract":"We present a new construction of a One Time Pad (OTP) with inherent diffusive properties and a redundancy injection mechanism that benefits from them. The construction is based on interpreting the plaintext and key as members of a permutation group in the Lehmer code representation after conversion to factoradic. The so constructed OTP translates any perturbation of the ciphertext to an unpredictable, metrically large random perturbation of the plaintext. This allows us to provide unconditional integrity assurance without extra key material. The redundancy is injected using Foata's \"pun\": the reading of the one-line representation as the cyclic one; we call this Pseudo Foata Injection. We obtain algorithms of quadratic complexity that implement both mechanisms.","sentences":["We present a new construction of a One Time Pad (OTP) with inherent diffusive properties and a redundancy injection mechanism that benefits from them.","The construction is based on interpreting the plaintext and key as members of a permutation group in the Lehmer code representation after conversion to factoradic.","The so constructed OTP translates any perturbation of the ciphertext to an unpredictable, metrically large random perturbation of the plaintext.","This allows us to provide unconditional integrity assurance without extra key material.","The redundancy is injected using Foata's \"pun\": the reading of the one-line representation as the cyclic one; we call this Pseudo Foata Injection.","We obtain algorithms of quadratic complexity that implement both mechanisms."],"url":"http://arxiv.org/abs/2404.07022v1","category":"cs.CR"}
{"created":"2024-04-10 14:14:42","title":"A 4x32Gb/s 1.8pJ/bit Collaborative Baud-Rate CDR with Background Eye-Climbing Algorithm and Low-Power Global Clock Distribution","abstract":"This paper presents design techniques for an energy-efficient multi-lane receiver (RX) with baud-rate clock and data recovery (CDR), which is essential for high-throughput low-latency communication in high-performance computing systems. The proposed low-power global clock distribution not only significantly reduces power consumption across multi-lane RXs but is capable of compensating for the frequency offset without any phase interpolators. To this end, a fractional divider controlled by CDR is placed close to the global phase locked loop. Moreover, in order to address the sub-optimal lock point of conventional baud-rate phase detectors, the proposed CDR employs a background eye-climbing algorithm, which optimizes the sampling phase and maximizes the vertical eye margin (VEM). Fabricated in a 28nm CMOS process, the proposed 4x32Gb/s RX shows a low integrated fractional spur of -40.4dBc at a 2500ppm frequency offset. Furthermore, it improves bit-error-rate performance by increasing the VEM by 17%. The entire RX achieves the energy efficiency of 1.8pJ/bit with the aggregate data rate of 128Gb/s.","sentences":["This paper presents design techniques for an energy-efficient multi-lane receiver (RX) with baud-rate clock and data recovery (CDR), which is essential for high-throughput low-latency communication in high-performance computing systems.","The proposed low-power global clock distribution not only significantly reduces power consumption across multi-lane RXs but is capable of compensating for the frequency offset without any phase interpolators.","To this end, a fractional divider controlled by CDR is placed close to the global phase locked loop.","Moreover, in order to address the sub-optimal lock point of conventional baud-rate phase detectors, the proposed CDR employs a background eye-climbing algorithm, which optimizes the sampling phase and maximizes the vertical eye margin (VEM).","Fabricated in a 28nm CMOS process, the proposed 4x32Gb/s RX shows a low integrated fractional spur of -40.4dBc at a 2500ppm frequency offset.","Furthermore, it improves bit-error-rate performance by increasing the VEM by 17%.","The entire RX achieves the energy efficiency of 1.8pJ/bit with the aggregate data rate of 128Gb/s."],"url":"http://arxiv.org/abs/2404.07021v1","category":"eess.SP"}
{"created":"2024-04-10 14:04:53","title":"Variational Quantum Crank-Nicolson and Method of Lines for the Solution of Initial Value Problems","abstract":"In this paper we use a Variational Quantum Algorithm to solve Initial Value Problems with the Implicit Crank-Nicolson and the Method of Lines evolution schemes. The unknown functions use a spectral decomposition with the Fourier basis. The examples developed to illustrate the implementation are the advection equation, the wave equation written as a first order system of coupled equations and the viscous Burgers equation as a non-linear case. The problems are solved using: i) standard Finite Differences as the solution to compare with, ii) the State Vector Formalism (SVF), and iii) the Sampling Error Formalism (SEF). Our results for these equations show that the SVF provides convergent solutions whereas those constructed with the SEF are not consistent with the increase of resolution. Byproducts of our implementation include the construction of cost functions for the two evolution schemes and an efficient method to simulate the SVF and SEF in classical computers.","sentences":["In this paper we use a Variational Quantum Algorithm to solve Initial Value Problems with the Implicit Crank-Nicolson and the Method of Lines evolution schemes.","The unknown functions use a spectral decomposition with the Fourier basis.","The examples developed to illustrate the implementation are the advection equation, the wave equation written as a first order system of coupled equations and the viscous Burgers equation as a non-linear case.","The problems are solved using: i) standard Finite Differences as the solution to compare with, ii) the State Vector Formalism (SVF), and iii) the Sampling Error Formalism (SEF).","Our results for these equations show that the SVF provides convergent solutions whereas those constructed with the SEF are not consistent with the increase of resolution.","Byproducts of our implementation include the construction of cost functions for the two evolution schemes and an efficient method to simulate the SVF and SEF in classical computers."],"url":"http://arxiv.org/abs/2404.07016v1","category":"quant-ph"}
{"created":"2024-04-10 13:50:46","title":"A Mathematical Theory for Learning Semantic Languages by Abstract Learners","abstract":"Recent advances in Large Language Models (LLMs) have demonstrated the emergence of capabilities (learned skills) when the number of system parameters and the size of training data surpass certain thresholds. The exact mechanisms behind such phenomena are not fully understood and remain a topic of active research. Inspired by the skill-text bipartite graph model presented in [1] for modeling semantic language, we develop a mathematical theory to explain the emergence of learned skills, taking the learning (or training) process into account. Our approach models the learning process for skills in the skill-text bipartite graph as an iterative decoding process in Low-Density Parity Check (LDPC) codes and Irregular Repetition Slotted ALOHA (IRSA). Using density evolution analysis, we demonstrate the emergence of learned skills when the ratio of the size of training texts to the number of skills exceeds a certain threshold. Our analysis also yields a scaling law for testing errors relative to the size of training texts. Upon completion of the training, we propose a method for semantic compression and discuss its application in semantic communication.","sentences":["Recent advances in Large Language Models (LLMs) have demonstrated the emergence of capabilities (learned skills) when the number of system parameters and the size of training data surpass certain thresholds.","The exact mechanisms behind such phenomena are not fully understood and remain a topic of active research.","Inspired by the skill-text bipartite graph model presented in [1] for modeling semantic language, we develop a mathematical theory to explain the emergence of learned skills, taking the learning (or training) process into account.","Our approach models the learning process for skills in the skill-text bipartite graph as an iterative decoding process in Low-Density Parity Check (LDPC) codes and Irregular Repetition Slotted ALOHA (IRSA).","Using density evolution analysis, we demonstrate the emergence of learned skills when the ratio of the size of training texts to the number of skills exceeds a certain threshold.","Our analysis also yields a scaling law for testing errors relative to the size of training texts.","Upon completion of the training, we propose a method for semantic compression and discuss its application in semantic communication."],"url":"http://arxiv.org/abs/2404.07009v1","category":"cs.CL"}
{"created":"2024-04-10 13:49:20","title":"Multi-Agent Soft Actor-Critic with Global Loss for Autonomous Mobility-on-Demand Fleet Control","abstract":"We study a sequential decision-making problem for a profit-maximizing operator of an Autonomous Mobility-on-Demand system. Optimizing a central operator's vehicle-to-request dispatching policy requires efficient and effective fleet control strategies. To this end, we employ a multi-agent Soft Actor-Critic algorithm combined with weighted bipartite matching. We propose a novel vehicle-based algorithm architecture and adapt the critic's loss function to appropriately consider global actions. Furthermore, we extend our algorithm to incorporate rebalancing capabilities. Through numerical experiments, we show that our approach outperforms state-of-the-art benchmarks by up to 12.9% for dispatching and up to 38.9% with integrated rebalancing.","sentences":["We study a sequential decision-making problem for a profit-maximizing operator of an Autonomous Mobility-on-Demand system.","Optimizing a central operator's vehicle-to-request dispatching policy requires efficient and effective fleet control strategies.","To this end, we employ a multi-agent Soft Actor-Critic algorithm combined with weighted bipartite matching.","We propose a novel vehicle-based algorithm architecture and adapt the critic's loss function to appropriately consider global actions.","Furthermore, we extend our algorithm to incorporate rebalancing capabilities.","Through numerical experiments, we show that our approach outperforms state-of-the-art benchmarks by up to 12.9% for dispatching and up to 38.9% with integrated rebalancing."],"url":"http://arxiv.org/abs/2404.06975v1","category":"eess.SY"}
{"created":"2024-04-10 13:45:52","title":"Opinion dynamics of two populations with time-delayed coupling","abstract":"We study a Hegselmann-Krause type opinion formation model for a system of two populations. The two groups interact with each other via subsets of individuals, namely the leaders, and natural time delay effects are considered. By using careful estimates of the system's trajectories, we are able to prove an asymptotic convergence to consensus result. Some numerical tests illustrate the theoretical result and point out some possible applications.","sentences":["We study a Hegselmann-Krause type opinion formation model for a system of two populations.","The two groups interact with each other via subsets of individuals, namely the leaders, and natural time delay effects are considered.","By using careful estimates of the system's trajectories, we are able to prove an asymptotic convergence to consensus result.","Some numerical tests illustrate the theoretical result and point out some possible applications."],"url":"http://arxiv.org/abs/2404.07007v1","category":"math.OC"}
{"created":"2024-04-10 13:30:15","title":"Magnetic-field induced spiral order in the electric polarization","abstract":"We present a phenomenological model for magnetoelectricity in multiferroic materials. The distinctive feature of the model is a two-component complex order parameter that encodes the electric polarization, along with a direct coupling between the polarization and magnetic field. Our model effectively elucidates that a sufficiently strong magnetic field can destroy electric polarization. Furthermore, the transition field strength diminishes with rising temperature, following a power-law relation with the exponent being precisely worked out. At lower field strength, the electric polarization takes a spiral order in the magnetic field, with the spiral wavelength inversely proportional to the magnetic field strength. We anticipate these predictions can be experimentally tested in future studies on multiferroic materials.","sentences":["We present a phenomenological model for magnetoelectricity in multiferroic materials.","The distinctive feature of the model is a two-component complex order parameter that encodes the electric polarization, along with a direct coupling between the polarization and magnetic field.","Our model effectively elucidates that a sufficiently strong magnetic field can destroy electric polarization.","Furthermore, the transition field strength diminishes with rising temperature, following a power-law relation with the exponent being precisely worked out.","At lower field strength, the electric polarization takes a spiral order in the magnetic field, with the spiral wavelength inversely proportional to the magnetic field strength.","We anticipate these predictions can be experimentally tested in future studies on multiferroic materials."],"url":"http://arxiv.org/abs/2404.07000v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-10 13:30:09","title":"On quantum Floquet theorem","abstract":"We consider the Schr\\\"odinger equation $ih\\partial_t\\psi = H\\psi$, $\\psi=\\psi(\\cdot,t)\\in L^2({\\mathbb T})$. The operator $H = -\\partial^2_x + V(x,t)$ includes smooth potential $V$, which is assumed to be time $T$-periodic. Let $W=W(t)$ be the fundamental solution of this linear ODE system on $L^2({\\mathbb T})$. Then according to terminology from Lyapunov-Floquet theory, ${\\cal M}=W(T)$ is the monodromy operator. We prove that ${\\cal M}$ is unitarily conjugated to $\\exp\\big(-\\frac{T}{ih} \\partial^2_x\\big) + {\\cal C}$, where ${\\cal C}$ is a compact operator with an arbitrarily small norm.","sentences":["We consider the Schr\\\"odinger equation $ih\\partial_t\\psi = H\\psi$, $\\psi=\\psi(\\cdot,t)\\in L^2({\\mathbb T})$.","The operator $H = -\\partial^2_x + V(x,t)$ includes smooth potential $V$, which is assumed to be time $T$-periodic.","Let $W=W(t)$ be the fundamental solution of this linear ODE system on $L^2({\\mathbb T})$.","Then according to terminology from Lyapunov-Floquet theory, ${\\cal M}=W(T)$ is the monodromy operator.","We prove that ${\\cal M}$ is unitarily conjugated to $\\exp\\big(-\\frac{T}{ih} \\partial^2_x\\big)","+ {\\cal C}$, where ${\\cal C}$ is a compact operator with an arbitrarily small norm."],"url":"http://arxiv.org/abs/2404.06999v1","category":"math.DS"}
{"created":"2024-04-10 13:26:41","title":"Analytical Formula for Calculations of Armour Losses in Three-Core Power Cables","abstract":"Over the past decade, significant progress has been made in the field of loss and rating calculations for armoured three-core cables. This development was prompted by an industry realization that the applicable international standards often overestimate losses, leading to unnecessarily bulky and more expensive cables. Starting with first-principles, this paper presents an accurate analytical formula for armour losses in three-core cables. The formula has undergone rigorous validation against 3D Finite Element Analysis (FEA) and demonstrate excellent accuracy. In the specific cases examined, the largest deviation from FEA results in terms of armour loss is approximately 2.4 percent for fully armoured cables. Although this study specifically focuses on armour losses, it establishes the groundwork for precise loss calculations in armoured three-core cables, including the conductor and screen losses. And the work presented here formed the basis for the complete loss calculations presented in the CIGRE Technical Brochure 908.","sentences":["Over the past decade, significant progress has been made in the field of loss and rating calculations for armoured three-core cables.","This development was prompted by an industry realization that the applicable international standards often overestimate losses, leading to unnecessarily bulky and more expensive cables.","Starting with first-principles, this paper presents an accurate analytical formula for armour losses in three-core cables.","The formula has undergone rigorous validation against 3D Finite Element Analysis (FEA) and demonstrate excellent accuracy.","In the specific cases examined, the largest deviation from FEA results in terms of armour loss is approximately 2.4 percent for fully armoured cables.","Although this study specifically focuses on armour losses, it establishes the groundwork for precise loss calculations in armoured three-core cables, including the conductor and screen losses.","And the work presented here formed the basis for the complete loss calculations presented in the CIGRE Technical Brochure 908."],"url":"http://arxiv.org/abs/2404.06998v1","category":"eess.SY"}
{"created":"2024-04-10 13:19:28","title":"Model-free Change-point Detection Using Modern Classifiers","abstract":"In contemporary data analysis, it is increasingly common to work with non-stationary complex datasets. These datasets typically extend beyond the classical low-dimensional Euclidean space, making it challenging to detect shifts in their distribution without relying on strong structural assumptions. This paper introduces a novel offline change-point detection method that leverages modern classifiers developed in the machine-learning community. With suitable data splitting, the test statistic is constructed through sequential computation of the Area Under the Curve (AUC) of a classifier, which is trained on data segments on both ends of the sequence. It is shown that the resulting AUC process attains its maxima at the true change-point location, which facilitates the change-point estimation. The proposed method is characterized by its complete nonparametric nature, significant versatility, considerable flexibility, and absence of stringent assumptions pertaining to the underlying data or any distributional shifts. Theoretically, we derive the limiting pivotal distribution of the proposed test statistic under null, as well as the asymptotic behaviors under both local and fixed alternatives. The weak consistency of the change-point estimator is provided. Extensive simulation studies and the analysis of two real-world datasets illustrate the superior performance of our approach compared to existing model-free change-point detection methods.","sentences":["In contemporary data analysis, it is increasingly common to work with non-stationary complex datasets.","These datasets typically extend beyond the classical low-dimensional Euclidean space, making it challenging to detect shifts in their distribution without relying on strong structural assumptions.","This paper introduces a novel offline change-point detection method that leverages modern classifiers developed in the machine-learning community.","With suitable data splitting, the test statistic is constructed through sequential computation of the Area Under the Curve (AUC) of a classifier, which is trained on data segments on both ends of the sequence.","It is shown that the resulting AUC process attains its maxima at the true change-point location, which facilitates the change-point estimation.","The proposed method is characterized by its complete nonparametric nature, significant versatility, considerable flexibility, and absence of stringent assumptions pertaining to the underlying data or any distributional shifts.","Theoretically, we derive the limiting pivotal distribution of the proposed test statistic under null, as well as the asymptotic behaviors under both local and fixed alternatives.","The weak consistency of the change-point estimator is provided.","Extensive simulation studies and the analysis of two real-world datasets illustrate the superior performance of our approach compared to existing model-free change-point detection methods."],"url":"http://arxiv.org/abs/2404.06995v1","category":"stat.ME"}
{"created":"2024-04-10 13:12:05","title":"New Parameters for Star Cluster Dynamics: the role of clusters initial conditions","abstract":"We recently introduced three new parameters that describe the shape of the normalized cumulative radial distribution (nCRD) of the innermost stars in globular clusters and trace the clusters dynamical evolution. Here we extend our previous investigations to the case of a large set of Monte Carlo simulations of globular clusters, started from a broad range of initial conditions. All the models are analyzed at the same age of 13 Gyr, when they have reached different evolutionary phases. The sample of models is well representative of the structural properties of the observed population of Galactic globular clusters. We confirm that the three nCRD parameters are powerful tools to distinguish systems in early stages of dynamical evolution, from those that already experienced core collapse. They might also help disentangle clusters hosting a low-mass intermediate-mass black hole of a few hundred solar masses, from cases with large concentrations of dark remnants in their centers. With respect to other dynamical indicators, the nCRD parameters offer the advantage of being fully empirical and easier to measure from observational data.","sentences":["We recently introduced three new parameters that describe the shape of the normalized cumulative radial distribution (nCRD) of the innermost stars in globular clusters and trace the clusters dynamical evolution.","Here we extend our previous investigations to the case of a large set of Monte Carlo simulations of globular clusters, started from a broad range of initial conditions.","All the models are analyzed at the same age of 13 Gyr, when they have reached different evolutionary phases.","The sample of models is well representative of the structural properties of the observed population of Galactic globular clusters.","We confirm that the three nCRD parameters are powerful tools to distinguish systems in early stages of dynamical evolution, from those that already experienced core collapse.","They might also help disentangle clusters hosting a low-mass intermediate-mass black hole of a few hundred solar masses, from cases with large concentrations of dark remnants in their centers.","With respect to other dynamical indicators, the nCRD parameters offer the advantage of being fully empirical and easier to measure from observational data."],"url":"http://arxiv.org/abs/2404.06992v1","category":"astro-ph.GA"}
{"created":"2024-04-10 12:22:19","title":"V-MAD: Video-based Morphing Attack Detection in Operational Scenarios","abstract":"In response to the rising threat of the face morphing attack, this paper introduces and explores the potential of Video-based Morphing Attack Detection (V-MAD) systems in real-world operational scenarios. While current morphing attack detection methods primarily focus on a single or a pair of images, V-MAD is based on video sequences, exploiting the video streams often acquired by face verification tools available, for instance, at airport gates. Through this study, we show for the first time the advantages that the availability of multiple probe frames can bring to the morphing attack detection task, especially in scenarios where the quality of probe images is varied and might be affected, for instance, by pose or illumination variations. Experimental results on a real operational database demonstrate that video sequences represent valuable information for increasing the robustness and performance of morphing attack detection systems.","sentences":["In response to the rising threat of the face morphing attack, this paper introduces and explores the potential of Video-based Morphing Attack Detection (V-MAD) systems in real-world operational scenarios.","While current morphing attack detection methods primarily focus on a single or a pair of images, V-MAD is based on video sequences, exploiting the video streams often acquired by face verification tools available, for instance, at airport gates.","Through this study, we show for the first time the advantages that the availability of multiple probe frames can bring to the morphing attack detection task, especially in scenarios where the quality of probe images is varied and might be affected, for instance, by pose or illumination variations.","Experimental results on a real operational database demonstrate that video sequences represent valuable information for increasing the robustness and performance of morphing attack detection systems."],"url":"http://arxiv.org/abs/2404.06963v1","category":"cs.CV"}
{"created":"2024-04-10 12:21:03","title":"Peak Time-Windowed Risk Estimation of Stochastic Processes","abstract":"This paper develops a method to upper-bound extreme-values of time-windowed risks for stochastic processes. Examples of such risks include the maximum average or 90% quantile of the current along a transmission line in any 5-minute window. This work casts the time-windowed risk analysis problem as an infinite-dimensional linear program in occupation measures. In particular, we employ the coherent risk measures of the mean and the expected shortfall (conditional value at risk) to define the maximal time-windowed risk along trajectories. The infinite-dimensional linear program must then be truncated into finite-dimensional optimization problems, such as by using the moment-sum of squares hierarchy of semidefinite programs. The infinite-dimensional linear program will have the same optimal value as the original nonconvex risk estimation task under compactness and regularity assumptions, and the sequence of semidefinite programs will converge to the true value under additional properties of algebraic characterization. The scheme is demonstrated for risk analysis of example stochastic processes.","sentences":["This paper develops a method to upper-bound extreme-values of time-windowed risks for stochastic processes.","Examples of such risks include the maximum average or 90% quantile of the current along a transmission line in any 5-minute window.","This work casts the time-windowed risk analysis problem as an infinite-dimensional linear program in occupation measures.","In particular, we employ the coherent risk measures of the mean and the expected shortfall (conditional value at risk) to define the maximal time-windowed risk along trajectories.","The infinite-dimensional linear program must then be truncated into finite-dimensional optimization problems, such as by using the moment-sum of squares hierarchy of semidefinite programs.","The infinite-dimensional linear program will have the same optimal value as the original nonconvex risk estimation task under compactness and regularity assumptions, and the sequence of semidefinite programs will converge to the true value under additional properties of algebraic characterization.","The scheme is demonstrated for risk analysis of example stochastic processes."],"url":"http://arxiv.org/abs/2404.06961v2","category":"math.OC"}
{"created":"2024-04-10 12:17:55","title":"Star-planet interaction and its impact on the stellar rotation","abstract":"The stellar rotation has an essential role in modifying the structure of the star and, therefore, the way these different interplays arise. On the other hand, changes in orbits impact the star's rotation and its evolution. The evolution of the star's rotation accounts for the angular momentum exchange with the planet and follows the effects of the internal transport of angular momentum and metallicity. Several models in the literature have aimed to find a theoretical way to study these interactions between the planet's orbital evolution and the star's rotation. Our work is a promising attempt to investigate these interactions from a model based on a new statistical approach. To this end, we propose a ``tidal interaction index'' that carries all the parameters of the star-planet system that can affect the transport of angular momentum and, consequently, the evolution of stellar rotation. This index is similar to the ``magnetic braking index'' whose most successful value equals 3, which expresses the seminal Skumunich law. Our model is computed for masses of the host star less than the Kraft limit for three orbital-rotation period regimes and the semi-major axis less than 1 AU. We consider planets with masses between 0.4M$_{\\oplus}$ and 20M$_{\\rm J}$ with orbital periods between 0.3 and 225 days. We show that the tidal index $q$ segregated by stellar mass without wind magnetic braking during the main-sequence phase is strongly anti-correlated with planetary mass. Finally, we conclude that in cases where planets retain less than 84\\% of the total angular momentum within the system, the magnetic braking mechanism proves to be more effective than tidal interactions, irrespective of whether the planets' angular momentum surpasses that of the host star.","sentences":["The stellar rotation has an essential role in modifying the structure of the star and, therefore, the way these different interplays arise.","On the other hand, changes in orbits impact the star's rotation and its evolution.","The evolution of the star's rotation accounts for the angular momentum exchange with the planet and follows the effects of the internal transport of angular momentum and metallicity.","Several models in the literature have aimed to find a theoretical way to study these interactions between the planet's orbital evolution and the star's rotation.","Our work is a promising attempt to investigate these interactions from a model based on a new statistical approach.","To this end, we propose a ``tidal interaction index'' that carries all the parameters of the star-planet system that can affect the transport of angular momentum and, consequently, the evolution of stellar rotation.","This index is similar to the ``magnetic braking index'' whose most successful value equals 3, which expresses the seminal Skumunich law.","Our model is computed for masses of the host star less than the Kraft limit for three orbital-rotation period regimes and the semi-major axis less than 1 AU.","We consider planets with masses between 0.4M$_{\\oplus}$ and 20M$_{\\rm J}$ with orbital periods between 0.3 and 225 days.","We show that the tidal index $q$ segregated by stellar mass without wind magnetic braking during the main-sequence phase is strongly anti-correlated with planetary mass.","Finally, we conclude that in cases where planets retain less than 84\\% of the total angular momentum within the system, the magnetic braking mechanism proves to be more effective than tidal interactions, irrespective of whether the planets' angular momentum surpasses that of the host star."],"url":"http://arxiv.org/abs/2404.06958v1","category":"astro-ph.EP"}
{"created":"2024-04-10 12:10:48","title":"Blow-up of stochastic semilinear parabolic equations driven by L\u00e9vy noise","abstract":"The blow-up phenomena of stochastic semilinear parabolic equations with additive as well as linear multiplicative L\\'evy noises are investigated in this work. By suitably modifying the concavity method in the stochastic context, we establish the blow-up phenomena of such systems defined on bounded domains.","sentences":["The blow-up phenomena of stochastic semilinear parabolic equations with additive as well as linear multiplicative L\\'evy noises are investigated in this work.","By suitably modifying the concavity method in the stochastic context, we establish the blow-up phenomena of such systems defined on bounded domains."],"url":"http://arxiv.org/abs/2404.06953v1","category":"math.PR"}
{"created":"2024-04-10 11:55:55","title":"The fluid dynamics of a viscoelastic fluid dripping onto a substrate","abstract":"Extensional flows of complex fluids are pivotal in industrial applications like spraying, atomisation, and microfluidic drop deposition. The Dripping-on-Substrate (DoS) technique is a conceptually simple, but dynamically-complex, probe of the extensional rheology of low-viscosity, non-Newtonian fluids. DoS involves capillary-driven thinning of a liquid bridge formed by a slowly dispensed drop onto a partially-wetting solid substrate. By following the filament thinning and pinch-off, the extensional viscosity and relaxation time can be determined. Importantly, DoS enables measurements for lower viscosity solutions than commercially available capillary break-up extensional rheometers. To understand DoS operation, we employ a computational rheology approach via adaptively-refined, time-dependent axisymmetric simulations using the open-source Eulerian code, \\textit{Basilisk}. The volume-of-fluid technique is used to capture the moving interface, and the log-conformation transformation enables a stable viscoelastic solution. We focus on understanding the roles of surface tension, elasticity, and finite chain extensibility in the Elasto-Capillary (EC) regime. Additionally, we explore perturbative effects of gravity and substrate wettability in setting the evolution of the self-similar thinning and pinch-off dynamics. To illustrate the interplay of these different forces, we construct a simple one-dimensional model capturing the initial thinning rates, balancing inertia and capillarity. This model also describes the structure of the transition region to the nonlinear EC regime, where elastic stresses counteract capillary pressure in the thread as the filament thins toward breakup. Finally, we propose a fitting methodology based on the analytical solutions for FENE-P fluids to enhance accuracy in determining the effective relaxation time for unknown fluids.","sentences":["Extensional flows of complex fluids are pivotal in industrial applications like spraying, atomisation, and microfluidic drop deposition.","The Dripping-on-Substrate (DoS) technique is a conceptually simple, but dynamically-complex, probe of the extensional rheology of low-viscosity, non-Newtonian fluids.","DoS involves capillary-driven thinning of a liquid bridge formed by a slowly dispensed drop onto a partially-wetting solid substrate.","By following the filament thinning and pinch-off, the extensional viscosity and relaxation time can be determined.","Importantly, DoS enables measurements for lower viscosity solutions than commercially available capillary break-up extensional rheometers.","To understand DoS operation, we employ a computational rheology approach via adaptively-refined, time-dependent axisymmetric simulations using the open-source Eulerian code, \\textit{Basilisk}.","The volume-of-fluid technique is used to capture the moving interface, and the log-conformation transformation enables a stable viscoelastic solution.","We focus on understanding the roles of surface tension, elasticity, and finite chain extensibility in the Elasto-Capillary (EC) regime.","Additionally, we explore perturbative effects of gravity and substrate wettability in setting the evolution of the self-similar thinning and pinch-off dynamics.","To illustrate the interplay of these different forces, we construct a simple one-dimensional model capturing the initial thinning rates, balancing inertia and capillarity.","This model also describes the structure of the transition region to the nonlinear EC regime, where elastic stresses counteract capillary pressure in the thread as the filament thins toward breakup.","Finally, we propose a fitting methodology based on the analytical solutions for FENE-P fluids to enhance accuracy in determining the effective relaxation time for unknown fluids."],"url":"http://arxiv.org/abs/2404.06947v1","category":"physics.flu-dyn"}
{"created":"2024-04-10 11:49:07","title":"Transport through a lattice with local loss: from quantum dots to lattice gases","abstract":"Recent work has studied fermion transport through a finite one-dimensional lattice of quantum dots, with localized particle loss from the central lattice site. The dots at each end of the lattice are connected to macroscopic leads, represented as zero-temperature reservoirs of free fermions with a given potential difference. Here we show how this model represents one limiting case of a larger class of models that can be realized with cold quantum gases in optical lattices. While quantum gas realizations allow many system parameters to be varied, we note limitations from finite size effects, and conclude that quantum dots and quantum gases offer complementary views on transport through lossy finite lattices.","sentences":["Recent work has studied fermion transport through a finite one-dimensional lattice of quantum dots, with localized particle loss from the central lattice site.","The dots at each end of the lattice are connected to macroscopic leads, represented as zero-temperature reservoirs of free fermions with a given potential difference.","Here we show how this model represents one limiting case of a larger class of models that can be realized with cold quantum gases in optical lattices.","While quantum gas realizations allow many system parameters to be varied, we note limitations from finite size effects, and conclude that quantum dots and quantum gases offer complementary views on transport through lossy finite lattices."],"url":"http://arxiv.org/abs/2404.06942v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-10 11:43:01","title":"Optimal Matching of Thermal Vibrations into Carbon Nanotubes","abstract":"Carbon nanotubes (CNTs) are promising candidates to improve the thermal conductivity of nano-composites. The main obstacle to these applications is the extremely high thermal boundary (Kapitza) resistance between the CNTs and their matrix. In this theoretical work our goal is to maximize the heat flux through the CNT by functionalizing the CNT ends. We use a Landauer approach to calculate and optimize the energy flux from a soft to a hard material in one dimension through a connecting continuous medium of varying elasticity and density. The transmission probability of phonons through the system is calculated both numerically and analytically. We find that over 90% of the maximum heat flux into CNT is possible for 1nm length of the intermediate material at room temperature (300K).","sentences":["Carbon nanotubes (CNTs) are promising candidates to improve the thermal conductivity of nano-composites.","The main obstacle to these applications is the extremely high thermal boundary (Kapitza) resistance between the CNTs and their matrix.","In this theoretical work our goal is to maximize the heat flux through the CNT by functionalizing the CNT ends.","We use a Landauer approach to calculate and optimize the energy flux from a soft to a hard material in one dimension through a connecting continuous medium of varying elasticity and density.","The transmission probability of phonons through the system is calculated both numerically and analytically.","We find that over 90% of the maximum heat flux into CNT is possible for 1nm length of the intermediate material at room temperature (300K)."],"url":"http://arxiv.org/abs/2404.06938v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-10 11:42:48","title":"Phenomenon of a stronger trapping behaviour in $\u039b$-type quantum systems with symmetry","abstract":"$\\Lambda$, $V$, $\\Xi$ (ladder), and other three-level quantum systems with one forbidden transition ($\\Lambda$-type systems) play an important role in quantum physics. Various applications require manipulation by such systems using as control shaped laser field. In this work, we study how degeneracy in energy states and Bohr frequencies of these systems affects the efficiency or difficulty of finding optimal shape of the control field. For this, we adopt the notion of higher order traps which was introduced in [A.N. Pechen and D.J. Tannor, Are there traps in quantum control landscapes? Phys. Rev. Lett. {\\bf 106}, 120402 (2011)], where second/third order traps were discovered for $\\Lambda$-type systems with one forbidden transition and with non-degenerate energy levels. We study control of such systems with and without denegeracy in their eigenstates and Bohr frequencies, and investigate how these degeneracies influence on the efficiency of optimizing the control laser field. We find that the degeneracy of Bohr frequencies in the $\\Xi$ system leads to the appearance of seventh order trap with a more significant attracting domain resulting in a more difficult optimization, while degeneracy in energy states of $\\Lambda$-type systems does not lead to increase of the order of the zero control trap compared to the non-degenerate case.","sentences":["$\\Lambda$, $V$, $\\Xi$ (ladder), and other three-level quantum systems with one forbidden transition ($\\Lambda$-type systems) play an important role in quantum physics.","Various applications require manipulation by such systems using as control shaped laser field.","In this work, we study how degeneracy in energy states and Bohr frequencies of these systems affects the efficiency or difficulty of finding optimal shape of the control field.","For this, we adopt the notion of higher order traps which was introduced in [A.N. Pechen and D.J. Tannor, Are there traps in quantum control landscapes?","Phys. Rev. Lett.","{\\bf 106}, 120402 (2011)], where second/third order traps were discovered for $\\Lambda$-type systems with one forbidden transition and with non-degenerate energy levels.","We study control of such systems with and without denegeracy in their eigenstates and Bohr frequencies, and investigate how these degeneracies influence on the efficiency of optimizing the control laser field.","We find that the degeneracy of Bohr frequencies in the $\\Xi$ system leads to the appearance of seventh order trap with a more significant attracting domain resulting in a more difficult optimization, while degeneracy in energy states of $\\Lambda$-type systems does not lead to increase of the order of the zero control trap compared to the non-degenerate case."],"url":"http://arxiv.org/abs/2404.06937v1","category":"quant-ph"}
{"created":"2024-04-10 11:30:07","title":"Multifractal phase in the weighted adjacency matrices of random Erd\u00f6s-R\u00e9nyi graphs","abstract":"We study the spectral properties of the adjacency matrix in the giant connected component of Erd\\\"os-R\\'enyi random graphs, with average connectivity $p$ and randomly distributed hopping amplitudes. By solving the self-consistent cavity equations satisfied by the matrix elements of the resolvent, we compute the probability distribution of the local density of states, which governs the scaling with the system size of the moments of the eigenvectors' amplitudes, as well as several other observables related to the spectral statistics. For small values of $p>1$ above the percolation threshold, we unveil the presence of an exotic delocalized but (weakly) multifractal phase in a broad region of the parameter space, which separates the localized phase found for $p\\le1$ from the fully-delocalized GOE-like phase expected for $p\\to \\infty$. We explore the fundamental physical mechanism underlying the emergence of delocalized multifractal states, rooted in the pronounced heterogeneity in the topology of the graph. This heterogeneity arises from the interplay between strong fluctuations in local degrees and hopping amplitudes, and leads to an effective fragmentation of the graph. We further support our findings by characterizing the level statistics and the two-point spatial correlations within the multifractal phase, and address the ensuing anomalous transport and relaxation properties affecting the quantum dynamical evolution.","sentences":["We study the spectral properties of the adjacency matrix in the giant connected component of Erd\\\"os-R\\'enyi random graphs, with average connectivity $p$ and randomly distributed hopping amplitudes.","By solving the self-consistent cavity equations satisfied by the matrix elements of the resolvent, we compute the probability distribution of the local density of states, which governs the scaling with the system size of the moments of the eigenvectors' amplitudes, as well as several other observables related to the spectral statistics.","For small values of $p>1$ above the percolation threshold, we unveil the presence of an exotic delocalized but (weakly) multifractal phase in a broad region of the parameter space, which separates the localized phase found for $p\\le1$ from the fully-delocalized GOE-like phase expected for $p\\to \\infty$. We explore the fundamental physical mechanism underlying the emergence of delocalized multifractal states, rooted in the pronounced heterogeneity in the topology of the graph.","This heterogeneity arises from the interplay between strong fluctuations in local degrees and hopping amplitudes, and leads to an effective fragmentation of the graph.","We further support our findings by characterizing the level statistics and the two-point spatial correlations within the multifractal phase, and address the ensuing anomalous transport and relaxation properties affecting the quantum dynamical evolution."],"url":"http://arxiv.org/abs/2404.06931v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-10 11:27:06","title":"Efficient Sound Field Reconstruction with Conditional Invertible Neural Networks","abstract":"In this study, we introduce a method for estimating sound fields in reverberant environments using a conditional invertible neural network (CINN). Sound field reconstruction can be hindered by experimental errors, limited spatial data, model mismatches, and long inference times, leading to potentially flawed and prolonged characterizations. Further, the complexity of managing inherent uncertainties often escalates computational demands or is neglected in models. Our approach seeks to balance accuracy and computational efficiency, while incorporating uncertainty estimates to tailor reconstructions to specific needs. By training a CINN with Monte Carlo simulations of random wave fields, our method reduces the dependency on extensive datasets and enables inference from sparse experimental data. The CINN proves versatile at reconstructing Room Impulse Responses (RIRs), by acting either as a likelihood model for maximum a posteriori estimation or as an approximate posterior distribution through amortized Bayesian inference. Compared to traditional Bayesian methods, the CINN achieves similar accuracy with greater efficiency and without requiring its adaptation to distinct sound field conditions.","sentences":["In this study, we introduce a method for estimating sound fields in reverberant environments using a conditional invertible neural network (CINN).","Sound field reconstruction can be hindered by experimental errors, limited spatial data, model mismatches, and long inference times, leading to potentially flawed and prolonged characterizations.","Further, the complexity of managing inherent uncertainties often escalates computational demands or is neglected in models.","Our approach seeks to balance accuracy and computational efficiency, while incorporating uncertainty estimates to tailor reconstructions to specific needs.","By training a CINN with Monte Carlo simulations of random wave fields, our method reduces the dependency on extensive datasets and enables inference from sparse experimental data.","The CINN proves versatile at reconstructing Room Impulse Responses (RIRs), by acting either as a likelihood model for maximum a posteriori estimation or as an approximate posterior distribution through amortized Bayesian inference.","Compared to traditional Bayesian methods, the CINN achieves similar accuracy with greater efficiency and without requiring its adaptation to distinct sound field conditions."],"url":"http://arxiv.org/abs/2404.06928v1","category":"eess.AS"}
{"created":"2024-04-10 11:24:34","title":"Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera SLAM with 3D Gaussian Splatting","abstract":"We present a real-time LiDAR-Inertial-Camera SLAM system with 3D Gaussian Splatting as the mapping backend. Leveraging robust pose estimates from our LiDAR-Inertial-Camera odometry, Coco-LIC, an incremental photo-realistic mapping system is proposed in this paper. We initialize 3D Gaussians from colorized LiDAR points and optimize them using differentiable rendering powered by 3D Gaussian Splatting. Meticulously designed strategies are employed to incrementally expand the Gaussian map and adaptively control its density, ensuring high-quality mapping with real-time capability. Experiments conducted in diverse scenarios demonstrate the superior performance of our method compared to existing radiance-field-based SLAM systems.","sentences":["We present a real-time LiDAR-Inertial-Camera SLAM system with 3D Gaussian Splatting as the mapping backend.","Leveraging robust pose estimates from our LiDAR-Inertial-Camera odometry, Coco-LIC, an incremental photo-realistic mapping system is proposed in this paper.","We initialize 3D Gaussians from colorized LiDAR points and optimize them using differentiable rendering powered by 3D Gaussian Splatting.","Meticulously designed strategies are employed to incrementally expand the Gaussian map and adaptively control its density, ensuring high-quality mapping with real-time capability.","Experiments conducted in diverse scenarios demonstrate the superior performance of our method compared to existing radiance-field-based SLAM systems."],"url":"http://arxiv.org/abs/2404.06926v1","category":"cs.RO"}
{"created":"2024-04-10 11:24:05","title":"From MRI to SARI: Parametric survey of accretion disk instabilities","abstract":"Accretion disks are highly unstable to magnetic instabilities driven by shear flow, where classically, the axisymmetric, weak-field Magneto-Rotational Instability (MRI) has received much attention through local WKB approximations. In contrast, discrete non-axisymmetric counterparts require a more involved analysis through a full global approach to deal with the influence of the nearby magnetohydrodynamic (MHD) continua. Recently, rigorous MHD spectroscopy identified a new type of an ultra-localised, non-axisymmetric instability in global disks with super-Alfv\\'enic flow. These Super-Alfv\\'enic Rotational Instabilities (SARIs) fill vast unstable regions in the complex eigenfrequency plane with (near-eigen)modes that corotate at the local Doppler velocity and are radially localised between Alfv\\'enic resonances. Unlike discrete modes, they are utterly insensitive to the radial disk boundaries. In this work, we independently confirm the existence of these unprecedented modes using our novel spectral MHD code Legolas reproducing and extending our earlier study with detailed eigenspectra and eigenfunctions. We calculate growth rates of SARIs and MRI in a variety of disk equilibria, highlighting the impact of field strength and orientation, and find correspondence with analytical predictions for thin, weakly magnetised disks. We show that non-axisymmetric modes can significantly extend instability regimes at high mode numbers, with maximal growth rates comparable to the MRI. Furthermore, we explicitly show a region filled with quasi-modes whose eigenfunctions are extremely localised in all directions. These modes must be ubiquitous in accretion disks, and play a role in local shearing box simulations. Finally, we revisit recent dispersion relations in the Appendix, highlighting their relation to our global framework.","sentences":["Accretion disks are highly unstable to magnetic instabilities driven by shear flow, where classically, the axisymmetric, weak-field Magneto-Rotational Instability (MRI) has received much attention through local WKB approximations.","In contrast, discrete non-axisymmetric counterparts require a more involved analysis through a full global approach to deal with the influence of the nearby magnetohydrodynamic (MHD) continua.","Recently, rigorous MHD spectroscopy identified a new type of an ultra-localised, non-axisymmetric instability in global disks with super-Alfv\\'enic flow.","These Super-Alfv\\'enic Rotational Instabilities (SARIs) fill vast unstable regions in the complex eigenfrequency plane with (near-eigen)modes that corotate at the local Doppler velocity and are radially localised between Alfv\\'enic resonances.","Unlike discrete modes, they are utterly insensitive to the radial disk boundaries.","In this work, we independently confirm the existence of these unprecedented modes using our novel spectral MHD code Legolas reproducing and extending our earlier study with detailed eigenspectra and eigenfunctions.","We calculate growth rates of SARIs and MRI in a variety of disk equilibria, highlighting the impact of field strength and orientation, and find correspondence with analytical predictions for thin, weakly magnetised disks.","We show that non-axisymmetric modes can significantly extend instability regimes at high mode numbers, with maximal growth rates comparable to the MRI.","Furthermore, we explicitly show a region filled with quasi-modes whose eigenfunctions are extremely localised in all directions.","These modes must be ubiquitous in accretion disks, and play a role in local shearing box simulations.","Finally, we revisit recent dispersion relations in the Appendix, highlighting their relation to our global framework."],"url":"http://arxiv.org/abs/2404.06925v1","category":"astro-ph.HE"}
{"created":"2024-04-10 11:19:04","title":"On metric equivalence of the Brieskorn-Pham hypersurfaces","abstract":"We show that two bi-Lipschitz equivalent Brieskorn-Pham hypersurfaces have the same multiplicities at $0$. Moreover we show that if two algebraic $(n-1)$-dimensional cones $P, R\\subset\\mathbb C^n$ with isolated singularities are homeomorphic, then they have the same degree.","sentences":["We show that two bi-Lipschitz equivalent Brieskorn-Pham hypersurfaces have the same multiplicities at $0$. Moreover we show that if two algebraic $(n-1)$-dimensional cones $P, R\\subset\\mathbb C^n$ with isolated singularities are homeomorphic, then they have the same degree."],"url":"http://arxiv.org/abs/2404.06922v1","category":"math.AG"}
{"created":"2024-04-10 11:02:19","title":"On weighted failure rate, its means and associated quantile version","abstract":"In this paper, we define weighted failure rate and their different means from the stand point of an application. We begin by emphasizing that the formation of n independent component series system having weighted failure rates with sum of weight functions being unity is same as a mixture of n distributions. We derive some parametric and non-parametric characterization results. We discuss on the form invariance property of baseline failure rate for a specific choice of weight function. Some bounds on means of aging functions are obtained. Here, we establish that weighted IFRA class is not closed under formation of coherent systems unlike the IFRA class. An interesting application of the present work is credited to the fact that the quantile version of means of failure rate is obtained as a special case of weighted means of failure rate.","sentences":["In this paper, we define weighted failure rate and their different means from the stand point of an application.","We begin by emphasizing that the formation of n independent component series system having weighted failure rates with sum of weight functions being unity is same as a mixture of n distributions.","We derive some parametric and non-parametric characterization results.","We discuss on the form invariance property of baseline failure rate for a specific choice of weight function.","Some bounds on means of aging functions are obtained.","Here, we establish that weighted IFRA class is not closed under formation of coherent systems unlike the IFRA class.","An interesting application of the present work is credited to the fact that the quantile version of means of failure rate is obtained as a special case of weighted means of failure rate."],"url":"http://arxiv.org/abs/2404.06909v1","category":"math.ST"}
{"created":"2024-04-10 10:57:44","title":"A nano vacuum gauge based on second-order coherence in optical levitation","abstract":"Accurate measurement of pressure with a wide dynamic range holds significant importance for various applications. This issue can be realized with a mechanical nano-oscillator, where the pressure-related collisions with surrounding molecules induce its energy dissipation. However, this energy dissipation of the nano-oscillator may be overshadowed by other processes. Here, we apply the second-order coherence analysis to accurately characterize those distinct dissipation processes. Based on an optically levitated nano-oscillator, we successfully obtain precise measurements of the air pressure surrounding the particles from atmosphere to 7E-6 mbar, over 8 orders of magnitude. It proves that the mechanical nano-oscillator is an extremely promising candidate for precision pressure sensing applications. Moreover, the second-order coherence analysis method on a classical system can pave the way to characterize the dynamic properties of an oscillator, which will benefit microscopic thermodynamics, precision measurement, and macroscopic quantum research.","sentences":["Accurate measurement of pressure with a wide dynamic range holds significant importance for various applications.","This issue can be realized with a mechanical nano-oscillator, where the pressure-related collisions with surrounding molecules induce its energy dissipation.","However, this energy dissipation of the nano-oscillator may be overshadowed by other processes.","Here, we apply the second-order coherence analysis to accurately characterize those distinct dissipation processes.","Based on an optically levitated nano-oscillator, we successfully obtain precise measurements of the air pressure surrounding the particles from atmosphere to 7E-6 mbar, over 8 orders of magnitude.","It proves that the mechanical nano-oscillator is an extremely promising candidate for precision pressure sensing applications.","Moreover, the second-order coherence analysis method on a classical system can pave the way to characterize the dynamic properties of an oscillator, which will benefit microscopic thermodynamics, precision measurement, and macroscopic quantum research."],"url":"http://arxiv.org/abs/2404.06907v1","category":"physics.optics"}
{"created":"2024-04-10 10:57:18","title":"SARA: Smart AI Reading Assistant for Reading Comprehension","abstract":"SARA integrates Eye Tracking and state-of-the-art large language models in a mixed reality framework to enhance the reading experience by providing personalized assistance in real-time. By tracking eye movements, SARA identifies the text segments that attract the user's attention the most and potentially indicate uncertain areas and comprehension issues. The process involves these key steps: text detection and extraction, gaze tracking and alignment, and assessment of detected reading difficulty. The results are customized solutions presented directly within the user's field of view as virtual overlays on identified difficult text areas. This support enables users to overcome challenges like unfamiliar vocabulary and complex sentences by offering additional context, rephrased solutions, and multilingual help. SARA's innovative approach demonstrates it has the potential to transform the reading experience and improve reading proficiency.","sentences":["SARA integrates Eye Tracking and state-of-the-art large language models in a mixed reality framework to enhance the reading experience by providing personalized assistance in real-time.","By tracking eye movements, SARA identifies the text segments that attract the user's attention the most and potentially indicate uncertain areas and comprehension issues.","The process involves these key steps: text detection and extraction, gaze tracking and alignment, and assessment of detected reading difficulty.","The results are customized solutions presented directly within the user's field of view as virtual overlays on identified difficult text areas.","This support enables users to overcome challenges like unfamiliar vocabulary and complex sentences by offering additional context, rephrased solutions, and multilingual help.","SARA's innovative approach demonstrates it has the potential to transform the reading experience and improve reading proficiency."],"url":"http://arxiv.org/abs/2404.06906v1","category":"cs.HC"}
{"created":"2024-04-10 10:45:30","title":"NFARec: A Negative Feedback-Aware Recommender Model","abstract":"Graph neural network (GNN)-based models have been extensively studied for recommendations, as they can extract high-order collaborative signals accurately which is required for high-quality recommender systems. However, they neglect the valuable information gained through negative feedback in two aspects: (1) different users might hold opposite feedback on the same item, which hampers optimal information propagation in GNNs, and (2) even when an item vastly deviates from users' preferences, they might still choose it and provide a negative rating. In this paper, we propose a negative feedback-aware recommender model (NFARec) that maximizes the leverage of negative feedback. To transfer information to multi-hop neighbors along an optimal path effectively, NFARec adopts a feedback-aware correlation that guides hypergraph convolutions (HGCs) to learn users' structural representations. Moreover, NFARec incorporates an auxiliary task - predicting the feedback sentiment polarity (i.e., positive or negative) of the next interaction - based on the Transformer Hawkes Process. The task is beneficial for understanding users by learning the sentiment expressed in their previous sequential feedback patterns and predicting future interactions. Extensive experiments demonstrate that NFARec outperforms competitive baselines. Our source code and data are released at https://github.com/WangXFng/NFARec.","sentences":["Graph neural network (GNN)-based models have been extensively studied for recommendations, as they can extract high-order collaborative signals accurately which is required for high-quality recommender systems.","However, they neglect the valuable information gained through negative feedback in two aspects: (1) different users might hold opposite feedback on the same item, which hampers optimal information propagation in GNNs, and (2) even when an item vastly deviates from users' preferences, they might still choose it and provide a negative rating.","In this paper, we propose a negative feedback-aware recommender model (NFARec) that maximizes the leverage of negative feedback.","To transfer information to multi-hop neighbors along an optimal path effectively, NFARec adopts a feedback-aware correlation that guides hypergraph convolutions (HGCs) to learn users' structural representations.","Moreover, NFARec incorporates an auxiliary task - predicting the feedback sentiment polarity (i.e., positive or negative) of the next interaction - based on the Transformer Hawkes Process.","The task is beneficial for understanding users by learning the sentiment expressed in their previous sequential feedback patterns and predicting future interactions.","Extensive experiments demonstrate that NFARec outperforms competitive baselines.","Our source code and data are released at https://github.com/WangXFng/NFARec."],"url":"http://arxiv.org/abs/2404.06900v1","category":"cs.IR"}
{"created":"2024-04-10 10:42:49","title":"SQUID oscillations in PbTe nanowire networks","abstract":"Network structures by semiconductor nanowires hold great promise for advanced quantum devices, especially for applications in topological quantum computing. In this study, we created networks of PbTe nanowires arranged in loop configurations. Using shadow-wall epitaxy, we defined superconducting quantum interference devices (SQUIDs) using the superconductor Pb. These SQUIDs exhibit oscillations in supercurrent upon the scanning of a magnetic field. Most of the oscillations can be fitted assuming a sinusoidal current-phase relation for each Josephson junction. Under certain conditions, the oscillations are found to be skewed, suggesting possible deviation from a sinusoidal behavior. Our results highlight the potential of PbTe nanowires for building complex quantum devices in the form of networks.","sentences":["Network structures by semiconductor nanowires hold great promise for advanced quantum devices, especially for applications in topological quantum computing.","In this study, we created networks of PbTe nanowires arranged in loop configurations.","Using shadow-wall epitaxy, we defined superconducting quantum interference devices (SQUIDs) using the superconductor Pb.","These SQUIDs exhibit oscillations in supercurrent upon the scanning of a magnetic field.","Most of the oscillations can be fitted assuming a sinusoidal current-phase relation for each Josephson junction.","Under certain conditions, the oscillations are found to be skewed, suggesting possible deviation from a sinusoidal behavior.","Our results highlight the potential of PbTe nanowires for building complex quantum devices in the form of networks."],"url":"http://arxiv.org/abs/2404.06899v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-10 10:34:34","title":"SparseAD: Sparse Query-Centric Paradigm for Efficient End-to-End Autonomous Driving","abstract":"End-to-End paradigms use a unified framework to implement multi-tasks in an autonomous driving system. Despite simplicity and clarity, the performance of end-to-end autonomous driving methods on sub-tasks is still far behind the single-task methods. Meanwhile, the widely used dense BEV features in previous end-to-end methods make it costly to extend to more modalities or tasks. In this paper, we propose a Sparse query-centric paradigm for end-to-end Autonomous Driving (SparseAD), where the sparse queries completely represent the whole driving scenario across space, time and tasks without any dense BEV representation. Concretely, we design a unified sparse architecture for perception tasks including detection, tracking, and online mapping. Moreover, we revisit motion prediction and planning, and devise a more justifiable motion planner framework. On the challenging nuScenes dataset, SparseAD achieves SOTA full-task performance among end-to-end methods and significantly narrows the performance gap between end-to-end paradigms and single-task methods. Codes will be released soon.","sentences":["End-to-End paradigms use a unified framework to implement multi-tasks in an autonomous driving system.","Despite simplicity and clarity, the performance of end-to-end autonomous driving methods on sub-tasks is still far behind the single-task methods.","Meanwhile, the widely used dense BEV features in previous end-to-end methods make it costly to extend to more modalities or tasks.","In this paper, we propose a Sparse query-centric paradigm for end-to-end Autonomous Driving (SparseAD), where the sparse queries completely represent the whole driving scenario across space, time and tasks without any dense BEV representation.","Concretely, we design a unified sparse architecture for perception tasks including detection, tracking, and online mapping.","Moreover, we revisit motion prediction and planning, and devise a more justifiable motion planner framework.","On the challenging nuScenes dataset, SparseAD achieves SOTA full-task performance among end-to-end methods and significantly narrows the performance gap between end-to-end paradigms and single-task methods.","Codes will be released soon."],"url":"http://arxiv.org/abs/2404.06892v1","category":"cs.CV"}
{"created":"2024-04-10 10:31:11","title":"Discrete time crystals in the presence of non-Markovian dynamics","abstract":"We study discrete time crystals (DTCs) in periodically driven quantum systems, in the presence of non-Markovian dissipation. In contrast to DTCs observed in earlier works in the presence of Markovian dynamics, using the open Dicke model in presence of Jaynes-Cummings-like dissipation, we show that non-Markovian regime can be highly beneficial for stabilizing DTCs over a wide range of parameter values. This may be attributed to periodically varying dissipation rates even at long times in the case of non-Markovian dynamics. Further the Markovian and non-Markovian regimes show sharp distinctions for intermediate strengths of the dissipator coefficient, with a time-independent steady-state in the Markovian regime being replaced by varied dynamical phases, including DTC order, in the non-Markovian regime. We also verify the robustness of the DTC phase in the non-Markovian regime by introducing errors both in the Hamiltonian as well as in the dissipation. Our study shows the possibility of using DTC as a probe for non-Markovian dynamics in periodically modulated open quantum systems, at long times.","sentences":["We study discrete time crystals (DTCs) in periodically driven quantum systems, in the presence of non-Markovian dissipation.","In contrast to DTCs observed in earlier works in the presence of Markovian dynamics, using the open Dicke model in presence of Jaynes-Cummings-like dissipation, we show that non-Markovian regime can be highly beneficial for stabilizing DTCs over a wide range of parameter values.","This may be attributed to periodically varying dissipation rates even at long times in the case of non-Markovian dynamics.","Further the Markovian and non-Markovian regimes show sharp distinctions for intermediate strengths of the dissipator coefficient, with a time-independent steady-state in the Markovian regime being replaced by varied dynamical phases, including DTC order, in the non-Markovian regime.","We also verify the robustness of the DTC phase in the non-Markovian regime by introducing errors both in the Hamiltonian as well as in the dissipation.","Our study shows the possibility of using DTC as a probe for non-Markovian dynamics in periodically modulated open quantum systems, at long times."],"url":"http://arxiv.org/abs/2404.06890v1","category":"quant-ph"}
{"created":"2024-04-10 10:29:08","title":"Edge Detection Quantumized: A Novel Quantum Algorithm For Image Processing","abstract":"Quantum image processing is a research field that explores the use of quantum computing and algorithms for image processing tasks such as image encoding and edge detection. Although classical edge detection algorithms perform reasonably well and are quite efficient, they become outright slower when it comes to large datasets with high-resolution images. Quantum computing promises to deliver a significant performance boost and breakthroughs in various sectors. Quantum Hadamard Edge Detection (QHED) algorithm, for example, works at constant time complexity, and thus detects edges much faster than any classical algorithm. However, the original QHED algorithm is designed for Quantum Probability Image Encoding (QPIE) and mainly works for binary images. This paper presents a novel protocol by combining the Flexible Representation of Quantum Images (FRQI) encoding and a modified QHED algorithm. An improved edge outline method has been proposed in this work resulting in a better object outline output and more accurate edge detection than the traditional QHED algorithm.","sentences":["Quantum image processing is a research field that explores the use of quantum computing and algorithms for image processing tasks such as image encoding and edge detection.","Although classical edge detection algorithms perform reasonably well and are quite efficient, they become outright slower when it comes to large datasets with high-resolution images.","Quantum computing promises to deliver a significant performance boost and breakthroughs in various sectors.","Quantum Hadamard Edge Detection (QHED) algorithm, for example, works at constant time complexity, and thus detects edges much faster than any classical algorithm.","However, the original QHED algorithm is designed for Quantum Probability Image Encoding (QPIE) and mainly works for binary images.","This paper presents a novel protocol by combining the Flexible Representation of Quantum Images (FRQI) encoding and a modified QHED algorithm.","An improved edge outline method has been proposed in this work resulting in a better object outline output and more accurate edge detection than the traditional QHED algorithm."],"url":"http://arxiv.org/abs/2404.06889v1","category":"quant-ph"}
{"created":"2024-04-10 10:22:20","title":"Topological states constructed by two different trivial quantum wires","abstract":"The topological states of the two-leg and three-leg ladders formed by two trivial quantum wires with different lattice constants are theoretically investigated. For the symmetric nearest-neighbor intra-chain hopping two-leg ladder, the inversion symmetry topological insulator phase with two degenerate topological edge states appears. When the inversion symmetry is broken, the topological insulators with one or two topological edge states of different energies and topological metals with edge states embedded in the bulk states could emerge dependent on the filling factor. The topological origin of these topological states in the two-leg ladders is the topological properties of the Chern insulators and Chern metals. According to the arrangement of two trivial quantum wires, we construct two types of three-leg ladders. Each type of the three-leg ladder could be divided into one trivial subspace and one topological nontrivial subspace by unitary transformation. The topological nontrivial subspace corresponds to the effective two-leg ladder model. As the filling factor changes, the system could be in topological insulators or topological metals phases. These rich topological states in the two-leg and three-leg ladders could be confirmed by current experimental techniques.","sentences":["The topological states of the two-leg and three-leg ladders formed by two trivial quantum wires with different lattice constants are theoretically investigated.","For the symmetric nearest-neighbor intra-chain hopping two-leg ladder, the inversion symmetry topological insulator phase with two degenerate topological edge states appears.","When the inversion symmetry is broken, the topological insulators with one or two topological edge states of different energies and topological metals with edge states embedded in the bulk states could emerge dependent on the filling factor.","The topological origin of these topological states in the two-leg ladders is the topological properties of the Chern insulators and Chern metals.","According to the arrangement of two trivial quantum wires, we construct two types of three-leg ladders.","Each type of the three-leg ladder could be divided into one trivial subspace and one topological nontrivial subspace by unitary transformation.","The topological nontrivial subspace corresponds to the effective two-leg ladder model.","As the filling factor changes, the system could be in topological insulators or topological metals phases.","These rich topological states in the two-leg and three-leg ladders could be confirmed by current experimental techniques."],"url":"http://arxiv.org/abs/2404.06886v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-10 10:11:45","title":"A note on \"Exact Solution of Bipartite Fluctuations in One-Dimensional Fermions''","abstract":"For a one-dimensional system of free fermions, we derive a connection between the full counting statistics of domain-wall and alternating occupancy states.","sentences":["For a one-dimensional system of free fermions, we derive a connection between the full counting statistics of domain-wall and alternating occupancy states."],"url":"http://arxiv.org/abs/2404.06881v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-10 10:08:25","title":"Correlation Functions of Photospheric Magnetic Fields in Solar Active Regions","abstract":"We used magnetograms acquired with the {\\it Helioseismic and Magnetic Imager} (HMI) on board the {\\it Solar Dynamics Observatory} (SDO) to calculate and analyze spatial correlation functions and the multi-fractal spectra in solar active regions (ARs). The analysis was performed for two very different types of ARs: i) simple bipolar magnetic structures with regular orientation (the magneto-morphological class A1), and ii) very complex multi-polar ARs (the magneto-morphological class B3). All ARs were explored at the developed phase during flareless periods. For correlation functions, the power-law and exponential approximations were calculated and compared. It was found that the exponential law holds for the correlation functions of both types of ARs within spatial scales of 1-36~Mm, while the power law failed to approximate the observed correlation functions. The property of multi-fractality was found in all ARs, being better pronounced for the complex B3-class ARs. Our results might imply that photospheric magnetic fields of an AR is a self-organized system, which, however, does not exhibit properties of self-organized criticality (SOC), and its fractal properties are an attribute of more broad (than SOC only) class of non-linear systems.","sentences":["We used magnetograms acquired with the {\\it Helioseismic and Magnetic Imager} (HMI) on board the {\\it Solar Dynamics Observatory} (SDO) to calculate and analyze spatial correlation functions and the multi-fractal spectra in solar active regions (ARs).","The analysis was performed for two very different types of ARs: i) simple bipolar magnetic structures with regular orientation (the magneto-morphological class A1), and ii) very complex multi-polar ARs (the magneto-morphological class B3).","All ARs were explored at the developed phase during flareless periods.","For correlation functions, the power-law and exponential approximations were calculated and compared.","It was found that the exponential law holds for the correlation functions of both types of ARs within spatial scales of 1-36~Mm, while the power law failed to approximate the observed correlation functions.","The property of multi-fractality was found in all ARs, being better pronounced for the complex B3-class ARs.","Our results might imply that photospheric magnetic fields of an AR is a self-organized system, which, however, does not exhibit properties of self-organized criticality (SOC), and its fractal properties are an attribute of more broad (than SOC only) class of non-linear systems."],"url":"http://arxiv.org/abs/2404.06879v1","category":"astro-ph.SR"}
{"created":"2024-04-10 10:06:48","title":"PROJECT-J: JWST observations of HH46~IRS and its outflow. Overview and first results","abstract":"We present the first results of the JWST program PROJECT-J (PROtostellar JEts Cradle Tested with JWST ), designed to study the Class I source HH46 IRS and its outflow through NIRSpec and MIRI spectroscopy (1.66 to 28 micron). The data provide line-images (~ 6.6\" in length with NIRSpec, and up to 20\" with MIRI) revealing unprecedented details within the jet, the molecular outflow and the cavity. We detect, for the first time, the red-shifted jet within ~ 90 au from the source. Dozens of shock-excited forbidden lines are observed, including highly ionized species such as [Ne III] 15.5 micron, suggesting that the gas is excited by high velocity (> 80 km/s) shocks in a relatively high density medium. Images of H2 lines at different excitations outline a complex molecular flow, where a bright cavity, molecular shells, and a jet-driven bow-shock interact with and are shaped by the ambient conditions. Additional NIRCam 2 micron images resolve the HH46 IRS ~ 110 au binary system and suggest that the large asymmetries observed between the jet and the H2 wide angle emission could be due to two separate outflows being driven by the two sources. The spectra of the unresolved binary show deep ice bands and plenty of gaseous lines in absorption, likely originating in a cold envelope or disk. In conclusion, JWST has unraveled for the first time the origin of the HH46 IRS complex outflow demonstrating its capability to investigate embedded regions around young stars, which remain elusive even at near-IR wavelengths.","sentences":["We present the first results of the JWST program PROJECT-J (PROtostellar JEts Cradle Tested with JWST ), designed to study the Class I source HH46 IRS and its outflow through NIRSpec and MIRI spectroscopy (1.66 to 28 micron).","The data provide line-images (~ 6.6\" in length with NIRSpec, and up to 20\" with MIRI) revealing unprecedented details within the jet, the molecular outflow and the cavity.","We detect, for the first time, the red-shifted jet within ~ 90 au from the source.","Dozens of shock-excited forbidden lines are observed, including highly ionized species such as [Ne III] 15.5 micron, suggesting that the gas is excited by high velocity (> 80 km/s) shocks in a relatively high density medium.","Images of H2 lines at different excitations outline a complex molecular flow, where a bright cavity, molecular shells, and a jet-driven bow-shock interact with and are shaped by the ambient conditions.","Additional NIRCam 2 micron images resolve the HH46 IRS ~ 110 au binary system and suggest that the large asymmetries observed between the jet and the H2 wide angle emission could be due to two separate outflows being driven by the two sources.","The spectra of the unresolved binary show deep ice bands and plenty of gaseous lines in absorption, likely originating in a cold envelope or disk.","In conclusion, JWST has unraveled for the first time the origin of the HH46 IRS complex outflow demonstrating its capability to investigate embedded regions around young stars, which remain elusive even at near-IR wavelengths."],"url":"http://arxiv.org/abs/2404.06878v1","category":"astro-ph.GA"}
{"created":"2024-04-10 09:35:50","title":"Monocular 3D lane detection for Autonomous Driving: Recent Achievements, Challenges, and Outlooks","abstract":"3D lane detection plays a crucial role in autonomous driving by extracting structural and traffic information from the road in 3D space to assist the self-driving car in rational, safe, and comfortable path planning and motion control. Due to the consideration of sensor costs and the advantages of visual data in color information, in practical applications, 3D lane detection based on monocular vision is one of the important research directions in the field of autonomous driving, which has attracted more and more attention in both industry and academia. Unfortunately, recent progress in visual perception seems insufficient to develop completely reliable 3D lane detection algorithms, which also hinders the development of vision-based fully autonomous self-driving cars, i.e., achieving level 5 autonomous driving, driving like human-controlled cars. This is one of the conclusions drawn from this review paper: there is still a lot of room for improvement and significant improvements are still needed in the 3D lane detection algorithm for autonomous driving cars using visual sensors. Motivated by this, this review defines, analyzes, and reviews the current achievements in the field of 3D lane detection research, and the vast majority of the current progress relies heavily on computationally complex deep learning models. In addition, this review covers the 3D lane detection pipeline, investigates the performance of state-of-the-art algorithms, analyzes the time complexity of cutting-edge modeling choices, and highlights the main achievements and limitations of current research efforts. The survey also includes a comprehensive discussion of available 3D lane detection datasets and the challenges that researchers have faced but have not yet resolved. Finally, our work outlines future research directions and welcomes researchers and practitioners to enter this exciting field.","sentences":["3D lane detection plays a crucial role in autonomous driving by extracting structural and traffic information from the road in 3D space to assist the self-driving car in rational, safe, and comfortable path planning and motion control.","Due to the consideration of sensor costs and the advantages of visual data in color information, in practical applications, 3D lane detection based on monocular vision is one of the important research directions in the field of autonomous driving, which has attracted more and more attention in both industry and academia.","Unfortunately, recent progress in visual perception seems insufficient to develop completely reliable 3D lane detection algorithms, which also hinders the development of vision-based fully autonomous self-driving cars, i.e., achieving level 5 autonomous driving, driving like human-controlled cars.","This is one of the conclusions drawn from this review paper: there is still a lot of room for improvement and significant improvements are still needed in the 3D lane detection algorithm for autonomous driving cars using visual sensors.","Motivated by this, this review defines, analyzes, and reviews the current achievements in the field of 3D lane detection research, and the vast majority of the current progress relies heavily on computationally complex deep learning models.","In addition, this review covers the 3D lane detection pipeline, investigates the performance of state-of-the-art algorithms, analyzes the time complexity of cutting-edge modeling choices, and highlights the main achievements and limitations of current research efforts.","The survey also includes a comprehensive discussion of available 3D lane detection datasets and the challenges that researchers have faced but have not yet resolved.","Finally, our work outlines future research directions and welcomes researchers and practitioners to enter this exciting field."],"url":"http://arxiv.org/abs/2404.06860v1","category":"cs.CV"}
{"created":"2024-04-10 09:29:12","title":"Number Theory in OSCAR","abstract":"We give a brief introduction to computational algebraic number theory in OSCAR. Our main focus is on number fields, rings of integers and their invariants. After recalling some classical results and their constructive counterparts, we showcase the functionality in two examples related to the investigation of the Cohen-Lenstra heuristic for quadratic fields and the Galois module structure of rings of integers.","sentences":["We give a brief introduction to computational algebraic number theory in OSCAR.","Our main focus is on number fields, rings of integers and their invariants.","After recalling some classical results and their constructive counterparts, we showcase the functionality in two examples related to the investigation of the Cohen-Lenstra heuristic for quadratic fields and the Galois module structure of rings of integers."],"url":"http://arxiv.org/abs/2404.06858v1","category":"math.NT"}
{"created":"2024-04-10 08:52:12","title":"Solving Parametric PDEs with Radial Basis Functions and Deep Neural Networks","abstract":"We propose the POD-DNN, a novel algorithm leveraging deep neural networks (DNNs) along with radial basis functions (RBFs) in the context of the proper orthogonal decomposition (POD) reduced basis method (RBM), aimed at approximating the parametric mapping of parametric partial differential equations on irregular domains. The POD-DNN algorithm capitalizes on the low-dimensional characteristics of the solution manifold for parametric equations, alongside the inherent offline-online computational strategy of RBM and DNNs. In numerical experiments, POD-DNN demonstrates significantly accelerated computation speeds during the online phase. Compared to other algorithms that utilize RBF without integrating DNNs, POD-DNN substantially improves the computational speed in the online inference process. Furthermore, under reasonable assumptions, we have rigorously derived upper bounds on the complexity of approximating parametric mappings with POD-DNN, thereby providing a theoretical analysis of the algorithm's empirical performance.","sentences":["We propose the POD-DNN, a novel algorithm leveraging deep neural networks (DNNs) along with radial basis functions (RBFs) in the context of the proper orthogonal decomposition (POD) reduced basis method (RBM), aimed at approximating the parametric mapping of parametric partial differential equations on irregular domains.","The POD-DNN algorithm capitalizes on the low-dimensional characteristics of the solution manifold for parametric equations, alongside the inherent offline-online computational strategy of RBM and DNNs.","In numerical experiments, POD-DNN demonstrates significantly accelerated computation speeds during the online phase.","Compared to other algorithms that utilize RBF without integrating DNNs, POD-DNN substantially improves the computational speed in the online inference process.","Furthermore, under reasonable assumptions, we have rigorously derived upper bounds on the complexity of approximating parametric mappings with POD-DNN, thereby providing a theoretical analysis of the algorithm's empirical performance."],"url":"http://arxiv.org/abs/2404.06834v1","category":"math.NA"}
{"created":"2024-04-10 08:49:27","title":"Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge","abstract":"Recent studies have highlighted the presence of cultural biases in Large Language Models (LLMs), yet often lack a robust methodology to dissect these phenomena comprehensively. Our work aims to bridge this gap by delving into the Food domain, a universally relevant yet culturally diverse aspect of human life. We introduce FmLAMA, a multilingual dataset centered on food-related cultural facts and variations in food practices. We analyze LLMs across various architectures and configurations, evaluating their performance in both monolingual and multilingual settings. By leveraging templates in six different languages, we investigate how LLMs interact with language-specific and cultural knowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias towards food knowledge prevalent in the United States; (2) Incorporating relevant cultural context significantly improves LLMs' ability to access cultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is highly dependent on the interplay between the probing language, the specific model architecture, and the cultural context in question. This research underscores the complexity of integrating cultural understanding into LLMs and emphasizes the importance of culturally diverse datasets to mitigate biases and enhance model performance across different cultural domains.","sentences":["Recent studies have highlighted the presence of cultural biases in Large Language Models (LLMs), yet often lack a robust methodology to dissect these phenomena comprehensively.","Our work aims to bridge this gap by delving into the Food domain, a universally relevant yet culturally diverse aspect of human life.","We introduce FmLAMA, a multilingual dataset centered on food-related cultural facts and variations in food practices.","We analyze LLMs across various architectures and configurations, evaluating their performance in both monolingual and multilingual settings.","By leveraging templates in six different languages, we investigate how LLMs interact with language-specific and cultural knowledge.","Our findings reveal that (1) LLMs demonstrate a pronounced bias towards food knowledge prevalent in the United States; (2) Incorporating relevant cultural context significantly improves LLMs' ability to access cultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is highly dependent on the interplay between the probing language, the specific model architecture, and the cultural context in question.","This research underscores the complexity of integrating cultural understanding into LLMs and emphasizes the importance of culturally diverse datasets to mitigate biases and enhance model performance across different cultural domains."],"url":"http://arxiv.org/abs/2404.06833v1","category":"cs.CL"}
{"created":"2024-04-10 08:24:53","title":"Challenges of Quantum Software Engineering for the Next Decade: The Road Ahead","abstract":"As quantum computers evolve, so does the complexity of the software that they can run. To make this software efficient, maintainable, reusable, and cost-effective, quality attributes that any industry-grade software should strive for, mature software engineering approaches should be applied during its design, development, and operation. Due to the significant differences between classical and quantum software, applying classical software engineering solutions to quantum software is difficult. This resulted in the birth of Quantum Software Engineering as a discipline in the contemporary software engineering landscape. In this work, a set of active researchers is currently addressing the challenges of Quantum Software Engineering and analyzing the most recent research advances in this domain. This analysis is used to identify needed breakthroughs and future research directions for Quantum Software Engineering.","sentences":["As quantum computers evolve, so does the complexity of the software that they can run.","To make this software efficient, maintainable, reusable, and cost-effective, quality attributes that any industry-grade software should strive for, mature software engineering approaches should be applied during its design, development, and operation.","Due to the significant differences between classical and quantum software, applying classical software engineering solutions to quantum software is difficult.","This resulted in the birth of Quantum Software Engineering as a discipline in the contemporary software engineering landscape.","In this work, a set of active researchers is currently addressing the challenges of Quantum Software Engineering and analyzing the most recent research advances in this domain.","This analysis is used to identify needed breakthroughs and future research directions for Quantum Software Engineering."],"url":"http://arxiv.org/abs/2404.06825v1","category":"cs.SE"}
{"created":"2024-04-10 08:23:05","title":"Error Mitigation for TDoA UWB Indoor Localization using Unsupervised Machine Learning","abstract":"Indoor positioning systems based on Ultra-wideband (UWB) technology are gaining recognition for their ability to provide cm-level localization accuracy. However, these systems often encounter challenges caused by dense multi-path fading, leading to positioning errors. To address this issue, in this letter, we propose a novel methodology for unsupervised anchor node selection using deep embedded clustering (DEC). Our approach uses an Auto Encoder (AE) before clustering, thereby better separating UWB features into separable clusters of UWB input signals. We furthermore investigate how to rank these clusters based on their cluster quality, allowing us to remove untrustworthy signals. Experimental results show the efficiency of our proposed method, demonstrating a significant 23.1% reduction in mean absolute error (MAE) compared to without anchor exclusion. Especially in the dense multi-path area, our algorithm achieves even more significant enhancements, reducing the MAE by 26.6% and the 95th percentile error by 49.3% compared to without anchor exclusion.","sentences":["Indoor positioning systems based on Ultra-wideband (UWB) technology are gaining recognition for their ability to provide cm-level localization accuracy.","However, these systems often encounter challenges caused by dense multi-path fading, leading to positioning errors.","To address this issue, in this letter, we propose a novel methodology for unsupervised anchor node selection using deep embedded clustering (DEC).","Our approach uses an Auto Encoder (AE) before clustering, thereby better separating UWB features into separable clusters of UWB input signals.","We furthermore investigate how to rank these clusters based on their cluster quality, allowing us to remove untrustworthy signals.","Experimental results show the efficiency of our proposed method, demonstrating a significant 23.1% reduction in mean absolute error (MAE) compared to without anchor exclusion.","Especially in the dense multi-path area, our algorithm achieves even more significant enhancements, reducing the MAE by 26.6% and the 95th percentile error by 49.3% compared to without anchor exclusion."],"url":"http://arxiv.org/abs/2404.06824v1","category":"cs.LG"}
{"created":"2024-04-10 08:11:12","title":"Enc2DB: A Hybrid and Adaptive Encrypted Query Processing Framework","abstract":"As cloud computing gains traction, data owners are outsourcing their data to cloud service providers (CSPs) for Database Service (DBaaS), bringing in a deviation of data ownership and usage, and intensifying privacy concerns, especially with potential breaches by hackers or CSP insiders. To address that, encrypted database services propose encrypting every tuple and query statement before submitting to the CSP, ensuring data confidentiality when the CSP is honest-but-curious, or even compromised. Existing solutions either employ property preserving cryptography schemes, which can perform certain operations over ciphertext without decrypting the data over the CSP, or utilize trusted execution environment (TEE) to safeguard data and computations from the CSP. Based on these efforts, we introduce Enc2DB, a novel secure database system, following a hybrid strategy on PostgreSQL and openGauss. We present a micro-benchmarking test and self-adaptive mode switch strategy that can dynamically choose the best execution path (cryptography or TEE) to answer a given query. Besides, we also design and implement a ciphertext index compatible with native cost model and query optimizers to accelerate query processing. Empirical study over TPC-C test justifies that Enc2DB outperforms pure TEE and cryptography solutions, and our ciphertext index implementation also outperforms the state-of-the-art cryptographic-based system.","sentences":["As cloud computing gains traction, data owners are outsourcing their data to cloud service providers (CSPs) for Database Service (DBaaS), bringing in a deviation of data ownership and usage, and intensifying privacy concerns, especially with potential breaches by hackers or CSP insiders.","To address that, encrypted database services propose encrypting every tuple and query statement before submitting to the CSP, ensuring data confidentiality when the CSP is honest-but-curious, or even compromised.","Existing solutions either employ property preserving cryptography schemes, which can perform certain operations over ciphertext without decrypting the data over the CSP, or utilize trusted execution environment (TEE) to safeguard data and computations from the CSP.","Based on these efforts, we introduce Enc2DB, a novel secure database system, following a hybrid strategy on PostgreSQL and openGauss.","We present a micro-benchmarking test and self-adaptive mode switch strategy that can dynamically choose the best execution path (cryptography or TEE) to answer a given query.","Besides, we also design and implement a ciphertext index compatible with native cost model and query optimizers to accelerate query processing.","Empirical study over TPC-C test justifies that Enc2DB outperforms pure TEE and cryptography solutions, and our ciphertext index implementation also outperforms the state-of-the-art cryptographic-based system."],"url":"http://arxiv.org/abs/2404.06819v1","category":"cs.CR"}
{"created":"2024-04-10 08:00:57","title":"Complementarity-constrained predictive control for efficient gas-balanced hybrid power systems","abstract":"Controlling gas turbines (GTs) efficiently is vital as GTs are used to balance power in onshore/offshore hybrid power systems with variable renewable energy and energy storage. However, predictive control of GTs is non-trivial when formulated as a dynamic optimisation problem due to the semi-continuous operating regions of GTs, which must be included to ensure complete combustion and high fuel efficiency. This paper studies two approaches for handling the semi-continuous operating regions of GTs in hybrid power systems through predictive control, dynamic optimisation, and complementarity constraints. The proposed solutions are qualitatively investigated and compared with baseline controllers in a case study involving GTs, offshore wind, and batteries. While one of the baseline controllers considers fuel efficiency, it employs a continuous formulation, which results in lower efficiency than the two proposed approaches as it does not account for the semi-continuous operating regions of each GT.","sentences":["Controlling gas turbines (GTs) efficiently is vital as GTs are used to balance power in onshore/offshore hybrid power systems with variable renewable energy and energy storage.","However, predictive control of GTs is non-trivial when formulated as a dynamic optimisation problem due to the semi-continuous operating regions of GTs, which must be included to ensure complete combustion and high fuel efficiency.","This paper studies two approaches for handling the semi-continuous operating regions of GTs in hybrid power systems through predictive control, dynamic optimisation, and complementarity constraints.","The proposed solutions are qualitatively investigated and compared with baseline controllers in a case study involving GTs, offshore wind, and batteries.","While one of the baseline controllers considers fuel efficiency, it employs a continuous formulation, which results in lower efficiency than the two proposed approaches as it does not account for the semi-continuous operating regions of each GT."],"url":"http://arxiv.org/abs/2404.06813v1","category":"eess.SY"}
{"created":"2024-04-10 07:58:09","title":"Strong stabilization of damped nonlinear Schr{\u00f6}dinger equation with saturation on unbounded domains","abstract":"We consider the damped nonlinear Schr\\''{o}dinger equation with saturation: i.e., the complex evolution equation contains in its left hand side, besides the potential term $V(x)u,$ a nonlinear term of the form $\\mathrm{i}\\mu u(t,x)/|u(t,x)|$ for a given parameter $\\mu >0$ (arising in optical applications on non-Kerr-like fibers). In the right hand side we assume a given forcing term $f(t,x).$ The important new difficulty, in contrast to previous results in the literature, comes from the fact that the spatial domain is assumed to be unbounded. We start by proving the existence and uniqueness of weak and strong solutions according the regularity of the data of the problem. The existence of solutions with a lower regularity is also obtained by working with a sequence of spaces verifying the Radon-Nikod\\'{y}m property. Concerning the asymptotic behavior for large times we prove a strong stabilization result. For instance, in the one dimensional case we prove that there is extinction in finite time of the solutions under the mere assumption that the $L^\\infty$-norm of the forcing term $f(t,x)$ becomes less than $\\mu$ after a finite time. This presents some analogies with the so called feedback \\textit{bang-bang controls} $v$ (here $v=-\\mathrm{i}\\mu u/|u|+f).$","sentences":["We consider the damped nonlinear Schr\\''{o}dinger equation with saturation: i.e., the complex evolution equation contains in its left hand side, besides the potential term $V(x)u,$ a nonlinear term of the form $\\mathrm{i}\\mu u(t,x)/|u(t,x)|$ for a given parameter $\\mu >0$ (arising in optical applications on non-Kerr-like fibers).","In the right hand side we assume a given forcing term $f(t,x).$","The important new difficulty, in contrast to previous results in the literature, comes from the fact that the spatial domain is assumed to be unbounded.","We start by proving the existence and uniqueness of weak and strong solutions according the regularity of the data of the problem.","The existence of solutions with a lower regularity is also obtained by working with a sequence of spaces verifying the Radon-Nikod\\'{y}m property.","Concerning the asymptotic behavior for large times we prove a strong stabilization result.","For instance, in the one dimensional case we prove that there is extinction in finite time of the solutions under the mere assumption that the $L^\\infty$-norm of the forcing term $f(t,x)$ becomes less than $\\mu$ after a finite time.","This presents some analogies with the so called feedback \\textit{bang-bang controls} $v$ (here $v=-\\mathrm{i}\\mu u/|u|+f).$"],"url":"http://arxiv.org/abs/2404.06811v1","category":"math.AP"}
{"created":"2024-04-10 07:52:41","title":"Near-Optimal Channel Estimation for Dense Array Systems","abstract":"By deploying a large number of antennas with sub-half-wavelength spacing in a compact space, dense array systems(DASs) can fully unleash the multiplexing-and-diversity gains of limited apertures. To acquire these gains, accurate channel state information acquisition is necessary but challenging due to the large antenna numbers. To overcome this obstacle, this paper reveals that exploiting the high spatial correlation of DAS channels is crucial while designing the observation matrix for optimal/near-optimal channel estimation. Firstly, we prove that the observation matrix design is equivalent to a time-domain duality of multiple-input multiple-output precoding, which can be ideally addressed by the water-filling principle. For practical realizations, a novel ice-filling algorithm is proposed to design amplitude-and-phase controllable observation matrices, and a majorization-minimization algorithm is proposed to address the phase-only controllable case. Particularly, we prove that the ice-filling algorithm can be viewed as a ``quantized\" water-filling algorithm. To support the sub-optimality of the proposed designs, we provide comprehensive analyses on the achievable mean square errors and their asymptotic expressions. Finally, numerical simulations verify that our proposed channel estimation designs can achieve the near-optimal performance and outperform existing approaches significantly.","sentences":["By deploying a large number of antennas with sub-half-wavelength spacing in a compact space, dense array systems(DASs) can fully unleash the multiplexing-and-diversity gains of limited apertures.","To acquire these gains, accurate channel state information acquisition is necessary but challenging due to the large antenna numbers.","To overcome this obstacle, this paper reveals that exploiting the high spatial correlation of DAS channels is crucial while designing the observation matrix for optimal/near-optimal channel estimation.","Firstly, we prove that the observation matrix design is equivalent to a time-domain duality of multiple-input multiple-output precoding, which can be ideally addressed by the water-filling principle.","For practical realizations, a novel ice-filling algorithm is proposed to design amplitude-and-phase controllable observation matrices, and a majorization-minimization algorithm is proposed to address the phase-only controllable case.","Particularly, we prove that the ice-filling algorithm can be viewed as a ``quantized\" water-filling algorithm.","To support the sub-optimality of the proposed designs, we provide comprehensive analyses on the achievable mean square errors and their asymptotic expressions.","Finally, numerical simulations verify that our proposed channel estimation designs can achieve the near-optimal performance and outperform existing approaches significantly."],"url":"http://arxiv.org/abs/2404.06806v1","category":"cs.IT"}
{"created":"2024-04-10 07:51:49","title":"A note on continuous functions on metric spaces","abstract":"Continuous functions on the unit interval are relatively tame from the logical and computational point of view. A similar behaviour is exhibited by continuous functions on compact metric spaces equipped with a countable dense subset. It is then a natural question what happens if we omit the latter 'extra data', i.e. work with 'unrepresented' compact metric spaces. In this paper, we study basic third-order statements about continuous functions on such unrepresented compact metric spaces in Kohlenbach's higher-order Reverse Mathematics. We establish that some (very specific) statements are classified in the (second-order) Big Five of Reverse Mathematics, while most variations/generalisations are not provable from the latter, and much stronger systems. Thus, continuous functions on unrepresented metric spaces are 'wild', though 'more tame' than (slightly) discontinuous functions on the reals.","sentences":["Continuous functions on the unit interval are relatively tame from the logical and computational point of view.","A similar behaviour is exhibited by continuous functions on compact metric spaces equipped with a countable dense subset.","It is then a natural question what happens if we omit the latter 'extra data', i.e. work with 'unrepresented' compact metric spaces.","In this paper, we study basic third-order statements about continuous functions on such unrepresented compact metric spaces in Kohlenbach's higher-order Reverse Mathematics.","We establish that some (very specific) statements are classified in the (second-order)","Big Five of Reverse Mathematics, while most variations/generalisations are not provable from the latter, and much stronger systems.","Thus, continuous functions on unrepresented metric spaces are 'wild', though 'more tame' than (slightly) discontinuous functions on the reals."],"url":"http://arxiv.org/abs/2404.06805v1","category":"math.LO"}
{"created":"2024-04-10 07:13:15","title":"Certifying the qubit space with a minimal number of parameters","abstract":"We present a precise certification test of the dimension of a qubit system on the public IBM quantum computer, using the determinant dimension witness and with a minimal number of independent parameters. We achieve it by mapping the Bloch sphere $\\pi/2$-rotation axis angle on the nonplanar so-called Viviani curve. During the implementation of the rotation by single qubit gates on IBM devices, we found the majority of qubits passing the test, although some specific qubits failed by more than ten standard deviations. The nature of those deviations has no simple explanation, as the test is robust against common non-idealities.","sentences":["We present a precise certification test of the dimension of a qubit system on the public IBM quantum computer, using the determinant dimension witness and with a minimal number of independent parameters.","We achieve it by mapping the Bloch sphere $\\pi/2$-rotation axis angle on the nonplanar so-called Viviani curve.","During the implementation of the rotation by single qubit gates on IBM devices, we found the majority of qubits passing the test, although some specific qubits failed by more than ten standard deviations.","The nature of those deviations has no simple explanation, as the test is robust against common non-idealities."],"url":"http://arxiv.org/abs/2404.06792v1","category":"quant-ph"}
{"created":"2024-04-10 07:04:21","title":"Modeling of antenna-coupled Si MOSFETs in the Terahertz Frequency Range","abstract":"We report on the modeling and experimental characterization of Si CMOS detectors of terahertz radiation based on antenna-coupled field-effect transistors (TeraFETs). The detectors are manufactured using TSMC's 65-nm technology. We apply two models -- the TSMC RF foundry model and our own ADS-HDM -- to simulate the Si CMOS TeraFET performance and compare their predictions with respective experimental data. Both models are implemented in the commercial circuit simulation software Keysight Advanced Design System (ADS). We find that the compact model TSMC RF is capable to predict the detector responsivity and its dependence on frequency and gate voltage with good accuracy up to the highest frequency of 1.2 THz covered in this study. This frequency is well beyond the tool's intended operation range for 5G communications and 110-GHz millimeter wave applications. We demonstrate that our self-developed physics-based ADS-HDM tool, which relies on an extended one-dimensional hydrodynamic transport model and can be adapted readily to other material technologies, has high predictive qualities comparable to those of the foundry model. We use the ADS-HDM to discuss the contribution of diffusive and plasmonic effects to the THz response of Si CMOS TeraFETs, finding that these effects, while becoming more significant with rising frequency, are never dominant. Finally, we estimate that the electrical NEP (perfect power coupling conditions) is on the order of 5 pW/$\\sqrt{\\rm{Hz}}$ at room-temperature.","sentences":["We report on the modeling and experimental characterization of Si CMOS detectors of terahertz radiation based on antenna-coupled field-effect transistors (TeraFETs).","The detectors are manufactured using TSMC's 65-nm technology.","We apply two models -- the TSMC RF foundry model and our own ADS-HDM -- to simulate the Si CMOS TeraFET performance and compare their predictions with respective experimental data.","Both models are implemented in the commercial circuit simulation software Keysight Advanced Design System (ADS).","We find that the compact model TSMC RF is capable to predict the detector responsivity and its dependence on frequency and gate voltage with good accuracy up to the highest frequency of 1.2 THz covered in this study.","This frequency is well beyond the tool's intended operation range for 5G communications and 110-GHz millimeter wave applications.","We demonstrate that our self-developed physics-based ADS-HDM tool, which relies on an extended one-dimensional hydrodynamic transport model and can be adapted readily to other material technologies, has high predictive qualities comparable to those of the foundry model.","We use the ADS-HDM to discuss the contribution of diffusive and plasmonic effects to the THz response of Si CMOS TeraFETs, finding that these effects, while becoming more significant with rising frequency, are never dominant.","Finally, we estimate that the electrical NEP (perfect power coupling conditions) is on the order of 5 pW/$\\sqrt{\\rm{Hz}}$ at room-temperature."],"url":"http://arxiv.org/abs/2404.06790v2","category":"physics.app-ph"}
{"created":"2024-04-10 06:52:55","title":"Statistical evaluation of 571 GaAs quantum point contact transistors showing the 0.7 anomaly in quantized conductance using millikelvin cryogenic on-chip multiplexing","abstract":"The mass production and the practical number of cryogenic quantum devices producible in a single chip are limited to the number of electrical contact pads and wiring of the cryostat or dilution refrigerator. It is, therefore, beneficial to contrast the measurements of hundreds of devices fabricated in a single chip in one cooldown process to promote the scalability, integrability, reliability, and reproducibility of quantum devices and to save evaluation time, cost and energy. Here, we use a cryogenic on-chip multiplexer architecture and investigate the statistics of the 0.7 anomaly observed on the first three plateaus of the quantized conductance of semiconductor quantum point contact (QPC) transistors. Our single chips contain 256 split gate field effect QPC transistors (QFET) each, with two 16-branch multiplexed source-drain and gate pads, allowing individual transistors to be selected, addressed and controlled through an electrostatic gate voltage process. A total of 1280 quantum transistors with nano-scale dimensions are patterned in 5 different chips of GaAs heterostructures. From the measurements of 571 functioning QPCs taken at temperatures T= 1.4 K and T= 40 mK, it is found that the spontaneous polarisation model and Kondo effect do not fit our results. Furthermore, some of the features in our data largely agreed with van Hove model with short-range interactions. Our approach provides further insight into the quantum mechanical properties and microscopic origin of the 0.7 anomaly in QPCs, paving the way for the development of semiconducting quantum circuits and integrated cryogenic electronics, for scalable quantum logic control, readout, synthesis, and processing applications.","sentences":["The mass production and the practical number of cryogenic quantum devices producible in a single chip are limited to the number of electrical contact pads and wiring of the cryostat or dilution refrigerator.","It is, therefore, beneficial to contrast the measurements of hundreds of devices fabricated in a single chip in one cooldown process to promote the scalability, integrability, reliability, and reproducibility of quantum devices and to save evaluation time, cost and energy.","Here, we use a cryogenic on-chip multiplexer architecture and investigate the statistics of the 0.7 anomaly observed on the first three plateaus of the quantized conductance of semiconductor quantum point contact (QPC) transistors.","Our single chips contain 256 split gate field effect QPC transistors (QFET) each, with two 16-branch multiplexed source-drain and gate pads, allowing individual transistors to be selected, addressed and controlled through an electrostatic gate voltage process.","A total of 1280 quantum transistors with nano-scale dimensions are patterned in 5 different chips of GaAs heterostructures.","From the measurements of 571 functioning QPCs taken at temperatures T= 1.4 K and T= 40 mK, it is found that the spontaneous polarisation model and Kondo effect do not fit our results.","Furthermore, some of the features in our data largely agreed with van Hove model with short-range interactions.","Our approach provides further insight into the quantum mechanical properties and microscopic origin of the 0.7 anomaly in QPCs, paving the way for the development of semiconducting quantum circuits and integrated cryogenic electronics, for scalable quantum logic control, readout, synthesis, and processing applications."],"url":"http://arxiv.org/abs/2404.06784v1","category":"quant-ph"}
{"created":"2024-04-10 06:43:01","title":"Two-Step Iterative GMM Structure for Estimating Mixed Correlation Coefficient Matrix","abstract":"In this article, we propose a new method for calculating the mixed correlation coefficient (Pearson, polyserial and polychoric) matrix and its covariance matrix based on the GMM framework. We build moment equations for each coefficient and align them together, then solve the system with Two-Step IGMM algorithm. Theory and simulation show that this estimation has consistency and asymptotic normality, and its efficiency is asymptotically equivalent to MLE. Moreover, it is much faster and the model setting is more flexible (the equations for each coefficient are blocked designed, you can only include the coefficients of interest instead of the entire correlation matrix), which can be a better initial estimation for structural equation model.","sentences":["In this article, we propose a new method for calculating the mixed correlation coefficient (Pearson, polyserial and polychoric) matrix and its covariance matrix based on the GMM framework.","We build moment equations for each coefficient and align them together, then solve the system with Two-Step IGMM algorithm.","Theory and simulation show that this estimation has consistency and asymptotic normality, and its efficiency is asymptotically equivalent to MLE.","Moreover, it is much faster and the model setting is more flexible (the equations for each coefficient are blocked designed, you can only include the coefficients of interest instead of the entire correlation matrix), which can be a better initial estimation for structural equation model."],"url":"http://arxiv.org/abs/2404.06781v1","category":"stat.CO"}
{"created":"2024-04-10 06:38:55","title":"The high-dimensional Weierstrass functions","abstract":"For a real analytic periodic function $\\phi:\\mathbb{R}\\to\\mathbb{R}^d$, an integer $b \\ge 2$ and $\\lambda\\in(1/b,1)$, we prove that the box dimension and the Hausdorff dimension of the graph of the Weierstrass function $W(x)=\\sum_{n=0}^{\\infty}{{\\lambda}^n\\phi(b^nx)}$ are both equal to $$\\min\\left\\{\\log_{\\lambda^{-1}}b,\\,1+\\left(\\,d-q\\,\\right)\\left(1+\\log_b\\lambda\\right)\\right\\},$$ where $q = q(\\phi, b, \\lambda)$ denotes the maximum dimension of all linear spaces $V < \\mathbb{R}^d$ such that the projection $\\pi_V W$ is Lipschitz.","sentences":["For a real analytic periodic function $\\phi:\\mathbb{R}\\to\\mathbb{R}^d$, an integer $b \\ge 2$ and $\\lambda\\in(1/b,1)$, we prove that the box dimension and the Hausdorff dimension of the graph of the Weierstrass function $W(x)=\\sum_{n=0}^{\\infty}{{\\lambda}^n\\phi(b^nx)}$ are both equal to $$\\min\\left\\{\\log_{\\lambda^{-1}}b,\\,1+\\left(\\,d-q\\,\\right)\\left(1+\\log_b\\lambda\\right)\\right\\},$$ where $q = q(\\phi, b, \\lambda)$ denotes the maximum dimension of all linear spaces $V < \\mathbb{R}^d$ such that the projection $\\pi_V W$ is Lipschitz."],"url":"http://arxiv.org/abs/2404.06778v1","category":"math.CA"}
{"created":"2024-04-10 06:30:08","title":"Adapting LLaMA Decoder to Vision Transformer","abstract":"This work examines whether decoder-only Transformers such as LLaMA, which were originally designed for large language models (LLMs), can be adapted to the computer vision field. We first \"LLaMAfy\" a standard ViT step-by-step to align with LLaMA's architecture, and find that directly applying a casual mask to the self-attention brings an attention collapse issue, resulting in the failure to the network training. We suggest to reposition the class token behind the image tokens with a post-sequence class token technique to overcome this challenge, enabling causal self-attention to efficiently capture the entire image's information. Additionally, we develop a soft mask strategy that gradually introduces a casual mask to the self-attention at the onset of training to facilitate the optimization behavior. The tailored model, dubbed as image LLaMA (iLLaMA), is akin to LLaMA in architecture and enables direct supervised learning. Its causal self-attention boosts computational efficiency and learns complex representation by elevating attention map ranks. iLLaMA rivals the performance with its encoder-only counterparts, achieving 75.1% ImageNet top-1 accuracy with only 5.7M parameters. Scaling the model to ~310M and pre-training on ImageNet-21K further enhances the accuracy to 86.0%. Extensive experiments demonstrate iLLaMA's reliable properties: calibration, shape-texture bias, quantization compatibility, ADE20K segmentation and CIFAR transfer learning. We hope our study can kindle fresh views to visual model design in the wave of LLMs. Pre-trained models and codes are available here.","sentences":["This work examines whether decoder-only Transformers such as LLaMA, which were originally designed for large language models (LLMs), can be adapted to the computer vision field.","We first \"LLaMAfy\" a standard ViT step-by-step to align with LLaMA's architecture, and find that directly applying a casual mask to the self-attention brings an attention collapse issue, resulting in the failure to the network training.","We suggest to reposition the class token behind the image tokens with a post-sequence class token technique to overcome this challenge, enabling causal self-attention to efficiently capture the entire image's information.","Additionally, we develop a soft mask strategy that gradually introduces a casual mask to the self-attention at the onset of training to facilitate the optimization behavior.","The tailored model, dubbed as image LLaMA (iLLaMA), is akin to LLaMA in architecture and enables direct supervised learning.","Its causal self-attention boosts computational efficiency and learns complex representation by elevating attention map ranks.","iLLaMA rivals the performance with its encoder-only counterparts, achieving 75.1% ImageNet top-1 accuracy with only 5.7M parameters.","Scaling the model to ~310M and pre-training on ImageNet-21K further enhances the accuracy to 86.0%.","Extensive experiments demonstrate iLLaMA's reliable properties: calibration, shape-texture bias, quantization compatibility, ADE20K segmentation and CIFAR transfer learning.","We hope our study can kindle fresh views to visual model design in the wave of LLMs.","Pre-trained models and codes are available here."],"url":"http://arxiv.org/abs/2404.06773v1","category":"cs.CV"}
{"created":"2024-04-10 06:03:53","title":"On the rank of the double cohomology of moment-angle complexes","abstract":"In \\cite{LPSS-2023}, the authors construct a cochain complex   $C\\!H^*(\\ZZ_{\\K})$ on the cohomology of a moment-angle complex $\\ZZ_{\\K}$   and call the resulting cohomology the double cohomology,   $H\\!H^*(\\ZZ_{\\K})$. In this paper, we study the change of   rank in double cohomology after gluing an $n$-simplex to   a simplicial complex $\\K$ in certain conditions. As an application,   we give a positive answer to an open problem in \\cite{LPSS-2023}:   For any even integer $r$, there always   exists a simlicial complex $\\K$ such that   $\\opn{rank} H\\!H^*(\\ZZ_{\\K})=r$.","sentences":["In \\cite{LPSS-2023}, the authors construct a cochain complex   $C\\!H^*(\\ZZ_{\\K})$ on the cohomology of a moment-angle complex $\\ZZ_{\\K}$   and call the resulting cohomology the double cohomology,   $H\\!H^*(\\ZZ_{\\K})$. In this paper, we study the change of   rank in double cohomology after gluing an $n$-simplex to   a simplicial complex $\\K$ in certain conditions.","As an application,   we give a positive answer to an open problem in \\cite{LPSS-2023}:   For any even integer $r$, there always   exists a simlicial complex $\\K$ such that   $\\opn{rank} H\\!H^*(\\ZZ_{\\K})=r$."],"url":"http://arxiv.org/abs/2404.06763v1","category":"math.AT"}
{"created":"2024-04-10 05:54:40","title":"Toward Holistic Planning and Control Optimization for Dual-Arm Rearrangement","abstract":"Long-horizon task and motion planning (TAMP) is notoriously difficult to solve, let alone optimally, due to the tight coupling between the interleaved (discrete) task and (continuous) motion planning phases, where each phase on its own is frequently an NP-hard or even PSPACE-hard computational challenge. In this study, we tackle the even more challenging goal of jointly optimizing task and motion plans for a real dual-arm system in which the two arms operate in close vicinity to solve highly constrained tabletop multi-object rearrangement problems. Toward that, we construct a tightly integrated planning and control optimization pipeline, Makespan-Optimized Dual-Arm Planner (MODAP) that combines novel sampling techniques for task planning with state-of-the-art trajectory optimization techniques. Compared to previous state-of-the-art, MODAP produces task and motion plans that better coordinate a dual-arm system, delivering significantly improved execution time improvements while simultaneously ensuring that the resulting time-parameterized trajectory conforms to specified acceleration and jerk limits.","sentences":["Long-horizon task and motion planning (TAMP) is notoriously difficult to solve, let alone optimally, due to the tight coupling between the interleaved (discrete) task and (continuous) motion planning phases, where each phase on its own is frequently an NP-hard or even PSPACE-hard computational challenge.","In this study, we tackle the even more challenging goal of jointly optimizing task and motion plans for a real dual-arm system in which the two arms operate in close vicinity to solve highly constrained tabletop multi-object rearrangement problems.","Toward that, we construct a tightly integrated planning and control optimization pipeline, Makespan-Optimized Dual-Arm Planner (MODAP) that combines novel sampling techniques for task planning with state-of-the-art trajectory optimization techniques.","Compared to previous state-of-the-art, MODAP produces task and motion plans that better coordinate a dual-arm system, delivering significantly improved execution time improvements while simultaneously ensuring that the resulting time-parameterized trajectory conforms to specified acceleration and jerk limits."],"url":"http://arxiv.org/abs/2404.06758v1","category":"cs.RO"}
{"created":"2024-04-10 05:38:59","title":"Synchronization Conditions for Nonlinear Oscillator Networks","abstract":"Understanding conditions for the synchronization of a network of interconnected oscillators is a challenging problem. Typically, only sufficient conditions are reported for the synchronization problem. Here, we adopted the Lyapunov-Floquet theory and the Master Stability Function approach in order to derive the synchronization conditions for a set of coupled nonlinear oscillators. We found that the positivity of the coupling constant is a necessary and sufficient condition for synchronizing linearly full-state coupled identical oscillators. Moreover, in the case of partial state coupling, the asymptotic convergence of volume in state space is ensured by a positive coupling constant. The numerical calculation of the Master Stability Function for a benchmark two-dimensional oscillator validates the synchronization corresponding to the positive coupling. The results are illustrated using numerical simulations and experimentation on benchmark oscillators.","sentences":["Understanding conditions for the synchronization of a network of interconnected oscillators is a challenging problem.","Typically, only sufficient conditions are reported for the synchronization problem.","Here, we adopted the Lyapunov-Floquet theory and the Master Stability Function approach in order to derive the synchronization conditions for a set of coupled nonlinear oscillators.","We found that the positivity of the coupling constant is a necessary and sufficient condition for synchronizing linearly full-state coupled identical oscillators.","Moreover, in the case of partial state coupling, the asymptotic convergence of volume in state space is ensured by a positive coupling constant.","The numerical calculation of the Master Stability Function for a benchmark two-dimensional oscillator validates the synchronization corresponding to the positive coupling.","The results are illustrated using numerical simulations and experimentation on benchmark oscillators."],"url":"http://arxiv.org/abs/2404.06752v1","category":"eess.SY"}
{"created":"2024-04-10 05:32:03","title":"CGNSDE: Conditional Gaussian Neural Stochastic Differential Equation for Modeling Complex Systems and Data Assimilation","abstract":"A new knowledge-based and machine learning hybrid modeling approach, called conditional Gaussian neural stochastic differential equation (CGNSDE), is developed to facilitate modeling complex dynamical systems and implementing analytic formulae of the associated data assimilation (DA). In contrast to the standard neural network predictive models, the CGNSDE is designed to effectively tackle both forward prediction tasks and inverse state estimation problems. The CGNSDE starts by exploiting a systematic causal inference via information theory to build a simple knowledge-based nonlinear model that nevertheless captures as much explainable physics as possible. Then, neural networks are supplemented to the knowledge-based model in a specific way, which not only characterizes the remaining features that are challenging to model with simple forms but also advances the use of analytic formulae to efficiently compute the nonlinear DA solution. These analytic formulae are used as an additional computationally affordable loss to train the neural networks that directly improve the DA accuracy. This DA loss function promotes the CGNSDE to capture the interactions between state variables and thus advances its modeling skills. With the DA loss, the CGNSDE is more capable of estimating extreme events and quantifying the associated uncertainty. Furthermore, crucial physical properties in many complex systems, such as the translate-invariant local dependence of state variables, can significantly simplify the neural network structures and facilitate the CGNSDE to be applied to high-dimensional systems. Numerical experiments based on chaotic systems with intermittency and strong non-Gaussian features indicate that the CGNSDE outperforms knowledge-based regression models, and the DA loss further enhances the modeling skills of the CGNSDE.","sentences":["A new knowledge-based and machine learning hybrid modeling approach, called conditional Gaussian neural stochastic differential equation (CGNSDE), is developed to facilitate modeling complex dynamical systems and implementing analytic formulae of the associated data assimilation (DA).","In contrast to the standard neural network predictive models, the CGNSDE is designed to effectively tackle both forward prediction tasks and inverse state estimation problems.","The CGNSDE starts by exploiting a systematic causal inference via information theory to build a simple knowledge-based nonlinear model that nevertheless captures as much explainable physics as possible.","Then, neural networks are supplemented to the knowledge-based model in a specific way, which not only characterizes the remaining features that are challenging to model with simple forms but also advances the use of analytic formulae to efficiently compute the nonlinear DA solution.","These analytic formulae are used as an additional computationally affordable loss to train the neural networks that directly improve the DA accuracy.","This DA loss function promotes the CGNSDE to capture the interactions between state variables and thus advances its modeling skills.","With the DA loss, the CGNSDE is more capable of estimating extreme events and quantifying the associated uncertainty.","Furthermore, crucial physical properties in many complex systems, such as the translate-invariant local dependence of state variables, can significantly simplify the neural network structures and facilitate the CGNSDE to be applied to high-dimensional systems.","Numerical experiments based on chaotic systems with intermittency and strong non-Gaussian features indicate that the CGNSDE outperforms knowledge-based regression models, and the DA loss further enhances the modeling skills of the CGNSDE."],"url":"http://arxiv.org/abs/2404.06749v1","category":"cs.LG"}
{"created":"2024-04-10 05:13:45","title":"Data-driven parallel Koopman subsystem modeling and distributed moving horizon state estimation for large-scale nonlinear processes","abstract":"In this work, we consider a state estimation problem for large-scale nonlinear processes in the absence of first-principles process models. By exploiting process operation data, both process modeling and state estimation design are addressed within a distributed framework. By leveraging the Koopman operator concept, a parallel subsystem modeling approach is proposed to establish interactive linear subsystem process models in higher-dimensional subspaces, each of which correlates with the original nonlinear subspace of the corresponding process subsystem via a nonlinear mapping. The data-driven linear subsystem models can be used to collaboratively characterize and predict the dynamical behaviors of the entire nonlinear process. Based on the established subsystem models, local state estimators that can explicitly handle process operation constraints are designed using moving horizon estimation. The local estimators are integrated via information exchange to form a distributed estimation scheme, which provides estimates of the unmeasured/unmeasurable state variables of the original nonlinear process in a linear manner. The proposed framework is applied to a chemical process and an agro-hydrological process to illustrate its effectiveness and applicability. Good open-loop predictability of the linear subsystem models is confirmed, and accurate estimates of the process states are obtained without requiring a first-principles process model.","sentences":["In this work, we consider a state estimation problem for large-scale nonlinear processes in the absence of first-principles process models.","By exploiting process operation data, both process modeling and state estimation design are addressed within a distributed framework.","By leveraging the Koopman operator concept, a parallel subsystem modeling approach is proposed to establish interactive linear subsystem process models in higher-dimensional subspaces, each of which correlates with the original nonlinear subspace of the corresponding process subsystem via a nonlinear mapping.","The data-driven linear subsystem models can be used to collaboratively characterize and predict the dynamical behaviors of the entire nonlinear process.","Based on the established subsystem models, local state estimators that can explicitly handle process operation constraints are designed using moving horizon estimation.","The local estimators are integrated via information exchange to form a distributed estimation scheme, which provides estimates of the unmeasured/unmeasurable state variables of the original nonlinear process in a linear manner.","The proposed framework is applied to a chemical process and an agro-hydrological process to illustrate its effectiveness and applicability.","Good open-loop predictability of the linear subsystem models is confirmed, and accurate estimates of the process states are obtained without requiring a first-principles process model."],"url":"http://arxiv.org/abs/2404.06746v1","category":"eess.SY"}
{"created":"2024-04-10 04:28:50","title":"SoK: Trusting Self-Sovereign Identity","abstract":"Digital identity is evolving from centralized systems to a decentralized approach known as Self-Sovereign Identity (SSI). SSI empowers individuals to control their digital identities, eliminating reliance on third-party data custodians and reducing the risk of data breaches. However, the concept of trust in SSI remains complex and fragmented. This paper systematically analyzes trust in SSI in light of its components and threats posed by various actors in the system. As a result, we derive three distinct trust models that capture the threats and mitigations identified across SSI literature and implementations. Our work provides a foundational framework for future SSI research and development, including a comprehensive catalogue of SSI components and design requirements for trust, shortcomings in existing SSI systems and areas for further exploration.","sentences":["Digital identity is evolving from centralized systems to a decentralized approach known as Self-Sovereign Identity (SSI).","SSI empowers individuals to control their digital identities, eliminating reliance on third-party data custodians and reducing the risk of data breaches.","However, the concept of trust in SSI remains complex and fragmented.","This paper systematically analyzes trust in SSI in light of its components and threats posed by various actors in the system.","As a result, we derive three distinct trust models that capture the threats and mitigations identified across SSI literature and implementations.","Our work provides a foundational framework for future SSI research and development, including a comprehensive catalogue of SSI components and design requirements for trust, shortcomings in existing SSI systems and areas for further exploration."],"url":"http://arxiv.org/abs/2404.06729v1","category":"cs.CR"}
{"created":"2024-04-10 04:24:09","title":"UAV-Assisted Enhanced Coverage and Capacity in Dynamic MU-mMIMO IoT Systems: A Deep Reinforcement Learning Approach","abstract":"This study focuses on a multi-user massive multiple-input multiple-output (MU-mMIMO) system by incorporating an unmanned aerial vehicle (UAV) as a decode-and-forward (DF) relay between the base station (BS) and multiple Internet-of-Things (IoT) devices. Our primary objective is to maximize the overall achievable rate (AR) by introducing a novel framework that integrates joint hybrid beamforming (HBF) and UAV localization in dynamic MU-mMIMO IoT systems. Particularly, HBF stages for BS and UAV are designed by leveraging slow time-varying angular information, whereas a deep reinforcement learning (RL) algorithm, namely deep deterministic policy gradient (DDPG) with continuous action space, is developed to train the UAV for its deployment. By using a customized reward function, the RL agent learns an optimal UAV deployment policy capable of adapting to both static and dynamic environments. The illustrative results show that the proposed DDPG-based UAV deployment (DDPG-UD) can achieve approximately 99.5% of the sum-rate capacity achieved by particle swarm optimization (PSO)-based UAV deployment (PSO-UD), while requiring a significantly reduced runtime at approximately 68.50% of that needed by PSO-UD, offering an efficient solution in dynamic MU-mMIMO environments.","sentences":["This study focuses on a multi-user massive multiple-input multiple-output (MU-mMIMO) system by incorporating an unmanned aerial vehicle (UAV) as a decode-and-forward (DF) relay between the base station (BS) and multiple Internet-of-Things (IoT) devices.","Our primary objective is to maximize the overall achievable rate (AR) by introducing a novel framework that integrates joint hybrid beamforming (HBF) and UAV localization in dynamic MU-mMIMO IoT systems.","Particularly, HBF stages for BS and UAV are designed by leveraging slow time-varying angular information, whereas a deep reinforcement learning (RL) algorithm, namely deep deterministic policy gradient (DDPG) with continuous action space, is developed to train the UAV for its deployment.","By using a customized reward function, the RL agent learns an optimal UAV deployment policy capable of adapting to both static and dynamic environments.","The illustrative results show that the proposed DDPG-based UAV deployment (DDPG-UD) can achieve approximately 99.5% of the sum-rate capacity achieved by particle swarm optimization (PSO)-based UAV deployment (PSO-UD), while requiring a significantly reduced runtime at approximately 68.50% of that needed by PSO-UD, offering an efficient solution in dynamic MU-mMIMO environments."],"url":"http://arxiv.org/abs/2404.06726v1","category":"eess.SP"}
{"created":"2024-04-10 04:19:59","title":"Global Contrastive Training for Multimodal Electronic Health Records with Language Supervision","abstract":"Modern electronic health records (EHRs) hold immense promise in tracking personalized patient health trajectories through sequential deep learning, owing to their extensive breadth, scale, and temporal granularity. Nonetheless, how to effectively leverage multiple modalities from EHRs poses significant challenges, given its complex characteristics such as high dimensionality, multimodality, sparsity, varied recording frequencies, and temporal irregularities. To this end, this paper introduces a novel multimodal contrastive learning framework, specifically focusing on medical time series and clinical notes. To tackle the challenge of sparsity and irregular time intervals in medical time series, the framework integrates temporal cross-attention transformers with a dynamic embedding and tokenization scheme for learning multimodal feature representations. To harness the interconnected relationships between medical time series and clinical notes, the framework equips a global contrastive loss, aligning a patient's multimodal feature representations with the corresponding discharge summaries. Since discharge summaries uniquely pertain to individual patients and represent a holistic view of the patient's hospital stay, machine learning models are led to learn discriminative multimodal features via global contrasting. Extensive experiments with a real-world EHR dataset demonstrated that our framework outperformed state-of-the-art approaches on the exemplar task of predicting the occurrence of nine postoperative complications for more than 120,000 major inpatient surgeries using multimodal data from UF health system split among three hospitals (UF Health Gainesville, UF Health Jacksonville, and UF Health Jacksonville-North).","sentences":["Modern electronic health records (EHRs) hold immense promise in tracking personalized patient health trajectories through sequential deep learning, owing to their extensive breadth, scale, and temporal granularity.","Nonetheless, how to effectively leverage multiple modalities from EHRs poses significant challenges, given its complex characteristics such as high dimensionality, multimodality, sparsity, varied recording frequencies, and temporal irregularities.","To this end, this paper introduces a novel multimodal contrastive learning framework, specifically focusing on medical time series and clinical notes.","To tackle the challenge of sparsity and irregular time intervals in medical time series, the framework integrates temporal cross-attention transformers with a dynamic embedding and tokenization scheme for learning multimodal feature representations.","To harness the interconnected relationships between medical time series and clinical notes, the framework equips a global contrastive loss, aligning a patient's multimodal feature representations with the corresponding discharge summaries.","Since discharge summaries uniquely pertain to individual patients and represent a holistic view of the patient's hospital stay, machine learning models are led to learn discriminative multimodal features via global contrasting.","Extensive experiments with a real-world EHR dataset demonstrated that our framework outperformed state-of-the-art approaches on the exemplar task of predicting the occurrence of nine postoperative complications for more than 120,000 major inpatient surgeries using multimodal data from UF health system split among three hospitals (UF Health Gainesville, UF Health Jacksonville, and UF Health Jacksonville-North)."],"url":"http://arxiv.org/abs/2404.06723v1","category":"cs.LG"}
{"created":"2024-04-10 04:15:50","title":"Gradient Descent is Pareto-Optimal in the Oracle Complexity and Memory Tradeoff for Feasibility Problems","abstract":"In this paper we provide oracle complexity lower bounds for finding a point in a given set using a memory-constrained algorithm that has access to a separation oracle. We assume that the set is contained within the unit $d$-dimensional ball and contains a ball of known radius $\\epsilon>0$. This setup is commonly referred to as the feasibility problem. We show that to solve feasibility problems with accuracy $\\epsilon \\geq e^{-d^{o(1)}}$, any deterministic algorithm either uses $d^{1+\\delta}$ bits of memory or must make at least $1/(d^{0.01\\delta }\\epsilon^{2\\frac{1-\\delta}{1+1.01 \\delta}-o(1)})$ oracle queries, for any $\\delta\\in[0,1]$. Additionally, we show that randomized algorithms either use $d^{1+\\delta}$ memory or make at least $1/(d^{2\\delta} \\epsilon^{2(1-4\\delta)-o(1)})$ queries for any $\\delta\\in[0,\\frac{1}{4}]$. Because gradient descent only uses linear memory $\\mathcal O(d\\ln 1/\\epsilon)$ but makes $\\Omega(1/\\epsilon^2)$ queries, our results imply that it is Pareto-optimal in the oracle complexity/memory tradeoff. Further, our results show that the oracle complexity for deterministic algorithms is always polynomial in $1/\\epsilon$ if the algorithm has less than quadratic memory in $d$. This reveals a sharp phase transition since with quadratic $\\mathcal O(d^2 \\ln1/\\epsilon)$ memory, cutting plane methods only require $\\mathcal O(d\\ln 1/\\epsilon)$ queries.","sentences":["In this paper we provide oracle complexity lower bounds for finding a point in a given set using a memory-constrained algorithm that has access to a separation oracle.","We assume that the set is contained within the unit $d$-dimensional ball and contains a ball of known radius $\\epsilon>0$. This setup is commonly referred to as the feasibility problem.","We show that to solve feasibility problems with accuracy $\\epsilon \\geq e^{-d^{o(1)}}$, any deterministic algorithm either uses $d^{1+\\delta}$ bits of memory or must make at least $1/(d^{0.01\\delta }\\epsilon^{2\\frac{1-\\delta}{1+1.01 \\delta}-o(1)})$ oracle queries, for any $\\delta\\in[0,1]$. Additionally, we show that randomized algorithms either use $d^{1+\\delta}$ memory or make at least $1/(d^{2\\delta} \\epsilon^{2(1-4\\delta)-o(1)})$ queries for any $\\delta\\in[0,\\frac{1}{4}]$. Because gradient descent only uses linear memory $\\mathcal O(d\\ln 1/\\epsilon)$ but makes $\\Omega(1/\\epsilon^2)$ queries, our results imply that it is Pareto-optimal in the oracle complexity/memory tradeoff.","Further, our results show that the oracle complexity for deterministic algorithms is always polynomial in $1/\\epsilon$ if the algorithm has less than quadratic memory in $d$. This reveals a sharp phase transition since with quadratic $\\mathcal O(d^2 \\ln1/\\epsilon)$ memory, cutting plane methods only require $\\mathcal O(d\\ln 1/\\epsilon)$ queries."],"url":"http://arxiv.org/abs/2404.06720v1","category":"math.OC"}
{"created":"2024-04-10 03:56:29","title":"Disentanglement of mixed interference fringes in optical interferometers: theory and applications","abstract":"Optical interferometric imaging enables astronomical observation at extremely high angular resolution. The necessary optical information for imaging, such as the optical path differences and visibilities, is easy to extract from fringes generated by the combination of two beams. With more than two apertures, the image-plane interference pattern becomes an increasingly indistinguishable mixture of fringe spacings and directions. For decades, the state-of-the-art approaches for obtaining two-aperture fringes from an interferometer array composed of many apertures are limited to pairwise combinations using bulk optics. Here, we derive and demonstrate a fringe disentanglement theory that can digitally transform the interference pattern of N apertures to N(N-1)/2 pairwise fringes without any optics, thus providing straightforward methods of information acquisition for interferometers. We demonstrate applications of our technique by both simulation and experiment, showing that this theory can be used for simultaneously sensing pistons and determining the individual visibilities of all combining apertures. Furthermore, we use the proposed theory to phase a 1.5-meter segmented flat telescope, demonstrating its validity for engineering implementation. This theory may not only benefit optical imaging but also interferometry-based measurements, by providing an exceptional capability to simplify the interferometric output generated by a system of many apertures.","sentences":["Optical interferometric imaging enables astronomical observation at extremely high angular resolution.","The necessary optical information for imaging, such as the optical path differences and visibilities, is easy to extract from fringes generated by the combination of two beams.","With more than two apertures, the image-plane interference pattern becomes an increasingly indistinguishable mixture of fringe spacings and directions.","For decades, the state-of-the-art approaches for obtaining two-aperture fringes from an interferometer array composed of many apertures are limited to pairwise combinations using bulk optics.","Here, we derive and demonstrate a fringe disentanglement theory that can digitally transform the interference pattern of N apertures to N(N-1)/2 pairwise fringes without any optics, thus providing straightforward methods of information acquisition for interferometers.","We demonstrate applications of our technique by both simulation and experiment, showing that this theory can be used for simultaneously sensing pistons and determining the individual visibilities of all combining apertures.","Furthermore, we use the proposed theory to phase a 1.5-meter segmented flat telescope, demonstrating its validity for engineering implementation.","This theory may not only benefit optical imaging but also interferometry-based measurements, by providing an exceptional capability to simplify the interferometric output generated by a system of many apertures."],"url":"http://arxiv.org/abs/2404.06716v1","category":"physics.optics"}
{"created":"2024-04-10 03:30:01","title":"CQIL: Inference Latency Optimization with Concurrent Computation of Quasi-Independent Layers","abstract":"The fast-growing large scale language models are delivering unprecedented performance on almost all natural language processing tasks. However, the effectiveness of large language models are reliant on an exponentially increasing number of parameters. The overwhelming computation complexity incurs a high inference latency that negatively affects user experience. Existing methods to improve inference efficiency, such as tensor parallelism and quantization, target to reduce per-layer computing latency, yet overlook the cumulative latency due to the number of layers. Recent works on reducing the cumulative latency through layer removing, however, lead to significant performance drop. Motivated by the similarity of inputs among adjacent layers, we propose to identify quasi-independent layers, which can be concurrently computed to significantly decrease inference latency. We also introduce a bypassing technique to mitigate the effect of information loss. Empirical experiments of the proposed approach on the LLaMA models confirm that Concurrent Computation of Quasi-Independent Layers (CQIL) can reduce latency by up to 48.3% on the LLaMA-33B model, while maintaining a close level of performance.","sentences":["The fast-growing large scale language models are delivering unprecedented performance on almost all natural language processing tasks.","However, the effectiveness of large language models are reliant on an exponentially increasing number of parameters.","The overwhelming computation complexity incurs a high inference latency that negatively affects user experience.","Existing methods to improve inference efficiency, such as tensor parallelism and quantization, target to reduce per-layer computing latency, yet overlook the cumulative latency due to the number of layers.","Recent works on reducing the cumulative latency through layer removing, however, lead to significant performance drop.","Motivated by the similarity of inputs among adjacent layers, we propose to identify quasi-independent layers, which can be concurrently computed to significantly decrease inference latency.","We also introduce a bypassing technique to mitigate the effect of information loss.","Empirical experiments of the proposed approach on the LLaMA models confirm that Concurrent Computation of Quasi-Independent Layers (CQIL) can reduce latency by up to 48.3% on the LLaMA-33B model, while maintaining a close level of performance."],"url":"http://arxiv.org/abs/2404.06709v1","category":"cs.CL"}
{"created":"2024-04-10 03:26:25","title":"Wigner function and intensity moments of spatio-temporal light fields","abstract":"The Wigner distribution function and the its spatial-angular moments (intensity-moments) are known as efficient instruments for characterization of complex quasimonochromatic light beams and their transformations. In this paper, the generalization of the WF-based approach to spatio-temporal (ST) light fields (wave packets, short pulses) is considered. It is shown that the ST intensity moments are related with the important characteristics of the wave-packet structure, especially, with the transverse orbital angular momentum (OAM) being a specific feature of the ST optical vortices (STOV). The ST moments' transformations in a paraxial optical system obey simple and unified rules involving the ray-transfer ABCD-matrix of the system. On this base, and with simple examples of the OAM-carrying optical pulses, the schemes and mechanisms of the STOV generation and transformation are presented. Examples of non-vortex ST wave packets with the transverse OAM and their possible realizations are discussed. The regular and unified formalism, developed in this paper, can be generalized and applied to more complex situations where the ST field propagates through inhomogeneous and random (scattering) media.","sentences":["The Wigner distribution function and the its spatial-angular moments (intensity-moments) are known as efficient instruments for characterization of complex quasimonochromatic light beams and their transformations.","In this paper, the generalization of the WF-based approach to spatio-temporal (ST) light fields (wave packets, short pulses) is considered.","It is shown that the ST intensity moments are related with the important characteristics of the wave-packet structure, especially, with the transverse orbital angular momentum (OAM) being a specific feature of the ST optical vortices (STOV).","The ST moments' transformations in a paraxial optical system obey simple and unified rules involving the ray-transfer ABCD-matrix of the system.","On this base, and with simple examples of the OAM-carrying optical pulses, the schemes and mechanisms of the STOV generation and transformation are presented.","Examples of non-vortex ST wave packets with the transverse OAM and their possible realizations are discussed.","The regular and unified formalism, developed in this paper, can be generalized and applied to more complex situations where the ST field propagates through inhomogeneous and random (scattering) media."],"url":"http://arxiv.org/abs/2404.06708v1","category":"physics.optics"}
{"created":"2024-04-10 03:24:49","title":"On activation in solid ionic electrolytes","abstract":"Ionic conductivity in solid electrolytes is commonly expected to exhibit Arrhenius dependence on temperature, determined by a well-defined activation energy. Consequently, a standard approach involves calculating this energy using quasi-static methods and using the Arrhenius form to extrapolate the numerical results from one temperature range to another. Despite the ubiquity of this Arrhenius-based modeling, disagreements frequently arise between theory and experiment, and even between different theoretical studies. By considering a tractable minimal model, we elucidate the reason behind the breakdown of the Arrhenius conductivity form. This breakdown is driven by non-trivial phase-space boundaries between conducting and non-conducting regimes, and depends on the kinetic properties of the system.","sentences":["Ionic conductivity in solid electrolytes is commonly expected to exhibit Arrhenius dependence on temperature, determined by a well-defined activation energy.","Consequently, a standard approach involves calculating this energy using quasi-static methods and using the Arrhenius form to extrapolate the numerical results from one temperature range to another.","Despite the ubiquity of this Arrhenius-based modeling, disagreements frequently arise between theory and experiment, and even between different theoretical studies.","By considering a tractable minimal model, we elucidate the reason behind the breakdown of the Arrhenius conductivity form.","This breakdown is driven by non-trivial phase-space boundaries between conducting and non-conducting regimes, and depends on the kinetic properties of the system."],"url":"http://arxiv.org/abs/2404.06707v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-10 03:22:42","title":"Iterative distributed moving horizon estimation of linear systems with penalties on both system disturbances and noise","abstract":"In this paper, partition-based distributed state estimation of general linear systems is considered. A distributed moving horizon state estimation scheme is developed via decomposing the entire system model into subsystem models and partitioning the global objective function of centralized moving horizon estimation (MHE) into local objective functions. The subsystem estimators of the distributed scheme that are required to be executed iteratively within each sampling period are designed based on MHE. Two distributed MHE algorithms are proposed to handle the unconstrained case and the case when hard constraints on states and disturbances, respectively. Sufficient conditions on the convergence of the estimates and the stability of the estimation error dynamics for the entire system are derived for both cases. A benchmark reactor-separator process example is introduced to illustrate the proposed distributed state estimation approach.","sentences":["In this paper, partition-based distributed state estimation of general linear systems is considered.","A distributed moving horizon state estimation scheme is developed via decomposing the entire system model into subsystem models and partitioning the global objective function of centralized moving horizon estimation (MHE) into local objective functions.","The subsystem estimators of the distributed scheme that are required to be executed iteratively within each sampling period are designed based on MHE.","Two distributed MHE algorithms are proposed to handle the unconstrained case and the case when hard constraints on states and disturbances, respectively.","Sufficient conditions on the convergence of the estimates and the stability of the estimation error dynamics for the entire system are derived for both cases.","A benchmark reactor-separator process example is introduced to illustrate the proposed distributed state estimation approach."],"url":"http://arxiv.org/abs/2404.06706v1","category":"eess.SY"}
{"created":"2024-04-10 03:19:03","title":"What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions","abstract":"There is increasing interest in the use of the LEArnable Front-end (LEAF) in a variety of speech processing systems. However, there is a dearth of analyses of what is actually learnt and the relative importance of training the different components of the front-end. In this paper, we investigate this question on keyword spotting, speech-based emotion recognition and language identification tasks and find that the filters for spectral decomposition and the low pass filter used to estimate spectral energy variations exhibit no learning and the per-channel energy normalisation (PCEN) is the key component that is learnt. Following this, we explore the potential of adapting only the PCEN layer with a small amount of noisy data to enable it to learn appropriate dynamic range compression that better suits the noise conditions. This in turn enables a system trained on clean speech to work more accurately on noisy test data as demonstrated by the experimental results reported in this paper.","sentences":["There is increasing interest in the use of the LEArnable Front-end (LEAF) in a variety of speech processing systems.","However, there is a dearth of analyses of what is actually learnt and the relative importance of training the different components of the front-end.","In this paper, we investigate this question on keyword spotting, speech-based emotion recognition and language identification tasks and find that the filters for spectral decomposition and the low pass filter used to estimate spectral energy variations exhibit no learning and the per-channel energy normalisation (PCEN) is the key component that is learnt.","Following this, we explore the potential of adapting only the PCEN layer with a small amount of noisy data to enable it to learn appropriate dynamic range compression that better suits the noise conditions.","This in turn enables a system trained on clean speech to work more accurately on noisy test data as demonstrated by the experimental results reported in this paper."],"url":"http://arxiv.org/abs/2404.06702v1","category":"eess.AS"}
{"created":"2024-04-10 03:16:00","title":"Covariance Regression with High-Dimensional Predictors","abstract":"In the high-dimensional landscape, addressing the challenges of covariance regression with high-dimensional covariates has posed difficulties for conventional methodologies. This paper addresses these hurdles by presenting a novel approach for high-dimensional inference with covariance matrix outcomes. The proposed methodology is illustrated through its application in elucidating brain coactivation patterns observed in functional magnetic resonance imaging (fMRI) experiments and unraveling complex associations within anatomical connections between brain regions identified through diffusion tensor imaging (DTI). In the pursuit of dependable statistical inference, we introduce an integrative approach based on penalized estimation. This approach combines data splitting, variable selection, aggregation of low-dimensional estimators, and robust variance estimation. It enables the construction of reliable confidence intervals for covariate coefficients, supported by theoretical confidence levels under specified conditions, where asymptotic distributions are provided. Through various types of simulation studies, the proposed approach performs well for covariance regression in the presence of high-dimensional covariates. This innovative approach is applied to the Lifespan Human Connectome Project (HCP) Aging Study, which aims to uncover a typical aging trajectory and variations in the brain connectome among mature and older adults. The proposed approach effectively identifies brain networks and associated predictors of white matter integrity, aligning with established knowledge of the human brain.","sentences":["In the high-dimensional landscape, addressing the challenges of covariance regression with high-dimensional covariates has posed difficulties for conventional methodologies.","This paper addresses these hurdles by presenting a novel approach for high-dimensional inference with covariance matrix outcomes.","The proposed methodology is illustrated through its application in elucidating brain coactivation patterns observed in functional magnetic resonance imaging (fMRI) experiments and unraveling complex associations within anatomical connections between brain regions identified through diffusion tensor imaging (DTI).","In the pursuit of dependable statistical inference, we introduce an integrative approach based on penalized estimation.","This approach combines data splitting, variable selection, aggregation of low-dimensional estimators, and robust variance estimation.","It enables the construction of reliable confidence intervals for covariate coefficients, supported by theoretical confidence levels under specified conditions, where asymptotic distributions are provided.","Through various types of simulation studies, the proposed approach performs well for covariance regression in the presence of high-dimensional covariates.","This innovative approach is applied to the Lifespan Human Connectome Project (HCP) Aging Study, which aims to uncover a typical aging trajectory and variations in the brain connectome among mature and older adults.","The proposed approach effectively identifies brain networks and associated predictors of white matter integrity, aligning with established knowledge of the human brain."],"url":"http://arxiv.org/abs/2404.06701v1","category":"stat.ME"}
{"created":"2024-04-10 17:27:54","title":"Scaling Laws for Data Filtering -- Data Curation cannot be Compute Agnostic","abstract":"Vision-language models (VLMs) are trained for thousands of GPU hours on carefully curated web datasets. In recent times, data curation has gained prominence with several works developing strategies to retain 'high-quality' subsets of 'raw' scraped data. For instance, the LAION public dataset retained only 10% of the total crawled data. However, these strategies are typically developed agnostic of the available compute for training. In this paper, we first demonstrate that making filtering decisions independent of training compute is often suboptimal: the limited high-quality data rapidly loses its utility when repeated, eventually requiring the inclusion of 'unseen' but 'lower-quality' data. To address this quality-quantity tradeoff ($\\texttt{QQT}$), we introduce neural scaling laws that account for the non-homogeneous nature of web data, an angle ignored in existing literature. Our scaling laws (i) characterize the $\\textit{differing}$ 'utility' of various quality subsets of web data; (ii) account for how utility diminishes for a data point at its 'nth' repetition; and (iii) formulate the mutual interaction of various data pools when combined, enabling the estimation of model performance on a combination of multiple data pools without ever jointly training on them. Our key message is that data curation $\\textit{cannot}$ be agnostic of the total compute that a model will be trained for. Our scaling laws allow us to curate the best possible pool for achieving top performance on Datacomp at various compute budgets, carving out a pareto-frontier for data curation. Code is available at https://github.com/locuslab/scaling_laws_data_filtering.","sentences":["Vision-language models (VLMs) are trained for thousands of GPU hours on carefully curated web datasets.","In recent times, data curation has gained prominence with several works developing strategies to retain 'high-quality' subsets of 'raw' scraped data.","For instance, the LAION public dataset retained only 10% of the total crawled data.","However, these strategies are typically developed agnostic of the available compute for training.","In this paper, we first demonstrate that making filtering decisions independent of training compute is often suboptimal: the limited high-quality data rapidly loses its utility when repeated, eventually requiring the inclusion of 'unseen' but 'lower-quality' data.","To address this quality-quantity tradeoff ($\\texttt{QQT}$), we introduce neural scaling laws that account for the non-homogeneous nature of web data, an angle ignored in existing literature.","Our scaling laws (i) characterize the $\\textit{differing}$ 'utility' of various quality subsets of web data; (ii) account for how utility diminishes for a data point at its 'nth' repetition; and (iii) formulate the mutual interaction of various data pools when combined, enabling the estimation of model performance on a combination of multiple data pools without ever jointly training on them.","Our key message is that data curation $\\textit{cannot}$ be agnostic of the total compute that a model will be trained for.","Our scaling laws allow us to curate the best possible pool for achieving top performance on Datacomp at various compute budgets, carving out a pareto-frontier for data curation.","Code is available at https://github.com/locuslab/scaling_laws_data_filtering."],"url":"http://arxiv.org/abs/2404.07177v1","category":"cs.LG"}
{"created":"2024-04-10 17:19:41","title":"Temperature Prediction for Stored Grain: A Multi-model Fusion Strategy Based on Machine Learning","abstract":"Temperature fluctuations significantly affect microorganism growth and pest activity in grain stacks. Thus, precise monitoring and forecasting of grain stack temperature are essential for maintaining the quality and safety of grain storage. This paper proposes a multi-model fusion approach to predict grain temperature using historical temperature data of stored grains and meteorological data from the region. Based on the proposed approaches, four distinct machine learning models, namely Adaboost, decision tree, extra trees, and random forest, are first developed. These models are then fine-tuned through parameter optimization to enhance their predictive capabilities. Subsequently, the optimized models are combined to form different ensemble models. In essence, the fusion process integrates the predictions of each individual model as new feature inputs into the prediction model. Furthermore, the study utilizes the random forest to identify the key factors influencing grain temperature, providing insights into the importance of different influencing factors. The experimental results demonstrate that the fusion models proposed in this paper have achieved higher prediction accuracy and robustness compared with the traditional prediction methods (i.e., single-model prediction). Additionally, the analysis of feature importance also offers empirical evidence for understanding the factors influencing grain temperature.","sentences":["Temperature fluctuations significantly affect microorganism growth and pest activity in grain stacks.","Thus, precise monitoring and forecasting of grain stack temperature are essential for maintaining the quality and safety of grain storage.","This paper proposes a multi-model fusion approach to predict grain temperature using historical temperature data of stored grains and meteorological data from the region.","Based on the proposed approaches, four distinct machine learning models, namely Adaboost, decision tree, extra trees, and random forest, are first developed.","These models are then fine-tuned through parameter optimization to enhance their predictive capabilities.","Subsequently, the optimized models are combined to form different ensemble models.","In essence, the fusion process integrates the predictions of each individual model as new feature inputs into the prediction model.","Furthermore, the study utilizes the random forest to identify the key factors influencing grain temperature, providing insights into the importance of different influencing factors.","The experimental results demonstrate that the fusion models proposed in this paper have achieved higher prediction accuracy and robustness compared with the traditional prediction methods (i.e., single-model prediction).","Additionally, the analysis of feature importance also offers empirical evidence for understanding the factors influencing grain temperature."],"url":"http://arxiv.org/abs/2404.07175v1","category":"cs.CE"}
{"created":"2024-04-10 17:02:34","title":"Adinkras and Pure Spinors","abstract":"The nilpotence variety for extended supersymmetric quantum mechanics is a cone over a quadric in projective space. The pure spinor correspondence, which relates the description of off-shell supermultiplets to the classification of modules over the corresponding hypersurface ring, reduces to a classical problem of linear algebra. Spinor bundles, which correspond to maximal Cohen-Macaulay modules, serve as basic building blocks. Koszul duality appears as a deformed version of the Bernstein-Gel'fand-Gel'fand correspondence that we make fully concrete. We illustrate in numerous examples the close relationship between these connections and the powerful graphical technology of Adinkras. We emphasize the role of R-symmetry for recovering higher-dimensional gauge and gravity multiplets.","sentences":["The nilpotence variety for extended supersymmetric quantum mechanics is a cone over a quadric in projective space.","The pure spinor correspondence, which relates the description of off-shell supermultiplets to the classification of modules over the corresponding hypersurface ring, reduces to a classical problem of linear algebra.","Spinor bundles, which correspond to maximal Cohen-Macaulay modules, serve as basic building blocks.","Koszul duality appears as a deformed version of the Bernstein-Gel'fand-Gel'fand correspondence that we make fully concrete.","We illustrate in numerous examples the close relationship between these connections and the powerful graphical technology of Adinkras.","We emphasize the role of R-symmetry for recovering higher-dimensional gauge and gravity multiplets."],"url":"http://arxiv.org/abs/2404.07167v1","category":"math-ph"}
{"created":"2024-04-10 16:13:53","title":"Microbial iron reduction under oxic conditions: implications for subsurface biogeochemistry","abstract":"Iron (Fe) reduction is one of Earth's most ancient microbial metabolisms, but after atmosphere-ocean oxygenation, this anaerobic process was relegated to niche anoxic environments below the water and soil surface. However, new technologies to monitor redox processes at the microscale relevant to microbial cells have recently revealed that the oxygen (O2) concentrations controlling the distribution of aerobic and anaerobic metabolisms are more heterogeneous than previously believed. To explore how O2 levels regulate microbial Fe reduction, we cultivated a facultative Fe-reducing bacterium using a cutting-edge microfluidic reactor integrated with transparent planar O2 sensors. Contrary to expectations, microbial growth induced Fe(III)-oxide (ferrihydrite) reduction under fully oxygenated conditions without forming O2-depleted microsites. Batch incubations highlighted the importance of the process at a larger scale, fundamentally changing our understanding of Fe cycling from the conceptualization of metal and nutrient mobility in the subsurface to our interpretation of Fe mineralogy in the rock record.","sentences":["Iron (Fe) reduction is one of Earth's most ancient microbial metabolisms, but after atmosphere-ocean oxygenation, this anaerobic process was relegated to niche anoxic environments below the water and soil surface.","However, new technologies to monitor redox processes at the microscale relevant to microbial cells have recently revealed that the oxygen (O2) concentrations controlling the distribution of aerobic and anaerobic metabolisms are more heterogeneous than previously believed.","To explore how O2 levels regulate microbial Fe reduction, we cultivated a facultative Fe-reducing bacterium using a cutting-edge microfluidic reactor integrated with transparent planar O2 sensors.","Contrary to expectations, microbial growth induced Fe(III)-oxide (ferrihydrite) reduction under fully oxygenated conditions without forming O2-depleted microsites.","Batch incubations highlighted the importance of the process at a larger scale, fundamentally changing our understanding of Fe cycling from the conceptualization of metal and nutrient mobility in the subsurface to our interpretation of Fe mineralogy in the rock record."],"url":"http://arxiv.org/abs/2404.07137v1","category":"physics.bio-ph"}
{"created":"2024-04-10 16:13:16","title":"To impute or not to? Testing multivariate normality on incomplete dataset: Revisiting the BHEP test","abstract":"In this paper, we focus on testing multivariate normality using the BHEP test with data that are missing completely at random. Our objective is twofold: first, to gain insight into the asymptotic behavior of BHEP test statistics under two widely used approaches for handling missing data, namely complete-case analysis and imputation, and second, to compare the power performance of test statistic under these approaches. It is observed that under the imputation approach, the affine invariance of test statistics is not preserved. To address this issue, we propose an appropriate bootstrap algorithm for approximating p-values. Extensive simulation studies demonstrate that both mean and median approaches exhibit greater power compared to testing with complete-case analysis, and open some questions for further research.","sentences":["In this paper, we focus on testing multivariate normality using the BHEP test with data that are missing completely at random.","Our objective is twofold: first, to gain insight into the asymptotic behavior of BHEP test statistics under two widely used approaches for handling missing data, namely complete-case analysis and imputation, and second, to compare the power performance of test statistic under these approaches.","It is observed that under the imputation approach, the affine invariance of test statistics is not preserved.","To address this issue, we propose an appropriate bootstrap algorithm for approximating p-values.","Extensive simulation studies demonstrate that both mean and median approaches exhibit greater power compared to testing with complete-case analysis, and open some questions for further research."],"url":"http://arxiv.org/abs/2404.07136v1","category":"stat.ME"}
{"created":"2024-04-10 16:08:13","title":"A conservative Eulerian finite element method for transport and diffusion in moving domains","abstract":"The paper introduces a finite element method for an Eulerian formulation of partial differential equations governing the transport and diffusion of a scalar quantity in a time-dependent domain. The method follows the idea from Lehrenfeld & Olshanskii [ESAIM: M2AN, 53(2): 585-614, 2019] of a solution extension to realise the Eulearian time-stepping scheme. However, a reformulation of the partial differential equation is suggested to derive a scheme which conserves the quantity under consideration exactly on the discrete level. For the spatial discretisation, the paper considers an unfitted finite element method. Ghost-penalty stabilisation is used to release the discrete solution extension and gives a scheme robust against arbitrary intersections between the mesh and geometry interface. The stability is analysed for both first- and second-order backward differentiation formula versions of the scheme. Several numerical examples in two and three spatial dimensions are included to illustrate the potential of this method.","sentences":["The paper introduces a finite element method for an Eulerian formulation of partial differential equations governing the transport and diffusion of a scalar quantity in a time-dependent domain.","The method follows the idea from Lehrenfeld & Olshanskii","[ESAIM: M2AN, 53(2): 585-614, 2019] of a solution extension to realise the Eulearian time-stepping scheme.","However, a reformulation of the partial differential equation is suggested to derive a scheme which conserves the quantity under consideration exactly on the discrete level.","For the spatial discretisation, the paper considers an unfitted finite element method.","Ghost-penalty stabilisation is used to release the discrete solution extension and gives a scheme robust against arbitrary intersections between the mesh and geometry interface.","The stability is analysed for both first- and second-order backward differentiation formula versions of the scheme.","Several numerical examples in two and three spatial dimensions are included to illustrate the potential of this method."],"url":"http://arxiv.org/abs/2404.07130v1","category":"math.NA"}
{"created":"2024-04-10 15:42:25","title":"Fabrication Tolerant Multi-Layer Integrated Photonic Topology Optimization","abstract":"Optimal multi-layer device design requires consideration of fabrication uncertainties associated with inter-layer alignment and conformal layering. We present layer-restricted topology optimization (TO), a novel technique which mitigates the effects of unwanted conformal layering for multi-layer structures and enables TO in multi-etch material platforms. We explore several approaches to achieve this result compatible with density-based TO projection techniques and geometric constraints. Then, we present a robust TO formulation to design devices resilient to inter-layer misalignment. The novel constraint and robust formulation are demonstrated in 2D grating couplers and a 3D polarization rotator.","sentences":["Optimal multi-layer device design requires consideration of fabrication uncertainties associated with inter-layer alignment and conformal layering.","We present layer-restricted topology optimization (TO), a novel technique which mitigates the effects of unwanted conformal layering for multi-layer structures and enables TO in multi-etch material platforms.","We explore several approaches to achieve this result compatible with density-based TO projection techniques and geometric constraints.","Then, we present a robust TO formulation to design devices resilient to inter-layer misalignment.","The novel constraint and robust formulation are demonstrated in 2D grating couplers and a 3D polarization rotator."],"url":"http://arxiv.org/abs/2404.07104v1","category":"physics.optics"}
{"created":"2024-04-10 15:38:02","title":"Unraveling Consumer Purchase Journey Using Neural Network Models","abstract":"This study utilizes an ensemble of feedforward neural network models to analyze large-volume and high-dimensional consumer touchpoints and their impact on purchase decisions. When applied to a proprietary dataset of consumer touchpoints and purchases from a global software service provider, the proposed approach demonstrates better predictive accuracy than both traditional models, such as logistic regression, naive Bayes, and k-nearest neighbors, as well as ensemble tree-based classifiers, such as bagging, random forest, AdaBoost, and gradient boosting. By calculating the Shapley values within this network, we provide nuanced insights into touchpoint effectiveness, as we not only assess the marginal impact of diverse touchpoint types but also offer a granular view of the impact distribution within a touchpoint type. Additionally, our model shows excellent adaptability and resilience with limited data resources. When the historical data is reduced from 40 to 1 month, our model shows only a modest 19% decrease in accuracy. This modeling framework can enable managers to more accurately and comprehensively evaluate consumer touchpoints, thereby enhancing the effectiveness and efficiency of their marketing campaigns.","sentences":["This study utilizes an ensemble of feedforward neural network models to analyze large-volume and high-dimensional consumer touchpoints and their impact on purchase decisions.","When applied to a proprietary dataset of consumer touchpoints and purchases from a global software service provider, the proposed approach demonstrates better predictive accuracy than both traditional models, such as logistic regression, naive Bayes, and k-nearest neighbors, as well as ensemble tree-based classifiers, such as bagging, random forest, AdaBoost, and gradient boosting.","By calculating the Shapley values within this network, we provide nuanced insights into touchpoint effectiveness, as we not only assess the marginal impact of diverse touchpoint types but also offer a granular view of the impact distribution within a touchpoint type.","Additionally, our model shows excellent adaptability and resilience with limited data resources.","When the historical data is reduced from 40 to 1 month, our model shows only a modest 19% decrease in accuracy.","This modeling framework can enable managers to more accurately and comprehensively evaluate consumer touchpoints, thereby enhancing the effectiveness and efficiency of their marketing campaigns."],"url":"http://arxiv.org/abs/2404.07098v1","category":"stat.AP"}
{"created":"2024-04-10 15:01:15","title":"Strong stretching theory of polydisperse curved polymer brushes","abstract":"We investigate the effect of polydispersity on the properties of curved linear brushes in good solvent. To this end, we extend the strong stretching theory for polydisperse brushes to curved geometries and investigate the polymer chain end profiles, bending moduli and other properties for experimentally relevant polymer chain length distributions of the Schulz-Zimm type. We also investigate the properties of End Exclusion Zones (EEZ) that may appear in convex geometries under certain conditions, and show that their position in the brush can be engineered by careful selection of the polymer length distribution. Lastly, we propose a method to engineer chain end profiles by engineering the polymer length distribution.","sentences":["We investigate the effect of polydispersity on the properties of curved linear brushes in good solvent.","To this end, we extend the strong stretching theory for polydisperse brushes to curved geometries and investigate the polymer chain end profiles, bending moduli and other properties for experimentally relevant polymer chain length distributions of the Schulz-Zimm type.","We also investigate the properties of End Exclusion Zones (EEZ) that may appear in convex geometries under certain conditions, and show that their position in the brush can be engineered by careful selection of the polymer length distribution.","Lastly, we propose a method to engineer chain end profiles by engineering the polymer length distribution."],"url":"http://arxiv.org/abs/2404.07069v1","category":"cond-mat.soft"}
{"created":"2024-04-10 14:50:43","title":"A Tight $O(4^k/p_c)$ Runtime Bound for a ($\u03bc$+1) GA on Jump$_k$ for Realistic Crossover Probabilities","abstract":"The Jump$_k$ benchmark was the first problem for which crossover was proven to give a speedup over mutation-only evolutionary algorithms. Jansen and Wegener (2002) proved an upper bound of $O({\\rm poly}(n) + 4^k/p_c)$ for the ($\\mu$+1)~Genetic Algorithm ($(\\mu+1)$ GA), but only for unrealistically small crossover probabilities $p_c$. To this date, it remains an open problem to prove similar upper bounds for realistic~$p_c$; the best known runtime bound for $p_c = \\Omega(1)$ is $O((n/\\chi)^{k-1})$, $\\chi$ a positive constant. Using recently developed techniques, we analyse the evolution of the population diversity, measured as sum of pairwise Hamming distances, for a variant of the \\muga on Jump$_k$. We show that population diversity converges to an equilibrium of near-perfect diversity. This yields an improved and tight time bound of $O(\\mu n \\log(k) + 4^k/p_c)$ for a range of~$k$ under the mild assumptions $p_c = O(1/k)$ and $\\mu \\in \\Omega(kn)$. For all constant~$k$ the restriction is satisfied for some $p_c = \\Omega(1)$. Our work partially solves a problem that has been open for more than 20 years.","sentences":["The Jump$_k$ benchmark was the first problem for which crossover was proven to give a speedup over mutation-only evolutionary algorithms.","Jansen and Wegener (2002) proved an upper bound of $O({\\rm poly}(n)","+ 4^k/p_c)$ for the ($\\mu$+1)~Genetic Algorithm ($(\\mu+1)$ GA), but only for unrealistically small crossover probabilities $p_c$. To this date, it remains an open problem to prove similar upper bounds for realistic~$p_c$; the best known runtime bound for $p_c = \\Omega(1)$ is $O((n/\\chi)^{k-1})$, $\\chi$ a positive constant.","Using recently developed techniques, we analyse the evolution of the population diversity, measured as sum of pairwise Hamming distances, for a variant of the \\muga on Jump$_k$. We show that population diversity converges to an equilibrium of near-perfect diversity.","This yields an improved and tight time bound of $O(\\mu n \\log(k) + 4^k/p_c)$ for a range of~$k$ under the mild assumptions $p_c","= O(1/k)$ and $\\mu \\in \\Omega(kn)$. For all constant~$k$ the restriction is satisfied for some $p_c = \\Omega(1)$. Our work partially solves a problem that has been open for more than 20 years."],"url":"http://arxiv.org/abs/2404.07061v1","category":"cs.NE"}
{"created":"2024-04-10 14:40:28","title":"Three-dimensional ${\\mathbb Z}_2$-gauge $N$-vector models","abstract":"We study the phase diagram and critical behaviors of three-dimensional lattice ${\\mathbb Z}_2$-gauge $N$-vector models, in which an $N$-component real field is minimally coupled with a ${\\mathbb   Z}_2$-gauge link variables. These models are invariant under global O($N$) and local ${\\mathbb Z}_2$ transformations. They present three phases characterized by the spontaneous breaking of the global O($N$) symmetry and by the different topological properties of the ${\\mathbb   Z}_2$-gauge correlations. We address the nature of the three transition lines separating the three phases. The theoretical predictions are supported by numerical finite-size scaling analyses of Monte Carlo data for the $N=2$ model. In this case, continuous transitions can be observed along both transition lines where the spins order, in the regime of small and large inverse gauge coupling $K$. Even though these continuous transitions belong to the same $XY$ universality class, their critical modes turn out to be different. When the gauge variables are disordered (small $K$), the relevant order-parameter field is a gauge-invariant bilinear combination of the vector field. On the other hand, when the gauge variables are ordered (large $K$), the order-parameter field is the gauge-dependent $N$-vector field, whose critical behavior can only be probed by using a stochastic gauge fixing that reduces the gauge freedom.","sentences":["We study the phase diagram and critical behaviors of three-dimensional lattice ${\\mathbb Z}_2$-gauge $N$-vector models, in which an $N$-component real field is minimally coupled with a ${\\mathbb   Z}_2$-gauge link variables.","These models are invariant under global O($N$) and local ${\\mathbb Z}_2$ transformations.","They present three phases characterized by the spontaneous breaking of the global O($N$) symmetry and by the different topological properties of the ${\\mathbb   Z}_2$-gauge correlations.","We address the nature of the three transition lines separating the three phases.","The theoretical predictions are supported by numerical finite-size scaling analyses of Monte Carlo data for the $N=2$ model.","In this case, continuous transitions can be observed along both transition lines where the spins order, in the regime of small and large inverse gauge coupling $K$. Even though these continuous transitions belong to the same $XY$ universality class, their critical modes turn out to be different.","When the gauge variables are disordered (small $K$), the relevant order-parameter field is a gauge-invariant bilinear combination of the vector field.","On the other hand, when the gauge variables are ordered (large $K$), the order-parameter field is the gauge-dependent $N$-vector field, whose critical behavior can only be probed by using a stochastic gauge fixing that reduces the gauge freedom."],"url":"http://arxiv.org/abs/2404.07050v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-10 14:28:09","title":"A Computational Analysis of the Dehumanisation of Migrants from Syria and Ukraine in Slovene News Media","abstract":"Dehumanisation involves the perception and or treatment of a social group's members as less than human. This phenomenon is rarely addressed with computational linguistic techniques. We adapt a recently proposed approach for English, making it easier to transfer to other languages and to evaluate, introducing a new sentiment resource, the use of zero-shot cross-lingual valence and arousal detection, and a new method for statistical significance testing. We then apply it to study attitudes to migration expressed in Slovene newspapers, to examine changes in the Slovene discourse on migration between the 2015-16 migration crisis following the war in Syria and the 2022-23 period following the war in Ukraine. We find that while this discourse became more negative and more intense over time, it is less dehumanising when specifically addressing Ukrainian migrants compared to others.","sentences":["Dehumanisation involves the perception and or treatment of a social group's members as less than human.","This phenomenon is rarely addressed with computational linguistic techniques.","We adapt a recently proposed approach for English, making it easier to transfer to other languages and to evaluate, introducing a new sentiment resource, the use of zero-shot cross-lingual valence and arousal detection, and a new method for statistical significance testing.","We then apply it to study attitudes to migration expressed in Slovene newspapers, to examine changes in the Slovene discourse on migration between the 2015-16 migration crisis following the war in Syria and the 2022-23 period following the war in Ukraine.","We find that while this discourse became more negative and more intense over time, it is less dehumanising when specifically addressing Ukrainian migrants compared to others."],"url":"http://arxiv.org/abs/2404.07036v1","category":"cs.CL"}
{"created":"2024-04-10 14:27:07","title":"Quantum Tunneling: From Theory to Error-Mitigated Quantum Simulation","abstract":"Ever since the discussions about a possible quantum computer arised, quantum simulations have been at the forefront of possible utilities and the task of quantum simulations is one that promises quantum advantage. In recent years, simulations of large molecules through VQE or dynamics of many-body spin Hamiltonians may be possible, and even able to achieve useful results with the use of error mitigation techniques. Simulating smaller models is also important, and currently, in the NISQ (Noisy intermediate-scale quantum) era, it is easier and less prone to errors. This current study encompasses the theoretical background and the hardware aware circuit implementation of a quantum tunneling simulation. Specifically, this study presents the theoretical background needed for such implementation and highlights the main steps of development. Building on classic approaches of quantum tunneling simulations, this study improves the result of such simulations by employing error mitigation techniques (ZNE and REM) and uses them in conjunction with multiprogramming of the quantum chip for solving the hardware under-utilization problem that arises in such contexts. Moreover, we highlight the need for hardware-aware circuit implementations and discuss these considerations in detail to give an end-to-end workflow overview of quantum simulations.","sentences":["Ever since the discussions about a possible quantum computer arised, quantum simulations have been at the forefront of possible utilities and the task of quantum simulations is one that promises quantum advantage.","In recent years, simulations of large molecules through VQE or dynamics of many-body spin Hamiltonians may be possible, and even able to achieve useful results with the use of error mitigation techniques.","Simulating smaller models is also important, and currently, in the NISQ (Noisy intermediate-scale quantum) era, it is easier and less prone to errors.","This current study encompasses the theoretical background and the hardware aware circuit implementation of a quantum tunneling simulation.","Specifically, this study presents the theoretical background needed for such implementation and highlights the main steps of development.","Building on classic approaches of quantum tunneling simulations, this study improves the result of such simulations by employing error mitigation techniques (ZNE and REM) and uses them in conjunction with multiprogramming of the quantum chip for solving the hardware under-utilization problem that arises in such contexts.","Moreover, we highlight the need for hardware-aware circuit implementations and discuss these considerations in detail to give an end-to-end workflow overview of quantum simulations."],"url":"http://arxiv.org/abs/2404.07034v1","category":"quant-ph"}
{"created":"2024-04-10 14:25:23","title":"An Evidential-enhanced Tri-Branch Consistency Learning Method for Semi-supervised Medical Image Segmentation","abstract":"Semi-supervised segmentation presents a promising approach for large-scale medical image analysis, effectively reducing annotation burdens while achieving comparable performance. This methodology holds substantial potential for streamlining the segmentation process and enhancing its feasibility within clinical settings for translational investigations. While cross-supervised training, based on distinct co-training sub-networks, has become a prevalent paradigm for this task, addressing critical issues such as predication disagreement and label-noise suppression requires further attention and progress in cross-supervised training. In this paper, we introduce an Evidential Tri-Branch Consistency learning framework (ETC-Net) for semi-supervised medical image segmentation. ETC-Net employs three branches: an evidential conservative branch, an evidential progressive branch, and an evidential fusion branch. The first two branches exhibit complementary characteristics, allowing them to address prediction diversity and enhance training stability. We also integrate uncertainty estimation from the evidential learning into cross-supervised training, mitigating the negative impact of erroneous supervision signals. Additionally, the evidential fusion branch capitalizes on the complementary attributes of the first two branches and leverages an evidence-based Dempster-Shafer fusion strategy, supervised by more reliable and accurate pseudo-labels of unlabeled data. Extensive experiments conducted on LA, Pancreas-CT, and ACDC datasets demonstrate that ETC-Net surpasses other state-of-the-art methods for semi-supervised segmentation. The code will be made available in the near future at https://github.com/Medsemiseg.","sentences":["Semi-supervised segmentation presents a promising approach for large-scale medical image analysis, effectively reducing annotation burdens while achieving comparable performance.","This methodology holds substantial potential for streamlining the segmentation process and enhancing its feasibility within clinical settings for translational investigations.","While cross-supervised training, based on distinct co-training sub-networks, has become a prevalent paradigm for this task, addressing critical issues such as predication disagreement and label-noise suppression requires further attention and progress in cross-supervised training.","In this paper, we introduce an Evidential Tri-Branch Consistency learning framework (ETC-Net) for semi-supervised medical image segmentation.","ETC-Net employs three branches: an evidential conservative branch, an evidential progressive branch, and an evidential fusion branch.","The first two branches exhibit complementary characteristics, allowing them to address prediction diversity and enhance training stability.","We also integrate uncertainty estimation from the evidential learning into cross-supervised training, mitigating the negative impact of erroneous supervision signals.","Additionally, the evidential fusion branch capitalizes on the complementary attributes of the first two branches and leverages an evidence-based Dempster-Shafer fusion strategy, supervised by more reliable and accurate pseudo-labels of unlabeled data.","Extensive experiments conducted on LA, Pancreas-CT, and ACDC datasets demonstrate that ETC-Net surpasses other state-of-the-art methods for semi-supervised segmentation.","The code will be made available in the near future at https://github.com/Medsemiseg."],"url":"http://arxiv.org/abs/2404.07032v1","category":"cs.CV"}
{"created":"2024-04-10 14:21:47","title":"Approximations of expectations under infinite product measures","abstract":"We are given a bounded Borel-measurable real-valued function on a product of countably many Polish spaces, and a product probability measure. We are interested in points in the product space that can be used to approximate the expected value of this function. We define two notions. A point is called a weak $\\epsilon$-approximation, where $\\epsilon \\geq 0$, if the Dirac measure on this point, except in finitely many coordinates where another measure can be taken, gives an expected value that is $\\epsilon$-close to the original expected value. A point is called a strong $\\epsilon$-approximation if the same holds under the restriction that in those finitely many coordinates the measure is equal to the original one. We prove that both the set of weak 0-approximation points and the set of strong $\\epsilon$-approximation points, for any $\\epsilon>0$, have measure 1 under the original measure. Finally, we provide two applications: (i) in Game Theory on the minmax guarantee levels of the players in games with infinitely many players, and (ii) in Decision Theory on the set of feasible expected payoffs in infinite duration problems.","sentences":["We are given a bounded Borel-measurable real-valued function on a product of countably many Polish spaces, and a product probability measure.","We are interested in points in the product space that can be used to approximate the expected value of this function.","We define two notions.","A point is called a weak $\\epsilon$-approximation, where $\\epsilon \\geq 0$, if the Dirac measure on this point, except in finitely many coordinates where another measure can be taken, gives an expected value that is $\\epsilon$-close to the original expected value.","A point is called a strong $\\epsilon$-approximation if the same holds under the restriction that in those finitely many coordinates the measure is equal to the original one.","We prove that both the set of weak 0-approximation points and the set of strong $\\epsilon$-approximation points, for any $\\epsilon>0$, have measure 1 under the original measure.","Finally, we provide two applications: (i) in Game Theory on the minmax guarantee levels of the players in games with infinitely many players, and (ii) in Decision Theory on the set of feasible expected payoffs in infinite duration problems."],"url":"http://arxiv.org/abs/2404.07028v1","category":"math.PR"}
{"created":"2024-04-10 13:39:11","title":"LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models","abstract":"We present the LM Transparency Tool (LM-TT), an open-source interactive toolkit for analyzing the internal workings of Transformer-based language models. Differently from previously existing tools that focus on isolated parts of the decision-making process, our framework is designed to make the entire prediction process transparent, and allows tracing back model behavior from the top-layer representation to very fine-grained parts of the model. Specifically, it (1) shows the important part of the whole input-to-output information flow, (2) allows attributing any changes done by a model block to individual attention heads and feed-forward neurons, (3) allows interpreting the functions of those heads or neurons. A crucial part of this pipeline is showing the importance of specific model components at each step. As a result, we are able to look at the roles of model components only in cases where they are important for a prediction. Since knowing which components should be inspected is key for analyzing large models where the number of these components is extremely high, we believe our tool will greatly support the interpretability community both in research settings and in practical applications.","sentences":["We present the LM Transparency Tool (LM-TT), an open-source interactive toolkit for analyzing the internal workings of Transformer-based language models.","Differently from previously existing tools that focus on isolated parts of the decision-making process, our framework is designed to make the entire prediction process transparent, and allows tracing back model behavior from the top-layer representation to very fine-grained parts of the model.","Specifically, it (1) shows the important part of the whole input-to-output information flow, (2) allows attributing any changes done by a model block to individual attention heads and feed-forward neurons, (3) allows interpreting the functions of those heads or neurons.","A crucial part of this pipeline is showing the importance of specific model components at each step.","As a result, we are able to look at the roles of model components only in cases where they are important for a prediction.","Since knowing which components should be inspected is key for analyzing large models where the number of these components is extremely high, we believe our tool will greatly support the interpretability community both in research settings and in practical applications."],"url":"http://arxiv.org/abs/2404.07004v1","category":"cs.CL"}
{"created":"2024-04-10 12:58:23","title":"Adaptive Strategy of Testing Alphas in High Dimensional Linear Factor Pricing Models","abstract":"In recent years, there has been considerable research on testing alphas in high-dimensional linear factor pricing models. In our study, we introduce a novel max-type test procedure that performs well under sparse alternatives. Furthermore, we demonstrate that this new max-type test procedure is asymptotically independent from the sum-type test procedure proposed by Pesaran and Yamagata (2017). Building on this, we propose a Fisher combination test procedure that exhibits good performance for both dense and sparse alternatives.","sentences":["In recent years, there has been considerable research on testing alphas in high-dimensional linear factor pricing models.","In our study, we introduce a novel max-type test procedure that performs well under sparse alternatives.","Furthermore, we demonstrate that this new max-type test procedure is asymptotically independent from the sum-type test procedure proposed by Pesaran and Yamagata (2017).","Building on this, we propose a Fisher combination test procedure that exhibits good performance for both dense and sparse alternatives."],"url":"http://arxiv.org/abs/2404.06984v1","category":"stat.ME"}
{"created":"2024-04-10 12:48:10","title":"The CAST package for training and assessment of spatial prediction models in R","abstract":"One key task in environmental science is to map environmental variables continuously in space or even in space and time. Machine learning algorithms are frequently used to learn from local field observations to make spatial predictions by estimating the value of the variable of interest in places where it has not been measured. However, the application of machine learning strategies for spatial mapping involves additional challenges compared to \"non-spatial\" prediction tasks that often originate from spatial autocorrelation and from training data that are not independent and identically distributed.   In the past few years, we developed a number of methods to support the application of machine learning for spatial data which involves the development of suitable cross-validation strategies for performance assessment and model selection, spatial feature selection, and methods to assess the area of applicability of the trained models. The intention of the CAST package is to support the application of machine learning strategies for predictive mapping by implementing such methods and making them available for easy integration into modelling workflows.   Here we introduce the CAST package and its core functionalities. At the case study of mapping plant species richness, we will go through the different steps of the modelling workflow and show how CAST can be used to support more reliable spatial predictions.","sentences":["One key task in environmental science is to map environmental variables continuously in space or even in space and time.","Machine learning algorithms are frequently used to learn from local field observations to make spatial predictions by estimating the value of the variable of interest in places where it has not been measured.","However, the application of machine learning strategies for spatial mapping involves additional challenges compared to \"non-spatial\" prediction tasks that often originate from spatial autocorrelation and from training data that are not independent and identically distributed.   ","In the past few years, we developed a number of methods to support the application of machine learning for spatial data which involves the development of suitable cross-validation strategies for performance assessment and model selection, spatial feature selection, and methods to assess the area of applicability of the trained models.","The intention of the CAST package is to support the application of machine learning strategies for predictive mapping by implementing such methods and making them available for easy integration into modelling workflows.   ","Here we introduce the CAST package and its core functionalities.","At the case study of mapping plant species richness, we will go through the different steps of the modelling workflow and show how CAST can be used to support more reliable spatial predictions."],"url":"http://arxiv.org/abs/2404.06978v1","category":"stat.ML"}
{"created":"2024-04-10 12:26:35","title":"Transcendence properties of the Artin-Hasse exponential modulo $p$","abstract":"Let $E_p(x)$ denote the Artin-Hasse exponential and let $\\overline{E}_p(x)$ denote its reduction modulo $p$ in $\\mathbb{F}_p[[x]]$. In this article we study transcendence properties of $\\overline{E}_p(x)$ over $\\mathbb{F}_p[x]$. We give two proofs that $\\overline{E}_p(x)$ is transcendental, affirmatively answering a question of Thakur. We also prove algebraic independence results: i) for $f_1,\\dots,f_r \\in x\\mathbb{F}_p[x]$ satisfying certain linear independence properties, we show that the $\\overline{E}_p(f_1), \\dots, \\overline{E}_p(f_r)$ are algebraically independent over $\\mathbb{F}_p[x]$ and ii) we determine the algebraic relations between $\\overline{E}_p(cx)$, where $c \\in \\mathbb{F}_p^\\times$. Our proof studies the higher derivatives of $\\overline{E}_p(x)$ and makes use of iterative differential Galois theory.","sentences":["Let $E_p(x)$ denote the Artin-Hasse exponential and let $\\overline{E}_p(x)$ denote its reduction modulo $p$ in $\\mathbb{F}_p[[x]]$. In this article we study transcendence properties of $\\overline{E}_p(x)$ over $\\mathbb{F}_p[x]$. We give two proofs that $\\overline{E}_p(x)$ is transcendental, affirmatively answering a question of Thakur.","We also prove algebraic independence results: i) for $f_1,\\dots,f_r \\in x\\mathbb{F}_p[x]$ satisfying certain linear independence properties, we show that the $\\overline{E}_p(f_1), \\dots, \\overline{E}_p(f_r)$ are algebraically independent over $\\mathbb{F}_p[x]$ and ii) we determine the algebraic relations between $\\overline{E}_p(cx)$, where $c \\in \\mathbb{F}_p^\\times$.","Our proof studies the higher derivatives of $\\overline{E}_p(x)$ and makes use of iterative differential Galois theory."],"url":"http://arxiv.org/abs/2404.06968v1","category":"math.NT"}
{"created":"2024-04-10 11:32:06","title":"Improving prediction accuracy by choosing resampling distribution via cross-validation","abstract":"In a regression model, prediction is typically performed after model selection. The large variability in the model selection makes the prediction unstable. Thus, it is essential to reduce the variability in model selection and improve prediction accuracy. To achieve this goal, a parametric bootstrap smoothing can be applied. In this method, model selection is performed for each resampling from a parametric distribution, and these models are then averaged such that the distribution of the selected models is considered. Here, the prediction accuracy is highly dependent on the choice of a distribution for resampling. In particular, an experimental study shows that the choice of error variance significantly changes the distribution of the selected model and thus plays a key role in improving the prediction accuracy. We also observed that the true error variance does not always provide optimal prediction accuracy. Therefore, it would not always be appropriate to use unbiased estimators of the true parameters or standard estimators of the parameters for the resampling distribution. In this study, we propose employing cross validation to choose a suitable resampling distribution rather than unbiased estimators of parameters. Our proposed method was applied to electricity demand data. The results indicate that the proposed method provides a better prediction accuracy than the existing method.","sentences":["In a regression model, prediction is typically performed after model selection.","The large variability in the model selection makes the prediction unstable.","Thus, it is essential to reduce the variability in model selection and improve prediction accuracy.","To achieve this goal, a parametric bootstrap smoothing can be applied.","In this method, model selection is performed for each resampling from a parametric distribution, and these models are then averaged such that the distribution of the selected models is considered.","Here, the prediction accuracy is highly dependent on the choice of a distribution for resampling.","In particular, an experimental study shows that the choice of error variance significantly changes the distribution of the selected model and thus plays a key role in improving the prediction accuracy.","We also observed that the true error variance does not always provide optimal prediction accuracy.","Therefore, it would not always be appropriate to use unbiased estimators of the true parameters or standard estimators of the parameters for the resampling distribution.","In this study, we propose employing cross validation to choose a suitable resampling distribution rather than unbiased estimators of parameters.","Our proposed method was applied to electricity demand data.","The results indicate that the proposed method provides a better prediction accuracy than the existing method."],"url":"http://arxiv.org/abs/2404.06932v1","category":"stat.CO"}
{"created":"2024-04-10 11:20:27","title":"Restoring the topological edge states in a finite optical superlattice","abstract":"We consider the emergence of edge states in a finite optical lattice and show that the boundaries of the lattice play a decisive role for their location in the corresponding energy spectrum. We introduce a simple parametrisation of the boundaries of the optical lattice and demonstrate the existence of an optimal choice of the values of the parameters which lead to an approximate restoration of chiral symmetry. A crucial property of this optimization is the suppression of tunneling between next-nearest neighboring wells of the lattice. This in turn allows the mapping of the optical lattice set-up to a finite SSH model. The topological character of the emerging edge states is discussed.","sentences":["We consider the emergence of edge states in a finite optical lattice and show that the boundaries of the lattice play a decisive role for their location in the corresponding energy spectrum.","We introduce a simple parametrisation of the boundaries of the optical lattice and demonstrate the existence of an optimal choice of the values of the parameters which lead to an approximate restoration of chiral symmetry.","A crucial property of this optimization is the suppression of tunneling between next-nearest neighboring wells of the lattice.","This in turn allows the mapping of the optical lattice set-up to a finite SSH model.","The topological character of the emerging edge states is discussed."],"url":"http://arxiv.org/abs/2404.06924v1","category":"quant-ph"}
{"created":"2024-04-10 11:06:47","title":"Cavity-enhanced Rydberg atom microwave receiver","abstract":"Developing microwave electric field sensing based on Rydberg atom has received significant attention due to its unique advantages. However, achieving effective coupling between Rydberg atom and the microwave electric field in the sensing process is a challenging problem that greatly impacts the sensitivity. To address this, we propose the use of a microwave resonant cavity to enhance the effective coupling between the Rydberg atoms and the microwave electric field. In our experiment, we use a three-photon excitation scheme to prepare Rydberg atoms, make measurements of electric fields without and with a microwave cavity in which the vapor cell is put inside. Through experimental testing, we achieve an 18 dB enhancement of power sensitivity. The experiment shows an effective enhancement in electric field pulse signal detection. This result provides a promising direction for enhancing the sensitivity of Rydberg atomic electric field sensors and paves the way for their application in precision electric field measurements.","sentences":["Developing microwave electric field sensing based on Rydberg atom has received significant attention due to its unique advantages.","However, achieving effective coupling between Rydberg atom and the microwave electric field in the sensing process is a challenging problem that greatly impacts the sensitivity.","To address this, we propose the use of a microwave resonant cavity to enhance the effective coupling between the Rydberg atoms and the microwave electric field.","In our experiment, we use a three-photon excitation scheme to prepare Rydberg atoms, make measurements of electric fields without and with a microwave cavity in which the vapor cell is put inside.","Through experimental testing, we achieve an 18 dB enhancement of power sensitivity.","The experiment shows an effective enhancement in electric field pulse signal detection.","This result provides a promising direction for enhancing the sensitivity of Rydberg atomic electric field sensors and paves the way for their application in precision electric field measurements."],"url":"http://arxiv.org/abs/2404.06915v1","category":"physics.atom-ph"}
{"created":"2024-04-10 10:32:29","title":"PACP: Priority-Aware Collaborative Perception for Connected and Autonomous Vehicles","abstract":"Surrounding perceptions are quintessential for safe driving for connected and autonomous vehicles (CAVs), where the Bird's Eye View has been employed to accurately capture spatial relationships among vehicles. However, severe inherent limitations of BEV, like blind spots, have been identified. Collaborative perception has emerged as an effective solution to overcoming these limitations through data fusion from multiple views of surrounding vehicles. While most existing collaborative perception strategies adopt a fully connected graph predicated on fairness in transmissions, they often neglect the varying importance of individual vehicles due to channel variations and perception redundancy. To address these challenges, we propose a novel Priority-Aware Collaborative Perception (PACP) framework to employ a BEV-match mechanism to determine the priority levels based on the correlation between nearby CAVs and the ego vehicle for perception. By leveraging submodular optimization, we find near-optimal transmission rates, link connectivity, and compression metrics. Moreover, we deploy a deep learning-based adaptive autoencoder to modulate the image reconstruction quality under dynamic channel conditions. Finally, we conduct extensive studies and demonstrate that our scheme significantly outperforms the state-of-the-art schemes by 8.27% and 13.60%, respectively, in terms of utility and precision of the Intersection over Union.","sentences":["Surrounding perceptions are quintessential for safe driving for connected and autonomous vehicles (CAVs), where the Bird's Eye View has been employed to accurately capture spatial relationships among vehicles.","However, severe inherent limitations of BEV, like blind spots, have been identified.","Collaborative perception has emerged as an effective solution to overcoming these limitations through data fusion from multiple views of surrounding vehicles.","While most existing collaborative perception strategies adopt a fully connected graph predicated on fairness in transmissions, they often neglect the varying importance of individual vehicles due to channel variations and perception redundancy.","To address these challenges, we propose a novel Priority-Aware Collaborative Perception (PACP) framework to employ a BEV-match mechanism to determine the priority levels based on the correlation between nearby CAVs and the ego vehicle for perception.","By leveraging submodular optimization, we find near-optimal transmission rates, link connectivity, and compression metrics.","Moreover, we deploy a deep learning-based adaptive autoencoder to modulate the image reconstruction quality under dynamic channel conditions.","Finally, we conduct extensive studies and demonstrate that our scheme significantly outperforms the state-of-the-art schemes by 8.27% and 13.60%, respectively, in terms of utility and precision of the Intersection over Union."],"url":"http://arxiv.org/abs/2404.06891v1","category":"cs.NI"}
{"created":"2024-04-10 04:59:51","title":"An Animation-based Augmentation Approach for Action Recognition from Discontinuous Video","abstract":"The study of action recognition has attracted considerable attention recently due to its broad applications in multiple areas. However, with the issue of discontinuous training video, which not only decreases the performance of action recognition model, but complicates the data augmentation process as well, still remains under-exploration. In this study, we introduce the 4A (Action Animation-based Augmentation Approach), an innovative pipeline for data augmentation to address the problem. The main contributions remain in our work includes: (1) we investigate the problem of severe decrease on performance of action recognition task training by discontinuous video, and the limitation of existing augmentation methods on solving this problem. (2) we propose a novel augmentation pipeline, 4A, to address the problem of discontinuous video for training, while achieving a smoother and natural-looking action representation than the latest data augmentation methodology. (3) We achieve the same performance with only 10% of the original data for training as with all of the original data from the real-world dataset, and a better performance on In-the-wild videos, by employing our data augmentation techniques.","sentences":["The study of action recognition has attracted considerable attention recently due to its broad applications in multiple areas.","However, with the issue of discontinuous training video, which not only decreases the performance of action recognition model, but complicates the data augmentation process as well, still remains under-exploration.","In this study, we introduce the 4A (Action Animation-based Augmentation Approach), an innovative pipeline for data augmentation to address the problem.","The main contributions remain in our work includes: (1) we investigate the problem of severe decrease on performance of action recognition task training by discontinuous video, and the limitation of existing augmentation methods on solving this problem.","(2) we propose a novel augmentation pipeline, 4A, to address the problem of discontinuous video for training, while achieving a smoother and natural-looking action representation than the latest data augmentation methodology.","(3) We achieve the same performance with only 10% of the original data for training as with all of the original data from the real-world dataset, and a better performance on In-the-wild videos, by employing our data augmentation techniques."],"url":"http://arxiv.org/abs/2404.06741v1","category":"cs.CV"}
{"created":"2024-04-10 04:49:00","title":"A Copula Graphical Model for Multi-Attribute Data using Optimal Transport","abstract":"Motivated by modern data forms such as images and multi-view data, the multi-attribute graphical model aims to explore the conditional independence structure among vectors. Under the Gaussian assumption, the conditional independence between vectors is characterized by blockwise zeros in the precision matrix. To relax the restrictive Gaussian assumption, in this paper, we introduce a novel semiparametric multi-attribute graphical model based on a new copula named Cyclically Monotone Copula. This new copula treats the distribution of the node vectors as multivariate marginals and transforms them into Gaussian distributions based on the optimal transport theory. Since the model allows the node vectors to have arbitrary continuous distributions, it is more flexible than the classical Gaussian copula method that performs coordinatewise Gaussianization. We establish the concentration inequalities of the estimated covariance matrices and provide sufficient conditions for selection consistency of the group graphical lasso estimator. For the setting with high-dimensional attributes, a {Projected Cyclically Monotone Copula} model is proposed to address the curse of dimensionality issue that arises from solving high-dimensional optimal transport problems. Numerical results based on synthetic and real data show the efficiency and flexibility of our methods.","sentences":["Motivated by modern data forms such as images and multi-view data, the multi-attribute graphical model aims to explore the conditional independence structure among vectors.","Under the Gaussian assumption, the conditional independence between vectors is characterized by blockwise zeros in the precision matrix.","To relax the restrictive Gaussian assumption, in this paper, we introduce a novel semiparametric multi-attribute graphical model based on a new copula named Cyclically Monotone Copula.","This new copula treats the distribution of the node vectors as multivariate marginals and transforms them into Gaussian distributions based on the optimal transport theory.","Since the model allows the node vectors to have arbitrary continuous distributions, it is more flexible than the classical Gaussian copula method that performs coordinatewise Gaussianization.","We establish the concentration inequalities of the estimated covariance matrices and provide sufficient conditions for selection consistency of the group graphical lasso estimator.","For the setting with high-dimensional attributes, a {Projected Cyclically Monotone Copula} model is proposed to address the curse of dimensionality issue that arises from solving high-dimensional optimal transport problems.","Numerical results based on synthetic and real data show the efficiency and flexibility of our methods."],"url":"http://arxiv.org/abs/2404.06735v1","category":"stat.ML"}
{"created":"2024-04-10 04:24:42","title":"Bayesian NeRF: Quantifying Uncertainty with Volume Density in Neural Radiance Fields","abstract":"We present the Bayesian Neural Radiance Field (NeRF), which explicitly quantifies uncertainty in geometric volume structures without the need for additional networks, making it adept for challenging observations and uncontrolled images. NeRF diverges from traditional geometric methods by offering an enriched scene representation, rendering color and density in 3D space from various viewpoints. However, NeRF encounters limitations in relaxing uncertainties by using geometric structure information, leading to inaccuracies in interpretation under insufficient real-world observations. Recent research efforts aimed at addressing this issue have primarily relied on empirical methods or auxiliary networks. To fundamentally address this issue, we propose a series of formulational extensions to NeRF. By introducing generalized approximations and defining density-related uncertainty, our method seamlessly extends to manage uncertainty not only for RGB but also for depth, without the need for additional networks or empirical assumptions. In experiments we show that our method significantly enhances performance on RGB and depth images in the comprehensive dataset, demonstrating the reliability of the Bayesian NeRF approach to quantifying uncertainty based on the geometric structure.","sentences":["We present the Bayesian Neural Radiance Field (NeRF), which explicitly quantifies uncertainty in geometric volume structures without the need for additional networks, making it adept for challenging observations and uncontrolled images.","NeRF diverges from traditional geometric methods by offering an enriched scene representation, rendering color and density in 3D space from various viewpoints.","However, NeRF encounters limitations in relaxing uncertainties by using geometric structure information, leading to inaccuracies in interpretation under insufficient real-world observations.","Recent research efforts aimed at addressing this issue have primarily relied on empirical methods or auxiliary networks.","To fundamentally address this issue, we propose a series of formulational extensions to NeRF.","By introducing generalized approximations and defining density-related uncertainty, our method seamlessly extends to manage uncertainty not only for RGB but also for depth, without the need for additional networks or empirical assumptions.","In experiments we show that our method significantly enhances performance on RGB and depth images in the comprehensive dataset, demonstrating the reliability of the Bayesian NeRF approach to quantifying uncertainty based on the geometric structure."],"url":"http://arxiv.org/abs/2404.06727v1","category":"cs.CV"}
{"created":"2024-04-10 04:14:15","title":"Shannon's inequality on non-collapsing RCD spaces","abstract":"We prove the Shannon's inequality on non-collapsing $\\mathsf{RCD}(0,N)$ spaces. In the proof, we use the characterization of the $\\mathsf{EVI}_{0,N}$-gradient flow of the relative entropy and the infinitesimal behavior of the heat kernel. Also we have a cone rigidity result. As an application, we have the so-called uncertainty principle inequality on such spaces.","sentences":["We prove the Shannon's inequality on non-collapsing $\\mathsf{RCD}(0,N)$ spaces.","In the proof, we use the characterization of the $\\mathsf{EVI}_{0,N}$-gradient flow of the relative entropy and the infinitesimal behavior of the heat kernel.","Also we have a cone rigidity result.","As an application, we have the so-called uncertainty principle inequality on such spaces."],"url":"http://arxiv.org/abs/2404.06719v1","category":"math.MG"}
{"created":"2024-04-10 03:19:16","title":"Algorithms and Analysis for Optimizing Robust Objectives in Fair Machine Learning","abstract":"The original position or veil of ignorance argument of John Rawls, perhaps the most famous argument for egalitarianism, states that our concept of fairness, justice, or welfare should be decided from behind a veil of ignorance, and thus must consider everyone impartially (invariant to our identity). This can be posed as a zero-sum game, where a Daemon constructs a world, and an adversarial Angel then places the Daemon into the world. This game incentivizes the Daemon to maximize the minimum utility over all people (i.e., to maximize egalitarian welfare). In some sense, this is the most extreme form of risk aversion or robustness, and we show that by weakening the Angel, milder robust objectives arise, which we argue are effective robust proxies for fair learning or allocation tasks. In particular, the utilitarian, Gini, and power-mean welfare concepts arise from special cases of the adversarial game, which has philosophical implications for the understanding of each of these concepts. We also motivate a new fairness concept that essentially fuses the nonlinearity of the power-mean with the piecewise nature of the Gini class. Then, exploiting the relationship between fairness and robustness, we show that these robust fairness concepts can all be efficiently optimized under mild conditions via standard maximin optimization techniques. Finally, we show that such methods apply in machine learning contexts, and moreover we show generalization bounds for robust fair machine learning tasks.","sentences":["The original position or veil of ignorance argument of John Rawls, perhaps the most famous argument for egalitarianism, states that our concept of fairness, justice, or welfare should be decided from behind a veil of ignorance, and thus must consider everyone impartially (invariant to our identity).","This can be posed as a zero-sum game, where a Daemon constructs a world, and an adversarial Angel then places the Daemon into the world.","This game incentivizes the Daemon to maximize the minimum utility over all people (i.e., to maximize egalitarian welfare).","In some sense, this is the most extreme form of risk aversion or robustness, and we show that by weakening the Angel, milder robust objectives arise, which we argue are effective robust proxies for fair learning or allocation tasks.","In particular, the utilitarian, Gini, and power-mean welfare concepts arise from special cases of the adversarial game, which has philosophical implications for the understanding of each of these concepts.","We also motivate a new fairness concept that essentially fuses the nonlinearity of the power-mean with the piecewise nature of the Gini class.","Then, exploiting the relationship between fairness and robustness, we show that these robust fairness concepts can all be efficiently optimized under mild conditions via standard maximin optimization techniques.","Finally, we show that such methods apply in machine learning contexts, and moreover we show generalization bounds for robust fair machine learning tasks."],"url":"http://arxiv.org/abs/2404.06703v1","category":"cs.GT"}
{"created":"2024-04-10 02:47:05","title":"Binomial Self-compensation for Motion Error in Dynamic 3D Scanning","abstract":"Phase shifting profilometry (PSP) is favored in high-precision 3D scanning due to its high accuracy, robustness, and pixel-wise property. However, a fundamental assumption of PSP that the object should remain static is violated in dynamic measurement, making PSP susceptible to object moving, resulting in ripple-like errors in the point clouds. We propose a pixel-wise and frame-wise loopable binomial self-compensation (BSC) algorithm to effectively and flexibly eliminate motion error in the four-step PSP. Our mathematical model demonstrates that by summing successive motion-affected phase frames weighted by binomial coefficients, motion error exponentially diminishes as the binomial order increases, accomplishing automatic error compensation through the motion-affected phase sequence, without the assistance of any intermediate variable. Extensive experiments show that our BSC outperforms the existing methods in reducing motion error, while achieving a depth map frame rate equal to the camera's acquisition rate (90 fps), enabling high-accuracy 3D reconstruction with a quasi-single-shot frame rate.","sentences":["Phase shifting profilometry (PSP) is favored in high-precision 3D scanning due to its high accuracy, robustness, and pixel-wise property.","However, a fundamental assumption of PSP that the object should remain static is violated in dynamic measurement, making PSP susceptible to object moving, resulting in ripple-like errors in the point clouds.","We propose a pixel-wise and frame-wise loopable binomial self-compensation (BSC) algorithm to effectively and flexibly eliminate motion error in the four-step PSP.","Our mathematical model demonstrates that by summing successive motion-affected phase frames weighted by binomial coefficients, motion error exponentially diminishes as the binomial order increases, accomplishing automatic error compensation through the motion-affected phase sequence, without the assistance of any intermediate variable.","Extensive experiments show that our BSC outperforms the existing methods in reducing motion error, while achieving a depth map frame rate equal to the camera's acquisition rate (90 fps), enabling high-accuracy 3D reconstruction with a quasi-single-shot frame rate."],"url":"http://arxiv.org/abs/2404.06693v1","category":"cs.CV"}
{"created":"2024-04-10 02:19:37","title":"Atlas-X Equity Financing: Unlocking New Methods to Securely Obfuscate Axe Inventory Data Based on Differential Privacy","abstract":"Banks publish daily a list of available securities/assets (axe list) to selected clients to help them effectively locate Long (buy) or Short (sell) trades at reduced financing rates. This reduces costs for the bank, as the list aggregates the bank's internal firm inventory per asset for all clients of long as well as short trades. However, this is somewhat problematic: (1) the bank's inventory is revealed; (2) trades of clients who contribute to the aggregated list, particularly those deemed large, are revealed to other clients. Clients conducting sizable trades with the bank and possessing a portion of the aggregated asset exceeding $50\\%$ are considered to be concentrated clients. This could potentially reveal a trading concentrated client's activity to their competitors, thus providing an unfair advantage over the market.   Atlas-X Axe Obfuscation, powered by new differential private methods, enables a bank to obfuscate its published axe list on a daily basis while under continual observation, thus maintaining an acceptable inventory Profit and Loss (P&L) cost pertaining to the noisy obfuscated axe list while reducing the clients' trading activity leakage. Our main differential private innovation is a differential private aggregator for streams (time series data) of both positive and negative integers under continual observation.   For the last two years, Atlas-X system has been live in production across three major regions-USA, Europe, and Asia-at J.P. Morgan, a major financial institution, facilitating significant profitability. To our knowledge, it is the first differential privacy solution to be deployed in the financial sector. We also report benchmarks of our algorithm based on (anonymous) real and synthetic data to showcase the quality of our obfuscation and its success in production.","sentences":["Banks publish daily a list of available securities/assets (axe list) to selected clients to help them effectively locate Long (buy) or Short (sell) trades at reduced financing rates.","This reduces costs for the bank, as the list aggregates the bank's internal firm inventory per asset for all clients of long as well as short trades.","However, this is somewhat problematic: (1) the bank's inventory is revealed; (2) trades of clients who contribute to the aggregated list, particularly those deemed large, are revealed to other clients.","Clients conducting sizable trades with the bank and possessing a portion of the aggregated asset exceeding $50\\%$ are considered to be concentrated clients.","This could potentially reveal a trading concentrated client's activity to their competitors, thus providing an unfair advantage over the market.   ","Atlas-X Axe Obfuscation, powered by new differential private methods, enables a bank to obfuscate its published axe list on a daily basis while under continual observation, thus maintaining an acceptable inventory Profit and Loss (P&L) cost pertaining to the noisy obfuscated axe list while reducing the clients' trading activity leakage.","Our main differential private innovation is a differential private aggregator for streams (time series data) of both positive and negative integers under continual observation.   ","For the last two years, Atlas-X system has been live in production across three major regions-USA, Europe, and Asia-at J.P. Morgan, a major financial institution, facilitating significant profitability.","To our knowledge, it is the first differential privacy solution to be deployed in the financial sector.","We also report benchmarks of our algorithm based on (anonymous) real and synthetic data to showcase the quality of our obfuscation and its success in production."],"url":"http://arxiv.org/abs/2404.06686v1","category":"cs.CR"}
{"created":"2024-04-10 00:25:09","title":"Deep Generative Data Assimilation in Multimodal Setting","abstract":"Robust integration of physical knowledge and data is key to improve computational simulations, such as Earth system models. Data assimilation is crucial for achieving this goal because it provides a systematic framework to calibrate model outputs with observations, which can include remote sensing imagery and ground station measurements, with uncertainty quantification. Conventional methods, including Kalman filters and variational approaches, inherently rely on simplifying linear and Gaussian assumptions, and can be computationally expensive. Nevertheless, with the rapid adoption of data-driven methods in many areas of computational sciences, we see the potential of emulating traditional data assimilation with deep learning, especially generative models. In particular, the diffusion-based probabilistic framework has large overlaps with data assimilation principles: both allows for conditional generation of samples with a Bayesian inverse framework. These models have shown remarkable success in text-conditioned image generation or image-controlled video synthesis. Likewise, one can frame data assimilation as observation-conditioned state calibration. In this work, we propose SLAMS: Score-based Latent Assimilation in Multimodal Setting. Specifically, we assimilate in-situ weather station data and ex-situ satellite imagery to calibrate the vertical temperature profiles, globally. Through extensive ablation, we demonstrate that SLAMS is robust even in low-resolution, noisy, and sparse data settings. To our knowledge, our work is the first to apply deep generative framework for multimodal data assimilation using real-world datasets; an important step for building robust computational simulators, including the next-generation Earth system models. Our code is available at: https://github.com/yongquan-qu/SLAMS","sentences":["Robust integration of physical knowledge and data is key to improve computational simulations, such as Earth system models.","Data assimilation is crucial for achieving this goal because it provides a systematic framework to calibrate model outputs with observations, which can include remote sensing imagery and ground station measurements, with uncertainty quantification.","Conventional methods, including Kalman filters and variational approaches, inherently rely on simplifying linear and Gaussian assumptions, and can be computationally expensive.","Nevertheless, with the rapid adoption of data-driven methods in many areas of computational sciences, we see the potential of emulating traditional data assimilation with deep learning, especially generative models.","In particular, the diffusion-based probabilistic framework has large overlaps with data assimilation principles: both allows for conditional generation of samples with a Bayesian inverse framework.","These models have shown remarkable success in text-conditioned image generation or image-controlled video synthesis.","Likewise, one can frame data assimilation as observation-conditioned state calibration.","In this work, we propose SLAMS: Score-based Latent Assimilation in Multimodal Setting.","Specifically, we assimilate in-situ weather station data and ex-situ satellite imagery to calibrate the vertical temperature profiles, globally.","Through extensive ablation, we demonstrate that SLAMS is robust even in low-resolution, noisy, and sparse data settings.","To our knowledge, our work is the first to apply deep generative framework for multimodal data assimilation using real-world datasets; an important step for building robust computational simulators, including the next-generation Earth system models.","Our code is available at: https://github.com/yongquan-qu/SLAMS"],"url":"http://arxiv.org/abs/2404.06665v1","category":"cs.CV"}
{"created":"2024-04-10 00:05:55","title":"Efficient Denoising using Score Embedding in Score-based Diffusion Models","abstract":"It is well known that training a denoising score-based diffusion models requires tens of thousands of epochs and a substantial number of image data to train the model. In this paper, we propose to increase the efficiency in training score-based diffusion models. Our method allows us to decrease the number of epochs needed to train the diffusion model. We accomplish this by solving the log-density Fokker-Planck (FP) Equation numerically to compute the score \\textit{before} training. The pre-computed score is embedded into the image to encourage faster training under slice Wasserstein distance. Consequently, it also allows us to decrease the number of images we need to train the neural network to learn an accurate score. We demonstrate through our numerical experiments the improved performance of our proposed method compared to standard score-based diffusion models. Our proposed method achieves a similar quality to the standard method meaningfully faster.","sentences":["It is well known that training a denoising score-based diffusion models requires tens of thousands of epochs and a substantial number of image data to train the model.","In this paper, we propose to increase the efficiency in training score-based diffusion models.","Our method allows us to decrease the number of epochs needed to train the diffusion model.","We accomplish this by solving the log-density Fokker-Planck (FP) Equation numerically to compute the score \\textit{before} training.","The pre-computed score is embedded into the image to encourage faster training under slice Wasserstein distance.","Consequently, it also allows us to decrease the number of images we need to train the neural network to learn an accurate score.","We demonstrate through our numerical experiments the improved performance of our proposed method compared to standard score-based diffusion models.","Our proposed method achieves a similar quality to the standard method meaningfully faster."],"url":"http://arxiv.org/abs/2404.06661v1","category":"cs.CV"}
{"created":"2024-04-09 23:51:29","title":"Leveraging Interesting Facts to Enhance User Engagement with Conversational Interfaces","abstract":"Conversational Task Assistants (CTAs) guide users in performing a multitude of activities, such as making recipes. However, ensuring that interactions remain engaging, interesting, and enjoyable for CTA users is not trivial, especially for time-consuming or challenging tasks. Grounded in psychological theories of human interest, we propose to engage users with contextual and interesting statements or facts during interactions with a multi-modal CTA, to reduce fatigue and task abandonment before a task is complete. To operationalize this idea, we train a high-performing classifier (82% F1-score) to automatically identify relevant and interesting facts for users. We use it to create an annotated dataset of task-specific interesting facts for the domain of cooking. Finally, we design and validate a dialogue policy to incorporate the identified relevant and interesting facts into a conversation, to improve user engagement and task completion. Live testing on a leading multi-modal voice assistant shows that 66% of the presented facts were received positively, leading to a 40% gain in the user satisfaction rating, and a 37% increase in conversation length. These findings emphasize that strategically incorporating interesting facts into the CTA experience can promote real-world user participation for guided task interactions.","sentences":["Conversational Task Assistants (CTAs) guide users in performing a multitude of activities, such as making recipes.","However, ensuring that interactions remain engaging, interesting, and enjoyable for CTA users is not trivial, especially for time-consuming or challenging tasks.","Grounded in psychological theories of human interest, we propose to engage users with contextual and interesting statements or facts during interactions with a multi-modal CTA, to reduce fatigue and task abandonment before a task is complete.","To operationalize this idea, we train a high-performing classifier (82% F1-score) to automatically identify relevant and interesting facts for users.","We use it to create an annotated dataset of task-specific interesting facts for the domain of cooking.","Finally, we design and validate a dialogue policy to incorporate the identified relevant and interesting facts into a conversation, to improve user engagement and task completion.","Live testing on a leading multi-modal voice assistant shows that 66% of the presented facts were received positively, leading to a 40% gain in the user satisfaction rating, and a 37% increase in conversation length.","These findings emphasize that strategically incorporating interesting facts into the CTA experience can promote real-world user participation for guided task interactions."],"url":"http://arxiv.org/abs/2404.06659v1","category":"cs.CL"}
{"created":"2024-04-09 23:43:31","title":"Gersten's Injectivity for Smooth Algebras over Valuation Rings","abstract":"Gersten's injectivity conjecture for a functor $F$ of ``motivic type'', predicts that given a semilocal, ``non-singular'', integral domain $R$ with a fraction field $K$, the restriction morphism induces an injection of $F(R)$ inside $F(K)$. We prove two new cases of this conjecture for smooth algebras over valuation rings. Namely, we show that the higher algebraic $K$-groups of a semilocal, integral domain that is an essentially smooth algebra over an equicharacteristic valuation ring inject inside the same of its fraction field. Secondly, we show that Gersten's injectivity is true for smooth algebras over, possibly of mixed-characteristic, valuation rings in the case of torsors under tori and also in the case of the Brauer group.","sentences":["Gersten's injectivity conjecture for a functor $F$ of ``motivic type'', predicts that given a semilocal, ``non-singular'', integral domain $R$ with a fraction field $K$, the restriction morphism induces an injection of $F(R)$ inside $F(K)$. We prove two new cases of this conjecture for smooth algebras over valuation rings.","Namely, we show that the higher algebraic $K$-groups of a semilocal, integral domain that is an essentially smooth algebra over an equicharacteristic valuation ring inject inside the same of its fraction field.","Secondly, we show that Gersten's injectivity is true for smooth algebras over, possibly of mixed-characteristic, valuation rings in the case of torsors under tori and also in the case of the Brauer group."],"url":"http://arxiv.org/abs/2404.06655v1","category":"math.AG"}
{"created":"2024-04-09 22:56:05","title":"Constraints on atmospheric water abundance and cloud deck pressure in the warm Neptune GJ 3470 b via CARMENES transmission spectroscopy","abstract":"Observations of cooler atmospheres of super-Earths and Neptune sized objects often show flat transmission spectra. The most likely cause of this trend is the presence of aerosols (i.e. clouds and hazes) in the atmospheres of such objects. High-resolution spectroscopy provides an opportunity to test this hypothesis by targeting molecular species whose spectral line cores extend above the level of such opaque decks. In this work, we analyse high-resolution infrared observations of the warm Neptune GJ 3470 b taken over two transits using CARMENES (R $\\sim$ 80,000) and look for signatures of H$_2$O (previously detected using HST WFC3+Spitzer observations) in these transits with a custom pipeline fully accounting for the effects of data cleaning on any potential exoplanet signal. We find that our data are potentially able to weakly detect ($\\sim3\\sigma$) an injected signal equivalent to the best-fit model from previous HST WFC3+Spitzer observations. However, we do not make a significant detection using the actual observations. Using a Bayesian framework to simultaneously constrain the H$_2$O Volume Mixing Ratio (VMR) and the cloud top pressure level, we select a family of models compatible with the non detection. These are either very high VMR, cloud-free models, solar-abundance models with a high cloud deck, or sub-solar abundance models with a moderate cloud deck. This is a broader range compared to published results from low-resolution spectroscopy, but is also compatible with them at a 1$\\sigma$ level.","sentences":["Observations of cooler atmospheres of super-Earths and Neptune sized objects often show flat transmission spectra.","The most likely cause of this trend is the presence of aerosols (i.e. clouds and hazes) in the atmospheres of such objects.","High-resolution spectroscopy provides an opportunity to test this hypothesis by targeting molecular species whose spectral line cores extend above the level of such opaque decks.","In this work, we analyse high-resolution infrared observations of the warm Neptune GJ 3470 b taken over two transits using CARMENES (R $\\sim$ 80,000) and look for signatures of H$_2$O (previously detected using HST WFC3+Spitzer observations) in these transits with a custom pipeline fully accounting for the effects of data cleaning on any potential exoplanet signal.","We find that our data are potentially able to weakly detect ($\\sim3\\sigma$) an injected signal equivalent to the best-fit model from previous HST WFC3+Spitzer observations.","However, we do not make a significant detection using the actual observations.","Using a Bayesian framework to simultaneously constrain the H$_2$O Volume Mixing Ratio (VMR) and the cloud top pressure level, we select a family of models compatible with the non detection.","These are either very high VMR, cloud-free models, solar-abundance models with a high cloud deck, or sub-solar abundance models with a moderate cloud deck.","This is a broader range compared to published results from low-resolution spectroscopy, but is also compatible with them at a 1$\\sigma$ level."],"url":"http://arxiv.org/abs/2404.06648v1","category":"astro-ph.EP"}
{"created":"2024-04-09 22:03:39","title":"Perplexed: Understanding When Large Language Models are Confused","abstract":"Large Language Models (LLMs) have become dominant in the Natural Language Processing (NLP) field causing a huge surge in progress in a short amount of time. However, their limitations are still a mystery and have primarily been explored through tailored datasets to analyze a specific human-level skill such as negation, name resolution, etc. In this paper, we introduce perplexed, a library for exploring where a particular language model is perplexed. To show the flexibility and types of insights that can be gained by perplexed, we conducted a case study focused on LLMs for code generation using an additional tool we built to help with the analysis of code models called codetokenizer. Specifically, we explore success and failure cases at the token level of code LLMs under different scenarios pertaining to the type of coding structure the model is predicting, e.g., a variable name or operator, and how predicting of internal verses external method invocations impact performance. From this analysis, we found that our studied code LLMs had their worst performance on coding structures where the code was not syntactically correct. Additionally, we found the models to generally perform worse at predicting internal method invocations than external ones. We have open sourced both of these tools to allow the research community to better understand LLMs in general and LLMs for code generation.","sentences":["Large Language Models (LLMs) have become dominant in the Natural Language Processing (NLP) field causing a huge surge in progress in a short amount of time.","However, their limitations are still a mystery and have primarily been explored through tailored datasets to analyze a specific human-level skill such as negation, name resolution, etc.","In this paper, we introduce perplexed, a library for exploring where a particular language model is perplexed.","To show the flexibility and types of insights that can be gained by perplexed, we conducted a case study focused on LLMs for code generation using an additional tool we built to help with the analysis of code models called codetokenizer.","Specifically, we explore success and failure cases at the token level of code LLMs under different scenarios pertaining to the type of coding structure the model is predicting, e.g., a variable name or operator, and how predicting of internal verses external method invocations impact performance.","From this analysis, we found that our studied code LLMs had their worst performance on coding structures where the code was not syntactically correct.","Additionally, we found the models to generally perform worse at predicting internal method invocations than external ones.","We have open sourced both of these tools to allow the research community to better understand LLMs in general and LLMs for code generation."],"url":"http://arxiv.org/abs/2404.06634v1","category":"cs.SE"}
{"created":"2024-04-09 21:35:57","title":"Thermal Barrier Coatings in burner rig experiment analyzed through LAser Shock for DAmage Monitoring (LASDAM) method","abstract":"This study investigates failure mechanisms in a typical thermal barrier coating (TBC) system comprising an EB-PVD columnar top coat, an aluminide bond coat, and a Ni-based single crystal superalloy substrate, simulating gas turbine operating conditions using a burner rig. TBC degradation, initiated by interfacial defects from the LASAT method, was studied during thermal gradient cycling under fast and slow cooling. In-situ optical and infrared imaging, along with ex-situ SEM cross-sectional analysis, monitored failure mechanisms. The Laser Shock for Damage Monitoring (LASDAM) technique provided insights into gradient and cooling rate impacts on columnar TBC damage. Results showed significant effects of cooling rate on delamination and localized failure at blister sites, with LASDAM revealing significant overheating at damage sites. Analysis included full-field temperature and damage assessment, emphasizing blister-driven delamination under severe thermal gradients. Discussion focused on elastic stored energy effects, noting that fast cooling induced transient conditions where reversed temperature gradients increased damage, limiting TBC lifespan","sentences":["This study investigates failure mechanisms in a typical thermal barrier coating (TBC) system comprising an EB-PVD columnar top coat, an aluminide bond coat, and a Ni-based single crystal superalloy substrate, simulating gas turbine operating conditions using a burner rig.","TBC degradation, initiated by interfacial defects from the LASAT method, was studied during thermal gradient cycling under fast and slow cooling.","In-situ optical and infrared imaging, along with ex-situ SEM cross-sectional analysis, monitored failure mechanisms.","The Laser Shock for Damage Monitoring (LASDAM) technique provided insights into gradient and cooling rate impacts on columnar TBC damage.","Results showed significant effects of cooling rate on delamination and localized failure at blister sites, with LASDAM revealing significant overheating at damage sites.","Analysis included full-field temperature and damage assessment, emphasizing blister-driven delamination under severe thermal gradients.","Discussion focused on elastic stored energy effects, noting that fast cooling induced transient conditions where reversed temperature gradients increased damage, limiting TBC lifespan"],"url":"http://arxiv.org/abs/2404.06629v1","category":"physics.app-ph"}
{"created":"2024-04-09 20:52:55","title":"Figuring Out Gas & Galaxies In Enzo (FOGGIE) VIII: Complex and Stochastic Metallicity Gradients at z > 2","abstract":"Gas-phase metallicity gradients are a crucial element in understanding the chemical evolution of galaxies. We use the FOGGIE simulations to study the metallicity gradients ($\\nabla Z$) of six Milky Way-like galaxies throughout their evolution. FOGGIE galaxies generally exhibit steep negative gradients for most of their history, with only a few short-lived instances reaching positive slopes that appear to arise mainly from interactions with other galaxies. FOGGIE concurs with other simulation results but disagrees with the robust observational finding that flat and positive gradients are common at $z>1$. By tracking the metallicity gradient at a rapid cadence of simulation outputs ($\\sim 5$--10 Myr), we find that theoretical gradients are highly stochastic: the FOGGIE galaxies spend $\\sim 30-50$\\% of their time far away from a smoothed trajectory inferred from analytic models or other, less high-cadence simulations. This rapid variation makes instantaneous gradients from observations more difficult to interpret in terms of physical processes. Because of these geometric and stochastic complications, we explore non-parametric methods of quantifying the evolving metallicity distribution at $z > 1$. We investigate how efficiently non-parametric measures of the 2-D metallicity distribution respond to metal production and mixing. Our results suggest that new methods of quantifying and interpreting gas-phase metallicity will be needed to relate trends in upcoming high-$z$ {\\it JWST} observations with the underlying physics of gas accretion, expulsion, and recycling in early galaxies.","sentences":["Gas-phase metallicity gradients are a crucial element in understanding the chemical evolution of galaxies.","We use the FOGGIE simulations to study the metallicity gradients ($\\nabla Z$) of six Milky Way-like galaxies throughout their evolution.","FOGGIE galaxies generally exhibit steep negative gradients for most of their history, with only a few short-lived instances reaching positive slopes that appear to arise mainly from interactions with other galaxies.","FOGGIE concurs with other simulation results but disagrees with the robust observational finding that flat and positive gradients are common at $z>1$. By tracking the metallicity gradient at a rapid cadence of simulation outputs ($\\sim 5$--10 Myr), we find that theoretical gradients are highly stochastic: the FOGGIE galaxies spend $\\sim 30-50$\\% of their time far away from a smoothed trajectory inferred from analytic models or other, less high-cadence simulations.","This rapid variation makes instantaneous gradients from observations more difficult to interpret in terms of physical processes.","Because of these geometric and stochastic complications, we explore non-parametric methods of quantifying the evolving metallicity distribution at $z > 1$. We investigate how efficiently non-parametric measures of the 2-D metallicity distribution respond to metal production and mixing.","Our results suggest that new methods of quantifying and interpreting gas-phase metallicity will be needed to relate trends in upcoming high-$z$ {\\it JWST} observations with the underlying physics of gas accretion, expulsion, and recycling in early galaxies."],"url":"http://arxiv.org/abs/2404.06613v1","category":"astro-ph.GA"}
{"created":"2024-04-09 20:08:14","title":"A General Identification Algorithm For Data Fusion Problems Under Systematic Selection","abstract":"Causal inference is made challenging by confounding, selection bias, and other complications. A common approach to addressing these difficulties is the inclusion of auxiliary data on the superpopulation of interest. Such data may measure a different set of variables, or be obtained under different experimental conditions than the primary dataset. Analysis based on multiple datasets must carefully account for similarities between datasets, while appropriately accounting for differences.   In addition, selection of experimental units into different datasets may be systematic; similar difficulties are encountered in missing data problems. Existing methods for combining datasets either do not consider this issue, or assume simple selection mechanisms.   In this paper, we provide a general approach, based on graphical causal models, for causal inference from data on the same superpopulation that is obtained under different experimental conditions. Our framework allows both arbitrary unobserved confounding, and arbitrary selection processes into different experimental regimes in our data.   We describe how systematic selection processes may be organized into a hierarchy similar to censoring processes in missing data: selected completely at random (SCAR), selected at random (SAR), and selected not at random (SNAR). In addition, we provide a general identification algorithm for interventional distributions in this setting.","sentences":["Causal inference is made challenging by confounding, selection bias, and other complications.","A common approach to addressing these difficulties is the inclusion of auxiliary data on the superpopulation of interest.","Such data may measure a different set of variables, or be obtained under different experimental conditions than the primary dataset.","Analysis based on multiple datasets must carefully account for similarities between datasets, while appropriately accounting for differences.   ","In addition, selection of experimental units into different datasets may be systematic; similar difficulties are encountered in missing data problems.","Existing methods for combining datasets either do not consider this issue, or assume simple selection mechanisms.   ","In this paper, we provide a general approach, based on graphical causal models, for causal inference from data on the same superpopulation that is obtained under different experimental conditions.","Our framework allows both arbitrary unobserved confounding, and arbitrary selection processes into different experimental regimes in our data.   ","We describe how systematic selection processes may be organized into a hierarchy similar to censoring processes in missing data: selected completely at random (SCAR), selected at random (SAR), and selected not at random (SNAR).","In addition, we provide a general identification algorithm for interventional distributions in this setting."],"url":"http://arxiv.org/abs/2404.06602v1","category":"stat.ME"}
{"created":"2024-04-09 20:07:01","title":"On the Symmetry TFT of Yang-Mills-Chern-Simons theory","abstract":"Three-dimensional Yang-Mills-Chern-Simons theory has the peculiar property that its one-form symmetry defects have non-trivial braiding, namely they are charged under the same symmetry they generate, which is then anomalous. This poses a few puzzles in describing the corresponding Symmetry TFT in a four-dimensional bulk. First, the braiding between lines at the boundary seems to be ill-defined when such lines are pulled into the bulk. Second, the Symmetry TFT appears to be too trivial to allow for topological boundary conditions encoding all the different global variants. We show that both of these puzzles can be solved by including endable (tubular) surfaces in the class of bulk topological operators one has to consider. In this way, we are able to reproduce all global variants of the theory, with their symmetries and their anomalies. We check the validity of our proposal also against a top-down holographic realization of the same class of theories.","sentences":["Three-dimensional Yang-Mills-Chern-Simons theory has the peculiar property that its one-form symmetry defects have non-trivial braiding, namely they are charged under the same symmetry they generate, which is then anomalous.","This poses a few puzzles in describing the corresponding Symmetry TFT in a four-dimensional bulk.","First, the braiding between lines at the boundary seems to be ill-defined when such lines are pulled into the bulk.","Second, the Symmetry TFT appears to be too trivial to allow for topological boundary conditions encoding all the different global variants.","We show that both of these puzzles can be solved by including endable (tubular) surfaces in the class of bulk topological operators one has to consider.","In this way, we are able to reproduce all global variants of the theory, with their symmetries and their anomalies.","We check the validity of our proposal also against a top-down holographic realization of the same class of theories."],"url":"http://arxiv.org/abs/2404.06601v1","category":"hep-th"}
{"created":"2024-04-09 20:06:50","title":"Wavelet-based resolvent analysis of non-stationary flows","abstract":"This work introduces a formulation of resolvent analysis that uses wavelet transforms rather than Fourier transforms in time. Under this formulation, resolvent analysis may extend to turbulent flows with non-stationary mean states; the optimal resolvent modes are augmented with a temporal dimension and are able to encode the time-transient trajectories that are most amplified by the linearised Navier-Stokes equations. We first show that the wavelet- and Fourier-based resolvent analyses give equivalent results for statistically-stationary flow by applying them to turbulent channel flow. We then use wavelet-based resolvent analysis to study the transient growth mechanism in the logarithmic layer of a turbulent channel flow by windowing the resolvent operator in time and frequency. The computed principal resolvent response mode, i.e. the velocity field optimally amplified by the linearised dynamics of the flow, exhibits the Orr mechanism, supporting the claim that this mechanism is key to linear transient energy growth. We also apply this method to non-stationary parallel shear flows such as an oscillating boundary layer, and three-dimensional channel flow in which a sudden spanwise pressure gradient perturbs a fully-developed turbulent channel flow. In both cases, wavelet-based resolvent analysis yields modes that are sensitive to the changing mean profile of the flow. For the oscillating boundary layer, wavelet-based resolvent analysis produces oscillating principal forcing and response modes that peak at times and wall-normal locations associated with high turbulent activity. For the three-dimensional turbulent channel flow, the resolvent modes gradually realign themselves with the mean flow as it deviates.","sentences":["This work introduces a formulation of resolvent analysis that uses wavelet transforms rather than Fourier transforms in time.","Under this formulation, resolvent analysis may extend to turbulent flows with non-stationary mean states; the optimal resolvent modes are augmented with a temporal dimension and are able to encode the time-transient trajectories that are most amplified by the linearised Navier-Stokes equations.","We first show that the wavelet- and Fourier-based resolvent analyses give equivalent results for statistically-stationary flow by applying them to turbulent channel flow.","We then use wavelet-based resolvent analysis to study the transient growth mechanism in the logarithmic layer of a turbulent channel flow by windowing the resolvent operator in time and frequency.","The computed principal resolvent response mode, i.e. the velocity field optimally amplified by the linearised dynamics of the flow, exhibits the Orr mechanism, supporting the claim that this mechanism is key to linear transient energy growth.","We also apply this method to non-stationary parallel shear flows such as an oscillating boundary layer, and three-dimensional channel flow in which a sudden spanwise pressure gradient perturbs a fully-developed turbulent channel flow.","In both cases, wavelet-based resolvent analysis yields modes that are sensitive to the changing mean profile of the flow.","For the oscillating boundary layer, wavelet-based resolvent analysis produces oscillating principal forcing and response modes that peak at times and wall-normal locations associated with high turbulent activity.","For the three-dimensional turbulent channel flow, the resolvent modes gradually realign themselves with the mean flow as it deviates."],"url":"http://arxiv.org/abs/2404.06600v1","category":"physics.flu-dyn"}
{"created":"2024-04-09 19:33:05","title":"Leveraging Latents for Efficient Thermography Classification and Segmentation","abstract":"Breast cancer is a prominent health concern worldwide, currently being the secondmost common and second-deadliest type of cancer in women. While current breast cancer diagnosis mainly relies on mammography imaging, in recent years the use of thermography for breast cancer imaging has been garnering growing popularity. Thermographic imaging relies on infrared cameras to capture body-emitted heat distributions. While these heat signatures have proven useful for computer-vision systems for accurate breast cancer segmentation and classification, prior work often relies on handcrafted feature engineering or complex architectures, potentially limiting the comparability and applicability of these methods. In this work, we present a novel algorithm for both breast cancer classification and segmentation. Rather than focusing efforts on manual feature and architecture engineering, our algorithm focuses on leveraging an informative, learned feature space, thus making our solution simpler to use and extend to other frameworks and downstream tasks, as well as more applicable to data-scarce settings. Our classification produces SOTA results, while we are the first work to produce segmentation regions studied in this paper.","sentences":["Breast cancer is a prominent health concern worldwide, currently being the secondmost common and second-deadliest type of cancer in women.","While current breast cancer diagnosis mainly relies on mammography imaging, in recent years the use of thermography for breast cancer imaging has been garnering growing popularity.","Thermographic imaging relies on infrared cameras to capture body-emitted heat distributions.","While these heat signatures have proven useful for computer-vision systems for accurate breast cancer segmentation and classification, prior work often relies on handcrafted feature engineering or complex architectures, potentially limiting the comparability and applicability of these methods.","In this work, we present a novel algorithm for both breast cancer classification and segmentation.","Rather than focusing efforts on manual feature and architecture engineering, our algorithm focuses on leveraging an informative, learned feature space, thus making our solution simpler to use and extend to other frameworks and downstream tasks, as well as more applicable to data-scarce settings.","Our classification produces SOTA results, while we are the first work to produce segmentation regions studied in this paper."],"url":"http://arxiv.org/abs/2404.06589v1","category":"eess.IV"}
{"created":"2024-04-09 19:26:25","title":"Penney's game for permutations","abstract":"We consider the permutation analogue of Penney's game for words. Two players, in order, each choose a permutation of length $k\\ge3$; then a sequence of independent random values from a continuous distribution is generated, until the relative order of the last $k$ numbers coincides with one of the chosen permutations, making that player the winner.   We compute the winning probabilities for all pairs of permutations of length 3 and some pairs of length 4, showing that, as in the original version for words, the game is non-transitive. Our proofs introduce new bijections for consecutive patterns in permutations. We also give some formulas to compute the winning probabilities more generally, and conjecture a winning strategy for the second player when $k$ is arbitrary.","sentences":["We consider the permutation analogue of Penney's game for words.","Two players, in order, each choose a permutation of length $k\\ge3$; then a sequence of independent random values from a continuous distribution is generated, until the relative order of the last $k$ numbers coincides with one of the chosen permutations, making that player the winner.   ","We compute the winning probabilities for all pairs of permutations of length 3 and some pairs of length 4, showing that, as in the original version for words, the game is non-transitive.","Our proofs introduce new bijections for consecutive patterns in permutations.","We also give some formulas to compute the winning probabilities more generally, and conjecture a winning strategy for the second player when $k$ is arbitrary."],"url":"http://arxiv.org/abs/2404.06585v1","category":"math.CO"}
{"created":"2024-04-09 18:54:18","title":"Bootstrapping conformal defect operators on a line","abstract":"We study a conformal field theory with cubic anisotropic symmetry in presence of a line defect. We compute the correlators of the low lying defect operators using Feynman diagrams and derive explicit expressions for the two, three and four point defect correlators at the cubic fixed point in $4-\\epsilon$ dimensions to $O(\\epsilon)$. We also compute the defect $g$-function for this setup and demonstrate that this is in agreement with the $g$-theorem, which states that the $g$-function is monotonic under the renormalisation group flow along the defect. Next, we focus on conformal bootstrap techniques to determine the CFT data associated with the defect operators, which is the main objective of the paper. We utilize the framework of crossing symmetric Polyakov bootstrap and compute the averaged CFT data to $O(\\epsilon)$ up to a finite number of ambiguities. We unmix the CFT data for the double trace operators at $O(\\epsilon)$ and use this to compute the $O(\\epsilon^2)$ data. Finally, we study these defect correlators non-perturbatively using numerical methods and isolate them near the free theory limit close to four dimensions.","sentences":["We study a conformal field theory with cubic anisotropic symmetry in presence of a line defect.","We compute the correlators of the low lying defect operators using Feynman diagrams and derive explicit expressions for the two, three and four point defect correlators at the cubic fixed point in $4-\\epsilon$ dimensions to $O(\\epsilon)$. We also compute the defect $g$-function for this setup and demonstrate that this is in agreement with the $g$-theorem, which states that the $g$-function is monotonic under the renormalisation group flow along the defect.","Next, we focus on conformal bootstrap techniques to determine the CFT data associated with the defect operators, which is the main objective of the paper.","We utilize the framework of crossing symmetric Polyakov bootstrap and compute the averaged CFT data to $O(\\epsilon)$ up to a finite number of ambiguities.","We unmix the CFT data for the double trace operators at $O(\\epsilon)$ and use this to compute the $O(\\epsilon^2)$ data.","Finally, we study these defect correlators non-perturbatively using numerical methods and isolate them near the free theory limit close to four dimensions."],"url":"http://arxiv.org/abs/2404.06576v1","category":"hep-th"}
{"created":"2024-04-09 18:52:30","title":"The Tensor-Train Stochastic Finite Volume Method for Uncertainty Quantification","abstract":"The stochastic finite volume method offers an efficient one-pass approach for assessing uncertainty in hyperbolic conservation laws. Still, it struggles with the curse of dimensionality when dealing with multiple stochastic variables. We introduce the stochastic finite volume method within the tensor-train framework to counteract this limitation. This integration, however, comes with its own set of difficulties, mainly due to the propensity for shock formation in hyperbolic systems. To overcome these issues, we have developed a tensor-train-adapted stochastic finite volume method that employs a global WENO reconstruction, making it suitable for such complex systems. This approach represents the first step in designing tensor-train techniques for hyperbolic systems and conservation laws involving shocks.","sentences":["The stochastic finite volume method offers an efficient one-pass approach for assessing uncertainty in hyperbolic conservation laws.","Still, it struggles with the curse of dimensionality when dealing with multiple stochastic variables.","We introduce the stochastic finite volume method within the tensor-train framework to counteract this limitation.","This integration, however, comes with its own set of difficulties, mainly due to the propensity for shock formation in hyperbolic systems.","To overcome these issues, we have developed a tensor-train-adapted stochastic finite volume method that employs a global WENO reconstruction, making it suitable for such complex systems.","This approach represents the first step in designing tensor-train techniques for hyperbolic systems and conservation laws involving shocks."],"url":"http://arxiv.org/abs/2404.06574v1","category":"math.NA"}
{"created":"2024-04-09 18:30:49","title":"Confidence Intervals on Multivariate Normal Quantiles for Environmental Specification Development in Multi-axis Shock and Vibration Testing","abstract":"This article describes two Monte Carlo methods for calculating confidence intervals on cumulative density function (CDF) based multivariate normal quantiles that allows for controlling the tail regions of a multivariate distribution where one is most concerned about extreme responses. The CDF based multivariate normal quantiles associated with bivariate distributions are represented as contours and for trivariate distributions represented as iso-surfaces. We first provide a novel methodology for an inverse problem, characterizing the uncertainty on the $\\tau^{\\mathrm{th}}$ multivariate quantile probability, when using concurrent univariate quantile probabilities. The uncertainty on the $\\tau^{\\mathrm{th}}$ multivariate quantile probability demonstrates inadequacy in univariate methods which neglect correlation between multiple variates. Limitations of traditional multivariate normal tolerance regions and simultaneous univariate tolerance methods are discussed thereby necessitating the need for confidence intervals on CDF based multivariate normal quantiles. Two Monte Carlo methods are discussed; the first calculates the CDF over a tessellated domain followed by taking a bootstrap confidence interval over the tessellated CDF. The CDF based multivariate quantiles are then estimated from the CDF confidence intervals. For the second method, only the point associated with highest probability density along the CDF based quantile is calculated, which greatly improves the computational speed compared to the first method. Monte Carlo simulation studies are used to assess the performance of the various methods. Finally, real data analysis is performed to illustrate a workflow for CDF based multivariate normal quantiles in the domain of mechanical shock and vibration to specify a minimum conservative test level for environmental specification.","sentences":["This article describes two Monte Carlo methods for calculating confidence intervals on cumulative density function (CDF) based multivariate normal quantiles that allows for controlling the tail regions of a multivariate distribution where one is most concerned about extreme responses.","The CDF based multivariate normal quantiles associated with bivariate distributions are represented as contours and for trivariate distributions represented as iso-surfaces.","We first provide a novel methodology for an inverse problem, characterizing the uncertainty on the $\\tau^{\\mathrm{th}}$ multivariate quantile probability, when using concurrent univariate quantile probabilities.","The uncertainty on the $\\tau^{\\mathrm{th}}$ multivariate quantile probability demonstrates inadequacy in univariate methods which neglect correlation between multiple variates.","Limitations of traditional multivariate normal tolerance regions and simultaneous univariate tolerance methods are discussed thereby necessitating the need for confidence intervals on CDF based multivariate normal quantiles.","Two Monte Carlo methods are discussed; the first calculates the CDF over a tessellated domain followed by taking a bootstrap confidence interval over the tessellated CDF.","The CDF based multivariate quantiles are then estimated from the CDF confidence intervals.","For the second method, only the point associated with highest probability density along the CDF based quantile is calculated, which greatly improves the computational speed compared to the first method.","Monte Carlo simulation studies are used to assess the performance of the various methods.","Finally, real data analysis is performed to illustrate a workflow for CDF based multivariate normal quantiles in the domain of mechanical shock and vibration to specify a minimum conservative test level for environmental specification."],"url":"http://arxiv.org/abs/2404.06565v1","category":"stat.ME"}
{"created":"2024-04-09 18:27:59","title":"Demonstration of MaskSearch: Efficiently Querying Image Masks for Machine Learning Workflows","abstract":"We demonstrate MaskSearch, a system designed to accelerate queries over databases of image masks generated by machine learning models. MaskSearch formalizes and accelerates a new category of queries for retrieving images and their corresponding masks based on mask properties, which support various applications, from identifying spurious correlations learned by models to exploring discrepancies between model saliency and human attention. This demonstration makes the following contributions:(1) the introduction of MaskSearch's graphical user interface (GUI), which enables interactive exploration of image databases through mask properties, (2) hands-on opportunities for users to explore MaskSearch's capabilities and constraints within machine learning workflows, and (3) an opportunity for conference attendees to understand how MaskSearch accelerates queries over image masks.","sentences":["We demonstrate MaskSearch, a system designed to accelerate queries over databases of image masks generated by machine learning models.","MaskSearch formalizes and accelerates a new category of queries for retrieving images and their corresponding masks based on mask properties, which support various applications, from identifying spurious correlations learned by models to exploring discrepancies between model saliency and human attention.","This demonstration makes the following contributions:(1) the introduction of MaskSearch's graphical user interface (GUI), which enables interactive exploration of image databases through mask properties, (2) hands-on opportunities for users to explore MaskSearch's capabilities and constraints within machine learning workflows, and (3) an opportunity for conference attendees to understand how MaskSearch accelerates queries over image masks."],"url":"http://arxiv.org/abs/2404.06563v1","category":"cs.DB"}
{"created":"2024-04-09 18:23:34","title":"The Impact of Print-and-Scan in Heterogeneous Morph Evaluation Scenarios","abstract":"Face morphing attacks present an emerging threat to the face recognition system. On top of that, printing and scanning the morphed images could obscure the artifacts generated during the morphing process, which makes morphed image detection even harder. In this work, we investigate the impact that printing and scanning has on morphing attacks through a series of heterogeneous tests. Our experiments show that we can increase the possibility of a false match by up to 5.64% for DiM and 16.00% for StyleGAN2 when providing an image that has been printed and scanned, regardless it is morphed or bona fide, to a Face Recognition (FR) system. Likewise, using Frechet Inception Distance (FID) metric, strictly print-scanned morph attacks performed on average 9.185% stronger than non-print-scanned digital morphs.","sentences":["Face morphing attacks present an emerging threat to the face recognition system.","On top of that, printing and scanning the morphed images could obscure the artifacts generated during the morphing process, which makes morphed image detection even harder.","In this work, we investigate the impact that printing and scanning has on morphing attacks through a series of heterogeneous tests.","Our experiments show that we can increase the possibility of a false match by up to 5.64% for DiM and 16.00% for StyleGAN2 when providing an image that has been printed and scanned, regardless it is morphed or bona fide, to a Face Recognition (FR) system.","Likewise, using Frechet Inception Distance (FID) metric, strictly print-scanned morph attacks performed on average 9.185% stronger than non-print-scanned digital morphs."],"url":"http://arxiv.org/abs/2404.06559v1","category":"cs.CV"}
{"created":"2024-04-09 18:05:29","title":"Modeling Analog-Digital-Converter Energy and Area for Compute-In-Memory Accelerator Design","abstract":"Analog Compute-in-Memory (CiM) accelerators use analog-digital converters (ADCs) to read the analog values that they compute. ADCs can consume significant energy and area, so architecture-level ADC decisions such as ADC resolution or number of ADCs can significantly impact overall CiM accelerator energy and area. Therefore, modeling how architecture-level decisions affect ADC energy and area is critical for performing architecture-level design space exploration of CiM accelerators.   This work presents an open-source architecture-level model to estimate ADC energy and area. To enable fast design space exploration, the model uses only architecture-level attributes while abstracting circuit-level details. Our model enables researchers to quickly and easily model key architecture-level tradeoffs in accelerators that use ADCs.","sentences":["Analog Compute-in-Memory (CiM) accelerators use analog-digital converters (ADCs) to read the analog values that they compute.","ADCs can consume significant energy and area, so architecture-level ADC decisions such as ADC resolution or number of ADCs can significantly impact overall CiM accelerator energy and area.","Therefore, modeling how architecture-level decisions affect ADC energy and area is critical for performing architecture-level design space exploration of CiM accelerators.   ","This work presents an open-source architecture-level model to estimate ADC energy and area.","To enable fast design space exploration, the model uses only architecture-level attributes while abstracting circuit-level details.","Our model enables researchers to quickly and easily model key architecture-level tradeoffs in accelerators that use ADCs."],"url":"http://arxiv.org/abs/2404.06553v1","category":"cs.AR"}
{"created":"2024-04-09 18:02:01","title":"Variational Stochastic Gradient Descent for Deep Neural Networks","abstract":"Optimizing deep neural networks is one of the main tasks in successful deep learning. Current state-of-the-art optimizers are adaptive gradient-based optimization methods such as Adam. Recently, there has been an increasing interest in formulating gradient-based optimizers in a probabilistic framework for better estimation of gradients and modeling uncertainties. Here, we propose to combine both approaches, resulting in the Variational Stochastic Gradient Descent (VSGD) optimizer. We model gradient updates as a probabilistic model and utilize stochastic variational inference (SVI) to derive an efficient and effective update rule. Further, we show how our VSGD method relates to other adaptive gradient-based optimizers like Adam. Lastly, we carry out experiments on two image classification datasets and four deep neural network architectures, where we show that VSGD outperforms Adam and SGD.","sentences":["Optimizing deep neural networks is one of the main tasks in successful deep learning.","Current state-of-the-art optimizers are adaptive gradient-based optimization methods such as Adam.","Recently, there has been an increasing interest in formulating gradient-based optimizers in a probabilistic framework for better estimation of gradients and modeling uncertainties.","Here, we propose to combine both approaches, resulting in the Variational Stochastic Gradient Descent (VSGD) optimizer.","We model gradient updates as a probabilistic model and utilize stochastic variational inference (SVI) to derive an efficient and effective update rule.","Further, we show how our VSGD method relates to other adaptive gradient-based optimizers like Adam.","Lastly, we carry out experiments on two image classification datasets and four deep neural network architectures, where we show that VSGD outperforms Adam and SGD."],"url":"http://arxiv.org/abs/2404.06549v1","category":"cs.LG"}
{"created":"2024-04-09 18:00:30","title":"Ladder Symmetries and Love Numbers of Reissner--Nordstr\u00f6m Black Holes","abstract":"It is well known that asymptotically flat black holes in general relativity have vanishing tidal Love numbers. In the case of Schwarzschild and Kerr black holes, this property has been shown to be a consequence of a hidden structure of ladder symmetries for the perturbations. In this work, we extend the ladder symmetries to non-rotating charged black holes in general relativity. As opposed to previous works in this context, we adopt a more general definition of Love numbers, including quadratic operators that mix gravitational and electromagnetic perturbations in the point-particle effective field theory. We show that the calculation of a subset of those couplings in full general relativity is affected by an ambiguity in the split between source and response, which we resolve through an analytic continuation. As a result, we derive a novel master equation that unifies scalar, electromagnetic and gravitational perturbations around Reissner--Nordstr\\\"om black holes. The equation is hypergeometric and can be obtained from previous formulations via nontrivial field redefinitions, which allow to systematically remove some of the singularities and make the presence of the ladder symmetries more manifest.","sentences":["It is well known that asymptotically flat black holes in general relativity have vanishing tidal Love numbers.","In the case of Schwarzschild and Kerr black holes, this property has been shown to be a consequence of a hidden structure of ladder symmetries for the perturbations.","In this work, we extend the ladder symmetries to non-rotating charged black holes in general relativity.","As opposed to previous works in this context, we adopt a more general definition of Love numbers, including quadratic operators that mix gravitational and electromagnetic perturbations in the point-particle effective field theory.","We show that the calculation of a subset of those couplings in full general relativity is affected by an ambiguity in the split between source and response, which we resolve through an analytic continuation.","As a result, we derive a novel master equation that unifies scalar, electromagnetic and gravitational perturbations around Reissner--Nordstr\\\"om black holes.","The equation is hypergeometric and can be obtained from previous formulations via nontrivial field redefinitions, which allow to systematically remove some of the singularities and make the presence of the ladder symmetries more manifest."],"url":"http://arxiv.org/abs/2404.06544v1","category":"gr-qc"}
{"created":"2024-04-09 18:00:03","title":"Exploring the nature of dark matter with the extreme galaxy AGC 114905","abstract":"AGC 114905 is a dwarf gas-rich ultra-diffuse galaxy seemingly in tension with the cold dark matter (CDM) model. Specifically, the galaxy appears to have an extremely low-density halo and a high baryon fraction, while CDM predicts dwarfs to have very dense and dominant dark haloes. The alleged tension relies on the galaxy's rotation curve decomposition, which depends heavily on its inclination. This inclination, estimated from the gas morphology, remains somewhat uncertain. We present unmatched ultra-deep optical imaging of AGC 114905 reaching surface brightness limits $\\mu_{\\rm r,lim} \\approx 32$ mag/arcsec$^2$ ($3\\sigma$; 10 arcsec $\\times$ 10 arcsec) obtained with the 10.4-m Gran Telescopio Canarias. With the new imaging, we characterise the galaxy's morphology, surface brightness, colours, and stellar mass profiles in great detail. The stellar disc has a similar extent as the gas, presents spiral arms-like features, and shows a well-defined edge. Stars and gas share similar morphology, and crucially, we find an inclination of $31\\pm2^\\circ$, in agreement with the previous determinations. We revisit the rotation curve decomposition of the galaxy, and we explore different mass models in the context of CDM, self-interacting dark matter (SIDM), fuzzy dark matter (FDM) or Modified Newtonian Dynamics (MOND). We find that the latter does not fit the circular speed of the galaxy, while CDM only does so with dark halo parameters rarely seen in cosmological simulations. Within the uncertainties, SIDM and FDM remain feasible candidates to explain the observed kinematics of AGC 114905.","sentences":["AGC 114905 is a dwarf gas-rich ultra-diffuse galaxy seemingly in tension with the cold dark matter (CDM) model.","Specifically, the galaxy appears to have an extremely low-density halo and a high baryon fraction, while CDM predicts dwarfs to have very dense and dominant dark haloes.","The alleged tension relies on the galaxy's rotation curve decomposition, which depends heavily on its inclination.","This inclination, estimated from the gas morphology, remains somewhat uncertain.","We present unmatched ultra-deep optical imaging of AGC 114905 reaching surface brightness limits $\\mu_{\\rm r,lim} \\approx 32$ mag/arcsec$^2$ ($3\\sigma$; 10 arcsec $\\times$ 10 arcsec) obtained with the 10.4-m Gran Telescopio Canarias.","With the new imaging, we characterise the galaxy's morphology, surface brightness, colours, and stellar mass profiles in great detail.","The stellar disc has a similar extent as the gas, presents spiral arms-like features, and shows a well-defined edge.","Stars and gas share similar morphology, and crucially, we find an inclination of $31\\pm2^\\circ$, in agreement with the previous determinations.","We revisit the rotation curve decomposition of the galaxy, and we explore different mass models in the context of CDM, self-interacting dark matter (SIDM), fuzzy dark matter (FDM) or Modified Newtonian Dynamics (MOND).","We find that the latter does not fit the circular speed of the galaxy, while CDM only does so with dark halo parameters rarely seen in cosmological simulations.","Within the uncertainties, SIDM and FDM remain feasible candidates to explain the observed kinematics of AGC 114905."],"url":"http://arxiv.org/abs/2404.06537v1","category":"astro-ph.GA"}
{"created":"2024-04-09 18:00:01","title":"Implications of $B \\to K \u03bd\\bar\u03bd$ under Rank-One Flavor Violation hypothesis","abstract":"We study the implications of the observed excess in $B^+ \\to K^+ \\nu \\bar{\\nu}$ under the assumption of Rank-One Flavour Violation, i.e. that New Physics couples to a single specific direction in flavour space. By varying this direction we perform analyses at the level of the low-energy EFT, the SMEFT, and with explicit mediators such as leptoquarks and colorless vectors ($Z^\\prime$ and $V^\\prime$). We study correlations with other flavour, electroweak and collider observables, finding that the most interesting ones are with $K \\to \\pi \\nu \\bar{\\nu}$, $B_s \\to \\mu^+ \\mu^-$, meson mixing and the LHC searches in $\\tau^+ \\tau^-$ high-energy tails. Among the various mediators, the scalar leptoquarks $\\tilde{R}_2$ and $S_1$ offer the best fits of the Belle-II excess, while being consistent with the other bounds. On the other hand, colorless vectors are strongly constrained by meson mixing and resonance searches in $p p \\to \\tau^+ \\tau^-$. In all cases we find that a flavour alignment close to the third generation is generically preferred.","sentences":["We study the implications of the observed excess in $B^+ \\to K^+ \\nu \\bar{\\nu}$ under the assumption of Rank-One Flavour Violation, i.e. that New Physics couples to a single specific direction in flavour space.","By varying this direction we perform analyses at the level of the low-energy EFT, the SMEFT, and with explicit mediators such as leptoquarks and colorless vectors ($Z^\\prime$ and $V^\\prime$).","We study correlations with other flavour, electroweak and collider observables, finding that the most interesting ones are with $K \\to \\pi \\nu \\bar{\\nu}$, $B_s \\to \\mu^+ \\mu^-$, meson mixing and the LHC searches in $\\tau^+ \\tau^-$ high-energy tails.","Among the various mediators, the scalar leptoquarks $\\tilde{R}_2$ and $S_1$ offer the best fits of the Belle-II excess, while being consistent with the other bounds.","On the other hand, colorless vectors are strongly constrained by meson mixing and resonance searches in $p p \\to \\tau^+ \\tau^-$.","In all cases we find that a flavour alignment close to the third generation is generically preferred."],"url":"http://arxiv.org/abs/2404.06533v1","category":"hep-ph"}
{"created":"2024-04-09 18:00:01","title":"Diagnosing local minima and accelerating convergence of variational quantum eigensolvers with quantum subspace techniques","abstract":"Recent research has shown that wavefunction evolution in real- and imaginary-time can generate quantum subspaces with significant utility for obtaining accurate ground state energies. Inspired by these methods, we propose combining quantum subspace techniques with the variational quantum eigensolver (VQE). In our approach, the parameterized quantum circuit is divided into a series of smaller subcircuits. The sequential application of these subcircuits to an initial state generates a set of wavefunctions that we use as a quantum subspace to obtain high-accuracy groundstate energies. We call this technique the circuit subspace variational quantum eigensolver (CSVQE) algorithm. By benchmarking CSVQE on a range of quantum chemistry problems, we show that it can achieve significant error reduction compared to conventional VQE, particularly for poorly optimized circuits, greatly improving convergence rates. Furthermore, we demonstrate that when applied to circuits trapped at a local minima, CSVQE can produce energies close to the global minimum of the energy landscape, making it a potentially powerful tool for diagnosing local minima.","sentences":["Recent research has shown that wavefunction evolution in real- and imaginary-time can generate quantum subspaces with significant utility for obtaining accurate ground state energies.","Inspired by these methods, we propose combining quantum subspace techniques with the variational quantum eigensolver (VQE).","In our approach, the parameterized quantum circuit is divided into a series of smaller subcircuits.","The sequential application of these subcircuits to an initial state generates a set of wavefunctions that we use as a quantum subspace to obtain high-accuracy groundstate energies.","We call this technique the circuit subspace variational quantum eigensolver (CSVQE) algorithm.","By benchmarking CSVQE on a range of quantum chemistry problems, we show that it can achieve significant error reduction compared to conventional VQE, particularly for poorly optimized circuits, greatly improving convergence rates.","Furthermore, we demonstrate that when applied to circuits trapped at a local minima, CSVQE can produce energies close to the global minimum of the energy landscape, making it a potentially powerful tool for diagnosing local minima."],"url":"http://arxiv.org/abs/2404.06534v1","category":"quant-ph"}
{"created":"2024-04-09 17:59:55","title":"Disentangling transitions in topological order induced by boundary decoherence","abstract":"We study the entanglement structure of topological orders subject to decoherence on the bipartition boundary. Focusing on the toric codes in $d$ space dimensions for $d=2,3,4$, we explore whether the boundary decoherence may be able to induce a disentangling transition, characterized by the destruction of mixed-state long-range entanglement across the bipartition, measured by topological entanglement negativity. A key insight of our approach is the connection between the negativity spectrum of the decohered mixed states and emergent symmetry-protected topological orders under certain symmetry-preserving perturbation localized on the bipartition boundary. This insight allows us to analytically derive the exact results of entanglement negativity without using a replica trick.","sentences":["We study the entanglement structure of topological orders subject to decoherence on the bipartition boundary.","Focusing on the toric codes in $d$ space dimensions for $d=2,3,4$, we explore whether the boundary decoherence may be able to induce a disentangling transition, characterized by the destruction of mixed-state long-range entanglement across the bipartition, measured by topological entanglement negativity.","A key insight of our approach is the connection between the negativity spectrum of the decohered mixed states and emergent symmetry-protected topological orders under certain symmetry-preserving perturbation localized on the bipartition boundary.","This insight allows us to analytically derive the exact results of entanglement negativity without using a replica trick."],"url":"http://arxiv.org/abs/2404.06514v1","category":"quant-ph"}
{"created":"2024-04-09 17:57:29","title":"On the Effect of (Near) Duplicate Subwords in Language Modelling","abstract":"Tokenisation is a core part of language models (LMs). It involves splitting a character sequence into subwords which are assigned arbitrary indices before being served to the LM. While typically lossless, however, this process may lead to less sample efficient LM training: as it removes character-level information, it could make it harder for LMs to generalise across similar subwords, such as now and Now. We refer to such subwords as near duplicates. In this paper, we study the impact of near duplicate subwords on LM training efficiency. First, we design an experiment that gives us an upper bound to how much we should expect a model to improve if we could perfectly generalise across near duplicates. We do this by duplicating each subword in our LM's vocabulary, creating perfectly equivalent classes of subwords. Experimentally, we find that LMs need roughly 17% more data when trained in a fully duplicated setting. Second, we investigate the impact of naturally occurring near duplicates on LMs. Here, we see that merging them considerably hurts LM performance. Therefore, although subword duplication negatively impacts LM training efficiency, naturally occurring near duplicates may not be as similar as anticipated, limiting the potential for performance improvements.","sentences":["Tokenisation is a core part of language models (LMs).","It involves splitting a character sequence into subwords which are assigned arbitrary indices before being served to the LM.","While typically lossless, however, this process may lead to less sample efficient LM training: as it removes character-level information, it could make it harder for LMs to generalise across similar subwords, such as now and Now.","We refer to such subwords as near duplicates.","In this paper, we study the impact of near duplicate subwords on LM training efficiency.","First, we design an experiment that gives us an upper bound to how much we should expect a model to improve if we could perfectly generalise across near duplicates.","We do this by duplicating each subword in our LM's vocabulary, creating perfectly equivalent classes of subwords.","Experimentally, we find that LMs need roughly 17% more data when trained in a fully duplicated setting.","Second, we investigate the impact of naturally occurring near duplicates on LMs.","Here, we see that merging them considerably hurts LM performance.","Therefore, although subword duplication negatively impacts LM training efficiency, naturally occurring near duplicates may not be as similar as anticipated, limiting the potential for performance improvements."],"url":"http://arxiv.org/abs/2404.06508v1","category":"cs.CL"}
{"created":"2024-04-09 17:54:10","title":"Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?","abstract":"Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.","sentences":["Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note.","A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology.","The relevant information can then be extracted and organized according to the structure of the SOAP note.","In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency.","The first approach generates the sections independently, while the second method generates them all together.","In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric.","We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators.","Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively.","With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics.","This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections."],"url":"http://arxiv.org/abs/2404.06503v1","category":"cs.CL"}
{"created":"2024-04-09 17:52:51","title":"Detection of Rydberg lines from the atmosphere of Betelgeuse","abstract":"Emission lines from Rydberg transitions are detected for the first time from a region close to the surface of Betelgeuse. The H30${\\alpha}$ line is observed at 231.905 GHz, with a FWHM ~42 km/s and extended wings. A second line at 232.025 GHz (FWHM ~21 km/s), is modeled as a combination of Rydberg transitions of abundant low First Ionization Potential metals. Both H30${\\alpha}$ and the Rydberg combined line X30${\\alpha}$ are fitted by Voigt profiles, and collisional broadening with electrons may be partly responsible for the Lorentzian contribution, indicating electron densities of a few 10$^8$cm$^{-3}$. X30${\\alpha}$ is located in a relatively smooth ring at a projected radius of 0.9x the optical photospheric radius R$_*$, whereas H30${\\alpha}$ is more clumpy, reaching a peak at ~1.4R$_*$. We use a semi-empirical thermodynamic atmospheric model of Betelgeuse to compute the 232 GHz (1.29mm) continuum and line profiles making simple assumptions. Photoionized abundant metals dominate the electron density and the predicted surface of continuum optical depth unity at 232 GHz occurs at ~1.3R$_*$, in good agreement with observations. Assuming a Saha-Boltzmann distribution for the level populations of Mg, Si, and Fe, the model predicts that the X30${\\alpha}$ emission arises in a region of radially-increasing temperature and turbulence. Inclusion of ionized C and non-LTE effects could modify the integrated fluxes and location of emission. These simulations confirm the identity of the Rydberg transition lines observed towards Betelgeuse, and reveal that such diagnostics can improve future atmospheric models.","sentences":["Emission lines from Rydberg transitions are detected for the first time from a region close to the surface of Betelgeuse.","The H30${\\alpha}$ line is observed at 231.905 GHz, with a FWHM ~42 km/s and extended wings.","A second line at 232.025 GHz (FWHM ~21 km/s), is modeled as a combination of Rydberg transitions of abundant low First Ionization Potential metals.","Both H30${\\alpha}$ and the Rydberg combined line X30${\\alpha}$ are fitted by Voigt profiles, and collisional broadening with electrons may be partly responsible for the Lorentzian contribution, indicating electron densities of a few 10$^8$cm$^{-3}$. X30${\\alpha}$ is located in a relatively smooth ring at a projected radius of 0.9x the optical photospheric radius R$_*$, whereas H30${\\alpha}$ is more clumpy, reaching a peak at ~1.4R$_*$.","We use a semi-empirical thermodynamic atmospheric model of Betelgeuse to compute the 232 GHz (1.29mm) continuum and line profiles making simple assumptions.","Photoionized abundant metals dominate the electron density and the predicted surface of continuum optical depth unity at 232 GHz occurs at ~1.3R$_*$, in good agreement with observations.","Assuming a Saha-Boltzmann distribution for the level populations of Mg, Si, and Fe, the model predicts that the X30${\\alpha}$ emission arises in a region of radially-increasing temperature and turbulence.","Inclusion of ionized C and non-LTE effects could modify the integrated fluxes and location of emission.","These simulations confirm the identity of the Rydberg transition lines observed towards Betelgeuse, and reveal that such diagnostics can improve future atmospheric models."],"url":"http://arxiv.org/abs/2404.06501v1","category":"astro-ph.SR"}
{"created":"2024-04-09 17:50:38","title":"Simultaneous linear connectivity of neural networks modulo permutation","abstract":"Neural networks typically exhibit permutation symmetries which contribute to the non-convexity of the networks' loss landscapes, since linearly interpolating between two permuted versions of a trained network tends to encounter a high loss barrier. Recent work has argued that permutation symmetries are the only sources of non-convexity, meaning there are essentially no such barriers between trained networks if they are permuted appropriately. In this work, we refine these arguments into three distinct claims of increasing strength. We show that existing evidence only supports \"weak linear connectivity\"-that for each pair of networks belonging to a set of SGD solutions, there exist (multiple) permutations that linearly connect it with the other networks. In contrast, the claim \"strong linear connectivity\"-that for each network, there exists one permutation that simultaneously connects it with the other networks-is both intuitively and practically more desirable. This stronger claim would imply that the loss landscape is convex after accounting for permutation, and enable linear interpolation between three or more independently trained models without increased loss. In this work, we introduce an intermediate claim-that for certain sequences of networks, there exists one permutation that simultaneously aligns matching pairs of networks from these sequences. Specifically, we discover that a single permutation aligns sequences of iteratively trained as well as iteratively pruned networks, meaning that two networks exhibit low loss barriers at each step of their optimization and sparsification trajectories respectively. Finally, we provide the first evidence that strong linear connectivity may be possible under certain conditions, by showing that barriers decrease with increasing network width when interpolating among three networks.","sentences":["Neural networks typically exhibit permutation symmetries which contribute to the non-convexity of the networks' loss landscapes, since linearly interpolating between two permuted versions of a trained network tends to encounter a high loss barrier.","Recent work has argued that permutation symmetries are the only sources of non-convexity, meaning there are essentially no such barriers between trained networks if they are permuted appropriately.","In this work, we refine these arguments into three distinct claims of increasing strength.","We show that existing evidence only supports \"weak linear connectivity\"-that for each pair of networks belonging to a set of SGD solutions, there exist (multiple) permutations that linearly connect it with the other networks.","In contrast, the claim \"strong linear connectivity\"-that for each network, there exists one permutation that simultaneously connects it with the other networks-is both intuitively and practically more desirable.","This stronger claim would imply that the loss landscape is convex after accounting for permutation, and enable linear interpolation between three or more independently trained models without increased loss.","In this work, we introduce an intermediate claim-that for certain sequences of networks, there exists one permutation that simultaneously aligns matching pairs of networks from these sequences.","Specifically, we discover that a single permutation aligns sequences of iteratively trained as well as iteratively pruned networks, meaning that two networks exhibit low loss barriers at each step of their optimization and sparsification trajectories respectively.","Finally, we provide the first evidence that strong linear connectivity may be possible under certain conditions, by showing that barriers decrease with increasing network width when interpolating among three networks."],"url":"http://arxiv.org/abs/2404.06498v1","category":"cs.LG"}
{"created":"2024-04-09 17:49:18","title":"Mechanism Design for ZK-Rollup Prover Markets","abstract":"In ZK-Rollups, provers spend significant computational resources to generate validity proofs. Their costs should be compensated properly, so a sustainable prover market can form over time. Existing transaction fee mechanisms (TFMs) such as EIP-1559, however, do not work in this setting, as EIP-1559 only generates negligible revenue because of burning, while provers often create or purchase specialized hardware in hopes of creating long-term revenue from proving, somewhat reminiscent of proof-of-work miners in the case of chains like Bitcoin. In this paper, we explore the design of transaction fee mechanisms for prover markets. The desiderata for such mechanisms include efficiency (social welfare is maximized), incentive compatibility (dominant bidding strategy exists), collusion resistance (no profitable collusion among provers exists), and off-chain agreement proofness (no profitable collusion between users and provers exists). This paper presents a prover market mechanism called Proo{\\phi} (pronounced proo-fee), and show the conditions under which it can satisfy these desired properties. In our mechanism, the users bid their fee for transaction inclusion, and the user transactions are selected through a first-price auction; the provers bid their proof generation capacity and cost, and the lowest-cost provers are selected. We add an upper bound of the total capacity of the included transactions each round to forestall off-chain agreements. Proo{\\phi} is Bayesian incentive compatible for users and off-chain agreement proof, and is also incentive-compatible for provers, when assuming the latter cannot employ Sybil attacks. We present a preliminary analysis of the Proo{\\phi} mechanism and its limitations, and raise open questions about the feasibility of an ideal mechanism for prover markets. This work is in progress, and this manuscript will be updated.","sentences":["In ZK-Rollups, provers spend significant computational resources to generate validity proofs.","Their costs should be compensated properly, so a sustainable prover market can form over time.","Existing transaction fee mechanisms (TFMs) such as EIP-1559, however, do not work in this setting, as EIP-1559 only generates negligible revenue because of burning, while provers often create or purchase specialized hardware in hopes of creating long-term revenue from proving, somewhat reminiscent of proof-of-work miners in the case of chains like Bitcoin.","In this paper, we explore the design of transaction fee mechanisms for prover markets.","The desiderata for such mechanisms include efficiency (social welfare is maximized), incentive compatibility (dominant bidding strategy exists), collusion resistance (no profitable collusion among provers exists), and off-chain agreement proofness (no profitable collusion between users and provers exists).","This paper presents a prover market mechanism called Proo{\\phi} (pronounced proo-fee), and show the conditions under which it can satisfy these desired properties.","In our mechanism, the users bid their fee for transaction inclusion, and the user transactions are selected through a first-price auction; the provers bid their proof generation capacity and cost, and the lowest-cost provers are selected.","We add an upper bound of the total capacity of the included transactions each round to forestall off-chain agreements.","Proo{\\phi} is Bayesian incentive compatible for users and off-chain agreement proof, and is also incentive-compatible for provers, when assuming the latter cannot employ Sybil attacks.","We present a preliminary analysis of the Proo{\\phi} mechanism and its limitations, and raise open questions about the feasibility of an ideal mechanism for prover markets.","This work is in progress, and this manuscript will be updated."],"url":"http://arxiv.org/abs/2404.06495v1","category":"cs.GT"}
{"created":"2024-04-09 17:43:28","title":"Finding Stable Price Zones in European Electricity Markets: Aiming to Square the Circle?","abstract":"The European day-ahead electricity market is split into multiple bidding zones. Within these zones, a uniform energy price is computed for each hour. Large bidding zones have been under scrutiny. The fact that zonal clearing ignores the transmission capacities within zones and the increase in renewables lead to a growing number of interventions in the generation of energy sources and large redispatch costs. The European Union Agency for the Cooperation of Energy Regulators (ACER) proposed alternative bidding zone configurations that should be analyzed as part of the Bidding Zone Review. For Germany, four alternative configurations were suggested. Bidding zones shall be stable and based on long-term, structural congestion in the grid. We analyzed the proposed configurations considering different clustering algorithms and periods. We found that the configurations do not reduce the price standard deviations within zones considerably, while the average prices across zones are similar. Other configurations identified based on clustering prices lead to lower price standard deviations but are not geographically coherent. Importantly, different configurations emerge depending on clustering features, algorithm, and period considered. Given the substantial changes in energy supply and demand that can be expected in the future, defining stable configurations appears to be a moving target.","sentences":["The European day-ahead electricity market is split into multiple bidding zones.","Within these zones, a uniform energy price is computed for each hour.","Large bidding zones have been under scrutiny.","The fact that zonal clearing ignores the transmission capacities within zones and the increase in renewables lead to a growing number of interventions in the generation of energy sources and large redispatch costs.","The European Union Agency for the Cooperation of Energy Regulators (ACER) proposed alternative bidding zone configurations that should be analyzed as part of the Bidding Zone Review.","For Germany, four alternative configurations were suggested.","Bidding zones shall be stable and based on long-term, structural congestion in the grid.","We analyzed the proposed configurations considering different clustering algorithms and periods.","We found that the configurations do not reduce the price standard deviations within zones considerably, while the average prices across zones are similar.","Other configurations identified based on clustering prices lead to lower price standard deviations but are not geographically coherent.","Importantly, different configurations emerge depending on clustering features, algorithm, and period considered.","Given the substantial changes in energy supply and demand that can be expected in the future, defining stable configurations appears to be a moving target."],"url":"http://arxiv.org/abs/2404.06489v1","category":"econ.GN"}
{"created":"2024-04-09 17:21:41","title":"Regression Discontinuity Design with Spillovers","abstract":"Researchers who estimate treatment effects using a regression discontinuity design (RDD) typically assume that there are no spillovers between the treated and control units. This may be unrealistic. We characterize the estimand of RDD in a setting where spillovers occur between units that are close in their values of the running variable. Under the assumption that spillovers are linear-in-means, we show that the estimand depends on the ratio of two terms: (1) the radius over which spillovers occur and (2) the choice of bandwidth used for the local linear regression. Specifically, RDD estimates direct treatment effect when radius is of larger order than the bandwidth, and total treatment effect when radius is of smaller order than the bandwidth. In the more realistic regime where radius is of similar order as the bandwidth, the RDD estimand is a mix of the above effects. To recover direct and spillover effects, we propose incorporating estimated spillover terms into local linear regression -- the local analog of peer effects regression. We also clarify the settings under which the donut-hole RD is able to eliminate the effects of spillovers.","sentences":["Researchers who estimate treatment effects using a regression discontinuity design (RDD) typically assume that there are no spillovers between the treated and control units.","This may be unrealistic.","We characterize the estimand of RDD in a setting where spillovers occur between units that are close in their values of the running variable.","Under the assumption that spillovers are linear-in-means, we show that the estimand depends on the ratio of two terms: (1) the radius over which spillovers occur and (2) the choice of bandwidth used for the local linear regression.","Specifically, RDD estimates direct treatment effect when radius is of larger order than the bandwidth, and total treatment effect when radius is of smaller order than the bandwidth.","In the more realistic regime where radius is of similar order as the bandwidth, the RDD estimand is a mix of the above effects.","To recover direct and spillover effects, we propose incorporating estimated spillover terms into local linear regression -- the local analog of peer effects regression.","We also clarify the settings under which the donut-hole RD is able to eliminate the effects of spillovers."],"url":"http://arxiv.org/abs/2404.06471v1","category":"econ.EM"}
{"created":"2024-04-09 17:01:43","title":"Thermal fluctuations of matter composition and quark nucleation in compact stars","abstract":"Context. At the extreme densities reached in the core of neutron stars, it is possible that quark deconfined matter is produced. The formation of this new phase of strongly interacting matter is likely to occur via a first-order phase transition for the typical temperatures reached in astrophysical processes. The first seeds of quark matter would then form through a process of nucleation within the metastable hadronic phase. Aims. Here we address the role of the thermal fluctuations in the hadronic composition on the nucleation of two-flavour quark matter. Methods. At finite temperature, thermodynamic quantities in a system fluctuate around average values. Being nucleation a local process, it is possible that it occurs in a subsystem whose composition makes the nucleation easier. We will consider the total probability of the nucleation as the product between the probability that a subsystem has a certain hadronic composition different from the average in the bulk, and the nucleation probability in that subsystem. Results. We will show how those fluctuations of the hadronic composition can increase the efficiency of nucleation already for temperatures $\\sim (0.1-1)$ keV. However, for temperatures $\\lesssim (1-10)$ MeV, the needed overpressure exceeds the maximum pressure reached in compact stars. Finally, for even larger temperatures the process of nucleation can take place, even taking into account finite size effects.","sentences":["Context.","At the extreme densities reached in the core of neutron stars, it is possible that quark deconfined matter is produced.","The formation of this new phase of strongly interacting matter is likely to occur via a first-order phase transition for the typical temperatures reached in astrophysical processes.","The first seeds of quark matter would then form through a process of nucleation within the metastable hadronic phase.","Aims.","Here we address the role of the thermal fluctuations in the hadronic composition on the nucleation of two-flavour quark matter.","Methods.","At finite temperature, thermodynamic quantities in a system fluctuate around average values.","Being nucleation a local process, it is possible that it occurs in a subsystem whose composition makes the nucleation easier.","We will consider the total probability of the nucleation as the product between the probability that a subsystem has a certain hadronic composition different from the average in the bulk, and the nucleation probability in that subsystem.","Results.","We will show how those fluctuations of the hadronic composition can increase the efficiency of nucleation already for temperatures $\\sim (0.1-1)$ keV.","However, for temperatures $\\lesssim (1-10)$ MeV, the needed overpressure exceeds the maximum pressure reached in compact stars.","Finally, for even larger temperatures the process of nucleation can take place, even taking into account finite size effects."],"url":"http://arxiv.org/abs/2404.06463v1","category":"nucl-th"}
{"created":"2024-04-09 17:00:53","title":"Analysis of Distributed Algorithms for Big-data","abstract":"The parallel and distributed processing are becoming de facto industry standard, and a large part of the current research is targeted on how to make computing scalable and distributed, dynamically, without allocating the resources on permanent basis. The present article focuses on the study and performance of distributed and parallel algorithms their file systems, to achieve scalability at local level (OpenMP platform), and at global level where computing and file systems are distributed. Various applications, algorithms,file systems have been used to demonstrate the areas, and their performance studies have been presented. The systems and applications chosen here are of open-source nature, due to their wider applicability.","sentences":["The parallel and distributed processing are becoming de facto industry standard, and a large part of the current research is targeted on how to make computing scalable and distributed, dynamically, without allocating the resources on permanent basis.","The present article focuses on the study and performance of distributed and parallel algorithms their file systems, to achieve scalability at local level (OpenMP platform), and at global level where computing and file systems are distributed.","Various applications, algorithms,file systems have been used to demonstrate the areas, and their performance studies have been presented.","The systems and applications chosen here are of open-source nature, due to their wider applicability."],"url":"http://arxiv.org/abs/2404.06461v1","category":"cs.DC"}
{"created":"2024-04-09 16:57:50","title":"Critical non-linearity for some evolution equations with Fujita-type critical exponent","abstract":"We consider the Cauchy problem for a class of non-linear evolution equations in the form \\[L(\\partial_t,\\partial_x) u=F(\\partial_t^\\ell u), \\quad (t,x)\\in [0,\\infty)\\times \\mathbb{R}^n;\\] here, $L(\\partial_t,\\partial_x)$ is a linear partial differential operator with constant coefficients, of order $m\\geq 1$ with respect to the time variable $t$, and $\\ell$ is a natural number satisfying $0\\leq \\ell\\leq m-1$. For several different choices of $L$, many authors have investigated the existence of global (in time) solutions to this problem when $F(s)=|s|^p$ is a power non-linearity, looking for a \\textit{critical exponent} $p_c>1$ such that global small data solutions exist in the supercritical case $p>p_c$, whereas no global weak solutions exist, under suitable sign assumptions on the data, in the subcritical case $1<p<p_c$. In the present paper we consider a more general non-linear term in the form $F(s)=|s|^p\\mu(|s|)$; for a large class of models, we provide an integral condition on $\\mu$ which allows to distinguish more precisely the region of existence of a global (in time) small data solution from that in which the problem admits no global (in time) weak solutions, refining the existing results about the critical exponents for power type non-linearities.","sentences":["We consider the Cauchy problem for a class of non-linear evolution equations in the form \\[L(\\partial_t,\\partial_x) u=F(\\partial_t^\\ell u), \\quad (t,x)\\in","[0,\\infty)\\times \\mathbb{R}^n;\\] here, $L(\\partial_t,\\partial_x)$ is a linear partial differential operator with constant coefficients, of order $m\\geq 1$ with respect to the time variable $t$, and $\\ell$ is a natural number satisfying $0\\leq \\ell\\leq m-1$. For several different choices of $L$, many authors have investigated the existence of global (in time) solutions to this problem when $F(s)=|s|^p$ is a power non-linearity, looking for a \\textit{critical exponent} $p_c>1$ such that global small data solutions exist in the supercritical case $p>p_c$, whereas no global weak solutions exist, under suitable sign assumptions on the data, in the subcritical case $1<p<p_c$. In the present paper we consider a more general non-linear term in the form $F(s)=|s|^p\\mu(|s|)$; for a large class of models, we provide an integral condition on $\\mu$ which allows to distinguish more precisely the region of existence of a global (in time) small data solution from that in which the problem admits no global (in time) weak solutions, refining the existing results about the critical exponents for power type non-linearities."],"url":"http://arxiv.org/abs/2404.06458v1","category":"math.AP"}
{"created":"2024-04-09 16:53:52","title":"PAAM: A Framework for Coordinated and Priority-Driven Accelerator Management in ROS 2","abstract":"This paper proposes a Priority-driven Accelerator Access Management (PAAM) framework for multi-process robotic applications built on top of the Robot Operating System (ROS) 2 middleware platform. The framework addresses the issue of predictable execution of time- and safety-critical callback chains that require hardware accelerators such as GPUs and TPUs. PAAM provides a standalone ROS executor that acts as an accelerator resource server, arbitrating accelerator access requests from all other callbacks at the application layer. This approach enables coordinated and priority-driven accelerator access management in multi-process robotic systems. The framework design is directly applicable to all types of accelerators and enables granular control over how specific chains access accelerators, making it possible to achieve predictable real-time support for accelerators used by safety-critical callback chains without making changes to underlying accelerator device drivers. The paper shows that PAAM also offers a theoretical analysis that can upper bound the worst-case response time of safety-critical callback chains that necessitate accelerator access. This paper also demonstrates that complex robotic systems with extensive accelerator usage that are integrated with PAAM may achieve up to a 91\\% reduction in end-to-end response time of their critical callback chains.","sentences":["This paper proposes a Priority-driven Accelerator Access Management (PAAM) framework for multi-process robotic applications built on top of the Robot Operating System (ROS) 2 middleware platform.","The framework addresses the issue of predictable execution of time- and safety-critical callback chains that require hardware accelerators such as GPUs and TPUs.","PAAM provides a standalone ROS executor that acts as an accelerator resource server, arbitrating accelerator access requests from all other callbacks at the application layer.","This approach enables coordinated and priority-driven accelerator access management in multi-process robotic systems.","The framework design is directly applicable to all types of accelerators and enables granular control over how specific chains access accelerators, making it possible to achieve predictable real-time support for accelerators used by safety-critical callback chains without making changes to underlying accelerator device drivers.","The paper shows that PAAM also offers a theoretical analysis that can upper bound the worst-case response time of safety-critical callback chains that necessitate accelerator access.","This paper also demonstrates that complex robotic systems with extensive accelerator usage that are integrated with PAAM may achieve up to a 91\\% reduction in end-to-end response time of their critical callback chains."],"url":"http://arxiv.org/abs/2404.06452v1","category":"cs.RO"}
{"created":"2024-04-09 16:49:42","title":"The Central Spanning Tree Problem","abstract":"Spanning trees are an important primitive in many data analysis tasks, when a data set needs to be summarized in terms of its \"skeleton\", or when a tree-shaped graph over all observations is required for downstream processing. Popular definitions of spanning trees include the minimum spanning tree and the optimum distance spanning tree, a.k.a. the minimum routing cost tree. When searching for the shortest spanning tree but admitting additional branching points, even shorter spanning trees can be realized: Steiner trees. Unfortunately, both minimum spanning and Steiner trees are not robust with respect to noise in the observations; that is, small perturbations of the original data set often lead to drastic changes in the associated spanning trees. In response, we make two contributions when the data lies in a Euclidean space: on the theoretical side, we introduce a new optimization problem, the \"(branched) central spanning tree\", which subsumes all previously mentioned definitions as special cases. On the practical side, we show empirically that the (branched) central spanning tree is more robust to noise in the data, and as such is better suited to summarize a data set in terms of its skeleton. We also propose a heuristic to address the NP-hard optimization problem, and illustrate its use on single cell RNA expression data from biology and 3D point clouds of plants.","sentences":["Spanning trees are an important primitive in many data analysis tasks, when a data set needs to be summarized in terms of its \"skeleton\", or when a tree-shaped graph over all observations is required for downstream processing.","Popular definitions of spanning trees include the minimum spanning tree and the optimum distance spanning tree, a.k.a.","the minimum routing cost tree.","When searching for the shortest spanning tree but admitting additional branching points, even shorter spanning trees can be realized: Steiner trees.","Unfortunately, both minimum spanning and Steiner trees are not robust with respect to noise in the observations; that is, small perturbations of the original data set often lead to drastic changes in the associated spanning trees.","In response, we make two contributions when the data lies in a Euclidean space: on the theoretical side, we introduce a new optimization problem, the \"(branched) central spanning tree\", which subsumes all previously mentioned definitions as special cases.","On the practical side, we show empirically that the (branched) central spanning tree is more robust to noise in the data, and as such is better suited to summarize a data set in terms of its skeleton.","We also propose a heuristic to address the NP-hard optimization problem, and illustrate its use on single cell RNA expression data from biology and 3D point clouds of plants."],"url":"http://arxiv.org/abs/2404.06447v1","category":"cs.DM"}
{"created":"2024-04-09 16:38:17","title":"ClassiPyGRB: Machine Learning-Based Classification and Visualization of Gamma Ray Bursts using t-SNE","abstract":"Gamma-ray burst (GRBs) are the brightest events in the universe. For decades, astrophysicists have known about their cosmological nature. Every year, space missions such as Fermi and SWIFT detect hundreds of them. In spite of this large sample, GRBs show a complex taxonomy in the first seconds after their appearance, which makes it very difficult to find similarities between them using conventional techniques. It is known that GRBs originate from the death of a massive star or from the merger of two compact objects. GRB classification is typically based on the duration of the burst (Kouveliotou et al., 1993). Nevertheless, events such as GRB 211211A (Yang et al., 2022), whose duration of about 50 seconds lies in the group of long GRBs, has challenged this categorization by the evidence of features related with the short GRB population (the kilonova emission and the properties of its host galaxy). Therefore, a classification based only on their gamma-ray duration does not provide a completely reliable determination of the progenitor. Motivated by this problem, Jespersen et al. (2020) and Steinhardt et al. (2023) carried out analysis of GRB light curves by using the t-SNE algorithm, showing that Swift/BAT GRBs database, consisting of light curves in four energy bands (15-25 keV, 25-50 keV, 50-100 keV, 100-350 keV), clusters into two groups corresponding with the typical long/short classification. However, in this case, this classification is based on the information provided by their gamma-ray emission light curves. ClassiPyGRB is a Python 3 package to download, process, visualize and classify GRBs database from the Swift/BAT Instrument (up to July 2022). It is distributed over the GNU General Public License Version 2 (1991). We also included a noise-reduction and an interpolation tools for achieving a deeper analysis of the data.","sentences":["Gamma-ray burst (GRBs) are the brightest events in the universe.","For decades, astrophysicists have known about their cosmological nature.","Every year, space missions such as Fermi and SWIFT detect hundreds of them.","In spite of this large sample, GRBs show a complex taxonomy in the first seconds after their appearance, which makes it very difficult to find similarities between them using conventional techniques.","It is known that GRBs originate from the death of a massive star or from the merger of two compact objects.","GRB classification is typically based on the duration of the burst (Kouveliotou et al., 1993).","Nevertheless, events such as GRB 211211A (Yang et al., 2022), whose duration of about 50 seconds lies in the group of long GRBs, has challenged this categorization by the evidence of features related with the short GRB population (the kilonova emission and the properties of its host galaxy).","Therefore, a classification based only on their gamma-ray duration does not provide a completely reliable determination of the progenitor.","Motivated by this problem, Jespersen et al. (2020) and Steinhardt et al.","(2023) carried out analysis of GRB light curves by using the t-SNE algorithm, showing that Swift/BAT GRBs database, consisting of light curves in four energy bands (15-25 keV, 25-50 keV, 50-100 keV, 100-350 keV), clusters into two groups corresponding with the typical long/short classification.","However, in this case, this classification is based on the information provided by their gamma-ray emission light curves.","ClassiPyGRB is a Python 3 package to download, process, visualize and classify GRBs database from the Swift/BAT Instrument (up to July 2022).","It is distributed over the GNU General Public License Version 2 (1991).","We also included a noise-reduction and an interpolation tools for achieving a deeper analysis of the data."],"url":"http://arxiv.org/abs/2404.06439v1","category":"astro-ph.HE"}
{"created":"2024-04-09 16:18:13","title":"A universal sequence of tensors for the asymptotic rank conjecture","abstract":"The exponent $\\sigma(T)$ of a tensor $T\\in\\mathbb{F}^d\\otimes\\mathbb{F}^d\\otimes\\mathbb{F}^d$ over a field $\\mathbb{F}$ captures the base of the exponential growth rate of the tensor rank of $T$ under Kronecker powers. Tensor exponents are fundamental from the standpoint of algorithms and computational complexity theory; for example, the exponent $\\omega$ of matrix multiplication can be characterized as $\\omega=2\\sigma(\\mathrm{MM}_2)$, where $\\mathrm{MM}_2\\in\\mathbb{F}^4\\otimes\\mathbb{F}^4\\otimes\\mathbb{F}^4$ is the tensor that represents $2\\times 2$ matrix multiplication.   Our main result is an explicit construction of a sequence $\\mathcal{U}_d$ of zero-one-valued tensors that is universal for the worst-case tensor exponent; more precisely, we show that $\\sigma(\\mathcal{U}_d)=\\sigma(d)$ where $\\sigma(d)=\\sup_{T\\in\\mathbb{F}^d\\otimes\\mathbb{F}^d\\otimes\\mathbb{F}^d}\\sigma(T)$. We also supply an explicit universal sequence $\\mathcal{U}_\\Delta$ localised to capture the worst-case exponent $\\sigma(\\Delta)$ of tensors with support contained in $\\Delta\\subseteq [d]\\times[d]\\times [d]$; by combining such sequences, we obtain a universal sequence $\\mathcal{T}_d$ such that $\\sigma(\\mathcal{T}_d)=1$ holds if and only if Strassen's asymptotic rank conjecture [Progr. Math. 120 (1994)] holds for $d$. Finally, we show that the limit $\\lim_{d\\rightarrow\\infty}\\sigma(d)$ exists and can be captured as $\\lim_{d\\rightarrow\\infty} \\sigma(D_d)$ for an explicit sequence $(D_d)_{d=1}^\\infty$ of tensors obtained by diagonalisation of the sequences $\\mathcal{U}_d$. As our second result we relate the absence of polynomials of fixed degree vanishing on tensors of low rank, or more generally asymptotic rank, with upper bounds on the exponent $\\sigma(d)$. Using this technique, one may bound asymptotic rank for all tensors of a given format, knowing enough specific tensors of low asymptotic rank.","sentences":["The exponent $\\sigma(T)$ of a tensor $T\\in\\mathbb{F}^d\\otimes\\mathbb{F}^d\\otimes\\mathbb{F}^d$ over a field $\\mathbb{F}$ captures the base of the exponential growth rate of the tensor rank of $T$ under Kronecker powers.","Tensor exponents are fundamental from the standpoint of algorithms and computational complexity theory; for example, the exponent $\\omega$ of matrix multiplication can be characterized as $\\omega=2\\sigma(\\mathrm{MM}_2)$, where $\\mathrm{MM}_2\\in\\mathbb{F}^4\\otimes\\mathbb{F}^4\\otimes\\mathbb{F}^4$ is the tensor that represents $2\\times 2$ matrix multiplication.   ","Our main result is an explicit construction of a sequence $\\mathcal{U}_d$ of zero-one-valued tensors that is universal for the worst-case tensor exponent; more precisely, we show that $\\sigma(\\mathcal{U}_d)=\\sigma(d)$ where $\\sigma(d)=\\sup_{T\\in\\mathbb{F}^d\\otimes\\mathbb{F}^d\\otimes\\mathbb{F}^d}\\sigma(T)$. We also supply an explicit universal sequence $\\mathcal{U}_\\Delta$ localised to capture the worst-case exponent $\\sigma(\\Delta)$ of tensors with support contained in $\\Delta\\subseteq [d]\\times[d]\\times [d]$; by combining such sequences, we obtain a universal sequence $\\mathcal{T}_d$ such that $\\sigma(\\mathcal{T}_d)=1$ holds if and only if Strassen's asymptotic rank conjecture","[Progr.","Math. 120 (1994)] holds for $d$. Finally, we show that the limit $\\lim_{d\\rightarrow\\infty}\\sigma(d)$ exists and can be captured as $\\lim_{d\\rightarrow\\infty} \\sigma(D_d)$ for an explicit sequence $(D_d)_{d=1}^\\infty$ of tensors obtained by diagonalisation of the sequences $\\mathcal{U}_d$. As our second result we relate the absence of polynomials of fixed degree vanishing on tensors of low rank, or more generally asymptotic rank, with upper bounds on the exponent $\\sigma(d)$. Using this technique, one may bound asymptotic rank for all tensors of a given format, knowing enough specific tensors of low asymptotic rank."],"url":"http://arxiv.org/abs/2404.06427v1","category":"cs.CC"}
{"created":"2024-04-09 16:15:03","title":"ZeST: Zero-Shot Material Transfer from a Single Image","abstract":"We propose ZeST, a method for zero-shot material transfer to an object in the input image given a material exemplar image. ZeST leverages existing diffusion adapters to extract implicit material representation from the exemplar image. This representation is used to transfer the material using pre-trained inpainting diffusion model on the object in the input image using depth estimates as geometry cue and grayscale object shading as illumination cues. The method works on real images without any training resulting a zero-shot approach. Both qualitative and quantitative results on real and synthetic datasets demonstrate that ZeST outputs photorealistic images with transferred materials. We also show the application of ZeST to perform multiple edits and robust material assignment under different illuminations. Project Page: https://ttchengab.github.io/zest","sentences":["We propose ZeST, a method for zero-shot material transfer to an object in the input image given a material exemplar image.","ZeST leverages existing diffusion adapters to extract implicit material representation from the exemplar image.","This representation is used to transfer the material using pre-trained inpainting diffusion model on the object in the input image using depth estimates as geometry cue and grayscale object shading as illumination cues.","The method works on real images without any training resulting a zero-shot approach.","Both qualitative and quantitative results on real and synthetic datasets demonstrate that ZeST outputs photorealistic images with transferred materials.","We also show the application of ZeST to perform multiple edits and robust material assignment under different illuminations.","Project Page: https://ttchengab.github.io/zest"],"url":"http://arxiv.org/abs/2404.06425v1","category":"cs.CV"}
{"created":"2024-04-09 16:10:39","title":"Bayesian Survival Analysis by Approximate Inference of Neural Networks","abstract":"Predicting future events always comes with uncertainty, but traditional non-Bayesian methods cannot distinguish certain from uncertain predictions or explain the confidence in their predictions. In survival analysis, Bayesian methods applied to state-of-the-art solutions in the healthcare and biomedical field are still novel, and their implications have not been fully evaluated. In this paper, we study the benefits of modeling uncertainty in deep neural networks for survival analysis with a focus on prediction and calibration performance. For this, we present a Bayesian deep learning framework that consists of three Bayesian network architectures, which we train by optimizing the Cox partial likelihood and combining input-dependent aleatoric uncertainty with model-specific epistemic uncertainty. This enables us to provide uncertainty estimates as credible intervals when predicting the survival curve or as a probability density function over the predicted median survival times. For our empirical analyses, we evaluated our proposed method on four benchmark datasets and found that our method demonstrates prediction performance comparable to the state-of-the-art based on the concordance index and outperforms all other Cox-based approaches in terms of the mean absolute error. Our work explicitly compares the extent to which different Bayesian approximation techniques differ from each other and improves the prediction over traditional non-Bayesian alternatives.","sentences":["Predicting future events always comes with uncertainty, but traditional non-Bayesian methods cannot distinguish certain from uncertain predictions or explain the confidence in their predictions.","In survival analysis, Bayesian methods applied to state-of-the-art solutions in the healthcare and biomedical field are still novel, and their implications have not been fully evaluated.","In this paper, we study the benefits of modeling uncertainty in deep neural networks for survival analysis with a focus on prediction and calibration performance.","For this, we present a Bayesian deep learning framework that consists of three Bayesian network architectures, which we train by optimizing the Cox partial likelihood and combining input-dependent aleatoric uncertainty with model-specific epistemic uncertainty.","This enables us to provide uncertainty estimates as credible intervals when predicting the survival curve or as a probability density function over the predicted median survival times.","For our empirical analyses, we evaluated our proposed method on four benchmark datasets and found that our method demonstrates prediction performance comparable to the state-of-the-art based on the concordance index and outperforms all other Cox-based approaches in terms of the mean absolute error.","Our work explicitly compares the extent to which different Bayesian approximation techniques differ from each other and improves the prediction over traditional non-Bayesian alternatives."],"url":"http://arxiv.org/abs/2404.06421v1","category":"cs.LG"}
{"created":"2024-04-09 16:04:12","title":"Quasi-Particle Self-Consistent $GW$ for Molecules","abstract":"We present the formalism and implementation of quasi-particle self-consistent GW (qsGW) and eigenvalue only quasi-particle self-consistent GW (evGW) adapted to standard quantum chemistry packages. Our implementation is benchmarked against high-level quantum chemistry computations (coupled-cluster theory) and experimental results using a representative set of molecules. Furthermore, we compare the qsGW approach for five molecules relevant for organic photovoltaics to self-consistent GW results (scGW) and analyze the effects of the self-consistency on the ground state density by comparing calculated dipole moments to their experimental values. We show that qsGW makes a significant improvement over conventional G0W0 and that partially self-consistent flavors (in particular evGW) can be excellent alternatives.","sentences":["We present the formalism and implementation of quasi-particle self-consistent GW (qsGW) and eigenvalue only quasi-particle self-consistent GW (evGW) adapted to standard quantum chemistry packages.","Our implementation is benchmarked against high-level quantum chemistry computations (coupled-cluster theory) and experimental results using a representative set of molecules.","Furthermore, we compare the qsGW approach for five molecules relevant for organic photovoltaics to self-consistent GW results (scGW) and analyze the effects of the self-consistency on the ground state density by comparing calculated dipole moments to their experimental values.","We show that qsGW makes a significant improvement over conventional G0W0 and that partially self-consistent flavors (in particular evGW) can be excellent alternatives."],"url":"http://arxiv.org/abs/2404.06415v1","category":"physics.chem-ph"}
{"created":"2024-04-09 16:03:26","title":"Large Language Models to the Rescue: Deadlock Resolution in Multi-Robot Systems","abstract":"Multi-agent robotic systems are prone to deadlocks in an obstacle environment where the system can get stuck away from its desired location under a smooth low-level control policy. Without an external intervention, often in terms of a high-level command, it is not possible to guarantee that just a low-level control policy can resolve such deadlocks. Utilizing the generalizability and low data requirements of large language models (LLMs), this paper explores the possibility of using LLMs for deadlock resolution. We propose a hierarchical control framework where an LLM resolves deadlocks by assigning a leader and direction for the leader to move along. A graph neural network (GNN) based low-level distributed control policy executes the assigned plan. We systematically study various prompting techniques to improve LLM's performance in resolving deadlocks. In particular, as part of prompt engineering, we provide in-context examples for LLMs. We conducted extensive experiments on various multi-robot environments with up to 15 agents and 40 obstacles. Our results demonstrate that LLM-based high-level planners are effective in resolving deadlocks in MRS.","sentences":["Multi-agent robotic systems are prone to deadlocks in an obstacle environment where the system can get stuck away from its desired location under a smooth low-level control policy.","Without an external intervention, often in terms of a high-level command, it is not possible to guarantee that just a low-level control policy can resolve such deadlocks.","Utilizing the generalizability and low data requirements of large language models (LLMs), this paper explores the possibility of using LLMs for deadlock resolution.","We propose a hierarchical control framework where an LLM resolves deadlocks by assigning a leader and direction for the leader to move along.","A graph neural network (GNN) based low-level distributed control policy executes the assigned plan.","We systematically study various prompting techniques to improve LLM's performance in resolving deadlocks.","In particular, as part of prompt engineering, we provide in-context examples for LLMs.","We conducted extensive experiments on various multi-robot environments with up to 15 agents and 40 obstacles.","Our results demonstrate that LLM-based high-level planners are effective in resolving deadlocks in MRS."],"url":"http://arxiv.org/abs/2404.06413v1","category":"cs.RO"}
{"created":"2024-04-09 15:50:29","title":"Radiation Tolerance of the LHCb Outer Tracker: in the Lab and in the Forward Region at the LHC","abstract":"During the detector construction phase between 2004 and 2006, it was discovered that the LHCb Outer Tracker (OT) detector suffered from gain loss after irradiation in the laboratory at moderate intensities. Under irradiation an insulating layer was formed on the anode wire. The aging was caused by contamination of the counting gas due to outgassing of the glue used in construction namely araldite AY103-1. The gain loss was concentrated upstream the gas flow, and at moderate irradiation intensity only. The aging rate was reduced by longterm flushing and by the addition of a few percent of O2 to the gas mixture. Furthermore, applying a large positive high voltage (beyond the amplification regime) has shown to remove the insulating deposits without damaging the wire surface. This paper presents the history of the developments together with the characteristics and the culprit of the aging phenomenon and the resulting detector performance in situ.","sentences":["During the detector construction phase between 2004 and 2006, it was discovered that the LHCb Outer Tracker (OT) detector suffered from gain loss after irradiation in the laboratory at moderate intensities.","Under irradiation an insulating layer was formed on the anode wire.","The aging was caused by contamination of the counting gas due to outgassing of the glue used in construction namely araldite AY103-1.","The gain loss was concentrated upstream the gas flow, and at moderate irradiation intensity only.","The aging rate was reduced by longterm flushing and by the addition of a few percent of O2 to the gas mixture.","Furthermore, applying a large positive high voltage (beyond the amplification regime) has shown to remove the insulating deposits without damaging the wire surface.","This paper presents the history of the developments together with the characteristics and the culprit of the aging phenomenon and the resulting detector performance in situ."],"url":"http://arxiv.org/abs/2404.06402v1","category":"physics.ins-det"}
{"created":"2024-04-09 15:48:31","title":"Bounded Edit Distance: Optimal Static and Dynamic Algorithms for Small Integer Weights","abstract":"The edit distance of two strings is the minimum number of insertions, deletions, and substitutions needed to transform one string into the other. The textbook algorithm determines the edit distance of length-$n$ strings in $O(n^2)$ time, which is optimal up to subpolynomial factors under Orthogonal Vectors Hypothesis. In the bounded version of the problem, parameterized by the edit distance $k$, the algorithm of Landau and Vishkin [JCSS'88] achieves $O(n+k^2)$ time, which is optimal as a function of $n$ and $k$.   The dynamic version of the problem asks to maintain the edit distance of two strings that change dynamically, with each update modeled as an edit. A folklore approach supports updates in $\\tilde O(k^2)$ time, where $\\tilde O(\\cdot)$ hides polylogarithmic factors. Recently, Charalampopoulos, Kociumaka, and Mozes [CPM'20] showed an algorithm with update time $\\tilde O(n)$, which is optimal under OVH in terms of $n$. The update time of $\\tilde O(\\min\\{n,k^2\\})$ raised an exciting open question of whether $\\tilde O(k)$ is possible; we answer it affirmatively.   Our solution relies on tools originating from weighted edit distance, where the weight of each edit depends on the edit type and the characters involved. The textbook algorithm supports weights, but the Landau-Vishkin approach does not, and a simple $O(nk)$-time procedure long remained the fastest for bounded weighted edit distance. Only recently, Das et al. [STOC'23] provided an $O(n+k^5)$-time algorithm, whereas Cassis, Kociumaka, and Wellnitz [FOCS'23] presented an $\\tilde O(n+\\sqrt{nk^3})$-time solution and a matching conditional lower bound. In this paper, we show that, for integer edit weights between $0$ and $W$, weighted edit distance can be computed in $\\tilde O(n+Wk^2)$ time and maintained dynamically in $\\tilde O(W^2k)$ time per update. Our static algorithm can also be implemented in $\\tilde O(n+k^{2.5})$ time.","sentences":["The edit distance of two strings is the minimum number of insertions, deletions, and substitutions needed to transform one string into the other.","The textbook algorithm determines the edit distance of length-$n$ strings in $O(n^2)$ time, which is optimal up to subpolynomial factors under Orthogonal Vectors Hypothesis.","In the bounded version of the problem, parameterized by the edit distance $k$, the algorithm of Landau and Vishkin","[JCSS'88] achieves $O(n+k^2)$ time, which is optimal as a function of $n$ and $k$.   The dynamic version of the problem asks to maintain the edit distance of two strings that change dynamically, with each update modeled as an edit.","A folklore approach supports updates in $\\tilde O(k^2)$ time, where $\\tilde O(\\cdot)$ hides polylogarithmic factors.","Recently, Charalampopoulos, Kociumaka, and Mozes [CPM'20] showed an algorithm with update time $\\tilde O(n)$, which is optimal under OVH in terms of $n$. The update time of $\\tilde O(\\min\\{n,k^2\\})$ raised an exciting open question of whether $\\tilde O(k)$ is possible; we answer it affirmatively.   ","Our solution relies on tools originating from weighted edit distance, where the weight of each edit depends on the edit type and the characters involved.","The textbook algorithm supports weights, but the Landau-Vishkin approach does not, and a simple $O(nk)$-time procedure long remained the fastest for bounded weighted edit distance.","Only recently, Das et al.","[STOC'23] provided an $O(n+k^5)$-time algorithm, whereas Cassis, Kociumaka, and Wellnitz","[FOCS'23] presented an $\\tilde O(n+\\sqrt{nk^3})$-time solution and a matching conditional lower bound.","In this paper, we show that, for integer edit weights between $0$ and $W$, weighted edit distance can be computed in $\\tilde O(n+Wk^2)$ time and maintained dynamically in $\\tilde O(W^2k)$ time per update.","Our static algorithm can also be implemented in $\\tilde O(n+k^{2.5})$ time."],"url":"http://arxiv.org/abs/2404.06401v1","category":"cs.DS"}
{"created":"2024-04-09 15:39:16","title":"Reconciling $S_8$: Insights from Interacting Dark Sectors","abstract":"We do a careful investigation of the prospects of dark energy (DE) interacting with cold dark matter (CDM) in alleviating the $S_8$ clustering tension. To this end, we consider various well-known parametrizations of the DE equation of state (EoS), and consider perturbations in both the dark sectors, along with an interaction term. Moreover, we perform a separate study for the phantom and non-phantom regimes. Using CMB, BAO and SNIa datasets, the constraints on the model parameters for each case have been obtained and a generic reduction in the $H_0-\\sigma_{8,0}$ correlation has been observed, both for constant and dynamical DE EoS. This reduction, coupled with a significant negative correlation between the interaction term and $\\sigma_{8,0}$, contributes to easing the clustering tension by lowering $\\sigma_{8,0}$ to somewhere in between the early CMB and late-time clustering measurements for the phantom regime, for almost all the models under consideration. In addition, this is achieved without exacerbating the Hubble tension. In this regard, the CPL and JBP models perform the best in relaxing the $S_8$ tension to $<1\\sigma$. However, for the non-phantom regime the $\\sigma_{8,0}$ tension tends to have worsened, which reassures the merits of phantom dark energy from latest data. We further do an investigation of the role of RSD datasets and find an overall reduction in tension, with a value of $\\sigma_{8,0}$ relatively closer to the CMB value. We finally check if further extensions of this scenario, like the inclusion of the sound speed of dark energy and warm dark matter interacting with DE, can have some effects.","sentences":["We do a careful investigation of the prospects of dark energy (DE) interacting with cold dark matter (CDM) in alleviating the $S_8$ clustering tension.","To this end, we consider various well-known parametrizations of the DE equation of state (EoS), and consider perturbations in both the dark sectors, along with an interaction term.","Moreover, we perform a separate study for the phantom and non-phantom regimes.","Using CMB, BAO and SNIa datasets, the constraints on the model parameters for each case have been obtained and a generic reduction in the $H_0-\\sigma_{8,0}$ correlation has been observed, both for constant and dynamical DE","EoS.","This reduction, coupled with a significant negative correlation between the interaction term and $\\sigma_{8,0}$, contributes to easing the clustering tension by lowering $\\sigma_{8,0}$ to somewhere in between the early CMB and late-time clustering measurements for the phantom regime, for almost all the models under consideration.","In addition, this is achieved without exacerbating the Hubble tension.","In this regard, the CPL and JBP models perform the best in relaxing the $S_8$ tension to $<1\\sigma$. However, for the non-phantom regime the $\\sigma_{8,0}$ tension tends to have worsened, which reassures the merits of phantom dark energy from latest data.","We further do an investigation of the role of RSD datasets and find an overall reduction in tension, with a value of $\\sigma_{8,0}$ relatively closer to the CMB value.","We finally check if further extensions of this scenario, like the inclusion of the sound speed of dark energy and warm dark matter interacting with DE, can have some effects."],"url":"http://arxiv.org/abs/2404.06396v1","category":"astro-ph.CO"}
{"created":"2024-04-09 15:35:02","title":"Exploring Neural Network Landscapes: Star-Shaped and Geodesic Connectivity","abstract":"One of the most intriguing findings in the structure of neural network landscape is the phenomenon of mode connectivity: For two typical global minima, there exists a path connecting them without barrier. This concept of mode connectivity has played a crucial role in understanding important phenomena in deep learning.   In this paper, we conduct a fine-grained analysis of this connectivity phenomenon. First, we demonstrate that in the overparameterized case, the connecting path can be as simple as a two-piece linear path, and the path length can be nearly equal to the Euclidean distance. This finding suggests that the landscape should be nearly convex in a certain sense. Second, we uncover a surprising star-shaped connectivity: For a finite number of typical minima, there exists a center on minima manifold that connects all of them simultaneously via linear paths. These results are provably valid for linear networks and two-layer ReLU networks under a teacher-student setup, and are empirically supported by models trained on MNIST and CIFAR-10.","sentences":["One of the most intriguing findings in the structure of neural network landscape is the phenomenon of mode connectivity: For two typical global minima, there exists a path connecting them without barrier.","This concept of mode connectivity has played a crucial role in understanding important phenomena in deep learning.   ","In this paper, we conduct a fine-grained analysis of this connectivity phenomenon.","First, we demonstrate that in the overparameterized case, the connecting path can be as simple as a two-piece linear path, and the path length can be nearly equal to the Euclidean distance.","This finding suggests that the landscape should be nearly convex in a certain sense.","Second, we uncover a surprising star-shaped connectivity: For a finite number of typical minima, there exists a center on minima manifold that connects all of them simultaneously via linear paths.","These results are provably valid for linear networks and two-layer ReLU networks under a teacher-student setup, and are empirically supported by models trained on MNIST and CIFAR-10."],"url":"http://arxiv.org/abs/2404.06391v1","category":"cs.LG"}
{"created":"2024-04-09 15:29:16","title":"The Power in Communication: Power Regularization of Communication for Autonomy in Cooperative Multi-Agent Reinforcement Learning","abstract":"Communication plays a vital role for coordination in Multi-Agent Reinforcement Learning (MARL) systems. However, misaligned agents can exploit other agents' trust and delegated power to the communication medium. In this paper, we propose power regularization as a method to limit the adverse effects of communication by misaligned agents, specifically communication which impairs the performance of cooperative agents. Power is a measure of the influence one agent's actions have over another agent's policy. By introducing power regularization, we aim to allow designers to control or reduce agents' dependency on communication when appropriate, and make them more resilient to performance deterioration due to misuses of communication. We investigate several environments in which power regularization can be a valuable capability for learning different policies that reduce the effect of power dynamics between agents during communication.","sentences":["Communication plays a vital role for coordination in Multi-Agent Reinforcement Learning (MARL) systems.","However, misaligned agents can exploit other agents' trust and delegated power to the communication medium.","In this paper, we propose power regularization as a method to limit the adverse effects of communication by misaligned agents, specifically communication which impairs the performance of cooperative agents.","Power is a measure of the influence one agent's actions have over another agent's policy.","By introducing power regularization, we aim to allow designers to control or reduce agents' dependency on communication when appropriate, and make them more resilient to performance deterioration due to misuses of communication.","We investigate several environments in which power regularization can be a valuable capability for learning different policies that reduce the effect of power dynamics between agents during communication."],"url":"http://arxiv.org/abs/2404.06387v1","category":"cs.MA"}
{"created":"2024-04-09 15:24:53","title":"Traffic Signal Control and Speed Offset Coordination Using Q-Learning for Arterial Road Networks","abstract":"Arterial traffic interacts with freeway traffic, yet the two are controlled independently. Arterial traffic signals do not take into account freeway traffic and how ramps control ingress traffic and have no control over egress traffic from the freeway. This often results in long queues in either direction that block ramps and spill over to arterial streets or freeway lanes. In this paper, we propose an adaptive arterial traffic control strategy that combines traffic signal control (TSC) and dynamic speed offset (DSO) coordination using a Q-learning algorithm for a traffic network that involves a freeway segment and adjacent arterial streets. The TSC agent computes the signal cycle length and split based on observed intersection demands and adjacent freeway off-ramp queues. The DSO agent computes the relative offset and the recommended speeds of both ways between consecutive intersections based on their physical distance, intersection queues, and signal cycles. We evaluate the performance of the proposed arterial traffic control strategy using microscopic traffic simulations of an arterial corridor with seven intersections near the I-710 freeway. The proposed QL-based control significantly outperforms a fixed-time control and MAXBAND in terms of the travel time and the number of stops under low or moderate demands. In high-demand scenarios, the travel-time benefit provided by the QL-based control is reduced as it mitigates off-ramp and intersection queues, which is a necessary trade-off in our perspective. In addition, mutual benefit is obtained by implementing freeway and arterial traffic control simultaneously.","sentences":["Arterial traffic interacts with freeway traffic, yet the two are controlled independently.","Arterial traffic signals do not take into account freeway traffic and how ramps control ingress traffic and have no control over egress traffic from the freeway.","This often results in long queues in either direction that block ramps and spill over to arterial streets or freeway lanes.","In this paper, we propose an adaptive arterial traffic control strategy that combines traffic signal control (TSC) and dynamic speed offset (DSO) coordination using a Q-learning algorithm for a traffic network that involves a freeway segment and adjacent arterial streets.","The TSC agent computes the signal cycle length and split based on observed intersection demands and adjacent freeway off-ramp queues.","The DSO agent computes the relative offset and the recommended speeds of both ways between consecutive intersections based on their physical distance, intersection queues, and signal cycles.","We evaluate the performance of the proposed arterial traffic control strategy using microscopic traffic simulations of an arterial corridor with seven intersections near the I-710 freeway.","The proposed QL-based control significantly outperforms a fixed-time control and MAXBAND in terms of the travel time and the number of stops under low or moderate demands.","In high-demand scenarios, the travel-time benefit provided by the QL-based control is reduced as it mitigates off-ramp and intersection queues, which is a necessary trade-off in our perspective.","In addition, mutual benefit is obtained by implementing freeway and arterial traffic control simultaneously."],"url":"http://arxiv.org/abs/2404.06382v1","category":"eess.SY"}
{"created":"2024-04-09 15:21:14","title":"Analyzing the Atmospheric Dispersion Correction of the Gemini Planet Imager: residual dispersion above design requirements","abstract":"The Atmospheric Dispersion Corrector (ADC) of the Gemini Planet Imager (GPI) corrects the chromatic dispersion caused by differential atmospheric refraction (DAR), making it an important optic for exoplanet observation. Despite requiring less than 5 mas of residual DAR to avoid potentially affecting the coronagraph, the GPI ADC averages $\\sim7$ and $\\sim11$ mas of residual DAR in $H$ and $J$ band respectively. We analyzed GPI data in those bands to find explanations for the underperformance. We found the model GPI uses to predict DAR underestimates humidity's impact on incident DAR, causing on average a 0.54 mas increase in $H$ band residual DAR. Additionally, the GPI ADC consistently undercorrects in $H$ band by about 7 mas, causing almost all the $H$ band residual DAR. $J$ band does not have such an offset. Perpendicular dispersion induced by the GPI ADC, potentially from a misalignment in the prisms' relative orientation, causes 86% of the residual DAR in $J$ band. Correcting these issues could reduce residual DAR, thereby improving exoplanet detection. We also made a new approximation for the index of refraction of air from 0.7 microns to 1.36 microns that more accurately accounts for the effects of humidity.","sentences":["The Atmospheric Dispersion Corrector (ADC) of the Gemini Planet Imager (GPI) corrects the chromatic dispersion caused by differential atmospheric refraction (DAR), making it an important optic for exoplanet observation.","Despite requiring less than 5 mas of residual DAR to avoid potentially affecting the coronagraph, the GPI ADC averages $\\sim7$ and $\\sim11$ mas of residual DAR in $H$ and $J$ band respectively.","We analyzed GPI data in those bands to find explanations for the underperformance.","We found the model GPI uses to predict DAR underestimates humidity's impact on incident DAR, causing on average a 0.54 mas increase in $H$ band residual DAR.","Additionally, the GPI ADC consistently undercorrects in $H$ band by about 7 mas, causing almost all the $H$ band residual DAR.","$J$ band does not have such an offset.","Perpendicular dispersion induced by the GPI ADC, potentially from a misalignment in the prisms' relative orientation, causes 86% of the residual DAR in $J$ band.","Correcting these issues could reduce residual DAR, thereby improving exoplanet detection.","We also made a new approximation for the index of refraction of air from 0.7 microns to 1.36 microns that more accurately accounts for the effects of humidity."],"url":"http://arxiv.org/abs/2404.06378v2","category":"astro-ph.IM"}
{"created":"2024-04-09 15:18:50","title":"Emergent Modified Gravity","abstract":"A complete canonical formulation of general covariance makes it possible to construct new modified theories of gravity that are not of higher-curvature form, as shown here in a spherically symmetric setting. The usual uniqueness theorems are evaded by using a crucial and novel ingredient, allowing for fundamental fields of gravity distinct from an emergent space-time metric that provides a geometrical structure to all solutions. As specific examples, there are new expansion-shear couplings in cosmological models, a form of modified Newtonian dynamics (MOND) can appear in a space-time covariant theory without introducing extra fields, and related effects help to make effective models of canonical quantum gravity fully consistent with general covariance.","sentences":["A complete canonical formulation of general covariance makes it possible to construct new modified theories of gravity that are not of higher-curvature form, as shown here in a spherically symmetric setting.","The usual uniqueness theorems are evaded by using a crucial and novel ingredient, allowing for fundamental fields of gravity distinct from an emergent space-time metric that provides a geometrical structure to all solutions.","As specific examples, there are new expansion-shear couplings in cosmological models, a form of modified Newtonian dynamics (MOND) can appear in a space-time covariant theory without introducing extra fields, and related effects help to make effective models of canonical quantum gravity fully consistent with general covariance."],"url":"http://arxiv.org/abs/2404.06375v1","category":"gr-qc"}
{"created":"2024-04-09 15:14:15","title":"Deterministic and Stochastic Geometric Mechanics for Hall MHD","abstract":"We derive new models of stochastic Hall magnetohydrodynamics (MHD) by using a symmetry-reduced stochastic Euler-Poincar\\'e variational principle. The new stochastic Hall MHD theory has potential applications for uncertainty quantification and data assimilation in space plasma (space weather) and solar physics. The stochastic geometric mechanics approach we take here produces coordinate-free results which may then be applied in a variety of spatial configurations.","sentences":["We derive new models of stochastic Hall magnetohydrodynamics (MHD) by using a symmetry-reduced stochastic Euler-Poincar\\'e variational principle.","The new stochastic Hall MHD theory has potential applications for uncertainty quantification and data assimilation in space plasma (space weather) and solar physics.","The stochastic geometric mechanics approach we take here produces coordinate-free results which may then be applied in a variety of spatial configurations."],"url":"http://arxiv.org/abs/2404.06528v1","category":"physics.plasm-ph"}
{"created":"2024-04-09 14:59:13","title":"Minimizing the determinant of the graph Laplacian","abstract":"In this paper, we study extremal values for the determinant of the weighted graph Laplacian under simple nondegeneracy conditions on the weights. We derive necessary and sufficient conditions for the determinant of the Laplacian to be bounded away from zero and for the existence of a minimizing set of weights. These conditions are given both in terms of properties of random spanning trees and in terms of a type of density on graphs. These results generalize and extend the work of [7].","sentences":["In this paper, we study extremal values for the determinant of the weighted graph Laplacian under simple nondegeneracy conditions on the weights.","We derive necessary and sufficient conditions for the determinant of the Laplacian to be bounded away from zero and for the existence of a minimizing set of weights.","These conditions are given both in terms of properties of random spanning trees and in terms of a type of density on graphs.","These results generalize and extend the work of [7]."],"url":"http://arxiv.org/abs/2404.06363v1","category":"math.CO"}
{"created":"2024-04-09 14:42:31","title":"HPNet: Dynamic Trajectory Forecasting with Historical Prediction Attention","abstract":"Predicting the trajectories of road agents is essential for autonomous driving systems. The recent mainstream methods follow a static paradigm, which predicts the future trajectory by using a fixed duration of historical frames. These methods make the predictions independently even at adjacent time steps, which leads to potential instability and temporal inconsistency. As successive time steps have largely overlapping historical frames, their forecasting should have intrinsic correlation, such as overlapping predicted trajectories should be consistent, or be different but share the same motion goal depending on the road situation. Motivated by this, in this work, we introduce HPNet, a novel dynamic trajectory forecasting method. Aiming for stable and accurate trajectory forecasting, our method leverages not only historical frames including maps and agent states, but also historical predictions. Specifically, we newly design a Historical Prediction Attention module to automatically encode the dynamic relationship between successive predictions. Besides, it also extends the attention range beyond the currently visible window benefitting from the use of historical predictions. The proposed Historical Prediction Attention together with the Agent Attention and Mode Attention is further formulated as the Triple Factorized Attention module, serving as the core design of HPNet.Experiments on the Argoverse and INTERACTION datasets show that HPNet achieves state-of-the-art performance, and generates accurate and stable future trajectories. Our code are available at https://github.com/XiaolongTang23/HPNet.","sentences":["Predicting the trajectories of road agents is essential for autonomous driving systems.","The recent mainstream methods follow a static paradigm, which predicts the future trajectory by using a fixed duration of historical frames.","These methods make the predictions independently even at adjacent time steps, which leads to potential instability and temporal inconsistency.","As successive time steps have largely overlapping historical frames, their forecasting should have intrinsic correlation, such as overlapping predicted trajectories should be consistent, or be different but share the same motion goal depending on the road situation.","Motivated by this, in this work, we introduce HPNet, a novel dynamic trajectory forecasting method.","Aiming for stable and accurate trajectory forecasting, our method leverages not only historical frames including maps and agent states, but also historical predictions.","Specifically, we newly design a Historical Prediction Attention module to automatically encode the dynamic relationship between successive predictions.","Besides, it also extends the attention range beyond the currently visible window benefitting from the use of historical predictions.","The proposed Historical Prediction Attention together with the Agent Attention and Mode Attention is further formulated as the Triple Factorized Attention module, serving as the core design of HPNet.","Experiments on the Argoverse and INTERACTION datasets show that HPNet achieves state-of-the-art performance, and generates accurate and stable future trajectories.","Our code are available at https://github.com/XiaolongTang23/HPNet."],"url":"http://arxiv.org/abs/2404.06351v2","category":"cs.CV"}
{"created":"2024-04-09 14:34:48","title":"RAR-b: Reasoning as Retrieval Benchmark","abstract":"Semantic textual similartiy (STS) and information retrieval tasks (IR) tasks have been the two major avenues to record the progress of embedding models in the past few years. Under the emerging Retrieval-augmented Generation (RAG) paradigm, we envision the need to evaluate next-level language understanding abilities of embedding models, and take a conscious look at the reasoning abilities stored in them. Addressing this, we pose the question: Can retrievers solve reasoning problems? By transforming reasoning tasks into retrieval tasks, we find that without specifically trained for reasoning-level language understanding, current state-of-the-art retriever models may still be far from being competent for playing the role of assisting LLMs, especially in reasoning-intensive tasks. Moreover, albeit trained to be aware of instructions, instruction-aware IR models are often better off without instructions in inference time for reasoning tasks, posing an overlooked retriever-LLM behavioral gap for the research community to align. However, recent decoder-based embedding models show great promise in narrowing the gap, highlighting the pathway for embedding models to achieve reasoning-level language understanding. We also show that, although current off-the-shelf re-ranker models fail on these tasks, injecting reasoning abilities into them through fine-tuning still appears easier than doing so to bi-encoders, and we are able to achieve state-of-the-art performance across all tasks by fine-tuning a reranking model. We release Reasoning as Retrieval Benchmark (RAR-b), a holistic suite of tasks and settings to evaluate the reasoning abilities stored in retriever models. RAR-b is available at https://github.com/gowitheflow-1998/RAR-b.","sentences":["Semantic textual similartiy (STS) and information retrieval tasks (IR) tasks have been the two major avenues to record the progress of embedding models in the past few years.","Under the emerging Retrieval-augmented Generation (RAG) paradigm, we envision the need to evaluate next-level language understanding abilities of embedding models, and take a conscious look at the reasoning abilities stored in them.","Addressing this, we pose the question: Can retrievers solve reasoning problems?","By transforming reasoning tasks into retrieval tasks, we find that without specifically trained for reasoning-level language understanding, current state-of-the-art retriever models may still be far from being competent for playing the role of assisting LLMs, especially in reasoning-intensive tasks.","Moreover, albeit trained to be aware of instructions, instruction-aware IR models are often better off without instructions in inference time for reasoning tasks, posing an overlooked retriever-LLM behavioral gap for the research community to align.","However, recent decoder-based embedding models show great promise in narrowing the gap, highlighting the pathway for embedding models to achieve reasoning-level language understanding.","We also show that, although current off-the-shelf re-ranker models fail on these tasks, injecting reasoning abilities into them through fine-tuning still appears easier than doing so to bi-encoders, and we are able to achieve state-of-the-art performance across all tasks by fine-tuning a reranking model.","We release Reasoning as Retrieval Benchmark (RAR-b), a holistic suite of tasks and settings to evaluate the reasoning abilities stored in retriever models.","RAR-b is available at https://github.com/gowitheflow-1998/RAR-b."],"url":"http://arxiv.org/abs/2404.06347v1","category":"cs.CL"}
{"created":"2024-04-09 14:30:59","title":"Does the Earth's rotation speed really tend to accelerate for a long time?","abstract":"Recently published studies suggested that the difference between Universal and Coordinated Time UT1-UTC could reach a large positive value in a few years, making it necessary to introduce a negative leap second into the UTC scale for the first time in its history. Based on the latest UT1 series provided by the International Earth Rotation and Reference Systems Service (IERS) and its prediction, it was shown that the tendency to acceleration of the Earth's rotation observed over past four years most likely will return to the deceleration, which is the usual behavior of the Earth rotational dynamics.","sentences":["Recently published studies suggested that the difference between Universal and Coordinated Time UT1-UTC could reach a large positive value in a few years, making it necessary to introduce a negative leap second into the UTC scale for the first time in its history.","Based on the latest UT1 series provided by the International Earth Rotation and Reference Systems Service (IERS) and its prediction, it was shown that the tendency to acceleration of the Earth's rotation observed over past four years most likely will return to the deceleration, which is the usual behavior of the Earth rotational dynamics."],"url":"http://arxiv.org/abs/2404.06343v1","category":"astro-ph.EP"}
{"created":"2024-04-09 14:26:05","title":"Oracle-Net for nonlinear compressed sensing in Electrical Impedance Tomography reconstruction problems","abstract":"Sparse recovery principles play an important role in solving many nonlinear ill-posed inverse problems. We investigate a variational framework with support Oracle for compressed sensing sparse reconstructions, where the available measurements are nonlinear and possibly corrupted by noise. A graph neural network, named Oracle-Net, is proposed to predict the support from the nonlinear measurements and is integrated into a regularized recovery model to enforce sparsity. The derived nonsmooth optimization problem is then efficiently solved through a constrained proximal gradient method. Error bounds on the approximate solution of the proposed Oracle-based optimization are provided in the context of the ill-posed Electrical Impedance Tomography problem. Numerical solutions of the EIT nonlinear inverse reconstruction problem confirm the potential of the proposed method which improves the reconstruction quality from undersampled measurements, under sparsity assumptions.","sentences":["Sparse recovery principles play an important role in solving many nonlinear ill-posed inverse problems.","We investigate a variational framework with support Oracle for compressed sensing sparse reconstructions, where the available measurements are nonlinear and possibly corrupted by noise.","A graph neural network, named Oracle-Net, is proposed to predict the support from the nonlinear measurements and is integrated into a regularized recovery model to enforce sparsity.","The derived nonsmooth optimization problem is then efficiently solved through a constrained proximal gradient method.","Error bounds on the approximate solution of the proposed Oracle-based optimization are provided in the context of the ill-posed Electrical Impedance Tomography problem.","Numerical solutions of the EIT nonlinear inverse reconstruction problem confirm the potential of the proposed method which improves the reconstruction quality from undersampled measurements, under sparsity assumptions."],"url":"http://arxiv.org/abs/2404.06342v1","category":"math.NA"}
{"created":"2024-04-09 14:25:27","title":"Finding fake reviews in e-commerce platforms by using hybrid algorithms","abstract":"Sentiment analysis, a vital component in natural language processing, plays a crucial role in understanding the underlying emotions and opinions expressed in textual data. In this paper, we propose an innovative ensemble approach for sentiment analysis for finding fake reviews that amalgamate the predictive capabilities of Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Decision Tree classifiers. Our ensemble architecture strategically combines these diverse models to capitalize on their strengths while mitigating inherent weaknesses, thereby achieving superior accuracy and robustness in fake review prediction. By combining all the models of our classifiers, the predictive performance is boosted and it also fosters adaptability to varied linguistic patterns and nuances present in real-world datasets. The metrics accounted for on fake reviews demonstrate the efficacy and competitiveness of the proposed ensemble method against traditional single-model approaches. Our findings underscore the potential of ensemble techniques in advancing the state-of-the-art in finding fake reviews using hybrid algorithms, with implications for various applications in different social media and e-platforms to find the best reviews and neglect the fake ones, eliminating puffery and bluffs.","sentences":["Sentiment analysis, a vital component in natural language processing, plays a crucial role in understanding the underlying emotions and opinions expressed in textual data.","In this paper, we propose an innovative ensemble approach for sentiment analysis for finding fake reviews that amalgamate the predictive capabilities of Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Decision Tree classifiers.","Our ensemble architecture strategically combines these diverse models to capitalize on their strengths while mitigating inherent weaknesses, thereby achieving superior accuracy and robustness in fake review prediction.","By combining all the models of our classifiers, the predictive performance is boosted and it also fosters adaptability to varied linguistic patterns and nuances present in real-world datasets.","The metrics accounted for on fake reviews demonstrate the efficacy and competitiveness of the proposed ensemble method against traditional single-model approaches.","Our findings underscore the potential of ensemble techniques in advancing the state-of-the-art in finding fake reviews using hybrid algorithms, with implications for various applications in different social media and e-platforms to find the best reviews and neglect the fake ones, eliminating puffery and bluffs."],"url":"http://arxiv.org/abs/2404.06339v1","category":"cs.CL"}
{"created":"2024-04-09 14:24:49","title":"Statistical Estimation of Mean Lorentzian Line Width in Spectra by Gaussian Processes","abstract":"We propose a statistical approach for estimating the mean line width in spectra comprising Lorentzian, Gaussian, or Voigt line shapes. Our approach uses Gaussian processes in two stages to jointly model a spectrum and its Fourier transform. We generate statistical samples for the mean line width by drawing realizations for the Fourier transform and its derivative using Markov chain Monte Carlo methods. In addition to being fully automated, our method enables well-calibrated uncertainty quantification of the mean line width estimate through Bayesian inference. We validate our method using a simulation study and apply it to an experimental Raman spectrum of $\\beta$-carotene.","sentences":["We propose a statistical approach for estimating the mean line width in spectra comprising Lorentzian, Gaussian, or Voigt line shapes.","Our approach uses Gaussian processes in two stages to jointly model a spectrum and its Fourier transform.","We generate statistical samples for the mean line width by drawing realizations for the Fourier transform and its derivative using Markov chain Monte Carlo methods.","In addition to being fully automated, our method enables well-calibrated uncertainty quantification of the mean line width estimate through Bayesian inference.","We validate our method using a simulation study and apply it to an experimental Raman spectrum of $\\beta$-carotene."],"url":"http://arxiv.org/abs/2404.06338v1","category":"stat.AP"}
{"created":"2024-04-09 14:06:21","title":"Flow Fusion, Exploiting Measurement Redundancy for Smarter Allocation","abstract":"In petroleum production systems, continuous multiphase flow rates are essential for efficient operation. They provide situational awareness, enable production optimization, improve reservoir management and planning, and form the basis for allocation. Furthermore, they can be crucial to ensure a fair revenue split between stakeholders for complex production systems where operators share the facilities. Yet, due to complex multiphase flow dynamics and uncertain subsurface fluid properties, the flow rates are challenging to obtain with high accuracy. Consequently, flow rate measurement and estimation solutions, such as multiphase flow meters and virtual flow meters, have different degrees of accuracy and suitability, and impact production decisions and production allocation accordingly.   We propose a field-proven, data-driven framework for reconciliation and allocation. With data validation and reconciliation as the theoretical backbone, the solution exploits measurement redundancy to fuse together relevant flow rate information to infer the most likely flow rates in the production system based on quantifiable uncertainties. The framework consists of four modules: data-processing, uncertainty estimation, reconciliation, and gross error detection. The latter, being the focus of this paper, is a means to identify and mitigate the effect of measurements subject to systematic error, which can invalidate the reconciliation.   In this paper, we highlight that a combination of statistical tests and supporting logic for gross error detection and elimination can be beneficial in obtaining a more justifiable production allocation. Using the maximum power measurement test, the module can be limited in its ability to pinpoint the erroneous measurement. Yet, it is demonstrated that the detections can be convenient indications of gross errors and where these might reside in the production system.","sentences":["In petroleum production systems, continuous multiphase flow rates are essential for efficient operation.","They provide situational awareness, enable production optimization, improve reservoir management and planning, and form the basis for allocation.","Furthermore, they can be crucial to ensure a fair revenue split between stakeholders for complex production systems where operators share the facilities.","Yet, due to complex multiphase flow dynamics and uncertain subsurface fluid properties, the flow rates are challenging to obtain with high accuracy.","Consequently, flow rate measurement and estimation solutions, such as multiphase flow meters and virtual flow meters, have different degrees of accuracy and suitability, and impact production decisions and production allocation accordingly.   ","We propose a field-proven, data-driven framework for reconciliation and allocation.","With data validation and reconciliation as the theoretical backbone, the solution exploits measurement redundancy to fuse together relevant flow rate information to infer the most likely flow rates in the production system based on quantifiable uncertainties.","The framework consists of four modules: data-processing, uncertainty estimation, reconciliation, and gross error detection.","The latter, being the focus of this paper, is a means to identify and mitigate the effect of measurements subject to systematic error, which can invalidate the reconciliation.   ","In this paper, we highlight that a combination of statistical tests and supporting logic for gross error detection and elimination can be beneficial in obtaining a more justifiable production allocation.","Using the maximum power measurement test, the module can be limited in its ability to pinpoint the erroneous measurement.","Yet, it is demonstrated that the detections can be convenient indications of gross errors and where these might reside in the production system."],"url":"http://arxiv.org/abs/2404.06328v1","category":"eess.SP"}
{"created":"2024-04-09 14:02:34","title":"From chiral EFT to perturbative QCD: a Bayesian model mixing approach to symmetric nuclear matter","abstract":"Constraining the equation of state (EOS) of strongly interacting, dense matter is the focus of intense experimental, observational, and theoretical effort. Chiral effective field theory ($\\chi$EFT) can describe the EOS between the typical densities of nuclei and those in the outer cores of neutron stars while perturbative QCD (pQCD) can be applied to properties of deconfined quark matter, both with quantified theoretical uncertainties. However, describing the complete range of densities between nuclear saturation and an almost-free quark gas with a single EOS that has well-quantified uncertainties is a challenging problem. In this work, we argue that Bayesian multi-model inference from $\\chi$EFT and pQCD can help bridge the gap between the two theories: we combine the Gaussian random variables that constitute the theories' predictions for the pressure as a function of the density in symmetric nuclear matter. We do this using two Bayesian model mixing procedures: a pointwise approach, and a correlated approach implemented via a Gaussian process (GP), and present results for the pressure and speed of sound in each. The second method produces a smooth $\\chi$EFT-to-pQCD EOS. Without input data in the intermediate region, the choice of prior on the EOS, encoded through the GP kernel, as the prior on the EOS function space significantly affects the result in that region. We also discuss future extensions and applications to neutron star matter guided by recent EOS constraints from nuclear theory, nuclear experiment, and multi-messenger astronomy.","sentences":["Constraining the equation of state (EOS) of strongly interacting, dense matter is the focus of intense experimental, observational, and theoretical effort.","Chiral effective field theory ($\\chi$EFT) can describe the EOS between the typical densities of nuclei and those in the outer cores of neutron stars while perturbative QCD (pQCD) can be applied to properties of deconfined quark matter, both with quantified theoretical uncertainties.","However, describing the complete range of densities between nuclear saturation and an almost-free quark gas with a single EOS that has well-quantified uncertainties is a challenging problem.","In this work, we argue that Bayesian multi-model inference from $\\chi$EFT and pQCD can help bridge the gap between the two theories: we combine the Gaussian random variables that constitute the theories' predictions for the pressure as a function of the density in symmetric nuclear matter.","We do this using two Bayesian model mixing procedures: a pointwise approach, and a correlated approach implemented via a Gaussian process (GP), and present results for the pressure and speed of sound in each.","The second method produces a smooth $\\chi$EFT-to-pQCD EOS.","Without input data in the intermediate region, the choice of prior on the EOS, encoded through the GP kernel, as the prior on the EOS function space significantly affects the result in that region.","We also discuss future extensions and applications to neutron star matter guided by recent EOS constraints from nuclear theory, nuclear experiment, and multi-messenger astronomy."],"url":"http://arxiv.org/abs/2404.06323v1","category":"nucl-th"}
{"created":"2024-04-09 14:01:52","title":"Elastic ribbons in bubble columns: when elasticity, capillarity and gravity govern equilibrium configurations","abstract":"Taking advantage of the competition between elasticity and capillarity has proven to be an efficient way to design structures by folding, bending, or assembling elastic objects in contact with liquid interfaces. Elastocapillary effects often occur at scales where gravity does not play an important role, such as in microfabrication processes. However, the influence of gravity can become significant at the desktop scale, which is relevant for numerous situations including model experiments used to provide a fundamental physics understanding, working at easily accessible scales. We focus here on the case of elastic ribbons placed in two-dimensional bubble columns: by introducing an elastic ribbon inside the central soap films of a staircase bubble structure in a square cross-section column, the deviation from Plateau's laws (capillarity-dominated case dictating the shape of usual foams) can be quantified as a function of the rigidity of the ribbon. For long ribbons, gravity cannot be neglected. We provide a detailed theoretical analysis of the ribbon profile, taking into account capillarity, elasticity and gravity. We compute the total energy of the system and perform energy minimization under constraints, using Lagrangian mechanics. The model is then validated via a comparison with experiments with three different ribbon thicknesses.","sentences":["Taking advantage of the competition between elasticity and capillarity has proven to be an efficient way to design structures by folding, bending, or assembling elastic objects in contact with liquid interfaces.","Elastocapillary effects often occur at scales where gravity does not play an important role, such as in microfabrication processes.","However, the influence of gravity can become significant at the desktop scale, which is relevant for numerous situations including model experiments used to provide a fundamental physics understanding, working at easily accessible scales.","We focus here on the case of elastic ribbons placed in two-dimensional bubble columns: by introducing an elastic ribbon inside the central soap films of a staircase bubble structure in a square cross-section column, the deviation from Plateau's laws (capillarity-dominated case dictating the shape of usual foams) can be quantified as a function of the rigidity of the ribbon.","For long ribbons, gravity cannot be neglected.","We provide a detailed theoretical analysis of the ribbon profile, taking into account capillarity, elasticity and gravity.","We compute the total energy of the system and perform energy minimization under constraints, using Lagrangian mechanics.","The model is then validated via a comparison with experiments with three different ribbon thicknesses."],"url":"http://arxiv.org/abs/2404.06322v1","category":"cond-mat.soft"}
{"created":"2024-04-09 13:47:37","title":"On adversarial training and the 1 Nearest Neighbor classifier","abstract":"The ability to fool deep learning classifiers with tiny perturbations of the input has lead to the development of adversarial training in which the loss with respect to adversarial examples is minimized in addition to the training examples. While adversarial training improves the robustness of the learned classifiers, the procedure is computationally expensive, sensitive to hyperparameters and may still leave the classifier vulnerable to other types of small perturbations. In this paper we analyze the adversarial robustness of the 1 Nearest Neighbor (1NN) classifier and compare its performance to adversarial training. We prove that under reasonable assumptions, the 1 NN classifier will be robust to {\\em any} small image perturbation of the training images and will give high adversarial accuracy on test images as the number of training examples goes to infinity. In experiments with 45 different binary image classification problems taken from CIFAR10, we find that 1NN outperform TRADES (a powerful adversarial training algorithm) in terms of average adversarial accuracy. In additional experiments with 69 pretrained robust models for CIFAR10, we find that 1NN outperforms almost all of them in terms of robustness to perturbations that are only slightly different from those seen during training. Taken together, our results suggest that modern adversarial training methods still fall short of the robustness of the simple 1NN classifier. our code can be found at https://github.com/amirhagai/On-Adversarial-Training-And-The-1-Nearest-Neighbor-Classifier","sentences":["The ability to fool deep learning classifiers with tiny perturbations of the input has lead to the development of adversarial training in which the loss with respect to adversarial examples is minimized in addition to the training examples.","While adversarial training improves the robustness of the learned classifiers, the procedure is computationally expensive, sensitive to hyperparameters and may still leave the classifier vulnerable to other types of small perturbations.","In this paper we analyze the adversarial robustness of the 1 Nearest Neighbor (1NN) classifier and compare its performance to adversarial training.","We prove that under reasonable assumptions, the 1 NN classifier will be robust to {\\em any} small image perturbation of the training images and will give high adversarial accuracy on test images as the number of training examples goes to infinity.","In experiments with 45 different binary image classification problems taken from CIFAR10, we find that 1NN outperform TRADES (a powerful adversarial training algorithm) in terms of average adversarial accuracy.","In additional experiments with 69 pretrained robust models for CIFAR10, we find that 1NN outperforms almost all of them in terms of robustness to perturbations that are only slightly different from those seen during training.","Taken together, our results suggest that modern adversarial training methods still fall short of the robustness of the simple 1NN classifier.","our code can be found at https://github.com/amirhagai/On-Adversarial-Training-And-The-1-Nearest-Neighbor-Classifier"],"url":"http://arxiv.org/abs/2404.06313v1","category":"cs.LG"}
{"created":"2024-04-09 13:35:12","title":"Dynamical structures in phase-separating non-reciprocal polar active mixtures","abstract":"Non-reciprocal systems exhibit diverse dynamical phases whose character depends on the type and degree of non-reciprocity. In this study, we theoretically investigate dynamical structures in a mixture of non-reciprocally aligning polar active particles with repulsion, focusing on the performance on (and connection between) different levels of description. Linear stability analyses of the associated continuum model predict a profound influence of non-reciprocity, leading to phase separation, (anti-)flocking and asymmetric clustering behavior. On the microscopic level, particle simulations confirm the emergence of these dynamical phases and allow for a more in-depth investigation of (microscopic) properties, including orientational correlations and susceptibilities. In particular, our findings demonstrate that certain dynamical properties, like a chase-and-run behavior in the asymmetrical clustering phase, are overlooked in mean-field continuum theory, making microscopic simulations an indispensable tool for studying the effects of non-reciprocal alignment couplings.","sentences":["Non-reciprocal systems exhibit diverse dynamical phases whose character depends on the type and degree of non-reciprocity.","In this study, we theoretically investigate dynamical structures in a mixture of non-reciprocally aligning polar active particles with repulsion, focusing on the performance on (and connection between) different levels of description.","Linear stability analyses of the associated continuum model predict a profound influence of non-reciprocity, leading to phase separation, (anti-)flocking and asymmetric clustering behavior.","On the microscopic level, particle simulations confirm the emergence of these dynamical phases and allow for a more in-depth investigation of (microscopic) properties, including orientational correlations and susceptibilities.","In particular, our findings demonstrate that certain dynamical properties, like a chase-and-run behavior in the asymmetrical clustering phase, are overlooked in mean-field continuum theory, making microscopic simulations an indispensable tool for studying the effects of non-reciprocal alignment couplings."],"url":"http://arxiv.org/abs/2404.06305v1","category":"cond-mat.soft"}
{"created":"2024-04-09 13:34:19","title":"Constraining the Coronal Properties of AB Dor in the Radio Regime","abstract":"We present a multiwavelength study of AB Doradus, combining modelling that incorporates a spectropolarimetric magnetic field map with 8.4 GHz radio interferometry to measure the coronal extent and density of this young star. We use the surface magnetic field map to produce a 3D extrapolation of AB Dor's coronal magnetic field. From this model we create synthetic radio images throughout the stellar rotation period which we can compare with the interferometric radio observations. Our models reproduce the two-lobe structure seen in the radio observations. We successfully fit the observed flux magnitude and lobe separation with our model. We conclude that that the features seen in the radio images are a result of centrifugal containment of hot gas at the peak of closed magnetic loops, and that the corona of AB Dor extends to about 8-10 stellar radii, making it much more extended than the present-day solar corona.","sentences":["We present a multiwavelength study of AB Doradus, combining modelling that incorporates a spectropolarimetric magnetic field map with 8.4 GHz radio interferometry to measure the coronal extent and density of this young star.","We use the surface magnetic field map to produce a 3D extrapolation of AB Dor's coronal magnetic field.","From this model we create synthetic radio images throughout the stellar rotation period which we can compare with the interferometric radio observations.","Our models reproduce the two-lobe structure seen in the radio observations.","We successfully fit the observed flux magnitude and lobe separation with our model.","We conclude that that the features seen in the radio images are a result of centrifugal containment of hot gas at the peak of closed magnetic loops, and that the corona of AB Dor extends to about 8-10 stellar radii, making it much more extended than the present-day solar corona."],"url":"http://arxiv.org/abs/2404.06304v1","category":"astro-ph.SR"}
{"created":"2024-04-09 13:19:25","title":"Optimal Stopping with Interdependent Values","abstract":"We study online selection problems in both the prophet and secretary settings, when arriving agents have interdependent values. In the interdependent values model, introduced in the seminal work of Milgrom and Weber [1982], each agent has a private signal and the value of an agent is a function of the signals held by all agents. Results in online selection crucially rely on some degree of independence of values, which is conceptually at odds with the interdependent values model. For prophet and secretary models under the standard independent values assumption, prior works provide constant factor approximations to the welfare. On the other hand, when agents have interdependent values, prior works in Economics and Computer Science provide truthful mechanisms that obtain optimal and approximately optimal welfare under certain assumptions on the valuation functions.   We bring together these two important lines of work and provide the first constant factor approximations for prophet and secretary problems with interdependent values. We consider both the algorithmic setting, where agents are non-strategic (but have interdependent values), and the mechanism design setting with strategic agents. All our results are constructive and use simple stopping rules.","sentences":["We study online selection problems in both the prophet and secretary settings, when arriving agents have interdependent values.","In the interdependent values model, introduced in the seminal work of Milgrom and Weber [1982], each agent has a private signal and the value of an agent is a function of the signals held by all agents.","Results in online selection crucially rely on some degree of independence of values, which is conceptually at odds with the interdependent values model.","For prophet and secretary models under the standard independent values assumption, prior works provide constant factor approximations to the welfare.","On the other hand, when agents have interdependent values, prior works in Economics and Computer Science provide truthful mechanisms that obtain optimal and approximately optimal welfare under certain assumptions on the valuation functions.   ","We bring together these two important lines of work and provide the first constant factor approximations for prophet and secretary problems with interdependent values.","We consider both the algorithmic setting, where agents are non-strategic (but have interdependent values), and the mechanism design setting with strategic agents.","All our results are constructive and use simple stopping rules."],"url":"http://arxiv.org/abs/2404.06293v1","category":"cs.GT"}
{"created":"2024-04-09 13:18:52","title":"nEMO: Dataset of Emotional Speech in Polish","abstract":"Speech emotion recognition has become increasingly important in recent years due to its potential applications in healthcare, customer service, and personalization of dialogue systems. However, a major issue in this field is the lack of datasets that adequately represent basic emotional states across various language families. As datasets covering Slavic languages are rare, there is a need to address this research gap. This paper presents the development of nEMO, a novel corpus of emotional speech in Polish. The dataset comprises over 3 hours of samples recorded with the participation of nine actors portraying six emotional states: anger, fear, happiness, sadness, surprise, and a neutral state. The text material used was carefully selected to represent the phonetics of the Polish language adequately. The corpus is freely available under the terms of a Creative Commons license (CC BY-NC-SA 4.0).","sentences":["Speech emotion recognition has become increasingly important in recent years due to its potential applications in healthcare, customer service, and personalization of dialogue systems.","However, a major issue in this field is the lack of datasets that adequately represent basic emotional states across various language families.","As datasets covering Slavic languages are rare, there is a need to address this research gap.","This paper presents the development of nEMO, a novel corpus of emotional speech in Polish.","The dataset comprises over 3 hours of samples recorded with the participation of nine actors portraying six emotional states: anger, fear, happiness, sadness, surprise, and a neutral state.","The text material used was carefully selected to represent the phonetics of the Polish language adequately.","The corpus is freely available under the terms of a Creative Commons license (CC BY-NC-SA 4.0)."],"url":"http://arxiv.org/abs/2404.06292v1","category":"cs.CL"}
{"created":"2024-04-09 13:13:24","title":"Counterfactual Reasoning for Multi-Label Image Classification via Patching-Based Training","abstract":"The key to multi-label image classification (MLC) is to improve model performance by leveraging label correlations. Unfortunately, it has been shown that overemphasizing co-occurrence relationships can cause the overfitting issue of the model, ultimately leading to performance degradation. In this paper, we provide a causal inference framework to show that the correlative features caused by the target object and its co-occurring objects can be regarded as a mediator, which has both positive and negative impacts on model predictions. On the positive side, the mediator enhances the recognition performance of the model by capturing co-occurrence relationships; on the negative side, it has the harmful causal effect that causes the model to make an incorrect prediction for the target object, even when only co-occurring objects are present in an image. To address this problem, we propose a counterfactual reasoning method to measure the total direct effect, achieved by enhancing the direct effect caused only by the target object. Due to the unknown location of the target object, we propose patching-based training and inference to accomplish this goal, which divides an image into multiple patches and identifies the pivot patch that contains the target object. Experimental results on multiple benchmark datasets with diverse configurations validate that the proposed method can achieve state-of-the-art performance.","sentences":["The key to multi-label image classification (MLC) is to improve model performance by leveraging label correlations.","Unfortunately, it has been shown that overemphasizing co-occurrence relationships can cause the overfitting issue of the model, ultimately leading to performance degradation.","In this paper, we provide a causal inference framework to show that the correlative features caused by the target object and its co-occurring objects can be regarded as a mediator, which has both positive and negative impacts on model predictions.","On the positive side, the mediator enhances the recognition performance of the model by capturing co-occurrence relationships; on the negative side, it has the harmful causal effect that causes the model to make an incorrect prediction for the target object, even when only co-occurring objects are present in an image.","To address this problem, we propose a counterfactual reasoning method to measure the total direct effect, achieved by enhancing the direct effect caused only by the target object.","Due to the unknown location of the target object, we propose patching-based training and inference to accomplish this goal, which divides an image into multiple patches and identifies the pivot patch that contains the target object.","Experimental results on multiple benchmark datasets with diverse configurations validate that the proposed method can achieve state-of-the-art performance."],"url":"http://arxiv.org/abs/2404.06287v1","category":"cs.CV"}
{"created":"2024-04-09 13:05:07","title":"IsoDAR@Yemilab: Preliminary Design Report -- Volume I: Cyclotron Driver","abstract":"This Preliminary Design Report (PDR) describes the IsoDAR electron-antineutrino source. Volumes I and II are site-independent and describe the cyclotron driver providing a 10~mA proton beam, and the medium energy beam transport line and target, respectively. Volume III describes the installation at the Yemilab underground laboratory in South Korea. The IsoDAR driver and target will produce a mole of electron-antineutrinos over the course of five years. Paired with a kton-scale liquid scintillator detector, it will enable an impressive particle physics program including searches for new symmetries, new interactions and new particles. Here in Volume I, we describe the driver, which includes the ion source, low energy beam transport, and cyclotron. The latter features radiofrequency quadrupole (RFQ) direct axial injection and represents the first accelerator purpose-built to make use of vortex motion.","sentences":["This Preliminary Design Report (PDR) describes the IsoDAR electron-antineutrino source.","Volumes I and II are site-independent and describe the cyclotron driver providing a 10~mA proton beam, and the medium energy beam transport line and target, respectively.","Volume III describes the installation at the Yemilab underground laboratory in South Korea.","The IsoDAR driver and target will produce a mole of electron-antineutrinos over the course of five years.","Paired with a kton-scale liquid scintillator detector, it will enable an impressive particle physics program including searches for new symmetries, new interactions and new particles.","Here in Volume I, we describe the driver, which includes the ion source, low energy beam transport, and cyclotron.","The latter features radiofrequency quadrupole (RFQ) direct axial injection and represents the first accelerator purpose-built to make use of vortex motion."],"url":"http://arxiv.org/abs/2404.06281v1","category":"physics.acc-ph"}
{"created":"2024-04-09 12:48:24","title":"Robust Confidence Intervals in Stereo Matching using Possibility Theory","abstract":"We propose a method for estimating disparity confidence intervals in stereo matching problems. Confidence intervals provide complementary information to usual confidence measures. To the best of our knowledge, this is the first method creating disparity confidence intervals based on the cost volume. This method relies on possibility distributions to interpret the epistemic uncertainty of the cost volume. Our method has the benefit of having a white-box nature, differing in this respect from current state-of-the-art deep neural networks approaches. The accuracy and size of confidence intervals are validated using the Middlebury stereo datasets as well as a dataset of satellite images. This contribution is freely available on GitHub.","sentences":["We propose a method for estimating disparity confidence intervals in stereo matching problems.","Confidence intervals provide complementary information to usual confidence measures.","To the best of our knowledge, this is the first method creating disparity confidence intervals based on the cost volume.","This method relies on possibility distributions to interpret the epistemic uncertainty of the cost volume.","Our method has the benefit of having a white-box nature, differing in this respect from current state-of-the-art deep neural networks approaches.","The accuracy and size of confidence intervals are validated using the Middlebury stereo datasets as well as a dataset of satellite images.","This contribution is freely available on GitHub."],"url":"http://arxiv.org/abs/2404.06273v1","category":"cs.CV"}
{"created":"2024-04-09 12:47:47","title":"The Gravitational Chiral Anomaly at Finite Temperature and Density","abstract":"We investigate the gravitational anomaly vertex $\\langle TTJ_5\\rangle$ (graviton - graviton - axial current) under conditions of finite density and temperature. Through a direct analysis of perturbative contributions, we demonstrate that neither finite temperature nor finite fermion density affects the gravitational chiral anomaly. These results find application in several contexts, from topological materials to the early universe plasma. They affect the decay of any axion or axion-like particle into gravitational waves, in very dense and hot environments.","sentences":["We investigate the gravitational anomaly vertex $\\langle TTJ_5\\rangle$ (graviton - graviton - axial current) under conditions of finite density and temperature.","Through a direct analysis of perturbative contributions, we demonstrate that neither finite temperature nor finite fermion density affects the gravitational chiral anomaly.","These results find application in several contexts, from topological materials to the early universe plasma.","They affect the decay of any axion or axion-like particle into gravitational waves, in very dense and hot environments."],"url":"http://arxiv.org/abs/2404.06272v1","category":"hep-th"}
{"created":"2024-04-09 12:25:06","title":"From Barlow Twins to Triplet Training: Differentiating Dementia with Limited Data","abstract":"Differential diagnosis of dementia is challenging due to overlapping symptoms, with structural magnetic resonance imaging (MRI) being the primary method for diagnosis. Despite the clinical value of computer-aided differential diagnosis, research has been limited, mainly due to the absence of public datasets that contain diverse types of dementia. This leaves researchers with small in-house datasets that are insufficient for training deep neural networks (DNNs). Self-supervised learning shows promise for utilizing unlabeled MRI scans in training, but small batch sizes for volumetric brain scans make its application challenging. To address these issues, we propose Triplet Training for differential diagnosis with limited target data. It consists of three key stages: (i) self-supervised pre-training on unlabeled data with Barlow Twins, (ii) self-distillation on task-related data, and (iii) fine-tuning on the target dataset. Our approach significantly outperforms traditional training strategies, achieving a balanced accuracy of 75.6%. We further provide insights into the training process by visualizing changes in the latent space after each step. Finally, we validate the robustness of Triplet Training in terms of its individual components in a comprehensive ablation study. Our code is available at https://github.com/ai-med/TripletTraining.","sentences":["Differential diagnosis of dementia is challenging due to overlapping symptoms, with structural magnetic resonance imaging (MRI) being the primary method for diagnosis.","Despite the clinical value of computer-aided differential diagnosis, research has been limited, mainly due to the absence of public datasets that contain diverse types of dementia.","This leaves researchers with small in-house datasets that are insufficient for training deep neural networks (DNNs).","Self-supervised learning shows promise for utilizing unlabeled MRI scans in training, but small batch sizes for volumetric brain scans make its application challenging.","To address these issues, we propose Triplet Training for differential diagnosis with limited target data.","It consists of three key stages: (i) self-supervised pre-training on unlabeled data with Barlow Twins, (ii) self-distillation on task-related data, and (iii) fine-tuning on the target dataset.","Our approach significantly outperforms traditional training strategies, achieving a balanced accuracy of 75.6%.","We further provide insights into the training process by visualizing changes in the latent space after each step.","Finally, we validate the robustness of Triplet Training in terms of its individual components in a comprehensive ablation study.","Our code is available at https://github.com/ai-med/TripletTraining."],"url":"http://arxiv.org/abs/2404.06253v1","category":"cs.CV"}
{"created":"2024-04-09 12:13:40","title":"LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks","abstract":"Visual object tracking plays a critical role in visual-based autonomous systems, as it aims to estimate the position and size of the object of interest within a live video. Despite significant progress made in this field, state-of-the-art (SOTA) trackers often fail when faced with adversarial perturbations in the incoming frames. This can lead to significant robustness and security issues when these trackers are deployed in the real world. To achieve high accuracy on both clean and adversarial data, we propose building a spatial-temporal continuous representation using the semantic text guidance of the object of interest. This novel continuous representation enables us to reconstruct incoming frames to maintain semantic and appearance consistency with the object of interest and its clean counterparts. As a result, our proposed method successfully defends against different SOTA adversarial tracking attacks while maintaining high accuracy on clean data. In particular, our method significantly increases tracking accuracy under adversarial attacks with around 90% relative improvement on UAV123, which is even higher than the accuracy on clean data.","sentences":["Visual object tracking plays a critical role in visual-based autonomous systems, as it aims to estimate the position and size of the object of interest within a live video.","Despite significant progress made in this field, state-of-the-art (SOTA) trackers often fail when faced with adversarial perturbations in the incoming frames.","This can lead to significant robustness and security issues when these trackers are deployed in the real world.","To achieve high accuracy on both clean and adversarial data, we propose building a spatial-temporal continuous representation using the semantic text guidance of the object of interest.","This novel continuous representation enables us to reconstruct incoming frames to maintain semantic and appearance consistency with the object of interest and its clean counterparts.","As a result, our proposed method successfully defends against different SOTA adversarial tracking attacks while maintaining high accuracy on clean data.","In particular, our method significantly increases tracking accuracy under adversarial attacks with around 90% relative improvement on UAV123, which is even higher than the accuracy on clean data."],"url":"http://arxiv.org/abs/2404.06247v1","category":"cs.CV"}
{"created":"2024-04-10 17:25:42","title":"Self-supervised Monocular Depth Estimation on Water Scenes via Specular Reflection Prior","abstract":"Monocular depth estimation from a single image is an ill-posed problem for computer vision due to insufficient reliable cues as the prior knowledge. Besides the inter-frame supervision, namely stereo and adjacent frames, extensive prior information is available in the same frame. Reflections from specular surfaces, informative intra-frame priors, enable us to reformulate the ill-posed depth estimation task as a multi-view synthesis. This paper proposes the first self-supervision for deep-learning depth estimation on water scenes via intra-frame priors, known as reflection supervision and geometrical constraints. In the first stage, a water segmentation network is performed to separate the reflection components from the entire image. Next, we construct a self-supervised framework to predict the target appearance from reflections, perceived as other perspectives. The photometric re-projection error, incorporating SmoothL1 and a novel photometric adaptive SSIM, is formulated to optimize pose and depth estimation by aligning the transformed virtual depths and source ones. As a supplement, the water surface is determined from real and virtual camera positions, which complement the depth of the water area. Furthermore, to alleviate these laborious ground truth annotations, we introduce a large-scale water reflection scene (WRS) dataset rendered from Unreal Engine 4. Extensive experiments on the WRS dataset prove the feasibility of the proposed method compared to state-of-the-art depth estimation techniques.","sentences":["Monocular depth estimation from a single image is an ill-posed problem for computer vision due to insufficient reliable cues as the prior knowledge.","Besides the inter-frame supervision, namely stereo and adjacent frames, extensive prior information is available in the same frame.","Reflections from specular surfaces, informative intra-frame priors, enable us to reformulate the ill-posed depth estimation task as a multi-view synthesis.","This paper proposes the first self-supervision for deep-learning depth estimation on water scenes via intra-frame priors, known as reflection supervision and geometrical constraints.","In the first stage, a water segmentation network is performed to separate the reflection components from the entire image.","Next, we construct a self-supervised framework to predict the target appearance from reflections, perceived as other perspectives.","The photometric re-projection error, incorporating SmoothL1 and a novel photometric adaptive SSIM, is formulated to optimize pose and depth estimation by aligning the transformed virtual depths and source ones.","As a supplement, the water surface is determined from real and virtual camera positions, which complement the depth of the water area.","Furthermore, to alleviate these laborious ground truth annotations, we introduce a large-scale water reflection scene (WRS) dataset rendered from Unreal Engine 4.","Extensive experiments on the WRS dataset prove the feasibility of the proposed method compared to state-of-the-art depth estimation techniques."],"url":"http://arxiv.org/abs/2404.07176v1","category":"cs.CV"}
{"created":"2024-04-10 16:54:07","title":"Evaluating Navigation and Comparison Performance of Computational Notebooks on Desktop and in Virtual Reality","abstract":"The computational notebook serves as a versatile tool for data analysis. However, its conventional user interface falls short of keeping pace with the ever-growing data-related tasks, signaling the need for novel approaches. With the rapid development of interaction techniques and computing environments, there is a growing interest in integrating emerging technologies in data-driven workflows. Virtual reality, in particular, has demonstrated its potential in interactive data visualizations. In this work, we aimed to experiment with adapting computational notebooks into VR and verify the potential benefits VR can bring. We focus on the navigation and comparison aspects as they are primitive components in analysts' workflow. To further improve comparison, we have designed and implemented a Branching&Merging functionality. We tested computational notebooks on the desktop and in VR, both with and without the added Branching&Merging capability. We found VR significantly facilitated navigation compared to desktop, and the ability to create branches enhanced comparison.","sentences":["The computational notebook serves as a versatile tool for data analysis.","However, its conventional user interface falls short of keeping pace with the ever-growing data-related tasks, signaling the need for novel approaches.","With the rapid development of interaction techniques and computing environments, there is a growing interest in integrating emerging technologies in data-driven workflows.","Virtual reality, in particular, has demonstrated its potential in interactive data visualizations.","In this work, we aimed to experiment with adapting computational notebooks into VR and verify the potential benefits VR can bring.","We focus on the navigation and comparison aspects as they are primitive components in analysts' workflow.","To further improve comparison, we have designed and implemented a Branching&Merging functionality.","We tested computational notebooks on the desktop and in VR, both with and without the added Branching&Merging capability.","We found VR significantly facilitated navigation compared to desktop, and the ability to create branches enhanced comparison."],"url":"http://arxiv.org/abs/2404.07161v1","category":"cs.HC"}
{"created":"2024-04-10 15:34:10","title":"MoCap-to-Visual Domain Adaptation for Efficient Human Mesh Estimation from 2D Keypoints","abstract":"This paper presents Key2Mesh, a model that takes a set of 2D human pose keypoints as input and estimates the corresponding body mesh. Since this process does not involve any visual (i.e. RGB image) data, the model can be trained on large-scale motion capture (MoCap) datasets, thereby overcoming the scarcity of image datasets with 3D labels. To enable the model's application on RGB images, we first run an off-the-shelf 2D pose estimator to obtain the 2D keypoints, and then feed these 2D keypoints to Key2Mesh. To improve the performance of our model on RGB images, we apply an adversarial domain adaptation (DA) method to bridge the gap between the MoCap and visual domains. Crucially, our DA method does not require 3D labels for visual data, which enables adaptation to target sets without the need for costly labels. We evaluate Key2Mesh for the task of estimating 3D human meshes from 2D keypoints, in the absence of RGB and mesh label pairs. Our results on widely used H3.6M and 3DPW datasets show that Key2Mesh sets the new state-of-the-art by outperforming other models in PA-MPJPE for both datasets, and in MPJPE and PVE for the 3DPW dataset. Thanks to our model's simple architecture, it operates at least 12x faster than the prior state-of-the-art model, LGD. Additional qualitative samples and code are available on the project website: https://key2mesh.github.io/.","sentences":["This paper presents Key2Mesh, a model that takes a set of 2D human pose keypoints as input and estimates the corresponding body mesh.","Since this process does not involve any visual (i.e. RGB image) data, the model can be trained on large-scale motion capture (MoCap) datasets, thereby overcoming the scarcity of image datasets with 3D labels.","To enable the model's application on RGB images, we first run an off-the-shelf 2D pose estimator to obtain the 2D keypoints, and then feed these 2D keypoints to Key2Mesh.","To improve the performance of our model on RGB images, we apply an adversarial domain adaptation (DA) method to bridge the gap between the MoCap and visual domains.","Crucially, our DA method does not require 3D labels for visual data, which enables adaptation to target sets without the need for costly labels.","We evaluate Key2Mesh for the task of estimating 3D human meshes from 2D keypoints, in the absence of RGB and mesh label pairs.","Our results on widely used H3.6M and 3DPW datasets show that Key2Mesh sets the new state-of-the-art by outperforming other models in PA-MPJPE for both datasets, and in MPJPE and PVE for the 3DPW dataset.","Thanks to our model's simple architecture, it operates at least 12x faster than the prior state-of-the-art model, LGD.","Additional qualitative samples and code are available on the project website: https://key2mesh.github.io/."],"url":"http://arxiv.org/abs/2404.07094v1","category":"cs.CV"}
{"created":"2024-04-10 08:54:43","title":"O2V-Mapping: Online Open-Vocabulary Mapping with Neural Implicit Representation","abstract":"Online construction of open-ended language scenes is crucial for robotic applications, where open-vocabulary interactive scene understanding is required. Recently, neural implicit representation has provided a promising direction for online interactive mapping. However, implementing open-vocabulary scene understanding capability into online neural implicit mapping still faces three challenges: lack of local scene updating ability, blurry spatial hierarchical semantic segmentation and difficulty in maintaining multi-view consistency. To this end, we proposed O2V-mapping, which utilizes voxel-based language and geometric features to create an open-vocabulary field, thus allowing for local updates during online training process. Additionally, we leverage a foundational model for image segmentation to extract language features on object-level entities, achieving clear segmentation boundaries and hierarchical semantic features. For the purpose of preserving consistency in 3D object properties across different viewpoints, we propose a spatial adaptive voxel adjustment mechanism and a multi-view weight selection method. Extensive experiments on open-vocabulary object localization and semantic segmentation demonstrate that O2V-mapping achieves online construction of language scenes while enhancing accuracy, outperforming the previous SOTA method.","sentences":["Online construction of open-ended language scenes is crucial for robotic applications, where open-vocabulary interactive scene understanding is required.","Recently, neural implicit representation has provided a promising direction for online interactive mapping.","However, implementing open-vocabulary scene understanding capability into online neural implicit mapping still faces three challenges: lack of local scene updating ability, blurry spatial hierarchical semantic segmentation and difficulty in maintaining multi-view consistency.","To this end, we proposed O2V-mapping, which utilizes voxel-based language and geometric features to create an open-vocabulary field, thus allowing for local updates during online training process.","Additionally, we leverage a foundational model for image segmentation to extract language features on object-level entities, achieving clear segmentation boundaries and hierarchical semantic features.","For the purpose of preserving consistency in 3D object properties across different viewpoints, we propose a spatial adaptive voxel adjustment mechanism and a multi-view weight selection method.","Extensive experiments on open-vocabulary object localization and semantic segmentation demonstrate that O2V-mapping achieves online construction of language scenes while enhancing accuracy, outperforming the previous SOTA method."],"url":"http://arxiv.org/abs/2404.06836v1","category":"cs.CV"}
{"created":"2024-04-10 08:54:00","title":"Tuning-Free Adaptive Style Incorporation for Structure-Consistent Text-Driven Style Transfer","abstract":"In this work, we target the task of text-driven style transfer in the context of text-to-image (T2I) diffusion models. The main challenge is consistent structure preservation while enabling effective style transfer effects. The past approaches in this field directly concatenate the content and style prompts for a prompt-level style injection, leading to unavoidable structure distortions. In this work, we propose a novel solution to the text-driven style transfer task, namely, Adaptive Style Incorporation~(ASI), to achieve fine-grained feature-level style incorporation. It consists of the Siamese Cross-Attention~(SiCA) to decouple the single-track cross-attention to a dual-track structure to obtain separate content and style features, and the Adaptive Content-Style Blending (AdaBlending) module to couple the content and style information from a structure-consistent manner. Experimentally, our method exhibits much better performance in both structure preservation and stylized effects.","sentences":["In this work, we target the task of text-driven style transfer in the context of text-to-image (T2I) diffusion models.","The main challenge is consistent structure preservation while enabling effective style transfer effects.","The past approaches in this field directly concatenate the content and style prompts for a prompt-level style injection, leading to unavoidable structure distortions.","In this work, we propose a novel solution to the text-driven style transfer task, namely, Adaptive Style Incorporation~(ASI), to achieve fine-grained feature-level style incorporation.","It consists of the Siamese Cross-Attention~(SiCA) to decouple the single-track cross-attention to a dual-track structure to obtain separate content and style features, and the Adaptive Content-Style Blending (AdaBlending) module to couple the content and style information from a structure-consistent manner.","Experimentally, our method exhibits much better performance in both structure preservation and stylized effects."],"url":"http://arxiv.org/abs/2404.06835v1","category":"cs.CV"}
{"created":"2024-04-10 08:06:15","title":"Towards Efficient and Real-Time Piano Transcription Using Neural Autoregressive Models","abstract":"In recent years, advancements in neural network designs and the availability of large-scale labeled datasets have led to significant improvements in the accuracy of piano transcription models. However, most previous work focused on high-performance offline transcription, neglecting deliberate consideration of model size. The goal of this work is to implement real-time inference for piano transcription while ensuring both high performance and lightweight. To this end, we propose novel architectures for convolutional recurrent neural networks, redesigning an existing autoregressive piano transcription model. First, we extend the acoustic module by adding a frequency-conditioned FiLM layer to the CNN module to adapt the convolutional filters on the frequency axis. Second, we improve note-state sequence modeling by using a pitchwise LSTM that focuses on note-state transitions within a note. In addition, we augment the autoregressive connection with an enhanced recursive context. Using these components, we propose two types of models; one for high performance and the other for high compactness. Through extensive experiments, we show that the proposed models are comparable to state-of-the-art models in terms of note accuracy on the MAESTRO dataset. We also investigate the effective model size and real-time inference latency by gradually streamlining the architecture. Finally, we conduct cross-data evaluation on unseen piano datasets and in-depth analysis to elucidate the effect of the proposed components in the view of note length and pitch range.","sentences":["In recent years, advancements in neural network designs and the availability of large-scale labeled datasets have led to significant improvements in the accuracy of piano transcription models.","However, most previous work focused on high-performance offline transcription, neglecting deliberate consideration of model size.","The goal of this work is to implement real-time inference for piano transcription while ensuring both high performance and lightweight.","To this end, we propose novel architectures for convolutional recurrent neural networks, redesigning an existing autoregressive piano transcription model.","First, we extend the acoustic module by adding a frequency-conditioned FiLM layer to the CNN module to adapt the convolutional filters on the frequency axis.","Second, we improve note-state sequence modeling by using a pitchwise LSTM that focuses on note-state transitions within a note.","In addition, we augment the autoregressive connection with an enhanced recursive context.","Using these components, we propose two types of models; one for high performance and the other for high compactness.","Through extensive experiments, we show that the proposed models are comparable to state-of-the-art models in terms of note accuracy on the MAESTRO dataset.","We also investigate the effective model size and real-time inference latency by gradually streamlining the architecture.","Finally, we conduct cross-data evaluation on unseen piano datasets and in-depth analysis to elucidate the effect of the proposed components in the view of note length and pitch range."],"url":"http://arxiv.org/abs/2404.06818v1","category":"eess.AS"}
{"created":"2024-04-10 06:56:04","title":"Fluid Simulation for a Finite Size Plasma","abstract":"Studies on finite-size plasma have attracted a lot of attention lately. They can form by ionizing liquid droplets by lasers. The dynamical behavior of such plasma droplets is, therefore, a topic of significant interest. In particular, questions related to the linear and nonlinear characteristics (associated with the inhomogeneous density typically at the edge of the droplet), the behavior of plasma expansion, etc., are of interest. A one-dimensional fluid simulation study has been carried out to investigate this behavior. It is observed that a slight imbalance in the charge density leads to oscillations that are concentrated and keep acquiring higher amplitude and sharper profile at the inhomogeneous edge region. Such oscillations lead to the expansion of the droplet. Though the fluid description breaks when the sharpness of these structures becomes comparable to the grid size, it provides a reasonable estimate of wave-breaking time. The presence of dissipative effects like diffusion is shown to arrest the sharpness of these structures. The dynamics of these structures in the presence of an externally applied oscillating electric field corresponding to a long wavelength radiation has also been studied.","sentences":["Studies on finite-size plasma have attracted a lot of attention lately.","They can form by ionizing liquid droplets by lasers.","The dynamical behavior of such plasma droplets is, therefore, a topic of significant interest.","In particular, questions related to the linear and nonlinear characteristics (associated with the inhomogeneous density typically at the edge of the droplet), the behavior of plasma expansion, etc., are of interest.","A one-dimensional fluid simulation study has been carried out to investigate this behavior.","It is observed that a slight imbalance in the charge density leads to oscillations that are concentrated and keep acquiring higher amplitude and sharper profile at the inhomogeneous edge region.","Such oscillations lead to the expansion of the droplet.","Though the fluid description breaks when the sharpness of these structures becomes comparable to the grid size, it provides a reasonable estimate of wave-breaking time.","The presence of dissipative effects like diffusion is shown to arrest the sharpness of these structures.","The dynamics of these structures in the presence of an externally applied oscillating electric field corresponding to a long wavelength radiation has also been studied."],"url":"http://arxiv.org/abs/2404.06786v1","category":"physics.plasm-ph"}
{"created":"2024-04-10 06:28:19","title":"Beyond Gait: Learning Knee Angle for Seamless Prosthesis Control in Multiple Scenarios","abstract":"Deep learning models have become a powerful tool in knee angle estimation for lower limb prostheses, owing to their adaptability across various gait phases and locomotion modes. Current methods utilize Multi-Layer Perceptrons (MLP), Long-Short Term Memory Networks (LSTM), and Convolutional Neural Networks (CNN), predominantly analyzing motion information from the thigh. Contrary to these approaches, our study introduces a holistic perspective by integrating whole-body movements as inputs. We propose a transformer-based probabilistic framework, termed the Angle Estimation Probabilistic Model (AEPM), that offers precise angle estimations across extensive scenarios beyond walking. AEPM achieves an overall RMSE of 6.70 degrees, with an RMSE of 3.45 degrees in walking scenarios. Compared to the state of the art, AEPM has improved the prediction accuracy for walking by 11.31%. Our method can achieve seamless adaptation between different locomotion modes. Also, this model can be utilized to analyze the synergy between the knee and other joints. We reveal that the whole body movement has valuable information for knee movement, which can provide insights into designing sensors for prostheses. The code is available at https://github.com/penway/Beyond-Gait-AEPM.","sentences":["Deep learning models have become a powerful tool in knee angle estimation for lower limb prostheses, owing to their adaptability across various gait phases and locomotion modes.","Current methods utilize Multi-Layer Perceptrons (MLP), Long-Short Term Memory Networks (LSTM), and Convolutional Neural Networks (CNN), predominantly analyzing motion information from the thigh.","Contrary to these approaches, our study introduces a holistic perspective by integrating whole-body movements as inputs.","We propose a transformer-based probabilistic framework, termed the Angle Estimation Probabilistic Model (AEPM), that offers precise angle estimations across extensive scenarios beyond walking.","AEPM achieves an overall RMSE of 6.70 degrees, with an RMSE of 3.45 degrees in walking scenarios.","Compared to the state of the art, AEPM has improved the prediction accuracy for walking by 11.31%.","Our method can achieve seamless adaptation between different locomotion modes.","Also, this model can be utilized to analyze the synergy between the knee and other joints.","We reveal that the whole body movement has valuable information for knee movement, which can provide insights into designing sensors for prostheses.","The code is available at https://github.com/penway/Beyond-Gait-AEPM."],"url":"http://arxiv.org/abs/2404.06772v1","category":"cs.RO"}
{"created":"2024-04-10 03:45:38","title":"A Reexamination of the COnfLUX 2.5D LU Factorization Algorithm","abstract":"This article conducts a reexamination of the research conducted by Kwasniewski et al., focusing on their adaptation of the 2.5D LU factorization algorithm with tournament pivoting, known as \\func{COnfLUX}. Our reexamination reveals potential concerns regarding the upper bound, empirical investigation methods, and lower bound, despite the original study providing a theoretical foundation and an instantiation of the proposed algorithm. This paper offers a reexamination of these matters, highlighting probable shortcomings in the original investigation. Our observations are intended to enhance the development and comprehension of parallel matrix factorization algorithms.","sentences":["This article conducts a reexamination of the research conducted by Kwasniewski et al., focusing on their adaptation of the 2.5D LU factorization algorithm with tournament pivoting, known as \\func{COnfLUX}.","Our reexamination reveals potential concerns regarding the upper bound, empirical investigation methods, and lower bound, despite the original study providing a theoretical foundation and an instantiation of the proposed algorithm.","This paper offers a reexamination of these matters, highlighting probable shortcomings in the original investigation.","Our observations are intended to enhance the development and comprehension of parallel matrix factorization algorithms."],"url":"http://arxiv.org/abs/2404.06713v1","category":"cs.DC"}
{"created":"2024-04-10 02:37:24","title":"Latent Chemical Space Searching for Plug-in Multi-objective Molecule Generation","abstract":"Molecular generation, an essential method for identifying new drug structures, has been supported by advancements in machine learning and computational technology. However, challenges remain in multi-objective generation, model adaptability, and practical application in drug discovery. In this study, we developed a versatile 'plug-in' molecular generation model that incorporates multiple objectives related to target affinity, drug-likeness, and synthesizability, facilitating its application in various drug development contexts. We improved the Particle Swarm Optimization (PSO) in the context of drug discoveries, and identified PSO-ENP as the optimal variant for multi-objective molecular generation and optimization through comparative experiments. The model also incorporates a novel target-ligand affinity predictor, enhancing the model's utility by supporting three-dimensional information and improving synthetic feasibility. Case studies focused on generating and optimizing drug-like big marine natural products were performed, underscoring PSO-ENP's effectiveness and demonstrating its considerable potential for practical drug discovery applications.","sentences":["Molecular generation, an essential method for identifying new drug structures, has been supported by advancements in machine learning and computational technology.","However, challenges remain in multi-objective generation, model adaptability, and practical application in drug discovery.","In this study, we developed a versatile 'plug-in' molecular generation model that incorporates multiple objectives related to target affinity, drug-likeness, and synthesizability, facilitating its application in various drug development contexts.","We improved the Particle Swarm Optimization (PSO) in the context of drug discoveries, and identified PSO-ENP as the optimal variant for multi-objective molecular generation and optimization through comparative experiments.","The model also incorporates a novel target-ligand affinity predictor, enhancing the model's utility by supporting three-dimensional information and improving synthetic feasibility.","Case studies focused on generating and optimizing drug-like big marine natural products were performed, underscoring PSO-ENP's effectiveness and demonstrating its considerable potential for practical drug discovery applications."],"url":"http://arxiv.org/abs/2404.06691v1","category":"q-bio.BM"}
{"created":"2024-04-09 22:57:42","title":"Efficiently Cooling Quantum Systems with Finite Resources: Insights from Thermodynamic Geometry","abstract":"Landauer's universal limit on heat dissipation during information erasure becomes increasingly crucial as computing devices shrink: minimising heat-induced errors demands optimal pure-state preparation. For this, however, Nernst's third law posits an infinite-resource requirement: either energy, time, or control complexity must diverge. Here, we address the practical challenge of efficiently cooling quantum systems using finite resources. We investigate the ensuing resource trade-offs and present efficient protocols for finite distinct energy gaps in settings pertaining to coherent or incoherent control, corresponding to quantum batteries and heat engines, respectively. Expressing energy bounds through thermodynamic length, our findings illuminate the optimal distribution of energy gaps, detailing the resource limitations of preparing pure states in practical settings.","sentences":["Landauer's universal limit on heat dissipation during information erasure becomes increasingly crucial as computing devices shrink: minimising heat-induced errors demands optimal pure-state preparation.","For this, however, Nernst's third law posits an infinite-resource requirement: either energy, time, or control complexity must diverge.","Here, we address the practical challenge of efficiently cooling quantum systems using finite resources.","We investigate the ensuing resource trade-offs and present efficient protocols for finite distinct energy gaps in settings pertaining to coherent or incoherent control, corresponding to quantum batteries and heat engines, respectively.","Expressing energy bounds through thermodynamic length, our findings illuminate the optimal distribution of energy gaps, detailing the resource limitations of preparing pure states in practical settings."],"url":"http://arxiv.org/abs/2404.06649v1","category":"quant-ph"}
{"created":"2024-04-09 22:17:20","title":"SAM-I-Am: Semantic Boosting for Zero-shot Atomic-Scale Electron Micrograph Segmentation","abstract":"Image segmentation is a critical enabler for tasks ranging from medical diagnostics to autonomous driving. However, the correct segmentation semantics - where are boundaries located? what segments are logically similar? - change depending on the domain, such that state-of-the-art foundation models can generate meaningless and incorrect results. Moreover, in certain domains, fine-tuning and retraining techniques are infeasible: obtaining labels is costly and time-consuming; domain images (micrographs) can be exponentially diverse; and data sharing (for third-party retraining) is restricted. To enable rapid adaptation of the best segmentation technology, we propose the concept of semantic boosting: given a zero-shot foundation model, guide its segmentation and adjust results to match domain expectations. We apply semantic boosting to the Segment Anything Model (SAM) to obtain microstructure segmentation for transmission electron microscopy. Our booster, SAM-I-Am, extracts geometric and textural features of various intermediate masks to perform mask removal and mask merging operations. We demonstrate a zero-shot performance increase of (absolute) +21.35%, +12.6%, +5.27% in mean IoU, and a -9.91%, -18.42%, -4.06% drop in mean false positive masks across images of three difficulty classes over vanilla SAM (ViT-L).","sentences":["Image segmentation is a critical enabler for tasks ranging from medical diagnostics to autonomous driving.","However, the correct segmentation semantics - where are boundaries located?","what segments are logically similar?","- change depending on the domain, such that state-of-the-art foundation models can generate meaningless and incorrect results.","Moreover, in certain domains, fine-tuning and retraining techniques are infeasible: obtaining labels is costly and time-consuming; domain images (micrographs) can be exponentially diverse; and data sharing (for third-party retraining) is restricted.","To enable rapid adaptation of the best segmentation technology, we propose the concept of semantic boosting: given a zero-shot foundation model, guide its segmentation and adjust results to match domain expectations.","We apply semantic boosting to the Segment Anything Model (SAM) to obtain microstructure segmentation for transmission electron microscopy.","Our booster, SAM-I-Am, extracts geometric and textural features of various intermediate masks to perform mask removal and mask merging operations.","We demonstrate a zero-shot performance increase of (absolute) +21.35%, +12.6%, +5.27% in mean IoU, and a -9.91%, -18.42%, -4.06% drop in mean false positive masks across images of three difficulty classes over vanilla SAM (ViT-L)."],"url":"http://arxiv.org/abs/2404.06638v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 21:21:36","title":"Adapted optimal transport between Gaussian processes in discrete time","abstract":"We derive explicitly the adapted $2$-Wasserstein distance between arbitrary non-degenerate Gaussian distributions on $\\mathbb{R}^N$ and characterize the optimal bicausal coupling(s). This leads to an adapted version of the Bures-Wasserstein distance on the space of positive definite matrices.","sentences":["We derive explicitly the adapted $2$-Wasserstein distance between arbitrary non-degenerate Gaussian distributions on $\\mathbb{R}^N$ and characterize the optimal bicausal coupling(s).","This leads to an adapted version of the Bures-Wasserstein distance on the space of positive definite matrices."],"url":"http://arxiv.org/abs/2404.06625v1","category":"math.PR"}
{"created":"2024-04-09 21:12:31","title":"Calibrating Higher-Order Statistics for Few-Shot Class-Incremental Learning with Pre-trained Vision Transformers","abstract":"Few-shot class-incremental learning (FSCIL) aims to adapt the model to new classes from very few data (5 samples) without forgetting the previously learned classes. Recent works in many-shot CIL (MSCIL) (using all available training data) exploited pre-trained models to reduce forgetting and achieve better plasticity. In a similar fashion, we use ViT models pre-trained on large-scale datasets for few-shot settings, which face the critical issue of low plasticity. FSCIL methods start with a many-shot first task to learn a very good feature extractor and then move to the few-shot setting from the second task onwards. While the focus of most recent studies is on how to learn the many-shot first task so that the model generalizes to all future few-shot tasks, we explore in this work how to better model the few-shot data using pre-trained models, irrespective of how the first task is trained. Inspired by recent works in MSCIL, we explore how using higher-order feature statistics can influence the classification of few-shot classes. We identify the main challenge of obtaining a good covariance matrix from few-shot data and propose to calibrate the covariance matrix for new classes based on semantic similarity to the many-shot base classes. Using the calibrated feature statistics in combination with existing methods significantly improves few-shot continual classification on several FSCIL benchmarks. Code is available at https://github.com/dipamgoswami/FSCIL-Calibration.","sentences":["Few-shot class-incremental learning (FSCIL) aims to adapt the model to new classes from very few data (5 samples) without forgetting the previously learned classes.","Recent works in many-shot CIL (MSCIL) (using all available training data) exploited pre-trained models to reduce forgetting and achieve better plasticity.","In a similar fashion, we use ViT models pre-trained on large-scale datasets for few-shot settings, which face the critical issue of low plasticity.","FSCIL methods start with a many-shot first task to learn a very good feature extractor and then move to the few-shot setting from the second task onwards.","While the focus of most recent studies is on how to learn the many-shot first task so that the model generalizes to all future few-shot tasks, we explore in this work how to better model the few-shot data using pre-trained models, irrespective of how the first task is trained.","Inspired by recent works in MSCIL, we explore how using higher-order feature statistics can influence the classification of few-shot classes.","We identify the main challenge of obtaining a good covariance matrix from few-shot data and propose to calibrate the covariance matrix for new classes based on semantic similarity to the many-shot base classes.","Using the calibrated feature statistics in combination with existing methods significantly improves few-shot continual classification on several FSCIL benchmarks.","Code is available at https://github.com/dipamgoswami/FSCIL-Calibration."],"url":"http://arxiv.org/abs/2404.06622v1","category":"cs.CV"}
{"created":"2024-04-09 21:10:17","title":"Encoder-Quantization-Motion-based Video Quality Metrics","abstract":"In an adaptive bitrate streaming application, the efficiency of video compression and the encoded video quality depend on both the video codec and the quality metric used to perform encoding optimization. The development of such a quality metric need large scale subjective datasets. In this work we merge several datasets into one to support the creation of a metric tailored for video compression and scaling. We proposed a set of HEVC lightweight features to boost performance of the metrics. Our metrics can be computed from tightly coupled encoding process with 4% compute overhead or from the decoding process in real-time. The proposed method can achieve better correlation than VMAF and P.1204.3. It can extrapolate to different dynamic ranges, and is suitable for real-time video quality metrics delivery in the bitstream. The performance is verified by in-distribution and cross-dataset tests. This work paves the way for adaptive client-side heuristics, real-time segment optimization, dynamic bitrate capping, and quality-dependent post-processing neural network switching, etc.","sentences":["In an adaptive bitrate streaming application, the efficiency of video compression and the encoded video quality depend on both the video codec and the quality metric used to perform encoding optimization.","The development of such a quality metric need large scale subjective datasets.","In this work we merge several datasets into one to support the creation of a metric tailored for video compression and scaling.","We proposed a set of HEVC lightweight features to boost performance of the metrics.","Our metrics can be computed from tightly coupled encoding process with 4% compute overhead or from the decoding process in real-time.","The proposed method can achieve better correlation than VMAF and P.1204.3.","It can extrapolate to different dynamic ranges, and is suitable for real-time video quality metrics delivery in the bitstream.","The performance is verified by in-distribution and cross-dataset tests.","This work paves the way for adaptive client-side heuristics, real-time segment optimization, dynamic bitrate capping, and quality-dependent post-processing neural network switching, etc."],"url":"http://arxiv.org/abs/2404.06620v1","category":"eess.IV"}
{"created":"2024-04-09 18:21:28","title":"PSF quality metrics in the problem of revealing Intermediate-Mass Black Holes using MICADO@ELT","abstract":"Nowadays, astronomers perform point spread function (PSF) fitting for most types of observational data. Interpolation of the PSF is often an intermediate step in such algorithms. In the case of the Multi-AO Imaging Camera for Deep Observations (MICADO) at the Extremely Large Telescope (ELT), PSF interpolation will play a crucial role in high-precision astrometry for stellar clusters and confirmation of the Intermediate-Mass Black Holes (IMBHs) presence. Significant PSF variations across the field of view invalidate the approach of deconvolution with a mean PSF or on-axis PSF. The ignoring of PSF variations can be especially unsatisfactory in the case of Single Conjugate Adaptive Optics (SCAO) observations, as these sophisticated and expensive systems are designed to achieve high resolution with ground-based telescopes by correcting for atmospheric turbulence in the direction of one reference star. In plenty of tasks, you face the question: How can I establish the quality of PSF fitting or interpolation? Our study aims to demonstrate the variety of PSF quality metrics, including the problem of revealing IMBHs in stellar clusters.","sentences":["Nowadays, astronomers perform point spread function (PSF) fitting for most types of observational data.","Interpolation of the PSF is often an intermediate step in such algorithms.","In the case of the Multi-AO Imaging Camera for Deep Observations (MICADO) at the Extremely Large Telescope (ELT), PSF interpolation will play a crucial role in high-precision astrometry for stellar clusters and confirmation of the Intermediate-Mass Black Holes (IMBHs) presence.","Significant PSF variations across the field of view invalidate the approach of deconvolution with a mean PSF or on-axis PSF.","The ignoring of PSF variations can be especially unsatisfactory in the case of Single Conjugate Adaptive Optics (SCAO) observations, as these sophisticated and expensive systems are designed to achieve high resolution with ground-based telescopes by correcting for atmospheric turbulence in the direction of one reference star.","In plenty of tasks, you face the question: How can I establish the quality of PSF fitting or interpolation?","Our study aims to demonstrate the variety of PSF quality metrics, including the problem of revealing IMBHs in stellar clusters."],"url":"http://arxiv.org/abs/2404.06558v1","category":"astro-ph.IM"}
{"created":"2024-04-09 17:49:07","title":"Existence of Mexican-hat dispersion and symmetry group of a layer","abstract":"Increased interest in physics of graphene and other two-dimensional materials boosted investigations of band structure near nodal points and lines. In contrast, group theoretical explanation of simple bands (that do not touch other bands), is sporadically present in the literature. This paper presents electronic dispersions up to forth order in momentum, near Brillouin zone (BZ) high symmetry points of all eighty layer groups. The method applies to non magnetic materials both with or without spin-orbit coupling. Particular attention is devoted to Mexican-hat dispersion, showing that it can appear only at BZ center of hexagonal layer groups. Presented symmetry adapted Taylor expansion of bands can be used to fit ab-initio or experimental band structures, or for analytical calculation of crystal properties. The results presented here might serve also as a guiding tool for design of new two-dimensional materials.","sentences":["Increased interest in physics of graphene and other two-dimensional materials boosted investigations of band structure near nodal points and lines.","In contrast, group theoretical explanation of simple bands (that do not touch other bands), is sporadically present in the literature.","This paper presents electronic dispersions up to forth order in momentum, near Brillouin zone (BZ) high symmetry points of all eighty layer groups.","The method applies to non magnetic materials both with or without spin-orbit coupling.","Particular attention is devoted to Mexican-hat dispersion, showing that it can appear only at BZ center of hexagonal layer groups.","Presented symmetry adapted Taylor expansion of bands can be used to fit ab-initio or experimental band structures, or for analytical calculation of crystal properties.","The results presented here might serve also as a guiding tool for design of new two-dimensional materials."],"url":"http://arxiv.org/abs/2404.06494v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 17:37:08","title":"GO4Align: Group Optimization for Multi-Task Alignment","abstract":"This paper proposes \\textit{GO4Align}, a multi-task optimization approach that tackles task imbalance by explicitly aligning the optimization across tasks. To achieve this, we design an adaptive group risk minimization strategy, compromising two crucial techniques in implementation: (i) dynamical group assignment, which clusters similar tasks based on task interactions; (ii) risk-guided group indicators, which exploit consistent task correlations with risk information from previous iterations. Comprehensive experimental results on diverse typical benchmarks demonstrate our method's performance superiority with even lower computational costs.","sentences":["This paper proposes \\textit{GO4Align}, a multi-task optimization approach that tackles task imbalance by explicitly aligning the optimization across tasks.","To achieve this, we design an adaptive group risk minimization strategy, compromising two crucial techniques in implementation: (i) dynamical group assignment, which clusters similar tasks based on task interactions; (ii) risk-guided group indicators, which exploit consistent task correlations with risk information from previous iterations.","Comprehensive experimental results on diverse typical benchmarks demonstrate our method's performance superiority with even lower computational costs."],"url":"http://arxiv.org/abs/2404.06486v1","category":"cs.LG"}
{"created":"2024-04-09 17:26:13","title":"Two-dimensional turbulence above topography: condensation transition and selection of minimum enstrophy solutions","abstract":"We consider two-dimensional flows above topography, revisiting the selective decay (or minimum-enstrophy) hypothesis of Bretherton and Haidvogel. We derive a `condensed branch' of solutions to the variational problem where a domain-scale condensate coexists with a flow at the (smaller) scale of the topography. The condensate arises through a supercritical bifurcation as the conserved energy of the initial condition exceeds a threshold value, a prediction that we quantitatively validate using Direct Numerical Simulations (DNS). We then consider the forced-dissipative case, showing how weak forcing and dissipation select a single dissipative state out of the continuum of solutions to the energy-conserving system predicted by selective decay. As the forcing strength increases, the condensate arises through a supercritical bifurcation for topographic-scale forcing and through a subcritical bifurcation for domain-scale forcing, both predictions being quantitatively validated by DNS. This method provides a way of determining the equilibrated state of forced-dissipative flows based on variational approaches to the associated energy-conserving system, such as the statistical mechanics of 2D flows or selective decay.","sentences":["We consider two-dimensional flows above topography, revisiting the selective decay (or minimum-enstrophy) hypothesis of Bretherton and Haidvogel.","We derive a `condensed branch' of solutions to the variational problem where a domain-scale condensate coexists with a flow at the (smaller) scale of the topography.","The condensate arises through a supercritical bifurcation as the conserved energy of the initial condition exceeds a threshold value, a prediction that we quantitatively validate using Direct Numerical Simulations (DNS).","We then consider the forced-dissipative case, showing how weak forcing and dissipation select a single dissipative state out of the continuum of solutions to the energy-conserving system predicted by selective decay.","As the forcing strength increases, the condensate arises through a supercritical bifurcation for topographic-scale forcing and through a subcritical bifurcation for domain-scale forcing, both predictions being quantitatively validated by DNS.","This method provides a way of determining the equilibrated state of forced-dissipative flows based on variational approaches to the associated energy-conserving system, such as the statistical mechanics of 2D flows or selective decay."],"url":"http://arxiv.org/abs/2404.06475v1","category":"physics.flu-dyn"}
{"created":"2024-04-09 17:17:23","title":"Neuromorphic In-Context Learning for Energy-Efficient MIMO Symbol Detection","abstract":"In-context learning (ICL), a property demonstrated by transformer-based sequence models, refers to the automatic inference of an input-output mapping based on examples of the mapping provided as context. ICL requires no explicit learning, i.e., no explicit updates of model weights, directly mapping context and new input to the new output. Prior work has proved the usefulness of ICL for detection in MIMO channels. In this setting, the context is given by pilot symbols, and ICL automatically adapts a detector, or equalizer, to apply to newly received signals. However, the implementation tested in prior art was based on conventional artificial neural networks (ANNs), which may prove too energy-demanding to be run on mobile devices. This paper evaluates a neuromorphic implementation of the transformer for ICL-based MIMO detection. This approach replaces ANNs with spiking neural networks (SNNs), and implements the attention mechanism via stochastic computing, requiring no multiplications, but only logical AND operations and counting. When using conventional digital CMOS hardware, the proposed implementation is shown to preserve accuracy, with a reduction in power consumption ranging from $5.4\\times$ to $26.8\\times$, depending on the model sizes, as compared to ANN-based implementations.","sentences":["In-context learning (ICL), a property demonstrated by transformer-based sequence models, refers to the automatic inference of an input-output mapping based on examples of the mapping provided as context.","ICL requires no explicit learning, i.e., no explicit updates of model weights, directly mapping context and new input to the new output.","Prior work has proved the usefulness of ICL for detection in MIMO channels.","In this setting, the context is given by pilot symbols, and ICL automatically adapts a detector, or equalizer, to apply to newly received signals.","However, the implementation tested in prior art was based on conventional artificial neural networks (ANNs), which may prove too energy-demanding to be run on mobile devices.","This paper evaluates a neuromorphic implementation of the transformer for ICL-based MIMO detection.","This approach replaces ANNs with spiking neural networks (SNNs), and implements the attention mechanism via stochastic computing, requiring no multiplications, but only logical AND operations and counting.","When using conventional digital CMOS hardware, the proposed implementation is shown to preserve accuracy, with a reduction in power consumption ranging from $5.4\\times$ to $26.8\\times$, depending on the model sizes, as compared to ANN-based implementations."],"url":"http://arxiv.org/abs/2404.06469v1","category":"eess.SP"}
{"created":"2024-04-09 17:12:38","title":"Phase space contraction of degenerately damped random splittings","abstract":"When studying out-of-equilibrium systems, one often excites the dynamics in some degrees of freedom while removing the excitation in others through damping. In order for the system to converge to a statistical steady state, the dynamics must transfer the energy from the excited modes to the dissipative directions. The precise mechanisms underlying this transfer are of particular interest and are the topic of this paper. We explore a class of randomly switched models introduced in [2,3] and provide some of the first results showing that minimal damping is sufficient to stabilize the system in a fluids model.","sentences":["When studying out-of-equilibrium systems, one often excites the dynamics in some degrees of freedom while removing the excitation in others through damping.","In order for the system to converge to a statistical steady state, the dynamics must transfer the energy from the excited modes to the dissipative directions.","The precise mechanisms underlying this transfer are of particular interest and are the topic of this paper.","We explore a class of randomly switched models introduced in [2,3] and provide some of the first results showing that minimal damping is sufficient to stabilize the system in a fluids model."],"url":"http://arxiv.org/abs/2404.06465v1","category":"math.PR"}
{"created":"2024-04-09 16:53:43","title":"SmartControl: Enhancing ControlNet for Handling Rough Visual Conditions","abstract":"Human visual imagination usually begins with analogies or rough sketches. For example, given an image with a girl playing guitar before a building, one may analogously imagine how it seems like if Iron Man playing guitar before Pyramid in Egypt. Nonetheless, visual condition may not be precisely aligned with the imaginary result indicated by text prompt, and existing layout-controllable text-to-image (T2I) generation models is prone to producing degraded generated results with obvious artifacts. To address this issue, we present a novel T2I generation method dubbed SmartControl, which is designed to modify the rough visual conditions for adapting to text prompt. The key idea of our SmartControl is to relax the visual condition on the areas that are conflicted with text prompts. In specific, a Control Scale Predictor (CSP) is designed to identify the conflict regions and predict the local control scales, while a dataset with text prompts and rough visual conditions is constructed for training CSP. It is worth noting that, even with a limited number (e.g., 1,000~2,000) of training samples, our SmartControl can generalize well to unseen objects. Extensive experiments on four typical visual condition types clearly show the efficacy of our SmartControl against state-of-the-arts. Source code, pre-trained models, and datasets are available at https://github.com/liuxiaoyu1104/SmartControl.","sentences":["Human visual imagination usually begins with analogies or rough sketches.","For example, given an image with a girl playing guitar before a building, one may analogously imagine how it seems like if Iron Man playing guitar before Pyramid in Egypt.","Nonetheless, visual condition may not be precisely aligned with the imaginary result indicated by text prompt, and existing layout-controllable text-to-image (T2I) generation models is prone to producing degraded generated results with obvious artifacts.","To address this issue, we present a novel T2I generation method dubbed SmartControl, which is designed to modify the rough visual conditions for adapting to text prompt.","The key idea of our SmartControl is to relax the visual condition on the areas that are conflicted with text prompts.","In specific, a Control Scale Predictor (CSP) is designed to identify the conflict regions and predict the local control scales, while a dataset with text prompts and rough visual conditions is constructed for training CSP.","It is worth noting that, even with a limited number (e.g., 1,000~2,000) of training samples, our SmartControl can generalize well to unseen objects.","Extensive experiments on four typical visual condition types clearly show the efficacy of our SmartControl against state-of-the-arts.","Source code, pre-trained models, and datasets are available at https://github.com/liuxiaoyu1104/SmartControl."],"url":"http://arxiv.org/abs/2404.06451v1","category":"cs.CV"}
{"created":"2024-04-09 16:45:34","title":"Multi-scale Dynamic and Hierarchical Relationship Modeling for Facial Action Units Recognition","abstract":"Human facial action units (AUs) are mutually related in a hierarchical manner, as not only they are associated with each other in both spatial and temporal domains but also AUs located in the same/close facial regions show stronger relationships than those of different facial regions. While none of existing approach thoroughly model such hierarchical inter-dependencies among AUs, this paper proposes to comprehensively model multi-scale AU-related dynamic and hierarchical spatio-temporal relationship among AUs for their occurrences recognition. Specifically, we first propose a novel multi-scale temporal differencing network with an adaptive weighting block to explicitly capture facial dynamics across frames at different spatial scales, which specifically considers the heterogeneity of range and magnitude in different AUs' activation. Then, a two-stage strategy is introduced to hierarchically model the relationship among AUs based on their spatial distribution (i.e., local and cross-region AU relationship modelling). Experimental results achieved on BP4D and DISFA show that our approach is the new state-of-the-art in the field of AU occurrence recognition. Our code is publicly available at https://github.com/CVI-SZU/MDHR.","sentences":["Human facial action units (AUs) are mutually related in a hierarchical manner, as not only they are associated with each other in both spatial and temporal domains but also AUs located in the same/close facial regions show stronger relationships than those of different facial regions.","While none of existing approach thoroughly model such hierarchical inter-dependencies among AUs, this paper proposes to comprehensively model multi-scale AU-related dynamic and hierarchical spatio-temporal relationship among AUs for their occurrences recognition.","Specifically, we first propose a novel multi-scale temporal differencing network with an adaptive weighting block to explicitly capture facial dynamics across frames at different spatial scales, which specifically considers the heterogeneity of range and magnitude in different AUs' activation.","Then, a two-stage strategy is introduced to hierarchically model the relationship among AUs based on their spatial distribution (i.e., local and cross-region AU relationship modelling).","Experimental results achieved on BP4D and DISFA show that our approach is the new state-of-the-art in the field of AU occurrence recognition.","Our code is publicly available at https://github.com/CVI-SZU/MDHR."],"url":"http://arxiv.org/abs/2404.06443v1","category":"cs.CV"}
{"created":"2024-04-09 15:36:50","title":"MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies","abstract":"The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .","sentences":["The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation.","This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative.","In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs.","While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research.","Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling.","For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation.","We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS.","With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal.","Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications.","MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM ."],"url":"http://arxiv.org/abs/2404.06395v1","category":"cs.CL"}
{"created":"2024-04-09 15:23:32","title":"Asymptotic-preserving finite difference method for partially dissipative hyperbolic systems","abstract":"In this paper, we analyze the preservation of asymptotic properties of partially dissipative hyperbolic systems when switching to a discrete setting. We prove that one of the simplest consistent and unconditionally stable numerical methods - the central finite-differences scheme - preserves both the asymptotic behaviour and the parabolic relaxation limit of one-dimensional partially dissipative hyperbolic systems which satisfy the Kalman rank condition.   The large time asymptotic-preserving property is achieved by conceiving time-weighted perturbed energy functionals in the spirit of the hypocoercivity theory. For the relaxation-preserving property, drawing inspiration from the observation that solutions in the continuous case exhibit distinct behaviours in low and high frequencies, we introduce a novel discrete Littlewood-Paley theory tailored to the central finite-difference scheme. This allows us to prove Bernstein-type estimates for discrete differential operators and leads to a new relaxation result: the strong convergence of the discrete linearized compressible Euler equations with damping towards the discrete heat equation, uniformly with respect to the mesh parameter.","sentences":["In this paper, we analyze the preservation of asymptotic properties of partially dissipative hyperbolic systems when switching to a discrete setting.","We prove that one of the simplest consistent and unconditionally stable numerical methods - the central finite-differences scheme - preserves both the asymptotic behaviour and the parabolic relaxation limit of one-dimensional partially dissipative hyperbolic systems which satisfy the Kalman rank condition.   ","The large time asymptotic-preserving property is achieved by conceiving time-weighted perturbed energy functionals in the spirit of the hypocoercivity theory.","For the relaxation-preserving property, drawing inspiration from the observation that solutions in the continuous case exhibit distinct behaviours in low and high frequencies, we introduce a novel discrete Littlewood-Paley theory tailored to the central finite-difference scheme.","This allows us to prove Bernstein-type estimates for discrete differential operators and leads to a new relaxation result: the strong convergence of the discrete linearized compressible Euler equations with damping towards the discrete heat equation, uniformly with respect to the mesh parameter."],"url":"http://arxiv.org/abs/2404.06380v1","category":"math.AP"}
{"created":"2024-04-09 15:04:27","title":"ClinLinker: Medical Entity Linking of Clinical Concept Mentions in Spanish","abstract":"Advances in natural language processing techniques, such as named entity recognition and normalization to widely used standardized terminologies like UMLS or SNOMED-CT, along with the digitalization of electronic health records, have significantly advanced clinical text analysis. This study presents ClinLinker, a novel approach employing a two-phase pipeline for medical entity linking that leverages the potential of in-domain adapted language models for biomedical text mining: initial candidate retrieval using a SapBERT-based bi-encoder and subsequent re-ranking with a cross-encoder, trained by following a contrastive-learning strategy to be tailored to medical concepts in Spanish. This methodology, focused initially on content in Spanish, substantially outperforming multilingual language models designed for the same purpose. This is true even for complex scenarios involving heterogeneous medical terminologies and being trained on a subset of the original data. Our results, evaluated using top-k accuracy at 25 and other top-k metrics, demonstrate our approach's performance on two distinct clinical entity linking Gold Standard corpora, DisTEMIST (diseases) and MedProcNER (clinical procedures), outperforming previous benchmarks by 40 points in DisTEMIST and 43 points in MedProcNER, both normalized to SNOMED-CT codes. These findings highlight our approach's ability to address language-specific nuances and set a new benchmark in entity linking, offering a potent tool for enhancing the utility of digital medical records. The resulting system is of practical value, both for large scale automatic generation of structured data derived from clinical records, as well as for exhaustive extraction and harmonization of predefined clinical variables of interest.","sentences":["Advances in natural language processing techniques, such as named entity recognition and normalization to widely used standardized terminologies like UMLS or SNOMED-CT, along with the digitalization of electronic health records, have significantly advanced clinical text analysis.","This study presents ClinLinker, a novel approach employing a two-phase pipeline for medical entity linking that leverages the potential of in-domain adapted language models for biomedical text mining: initial candidate retrieval using a SapBERT-based bi-encoder and subsequent re-ranking with a cross-encoder, trained by following a contrastive-learning strategy to be tailored to medical concepts in Spanish.","This methodology, focused initially on content in Spanish, substantially outperforming multilingual language models designed for the same purpose.","This is true even for complex scenarios involving heterogeneous medical terminologies and being trained on a subset of the original data.","Our results, evaluated using top-k accuracy at 25 and other top-k metrics, demonstrate our approach's performance on two distinct clinical entity linking Gold Standard corpora, DisTEMIST (diseases) and MedProcNER (clinical procedures), outperforming previous benchmarks by 40 points in DisTEMIST and 43 points in MedProcNER, both normalized to SNOMED-CT codes.","These findings highlight our approach's ability to address language-specific nuances and set a new benchmark in entity linking, offering a potent tool for enhancing the utility of digital medical records.","The resulting system is of practical value, both for large scale automatic generation of structured data derived from clinical records, as well as for exhaustive extraction and harmonization of predefined clinical variables of interest."],"url":"http://arxiv.org/abs/2404.06367v1","category":"cs.CL"}
{"created":"2024-04-09 15:02:01","title":"Dynamic Resolution Guidance for Facial Expression Recognition","abstract":"Facial expression recognition (FER) is vital for human-computer interaction and emotion analysis, yet recognizing expressions in low-resolution images remains challenging. This paper introduces a practical method called Dynamic Resolution Guidance for Facial Expression Recognition (DRGFER) to effectively recognize facial expressions in images with varying resolutions without compromising FER model accuracy. Our framework comprises two main components: the Resolution Recognition Network (RRN) and the Multi-Resolution Adaptation Facial Expression Recognition Network (MRAFER). The RRN determines image resolution, outputs a binary vector, and the MRAFER assigns images to suitable facial expression recognition networks based on resolution. We evaluated DRGFER on widely-used datasets RAFDB and FERPlus, demonstrating that our method retains optimal model performance at each resolution and outperforms alternative resolution approaches. The proposed framework exhibits robustness against resolution variations and facial expressions, offering a promising solution for real-world applications.","sentences":["Facial expression recognition (FER) is vital for human-computer interaction and emotion analysis, yet recognizing expressions in low-resolution images remains challenging.","This paper introduces a practical method called Dynamic Resolution Guidance for Facial Expression Recognition (DRGFER) to effectively recognize facial expressions in images with varying resolutions without compromising FER model accuracy.","Our framework comprises two main components: the Resolution Recognition Network (RRN) and the Multi-Resolution Adaptation Facial Expression Recognition Network (MRAFER).","The RRN determines image resolution, outputs a binary vector, and the MRAFER assigns images to suitable facial expression recognition networks based on resolution.","We evaluated DRGFER on widely-used datasets RAFDB and FERPlus, demonstrating that our method retains optimal model performance at each resolution and outperforms alternative resolution approaches.","The proposed framework exhibits robustness against resolution variations and facial expressions, offering a promising solution for real-world applications."],"url":"http://arxiv.org/abs/2404.06365v1","category":"cs.CV"}
{"created":"2024-04-09 14:40:08","title":"CausalBench: A Comprehensive Benchmark for Causal Learning Capability of Large Language Models","abstract":"Causality reveals fundamental principles behind data distributions in real-world scenarios, and the capability of large language models (LLMs) to understand causality directly impacts their efficacy across explaining outputs, adapting to new evidence, and generating counterfactuals. With the proliferation of LLMs, the evaluation of this capacity is increasingly garnering attention. However, the absence of a comprehensive benchmark has rendered existing evaluation studies being straightforward, undiversified, and homogeneous. To address these challenges, this paper proposes a comprehensive benchmark, namely CausalBench, to evaluate the causality understanding capabilities of LLMs. Originating from the causal research community, CausalBench encompasses three causal learning-related tasks, which facilitate a convenient comparison of LLMs' performance with classic causal learning algorithms. Meanwhile, causal networks of varying scales and densities are integrated in CausalBench, to explore the upper limits of LLMs' capabilities across task scenarios of varying difficulty. Notably, background knowledge and structured data are also incorporated into CausalBench to thoroughly unlock the underlying potential of LLMs for long-text comprehension and prior information utilization. Based on CausalBench, this paper evaluates nineteen leading LLMs and unveils insightful conclusions in diverse aspects. Firstly, we present the strengths and weaknesses of LLMs and quantitatively explore the upper limits of their capabilities across various scenarios. Meanwhile, we further discern the adaptability and abilities of LLMs to specific structural networks and complex chain of thought structures. Moreover, this paper quantitatively presents the differences across diverse information sources and uncovers the gap between LLMs' capabilities in causal understanding within textual contexts and numerical domains.","sentences":["Causality reveals fundamental principles behind data distributions in real-world scenarios, and the capability of large language models (LLMs) to understand causality directly impacts their efficacy across explaining outputs, adapting to new evidence, and generating counterfactuals.","With the proliferation of LLMs, the evaluation of this capacity is increasingly garnering attention.","However, the absence of a comprehensive benchmark has rendered existing evaluation studies being straightforward, undiversified, and homogeneous.","To address these challenges, this paper proposes a comprehensive benchmark, namely CausalBench, to evaluate the causality understanding capabilities of LLMs.","Originating from the causal research community, CausalBench encompasses three causal learning-related tasks, which facilitate a convenient comparison of LLMs' performance with classic causal learning algorithms.","Meanwhile, causal networks of varying scales and densities are integrated in CausalBench, to explore the upper limits of LLMs' capabilities across task scenarios of varying difficulty.","Notably, background knowledge and structured data are also incorporated into CausalBench to thoroughly unlock the underlying potential of LLMs for long-text comprehension and prior information utilization.","Based on CausalBench, this paper evaluates nineteen leading LLMs and unveils insightful conclusions in diverse aspects.","Firstly, we present the strengths and weaknesses of LLMs and quantitatively explore the upper limits of their capabilities across various scenarios.","Meanwhile, we further discern the adaptability and abilities of LLMs to specific structural networks and complex chain of thought structures.","Moreover, this paper quantitatively presents the differences across diverse information sources and uncovers the gap between LLMs' capabilities in causal understanding within textual contexts and numerical domains."],"url":"http://arxiv.org/abs/2404.06349v1","category":"cs.LG"}
{"created":"2024-04-09 13:17:23","title":"Size selection of crack front defects: Multiple fracture-plane interactions and intrinsic lengthscales","abstract":"Material failure is mediated by the propagation of cracks, which in realistic 3D materials typically involve multiple coexisting fracture planes. Multiple fracture-plane interactions create poorly understood out-of-plane crack structures, such as step defects on tensile fracture surfaces. Steps form once a slowly moving, distorted crack front segments into disconnected overlapping fracture planes separated by a stabilizing distance $h_{\\rm max}$. Our experiments on numerous brittle hydrogels reveal that $h_{\\rm max}$ varies linearly with both a nonlinear elastic length $\\Gamma(v)/\\mu$ and a dissipation length $\\xi$. Here, $\\Gamma(v)$ is the measured crack velocity $v$-dependent fracture energy and $\\mu$ is the shear modulus. These intrinsic lengthscales point the way to a fundamental understanding of multiple-crack interactions in 3D that lead to the formation of stable out-of-plane fracture structures.","sentences":["Material failure is mediated by the propagation of cracks, which in realistic 3D materials typically involve multiple coexisting fracture planes.","Multiple fracture-plane interactions create poorly understood out-of-plane crack structures, such as step defects on tensile fracture surfaces.","Steps form once a slowly moving, distorted crack front segments into disconnected overlapping fracture planes separated by a stabilizing distance $h_{\\rm max}$. Our experiments on numerous brittle hydrogels reveal that $h_{\\rm max}$ varies linearly with both a nonlinear elastic length $\\Gamma(v)/\\mu$ and a dissipation length $\\xi$. Here, $\\Gamma(v)$ is the measured crack velocity $v$-dependent fracture energy and $\\mu$ is the shear modulus.","These intrinsic lengthscales point the way to a fundamental understanding of multiple-crack interactions in 3D that lead to the formation of stable out-of-plane fracture structures."],"url":"http://arxiv.org/abs/2404.06289v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 12:35:16","title":"The walled Brauer category and stable cohomology of $\\mathrm{IA}_n$","abstract":"The IA-automorphism group is the group of automorphisms of the free group $F_n$ that act trivially on the abelianization $F_n^{\\mathrm{ab}}$. This group is in many ways analoguous to Torelli groups of surfaces and their higher dimensional analogues. In recent work, the stable rational cohomology of such groups was studied by Kupers and Randal-Williams, using the machinery of so-called Brauer categories. In this paper, we adapt their methods to study the stable rational cohomology of the IA-automorphism group. We obtain a conjectural description of the algebraic part of the stable rational cohomology and prove that it holds up to degree $Q+1$, given the assumption that the stable cohomology groups are stably finite dimensional in degrees up to $Q$. In particular, this allows us to compute the algebraic part of the stable cohomology in degree 2, which we show agrees with the part generated by the first cohomology group via the cup product map and which has previously been computed by Pettet.   In the appendix, written by Mai Katada, it is shown how the results of the paper can be applied to compute the stable Albanese (co)homology of the IA-automorphism group.","sentences":["The IA-automorphism group is the group of automorphisms of the free group $F_n$ that act trivially on the abelianization $F_n^{\\mathrm{ab}}$. This group is in many ways analoguous to Torelli groups of surfaces and their higher dimensional analogues.","In recent work, the stable rational cohomology of such groups was studied by Kupers and Randal-Williams, using the machinery of so-called Brauer categories.","In this paper, we adapt their methods to study the stable rational cohomology of the IA-automorphism group.","We obtain a conjectural description of the algebraic part of the stable rational cohomology and prove that it holds up to degree $Q+1$, given the assumption that the stable cohomology groups are stably finite dimensional in degrees up to $Q$. In particular, this allows us to compute the algebraic part of the stable cohomology in degree 2, which we show agrees with the part generated by the first cohomology group via the cup product map and which has previously been computed by Pettet.   ","In the appendix, written by Mai Katada, it is shown how the results of the paper can be applied to compute the stable Albanese (co)homology of the IA-automorphism group."],"url":"http://arxiv.org/abs/2404.06263v1","category":"math.AT"}
{"created":"2024-04-09 10:53:29","title":"Deep Learning Method for Computing Committor Functions with Adaptive Sampling","abstract":"The committor function is a central object for quantifying the transitions between metastable states of dynamical systems. Recently, a number of computational methods based on deep neural networks have been developed for computing the high-dimensional committor function. The success of the methods relies on sampling adequate data for the transition, which still is a challenging task for complex systems at low temperatures. In this work, we propose a deep learning method with two novel adaptive sampling schemes (I and II). In the two schemes, the data are generated actively with a modified potential where the bias potential is constructed from the learned committor function. We theoretically demonstrate the advantages of the sampling schemes and show that the data in sampling scheme II are uniformly distributed along the transition tube. This makes a promising method for studying the transition of complex systems. The efficiency of the method is illustrated in high-dimensional systems including the alanine dipeptide and a solvated dimer system.","sentences":["The committor function is a central object for quantifying the transitions between metastable states of dynamical systems.","Recently, a number of computational methods based on deep neural networks have been developed for computing the high-dimensional committor function.","The success of the methods relies on sampling adequate data for the transition, which still is a challenging task for complex systems at low temperatures.","In this work, we propose a deep learning method with two novel adaptive sampling schemes (I and II).","In the two schemes, the data are generated actively with a modified potential where the bias potential is constructed from the learned committor function.","We theoretically demonstrate the advantages of the sampling schemes and show that the data in sampling scheme II are uniformly distributed along the transition tube.","This makes a promising method for studying the transition of complex systems.","The efficiency of the method is illustrated in high-dimensional systems including the alanine dipeptide and a solvated dimer system."],"url":"http://arxiv.org/abs/2404.06206v1","category":"physics.comp-ph"}
{"created":"2024-04-09 10:52:56","title":"Adaptive Unit Root Inference in Autoregressions using the Lasso Solution Path","abstract":"We show that the activation knot of a potentially non-stationary regressor on the adaptive Lasso solution path in autoregressions can be leveraged for selection-free inference about a unit root. The resulting test has asymptotic power against local alternatives in $1/T$ neighbourhoods, unlike post-selection inference methods based on consistent model selection. Exploiting the information enrichment principle devised by Reinschl\\\"ussel and Arnold arXiv:2402.16580 [stat.ME] to improve the Lasso-based selection of ADF models, we propose a composite statistic and analyse its asymptotic distribution and local power function. Monte Carlo evidence shows that the combined test dominates the comparable post-selection inference methods of Tibshirani et al. [JASA, 2016, 514, 600-620] and may surpass the power of established unit root tests against local alternatives. We apply the new tests to groundwater level time series for Germany and find evidence rejecting stochastic trends to explain observed long-term declines in mean water levels.","sentences":["We show that the activation knot of a potentially non-stationary regressor on the adaptive Lasso solution path in autoregressions can be leveraged for selection-free inference about a unit root.","The resulting test has asymptotic power against local alternatives in $1/T$ neighbourhoods, unlike post-selection inference methods based on consistent model selection.","Exploiting the information enrichment principle devised by Reinschl\\\"ussel and Arnold arXiv:2402.16580","[stat.","ME] to improve the Lasso-based selection of ADF models, we propose a composite statistic and analyse its asymptotic distribution and local power function.","Monte Carlo evidence shows that the combined test dominates the comparable post-selection inference methods of Tibshirani et al.","[JASA, 2016, 514, 600-620] and may surpass the power of established unit root tests against local alternatives.","We apply the new tests to groundwater level time series for Germany and find evidence rejecting stochastic trends to explain observed long-term declines in mean water levels."],"url":"http://arxiv.org/abs/2404.06205v1","category":"stat.ME"}
{"created":"2024-04-09 10:25:48","title":"Dynamics of large oscillator populations with random interactions","abstract":"We explore large populations of phase oscillators interacting via random coupling functions. Two types of coupling terms, the Kuramoto-Daido coupling and the Winfree coupling, are considered. Under the assumption of statistical independence of the phases and the couplings, we derive reduced averaged equations with effective non-random coupling terms. As a particular example, we study interactions that have the same shape but possess random coupling strengths and random phase shifts. While randomness in coupling strengths just renormalizes the interaction, a distribution of the phase shifts in coupling reshapes the coupling function.","sentences":["We explore large populations of phase oscillators interacting via random coupling functions.","Two types of coupling terms, the Kuramoto-Daido coupling and the Winfree coupling, are considered.","Under the assumption of statistical independence of the phases and the couplings, we derive reduced averaged equations with effective non-random coupling terms.","As a particular example, we study interactions that have the same shape but possess random coupling strengths and random phase shifts.","While randomness in coupling strengths just renormalizes the interaction, a distribution of the phase shifts in coupling reshapes the coupling function."],"url":"http://arxiv.org/abs/2404.06193v1","category":"nlin.AO"}
{"created":"2024-04-09 10:08:03","title":"Shock wave generation and propagation in dissipative and nonlocal nonlinear Rydberg media","abstract":"We investigate the generation of optical shock waves in strongly interacting Rydberg atomic gases with a spatially homogeneous dissipative potential. The Rydberg atom interaction induces an optical nonlocal nonlinarity. We focus on local nonlinear ($R_b\\ll R_0$) and nonlocal nonlinear ($R_b\\sim R_0$) regimes, where $R_b$ and $R_0$ are the characteristic length of the Rydberg nonlinearity and beam width, respectively. In the local regime, we show spatial width and contrast of the shock wave change monotonically when increasing strength of the dissipative potential and optical intensity. In the nonlocal regime, the characteristic quantity of the shock wave depend on $R_b/R_0$ and dissipative potential nontrivially and on the intensity monotonically. We find that formation of shock waves dominantly takes place when $R_b$ is smaller than $R_0$, while the propagation dynamics is largely linear when $R_b$ is comparable to or larger than $R_0$. Our results reveal nontrivial roles played by dissipation and nonlocality in the generation of shock waves, and provide a route to manipulate their profiles and stability. Our study furthermore opens new avenues to explore non-Hermitian physics, and nonlinear wave generation and propagation by controlling dissipation and nonlocality in the Rydberg media.","sentences":["We investigate the generation of optical shock waves in strongly interacting Rydberg atomic gases with a spatially homogeneous dissipative potential.","The Rydberg atom interaction induces an optical nonlocal nonlinarity.","We focus on local nonlinear ($R_b\\ll R_0$) and nonlocal nonlinear ($R_b\\sim R_0$) regimes, where $R_b$ and $R_0$ are the characteristic length of the Rydberg nonlinearity and beam width, respectively.","In the local regime, we show spatial width and contrast of the shock wave change monotonically when increasing strength of the dissipative potential and optical intensity.","In the nonlocal regime, the characteristic quantity of the shock wave depend on $R_b/R_0$ and dissipative potential nontrivially and on the intensity monotonically.","We find that formation of shock waves dominantly takes place when $R_b$ is smaller than $R_0$, while the propagation dynamics is largely linear when $R_b$ is comparable to or larger than $R_0$. Our results reveal nontrivial roles played by dissipation and nonlocality in the generation of shock waves, and provide a route to manipulate their profiles and stability.","Our study furthermore opens new avenues to explore non-Hermitian physics, and nonlinear wave generation and propagation by controlling dissipation and nonlocality in the Rydberg media."],"url":"http://arxiv.org/abs/2404.06183v1","category":"physics.optics"}
{"created":"2024-04-09 10:03:44","title":"YOLC: You Only Look Clusters for Tiny Object Detection in Aerial Images","abstract":"Detecting objects from aerial images poses significant challenges due to the following factors: 1) Aerial images typically have very large sizes, generally with millions or even hundreds of millions of pixels, while computational resources are limited. 2) Small object size leads to insufficient information for effective detection. 3) Non-uniform object distribution leads to computational resource wastage. To address these issues, we propose YOLC (You Only Look Clusters), an efficient and effective framework that builds on an anchor-free object detector, CenterNet. To overcome the challenges posed by large-scale images and non-uniform object distribution, we introduce a Local Scale Module (LSM) that adaptively searches cluster regions for zooming in for accurate detection. Additionally, we modify the regression loss using Gaussian Wasserstein distance (GWD) to obtain high-quality bounding boxes. Deformable convolution and refinement methods are employed in the detection head to enhance the detection of small objects. We perform extensive experiments on two aerial image datasets, including Visdrone2019 and UAVDT, to demonstrate the effectiveness and superiority of our proposed approach.","sentences":["Detecting objects from aerial images poses significant challenges due to the following factors: 1) Aerial images typically have very large sizes, generally with millions or even hundreds of millions of pixels, while computational resources are limited.","2) Small object size leads to insufficient information for effective detection.","3) Non-uniform object distribution leads to computational resource wastage.","To address these issues, we propose YOLC (You Only Look Clusters), an efficient and effective framework that builds on an anchor-free object detector, CenterNet.","To overcome the challenges posed by large-scale images and non-uniform object distribution, we introduce a Local Scale Module (LSM) that adaptively searches cluster regions for zooming in for accurate detection.","Additionally, we modify the regression loss using Gaussian Wasserstein distance (GWD) to obtain high-quality bounding boxes.","Deformable convolution and refinement methods are employed in the detection head to enhance the detection of small objects.","We perform extensive experiments on two aerial image datasets, including Visdrone2019 and UAVDT, to demonstrate the effectiveness and superiority of our proposed approach."],"url":"http://arxiv.org/abs/2404.06180v1","category":"cs.CV"}
{"created":"2024-04-09 09:07:16","title":"A look at the operator product expansion in critical dynamics","abstract":"We consider the critical relaxation of the Ising model, the so-called model A, and study its operator product expansion. Within perturbation theory, we focus on the operator product expansions of the two-point function and the response function. At the fixed point, we normalize the coefficients and the scaling variables so that the result displays universality. The role of the fluctuation-dissipation theorem is also discussed, and it is shown that it provides non-perturbative relations among the operator product expansion coefficients. Finally, the large N limit is considered.","sentences":["We consider the critical relaxation of the Ising model, the so-called model A, and study its operator product expansion.","Within perturbation theory, we focus on the operator product expansions of the two-point function and the response function.","At the fixed point, we normalize the coefficients and the scaling variables so that the result displays universality.","The role of the fluctuation-dissipation theorem is also discussed, and it is shown that it provides non-perturbative relations among the operator product expansion coefficients.","Finally, the large N limit is considered."],"url":"http://arxiv.org/abs/2404.06142v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-09 09:05:23","title":"DiffHarmony: Latent Diffusion Model Meets Image Harmonization","abstract":"Image harmonization, which involves adjusting the foreground of a composite image to attain a unified visual consistency with the background, can be conceptualized as an image-to-image translation task. Diffusion models have recently promoted the rapid development of image-to-image translation tasks . However, training diffusion models from scratch is computationally intensive. Fine-tuning pre-trained latent diffusion models entails dealing with the reconstruction error induced by the image compression autoencoder, making it unsuitable for image generation tasks that involve pixel-level evaluation metrics. To deal with these issues, in this paper, we first adapt a pre-trained latent diffusion model to the image harmonization task to generate the harmonious but potentially blurry initial images. Then we implement two strategies: utilizing higher-resolution images during inference and incorporating an additional refinement stage, to further enhance the clarity of the initially harmonized images. Extensive experiments on iHarmony4 datasets demonstrate the superiority of our proposed method. The code and model will be made publicly available at https://github.com/nicecv/DiffHarmony .","sentences":["Image harmonization, which involves adjusting the foreground of a composite image to attain a unified visual consistency with the background, can be conceptualized as an image-to-image translation task.","Diffusion models have recently promoted the rapid development of image-to-image translation tasks .","However, training diffusion models from scratch is computationally intensive.","Fine-tuning pre-trained latent diffusion models entails dealing with the reconstruction error induced by the image compression autoencoder, making it unsuitable for image generation tasks that involve pixel-level evaluation metrics.","To deal with these issues, in this paper, we first adapt a pre-trained latent diffusion model to the image harmonization task to generate the harmonious but potentially blurry initial images.","Then we implement two strategies: utilizing higher-resolution images during inference and incorporating an additional refinement stage, to further enhance the clarity of the initially harmonized images.","Extensive experiments on iHarmony4 datasets demonstrate the superiority of our proposed method.","The code and model will be made publicly available at https://github.com/nicecv/DiffHarmony ."],"url":"http://arxiv.org/abs/2404.06139v1","category":"cs.CV"}
{"created":"2024-04-09 09:02:21","title":"Mansformer: Efficient Transformer of Mixed Attention for Image Deblurring and Beyond","abstract":"Transformer has made an enormous success in natural language processing and high-level vision over the past few years. However, the complexity of self-attention is quadratic to the image size, which makes it infeasible for high-resolution vision tasks. In this paper, we propose the Mansformer, a Transformer of mixed attention that combines multiple self-attentions, gate, and multi-layer perceptions (MLPs), to explore and employ more possibilities of self-attention. Taking efficiency into account, we design four kinds of self-attention, whose complexities are all linear. By elaborate adjustment of the tensor shapes and dimensions for the dot product, we split the typical self-attention of quadratic complexity into four operations of linear complexity. To adaptively merge these different kinds of self-attention, we take advantage of an architecture similar to Squeeze-and-Excitation Networks. Furthermore, we make it to merge the two-staged Transformer design into one stage by the proposed gated-dconv MLP. Image deblurring is our main target, while extensive quantitative and qualitative evaluations show that this method performs favorably against the state-of-the-art methods far more than simply deblurring. The source codes and trained models will be made available to the public.","sentences":["Transformer has made an enormous success in natural language processing and high-level vision over the past few years.","However, the complexity of self-attention is quadratic to the image size, which makes it infeasible for high-resolution vision tasks.","In this paper, we propose the Mansformer, a Transformer of mixed attention that combines multiple self-attentions, gate, and multi-layer perceptions (MLPs), to explore and employ more possibilities of self-attention.","Taking efficiency into account, we design four kinds of self-attention, whose complexities are all linear.","By elaborate adjustment of the tensor shapes and dimensions for the dot product, we split the typical self-attention of quadratic complexity into four operations of linear complexity.","To adaptively merge these different kinds of self-attention, we take advantage of an architecture similar to Squeeze-and-Excitation Networks.","Furthermore, we make it to merge the two-staged Transformer design into one stage by the proposed gated-dconv MLP.","Image deblurring is our main target, while extensive quantitative and qualitative evaluations show that this method performs favorably against the state-of-the-art methods far more than simply deblurring.","The source codes and trained models will be made available to the public."],"url":"http://arxiv.org/abs/2404.06135v1","category":"cs.CV"}
{"created":"2024-04-09 08:58:36","title":"The turnpike property for high-dimensional interacting agent systems in discrete time","abstract":"We investigate the interior turnpike phenomenon for discrete-time multi-agent optimal control problems. While for continuous systems the turnpike property has been established, we focus here on first-order discretizations of such systems. It is shown that the resulting time-discrete system inherits the turnpike property with estimates of the same type as in the continuous case. In particular, we prove that the discrete time optimal control problem is strictly dissipative and the cheap control assumption holds.","sentences":["We investigate the interior turnpike phenomenon for discrete-time multi-agent optimal control problems.","While for continuous systems the turnpike property has been established, we focus here on first-order discretizations of such systems.","It is shown that the resulting time-discrete system inherits the turnpike property with estimates of the same type as in the continuous case.","In particular, we prove that the discrete time optimal control problem is strictly dissipative and the cheap control assumption holds."],"url":"http://arxiv.org/abs/2404.06134v1","category":"math.OC"}
{"created":"2024-04-09 08:57:02","title":"REPUBLIC: A variability-preserving systematic-correction algorithm for PLATO's multi-camera light curves","abstract":"Space-based photometry missions produce exquisite light curves that contain a wealth of stellar variability on a wide range of timescales. Light curves also typically contain significant instrumental systematics -- spurious, non-astrophysical trends that are common, in varying degrees, to many light curves. Empirical systematics-correction approaches using the information in the light curves themselves have been very successful, but tend to suppress astrophysical signals, particularly on longer timescales. Unlike its predecessors, the PLATO mission will use multiple cameras to monitor the same stars. We present REPUBLIC, a novel systematics-correction algorithm which exploits this multi-camera configuration to correct systematics that differ between cameras, while preserving the component of each star's signal that is common to all cameras, regardless of timescale. Through simulations with astrophysical signals (star spots and planetary transits), Kepler-like errors, and white noise, we demonstrate REPUBLIC's ability to preserve long-term astrophysical signals usually lost in standard correction techniques. We also explore REPUBLIC's performance with different number of cameras and systematic properties. We conclude that REPUBLIC should be considered a potential complement to existing strategies for systematic correction in multi-camera surveys, with its utility contingent upon further validation and adaptation to the specific characteristics of the PLATO mission data","sentences":["Space-based photometry missions produce exquisite light curves that contain a wealth of stellar variability on a wide range of timescales.","Light curves also typically contain significant instrumental systematics -- spurious, non-astrophysical trends that are common, in varying degrees, to many light curves.","Empirical systematics-correction approaches using the information in the light curves themselves have been very successful, but tend to suppress astrophysical signals, particularly on longer timescales.","Unlike its predecessors, the PLATO mission will use multiple cameras to monitor the same stars.","We present REPUBLIC, a novel systematics-correction algorithm which exploits this multi-camera configuration to correct systematics that differ between cameras, while preserving the component of each star's signal that is common to all cameras, regardless of timescale.","Through simulations with astrophysical signals (star spots and planetary transits), Kepler-like errors, and white noise, we demonstrate REPUBLIC's ability to preserve long-term astrophysical signals usually lost in standard correction techniques.","We also explore REPUBLIC's performance with different number of cameras and systematic properties.","We conclude that REPUBLIC should be considered a potential complement to existing strategies for systematic correction in multi-camera surveys, with its utility contingent upon further validation and adaptation to the specific characteristics of the PLATO mission data"],"url":"http://arxiv.org/abs/2404.06132v1","category":"astro-ph.IM"}
{"created":"2024-04-09 08:56:43","title":"Adaptable Recovery Behaviors in Robotics: A Behavior Trees and Motion Generators(BTMG) Approach for Failure Management","abstract":"In dynamic operational environments, particularly in collaborative robotics, the inevitability of failures necessitates robust and adaptable recovery strategies. Traditional automated recovery strategies, while effective for predefined scenarios, often lack the flexibility required for on-the-fly task management and adaptation to expected failures. Addressing this gap, we propose a novel approach that models recovery behaviors as adaptable robotic skills, leveraging the Behavior Trees and Motion Generators~(BTMG) framework for policy representation. This approach distinguishes itself by employing reinforcement learning~(RL) to dynamically refine recovery behavior parameters, enabling a tailored response to a wide array of failure scenarios with minimal human intervention. We assess our methodology through a series of progressively challenging scenarios within a peg-in-a-hole task, demonstrating the approach's effectiveness in enhancing operational efficiency and task success rates in collaborative robotics settings. We validate our approach using a dual-arm KUKA robot.","sentences":["In dynamic operational environments, particularly in collaborative robotics, the inevitability of failures necessitates robust and adaptable recovery strategies.","Traditional automated recovery strategies, while effective for predefined scenarios, often lack the flexibility required for on-the-fly task management and adaptation to expected failures.","Addressing this gap, we propose a novel approach that models recovery behaviors as adaptable robotic skills, leveraging the Behavior Trees and Motion Generators~(BTMG) framework for policy representation.","This approach distinguishes itself by employing reinforcement learning~(RL) to dynamically refine recovery behavior parameters, enabling a tailored response to a wide array of failure scenarios with minimal human intervention.","We assess our methodology through a series of progressively challenging scenarios within a peg-in-a-hole task, demonstrating the approach's effectiveness in enhancing operational efficiency and task success rates in collaborative robotics settings.","We validate our approach using a dual-arm KUKA robot."],"url":"http://arxiv.org/abs/2404.06129v1","category":"cs.RO"}
{"created":"2024-04-09 08:41:13","title":"DreamView: Injecting View-specific Text Guidance into Text-to-3D Generation","abstract":"Text-to-3D generation, which synthesizes 3D assets according to an overall text description, has significantly progressed. However, a challenge arises when the specific appearances need customizing at designated viewpoints but referring solely to the overall description for generating 3D objects. For instance, ambiguity easily occurs when producing a T-shirt with distinct patterns on its front and back using a single overall text guidance. In this work, we propose DreamView, a text-to-image approach enabling multi-view customization while maintaining overall consistency by adaptively injecting the view-specific and overall text guidance through a collaborative text guidance injection module, which can also be lifted to 3D generation via score distillation sampling. DreamView is trained with large-scale rendered multi-view images and their corresponding view-specific texts to learn to balance the separate content manipulation in each view and the global consistency of the overall object, resulting in a dual achievement of customization and consistency. Consequently, DreamView empowers artists to design 3D objects creatively, fostering the creation of more innovative and diverse 3D assets. Code and model will be released at https://github.com/iSEE-Laboratory/DreamView.","sentences":["Text-to-3D generation, which synthesizes 3D assets according to an overall text description, has significantly progressed.","However, a challenge arises when the specific appearances need customizing at designated viewpoints but referring solely to the overall description for generating 3D objects.","For instance, ambiguity easily occurs when producing a T-shirt with distinct patterns on its front and back using a single overall text guidance.","In this work, we propose DreamView, a text-to-image approach enabling multi-view customization while maintaining overall consistency by adaptively injecting the view-specific and overall text guidance through a collaborative text guidance injection module, which can also be lifted to 3D generation via score distillation sampling.","DreamView is trained with large-scale rendered multi-view images and their corresponding view-specific texts to learn to balance the separate content manipulation in each view and the global consistency of the overall object, resulting in a dual achievement of customization and consistency.","Consequently, DreamView empowers artists to design 3D objects creatively, fostering the creation of more innovative and diverse 3D assets.","Code and model will be released at https://github.com/iSEE-Laboratory/DreamView."],"url":"http://arxiv.org/abs/2404.06119v1","category":"cs.CV"}
{"created":"2024-04-09 08:20:37","title":"Revising Densification in Gaussian Splatting","abstract":"In this paper, we address the limitations of Adaptive Density Control (ADC) in 3D Gaussian Splatting (3DGS), a scene representation method achieving high-quality, photorealistic results for novel view synthesis. ADC has been introduced for automatic 3D point primitive management, controlling densification and pruning, however, with certain limitations in the densification logic. Our main contribution is a more principled, pixel-error driven formulation for density control in 3DGS, leveraging an auxiliary, per-pixel error function as the criterion for densification. We further introduce a mechanism to control the total number of primitives generated per scene and correct a bias in the current opacity handling strategy of ADC during cloning operations. Our approach leads to consistent quality improvements across a variety of benchmark scenes, without sacrificing the method's efficiency.","sentences":["In this paper, we address the limitations of Adaptive Density Control (ADC) in 3D Gaussian Splatting (3DGS), a scene representation method achieving high-quality, photorealistic results for novel view synthesis.","ADC has been introduced for automatic 3D point primitive management, controlling densification and pruning, however, with certain limitations in the densification logic.","Our main contribution is a more principled, pixel-error driven formulation for density control in 3DGS, leveraging an auxiliary, per-pixel error function as the criterion for densification.","We further introduce a mechanism to control the total number of primitives generated per scene and correct a bias in the current opacity handling strategy of ADC during cloning operations.","Our approach leads to consistent quality improvements across a variety of benchmark scenes, without sacrificing the method's efficiency."],"url":"http://arxiv.org/abs/2404.06109v1","category":"cs.CV"}
{"created":"2024-04-09 07:49:30","title":"Hash3D: Training-free Acceleration for 3D Generation","abstract":"The evolution of 3D generative modeling has been notably propelled by the adoption of 2D diffusion models. Despite this progress, the cumbersome optimization process per se presents a critical hurdle to efficiency. In this paper, we introduce Hash3D, a universal acceleration for 3D generation without model training. Central to Hash3D is the insight that feature-map redundancy is prevalent in images rendered from camera positions and diffusion time-steps in close proximity. By effectively hashing and reusing these feature maps across neighboring timesteps and camera angles, Hash3D substantially prevents redundant calculations, thus accelerating the diffusion model's inference in 3D generation tasks. We achieve this through an adaptive grid-based hashing. Surprisingly, this feature-sharing mechanism not only speed up the generation but also enhances the smoothness and view consistency of the synthesized 3D objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3D's versatility to speed up optimization, enhancing efficiency by 1.3 to 4 times. Additionally, Hash3D's integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly 30 seconds. The project page is at https://adamdad.github.io/hash3D/.","sentences":["The evolution of 3D generative modeling has been notably propelled by the adoption of 2D diffusion models.","Despite this progress, the cumbersome optimization process per se presents a critical hurdle to efficiency.","In this paper, we introduce Hash3D, a universal acceleration for 3D generation without model training.","Central to Hash3D is the insight that feature-map redundancy is prevalent in images rendered from camera positions and diffusion time-steps in close proximity.","By effectively hashing and reusing these feature maps across neighboring timesteps and camera angles, Hash3D substantially prevents redundant calculations, thus accelerating the diffusion model's inference in 3D generation tasks.","We achieve this through an adaptive grid-based hashing.","Surprisingly, this feature-sharing mechanism not only speed up the generation but also enhances the smoothness and view consistency of the synthesized 3D objects.","Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3D's versatility to speed up optimization, enhancing efficiency by 1.3 to 4 times.","Additionally, Hash3D's integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly 30 seconds.","The project page is at https://adamdad.github.io/hash3D/."],"url":"http://arxiv.org/abs/2404.06091v1","category":"cs.CV"}
{"created":"2024-04-09 07:32:55","title":"End-to-end training of Multimodal Model and ranking Model","abstract":"Traditional recommender systems heavily rely on ID features, which often encounter challenges related to cold-start and generalization. Modeling pre-extracted content features can mitigate these issues, but is still a suboptimal solution due to the discrepancies between training tasks and model parameters. End-to-end training presents a promising solution for these problems, yet most of the existing works mainly focus on retrieval models, leaving the multimodal techniques under-utilized. In this paper, we propose an industrial multimodal recommendation framework named EM3: End-to-end training of Multimodal Model and ranking Model, which sufficiently utilizes multimodal information and allows personalized ranking tasks to directly train the core modules in the multimodal model to obtain more task-oriented content features, without overburdening resource consumption. First, we propose Fusion-Q-Former, which consists of transformers and a set of trainable queries, to fuse different modalities and generate fixed-length and robust multimodal embeddings. Second, in our sequential modeling for user content interest, we utilize Low-Rank Adaptation technique to alleviate the conflict between huge resource consumption and long sequence length. Third, we propose a novel Content-ID-Contrastive learning task to complement the advantages of content and ID by aligning them with each other, obtaining more task-oriented content embeddings and more generalized ID embeddings. In experiments, we implement EM3 on different ranking models in two scenario, achieving significant improvements in both offline evaluation and online A/B test, verifying the generalizability of our method. Ablation studies and visualization are also performed. Furthermore, we also conduct experiments on two public datasets to show that our proposed method outperforms the state-of-the-art methods.","sentences":["Traditional recommender systems heavily rely on ID features, which often encounter challenges related to cold-start and generalization.","Modeling pre-extracted content features can mitigate these issues, but is still a suboptimal solution due to the discrepancies between training tasks and model parameters.","End-to-end training presents a promising solution for these problems, yet most of the existing works mainly focus on retrieval models, leaving the multimodal techniques under-utilized.","In this paper, we propose an industrial multimodal recommendation framework named EM3: End-to-end training of Multimodal Model and ranking Model, which sufficiently utilizes multimodal information and allows personalized ranking tasks to directly train the core modules in the multimodal model to obtain more task-oriented content features, without overburdening resource consumption.","First, we propose Fusion-Q-Former, which consists of transformers and a set of trainable queries, to fuse different modalities and generate fixed-length and robust multimodal embeddings.","Second, in our sequential modeling for user content interest, we utilize Low-Rank Adaptation technique to alleviate the conflict between huge resource consumption and long sequence length.","Third, we propose a novel Content-ID-Contrastive learning task to complement the advantages of content and ID by aligning them with each other, obtaining more task-oriented content embeddings and more generalized ID embeddings.","In experiments, we implement EM3 on different ranking models in two scenario, achieving significant improvements in both offline evaluation and online A/B test, verifying the generalizability of our method.","Ablation studies and visualization are also performed.","Furthermore, we also conduct experiments on two public datasets to show that our proposed method outperforms the state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.06078v1","category":"cs.IR"}
{"created":"2024-04-09 07:08:00","title":"Unified Entropy Optimization for Open-Set Test-Time Adaptation","abstract":"Test-time adaptation (TTA) aims at adapting a model pre-trained on the labeled source domain to the unlabeled target domain. Existing methods usually focus on improving TTA performance under covariate shifts, while neglecting semantic shifts. In this paper, we delve into a realistic open-set TTA setting where the target domain may contain samples from unknown classes. Many state-of-the-art closed-set TTA methods perform poorly when applied to open-set scenarios, which can be attributed to the inaccurate estimation of data distribution and model confidence. To address these issues, we propose a simple but effective framework called unified entropy optimization (UniEnt), which is capable of simultaneously adapting to covariate-shifted in-distribution (csID) data and detecting covariate-shifted out-of-distribution (csOOD) data. Specifically, UniEnt first mines pseudo-csID and pseudo-csOOD samples from test data, followed by entropy minimization on the pseudo-csID data and entropy maximization on the pseudo-csOOD data. Furthermore, we introduce UniEnt+ to alleviate the noise caused by hard data partition leveraging sample-level confidence. Extensive experiments on CIFAR benchmarks and Tiny-ImageNet-C show the superiority of our framework. The code is available at https://github.com/gaozhengqing/UniEnt","sentences":["Test-time adaptation (TTA) aims at adapting a model pre-trained on the labeled source domain to the unlabeled target domain.","Existing methods usually focus on improving TTA performance under covariate shifts, while neglecting semantic shifts.","In this paper, we delve into a realistic open-set TTA setting where the target domain may contain samples from unknown classes.","Many state-of-the-art closed-set TTA methods perform poorly when applied to open-set scenarios, which can be attributed to the inaccurate estimation of data distribution and model confidence.","To address these issues, we propose a simple but effective framework called unified entropy optimization (UniEnt), which is capable of simultaneously adapting to covariate-shifted in-distribution (csID) data and detecting covariate-shifted out-of-distribution (csOOD) data.","Specifically, UniEnt first mines pseudo-csID and pseudo-csOOD samples from test data, followed by entropy minimization on the pseudo-csID data and entropy maximization on the pseudo-csOOD data.","Furthermore, we introduce UniEnt+ to alleviate the noise caused by hard data partition leveraging sample-level confidence.","Extensive experiments on CIFAR benchmarks and Tiny-ImageNet-C show the superiority of our framework.","The code is available at https://github.com/gaozhengqing/UniEnt"],"url":"http://arxiv.org/abs/2404.06065v1","category":"cs.CV"}
{"created":"2024-04-09 06:53:12","title":"Efficient Quantum Circuits for Machine Learning Activation Functions including Constant T-depth ReLU","abstract":"In recent years, Quantum Machine Learning (QML) has increasingly captured the interest of researchers. Among the components in this domain, activation functions hold a fundamental and indispensable role. Our research focuses on the development of activation functions quantum circuits for integration into fault-tolerant quantum computing architectures, with an emphasis on minimizing $T$-depth. Specifically, we present novel implementations of ReLU and leaky ReLU activation functions, achieving constant $T$-depths of 4 and 8, respectively. Leveraging quantum lookup tables, we extend our exploration to other activation functions such as the sigmoid. This approach enables us to customize precision and $T$-depth by adjusting the number of qubits, making our results more adaptable to various application scenarios. This study represents a significant advancement towards enhancing the practicality and application of quantum machine learning.","sentences":["In recent years, Quantum Machine Learning (QML) has increasingly captured the interest of researchers.","Among the components in this domain, activation functions hold a fundamental and indispensable role.","Our research focuses on the development of activation functions quantum circuits for integration into fault-tolerant quantum computing architectures, with an emphasis on minimizing $T$-depth.","Specifically, we present novel implementations of ReLU and leaky ReLU activation functions, achieving constant $T$-depths of 4 and 8, respectively.","Leveraging quantum lookup tables, we extend our exploration to other activation functions such as the sigmoid.","This approach enables us to customize precision and $T$-depth by adjusting the number of qubits, making our results more adaptable to various application scenarios.","This study represents a significant advancement towards enhancing the practicality and application of quantum machine learning."],"url":"http://arxiv.org/abs/2404.06059v1","category":"quant-ph"}
{"created":"2024-04-09 05:11:28","title":"Band-Attention Modulated RetNet for Face Forgery Detection","abstract":"The transformer networks are extensively utilized in face forgery detection due to their scalability across large datasets.Despite their success, transformers face challenges in balancing the capture of global context, which is crucial for unveiling forgery clues, with computational complexity.To mitigate this issue, we introduce Band-Attention modulated RetNet (BAR-Net), a lightweight network designed to efficiently process extensive visual contexts while avoiding catastrophic forgetting.Our approach empowers the target token to perceive global information by assigning differential attention levels to tokens at varying distances. We implement self-attention along both spatial axes, thereby maintaining spatial priors and easing the computational burden.Moreover, we present the adaptive frequency Band-Attention Modulation mechanism, which treats the entire Discrete Cosine Transform spectrogram as a series of frequency bands with learnable weights.Together, BAR-Net achieves favorable performance on several face forgery datasets, outperforming current state-of-the-art methods.","sentences":["The transformer networks are extensively utilized in face forgery detection due to their scalability across large datasets.","Despite their success, transformers face challenges in balancing the capture of global context, which is crucial for unveiling forgery clues, with computational complexity.","To mitigate this issue, we introduce Band-Attention modulated RetNet (BAR-Net), a lightweight network designed to efficiently process extensive visual contexts while avoiding catastrophic forgetting.","Our approach empowers the target token to perceive global information by assigning differential attention levels to tokens at varying distances.","We implement self-attention along both spatial axes, thereby maintaining spatial priors and easing the computational burden.","Moreover, we present the adaptive frequency Band-Attention Modulation mechanism, which treats the entire Discrete Cosine Transform spectrogram as a series of frequency bands with learnable weights.","Together, BAR-Net achieves favorable performance on several face forgery datasets, outperforming current state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.06022v1","category":"cs.CV"}
{"created":"2024-04-09 05:07:26","title":"Combinational Nonuniform Timeslicing of Dynamic Networks","abstract":"Dynamic networks represent the complex and evolving interrelationships between real-world entities. Given the scale and variability of these networks, finding an optimal slicing interval is essential for meaningful analysis. Nonuniform timeslicing, which adapts to density changes within the network, is drawing attention as a solution to this problem. In this research, we categorized existing algorithms into two domains -- data mining and visualization -- according to their approach to the problem. Data mining approach focuses on capturing temporal patterns of dynamic networks, while visualization approach emphasizes lessening the burden of analysis. We then introduce a novel nonuniform timeslicing method that synthesizes the strengths of both approaches, demonstrating its efficacy with a real-world data. The findings suggest that combining the two approaches offers the potential for more effective network analysis.","sentences":["Dynamic networks represent the complex and evolving interrelationships between real-world entities.","Given the scale and variability of these networks, finding an optimal slicing interval is essential for meaningful analysis.","Nonuniform timeslicing, which adapts to density changes within the network, is drawing attention as a solution to this problem.","In this research, we categorized existing algorithms into two domains -- data mining and visualization -- according to their approach to the problem.","Data mining approach focuses on capturing temporal patterns of dynamic networks, while visualization approach emphasizes lessening the burden of analysis.","We then introduce a novel nonuniform timeslicing method that synthesizes the strengths of both approaches, demonstrating its efficacy with a real-world data.","The findings suggest that combining the two approaches offers the potential for more effective network analysis."],"url":"http://arxiv.org/abs/2404.06021v1","category":"cs.HC"}
{"created":"2024-04-09 04:59:22","title":"Preprocessed GMRES for fast solution of linear equations","abstract":"The article mainly introduces preprocessing algorithms for solving linear equation systems. This algorithm uses three algorithms as inner iterations, namely RPCG algorithm, ADI algorithm, and Kaczmarz algorithm. Then, it uses BA-GMRES as an outer iteration to solve the linear equation system. These three algorithms can indirectly generate preprocessing matrices, which are used for solving equation systems. In addition, we provide corresponding convergence analysis and numerical examples. Through numerical examples, we demonstrate the effectiveness and feasibility of these preprocessing methods. Furthermore, in the Kaczmarz algorithm, we introduce both constant step size and adaptive step size, and extend the parameter range of the Kaczmarz algorithm to $\\alpha\\in(0,\\infty)$. We also study the solution rate of linear equation systems using different step sizes. Numerical examples show that both constant step size and adaptive step size have higher solution efficiency than the solving algorithm without preprocessing.","sentences":["The article mainly introduces preprocessing algorithms for solving linear equation systems.","This algorithm uses three algorithms as inner iterations, namely RPCG algorithm, ADI algorithm, and Kaczmarz algorithm.","Then, it uses BA-GMRES as an outer iteration to solve the linear equation system.","These three algorithms can indirectly generate preprocessing matrices, which are used for solving equation systems.","In addition, we provide corresponding convergence analysis and numerical examples.","Through numerical examples, we demonstrate the effectiveness and feasibility of these preprocessing methods.","Furthermore, in the Kaczmarz algorithm, we introduce both constant step size and adaptive step size, and extend the parameter range of the Kaczmarz algorithm to $\\alpha\\in(0,\\infty)$. We also study the solution rate of linear equation systems using different step sizes.","Numerical examples show that both constant step size and adaptive step size have higher solution efficiency than the solving algorithm without preprocessing."],"url":"http://arxiv.org/abs/2404.06018v1","category":"math.NA"}
{"created":"2024-04-09 04:47:01","title":"Using 3-Objective Evolutionary Algorithms for the Dynamic Chance Constrained Knapsack Problem","abstract":"Real-world optimization problems often involve stochastic and dynamic components. Evolutionary algorithms are particularly effective in these scenarios, as they can easily adapt to uncertain and changing environments but often uncertainty and dynamic changes are studied in isolation. In this paper, we explore the use of 3-objective evolutionary algorithms for the chance constrained knapsack problem with dynamic constraints. In our setting, the weights of the items are stochastic and the knapsack's capacity changes over time. We introduce a 3-objective formulation that is able to deal with the stochastic and dynamic components at the same time and is independent of the confidence level required for the constraint. This new approach is then compared to the 2-objective formulation which is limited to a single confidence level. We evaluate the approach using two different multi-objective evolutionary algorithms (MOEAs), namely the global simple evolutionary multi-objective optimizer (GSEMO) and the multi-objective evolutionary algorithm based on decomposition (MOEA/D), across various benchmark scenarios. Our analysis highlights the advantages of the 3-objective formulation over the 2-objective formulation in addressing the dynamic chance constrained knapsack problem.","sentences":["Real-world optimization problems often involve stochastic and dynamic components.","Evolutionary algorithms are particularly effective in these scenarios, as they can easily adapt to uncertain and changing environments but often uncertainty and dynamic changes are studied in isolation.","In this paper, we explore the use of 3-objective evolutionary algorithms for the chance constrained knapsack problem with dynamic constraints.","In our setting, the weights of the items are stochastic and the knapsack's capacity changes over time.","We introduce a 3-objective formulation that is able to deal with the stochastic and dynamic components at the same time and is independent of the confidence level required for the constraint.","This new approach is then compared to the 2-objective formulation which is limited to a single confidence level.","We evaluate the approach using two different multi-objective evolutionary algorithms (MOEAs), namely the global simple evolutionary multi-objective optimizer (GSEMO) and the multi-objective evolutionary algorithm based on decomposition (MOEA/D), across various benchmark scenarios.","Our analysis highlights the advantages of the 3-objective formulation over the 2-objective formulation in addressing the dynamic chance constrained knapsack problem."],"url":"http://arxiv.org/abs/2404.06014v1","category":"cs.NE"}
{"created":"2024-04-09 04:17:51","title":"FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models","abstract":"The rapid development of large language model (LLM) evaluation methodologies and datasets has led to a profound challenge: integrating state-of-the-art evaluation techniques cost-effectively while ensuring reliability, reproducibility, and efficiency. Currently, there is a notable absence of a unified and adaptable framework that seamlessly integrates various evaluation approaches. Moreover, the reliability of evaluation findings is often questionable due to potential data contamination, with the evaluation efficiency commonly overlooked when facing the substantial costs associated with LLM inference. In response to these challenges, we introduce FreeEval, a modular and scalable framework crafted to enable trustworthy and efficient automatic evaluations of LLMs. Firstly, FreeEval's unified abstractions simplify the integration and improve the transparency of diverse evaluation methodologies, encompassing dynamic evaluation that demand sophisticated LLM interactions. Secondly, the framework integrates meta-evaluation techniques like human evaluation and data contamination detection, which, along with dynamic evaluation modules in the platform, enhance the fairness of the evaluation outcomes. Lastly, FreeEval is designed with a high-performance infrastructure, including distributed computation and caching strategies, enabling extensive evaluations across multi-node, multi-GPU clusters for open-source and proprietary LLMs.","sentences":["The rapid development of large language model (LLM) evaluation methodologies and datasets has led to a profound challenge: integrating state-of-the-art evaluation techniques cost-effectively while ensuring reliability, reproducibility, and efficiency.","Currently, there is a notable absence of a unified and adaptable framework that seamlessly integrates various evaluation approaches.","Moreover, the reliability of evaluation findings is often questionable due to potential data contamination, with the evaluation efficiency commonly overlooked when facing the substantial costs associated with LLM inference.","In response to these challenges, we introduce FreeEval, a modular and scalable framework crafted to enable trustworthy and efficient automatic evaluations of LLMs.","Firstly, FreeEval's unified abstractions simplify the integration and improve the transparency of diverse evaluation methodologies, encompassing dynamic evaluation that demand sophisticated LLM interactions.","Secondly, the framework integrates meta-evaluation techniques like human evaluation and data contamination detection, which, along with dynamic evaluation modules in the platform, enhance the fairness of the evaluation outcomes.","Lastly, FreeEval is designed with a high-performance infrastructure, including distributed computation and caching strategies, enabling extensive evaluations across multi-node, multi-GPU clusters for open-source and proprietary LLMs."],"url":"http://arxiv.org/abs/2404.06003v1","category":"cs.CL"}
{"created":"2024-04-09 03:54:28","title":"AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts","abstract":"As Large Language Models (LLMs) and generative AI become more widespread, the content safety risks associated with their use also increase. We find a notable deficiency in high-quality content safety datasets and benchmarks that comprehensively cover a wide range of critical safety areas. To address this, we define a broad content safety risk taxonomy, comprising 13 critical risk and 9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a new dataset of approximately 26, 000 human-LLM interaction instances, complete with human annotations adhering to the taxonomy. We plan to release this dataset to the community to further research and to help benchmark LLM models for safety. To demonstrate the effectiveness of the dataset, we instruction-tune multiple LLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS), not only surpass or perform competitively with the state-of-the-art LLM-based safety models and general purpose LLMs, but also exhibit robustness across multiple jail-break attack categories. We also show how using AEGISSAFETYDATASET during the LLM alignment phase does not negatively impact the performance of the aligned models on MT Bench scores. Furthermore, we propose AEGIS, a novel application of a no-regret online adaptation framework with strong theoretical guarantees, to perform content moderation with an ensemble of LLM content safety experts in deployment","sentences":["As Large Language Models (LLMs) and generative AI become more widespread, the content safety risks associated with their use also increase.","We find a notable deficiency in high-quality content safety datasets and benchmarks that comprehensively cover a wide range of critical safety areas.","To address this, we define a broad content safety risk taxonomy, comprising 13 critical risk and 9 sparse risk categories.","Additionally, we curate AEGISSAFETYDATASET, a new dataset of approximately 26, 000 human-LLM interaction instances, complete with human annotations adhering to the taxonomy.","We plan to release this dataset to the community to further research and to help benchmark LLM models for safety.","To demonstrate the effectiveness of the dataset, we instruction-tune multiple LLM-based safety models.","We show that our models (named AEGISSAFETYEXPERTS), not only surpass or perform competitively with the state-of-the-art LLM-based safety models and general purpose LLMs, but also exhibit robustness across multiple jail-break attack categories.","We also show how using AEGISSAFETYDATASET during the LLM alignment phase does not negatively impact the performance of the aligned models on MT Bench scores.","Furthermore, we propose AEGIS, a novel application of a no-regret online adaptation framework with strong theoretical guarantees, to perform content moderation with an ensemble of LLM content safety experts in deployment"],"url":"http://arxiv.org/abs/2404.05993v1","category":"cs.LG"}
{"created":"2024-04-09 03:10:45","title":"A Cyber Manufacturing IoT System for Adaptive Machine Learning Model Deployment by Interactive Causality Enabled Self-Labeling","abstract":"Machine Learning (ML) has been demonstrated to improve productivity in many manufacturing applications. To host these ML applications, several software and Industrial Internet of Things (IIoT) systems have been proposed for manufacturing applications to deploy ML applications and provide real-time intelligence. Recently, an interactive causality enabled self-labeling method has been proposed to advance adaptive ML applications in cyber-physical systems, especially manufacturing, by automatically adapting and personalizing ML models after deployment to counter data distribution shifts. The unique features of the self-labeling method require a novel software system to support dynamism at various levels.   This paper proposes the AdaptIoT system, comprised of an end-to-end data streaming pipeline, ML service integration, and an automated self-labeling service. The self-labeling service consists of causal knowledge bases and automated full-cycle self-labeling workflows to adapt multiple ML models simultaneously. AdaptIoT employs a containerized microservice architecture to deliver a scalable and portable solution for small and medium-sized manufacturers. A field demonstration of a self-labeling adaptive ML application is conducted with a makerspace and shows reliable performance.","sentences":["Machine Learning (ML) has been demonstrated to improve productivity in many manufacturing applications.","To host these ML applications, several software and Industrial Internet of Things (IIoT) systems have been proposed for manufacturing applications to deploy ML applications and provide real-time intelligence.","Recently, an interactive causality enabled self-labeling method has been proposed to advance adaptive ML applications in cyber-physical systems, especially manufacturing, by automatically adapting and personalizing ML models after deployment to counter data distribution shifts.","The unique features of the self-labeling method require a novel software system to support dynamism at various levels.   ","This paper proposes the AdaptIoT system, comprised of an end-to-end data streaming pipeline, ML service integration, and an automated self-labeling service.","The self-labeling service consists of causal knowledge bases and automated full-cycle self-labeling workflows to adapt multiple ML models simultaneously.","AdaptIoT employs a containerized microservice architecture to deliver a scalable and portable solution for small and medium-sized manufacturers.","A field demonstration of a self-labeling adaptive ML application is conducted with a makerspace and shows reliable performance."],"url":"http://arxiv.org/abs/2404.05976v1","category":"cs.LG"}
{"created":"2024-04-09 02:51:05","title":"LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders","abstract":"Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 3 popular LLMs ranging from 1.3B to 7B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data. Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data.","sentences":["Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks.","Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations.","In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder.","LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning.","We demonstrate the effectiveness of LLM2Vec by applying it to 3 popular LLMs ranging from 1.3B to 7B parameters and evaluate the transformed models on English word- and sequence-level tasks.","We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB).","Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data.","Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data."],"url":"http://arxiv.org/abs/2404.05961v1","category":"cs.CL"}
{"created":"2024-04-09 00:58:09","title":"Experimental Demonstration of Controllable PT and anti-PT Coupling in a non-Hermitian Metamaterial","abstract":"Non-Hermiticity has recently emerged as a rapidly developing field due to its exotic characteristics related to open systems, where the dissipation plays a critical role. In the presence of balanced energy gain and loss with environment, the system exhibits parity-time (PT) symmetry, meanwhile as the conjugate counterpart, anti-PT symmetry can be achieved with dissipative coupling within the system. Here, we demonstrate the coherence of complex dissipative coupling can control the transition between PT and anti-PT symmetry in an electromagnetic metamaterial. Notably, the achievement of the anti-PT symmetric phase is independent of variations in dissipation. Furthermore, we observe phase transitions as the system crosses exceptional points in both anti-PT and PT symmetric metamaterial configurations, achieved by manipulating the frequency and dissipation of resonators. This work provides a promising metamaterial design for broader exploration of non-Hermitian physics and practical application with controllable Hamiltonian.","sentences":["Non-Hermiticity has recently emerged as a rapidly developing field due to its exotic characteristics related to open systems, where the dissipation plays a critical role.","In the presence of balanced energy gain and loss with environment, the system exhibits parity-time (PT) symmetry, meanwhile as the conjugate counterpart, anti-PT symmetry can be achieved with dissipative coupling within the system.","Here, we demonstrate the coherence of complex dissipative coupling can control the transition between PT and anti-PT symmetry in an electromagnetic metamaterial.","Notably, the achievement of the anti-PT symmetric phase is independent of variations in dissipation.","Furthermore, we observe phase transitions as the system crosses exceptional points in both anti-PT and PT symmetric metamaterial configurations, achieved by manipulating the frequency and dissipation of resonators.","This work provides a promising metamaterial design for broader exploration of non-Hermitian physics and practical application with controllable Hamiltonian."],"url":"http://arxiv.org/abs/2404.05922v1","category":"physics.optics"}
{"created":"2024-04-09 00:51:24","title":"Inclusive Practices for Child-Centered AI Design and Testing","abstract":"We explore ideas and inclusive practices for designing and testing child-centered artificially intelligent technologies for neurodivergent children. AI is promising for supporting social communication, self-regulation, and sensory processing challenges common for neurodivergent children. The authors, both neurodivergent individuals and related to neurodivergent people, draw from their professional and personal experiences to offer insights on creating AI technologies that are accessible and include input from neurodivergent children. We offer ideas for designing AI technologies for neurodivergent children and considerations for including them in the design process while accounting for their sensory sensitivities. We conclude by emphasizing the importance of adaptable and supportive AI technologies and design processes and call for further conversation to refine child-centered AI design and testing methods.","sentences":["We explore ideas and inclusive practices for designing and testing child-centered artificially intelligent technologies for neurodivergent children.","AI is promising for supporting social communication, self-regulation, and sensory processing challenges common for neurodivergent children.","The authors, both neurodivergent individuals and related to neurodivergent people, draw from their professional and personal experiences to offer insights on creating AI technologies that are accessible and include input from neurodivergent children.","We offer ideas for designing AI technologies for neurodivergent children and considerations for including them in the design process while accounting for their sensory sensitivities.","We conclude by emphasizing the importance of adaptable and supportive AI technologies and design processes and call for further conversation to refine child-centered AI design and testing methods."],"url":"http://arxiv.org/abs/2404.05920v1","category":"cs.HC"}
{"created":"2024-04-09 00:43:45","title":"AdaGossip: Adaptive Consensus Step-size for Decentralized Deep Learning with Communication Compression","abstract":"Decentralized learning is crucial in supporting on-device learning over large distributed datasets, eliminating the need for a central server. However, the communication overhead remains a major bottleneck for the practical realization of such decentralized setups. To tackle this issue, several algorithms for decentralized training with compressed communication have been proposed in the literature. Most of these algorithms introduce an additional hyper-parameter referred to as consensus step-size which is tuned based on the compression ratio at the beginning of the training. In this work, we propose AdaGossip, a novel technique that adaptively adjusts the consensus step-size based on the compressed model differences between neighboring agents. We demonstrate the effectiveness of the proposed method through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, Imagenette, and ImageNet), model architectures, and network topologies. Our experiments show that the proposed method achieves superior performance ($0-2\\%$ improvement in test accuracy) compared to the current state-of-the-art method for decentralized learning with communication compression.","sentences":["Decentralized learning is crucial in supporting on-device learning over large distributed datasets, eliminating the need for a central server.","However, the communication overhead remains a major bottleneck for the practical realization of such decentralized setups.","To tackle this issue, several algorithms for decentralized training with compressed communication have been proposed in the literature.","Most of these algorithms introduce an additional hyper-parameter referred to as consensus step-size which is tuned based on the compression ratio at the beginning of the training.","In this work, we propose AdaGossip, a novel technique that adaptively adjusts the consensus step-size based on the compressed model differences between neighboring agents.","We demonstrate the effectiveness of the proposed method through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, Imagenette, and ImageNet), model architectures, and network topologies.","Our experiments show that the proposed method achieves superior performance ($0-2\\%$ improvement in test accuracy) compared to the current state-of-the-art method for decentralized learning with communication compression."],"url":"http://arxiv.org/abs/2404.05919v1","category":"cs.LG"}
{"created":"2024-04-10 17:58:04","title":"Toward a Better Understanding of Fourier Neural Operators: Analysis and Improvement from a Spectral Perspective","abstract":"In solving partial differential equations (PDEs), Fourier Neural Operators (FNOs) have exhibited notable effectiveness compared to Convolutional Neural Networks (CNNs). This paper presents clear empirical evidence through spectral analysis to elucidate the superiority of FNO over CNNs: FNO is significantly more capable of learning low-frequencies. This empirical evidence also unveils FNO's distinct low-frequency bias, which limits FNO's effectiveness in learning high-frequency information from PDE data. To tackle this challenge, we introduce SpecBoost, an ensemble learning framework that employs multiple FNOs to better capture high-frequency information. Specifically, a secondary FNO is utilized to learn the overlooked high-frequency information from the prediction residual of the initial FNO. Experiments demonstrate that SpecBoost noticeably enhances FNO's prediction accuracy on diverse PDE applications, achieving an up to 71% improvement.","sentences":["In solving partial differential equations (PDEs), Fourier Neural Operators (FNOs) have exhibited notable effectiveness compared to Convolutional Neural Networks (CNNs).","This paper presents clear empirical evidence through spectral analysis to elucidate the superiority of FNO over CNNs: FNO is significantly more capable of learning low-frequencies.","This empirical evidence also unveils FNO's distinct low-frequency bias, which limits FNO's effectiveness in learning high-frequency information from PDE data.","To tackle this challenge, we introduce SpecBoost, an ensemble learning framework that employs multiple FNOs to better capture high-frequency information.","Specifically, a secondary FNO is utilized to learn the overlooked high-frequency information from the prediction residual of the initial FNO.","Experiments demonstrate that SpecBoost noticeably enhances FNO's prediction accuracy on diverse PDE applications, achieving an up to 71% improvement."],"url":"http://arxiv.org/abs/2404.07200v1","category":"cs.LG"}
{"created":"2024-04-10 17:41:41","title":"GCV-Turbo: End-to-end Acceleration of GNN-based Computer Vision Tasks on FPGA","abstract":"Graph neural networks (GNNs) have recently empowered various novel computer vision (CV) tasks. In GNN-based CV tasks, a combination of CNN layers and GNN layers or only GNN layers are employed. This paper introduces GCV-Turbo, a domain-specific accelerator on FPGA for end-to-end acceleration of GNN-based CV tasks. GCV-Turbo consists of two key components: (1) a \\emph{novel} hardware architecture optimized for the computation kernels in both CNNs and GNNs using the same set of computation resources. (2) a PyTorch-compatible compiler that takes a user-defined model as input, performs end-to-end optimization for the computation graph of a given GNN-based CV task, and produces optimized code for hardware execution. The hardware architecture and the compiler work synergistically to support a variety of GNN-based CV tasks. We implement GCV-Turbo on a state-of-the-art FPGA and evaluate its performance across six representative GNN-based CV tasks with diverse input data modalities (e.g., image, human skeleton, point cloud). Compared with state-of-the-art CPU (GPU) implementations, GCV-Turbo achieves an average latency reduction of $68.4\\times$ ($4.1\\times$) on these six GNN-based CV tasks. Moreover, GCV-Turbo supports the execution of the standalone CNNs or GNNs, achieving performance comparable to that of state-of-the-art CNN (GNN) accelerators for widely used CNN-only (GNN-only) models.","sentences":["Graph neural networks (GNNs) have recently empowered various novel computer vision (CV) tasks.","In GNN-based CV tasks, a combination of CNN layers and GNN layers or only GNN layers are employed.","This paper introduces GCV-Turbo, a domain-specific accelerator on FPGA for end-to-end acceleration of GNN-based CV tasks.","GCV-Turbo consists of two key components: (1) a \\emph{novel} hardware architecture optimized for the computation kernels in both CNNs and GNNs using the same set of computation resources.","(2) a PyTorch-compatible compiler that takes a user-defined model as input, performs end-to-end optimization for the computation graph of a given GNN-based CV task, and produces optimized code for hardware execution.","The hardware architecture and the compiler work synergistically to support a variety of GNN-based CV tasks.","We implement GCV-Turbo on a state-of-the-art FPGA and evaluate its performance across six representative GNN-based CV tasks with diverse input data modalities (e.g., image, human skeleton, point cloud).","Compared with state-of-the-art CPU (GPU) implementations, GCV-Turbo achieves an average latency reduction of $68.4\\times$ ($4.1\\times$) on these six GNN-based CV tasks.","Moreover, GCV-Turbo supports the execution of the standalone CNNs or GNNs, achieving performance comparable to that of state-of-the-art CNN (GNN) accelerators for widely used CNN-only (GNN-only) models."],"url":"http://arxiv.org/abs/2404.07188v1","category":"cs.DC"}
{"created":"2024-04-10 16:39:50","title":"Lost in Translation: Modern Neural Networks Still Struggle With Small Realistic Image Transformations","abstract":"Deep neural networks that achieve remarkable performance in image classification have previously been shown to be easily fooled by tiny transformations such as a one pixel translation of the input image. In order to address this problem, two approaches have been proposed in recent years. The first approach suggests using huge datasets together with data augmentation in the hope that a highly varied training set will teach the network to learn to be invariant. The second approach suggests using architectural modifications based on sampling theory to deal explicitly with image translations. In this paper, we show that these approaches still fall short in robustly handling 'natural' image translations that simulate a subtle change in camera orientation. Our findings reveal that a mere one-pixel translation can result in a significant change in the predicted image representation for approximately 40% of the test images in state-of-the-art models (e.g. open-CLIP trained on LAION-2B or DINO-v2) , while models that are explicitly constructed to be robust to cyclic translations can still be fooled with 1 pixel realistic (non-cyclic) translations 11% of the time. We present Robust Inference by Crop Selection: a simple method that can be proven to achieve any desired level of consistency, although with a modest tradeoff with the model's accuracy. Importantly, we demonstrate how employing this method reduces the ability to fool state-of-the-art models with a 1 pixel translation to less than 5% while suffering from only a 1% drop in classification accuracy. Additionally, we show that our method can be easy adjusted to deal with circular shifts as well. In such case we achieve 100% robustness to integer shifts with state-of-the-art accuracy, and with no need for any further training.","sentences":["Deep neural networks that achieve remarkable performance in image classification have previously been shown to be easily fooled by tiny transformations such as a one pixel translation of the input image.","In order to address this problem, two approaches have been proposed in recent years.","The first approach suggests using huge datasets together with data augmentation in the hope that a highly varied training set will teach the network to learn to be invariant.","The second approach suggests using architectural modifications based on sampling theory to deal explicitly with image translations.","In this paper, we show that these approaches still fall short in robustly handling 'natural' image translations that simulate a subtle change in camera orientation.","Our findings reveal that a mere one-pixel translation can result in a significant change in the predicted image representation for approximately 40% of the test images in state-of-the-art models (e.g. open-CLIP trained on LAION-2B or DINO-v2) , while models that are explicitly constructed to be robust to cyclic translations can still be fooled with 1 pixel realistic (non-cyclic) translations 11% of the time.","We present Robust Inference by Crop Selection: a simple method that can be proven to achieve any desired level of consistency, although with a modest tradeoff with the model's accuracy.","Importantly, we demonstrate how employing this method reduces the ability to fool state-of-the-art models with a 1 pixel translation to less than 5% while suffering from only a 1% drop in classification accuracy.","Additionally, we show that our method can be easy adjusted to deal with circular shifts as well.","In such case we achieve 100% robustness to integer shifts with state-of-the-art accuracy, and with no need for any further training."],"url":"http://arxiv.org/abs/2404.07153v1","category":"cs.CV"}
{"created":"2024-04-10 16:14:02","title":"Nonexistence of Courant-type nodal domain bounds for eigenfunctions of the Dirichlet-to-Neumann operator","abstract":"Given a compact manifold $\\mathcal M$ with boundary of dimension $n\\geq 3$ and any integers $K$ and $N$, we show that there exists a metric on $\\mathcal M$ for which the first $K$ nonconstant eigenfunctions of the Dirichlet-to-Neumann map on $\\partial\\mathcal M$ have at least $N$ nodal components. This provides a negative answer to the question of whether the number of nodal domains of Dirichlet-to-Neumann eigenfunctions satisfies a Courant-type bound, which has been featured in recent surveys by Girouard and Polterovich [21, Open problem 9] and by Colbois, Girouard, Gordon and Sher [9, Open question 10.14].","sentences":["Given a compact manifold $\\mathcal M$ with boundary of dimension $n\\geq 3$ and any integers $K$ and $N$, we show that there exists a metric on $\\mathcal M$ for which the first $K$ nonconstant eigenfunctions of the Dirichlet-to-Neumann map on $\\partial\\mathcal M$ have at least $N$ nodal components.","This provides a negative answer to the question of whether the number of nodal domains of Dirichlet-to-Neumann eigenfunctions satisfies a Courant-type bound, which has been featured in recent surveys by Girouard and Polterovich [21, Open problem 9] and by Colbois, Girouard, Gordon and Sher [9, Open question 10.14]."],"url":"http://arxiv.org/abs/2404.07138v1","category":"math.SP"}
{"created":"2024-04-10 15:18:29","title":"Mid-Infrared Spectrum of the Disk around the Forming Companion GQ Lup B Revealed by JWST/MIRI","abstract":"GQ Lup B is a forming brown dwarf companion ($M\\sim10-30~M_J$) showing evidence for an infrared excess associated with a disk surronding the companion itself. Here we present mid-infrared (MIR) observations of GQ Lup B with the Medium Resolution Spectrograph (MRS) on JWST, spanning $4.8-11.7~\\mu$m. We remove the stellar contamination using reference differential imaging based on principal component analysis (PCA), demonstrating that the MRS can perform high-contrast science. Our observations provide a sensitive probe of the disk surrounding GQ Lup B. We find no sign of a silicate feature, similar to other disk surrounding very low mass objects, which likely implies significant grain growth ($a_{\\mathrm{min}}\\gtrsim5~\\mu$m), and potentially dust settling. Additionally, we find that if the emission is dominated by an inner wall, the disk around the companion might have an inner cavity larger than the one set by sublimation. Conversely, if our data probe the emission from a thin flat disk, we find the disk to be very compact. More observations are required to confirm this finding and assess the vertical structure of the disk. This approach paves the path to the future study of circumplanetary disks and their physical properties. Our results demonstrate that MIR spectroscopic observations can reveal the physical characteristics of disks around forming companions, providing unique insights into the formation of giant planets, brown dwarfs and their satellites.","sentences":["GQ Lup B is a forming brown dwarf companion ($M\\sim10-30~M_J$) showing evidence for an infrared excess associated with a disk surronding the companion itself.","Here we present mid-infrared (MIR) observations of GQ Lup B with the Medium Resolution Spectrograph (MRS) on JWST, spanning $4.8-11.7~\\mu$m.","We remove the stellar contamination using reference differential imaging based on principal component analysis (PCA), demonstrating that the MRS can perform high-contrast science.","Our observations provide a sensitive probe of the disk surrounding GQ Lup B. We find no sign of a silicate feature, similar to other disk surrounding very low mass objects, which likely implies significant grain growth ($a_{\\mathrm{min}}\\gtrsim5~\\mu$m), and potentially dust settling.","Additionally, we find that if the emission is dominated by an inner wall, the disk around the companion might have an inner cavity larger than the one set by sublimation.","Conversely, if our data probe the emission from a thin flat disk, we find the disk to be very compact.","More observations are required to confirm this finding and assess the vertical structure of the disk.","This approach paves the path to the future study of circumplanetary disks and their physical properties.","Our results demonstrate that MIR spectroscopic observations can reveal the physical characteristics of disks around forming companions, providing unique insights into the formation of giant planets, brown dwarfs and their satellites."],"url":"http://arxiv.org/abs/2404.07086v1","category":"astro-ph.EP"}
{"created":"2024-04-10 13:58:43","title":"Numerical approximation of SDEs driven by fractional Brownian motion for all $H\\in(0,1)$ using WIS integration","abstract":"We examine the numerical approximation of a quasilinear stochastic differential equation (SDE) with multiplicative fractional Brownian motion. The stochastic integral is interpreted in the Wick-It\\^o-Skorohod (WIS) sense that is well defined and centered for all $H\\in(0,1)$. We give an introduction to the theory of WIS integration before we examine existence and uniqueness of a solution to the SDE. We then introduce our numerical method which is based on the theoretical results in \\cite{Mishura2008article, Mishura2008} for $H\\geq \\frac{1}{2}$. We construct explicitly a translation operator required for the practical implementation of the method and are not aware of any other implementation of a numerical method for the WIS SDE. We then prove a strong convergence result that gives, in the non-autonomous case, an error of $O(\\Delta t^H)$ and in the non-autonomous case $O(\\Delta t^{\\min(H,\\zeta)})$, where $\\zeta$ is a H\\\"older continuity parameter. We present some numerical experiments and conjecture that the theoretical results may not be optimal since we observe numerically a rate of $\\min(H+\\frac{1}{2},1)$ in the autonomous case. This work opens up the possibility to efficiently simulate SDEs for all $H$ values, including small values of $H$ when the stochastic integral is interpreted in the WIS sense.","sentences":["We examine the numerical approximation of a quasilinear stochastic differential equation (SDE) with multiplicative fractional Brownian motion.","The stochastic integral is interpreted in the Wick-It\\^o-Skorohod (WIS) sense that is well defined and centered for all $H\\in(0,1)$.","We give an introduction to the theory of WIS integration before we examine existence and uniqueness of a solution to the SDE.","We then introduce our numerical method which is based on the theoretical results in \\cite{Mishura2008article, Mishura2008} for $H\\geq \\frac{1}{2}$. We construct explicitly a translation operator required for the practical implementation of the method and are not aware of any other implementation of a numerical method for the WIS SDE.","We then prove a strong convergence result that gives, in the non-autonomous case, an error of $O(\\Delta t^H)$ and in the non-autonomous case $O(\\Delta t^{\\min(H,\\zeta)})$, where $\\zeta$ is a H\\\"older continuity parameter.","We present some numerical experiments and conjecture that the theoretical results may not be optimal since we observe numerically a rate of $\\min(H+\\frac{1}{2},1)$ in the autonomous case.","This work opens up the possibility to efficiently simulate SDEs for all $H$ values, including small values of $H$ when the stochastic integral is interpreted in the WIS sense."],"url":"http://arxiv.org/abs/2404.07013v1","category":"math.NA"}
{"created":"2024-04-10 13:32:09","title":"Deformations of the scalar curvature of a partially integrable pseudohermitian manifold","abstract":"We consider deformations of the scalar curvature of a partially integrable pseudohermitian manifold, in analogy with the work of Fischer and Marsden on Riemannian manifolds. In particular, we introduce and discuss $R$-singular spaces, give sufficient conditions for the stability of the scalar curvature, and give a partial infinitesimal rigidity result for the scalar curvature of a compact, torsion-free, scalar-flat, integrable pseudohermitian manifold.","sentences":["We consider deformations of the scalar curvature of a partially integrable pseudohermitian manifold, in analogy with the work of Fischer and Marsden on Riemannian manifolds.","In particular, we introduce and discuss $R$-singular spaces, give sufficient conditions for the stability of the scalar curvature, and give a partial infinitesimal rigidity result for the scalar curvature of a compact, torsion-free, scalar-flat, integrable pseudohermitian manifold."],"url":"http://arxiv.org/abs/2404.07002v1","category":"math.DG"}
{"created":"2024-04-10 12:52:09","title":"A priori regularity estimates for equations degenerating on nodal sets","abstract":"We prove $\\textit{a priori}$ and $\\textit{a posteriori}$ H\\\"older bounds and Schauder $C^{1,\\alpha}$ estimates for continuous solutions to singular/degenerate equations with variable coefficients of type $$ \\mathrm{div}\\left(|u|^a A\\nabla w\\right)=0\\qquad\\mathrm{in \\ }\\Omega\\subset\\mathbb{R}^n, $$ where the weight $u$ solves an elliptic equation of type $\\mathrm{div}\\left(A\\nabla u\\right)=0$ with a Lipschitz-continuous and uniformly elliptic matrix $A$ and has a nontrivial, possibly singular, nodal set. Such estimates are uniform with respect to $u$ in a class of normalized solutions having bounded Almgren's frequency. More precisely, we provide $\\textit{a priori}$ H\\\"{o}lder bounds in any space dimension, and Schauder estimates when $n=2$. When $a=2$, the results apply to the ratios of two solutions to the same PDE sharing their zero sets. Then, one can infer higher order boundary Harnack principles on nodal domains by applying the Schauder estimates for solutions to the auxiliary degenerate equation. The results are based upon a fine blow-up argument, Liouville theorems and quasiconformal maps.","sentences":["We prove $\\textit{a priori}$ and $\\textit{a posteriori}$ H\\\"older bounds and Schauder $C^{1,\\alpha}$ estimates for continuous solutions to singular/degenerate equations with variable coefficients of type $$ \\mathrm{div}\\left(|u|^a A\\nabla w\\right)=0\\qquad\\mathrm{in","\\ }\\Omega\\subset\\mathbb{R}^n, $$ where the weight $u$ solves an elliptic equation of type $\\mathrm{div}\\left(A\\nabla u\\right)=0$ with a Lipschitz-continuous and uniformly elliptic matrix $A$ and has a nontrivial, possibly singular, nodal set.","Such estimates are uniform with respect to $u$ in a class of normalized solutions having bounded Almgren's frequency.","More precisely, we provide $\\textit{a priori}$ H\\\"{o}lder bounds in any space dimension, and Schauder estimates when $n=2$. When $a=2$, the results apply to the ratios of two solutions to the same PDE sharing their zero sets.","Then, one can infer higher order boundary Harnack principles on nodal domains by applying the Schauder estimates for solutions to the auxiliary degenerate equation.","The results are based upon a fine blow-up argument, Liouville theorems and quasiconformal maps."],"url":"http://arxiv.org/abs/2404.06980v1","category":"math.AP"}
{"created":"2024-04-10 12:19:06","title":"Brownian particles controlled by their occupation measure","abstract":"In this article, we study a finite horizon linear-quadratic stochastic control problem for Brownian particles, where the cost functions depend on the state and the occupation measure of the particles. To address this problem, we develop an It\\^o formula for the flow of occupation measure, which enables us to derive the associated Hamilton-Jacobi-Bellman equation. Then, thanks to a Feynman-Kac formula and the Bou\\'e-Dupuis formula, we construct an optimal strategy and an optimal trajectory. Finally, we illustrate our result when the cost-function is the volume of the sausage associated to the particles.","sentences":["In this article, we study a finite horizon linear-quadratic stochastic control problem for Brownian particles, where the cost functions depend on the state and the occupation measure of the particles.","To address this problem, we develop an It\\^o formula for the flow of occupation measure, which enables us to derive the associated Hamilton-Jacobi-Bellman equation.","Then, thanks to a Feynman-Kac formula and the Bou\\'e-Dupuis formula, we construct an optimal strategy and an optimal trajectory.","Finally, we illustrate our result when the cost-function is the volume of the sausage associated to the particles."],"url":"http://arxiv.org/abs/2404.06960v1","category":"math.PR"}
{"created":"2024-04-10 11:50:37","title":"A negative result on regularity estimates on finite radial Morse index solutions to elliptic problems","abstract":"In the regularity theory of solutions to elliptic partial differential equations often the concept of stability plays the role of a sufficient condition for smoothness. It is a natural question to ask if this holds true for nonstable but finite Morse index solutions. We provide a negative answer showing the existence of sequences of solutions with radial Morse index equal to 1 for which regularity estimates can not be satisfied.","sentences":["In the regularity theory of solutions to elliptic partial differential equations often the concept of stability plays the role of a sufficient condition for smoothness.","It is a natural question to ask if this holds true for nonstable but finite Morse index solutions.","We provide a negative answer showing the existence of sequences of solutions with radial Morse index equal to 1 for which regularity estimates can not be satisfied."],"url":"http://arxiv.org/abs/2404.06944v1","category":"math.AP"}
{"created":"2024-04-10 11:28:09","title":"Exact solution of a two-parameter extended Bariev model","abstract":"An exactly solvable strongly correlated electron model with two parameters is constructed in the frame work of the quantum inverse scattering method. Through the nested Bethe ansatz prodedure, the Bethe ansatz equations are obtained and the exact ground state energy in the thermodynamic limit is derived.","sentences":["An exactly solvable strongly correlated electron model with two parameters is constructed in the frame work of the quantum inverse scattering method.","Through the nested Bethe ansatz prodedure, the Bethe ansatz equations are obtained and the exact ground state energy in the thermodynamic limit is derived."],"url":"http://arxiv.org/abs/2404.06929v1","category":"cond-mat.str-el"}
{"created":"2024-04-10 10:38:24","title":"CaDRec: Contextualized and Debiased Recommender Model","abstract":"Recommender models aimed at mining users' behavioral patterns have raised great attention as one of the essential applications in daily life. Recent work on graph neural networks (GNNs) or debiasing methods has attained remarkable gains. However, they still suffer from (1) over-smoothing node embeddings caused by recursive convolutions with GNNs, and (2) the skewed distribution of interactions due to popularity and user-individual biases. This paper proposes a contextualized and debiased recommender model (CaDRec). To overcome the over-smoothing issue, we explore a novel hypergraph convolution operator that can select effective neighbors during convolution by introducing both structural context and sequential context. To tackle the skewed distribution, we propose two strategies for disentangling interactions: (1) modeling individual biases to learn unbiased item embeddings, and (2) incorporating item popularity with positional encoding. Moreover, we mathematically show that the imbalance of the gradients to update item embeddings exacerbates the popularity bias, thus adopting regularization and weighting schemes as solutions. Extensive experiments on four datasets demonstrate the superiority of the CaDRec against state-of-the-art (SOTA) methods. Our source code and data are released at https://github.com/WangXFng/CaDRec.","sentences":["Recommender models aimed at mining users' behavioral patterns have raised great attention as one of the essential applications in daily life.","Recent work on graph neural networks (GNNs) or debiasing methods has attained remarkable gains.","However, they still suffer from (1) over-smoothing node embeddings caused by recursive convolutions with GNNs, and (2) the skewed distribution of interactions due to popularity and user-individual biases.","This paper proposes a contextualized and debiased recommender model (CaDRec).","To overcome the over-smoothing issue, we explore a novel hypergraph convolution operator that can select effective neighbors during convolution by introducing both structural context and sequential context.","To tackle the skewed distribution, we propose two strategies for disentangling interactions: (1) modeling individual biases to learn unbiased item embeddings, and (2) incorporating item popularity with positional encoding.","Moreover, we mathematically show that the imbalance of the gradients to update item embeddings exacerbates the popularity bias, thus adopting regularization and weighting schemes as solutions.","Extensive experiments on four datasets demonstrate the superiority of the CaDRec against state-of-the-art (SOTA) methods.","Our source code and data are released at https://github.com/WangXFng/CaDRec."],"url":"http://arxiv.org/abs/2404.06895v1","category":"cs.IR"}
{"created":"2024-04-10 09:47:14","title":"The 'Sandwich' meta-framework for architecture agnostic deep privacy-preserving transfer learning for non-invasive brainwave decoding","abstract":"Machine learning has enhanced the performance of decoding signals indicating human behaviour. EEG decoding, as an exemplar indicating neural activity and human thoughts non-invasively, has been helpful in neural activity analysis and aiding patients via brain-computer interfaces. However, training machine learning algorithms on EEG encounters two primary challenges: variability across data sets and privacy concerns using data from individuals and data centres. Our objective is to address these challenges by integrating transfer learning for data variability and federated learning for data privacy into a unified approach. We introduce the Sandwich as a novel deep privacy-preserving meta-framework combining transfer learning and federated learning. The Sandwich framework comprises three components: federated networks (first layers) that handle data set differences at the input level, a shared network (middle layer) learning common rules and applying transfer learning, and individual classifiers (final layers) for specific tasks of each data set. It enables the central network (central server) to benefit from multiple data sets, while local branches (local servers) maintain data and label privacy. We evaluated the `Sandwich' meta-architecture in various configurations using the BEETL motor imagery challenge, a benchmark for heterogeneous EEG data sets. Compared with baseline models, our `Sandwich' implementations showed superior performance. The best-performing model, the Inception Sandwich with deep set alignment (Inception-SD-Deepset), exceeded baseline methods by 9%. The `Sandwich' framework demonstrates significant advancements in federated deep transfer learning for diverse tasks and data sets. It outperforms conventional deep learning methods, showcasing the potential for effective use of larger, heterogeneous data sets with enhanced privacy as a model-agnostic meta-framework.","sentences":["Machine learning has enhanced the performance of decoding signals indicating human behaviour.","EEG decoding, as an exemplar indicating neural activity and human thoughts non-invasively, has been helpful in neural activity analysis and aiding patients via brain-computer interfaces.","However, training machine learning algorithms on EEG encounters two primary challenges: variability across data sets and privacy concerns using data from individuals and data centres.","Our objective is to address these challenges by integrating transfer learning for data variability and federated learning for data privacy into a unified approach.","We introduce the Sandwich as a novel deep privacy-preserving meta-framework combining transfer learning and federated learning.","The Sandwich framework comprises three components: federated networks (first layers) that handle data set differences at the input level, a shared network (middle layer) learning common rules and applying transfer learning, and individual classifiers (final layers) for specific tasks of each data set.","It enables the central network (central server) to benefit from multiple data sets, while local branches (local servers) maintain data and label privacy.","We evaluated the `Sandwich' meta-architecture in various configurations using the BEETL motor imagery challenge, a benchmark for heterogeneous EEG data sets.","Compared with baseline models, our `Sandwich' implementations showed superior performance.","The best-performing model, the Inception Sandwich with deep set alignment (Inception-SD-Deepset), exceeded baseline methods by 9%.","The `Sandwich' framework demonstrates significant advancements in federated deep transfer learning for diverse tasks and data sets.","It outperforms conventional deep learning methods, showcasing the potential for effective use of larger, heterogeneous data sets with enhanced privacy as a model-agnostic meta-framework."],"url":"http://arxiv.org/abs/2404.06868v1","category":"eess.SP"}
{"created":"2024-04-10 09:45:32","title":"The G\u00f6del Universe as the Lie Group with left-invariant Lorentz metric","abstract":"The author studies the G\\\"odel Universe as the Lie group with left-invariant Lorentz metric. The expressions for timelike and isotropic geodesics in elementary functions are found by methods of geometric theory of optimal control for the search of geodesics on Lie groups with left-invariant (sub-)Lorentz metrics. It is proved that the G\\\"odel Universe has no closed timelike or isotropic geodesics.","sentences":["The author studies the G\\\"odel Universe as the Lie group with left-invariant Lorentz metric.","The expressions for timelike and isotropic geodesics in elementary functions are found by methods of geometric theory of optimal control for the search of geodesics on Lie groups with left-invariant (sub-)Lorentz metrics.","It is proved that the G\\\"odel Universe has no closed timelike or isotropic geodesics."],"url":"http://arxiv.org/abs/2404.06866v1","category":"math.DG"}
{"created":"2024-04-10 09:28:19","title":"Decay characterization of solutions to semi-linear structurally damped $\u03c3$-evolution equations with time-dependent damping","abstract":"In this paper, we study the Cauchy problem to the linear damped $\\sigma$-evolution equation with time-dependent damping in the effective cases \\begin{equation*} u_{t t}+(-\\Delta)^\\sigma u+b(t)(-\\Delta)^\\delta u_t=0, \\end{equation*} and investigate the decay rates of the solution and its derivatives that are expressed in terms of the decay character of the initial data $u_0(x)=u(0, x)$ and $u_1(x)=u_t(0, x)$. We are interested also in the existence and decay rate of the global in time solution with small data for the corresponding semi-linear problem with the nonlinear term of power type $||D|^\\gamma u|^p$. The blow-up results for solutions to the semi-linear problem in the case $\\gamma=0$ are presented to show the sharpness of the exponent $p$.","sentences":["In this paper, we study the Cauchy problem to the linear damped $\\sigma$-evolution equation with time-dependent damping in the effective cases \\begin{equation*} u_{t t}+(-\\Delta)^\\sigma u+b(t)(-\\Delta)^\\delta u_t=0, \\end{equation*} and investigate the decay rates of the solution and its derivatives that are expressed in terms of the decay character of the initial data $u_0(x)=u(0, x)$ and $u_1(x)=u_t(0, x)$.","We are interested also in the existence and decay rate of the global in time solution with small data for the corresponding semi-linear problem with the nonlinear term of power type","$||D|^\\gamma u|^p$. The blow-up results for solutions to the semi-linear problem in the case $\\gamma=0$ are presented to show the sharpness of the exponent $p$."],"url":"http://arxiv.org/abs/2404.06855v1","category":"math.AP"}
{"created":"2024-04-10 08:48:09","title":"SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection","abstract":"Detecting anomalies in images has become a well-explored problem in both academia and industry. State-of-the-art algorithms are able to detect defects in increasingly difficult settings and data modalities. However, most current methods are not suited to address 3D objects captured from differing poses. While solutions using Neural Radiance Fields (NeRFs) have been proposed, they suffer from excessive computation requirements, which hinder real-world usability. For this reason, we propose the novel 3D Gaussian splatting-based framework SplatPose which, given multi-view images of a 3D object, accurately estimates the pose of unseen views in a differentiable manner, and detects anomalies in them. We achieve state-of-the-art results in both training and inference speed, and detection performance, even when using less training data than competing methods. We thoroughly evaluate our framework using the recently proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly detection (MAD) data set.","sentences":["Detecting anomalies in images has become a well-explored problem in both academia and industry.","State-of-the-art algorithms are able to detect defects in increasingly difficult settings and data modalities.","However, most current methods are not suited to address 3D objects captured from differing poses.","While solutions using Neural Radiance Fields (NeRFs) have been proposed, they suffer from excessive computation requirements, which hinder real-world usability.","For this reason, we propose the novel 3D Gaussian splatting-based framework SplatPose which, given multi-view images of a 3D object, accurately estimates the pose of unseen views in a differentiable manner, and detects anomalies in them.","We achieve state-of-the-art results in both training and inference speed, and detection performance, even when using less training data than competing methods.","We thoroughly evaluate our framework using the recently proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly detection (MAD) data set."],"url":"http://arxiv.org/abs/2404.06832v1","category":"cs.CV"}
{"created":"2024-04-10 08:19:46","title":"Origins of Fine Structure in DNA Melting Curves","abstract":"With the help of one-dimensional random Potts-like model we study the origins of fine structure observed on differential melting profiles of double-stranded DNA. We assess the effects of sequence arrangement on DNA melting curves through the comparison of results for random, correlated, and block sequences. Our results re-confirm the smearing out the fine structure with the increase of chain length for all types of sequence arrangements and suggest fine structure to be a finite-size effect. We have found, that the fine structure in chains comprised of blocks with the correlation in sequence is more persistent, probably, because of increased sequence disorder the blocks introduce. Many natural DNAs show a well-expressed fine structure of melting profiles. In view of our results it might mean the existence of blocks in such DNAs. The very observation of fine structure may also mean, that there exists an optimal length for natural DNAs \\emph{in vivo}.","sentences":["With the help of one-dimensional random Potts-like model we study the origins of fine structure observed on differential melting profiles of double-stranded DNA.","We assess the effects of sequence arrangement on DNA melting curves through the comparison of results for random, correlated, and block sequences.","Our results re-confirm the smearing out the fine structure with the increase of chain length for all types of sequence arrangements and suggest fine structure to be a finite-size effect.","We have found, that the fine structure in chains comprised of blocks with the correlation in sequence is more persistent, probably, because of increased sequence disorder the blocks introduce.","Many natural DNAs show a well-expressed fine structure of melting profiles.","In view of our results it might mean the existence of blocks in such DNAs.","The very observation of fine structure may also mean, that there exists an optimal length for natural DNAs \\emph{in vivo}."],"url":"http://arxiv.org/abs/2404.06822v1","category":"cond-mat.soft"}
{"created":"2024-04-10 08:04:15","title":"Machine learning assisted optical diagnostics on a cylindrical atmospheric pressure surface dielectric barrier discharge","abstract":"The present study explores combining machine learning (ML) algorithms with standard optical diagnostics (such as time--integrated emission spectroscopy and imaging) to accurately predict operating conditions and assess the emission uniformity of a cylindrical surface Dielectric Barrier Discharge (SDBD). It is demonstrated that ML can be complementary with these optical diagnostics and identify peculiarities associated with the discharge emission pattern at different high voltage waveforms (AC and pulsed) and amplitudes. By employing unsupervised (Principal Component Analysis (PCA)) and supervised (Multilayer Perceptron (MLP) neural networks) algorithms, the applied voltage waveform and amplitude are categorised and predicted based on correlations/differences identified within large amounts of corresponding data. PCA allowed us to effectively classify the voltage waveforms and amplitudes applied to the SDBD through a transformation of the spectroscopic/imaging data into principal components (PCs) and their projection to a two-dimensional PC space. Furthermore, an accurate prediction of the voltage amplitude is achieved using the MLP which is trained with PCA--preprocessed data. A particularly interesting aspect of this concept involves examining the uniformity of the emission pattern of the discharge. This is achieved by analysing spectroscopic data recorded at four different regions around the SDBD surface using the two ML--based techniques. These discoveries are instrumental in enhancing plasma--induced processes. They open up new avenues for real--time control, monitoring, and optimization of plasma--based applications across diverse fields such as flow control for the present SDBD.","sentences":["The present study explores combining machine learning (ML) algorithms with standard optical diagnostics (such as time--integrated emission spectroscopy and imaging) to accurately predict operating conditions and assess the emission uniformity of a cylindrical surface Dielectric Barrier Discharge (SDBD).","It is demonstrated that ML can be complementary with these optical diagnostics and identify peculiarities associated with the discharge emission pattern at different high voltage waveforms (AC and pulsed) and amplitudes.","By employing unsupervised (Principal Component Analysis (PCA)) and supervised (Multilayer Perceptron (MLP) neural networks) algorithms, the applied voltage waveform and amplitude are categorised and predicted based on correlations/differences identified within large amounts of corresponding data.","PCA allowed us to effectively classify the voltage waveforms and amplitudes applied to the SDBD through a transformation of the spectroscopic/imaging data into principal components (PCs) and their projection to a two-dimensional PC space.","Furthermore, an accurate prediction of the voltage amplitude is achieved using the MLP which is trained with PCA--preprocessed data.","A particularly interesting aspect of this concept involves examining the uniformity of the emission pattern of the discharge.","This is achieved by analysing spectroscopic data recorded at four different regions around the SDBD surface using the two ML--based techniques.","These discoveries are instrumental in enhancing plasma--induced processes.","They open up new avenues for real--time control, monitoring, and optimization of plasma--based applications across diverse fields such as flow control for the present SDBD."],"url":"http://arxiv.org/abs/2404.06817v1","category":"physics.plasm-ph"}
{"created":"2024-04-10 08:03:19","title":"On the Cauchy problem for logarithmic fractional Schr{\u00f6}dinger equation","abstract":"We consider the fractional Schrodinger equation with a logarithmic nonlinearity, when the power of the Laplacian is between zero and one. We prove global existence results in three different functional spaces: the Sobolev space corresponding to the quadratic form domain of the fractional Laplacian, the energy space, and a space contained in the operator domain of the fractional Laplacian. For this last case, a finite momentum assumption is made, and the key step consists in estimating the Lie commutator between the fractional Laplacian and the multiplication by a monomial.","sentences":["We consider the fractional Schrodinger equation with a logarithmic nonlinearity, when the power of the Laplacian is between zero and one.","We prove global existence results in three different functional spaces: the Sobolev space corresponding to the quadratic form domain of the fractional Laplacian, the energy space, and a space contained in the operator domain of the fractional Laplacian.","For this last case, a finite momentum assumption is made, and the key step consists in estimating the Lie commutator between the fractional Laplacian and the multiplication by a monomial."],"url":"http://arxiv.org/abs/2404.06816v1","category":"math.AP"}
{"created":"2024-04-10 07:49:38","title":"Identification of Settling Velocity with Physics Informed Neural Networks For Sediment Laden Flows","abstract":"Physics-Informed Neural Networks (PINNs) have shown great potential in the context of fluid dynamics simulations, particularly in reconstructing flow fields and identifying key parameters. In this study, we explore the application of PINNs to recover the dimensionless settling velocity for sedimentation flow. The flow involves sediment-laden fresh water overlying salt water, which is described by Navier-Stokes equations coupled with sediment concentration and salinity transport equations. Two cases are investigated: one where the training data contains the salinity and sediment concentration fields, and another where it contains the velocity field. For both cases, we investigate several flow regimes and show that the model is capable of inferring the unknown parameter and reconstructing the hydrodynamic field of the flow. The quality of the model inference is assessed by comparing it with numerical simulations from a high-fidelity semi-Lagrangian solver. We demonstrate the model's robustness to noise by training it with data corrupted by noise of varying magnitudes, highlighting the potential of PINNs for real-world applications.","sentences":["Physics-Informed Neural Networks (PINNs) have shown great potential in the context of fluid dynamics simulations, particularly in reconstructing flow fields and identifying key parameters.","In this study, we explore the application of PINNs to recover the dimensionless settling velocity for sedimentation flow.","The flow involves sediment-laden fresh water overlying salt water, which is described by Navier-Stokes equations coupled with sediment concentration and salinity transport equations.","Two cases are investigated: one where the training data contains the salinity and sediment concentration fields, and another where it contains the velocity field.","For both cases, we investigate several flow regimes and show that the model is capable of inferring the unknown parameter and reconstructing the hydrodynamic field of the flow.","The quality of the model inference is assessed by comparing it with numerical simulations from a high-fidelity semi-Lagrangian solver.","We demonstrate the model's robustness to noise by training it with data corrupted by noise of varying magnitudes, highlighting the potential of PINNs for real-world applications."],"url":"http://arxiv.org/abs/2404.06802v1","category":"physics.flu-dyn"}
{"created":"2024-04-10 05:43:21","title":"Differential Harnack inequalities for Fisher-KPP type equations on Riemannian manifolds","abstract":"We obtain almost optimal differential Harnack inequalities for a class of nonlinear parabolic equations on Riemannian manifolds with Bakry-\\'{E}mery Ricci curvature bounded below, which includes the classical Fisher-KPP equation and Newell-Whitehead equation. Compared to existing research, we do not impose any additional conditions on the positive solutions. As its application, we derive some optimal Liouville properties.","sentences":["We obtain almost optimal differential Harnack inequalities for a class of nonlinear parabolic equations on Riemannian manifolds with Bakry-\\'{E}mery Ricci curvature bounded below, which includes the classical Fisher-KPP equation and Newell-Whitehead equation.","Compared to existing research, we do not impose any additional conditions on the positive solutions.","As its application, we derive some optimal Liouville properties."],"url":"http://arxiv.org/abs/2404.06755v1","category":"math.AP"}
{"created":"2024-04-10 05:41:05","title":"MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D Reconstruction of Indoor Scenes from Monocular RGB Views","abstract":"Current monocular 3D scene reconstruction (3DR) works are either fully-supervised, or not generalizable, or implicit in 3D representation. We propose a novel framework - MonoSelfRecon that for the first time achieves explicit 3D mesh reconstruction for generalizable indoor scenes with monocular RGB views by purely self-supervision on voxel-SDF (signed distance function). MonoSelfRecon follows an Autoencoder-based architecture, decodes voxel-SDF and a generalizable Neural Radiance Field (NeRF), which is used to guide voxel-SDF in self-supervision. We propose novel self-supervised losses, which not only support pure self-supervision, but can be used together with supervised signals to further boost supervised training. Our experiments show that \"MonoSelfRecon\" trained in pure self-supervision outperforms current best self-supervised indoor depth estimation models and is comparable to 3DR models trained in fully supervision with depth annotations. MonoSelfRecon is not restricted by specific model design, which can be used to any models with voxel-SDF for purely self-supervised manner.","sentences":["Current monocular 3D scene reconstruction (3DR) works are either fully-supervised, or not generalizable, or implicit in 3D representation.","We propose a novel framework - MonoSelfRecon that for the first time achieves explicit 3D mesh reconstruction for generalizable indoor scenes with monocular RGB views by purely self-supervision on voxel-SDF (signed distance function).","MonoSelfRecon follows an Autoencoder-based architecture, decodes voxel-SDF and a generalizable Neural Radiance Field (NeRF), which is used to guide voxel-SDF in self-supervision.","We propose novel self-supervised losses, which not only support pure self-supervision, but can be used together with supervised signals to further boost supervised training.","Our experiments show that \"MonoSelfRecon\" trained in pure self-supervision outperforms current best self-supervised indoor depth estimation models and is comparable to 3DR models trained in fully supervision with depth annotations.","MonoSelfRecon is not restricted by specific model design, which can be used to any models with voxel-SDF for purely self-supervised manner."],"url":"http://arxiv.org/abs/2404.06753v1","category":"cs.CV"}
{"created":"2024-04-10 04:29:24","title":"Global well-posedness of the nonlinear Hartree equation for infinitely many particles with singular interaction","abstract":"The nonlinear Hartree equation (NLH) in the Heisenberg picture admits steady states of the form $\\gamma_f=f(-\\Delta)$ representing quantum states of infinitely many particles. In this article, we consider the time evolution of perturbations from a large class of such steady states via the three-dimensional NLH. We prove that if the interaction potential $w$ has finite measure and initial states have finite relative entropy, then solutions preserve the relative free energy, and they exist globally in time. This result extends the important work of Lewin and Sabin [arXiv:1310.0603] to singular interaction cases.","sentences":["The nonlinear Hartree equation (NLH) in the Heisenberg picture admits steady states of the form $\\gamma_f=f(-\\Delta)$ representing quantum states of infinitely many particles.","In this article, we consider the time evolution of perturbations from a large class of such steady states via the three-dimensional NLH.","We prove that if the interaction potential $w$ has finite measure and initial states have finite relative entropy, then solutions preserve the relative free energy, and they exist globally in time.","This result extends the important work of Lewin and Sabin [arXiv:1310.0603] to singular interaction cases."],"url":"http://arxiv.org/abs/2404.06730v1","category":"math.AP"}
{"created":"2024-04-10 02:25:40","title":"Bigraded path homology and the magnitude-path spectral sequence","abstract":"Two important invariants of directed graphs, namely magnitude homology and path homology, have recently been shown to be intimately connected: there is a 'magnitude-path spectral sequence' or 'MPSS' in which magnitude homology appears as the first page, and in which path homology appears as an axis of the second page. In this paper we study the homological and computational properties of the spectral sequence, and in particular of the full second page, which we now call 'bigraded path homology'. We demonstrate that every page of the MPSS deserves to be regarded as a homology theory in its own right, satisfying excision and Kunneth theorems (along with a homotopy invariance property already established by Asao), and that magnitude homology and bigraded path homology also satisfy Mayer-Vietoris theorems. We construct a homotopy theory of graphs (in the form of a cofibration category structure) in which weak equivalences are the maps inducing isomorphisms on bigraded path homology, strictly refining an existing structure based on ordinary path homology. And we provide complete computations of the MPSS for two important families of graphs - the directed and bi-directed cycles - which demonstrate the power of both the MPSS, and bigraded path homology in particular, to distinguish graphs that ordinary path homology cannot.","sentences":["Two important invariants of directed graphs, namely magnitude homology and path homology, have recently been shown to be intimately connected: there is a 'magnitude-path spectral sequence' or 'MPSS' in which magnitude homology appears as the first page, and in which path homology appears as an axis of the second page.","In this paper we study the homological and computational properties of the spectral sequence, and in particular of the full second page, which we now call 'bigraded path homology'.","We demonstrate that every page of the MPSS deserves to be regarded as a homology theory in its own right, satisfying excision and Kunneth theorems (along with a homotopy invariance property already established by Asao), and that magnitude homology and bigraded path homology also satisfy Mayer-Vietoris theorems.","We construct a homotopy theory of graphs (in the form of a cofibration category structure) in which weak equivalences are the maps inducing isomorphisms on bigraded path homology, strictly refining an existing structure based on ordinary path homology.","And we provide complete computations of the MPSS for two important families of graphs - the directed and bi-directed cycles - which demonstrate the power of both the MPSS, and bigraded path homology in particular, to distinguish graphs that ordinary path homology cannot."],"url":"http://arxiv.org/abs/2404.06689v1","category":"math.AT"}
{"created":"2024-04-10 01:28:15","title":"Non-extensive Effects on the QCD Equation of State and Fluctuations of Conserved Charges within Polyakov Quark Meson Model","abstract":"The influence of non-extensive Tsallis statistics on the hadron phase structure has been investigated using the Polyakov-quark-meson (PQM) model. The analysis examines the non-extensive effects on the temperature dependence of PQM order parameters, thermodynamic quantities related to the QCD equation of state, and fluctuations of conserved charges at varying chemical potentials. The results show that non-extensive effects have the most significant deviations near the crossover region. The pseudo-critical temperature $T_{\\chi}(\\mu_B)$ is not a universal constant and decreases with increasing non-extensive $q$ parameter. The chiral phase diagram of the PQM model indicates a decrease in the behavior of the ($T_{\\chi}-\\mu_B$) plane with increasing non-extensive $q$ parameter. The PQM model exhibits good qualitative agreement with lattice QCD calculations. Moreover, these findings suggest the existence of a Tsallis limit, which serves as an alternative to the Stefan-Boltzmann (SB) limit for the massless ideal gas. The critical endpoint (CEP) exhibits lower temperature but higher chemical potential with increasing non-extensive $q$ parameter. Overall, this study highlights the importance of non-extensive Tsallis statistics in characterizing the quark-hadron phase structure of the PQM model and contributes to a deeper understanding of non-extensive effects in the quark-hadron phase transition.","sentences":["The influence of non-extensive Tsallis statistics on the hadron phase structure has been investigated using the Polyakov-quark-meson (PQM) model.","The analysis examines the non-extensive effects on the temperature dependence of PQM order parameters, thermodynamic quantities related to the QCD equation of state, and fluctuations of conserved charges at varying chemical potentials.","The results show that non-extensive effects have the most significant deviations near the crossover region.","The pseudo-critical temperature $T_{\\chi}(\\mu_B)$ is not a universal constant and decreases with increasing non-extensive $q$ parameter.","The chiral phase diagram of the PQM model indicates a decrease in the behavior of the ($T_{\\chi}-\\mu_B$) plane with increasing non-extensive $q$ parameter.","The PQM model exhibits good qualitative agreement with lattice QCD calculations.","Moreover, these findings suggest the existence of a Tsallis limit, which serves as an alternative to the Stefan-Boltzmann (SB) limit for the massless ideal gas.","The critical endpoint (CEP) exhibits lower temperature but higher chemical potential with increasing non-extensive $q$ parameter.","Overall, this study highlights the importance of non-extensive Tsallis statistics in characterizing the quark-hadron phase structure of the PQM model and contributes to a deeper understanding of non-extensive effects in the quark-hadron phase transition."],"url":"http://arxiv.org/abs/2404.06673v1","category":"hep-ph"}
{"created":"2024-04-10 00:07:32","title":"Modular Vector Fields for Lattice Polarized K3","abstract":"We consider a moduli space of lattice polarized K3 surfaces with the additional information of a frame of the trascendental cohomology with respect to the lattice polarization. This moduli space is proved to be quasi-affine, and the existence of vector fields on it, called modular vector fields, is proved. A purely algebraic version of the algebra of Siegel quasi-modular forms is obtained as the algebra of global regular functions over this moduli space, with a differential structure coming from the modular vector fields. By means of trascendental considerations we are able to obtain a differential algebra of meromorphic Siegel quasi-modular forms from the previous algebra.","sentences":["We consider a moduli space of lattice polarized K3 surfaces with the additional information of a frame of the trascendental cohomology with respect to the lattice polarization.","This moduli space is proved to be quasi-affine, and the existence of vector fields on it, called modular vector fields, is proved.","A purely algebraic version of the algebra of Siegel quasi-modular forms is obtained as the algebra of global regular functions over this moduli space, with a differential structure coming from the modular vector fields.","By means of trascendental considerations we are able to obtain a differential algebra of meromorphic Siegel quasi-modular forms from the previous algebra."],"url":"http://arxiv.org/abs/2404.06662v1","category":"math.AG"}
{"created":"2024-04-09 21:35:51","title":"Quantum graph models for transport in filamentary switching","abstract":"The formation of metallic nanofilaments bridging two electrodes across an insulator is a mechanism for resistive switching. Examples of such phenomena include atomic synapses, which constitute a distinct class of memristive devices whose behavior is closely tied to the properties of the filament. Until recently, experimental investigation of the low-temperature regime and quantum transport effects has been limited. However, with growing interest in understanding the true impacts of the filament on device conductance, comprehending quantum effects has become crucial for quantum neuromorphic hardware. We discuss quantum transport resulting from filamentary switching in a narrow region where the continuous approximation of the contact is not valid, and only a few atoms are involved. In this scenario, the filament can be represented by a graph depicting the adjacency of atoms and the overlap between atomic orbitals. Using quantum graphs, we calculate the scattering amplitude of charge carriers on this graph and explore the interplay between filamentary formation and quantum transport effects.","sentences":["The formation of metallic nanofilaments bridging two electrodes across an insulator is a mechanism for resistive switching.","Examples of such phenomena include atomic synapses, which constitute a distinct class of memristive devices whose behavior is closely tied to the properties of the filament.","Until recently, experimental investigation of the low-temperature regime and quantum transport effects has been limited.","However, with growing interest in understanding the true impacts of the filament on device conductance, comprehending quantum effects has become crucial for quantum neuromorphic hardware.","We discuss quantum transport resulting from filamentary switching in a narrow region where the continuous approximation of the contact is not valid, and only a few atoms are involved.","In this scenario, the filament can be represented by a graph depicting the adjacency of atoms and the overlap between atomic orbitals.","Using quantum graphs, we calculate the scattering amplitude of charge carriers on this graph and explore the interplay between filamentary formation and quantum transport effects."],"url":"http://arxiv.org/abs/2404.06628v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-09 21:09:22","title":"FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations","abstract":"The accurate evaluation of differential treatment in language models to specific groups is critical to ensuring a positive and safe user experience. An ideal evaluation should have the properties of being robust, extendable to new groups or attributes, and being able to capture biases that appear in typical usage (rather than just extreme, rare cases). Relatedly, bias evaluation should surface not only egregious biases but also ones that are subtle and commonplace, such as a likelihood for talking about appearances with regard to women. We present FairPair, an evaluation framework for assessing differential treatment that occurs during ordinary usage. FairPair operates through counterfactual pairs, but crucially, the paired continuations are grounded in the same demographic group, which ensures equivalent comparison. Additionally, unlike prior work, our method factors in the inherent variability that comes from the generation process itself by measuring the sampling variability. We present an evaluation of several commonly used generative models and a qualitative analysis that indicates a preference for discussing family and hobbies with regard to women.","sentences":["The accurate evaluation of differential treatment in language models to specific groups is critical to ensuring a positive and safe user experience.","An ideal evaluation should have the properties of being robust, extendable to new groups or attributes, and being able to capture biases that appear in typical usage (rather than just extreme, rare cases).","Relatedly, bias evaluation should surface not only egregious biases but also ones that are subtle and commonplace, such as a likelihood for talking about appearances with regard to women.","We present FairPair, an evaluation framework for assessing differential treatment that occurs during ordinary usage.","FairPair operates through counterfactual pairs, but crucially, the paired continuations are grounded in the same demographic group, which ensures equivalent comparison.","Additionally, unlike prior work, our method factors in the inherent variability that comes from the generation process itself by measuring the sampling variability.","We present an evaluation of several commonly used generative models and a qualitative analysis that indicates a preference for discussing family and hobbies with regard to women."],"url":"http://arxiv.org/abs/2404.06619v1","category":"cs.CL"}
{"created":"2024-04-09 20:26:24","title":"Internal Lagrangians and spatial-gauge symmetries","abstract":"A direct reformulation of the Hamiltonian formalism in terms of the intrinsic geometry of infinitely prolonged differential equations is obtained. Concepts of spatial equation and spatial-gauge symmetry of a Lagrangian system of equations are introduced. A non-covariant canonical variational principle is proposed and demonstrated using the Maxwell equations as an example. A covariant canonical variational principle is formulated. The results obtained are applicable to any variational equations, including those that do not originate in physics.","sentences":["A direct reformulation of the Hamiltonian formalism in terms of the intrinsic geometry of infinitely prolonged differential equations is obtained.","Concepts of spatial equation and spatial-gauge symmetry of a Lagrangian system of equations are introduced.","A non-covariant canonical variational principle is proposed and demonstrated using the Maxwell equations as an example.","A covariant canonical variational principle is formulated.","The results obtained are applicable to any variational equations, including those that do not originate in physics."],"url":"http://arxiv.org/abs/2404.06606v1","category":"math-ph"}
{"created":"2024-04-09 20:00:12","title":"Spectral decomposition and Siegel-Veech transforms for strata: The case of marked tori","abstract":"Generalizing the well-known construction of Eisenstein series on the modular curves, Siegel-Veech transforms provide a natural construction of square-integrable functions on strata of differentials on Riemannian surfaces. This space carries actions of the foliated Laplacian derived from the SL(2,R)-action as well as various differential operators related to relative period translations.   In the paper we give spectral decompositions for the stratum of tori with two marked points. This is a homogeneous space for a special affine group, which is not reductive and thus does not fall into well-studied cases of the Langlands program, but still allows to employ techniques from representation theory and global analysis. Even for this simple stratum exhibiting all Siegel-Veech transforms requires novel configurations of saddle connections. We also show that the contiunuous spectrum of the foliated Laplacian is much larger than the space of Siegel-Veech transforms, as opposed to the case of the modular curve. This defect can be remedied by using instead a compound Laplacian involving relative period translations.","sentences":["Generalizing the well-known construction of Eisenstein series on the modular curves, Siegel-Veech transforms provide a natural construction of square-integrable functions on strata of differentials on Riemannian surfaces.","This space carries actions of the foliated Laplacian derived from the SL(2,R)-action as well as various differential operators related to relative period translations.   ","In the paper we give spectral decompositions for the stratum of tori with two marked points.","This is a homogeneous space for a special affine group, which is not reductive and thus does not fall into well-studied cases of the Langlands program, but still allows to employ techniques from representation theory and global analysis.","Even for this simple stratum exhibiting all Siegel-Veech transforms requires novel configurations of saddle connections.","We also show that the contiunuous spectrum of the foliated Laplacian is much larger than the space of Siegel-Veech transforms, as opposed to the case of the modular curve.","This defect can be remedied by using instead a compound Laplacian involving relative period translations."],"url":"http://arxiv.org/abs/2404.06597v1","category":"math.NT"}
{"created":"2024-04-09 19:54:03","title":"Superoperator master equations for depolarizing dynamics","abstract":"The work is devoted to superoperator master equations. Namely, the superoperator master equations in the case of the twirling hyperprojector with respect to the whole unitary group are derived. To be consistent with such a hyperprojector the free dynamics is assumed to be depolarizing. And it is perturbed by the arbitrary Gorini--Kossakowski--Sudarshan--Lindblad generator. The explicit form of the second order master equations are presented in this case.","sentences":["The work is devoted to superoperator master equations.","Namely, the superoperator master equations in the case of the twirling hyperprojector with respect to the whole unitary group are derived.","To be consistent with such a hyperprojector the free dynamics is assumed to be depolarizing.","And it is perturbed by the arbitrary Gorini--Kossakowski--Sudarshan--Lindblad generator.","The explicit form of the second order master equations are presented in this case."],"url":"http://arxiv.org/abs/2404.06595v1","category":"quant-ph"}
{"created":"2024-04-09 19:29:19","title":"Phylogeny-Informed Interaction Estimation Accelerates Co-Evolutionary Learning","abstract":"Co-evolution is a powerful problem-solving approach. However, fitness evaluation in co-evolutionary algorithms can be computationally expensive, as the quality of an individual in one population is defined by its interactions with many (or all) members of one or more other populations. To accelerate co-evolutionary systems, we introduce phylogeny-informed interaction estimation, which uses runtime phylogenetic analysis to estimate interaction outcomes between individuals based on how their relatives performed against each other. We test our interaction estimation method with three distinct co-evolutionary systems: two systems focused on measuring problem-solving success and one focused on measuring evolutionary open-endedness. We find that phylogeny-informed estimation can substantially reduce the computation required to solve problems, particularly at the beginning of long-term evolutionary runs. Additionally, we find that our estimation method initially jump-starts the evolution of neural complexity in our open-ended domain, but estimation-free systems eventually \"catch-up\" if given enough time. More broadly, continued refinements to these phylogeny-informed interaction estimation methods offers a promising path to reducing the computational cost of running co-evolutionary systems while maintaining their open-endedness.","sentences":["Co-evolution is a powerful problem-solving approach.","However, fitness evaluation in co-evolutionary algorithms can be computationally expensive, as the quality of an individual in one population is defined by its interactions with many (or all) members of one or more other populations.","To accelerate co-evolutionary systems, we introduce phylogeny-informed interaction estimation, which uses runtime phylogenetic analysis to estimate interaction outcomes between individuals based on how their relatives performed against each other.","We test our interaction estimation method with three distinct co-evolutionary systems: two systems focused on measuring problem-solving success and one focused on measuring evolutionary open-endedness.","We find that phylogeny-informed estimation can substantially reduce the computation required to solve problems, particularly at the beginning of long-term evolutionary runs.","Additionally, we find that our estimation method initially jump-starts the evolution of neural complexity in our open-ended domain, but estimation-free systems eventually \"catch-up\" if given enough time.","More broadly, continued refinements to these phylogeny-informed interaction estimation methods offers a promising path to reducing the computational cost of running co-evolutionary systems while maintaining their open-endedness."],"url":"http://arxiv.org/abs/2404.06588v1","category":"cs.NE"}
{"created":"2024-04-09 19:26:43","title":"Integrability of the sub-Riemannian geodesic flow of the left-invariant metric on the Heisenberg group","abstract":"In this paper, we study two different classes of normal geodesic flows corresponding to the left-invariant sub-Riemannian metric on the $(2n+1)$-dimensional Heisenberg group. The first class corresponds to the left-invariant distribution, while the second corresponds to the right-invariant one. We prove that corresponding Hamiltonian L-L and L-R systems are completely integrable.","sentences":["In this paper, we study two different classes of normal geodesic flows corresponding to the left-invariant sub-Riemannian metric on the $(2n+1)$-dimensional Heisenberg group.","The first class corresponds to the left-invariant distribution, while the second corresponds to the right-invariant one.","We prove that corresponding Hamiltonian L-L and L-R systems are completely integrable."],"url":"http://arxiv.org/abs/2404.06586v1","category":"math.DG"}
{"created":"2024-04-09 19:25:16","title":"Lecture notes on rough paths and applications to machine learning","abstract":"These notes expound the recent use of the signature transform and rough path theory in data science and machine learning. We develop the core theory of the signature from first principles and then survey some recent popular applications of this approach, including signature-based kernel methods and neural rough differential equations. The notes are based on a course given by the two authors at Imperial College London.","sentences":["These notes expound the recent use of the signature transform and rough path theory in data science and machine learning.","We develop the core theory of the signature from first principles and then survey some recent popular applications of this approach, including signature-based kernel methods and neural rough differential equations.","The notes are based on a course given by the two authors at Imperial College London."],"url":"http://arxiv.org/abs/2404.06583v1","category":"cs.LG"}
{"created":"2024-04-09 18:20:16","title":"Temporal True and Surrogate Fitness Landscape Analysis for Expensive Bi-Objective Optimisation","abstract":"Many real-world problems have expensive-to-compute fitness functions and are multi-objective in nature. Surrogate-assisted evolutionary algorithms are often used to tackle such problems. Despite this, literature about analysing the fitness landscapes induced by surrogate models is limited, and even non-existent for multi-objective problems. This study addresses this critical gap by comparing landscapes of the true fitness function with those of surrogate models for multi-objective functions. Moreover, it does so temporally by examining landscape features at different points in time during optimisation, in the vicinity of the population at that point in time. We consider the BBOB bi-objective benchmark functions in our experiments. The results of the fitness landscape analysis reveals significant differences between true and surrogate features at different time points during optimisation. Despite these differences, the true and surrogate landscape features still show high correlations between each other. Furthermore, this study identifies which landscape features are related to search and demonstrates that both surrogate and true landscape features are capable of predicting algorithm performance. These findings indicate that temporal analysis of the landscape features may help to facilitate the design of surrogate switching approaches to improve performance in multi-objective optimisation.","sentences":["Many real-world problems have expensive-to-compute fitness functions and are multi-objective in nature.","Surrogate-assisted evolutionary algorithms are often used to tackle such problems.","Despite this, literature about analysing the fitness landscapes induced by surrogate models is limited, and even non-existent for multi-objective problems.","This study addresses this critical gap by comparing landscapes of the true fitness function with those of surrogate models for multi-objective functions.","Moreover, it does so temporally by examining landscape features at different points in time during optimisation, in the vicinity of the population at that point in time.","We consider the BBOB bi-objective benchmark functions in our experiments.","The results of the fitness landscape analysis reveals significant differences between true and surrogate features at different time points during optimisation.","Despite these differences, the true and surrogate landscape features still show high correlations between each other.","Furthermore, this study identifies which landscape features are related to search and demonstrates that both surrogate and true landscape features are capable of predicting algorithm performance.","These findings indicate that temporal analysis of the landscape features may help to facilitate the design of surrogate switching approaches to improve performance in multi-objective optimisation."],"url":"http://arxiv.org/abs/2404.06557v1","category":"cs.NE"}
{"created":"2024-04-09 18:19:19","title":"Symmetric Discrete Optimal Control and Deep Learning","abstract":"We analyze discrete optimal control problems and their connection with back propagation and deep learning. We consider in particular the symmetric representation of the discrete rigid body equations developed via optimal control analysis and optimal flows on adjoint orbits","sentences":["We analyze discrete optimal control problems and their connection with back propagation and deep learning.","We consider in particular the symmetric representation of the discrete rigid body equations developed via optimal control analysis and optimal flows on adjoint orbits"],"url":"http://arxiv.org/abs/2404.06556v1","category":"math.OC"}
{"created":"2024-04-09 18:09:43","title":"Deformations of exterior differential ideals and applications","abstract":"We develop some basic facts on deformations of exterior differential ideals on a smooth complex algebraic variety. With these tools we study deformations of several types of differential ideals, obtaining several irreducible components of the corresponding moduli spaces","sentences":["We develop some basic facts on deformations of exterior differential ideals on a smooth complex algebraic variety.","With these tools we study deformations of several types of differential ideals, obtaining several irreducible components of the corresponding moduli spaces"],"url":"http://arxiv.org/abs/2404.06554v1","category":"math.AG"}
{"created":"2024-04-09 18:00:00","title":"Solvable models of two-level systems coupled to itinerant electrons: Robust non-Fermi liquid and quantum critical pairing","abstract":"Strange metal behavior is traditionally associated with an underlying putative quantum critical point at zero temperature. However, in many correlated metals, e.g., high-Tc cuprate superconductors, strange metallicity persists at low temperatures over an extended range of microscopic parameters, suggesting the existence of an underlying quantum critical phase, whose possible physical origins remain poorly understood. Systematic investigations of physical scenarios giving rise to such a critical, non-Fermi liquid (NFL) phase are therefore crucial to better understand this puzzling behavior. In a previous work [Bashan et al. arXiv:2310.07768], we considered a solvable large-N model consisting of itinerant electrons coupled to local two-level systems (TLSs) via spatially random interactions, inspired by the possibility of emergent metallic glassiness due to frustrated competing orders, and found that the system hosts an NFL phase with tunable exponents at intermediate couplings. In this work, we expand our investigation to the following: (i) We study the extent to which this NFL phase is generic by considering various deformations of our theory, including coupling of electrons to multiple operators of the TLSs and arbitrarily directed TLS-fields. We find that the physical picture obtained in Bashan et al. qualitatively persist in a wide region of parameter space, showcasing the robustness of the NFL phase; (ii) We analyze the superconducting instability due to coupling of TLSs to electrons, and find a rich structure, including quantum critical pairing associated with the NFL phase and conventional BCS pairing in the weak and strong coupling limits; (iii) We elaborate on the analysis of Bashan et al., including single-particle, transport and thermodynamic properties.","sentences":["Strange metal behavior is traditionally associated with an underlying putative quantum critical point at zero temperature.","However, in many correlated metals, e.g., high-Tc cuprate superconductors, strange metallicity persists at low temperatures over an extended range of microscopic parameters, suggesting the existence of an underlying quantum critical phase, whose possible physical origins remain poorly understood.","Systematic investigations of physical scenarios giving rise to such a critical, non-Fermi liquid (NFL) phase are therefore crucial to better understand this puzzling behavior.","In a previous work [Bashan et al. arXiv:2310.07768], we considered a solvable large-N model consisting of itinerant electrons coupled to local two-level systems (TLSs) via spatially random interactions, inspired by the possibility of emergent metallic glassiness due to frustrated competing orders, and found that the system hosts an NFL phase with tunable exponents at intermediate couplings.","In this work, we expand our investigation to the following: (i) We study the extent to which this NFL phase is generic by considering various deformations of our theory, including coupling of electrons to multiple operators of the TLSs and arbitrarily directed TLS-fields.","We find that the physical picture obtained in Bashan et al. qualitatively persist in a wide region of parameter space, showcasing the robustness of the NFL phase; (ii) We analyze the superconducting instability due to coupling of TLSs to electrons, and find a rich structure, including quantum critical pairing associated with the NFL phase and conventional BCS pairing in the weak and strong coupling limits; (iii) We elaborate on the analysis of Bashan et al., including single-particle, transport and thermodynamic properties."],"url":"http://arxiv.org/abs/2404.06532v1","category":"cond-mat.str-el"}
{"created":"2024-04-09 17:59:47","title":"Superpolynomial Lower Bounds for Smooth 3-LCCs and Sharp Bounds for Designs","abstract":"We give improved lower bounds for binary $3$-query locally correctable codes (3-LCCs) $C \\colon \\{0,1\\}^k \\rightarrow \\{0,1\\}^n$. Specifically, we prove:   (1) If $C$ is a linear design 3-LCC, then $n \\geq 2^{(1 - o(1))\\sqrt{k} }$. A design 3-LCC has the additional property that the correcting sets for every codeword bit form a perfect matching and every pair of codeword bits is queried an equal number of times across all matchings. Our bound is tight up to a factor $\\sqrt{8}$ in the exponent of $2$, as the best construction of binary $3$-LCCs (obtained by taking Reed-Muller codes on $\\mathbb{F}_4$ and applying a natural projection map) is a design $3$-LCC with $n \\leq 2^{\\sqrt{8 k}}$. Up to a $\\sqrt{8}$ factor, this resolves the Hamada conjecture on the maximum $\\mathbb{F}_2$-codimension of a $4$-design.   (2) If $C$ is a smooth, non-linear $3$-LCC with near-perfect completeness, then, $n \\geq k^{\\Omega(\\log k)}$.   (3) If $C$ is a smooth, non-linear $3$-LCC with completeness $1 - \\varepsilon$, then $n \\geq \\tilde{\\Omega}(k^{\\frac{1}{2\\varepsilon}})$. In particular, when $\\varepsilon$ is a small constant, this implies a lower bound for general non-linear LCCs that beats the prior best $n \\geq \\tilde{\\Omega}(k^3)$ lower bound of [AGKM23] by a polynomial factor.   Our design LCC lower bound is obtained via a fine-grained analysis of the Kikuchi matrix method applied to a variant of the matrix used in [KM23]. Our lower bounds for non-linear codes are obtained by designing a from-scratch reduction from nonlinear $3$-LCCs to a system of \"chain polynomial equations\": polynomial equations with similar structure to the long chain derivations that arise in the lower bounds for linear $3$-LCCs [KM23].","sentences":["We give improved lower bounds for binary $3$-query locally correctable codes (3-LCCs) $C \\colon \\{0,1\\}^k \\rightarrow \\{0,1\\}^n$. Specifically, we prove:   (1) If $C$ is a linear design 3-LCC, then $n \\geq 2^{(1 - o(1))\\sqrt{k} }$.","A design 3-LCC has the additional property that the correcting sets for every codeword bit form a perfect matching and every pair of codeword bits is queried an equal number of times across all matchings.","Our bound is tight up to a factor $\\sqrt{8}$ in the exponent of $2$, as the best construction of binary $3$-LCCs (obtained by taking Reed-Muller codes on $\\mathbb{F}_4$ and applying a natural projection map) is a design $3$-LCC with $n \\leq","2^{\\sqrt{8 k}}$.","Up to a $\\sqrt{8}$ factor, this resolves the Hamada conjecture on the maximum $\\mathbb{F}_2$-codimension of a $4$-design.   ","(2) If $C$ is a smooth, non-linear $3$-LCC with near-perfect completeness, then, $n \\geq k^{\\Omega(\\log k)}$.   (3) If $C$ is a smooth, non-linear $3$-LCC with completeness $1 - \\varepsilon$, then $n \\geq \\tilde{\\Omega}(k^{\\frac{1}{2\\varepsilon}})$. In particular, when $\\varepsilon$ is a small constant, this implies a lower bound for general non-linear LCCs that beats the prior best $n \\geq \\tilde{\\Omega}(k^3)$ lower bound of [AGKM23] by a polynomial factor.   ","Our design LCC lower bound is obtained via a fine-grained analysis of the Kikuchi matrix method applied to a variant of the matrix used in [KM23].","Our lower bounds for non-linear codes are obtained by designing a from-scratch reduction from nonlinear $3$-LCCs to a system of \"chain polynomial equations\": polynomial equations with similar structure to the long chain derivations that arise in the lower bounds for linear $3$-LCCs","[KM23]."],"url":"http://arxiv.org/abs/2404.06513v1","category":"cs.CC"}
{"created":"2024-04-09 17:54:23","title":"Evidence for Primordial Alignment: Insights from Stellar Obliquity Measurements for Compact Sub-Saturn Systems","abstract":"Despite decades of effort, the mechanisms by which the spin axis of a star and the orbital axes of its planets become misaligned remain elusive. Particularly, it is of great interest whether the large spin-orbit misalignments observed are driven primarily by high-eccentricity migration -- expected to have occurred for short-period, isolated planets -- or reflect a more universal process that operates across systems with a variety of present-day architectures. Compact multi-planet systems offer a unique opportunity to differentiate between these competing hypotheses, as their tightly-packed configurations preclude violent dynamical histories, including high-eccentricity migration, allowing them to trace the primordial disk plane. In this context, we report measurements of the sky-projected stellar obliquity ($\\lambda$) via the Rossiter-McLaughlin effect for two sub-Saturns in multiple-transiting systems: TOI-5126 b ($\\lambda=1\\pm 48\\,^{\\circ}$) and TOI-5398 b ($\\lambda=-24^{+14}_{-13} \\,^{\\circ}$). Both are spin-orbit aligned, joining a fast-growing group of just three other compact sub-Saturn systems, all of which exhibit spin-orbit alignment. Our results strongly suggest that sub-Saturn systems are primordially aligned and become misaligned largely in the post-disk phase through violent dynamical interactions inherent to eccentric migration, as appears to be the case increasingly for other exoplanet populations.","sentences":["Despite decades of effort, the mechanisms by which the spin axis of a star and the orbital axes of its planets become misaligned remain elusive.","Particularly, it is of great interest whether the large spin-orbit misalignments observed are driven primarily by high-eccentricity migration -- expected to have occurred for short-period, isolated planets -- or reflect a more universal process that operates across systems with a variety of present-day architectures.","Compact multi-planet systems offer a unique opportunity to differentiate between these competing hypotheses, as their tightly-packed configurations preclude violent dynamical histories, including high-eccentricity migration, allowing them to trace the primordial disk plane.","In this context, we report measurements of the sky-projected stellar obliquity ($\\lambda$) via the Rossiter-McLaughlin effect for two sub-Saturns in multiple-transiting systems:","TOI-5126 b ($\\lambda=1\\pm 48\\,^{\\circ}$) and TOI-5398 b ($\\lambda=-24^{+14}_{-13} \\,^{\\circ}$).","Both are spin-orbit aligned, joining a fast-growing group of just three other compact sub-Saturn systems, all of which exhibit spin-orbit alignment.","Our results strongly suggest that sub-Saturn systems are primordially aligned and become misaligned largely in the post-disk phase through violent dynamical interactions inherent to eccentric migration, as appears to be the case increasingly for other exoplanet populations."],"url":"http://arxiv.org/abs/2404.06504v1","category":"astro-ph.EP"}
{"created":"2024-04-09 17:51:21","title":"A Machine Learning Framework for the Prediction of Grain Boundary Segregation in Chemically Complex Environments","abstract":"The discovery of complex concentrated alloys has unveiled materials with diverse atomic environments, prompting the exploration of solute segregation beyond dilute alloys. Data-driven methods offer promising for modeling segregation in such chemically complex environments, and are employed in this study to understand segregation behavior of a refractory complex concentrated alloy, NbMoTaW. A flexible methodology is developed that uses composable computational modules, with different arrangements of these modules employed to obtain site availabilities at absolute zero and the corresponding density of states beyond the dilute limit, resulting in an extremely large dataset containing 10 million data points. The artificial neural network developed here can rely solely on descriptions of local atomic environments to predict behavior at the dilute limit with very small errors, while the addition of negative segregation instance classification allows any solute concentration from zero up to the equiatomic concentration for ternary or quaternary alloys to be modeled at room temperature. The machine learning model thus achieves a significant speed advantage over traditional atomistic simulations, being four orders of magnitude faster, while only experiencing a minimal reduction in accuracy. This efficiency presents a powerful tool for rapid microstructural and interfacial design in unseen domains. Scientifically, our approach reveals a transition in the segregation behavior of Mo from unfavorable in simple systems to favorable in complex environments. Additionally, increasing solute concentration was observed to cause anti-segregation sites to begin to fill, challenging conventional understanding and highlighting the complexity of segregation dynamics in chemically complex environments.","sentences":["The discovery of complex concentrated alloys has unveiled materials with diverse atomic environments, prompting the exploration of solute segregation beyond dilute alloys.","Data-driven methods offer promising for modeling segregation in such chemically complex environments, and are employed in this study to understand segregation behavior of a refractory complex concentrated alloy, NbMoTaW. A flexible methodology is developed that uses composable computational modules, with different arrangements of these modules employed to obtain site availabilities at absolute zero and the corresponding density of states beyond the dilute limit, resulting in an extremely large dataset containing 10 million data points.","The artificial neural network developed here can rely solely on descriptions of local atomic environments to predict behavior at the dilute limit with very small errors, while the addition of negative segregation instance classification allows any solute concentration from zero up to the equiatomic concentration for ternary or quaternary alloys to be modeled at room temperature.","The machine learning model thus achieves a significant speed advantage over traditional atomistic simulations, being four orders of magnitude faster, while only experiencing a minimal reduction in accuracy.","This efficiency presents a powerful tool for rapid microstructural and interfacial design in unseen domains.","Scientifically, our approach reveals a transition in the segregation behavior of Mo from unfavorable in simple systems to favorable in complex environments.","Additionally, increasing solute concentration was observed to cause anti-segregation sites to begin to fill, challenging conventional understanding and highlighting the complexity of segregation dynamics in chemically complex environments."],"url":"http://arxiv.org/abs/2404.06499v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 17:49:58","title":"Lattice determination of the Batalin-Vilkovisky function and the strong running interaction","abstract":"The Batalin-Vilkovisky function is a central component in the modern formulation of the background field method and the physical applications derived from it. In the present work we report on novel lattice results for this particular quantity, obtained by capitalizing on its equality with the Kugo-Ojima function in the Landau gauge. The results of the lattice simulation are in very good agreement with the predictions derived from a continuum analysis based on the corresponding Schwinger-Dyson equations. In addition, we show that an important relation connecting this function with the ghost propagator is fulfilled rather accurately. With the aid of these results, we carry out the first completely lattice-based determination of the process-independent strong running interaction, employed in a variety of phenomenological studies.","sentences":["The Batalin-Vilkovisky function is a central component in the modern formulation of the background field method and the physical applications derived from it.","In the present work we report on novel lattice results for this particular quantity, obtained by capitalizing on its equality with the Kugo-Ojima function in the Landau gauge.","The results of the lattice simulation are in very good agreement with the predictions derived from a continuum analysis based on the corresponding Schwinger-Dyson equations.","In addition, we show that an important relation connecting this function with the ghost propagator is fulfilled rather accurately.","With the aid of these results, we carry out the first completely lattice-based determination of the process-independent strong running interaction, employed in a variety of phenomenological studies."],"url":"http://arxiv.org/abs/2404.06496v1","category":"hep-lat"}
{"created":"2024-04-09 17:44:01","title":"Convergence analysis of novel discontinuous Galerkin methods for a convection dominated problem","abstract":"In this paper, we propose and analyze a numerically stable and convergent scheme for a convection-diffusion-reaction equation in the convection-dominated regime. Discontinuous Galerkin (DG) methods are considered since standard finite element methods for the convection-dominated equation cause spurious oscillations. We choose to follow a novel DG finite element differential calculus framework introduced in Feng et al. (2016) and approximate the infinite-dimensional operators in the equation with the finite-dimensional DG differential operators. Specifically, we construct the numerical method by using the dual-wind discontinuous Galerkin (DWDG) formulation for the diffusive term and the average discrete gradient operator for the convective term along with standard DG stabilization. We prove that the method converges optimally in the convection-dominated regime. Numerical results are provided to support the theoretical findings.","sentences":["In this paper, we propose and analyze a numerically stable and convergent scheme for a convection-diffusion-reaction equation in the convection-dominated regime.","Discontinuous Galerkin (DG) methods are considered since standard finite element methods for the convection-dominated equation cause spurious oscillations.","We choose to follow a novel DG finite element differential calculus framework introduced in Feng et al. (2016) and approximate the infinite-dimensional operators in the equation with the finite-dimensional DG differential operators.","Specifically, we construct the numerical method by using the dual-wind discontinuous Galerkin (DWDG) formulation for the diffusive term and the average discrete gradient operator for the convective term along with standard DG stabilization.","We prove that the method converges optimally in the convection-dominated regime.","Numerical results are provided to support the theoretical findings."],"url":"http://arxiv.org/abs/2404.06490v1","category":"math.NA"}
{"created":"2024-04-09 17:40:29","title":"Uncovering Tidal Treasures: Automated Classification of Faint Tidal Features in DECaLS Data","abstract":"Tidal features are a key observable prediction of the hierarchical model of galaxy formation and contain a wealth of information about the properties and history of a galaxy. Modern wide-field surveys such as LSST and Euclid will revolutionise the study of tidal features. However, the volume of data will far surpass the capacity to inspect each galaxy to identify the feature visually, thereby motivating an urgent need to develop automated detection methods. This paper presents a visual classification of $\\sim$2,000 galaxies from the DECaLS survey into different tidal feature categories: arms, streams, shells, and diffuse. Using these labels, we trained a Convolutional Neural Network (CNN) to reproduce the assigned visual classifications. Overall our network performed well and retrieved a median $81.1^{+5.8}_{-6.5}$, $65.7^{+5.0}_{-8.4}$, $91.3^{+6.0}_{-5.9}$, and $82.3^{+1.4}_{-7.9}$ per cent of the actual instances of arm, stream, shell, and diffuse features respectively for just 20 per cent contamination. We verified that the network was classifying the images correctly by using a Gradient-weighted Class Activation Mapping analysis to highlight important regions on the images for a given classification. This is the first demonstration of using CNNs to classify tidal features into sub-categories, and it will pave the way for the identification of different categories of tidal features in the vast samples of galaxies that forthcoming wide-field surveys will deliver.","sentences":["Tidal features are a key observable prediction of the hierarchical model of galaxy formation and contain a wealth of information about the properties and history of a galaxy.","Modern wide-field surveys such as LSST and Euclid will revolutionise the study of tidal features.","However, the volume of data will far surpass the capacity to inspect each galaxy to identify the feature visually, thereby motivating an urgent need to develop automated detection methods.","This paper presents a visual classification of $\\sim$2,000 galaxies from the DECaLS survey into different tidal feature categories: arms, streams, shells, and diffuse.","Using these labels, we trained a Convolutional Neural Network (CNN) to reproduce the assigned visual classifications.","Overall our network performed well and retrieved a median $81.1^{+5.8}_{-6.5}$, $65.7^{+5.0}_{-8.4}$, $91.3^{+6.0}_{-5.9}$, and $82.3^{+1.4}_{-7.9}$ per cent of the actual instances of arm, stream, shell, and diffuse features respectively for just 20 per cent contamination.","We verified that the network was classifying the images correctly by using a Gradient-weighted Class Activation Mapping analysis to highlight important regions on the images for a given classification.","This is the first demonstration of using CNNs to classify tidal features into sub-categories, and it will pave the way for the identification of different categories of tidal features in the vast samples of galaxies that forthcoming wide-field surveys will deliver."],"url":"http://arxiv.org/abs/2404.06487v1","category":"astro-ph.GA"}
{"created":"2024-04-09 17:01:10","title":"Periodic solutions to integro-differential equations: variational formulation, symmetry, and regularity","abstract":"We consider nonconstant periodic constrained minimizers of semilinear elliptic equations for integro-differential operators in $\\mathbb{R}$. We prove that, after an appropriate translation, each of them is necessarily an even function which is decreasing in half its period. In particular, it has only two critical points in half its period, the absolute maximum and minimum. If these statements hold for all nonconstant periodic solutions, and not only for constrained minimizers, remains as an open problem.   Our results apply to operators with kernels in two different classes: kernels $K$ which are convex and kernels for which $K(\\tau^{1/2})$ is a completely monotonic function of $\\tau$. This last new class arose in our previous work on nonlocal Delaunay surfaces in $\\mathbb{R}^n$. Due to their symmetry of revolution, it gave rise to a 1d problem involving an operator with a nonconvex kernel. Our proofs are based on a not so well-known Riesz rearrangement inequality on the circle $\\mathbb{S}^1$ established in 1976.   We also put in evidence a new regularity fact which is a truly nonlocal-semilinear effect and also occurs in the nonperiodic setting. Namely, for nonlinearities in $C^\\beta$ and when $2s+\\beta <1$ ($2s$ being the order of the operator), the solution is not always $C^{2s+\\beta-\\epsilon}$ for all $\\epsilon>0$.","sentences":["We consider nonconstant periodic constrained minimizers of semilinear elliptic equations for integro-differential operators in $\\mathbb{R}$. We prove that, after an appropriate translation, each of them is necessarily an even function which is decreasing in half its period.","In particular, it has only two critical points in half its period, the absolute maximum and minimum.","If these statements hold for all nonconstant periodic solutions, and not only for constrained minimizers, remains as an open problem.   ","Our results apply to operators with kernels in two different classes: kernels $K$ which are convex and kernels for which $K(\\tau^{1/2})$ is a completely monotonic function of $\\tau$. This last new class arose in our previous work on nonlocal Delaunay surfaces in $\\mathbb{R}^n$. Due to their symmetry of revolution, it gave rise to a 1d problem involving an operator with a nonconvex kernel.","Our proofs are based on a not so well-known Riesz rearrangement inequality on the circle $\\mathbb{S}^1$ established in 1976.   ","We also put in evidence a new regularity fact which is a truly nonlocal-semilinear effect and also occurs in the nonperiodic setting.","Namely, for nonlinearities in $C^\\beta$ and when $2s+\\beta <1$ ($2s$ being the order of the operator), the solution is not always $C^{2s+\\beta-\\epsilon}$ for all $\\epsilon>0$."],"url":"http://arxiv.org/abs/2404.06462v1","category":"math.AP"}
{"created":"2024-04-09 17:00:43","title":"Learning Locally Interacting Discrete Dynamical Systems: Towards Data-Efficient and Scalable Prediction","abstract":"Locally interacting dynamical systems, such as epidemic spread, rumor propagation through crowd, and forest fire, exhibit complex global dynamics originated from local, relatively simple, and often stochastic interactions between dynamic elements. Their temporal evolution is often driven by transitions between a finite number of discrete states. Despite significant advancements in predictive modeling through deep learning, such interactions among many elements have rarely explored as a specific domain for predictive modeling. We present Attentive Recurrent Neural Cellular Automata (AR-NCA), to effectively discover unknown local state transition rules by associating the temporal information between neighboring cells in a permutation-invariant manner. AR-NCA exhibits the superior generalizability across various system configurations (i.e., spatial distribution of states), data efficiency and robustness in extremely data-limited scenarios even in the presence of stochastic interactions, and scalability through spatial dimension-independent prediction.","sentences":["Locally interacting dynamical systems, such as epidemic spread, rumor propagation through crowd, and forest fire, exhibit complex global dynamics originated from local, relatively simple, and often stochastic interactions between dynamic elements.","Their temporal evolution is often driven by transitions between a finite number of discrete states.","Despite significant advancements in predictive modeling through deep learning, such interactions among many elements have rarely explored as a specific domain for predictive modeling.","We present Attentive Recurrent Neural Cellular Automata (AR-NCA), to effectively discover unknown local state transition rules by associating the temporal information between neighboring cells in a permutation-invariant manner.","AR-NCA exhibits the superior generalizability across various system configurations (i.e., spatial distribution of states), data efficiency and robustness in extremely data-limited scenarios even in the presence of stochastic interactions, and scalability through spatial dimension-independent prediction."],"url":"http://arxiv.org/abs/2404.06460v1","category":"eess.SY"}
{"created":"2024-04-09 17:00:33","title":"A hybrid discrete-continuum modelling approach for the interactions of the immune system with oncolytic viral infections","abstract":"Oncolytic virotherapy, utilizing genetically modified viruses to combat cancer and trigger anti-cancer immune responses, has garnered significant attention in recent years. In our previous work arXiv:2305.12386, we developed a stochastic agent-based model elucidating the spatial dynamics of infected and uninfected cells within solid tumours. Building upon this foundation, we present a novel stochastic agent-based model to describe the intricate interplay between the virus and the immune system; the agents' dynamics are coupled with a balance equation for the concentration of the chemoattractant that guides the movement of immune cells. We formally derive the continuum limit of the model and carry out a systematic quantitative comparison between this system of PDEs and the individual-based model in two spatial dimensions. Furthermore, we describe the traveling waves of the three populations, with the uninfected proliferative cells trying to escape from the infected cells while immune cells infiltrate the tumour.   Simulations show a good agreement between agent-based approaches and numerical results for the continuum model. Some parameter ranges give rise to oscillations of cell number in both models, in line with the behaviour of the corresponding nonspatial model, which presents Hopf bifurcations. Nevertheless, in some situations the behaviours of the two models may differ significantly, suggesting that stochasticity plays a key role in the dynamics. Our results highlight that a too rapid immune response, before the infection is well-established, appears to decrease the efficacy of the therapy and thus some care is needed when oncolytic virotherapy is combined with immunotherapy. This further suggests the importance of clinically improving the modulation of the immune response according to the tumour's characteristics and to the immune capabilities of the patients.","sentences":["Oncolytic virotherapy, utilizing genetically modified viruses to combat cancer and trigger anti-cancer immune responses, has garnered significant attention in recent years.","In our previous work arXiv:2305.12386, we developed a stochastic agent-based model elucidating the spatial dynamics of infected and uninfected cells within solid tumours.","Building upon this foundation, we present a novel stochastic agent-based model to describe the intricate interplay between the virus and the immune system; the agents' dynamics are coupled with a balance equation for the concentration of the chemoattractant that guides the movement of immune cells.","We formally derive the continuum limit of the model and carry out a systematic quantitative comparison between this system of PDEs and the individual-based model in two spatial dimensions.","Furthermore, we describe the traveling waves of the three populations, with the uninfected proliferative cells trying to escape from the infected cells while immune cells infiltrate the tumour.   ","Simulations show a good agreement between agent-based approaches and numerical results for the continuum model.","Some parameter ranges give rise to oscillations of cell number in both models, in line with the behaviour of the corresponding nonspatial model, which presents Hopf bifurcations.","Nevertheless, in some situations the behaviours of the two models may differ significantly, suggesting that stochasticity plays a key role in the dynamics.","Our results highlight that a too rapid immune response, before the infection is well-established, appears to decrease the efficacy of the therapy and thus some care is needed when oncolytic virotherapy is combined with immunotherapy.","This further suggests the importance of clinically improving the modulation of the immune response according to the tumour's characteristics and to the immune capabilities of the patients."],"url":"http://arxiv.org/abs/2404.06459v1","category":"q-bio.PE"}
{"created":"2024-04-09 16:51:08","title":"Deep-Learning Database of Density Functional Theory Hamiltonians for Twisted Materials","abstract":"Moir\\'e-twisted materials have garnered significant research interest due to their distinctive properties and intriguing physics. However, conducting first-principles studies on such materials faces challenges, notably the formidable computational cost associated with simulating ultra-large twisted structures. This obstacle impedes the construction of a twisted materials database crucial for datadriven materials discovery. Here, by using high-throughput calculations and state-of-the-art neural network methods, we construct a Deep-learning Database of density functional theory (DFT) Hamiltonians for Twisted materials named DDHT. The DDHT database comprises trained neural-network models of over a hundred homo-bilayer and hetero-bilayer moir\\'e-twisted materials. These models enable accurate prediction of the DFT Hamiltonian for these materials across arbitrary twist angles, with an averaged mean absolute error of approximately 1.0 meV or lower. The database facilitates the exploration of flat bands and correlated materials platforms within ultra-large twisted structures.","sentences":["Moir\\'e-twisted materials have garnered significant research interest due to their distinctive properties and intriguing physics.","However, conducting first-principles studies on such materials faces challenges, notably the formidable computational cost associated with simulating ultra-large twisted structures.","This obstacle impedes the construction of a twisted materials database crucial for datadriven materials discovery.","Here, by using high-throughput calculations and state-of-the-art neural network methods, we construct a Deep-learning Database of density functional theory (DFT) Hamiltonians for Twisted materials named DDHT.","The DDHT database comprises trained neural-network models of over a hundred homo-bilayer and hetero-bilayer moir\\'e-twisted materials.","These models enable accurate prediction of the DFT Hamiltonian for these materials across arbitrary twist angles, with an averaged mean absolute error of approximately 1.0 meV or lower.","The database facilitates the exploration of flat bands and correlated materials platforms within ultra-large twisted structures."],"url":"http://arxiv.org/abs/2404.06449v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 16:28:54","title":"Seasonal Fire Prediction using Spatio-Temporal Deep Neural Networks","abstract":"With climate change expected to exacerbate fire weather conditions, the accurate anticipation of wildfires on a global scale becomes increasingly crucial for disaster mitigation. In this study, we utilize SeasFire, a comprehensive global wildfire dataset with climate, vegetation, oceanic indices, and human-related variables, to enable seasonal wildfire forecasting with machine learning. For the predictive analysis, we train deep learning models with different architectures that capture the spatio-temporal context leading to wildfires. Our investigation focuses on assessing the effectiveness of these models in predicting the presence of burned areas at varying forecasting time horizons globally, extending up to six months into the future, and on how different spatial or/and temporal context affects the performance of the models. Our findings demonstrate the great potential of deep learning models in seasonal fire forecasting; longer input time-series leads to more robust predictions across varying forecasting horizons, while integrating spatial information to capture wildfire spatio-temporal dynamics boosts performance. Finally, our results hint that in order to enhance performance at longer forecasting horizons, a larger receptive field spatially needs to be considered.","sentences":["With climate change expected to exacerbate fire weather conditions, the accurate anticipation of wildfires on a global scale becomes increasingly crucial for disaster mitigation.","In this study, we utilize SeasFire, a comprehensive global wildfire dataset with climate, vegetation, oceanic indices, and human-related variables, to enable seasonal wildfire forecasting with machine learning.","For the predictive analysis, we train deep learning models with different architectures that capture the spatio-temporal context leading to wildfires.","Our investigation focuses on assessing the effectiveness of these models in predicting the presence of burned areas at varying forecasting time horizons globally, extending up to six months into the future, and on how different spatial or/and temporal context affects the performance of the models.","Our findings demonstrate the great potential of deep learning models in seasonal fire forecasting; longer input time-series leads to more robust predictions across varying forecasting horizons, while integrating spatial information to capture wildfire spatio-temporal dynamics boosts performance.","Finally, our results hint that in order to enhance performance at longer forecasting horizons, a larger receptive field spatially needs to be considered."],"url":"http://arxiv.org/abs/2404.06437v1","category":"cs.CV"}
{"created":"2024-04-09 16:25:07","title":"Quantum Graph Optimization Algorithm","abstract":"Quadratic unconstrained binary optimization (QUBO) tasks are very important in chemistry, finance, job scheduling, and so on, which can be represented using graph structures, with the variables as nodes and the interaction between them as edges. Variational quantum algorithms, especially the Quantum Approximate Optimization Algorithm (QAOA) and its variants, present a promising way, potentially exceeding the capabilities of classical algorithms, for addressing QUBO tasks. However, the possibility of using message-passing machines, inspired by classical graph neural networks, to enhance the power and performance of these quantum algorithms for QUBO tasks was not investigated. This study introduces a novel variational quantum graph optimization algorithm that integrates the message-passing mechanism, which demonstrates significant improvements in performance for solving QUBO problems in terms of resource efficiency and solution precision, compared to QAOA, its variants, and other quantum graph neural networks. Furthermore, in terms of scalability on QUBO tasks, our algorithm shows superior performance compared to QAOA, presenting a substantial advancement in the field of quantum approximate optimization.","sentences":["Quadratic unconstrained binary optimization (QUBO) tasks are very important in chemistry, finance, job scheduling, and so on, which can be represented using graph structures, with the variables as nodes and the interaction between them as edges.","Variational quantum algorithms, especially the Quantum Approximate Optimization Algorithm (QAOA) and its variants, present a promising way, potentially exceeding the capabilities of classical algorithms, for addressing QUBO tasks.","However, the possibility of using message-passing machines, inspired by classical graph neural networks, to enhance the power and performance of these quantum algorithms for QUBO tasks was not investigated.","This study introduces a novel variational quantum graph optimization algorithm that integrates the message-passing mechanism, which demonstrates significant improvements in performance for solving QUBO problems in terms of resource efficiency and solution precision, compared to QAOA, its variants, and other quantum graph neural networks.","Furthermore, in terms of scalability on QUBO tasks, our algorithm shows superior performance compared to QAOA, presenting a substantial advancement in the field of quantum approximate optimization."],"url":"http://arxiv.org/abs/2404.06434v1","category":"quant-ph"}
{"created":"2024-04-09 16:24:56","title":"Alfven Wave Mode Conversion in Neutron Star Magnetospheres: A Semi-analytic Approach","abstract":"We write down the force-free electrodynamics (FFE) equations in dipole coordinates, and solve for normal modes corresponding to Alfv\\'enic perturbations in the magnetosphere of a neutron star. We show that a single Alfv\\'en wave propagating on dipole field lines spontaneously sources a fast magnetosonic (fms) wave at the next order in the perturbation expansion, without needing 3-wave interaction. The frequency of the sourced fms wave is twice the original Alfv\\'en wave frequency, and the wave propagates spherically outwards. The properties of the outgoing fms wave can be computed exactly using the usual devices of classical electrodynamics. We extend the calculation to the closed zone of a rotating neutron star magnetosphere, and show that the Alfv\\'en wave also sources a spherical fms wave but at the same frequency as the primary Alfv\\'en wave.","sentences":["We write down the force-free electrodynamics (FFE) equations in dipole coordinates, and solve for normal modes corresponding to Alfv\\'enic perturbations in the magnetosphere of a neutron star.","We show that a single Alfv\\'en wave propagating on dipole field lines spontaneously sources a fast magnetosonic (fms) wave at the next order in the perturbation expansion, without needing 3-wave interaction.","The frequency of the sourced fms wave is twice the original Alfv\\'en wave frequency, and the wave propagates spherically outwards.","The properties of the outgoing fms wave can be computed exactly using the usual devices of classical electrodynamics.","We extend the calculation to the closed zone of a rotating neutron star magnetosphere, and show that the Alfv\\'en wave also sources a spherical fms wave but at the same frequency as the primary Alfv\\'en wave."],"url":"http://arxiv.org/abs/2404.06431v1","category":"astro-ph.HE"}
{"created":"2024-04-09 16:19:23","title":"Maximality and Cauchy developments of Lorentzian length spaces","abstract":"This article suggests the definition of 'Lorentzian space' weakening the notion of Lorentzian length space just as much that it allows for a functor from the category of causally continuous Lorentzian manifolds to the corresponding category of Lorentzian spaces, and considers three problems in the context of maximal Cauchy developments of Lorentzian length spaces (LLSs): The first is to define pointed Gromov-Hausdorff metrics for spatially and temporally noncompact LLSs, the second to present an explicit non-spacetime example of a maximal vacuum Cauchy development in the LLS category, the third to define canonical representatives for developments. A certain regularity property for geodesics plays a key role in each of the problems.","sentences":["This article suggests the definition of 'Lorentzian space' weakening the notion of Lorentzian length space just as much that it allows for a functor from the category of causally continuous Lorentzian manifolds to the corresponding category of Lorentzian spaces, and considers three problems in the context of maximal Cauchy developments of Lorentzian length spaces (LLSs): The first is to define pointed Gromov-Hausdorff metrics for spatially and temporally noncompact LLSs, the second to present an explicit non-spacetime example of a maximal vacuum Cauchy development in the LLS category, the third to define canonical representatives for developments.","A certain regularity property for geodesics plays a key role in each of the problems."],"url":"http://arxiv.org/abs/2404.06428v1","category":"math.DG"}
{"created":"2024-04-09 16:14:45","title":"Detection of Contact Binary Candidates Observed By TESS Using Autoencoder Neural Network","abstract":"Contact binary may be the progenitor of a red nova that eventually produces a merger event and have a cut-off period around 0.2 days. Therefore, a large number of contact binaries is needed to search for the progenitor of red novae and to study the characteristics of short-period contact binaries. In this paper, we employ the Phoebe program to generate a large number of light curves based on the fundamental parameters of contact binaries. Using these light curves as samples, an autoencoder model is trained, which can reconstruct the light curves of contact binaries very well. When the error between the output light curve from the model and the input light curve is large, it may be due to other types of variable stars. The goodness of fit (R2) between the output light curve from the model and the input light curve is calculated. Based on the thresholds for global goodness of fit (R2), period, range magnitude, and local goodness of fit (R2), a total of 1322 target candidates were obtained.","sentences":["Contact binary may be the progenitor of a red nova that eventually produces a merger event and have a cut-off period around 0.2 days.","Therefore, a large number of contact binaries is needed to search for the progenitor of red novae and to study the characteristics of short-period contact binaries.","In this paper, we employ the Phoebe program to generate a large number of light curves based on the fundamental parameters of contact binaries.","Using these light curves as samples, an autoencoder model is trained, which can reconstruct the light curves of contact binaries very well.","When the error between the output light curve from the model and the input light curve is large, it may be due to other types of variable stars.","The goodness of fit (R2) between the output light curve from the model and the input light curve is calculated.","Based on the thresholds for global goodness of fit (R2), period, range magnitude, and local goodness of fit (R2), a total of 1322 target candidates were obtained."],"url":"http://arxiv.org/abs/2404.06424v1","category":"astro-ph.SR"}
{"created":"2024-04-09 16:06:34","title":"Existence and uniqueness theorems for one class of Hammerstein-type nonlinear integral equations","abstract":"The class of nonlinear integral equations on the positive half-line with a monotone operator of Hammerstein type is studied. With various partial representations of the corresponding kernel and nonlinearity, this class of equations has applications in the dynamic theory of $p$-adic strings, in the kinetic theory of gases, in the theory of radiation transfer and in the mathematical theory of the geographical spread of epidemic diseases. A constructive theorem for the existence of a nontrivial bounded solution is proved. The asymptotic behavior of the constructed solution at infinity is studied. We also prove a theorem for the uniqueness of a solution in the class of nonnegative nontrivial and bounded functions. At the end of the work, specific particular examples of the kernel and nonlinearity of this class of equations are given, which are of independent interest.","sentences":["The class of nonlinear integral equations on the positive half-line with a monotone operator of Hammerstein type is studied.","With various partial representations of the corresponding kernel and nonlinearity, this class of equations has applications in the dynamic theory of $p$-adic strings, in the kinetic theory of gases, in the theory of radiation transfer and in the mathematical theory of the geographical spread of epidemic diseases.","A constructive theorem for the existence of a nontrivial bounded solution is proved.","The asymptotic behavior of the constructed solution at infinity is studied.","We also prove a theorem for the uniqueness of a solution in the class of nonnegative nontrivial and bounded functions.","At the end of the work, specific particular examples of the kernel and nonlinearity of this class of equations are given, which are of independent interest."],"url":"http://arxiv.org/abs/2404.06416v1","category":"math.AP"}
{"created":"2024-04-09 16:02:48","title":"Gravitational wave seismology of charged strange stars in the Cowling approximation: the fluid pulsation modes","abstract":"In this work we study, within the framework of Cowling approximation, the effect of the electric charge on the gravitational wave frequency of fluid oscillation modes of strange quark stars. For this purpose, the dense matter of the stellar fluid is described by the MIT bag model equation of state (EoS), while for the electric charge profile, we consider that the electric charge density is proportional to the energy density. We find that the gravitational wave frequencies change with the increment of electric charge; these effects are more noticeable at higher total mass values. We obtain that the $f$-mode is very sensitive to the change in the electric charge of the star. Furthermore, in the case of the $p_1$ mode, the effect of the electric charge is not very significant. Our results reveal that the study of the fundamental pulsation mode of an electrically charged compact star is very important to distinguish whether compact stars could contain electric charge.","sentences":["In this work we study, within the framework of Cowling approximation, the effect of the electric charge on the gravitational wave frequency of fluid oscillation modes of strange quark stars.","For this purpose, the dense matter of the stellar fluid is described by the MIT bag model equation of state (EoS), while for the electric charge profile, we consider that the electric charge density is proportional to the energy density.","We find that the gravitational wave frequencies change with the increment of electric charge; these effects are more noticeable at higher total mass values.","We obtain that the $f$-mode is very sensitive to the change in the electric charge of the star.","Furthermore, in the case of the $p_1$ mode, the effect of the electric charge is not very significant.","Our results reveal that the study of the fundamental pulsation mode of an electrically charged compact star is very important to distinguish whether compact stars could contain electric charge."],"url":"http://arxiv.org/abs/2404.06412v1","category":"astro-ph.SR"}
{"created":"2024-04-09 15:54:03","title":"Emergent Dynamics in Neural Cellular Automata","abstract":"Neural Cellular Automata (NCA) models are trainable variations of traditional Cellular Automata (CA). Emergent motion in the patterns created by NCA has been successfully applied to synthesize dynamic textures. However, the conditions required for an NCA to display dynamic patterns remain unexplored. Here, we investigate the relationship between the NCA architecture and the emergent dynamics of the trained models. Specifically, we vary the number of channels in the cell state and the number of hidden neurons in the MultiLayer Perceptron (MLP), and draw a relationship between the combination of these two variables and the motion strength between successive frames. Our analysis reveals that the disparity and proportionality between these two variables have a strong correlation with the emergent dynamics in the NCA output. We thus propose a design principle for creating dynamic NCA.","sentences":["Neural Cellular Automata (NCA) models are trainable variations of traditional Cellular Automata (CA).","Emergent motion in the patterns created by NCA has been successfully applied to synthesize dynamic textures.","However, the conditions required for an NCA to display dynamic patterns remain unexplored.","Here, we investigate the relationship between the NCA architecture and the emergent dynamics of the trained models.","Specifically, we vary the number of channels in the cell state and the number of hidden neurons in the MultiLayer Perceptron (MLP), and draw a relationship between the combination of these two variables and the motion strength between successive frames.","Our analysis reveals that the disparity and proportionality between these two variables have a strong correlation with the emergent dynamics in the NCA output.","We thus propose a design principle for creating dynamic NCA."],"url":"http://arxiv.org/abs/2404.06406v1","category":"cs.CV"}
{"created":"2024-04-09 15:46:00","title":"Dynamic Deep Learning Based Super-Resolution For The Shallow Water Equations","abstract":"Using the nonlinear shallow water equations as benchmark, we demonstrate that a simulation with the ICON-O ocean model with a 20km resolution that is frequently corrected by a U-net-type neural network can achieve discretization errors of a simulation with 10km resolution. The network, originally developed for image-based super-resolution in post-processing, is trained to compute the difference between solutions on both meshes and is used to correct the coarse mesh every 12h. Our setup is the Galewsky test case, modeling transition of a barotropic instability into turbulent flow. We show that the ML-corrected coarse resolution run correctly maintains a balance flow and captures the transition to turbulence in line with the higher resolution simulation. After 8 day of simulation, the $L_2$-error of the corrected run is similar to a simulation run on the finer mesh. While mass is conserved in the corrected runs, we observe some spurious generation of kinetic energy.","sentences":["Using the nonlinear shallow water equations as benchmark, we demonstrate that a simulation with the ICON-O ocean model with a 20km resolution that is frequently corrected by a U-net-type neural network can achieve discretization errors of a simulation with 10km resolution.","The network, originally developed for image-based super-resolution in post-processing, is trained to compute the difference between solutions on both meshes and is used to correct the coarse mesh every 12h.","Our setup is the Galewsky test case, modeling transition of a barotropic instability into turbulent flow.","We show that the ML-corrected coarse resolution run correctly maintains a balance flow and captures the transition to turbulence in line with the higher resolution simulation.","After 8 day of simulation, the $L_2$-error of the corrected run is similar to a simulation run on the finer mesh.","While mass is conserved in the corrected runs, we observe some spurious generation of kinetic energy."],"url":"http://arxiv.org/abs/2404.06400v1","category":"cs.LG"}
{"created":"2024-04-09 15:27:58","title":"Master symmetries of non-linear systems","abstract":"We explore new symmetries in two-component third-order Burgers' type systems in (1+1)-dimension using Wang's O-scheme. We also find a master symmetry for a (2+1)-dimensional Davey-Stewartson type system. These results shed light on the behavior of these equations and help us understand their integrability properties. Our approach offers a practical method for identifying symmetries, contributing to the study of integrable systems in mathematics and physics.","sentences":["We explore new symmetries in two-component third-order Burgers' type systems in (1+1)-dimension using Wang's O-scheme.","We also find a master symmetry for a (2+1)-dimensional Davey-Stewartson type system.","These results shed light on the behavior of these equations and help us understand their integrability properties.","Our approach offers a practical method for identifying symmetries, contributing to the study of integrable systems in mathematics and physics."],"url":"http://arxiv.org/abs/2404.06384v1","category":"nlin.SI"}
{"created":"2024-04-09 15:23:35","title":"Causal third-order viscous hydrodynamics within relaxation-time approximation","abstract":"In the present work, we derive a linearly stable and causal theory of relativistic third-order viscous hydrodynamics from the Boltzmann equation with relaxation-time approximation. We employ viscous correction to the distribution function obtained using a Chapman-Enskog like iterative solution of the Boltzmann equation. Our derivation highlights the necessity of incorporating a new dynamical degree of freedom, specifically an irreducible tensors of rank three, within this framework. This differs from the recent formulation of causal third-order theory from the method of moments which requires two dynamical degrees of freedom: an irreducible third-rank and a fourth-rank tensor. We verify the linear stability and causality of the proposed formulation by examining perturbations around a global equilibrium state.","sentences":["In the present work, we derive a linearly stable and causal theory of relativistic third-order viscous hydrodynamics from the Boltzmann equation with relaxation-time approximation.","We employ viscous correction to the distribution function obtained using a Chapman-Enskog like iterative solution of the Boltzmann equation.","Our derivation highlights the necessity of incorporating a new dynamical degree of freedom, specifically an irreducible tensors of rank three, within this framework.","This differs from the recent formulation of causal third-order theory from the method of moments which requires two dynamical degrees of freedom: an irreducible third-rank and a fourth-rank tensor.","We verify the linear stability and causality of the proposed formulation by examining perturbations around a global equilibrium state."],"url":"http://arxiv.org/abs/2404.06381v1","category":"hep-ph"}
{"created":"2024-04-09 15:11:40","title":"ABP estimate and comparison principle for cone degenerate quasilinear elliptic equations","abstract":"In this paper, we study the cone degenerate quasilinear elliptic equations. We provide the existence of the viscosity solutions by proving Alexandrov-Bakelman-Pucci and H\\\"older estimates. Further more, we give the comparison principle by an equivalent transformation.","sentences":["In this paper, we study the cone degenerate quasilinear elliptic equations.","We provide the existence of the viscosity solutions by proving Alexandrov-Bakelman-Pucci and H\\\"older estimates.","Further more, we give the comparison principle by an equivalent transformation."],"url":"http://arxiv.org/abs/2404.06372v1","category":"math.AP"}
{"created":"2024-04-09 15:03:10","title":"Did Louis de Broglie miss the discovery of the Schr\u00f6dinger equation?","abstract":"In this note, we discuss a historical point regarding Schr\\\"odinger's discovery of the famous quantum wave equation in 1926 following de Broglie's fundamental works published in 1923-1925 regarding the introduction of matter waves. Drawing on the work of historians and personal analysis, we show that de Broglie was very close to the discovery of the Schr\\\"odinger equation (at least for the stationary one-electron problem).","sentences":["In this note, we discuss a historical point regarding Schr\\\"odinger's discovery of the famous quantum wave equation in 1926 following de Broglie's fundamental works published in 1923-1925 regarding the introduction of matter waves.","Drawing on the work of historians and personal analysis, we show that de Broglie was very close to the discovery of the Schr\\\"odinger equation (at least for the stationary one-electron problem)."],"url":"http://arxiv.org/abs/2404.06366v1","category":"physics.hist-ph"}
{"created":"2024-04-09 14:55:48","title":"Meaningfulness and Genericity in a Subsuming Framework","abstract":"This paper studies the notion of meaningfulness for a unifying framework called dBang-calculus, which subsumes both call-by-name (dCbN) and call-by-value (dCbV). We first characterize meaningfulness in dBang by means of typability and inhabitation in an associated non-idempotent intersection type system previously proposed in the literature. We validate the proposed notion of meaningfulness by showing two properties (1) consistency of the theory $\\mathcal{H}$ equating meaningless terms and (2) genericity, stating that meaningless subterms have no bearing on the significance of meaningful terms. The theory $\\mathcal{H}$ is also shown to have a unique consistent and maximal extension. Last but not least, we show that the notions of meaningfulness and genericity in the literature for dCbN and dCbV are subsumed by the respectively ones proposed here for the dBang-calculus.","sentences":["This paper studies the notion of meaningfulness for a unifying framework called dBang-calculus, which subsumes both call-by-name (dCbN) and call-by-value (dCbV).","We first characterize meaningfulness in dBang by means of typability and inhabitation in an associated non-idempotent intersection type system previously proposed in the literature.","We validate the proposed notion of meaningfulness by showing two properties (1) consistency of the theory $\\mathcal{H}$ equating meaningless terms and (2) genericity, stating that meaningless subterms have no bearing on the significance of meaningful terms.","The theory $\\mathcal{H}$ is also shown to have a unique consistent and maximal extension.","Last but not least, we show that the notions of meaningfulness and genericity in the literature for dCbN and dCbV are subsumed by the respectively ones proposed here for the dBang-calculus."],"url":"http://arxiv.org/abs/2404.06361v1","category":"cs.LO"}
{"created":"2024-04-09 14:44:58","title":"On harmonic maps from the complex plane to hyperbolic 3-space","abstract":"For any twisted ideal polygon in $\\mathbb{H}^3$, we construct a harmonic map from $\\mathbb{C}$ to $\\mathbb{H}^3$ with a polynomial Hopf differential, that is asymptotic to the given polygon, and is a bounded distance from a pleated plane. Our proof uses the harmonic map heat flow. We also show that such a harmonic map is unique once we prescribe the principal part of its Hopf differential.","sentences":["For any twisted ideal polygon in $\\mathbb{H}^3$, we construct a harmonic map from $\\mathbb{C}$ to $\\mathbb{H}^3$ with a polynomial Hopf differential, that is asymptotic to the given polygon, and is a bounded distance from a pleated plane.","Our proof uses the harmonic map heat flow.","We also show that such a harmonic map is unique once we prescribe the principal part of its Hopf differential."],"url":"http://arxiv.org/abs/2404.06354v1","category":"math.DG"}
{"created":"2024-04-09 14:33:03","title":"Synaptogen: A cross-domain generative device model for large-scale neuromorphic circuit design","abstract":"We present a fast generative modeling approach for resistive memories that reproduces the complex statistical properties of real-world devices. To enable efficient modeling of analog circuits, the model is implemented in Verilog-A. By training on extensive measurement data of integrated 1T1R arrays (6,000 cycles of 512 devices), an autoregressive stochastic process accurately accounts for the cross-correlations between the switching parameters, while non-linear transformations ensure agreement with both cycle-to-cycle (C2C) and device-to-device (D2D) variability. Benchmarks show that this statistically comprehensive model achieves read/write throughputs exceeding those of even highly simplified and deterministic compact models.","sentences":["We present a fast generative modeling approach for resistive memories that reproduces the complex statistical properties of real-world devices.","To enable efficient modeling of analog circuits, the model is implemented in Verilog-A. By training on extensive measurement data of integrated 1T1R arrays (6,000 cycles of 512 devices), an autoregressive stochastic process accurately accounts for the cross-correlations between the switching parameters, while non-linear transformations ensure agreement with both cycle-to-cycle (C2C) and device-to-device (D2D) variability.","Benchmarks show that this statistically comprehensive model achieves read/write throughputs exceeding those of even highly simplified and deterministic compact models."],"url":"http://arxiv.org/abs/2404.06344v1","category":"cs.NE"}
{"created":"2024-04-09 14:32:39","title":"Onboard Processing of Hyperspectral Imagery: Deep Learning Advancements, Methodologies, Challenges, and Emerging Trends","abstract":"Recent advancements in deep learning techniques have spurred considerable interest in their application to hyperspectral imagery processing. This paper provides a comprehensive review of the latest developments in this field, focusing on methodologies, challenges, and emerging trends. Deep learning architectures such as Convolutional Neural Networks (CNNs), Autoencoders, Deep Belief Networks (DBNs), Generative Adversarial Networks (GANs), and Recurrent Neural Networks (RNNs) are examined for their suitability in processing hyperspectral data. Key challenges, including limited training data and computational constraints, are identified, along with strategies such as data augmentation and noise reduction using GANs. The paper discusses the efficacy of different network architectures, highlighting the advantages of lightweight CNN models and 1D CNNs for onboard processing. Moreover, the potential of hardware accelerators, particularly Field Programmable Gate Arrays (FPGAs), for enhancing processing efficiency is explored. The review concludes with insights into ongoing research trends, including the integration of deep learning techniques into Earth observation missions such as the CHIME mission, and emphasizes the need for further exploration and refinement of deep learning methodologies to address the evolving demands of hyperspectral image processing.","sentences":["Recent advancements in deep learning techniques have spurred considerable interest in their application to hyperspectral imagery processing.","This paper provides a comprehensive review of the latest developments in this field, focusing on methodologies, challenges, and emerging trends.","Deep learning architectures such as Convolutional Neural Networks (CNNs), Autoencoders, Deep Belief Networks (DBNs), Generative Adversarial Networks (GANs), and Recurrent Neural Networks (RNNs) are examined for their suitability in processing hyperspectral data.","Key challenges, including limited training data and computational constraints, are identified, along with strategies such as data augmentation and noise reduction using GANs.","The paper discusses the efficacy of different network architectures, highlighting the advantages of lightweight CNN models and 1D CNNs for onboard processing.","Moreover, the potential of hardware accelerators, particularly Field Programmable Gate Arrays (FPGAs), for enhancing processing efficiency is explored.","The review concludes with insights into ongoing research trends, including the integration of deep learning techniques into Earth observation missions such as the CHIME mission, and emphasizes the need for further exploration and refinement of deep learning methodologies to address the evolving demands of hyperspectral image processing."],"url":"http://arxiv.org/abs/2404.06526v1","category":"eess.IV"}
{"created":"2024-04-09 14:18:05","title":"On the 576-fold periodicity of the spectrum SQFT: The proof of the lower bound via the Anderson duality pairing","abstract":"We are aimed at giving a differential geometric, and accordingly physical, explanation of the 576-periodicity of TMF. In this paper, we settle the problem of giving the lower bound 576. We formulate the problem as follows: we assume a spectrum $\\mathrm{SQFT}$ with some conditions, suggest from physical considerations about the classifying spectrum for two-dimensional $\\mathcal{N}=(0,1)$-supersymmetric quantum field theories, and show that the periodicity of $\\mathrm{SQFT}$ is no less than 576. The main tool for the proof is the analogue of the Anderson duality pairing introduced by the second-named author and Tachikawa. We do not rely on the Segal-Stolz-Teichner conjecture, so in particular we do not use any comparison map with TMF.","sentences":["We are aimed at giving a differential geometric, and accordingly physical, explanation of the 576-periodicity of TMF.","In this paper, we settle the problem of giving the lower bound 576.","We formulate the problem as follows: we assume a spectrum $\\mathrm{SQFT}$ with some conditions, suggest from physical considerations about the classifying spectrum for two-dimensional $\\mathcal{N}=(0,1)$-supersymmetric quantum field theories, and show that the periodicity of $\\mathrm{SQFT}$ is no less than 576.","The main tool for the proof is the analogue of the Anderson duality pairing introduced by the second-named author and Tachikawa.","We do not rely on the Segal-Stolz-Teichner conjecture, so in particular we do not use any comparison map with TMF."],"url":"http://arxiv.org/abs/2404.06333v1","category":"math.AT"}
{"created":"2024-04-09 14:09:14","title":"Sparse space-time resolvent analysis for statistically-stationary and time-varying flows","abstract":"Resolvent analysis provides a framework to predict coherent spatio-temporal structures of largest linear energy amplification, through a singular value decomposition (SVD) of the resolvent operator, obtained by linearizing the Navier--Stokes equations about a known turbulent mean velocity profile. Resolvent analysis utilizes a Fourier decomposition in time, which has thus-far limited its application to statistically-stationary or time-periodic flows. This work develops a variant of resolvent analysis applicable to time-evolving flows, and proposes a variant that identifies spatio-temporally sparse structures, applicable to either stationary or time-varying mean velocity profiles. Spatio-temporal resolvent analysis is formulated through the incorporation of the temporal dimension to the numerical domain via a discrete time-differentiation operator. Sparsity (which manifests in localisation) is achieved through the addition of an L1-norm penalisation term to the optimisation associated with the SVD. This modified optimization problem can be formulated as a nonlinear eigenproblem, and solved via an inverse power method. We first demonstrate the implementation of the sparse analysis on statistically-stationary turbulent channel flow, and demonstrate that the sparse variant can identify aspects of the physics not directly evident from standard resolvent analysis. This is followed by applying the sparse space-time formulation on systems that are time-varying: a time-periodic turbulent Stokes boundary layer, and then a turbulent channel flow with a sudden change in pressure gradient. We present results demonstrating how the sparsity-promoting variant can either change the quantitative structure of the leading space-time modes to increase their sparsity, or identify entirely different linear amplification mechanisms compared to non-sparse resolvent analysis.","sentences":["Resolvent analysis provides a framework to predict coherent spatio-temporal structures of largest linear energy amplification, through a singular value decomposition (SVD) of the resolvent operator, obtained by linearizing the Navier--Stokes equations about a known turbulent mean velocity profile.","Resolvent analysis utilizes a Fourier decomposition in time, which has thus-far limited its application to statistically-stationary or time-periodic flows.","This work develops a variant of resolvent analysis applicable to time-evolving flows, and proposes a variant that identifies spatio-temporally sparse structures, applicable to either stationary or time-varying mean velocity profiles.","Spatio-temporal resolvent analysis is formulated through the incorporation of the temporal dimension to the numerical domain via a discrete time-differentiation operator.","Sparsity (which manifests in localisation) is achieved through the addition of an L1-norm penalisation term to the optimisation associated with the SVD.","This modified optimization problem can be formulated as a nonlinear eigenproblem, and solved via an inverse power method.","We first demonstrate the implementation of the sparse analysis on statistically-stationary turbulent channel flow, and demonstrate that the sparse variant can identify aspects of the physics not directly evident from standard resolvent analysis.","This is followed by applying the sparse space-time formulation on systems that are time-varying: a time-periodic turbulent Stokes boundary layer, and then a turbulent channel flow with a sudden change in pressure gradient.","We present results demonstrating how the sparsity-promoting variant can either change the quantitative structure of the leading space-time modes to increase their sparsity, or identify entirely different linear amplification mechanisms compared to non-sparse resolvent analysis."],"url":"http://arxiv.org/abs/2404.06331v1","category":"physics.flu-dyn"}
{"created":"2024-04-09 13:58:32","title":"An Overview of Absolute Value Equations: From Theory to Solution Methods and Challenges","abstract":"This paper provides a thorough exploration of the absolute value equations $Ax-|x|=b$, a seemingly straightforward concept that has gained heightened attention in recent years. It is an NP-hard and nondifferentiable problem and equivalent with the standard linear complementarity problem. Offering a comprehensive review of existing literature, the study delves into theorems concerning the existence and nonexistence of solutions to the absolute value equations, along with numerical methods for effectively addressing this complex equation. Going beyond conventional approaches, the paper investigates strategies for obtaining solutions with minimal norms, techniques for correcting infeasible systems, and other pertinent topics. By pinpointing challenging issues and emphasizing open problems, this paper serves as a valuable guide for shaping the future research trajectory in this dynamic and multifaceted field.","sentences":["This paper provides a thorough exploration of the absolute value equations $Ax-|x|=b$, a seemingly straightforward concept that has gained heightened attention in recent years.","It is an NP-hard and nondifferentiable problem and equivalent with the standard linear complementarity problem.","Offering a comprehensive review of existing literature, the study delves into theorems concerning the existence and nonexistence of solutions to the absolute value equations, along with numerical methods for effectively addressing this complex equation.","Going beyond conventional approaches, the paper investigates strategies for obtaining solutions with minimal norms, techniques for correcting infeasible systems, and other pertinent topics.","By pinpointing challenging issues and emphasizing open problems, this paper serves as a valuable guide for shaping the future research trajectory in this dynamic and multifaceted field."],"url":"http://arxiv.org/abs/2404.06319v1","category":"math.OC"}
{"created":"2024-04-09 13:56:20","title":"Variational Optimization for Constructing Inverse Potentials of Proton-Proton Scattering: A Phase Function Method Study","abstract":"Background: The phase-shift analysis for proton-proton scattering has been studied by various research groups using the realistic potentials to be comprised of various internal interactions based on an exchange of pions and mesons, involving a large number of parameters. Purpose: The goal of the research is to construct inverse potentials for various l-channels of proton-proton (pp) elastic scattering using the 3-parameter Morse function in combination with atomic Hulthen by utilizing the phase function method and variational optimization technique. Methodology: The implementation of variational optimization begins with randomly assigning initial values to the Morse model parameters. Utilizing the Morse + Hulthen potential as input, the phase equations for various l-channels are numerically solved using the RK-5 method for obtaining the simulated Scattering Phase Shift (SPS). Mean Squared error between simulated and expected SPS has been chosen as the cost function. Variational optimization proceeds iteratively by adjusting potential parameters and re-evaluating the cost function until convergence is achieved. Results: All the obtained scattering phase shifts for various l-channels have been found to converge to a mean squared error <= 0.3. The computed cross-sections matched the experimental ones to less than 1% for energies up to 25 MeV. The scattering parameters are also found to closely match the experimental data. Conclusion: The inverse potentials constructed for various l-channels using Morse + atomic Hulthen are on par with the currently available high-precision realistic potentials.","sentences":["Background: The phase-shift analysis for proton-proton scattering has been studied by various research groups using the realistic potentials to be comprised of various internal interactions based on an exchange of pions and mesons, involving a large number of parameters.","Purpose: The goal of the research is to construct inverse potentials for various l-channels of proton-proton (pp) elastic scattering using the 3-parameter Morse function in combination with atomic Hulthen by utilizing the phase function method and variational optimization technique.","Methodology:","The implementation of variational optimization begins with randomly assigning initial values to the Morse model parameters.","Utilizing the Morse + Hulthen potential as input, the phase equations for various l-channels are numerically solved using the RK-5 method for obtaining the simulated Scattering Phase Shift (SPS).","Mean Squared error between simulated and expected SPS has been chosen as the cost function.","Variational optimization proceeds iteratively by adjusting potential parameters and re-evaluating the cost function until convergence is achieved.","Results: All the obtained scattering phase shifts for various l-channels have been found to converge to a mean squared error <= 0.3.","The computed cross-sections matched the experimental ones to less than 1% for energies up to 25 MeV.","The scattering parameters are also found to closely match the experimental data.","Conclusion: The inverse potentials constructed for various l-channels using Morse + atomic Hulthen are on par with the currently available high-precision realistic potentials."],"url":"http://arxiv.org/abs/2404.06318v1","category":"nucl-th"}
{"created":"2024-04-09 13:48:53","title":"Qiskit-Torch-Module: Fast Prototyping of Quantum Neural Networks","abstract":"Quantum computer simulation software is an integral tool for the research efforts in the quantum computing community. An important aspect is the efficiency of respective frameworks, especially for training variational quantum algorithms. Focusing on the widely used Qiskit software environment, we develop the qiskit-torch-module. It improves runtime performance by two orders of magnitude over comparable libraries, while facilitating low-overhead integration with existing codebases. Moreover, the framework provides advanced tools for integrating quantum neural networks with PyTorch. The pipeline is tailored for single-machine compute systems, which constitute a widely employed setup in day-to-day research efforts.","sentences":["Quantum computer simulation software is an integral tool for the research efforts in the quantum computing community.","An important aspect is the efficiency of respective frameworks, especially for training variational quantum algorithms.","Focusing on the widely used Qiskit software environment, we develop the qiskit-torch-module.","It improves runtime performance by two orders of magnitude over comparable libraries, while facilitating low-overhead integration with existing codebases.","Moreover, the framework provides advanced tools for integrating quantum neural networks with PyTorch.","The pipeline is tailored for single-machine compute systems, which constitute a widely employed setup in day-to-day research efforts."],"url":"http://arxiv.org/abs/2404.06314v1","category":"quant-ph"}
{"created":"2024-04-09 13:42:42","title":"Dynamical dark energy in light of cosmic distance measurements II: a study using current observations","abstract":"We extract key information of dark energy from current observations of BAO, OHD and $H_0$, and find hints of dynamical behaviour of dark energy. In particular, a dynamical dark energy model whose equation of state crosses $-1$ is favoured by observations. We also find that the Universe has started accelerating at a lower redshift than expected.","sentences":["We extract key information of dark energy from current observations of BAO, OHD and $H_0$, and find hints of dynamical behaviour of dark energy.","In particular, a dynamical dark energy model whose equation of state crosses $-1$ is favoured by observations.","We also find that the Universe has started accelerating at a lower redshift than expected."],"url":"http://arxiv.org/abs/2404.06310v2","category":"astro-ph.CO"}
{"created":"2024-04-09 13:39:37","title":"Audio-Visual Generalized Zero-Shot Learning using Pre-Trained Large Multi-Modal Models","abstract":"Audio-visual zero-shot learning methods commonly build on features extracted from pre-trained models, e.g. video or audio classification models. However, existing benchmarks predate the popularization of large multi-modal models, such as CLIP and CLAP. In this work, we explore such large pre-trained models to obtain features, i.e. CLIP for visual features, and CLAP for audio features. Furthermore, the CLIP and CLAP text encoders provide class label embeddings which are combined to boost the performance of the system. We propose a simple yet effective model that only relies on feed-forward neural networks, exploiting the strong generalization capabilities of the new audio, visual and textual features. Our framework achieves state-of-the-art performance on VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL with our new features. Code and data available at: https://github.com/dkurzend/ClipClap-GZSL.","sentences":["Audio-visual zero-shot learning methods commonly build on features extracted from pre-trained models, e.g. video or audio classification models.","However, existing benchmarks predate the popularization of large multi-modal models, such as CLIP and CLAP.","In this work, we explore such large pre-trained models to obtain features, i.e. CLIP for visual features, and CLAP for audio features.","Furthermore, the CLIP and CLAP text encoders provide class label embeddings which are combined to boost the performance of the system.","We propose a simple yet effective model that only relies on feed-forward neural networks, exploiting the strong generalization capabilities of the new audio, visual and textual features.","Our framework achieves state-of-the-art performance on VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL with our new features.","Code and data available at: https://github.com/dkurzend/ClipClap-GZSL."],"url":"http://arxiv.org/abs/2404.06309v1","category":"cs.CV"}
{"created":"2024-04-09 13:34:15","title":"Dynamical dark energy in light of cosmic distance measurements I: a demonstration using simulated datasets","abstract":"We develop methods to extract key dark energy information from cosmic distance measurements including the BAO scales and supernovae luminosity distances. Demonstrated using simulated datasets of the complete DESI, LSST and Roman surveys designed for BAO and SNe distance measurements, we show that using our method, the dynamical behaviour of the energy, pressure, equation of state (with its time derivative) of dark energy and the cosmic deceleration function can all be accurately recovered from high-quality data, which allows for robust diagnostic tests for dark energy models.","sentences":["We develop methods to extract key dark energy information from cosmic distance measurements including the BAO scales and supernovae luminosity distances.","Demonstrated using simulated datasets of the complete DESI, LSST and Roman surveys designed for BAO and SNe distance measurements, we show that using our method, the dynamical behaviour of the energy, pressure, equation of state (with its time derivative) of dark energy and the cosmic deceleration function can all be accurately recovered from high-quality data, which allows for robust diagnostic tests for dark energy models."],"url":"http://arxiv.org/abs/2404.06303v2","category":"astro-ph.CO"}
{"created":"2024-04-09 13:27:18","title":"Models of $2$-nondegenerate CR hypersurface in $\\mathbb{C}^N$","abstract":"We show that every point in a uniformly $2$-nondegenerate CR hypersurface is canonically associated with a model $2$-nondegenerate structure. The $2$-nondegenerate models are basic CR invariants playing the same fundamental role as quadrics do in the Levi nondegenerate case. We characterize all $2$-nondegenerate models and show that the moduli space of such hypersurfaces in $\\mathbb{C}^N$ is infinite dimensional for each $N>3$. We derive a normal form for these models' defining equations that is unique up to an action of a finite dimensional Lie group. We generalize recently introduced CR invariants termed modified symbols, and show how to compute these intrinsically defined invariants from a model's defining equation. We show that these models automatically possess infinitesimal symmetries spanning a complement to their Levi kernel and derive explicit formulas for them.","sentences":["We show that every point in a uniformly $2$-nondegenerate CR hypersurface is canonically associated with a model $2$-nondegenerate structure.","The $2$-nondegenerate models are basic CR invariants playing the same fundamental role as quadrics do in the Levi nondegenerate case.","We characterize all $2$-nondegenerate models and show that the moduli space of such hypersurfaces in $\\mathbb{C}^N$ is infinite dimensional for each $N>3$. We derive a normal form for these models' defining equations that is unique up to an action of a finite dimensional Lie group.","We generalize recently introduced CR invariants termed modified symbols, and show how to compute these intrinsically defined invariants from a model's defining equation.","We show that these models automatically possess infinitesimal symmetries spanning a complement to their Levi kernel and derive explicit formulas for them."],"url":"http://arxiv.org/abs/2404.06525v1","category":"math.CV"}
{"created":"2024-04-09 13:22:55","title":"Calculation of toroidal Alfv\u00e9n eigenmode mode structure in general axisymmetric toroidal geometry","abstract":"A workflow is developed based on the ideal MHD model to investigate the linear physics of various Alfv\\'en eigenmodes in general axisymmetric toroidal geometry, by solving the coupled shear Alfv\\'en wave (SAW) and ion sound wave (ISW) equations in ballooning space. The model equations are solved by the FALCON code in the singular layer, and the corresponding solutions are then taken as the boundary conditions for calculating parallel mode structures in the whole ballooning space. As an application of the code, the frequencies and mode structures of toroidal Alfv\\'en eigenmode (TAE) are calculated in the reference equilibria of the Divertor Tokamak Test facility (DTT) with positive and negative triangularities, respectively. By properly handling the boundary conditions, we demonstrate finite TAE damping due to coupling with the local acoustic continuum, and find that the damping rate is small for typical plasma parameters.","sentences":["A workflow is developed based on the ideal MHD model to investigate the linear physics of various Alfv\\'en eigenmodes in general axisymmetric toroidal geometry, by solving the coupled shear Alfv\\'en wave (SAW) and ion sound wave (ISW) equations in ballooning space.","The model equations are solved by the FALCON code in the singular layer, and the corresponding solutions are then taken as the boundary conditions for calculating parallel mode structures in the whole ballooning space.","As an application of the code, the frequencies and mode structures of toroidal Alfv\\'en eigenmode (TAE) are calculated in the reference equilibria of the Divertor Tokamak Test facility (DTT) with positive and negative triangularities, respectively.","By properly handling the boundary conditions, we demonstrate finite TAE damping due to coupling with the local acoustic continuum, and find that the damping rate is small for typical plasma parameters."],"url":"http://arxiv.org/abs/2404.06296v1","category":"physics.plasm-ph"}
{"created":"2024-04-09 12:51:00","title":"Blow-up of classical solutions of quasilinear wave equations in one space dimension","abstract":"This paper studies the upper bound of the lifespan of classical solutions of the initial value problems for one dimensional wave equations with quasilinear terms of space-, or time-derivatives of the unknown function. The results are same as those of the semilinear case. But it is quite meaningful to consider this kind of problems for the purpose to cover the optimality of the general theory for nonlinear wave equations by many model equations as far as possible.","sentences":["This paper studies the upper bound of the lifespan of classical solutions of the initial value problems for one dimensional wave equations with quasilinear terms of space-, or time-derivatives of the unknown function.","The results are same as those of the semilinear case.","But it is quite meaningful to consider this kind of problems for the purpose to cover the optimality of the general theory for nonlinear wave equations by many model equations as far as possible."],"url":"http://arxiv.org/abs/2404.06274v1","category":"math.AP"}
{"created":"2024-04-09 12:35:12","title":"A Constant self-consistent scattering lifetime in superconducting Strontium Ruthenate","abstract":"In this numerical work, we find a self-consistent constant scattering superconducting lifetime for two different values of the disorder parameters, the inverse atomic strength, and the stoichiometric impurity in the triplet paired unconventional super-conductor strontium ruthenate. This finding is relevant for experimentalists given that the expressions for the ultrasound attenuation and the electronic thermal conductivity depend on the superconducting scattering lifetime, and a constant lifetime fits well nonequilibrium experimental data. Henceforth, this work helps experimentalists in their interpretation of the acquired data. Additionally, we encountered tiny imaginary parts of the self-energy that resembles the Miyake-Narikiyo tiny gap out-side the unitary elastic scattering limit, and below the threshold zero gap value of 1.0 meV.","sentences":["In this numerical work, we find a self-consistent constant scattering superconducting lifetime for two different values of the disorder parameters, the inverse atomic strength, and the stoichiometric impurity in the triplet paired unconventional super-conductor strontium ruthenate.","This finding is relevant for experimentalists given that the expressions for the ultrasound attenuation and the electronic thermal conductivity depend on the superconducting scattering lifetime, and a constant lifetime fits well nonequilibrium experimental data.","Henceforth, this work helps experimentalists in their interpretation of the acquired data.","Additionally, we encountered tiny imaginary parts of the self-energy that resembles the Miyake-Narikiyo tiny gap out-side the unitary elastic scattering limit, and below the threshold zero gap value of 1.0 meV."],"url":"http://arxiv.org/abs/2404.06262v1","category":"cond-mat.supr-con"}
{"created":"2024-04-09 12:29:56","title":"DDPG-E2E: A Novel Policy Gradient Approach for End-to-End Communication Systems","abstract":"The End-to-end (E2E) learning-based approach has great potential to reshape the existing communication systems by replacing the transceivers with deep neural networks. To this end, the E2E learning approach needs to assume the availability of prior channel information to mathematically formulate a differentiable channel layer for the backpropagation (BP) of the error gradients, thereby jointly optimizing the transmitter and the receiver. However, accurate and instantaneous channel state information is hardly obtained in practical wireless communication scenarios. Moreover, the existing E2E learning-based solutions exhibit limited performance in data transmissions with large block lengths. In this article, these practical issues are addressed by our proposed deep deterministic policy gradient-based E2E communication system. In particular, the proposed solution utilizes a reward feedback mechanism to train both the transmitter and the receiver, which alleviates the information loss of error gradients during BP. In addition, a convolutional neural network (CNN)-based architecture is developed to mitigate the curse of dimensionality problem when transmitting messages with large block lengths. Extensive simulations then demonstrate that our proposed solution can not only jointly train the transmitter and the receiver simultaneously without requiring the prior channel knowledge but also can obtain significant performance improvement on block error rate compared to state-of-the-art solutions.","sentences":["The End-to-end (E2E) learning-based approach has great potential to reshape the existing communication systems by replacing the transceivers with deep neural networks.","To this end, the E2E learning approach needs to assume the availability of prior channel information to mathematically formulate a differentiable channel layer for the backpropagation (BP) of the error gradients, thereby jointly optimizing the transmitter and the receiver.","However, accurate and instantaneous channel state information is hardly obtained in practical wireless communication scenarios.","Moreover, the existing E2E learning-based solutions exhibit limited performance in data transmissions with large block lengths.","In this article, these practical issues are addressed by our proposed deep deterministic policy gradient-based E2E communication system.","In particular, the proposed solution utilizes a reward feedback mechanism to train both the transmitter and the receiver, which alleviates the information loss of error gradients during BP.","In addition, a convolutional neural network (CNN)-based architecture is developed to mitigate the curse of dimensionality problem when transmitting messages with large block lengths.","Extensive simulations then demonstrate that our proposed solution can not only jointly train the transmitter and the receiver simultaneously without requiring the prior channel knowledge but also can obtain significant performance improvement on block error rate compared to state-of-the-art solutions."],"url":"http://arxiv.org/abs/2404.06257v1","category":"cs.NI"}
{"created":"2024-04-09 12:20:46","title":"On checking $\\mathrm{L}^p$-admissibility for parabolic control systems","abstract":"In this note we discuss the difficulty of verifying $\\mathrm{L}^p$-admissibility for $p\\neq 2$ -- that even manifests in the presence of a self-adjoint semigroup generator on a Hilbert space -- and survey tests for $\\mathrm{L}^p$-admissibility of given control operators. These tests are obtained by virtue of either mapping properties of boundary trace operators, yielding a characterization of admissibility via abstract interpolation spaces; or through Laplace--Carleson embeddings, slightly extending results from Jacob, Partington and Pott to a class of systems which are not necessarily diagonal with respect to sequence spaces. Special focus is laid on illustrating the theory by means of examples based on the heat equation on various domains.","sentences":["In this note we discuss the difficulty of verifying $\\mathrm{L}^p$-admissibility for $p\\neq 2$ -- that even manifests in the presence of a self-adjoint semigroup generator on a Hilbert space -- and survey tests for $\\mathrm{L}^p$-admissibility of given control operators.","These tests are obtained by virtue of either mapping properties of boundary trace operators, yielding a characterization of admissibility via abstract interpolation spaces; or through Laplace--Carleson embeddings, slightly extending results from Jacob, Partington and Pott to a class of systems which are not necessarily diagonal with respect to sequence spaces.","Special focus is laid on illustrating the theory by means of examples based on the heat equation on various domains."],"url":"http://arxiv.org/abs/2404.06250v1","category":"math.OC"}
{"created":"2024-04-09 11:42:32","title":"Aggressive or Imperceptible, or Both: Network Pruning Assisted Hybrid Byzantines in Federated Learning","abstract":"Federated learning (FL) has been introduced to enable a large number of clients, possibly mobile devices, to collaborate on generating a generalized machine learning model thanks to utilizing a larger number of local samples without sharing to offer certain privacy to collaborating clients. However, due to the participation of a large number of clients, it is often difficult to profile and verify each client, which leads to a security threat that malicious participants may hamper the accuracy of the trained model by conveying poisoned models during the training. Hence, the aggregation framework at the parameter server also needs to minimize the detrimental effects of these malicious clients. A plethora of attack and defence strategies have been analyzed in the literature. However, often the Byzantine problem is analyzed solely from the outlier detection perspective, being oblivious to the topology of neural networks (NNs).   In the scope of this work, we argue that by extracting certain side information specific to the NN topology, one can design stronger attacks. Hence, inspired by the sparse neural networks, we introduce a hybrid sparse Byzantine attack that is composed of two parts: one exhibiting a sparse nature and attacking only certain NN locations with higher sensitivity, and the other being more silent but accumulating over time, where each ideally targets a different type of defence mechanism, and together they form a strong but imperceptible attack. Finally, we show through extensive simulations that the proposed hybrid Byzantine attack is effective against 8 different defence methods.","sentences":["Federated learning (FL) has been introduced to enable a large number of clients, possibly mobile devices, to collaborate on generating a generalized machine learning model thanks to utilizing a larger number of local samples without sharing to offer certain privacy to collaborating clients.","However, due to the participation of a large number of clients, it is often difficult to profile and verify each client, which leads to a security threat that malicious participants may hamper the accuracy of the trained model by conveying poisoned models during the training.","Hence, the aggregation framework at the parameter server also needs to minimize the detrimental effects of these malicious clients.","A plethora of attack and defence strategies have been analyzed in the literature.","However, often the Byzantine problem is analyzed solely from the outlier detection perspective, being oblivious to the topology of neural networks (NNs).   ","In the scope of this work, we argue that by extracting certain side information specific to the NN topology, one can design stronger attacks.","Hence, inspired by the sparse neural networks, we introduce a hybrid sparse Byzantine attack that is composed of two parts: one exhibiting a sparse nature and attacking only certain NN locations with higher sensitivity, and the other being more silent but accumulating over time, where each ideally targets a different type of defence mechanism, and together they form a strong but imperceptible attack.","Finally, we show through extensive simulations that the proposed hybrid Byzantine attack is effective against 8 different defence methods."],"url":"http://arxiv.org/abs/2404.06230v1","category":"cs.LG"}
{"created":"2024-04-09 11:27:07","title":"Message Passing Variational Autoregressive Network for Solving Intractable Ising Models","abstract":"Many deep neural networks have been used to solve Ising models, including autoregressive neural networks, convolutional neural networks, recurrent neural networks, and graph neural networks. Learning a probability distribution of energy configuration or finding the ground states of a disordered, fully connected Ising model is essential for statistical mechanics and NP-hard problems. Despite tremendous efforts, a neural network architecture with the ability to high-accurately solve these fully connected and extremely intractable problems on larger systems is still lacking. Here we propose a variational autoregressive architecture with a message passing mechanism, which can effectively utilize the interactions between spin variables. The new network trained under an annealing framework outperforms existing methods in solving several prototypical Ising spin Hamiltonians, especially for larger spin systems at low temperatures. The advantages also come from the great mitigation of mode collapse during the training process of deep neural networks. Considering these extremely difficult problems to be solved, our method extends the current computational limits of unsupervised neural networks to solve combinatorial optimization problems.","sentences":["Many deep neural networks have been used to solve Ising models, including autoregressive neural networks, convolutional neural networks, recurrent neural networks, and graph neural networks.","Learning a probability distribution of energy configuration or finding the ground states of a disordered, fully connected Ising model is essential for statistical mechanics and NP-hard problems.","Despite tremendous efforts, a neural network architecture with the ability to high-accurately solve these fully connected and extremely intractable problems on larger systems is still lacking.","Here we propose a variational autoregressive architecture with a message passing mechanism, which can effectively utilize the interactions between spin variables.","The new network trained under an annealing framework outperforms existing methods in solving several prototypical Ising spin Hamiltonians, especially for larger spin systems at low temperatures.","The advantages also come from the great mitigation of mode collapse during the training process of deep neural networks.","Considering these extremely difficult problems to be solved, our method extends the current computational limits of unsupervised neural networks to solve combinatorial optimization problems."],"url":"http://arxiv.org/abs/2404.06225v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-09 11:12:39","title":"Quantum Circuit $C^*$-algebra Net","abstract":"This paper introduces quantum circuit $C^*$-algebra net, which provides a connection between $C^*$-algebra nets proposed in classical machine learning and quantum circuits. Using $C^*$-algebra, a generalization of the space of complex numbers, we can represent quantum gates as weight parameters of a neural network. By introducing additional parameters, we can induce interaction among multiple circuits constructed by quantum gates. This interaction enables the circuits to share information among them, which contributes to improved generalization performance in machine learning tasks. As an application, we propose to use the quantum circuit $C^*$-algebra net to encode classical data into quantum states, which enables us to integrate classical data into quantum algorithms. Numerical results demonstrate that the interaction among circuits improves performance significantly in image classification, and encoded data by the quantum circuit $C^*$-algebra net are useful for downstream quantum machine learning tasks.","sentences":["This paper introduces quantum circuit $C^*$-algebra net, which provides a connection between $C^*$-algebra nets proposed in classical machine learning and quantum circuits.","Using $C^*$-algebra, a generalization of the space of complex numbers, we can represent quantum gates as weight parameters of a neural network.","By introducing additional parameters, we can induce interaction among multiple circuits constructed by quantum gates.","This interaction enables the circuits to share information among them, which contributes to improved generalization performance in machine learning tasks.","As an application, we propose to use the quantum circuit $C^*$-algebra net to encode classical data into quantum states, which enables us to integrate classical data into quantum algorithms.","Numerical results demonstrate that the interaction among circuits improves performance significantly in image classification, and encoded data by the quantum circuit $C^*$-algebra net are useful for downstream quantum machine learning tasks."],"url":"http://arxiv.org/abs/2404.06218v1","category":"cs.LG"}
{"created":"2024-04-09 11:10:00","title":"VI-OOD: A Unified Representation Learning Framework for Textual Out-of-distribution Detection","abstract":"Out-of-distribution (OOD) detection plays a crucial role in ensuring the safety and reliability of deep neural networks in various applications. While there has been a growing focus on OOD detection in visual data, the field of textual OOD detection has received less attention. Only a few attempts have been made to directly apply general OOD detection methods to natural language processing (NLP) tasks, without adequately considering the characteristics of textual data. In this paper, we delve into textual OOD detection with Transformers. We first identify a key problem prevalent in existing OOD detection methods: the biased representation learned through the maximization of the conditional likelihood $p(y\\mid x)$ can potentially result in subpar performance. We then propose a novel variational inference framework for OOD detection (VI-OOD), which maximizes the likelihood of the joint distribution $p(x, y)$ instead of $p(y\\mid x)$. VI-OOD is tailored for textual OOD detection by efficiently exploiting the representations of pre-trained Transformers. Through comprehensive experiments on various text classification tasks, VI-OOD demonstrates its effectiveness and wide applicability. Our code has been released at \\url{https://github.com/liam0949/LLM-OOD}.","sentences":["Out-of-distribution (OOD) detection plays a crucial role in ensuring the safety and reliability of deep neural networks in various applications.","While there has been a growing focus on OOD detection in visual data, the field of textual OOD detection has received less attention.","Only a few attempts have been made to directly apply general OOD detection methods to natural language processing (NLP) tasks, without adequately considering the characteristics of textual data.","In this paper, we delve into textual OOD detection with Transformers.","We first identify a key problem prevalent in existing OOD detection methods: the biased representation learned through the maximization of the conditional likelihood $p(y\\mid x)$ can potentially result in subpar performance.","We then propose a novel variational inference framework for OOD detection (VI-OOD), which maximizes the likelihood of the joint distribution $p(x, y)$ instead of $p(y\\mid x)$. VI-OOD is tailored for textual OOD detection by efficiently exploiting the representations of pre-trained Transformers.","Through comprehensive experiments on various text classification tasks, VI-OOD demonstrates its effectiveness and wide applicability.","Our code has been released at \\url{https://github.com/liam0949/LLM-OOD}."],"url":"http://arxiv.org/abs/2404.06217v1","category":"cs.CL"}
{"created":"2024-04-09 11:07:57","title":"Privacy-preserving Scanpath Comparison for Pervasive Eye Tracking","abstract":"As eye tracking becomes pervasive with screen-based devices and head-mounted displays, privacy concerns regarding eye-tracking data have escalated. While state-of-the-art approaches for privacy-preserving eye tracking mostly involve differential privacy and empirical data manipulations, previous research has not focused on methods for scanpaths. We introduce a novel privacy-preserving scanpath comparison protocol designed for the widely used Needleman-Wunsch algorithm, a generalized version of the edit distance algorithm. Particularly, by incorporating the Paillier homomorphic encryption scheme, our protocol ensures that no private information is revealed. Furthermore, we introduce a random processing strategy and a multi-layered masking method to obfuscate the values while preserving the original order of encrypted editing operation costs. This minimizes communication overhead, requiring a single communication round for each iteration of the Needleman-Wunsch process. We demonstrate the efficiency and applicability of our protocol on three publicly available datasets with comprehensive computational performance analyses and make our source code publicly accessible.","sentences":["As eye tracking becomes pervasive with screen-based devices and head-mounted displays, privacy concerns regarding eye-tracking data have escalated.","While state-of-the-art approaches for privacy-preserving eye tracking mostly involve differential privacy and empirical data manipulations, previous research has not focused on methods for scanpaths.","We introduce a novel privacy-preserving scanpath comparison protocol designed for the widely used Needleman-Wunsch algorithm, a generalized version of the edit distance algorithm.","Particularly, by incorporating the Paillier homomorphic encryption scheme, our protocol ensures that no private information is revealed.","Furthermore, we introduce a random processing strategy and a multi-layered masking method to obfuscate the values while preserving the original order of encrypted editing operation costs.","This minimizes communication overhead, requiring a single communication round for each iteration of the Needleman-Wunsch process.","We demonstrate the efficiency and applicability of our protocol on three publicly available datasets with comprehensive computational performance analyses and make our source code publicly accessible."],"url":"http://arxiv.org/abs/2404.06216v1","category":"cs.CR"}
{"created":"2024-04-09 11:03:09","title":"Quantum Approach to Bound States in Field Theory","abstract":"It is well known that (possibly non-unique) suitable field dynamics can be prescribed in spacetimes with timelike boundaries by means of appropriate boundary conditions. In Ref. [J. Math. Phys. {\\bf 21}, 2802 (1980)], Wald derived a conserved energy functional for each prescribed dynamics. This conserved energy is related to the positive self-adjoint extensions of the spatial part $A$ of the wave equation $\\partial^2\\Phi/\\partial t^2=-A\\Phi$ ($A$ may not be, in principle, essentially self-adjoint). This is quite surprising since the canonical energy is not conserved in these cases. In this paper, we rederive this energy functional from an action principle (with appropriate boundary terms) following Ref. [Phys. Rev. D, {\\bf 69}, 085005, (2004)] and consider field dynamics arising from non-positive self-adjoint extensions of $A$. The spectrum of the resulting theory fails to be positive and unstable mode solutions for classical fields come to light. By studying fields in half-Minkowski spacetime, we illustrate that these unstable classical solutions come as a consequence of an inverted parabolic potential governing their dynamics. From the quantum mechanical point of view, this leads to an effective inverted harmonic oscillator at the boundary. We then explore these unstable modes behavior, as well as their instabilities, at the quantum level.","sentences":["It is well known that (possibly non-unique) suitable field dynamics can be prescribed in spacetimes with timelike boundaries by means of appropriate boundary conditions.","In Ref.","[J. Math. Phys. {\\bf 21}, 2802 (1980)], Wald derived a conserved energy functional for each prescribed dynamics.","This conserved energy is related to the positive self-adjoint extensions of the spatial part $A$ of the wave equation $\\partial^2\\Phi/\\partial t^2=-A\\Phi$ ($A$ may not be, in principle, essentially self-adjoint).","This is quite surprising since the canonical energy is not conserved in these cases.","In this paper, we rederive this energy functional from an action principle (with appropriate boundary terms) following Ref.","[Phys. Rev. D, {\\bf 69}, 085005, (2004)] and consider field dynamics arising from non-positive self-adjoint extensions of $A$.","The spectrum of the resulting theory fails to be positive and unstable mode solutions for classical fields come to light.","By studying fields in half-Minkowski spacetime, we illustrate that these unstable classical solutions come as a consequence of an inverted parabolic potential governing their dynamics.","From the quantum mechanical point of view, this leads to an effective inverted harmonic oscillator at the boundary.","We then explore these unstable modes behavior, as well as their instabilities, at the quantum level."],"url":"http://arxiv.org/abs/2404.06213v1","category":"hep-th"}
{"created":"2024-04-10 16:07:38","title":"What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation","abstract":"In-context learning is a powerful emergent ability in transformer models. Prior work in mechanistic interpretability has identified a circuit element that may be critical for in-context learning -- the induction head (IH), which performs a match-and-copy operation. During training of large transformers on natural language data, IHs emerge around the same time as a notable phase change in the loss. Despite the robust evidence for IHs and this interesting coincidence with the phase change, relatively little is known about the diversity and emergence dynamics of IHs. Why is there more than one IH, and how are they dependent on each other? Why do IHs appear all of a sudden, and what are the subcircuits that enable them to emerge? We answer these questions by studying IH emergence dynamics in a controlled setting by training on synthetic data. In doing so, we develop and share a novel optogenetics-inspired causal framework for modifying activations throughout training. Using this framework, we delineate the diverse and additive nature of IHs. By clamping subsets of activations throughout training, we then identify three underlying subcircuits that interact to drive IH formation, yielding the phase change. Furthermore, these subcircuits shed light on data-dependent properties of formation, such as phase change timing, already showing the promise of this more in-depth understanding of subcircuits that need to \"go right\" for an induction head.","sentences":["In-context learning is a powerful emergent ability in transformer models.","Prior work in mechanistic interpretability has identified a circuit element that may be critical for in-context learning -- the induction head (IH), which performs a match-and-copy operation.","During training of large transformers on natural language data, IHs emerge around the same time as a notable phase change in the loss.","Despite the robust evidence for IHs and this interesting coincidence with the phase change, relatively little is known about the diversity and emergence dynamics of IHs.","Why is there more than one IH, and how are they dependent on each other?","Why do IHs appear all of a sudden, and what are the subcircuits that enable them to emerge?","We answer these questions by studying IH emergence dynamics in a controlled setting by training on synthetic data.","In doing so, we develop and share a novel optogenetics-inspired causal framework for modifying activations throughout training.","Using this framework, we delineate the diverse and additive nature of IHs.","By clamping subsets of activations throughout training, we then identify three underlying subcircuits that interact to drive IH formation, yielding the phase change.","Furthermore, these subcircuits shed light on data-dependent properties of formation, such as phase change timing, already showing the promise of this more in-depth understanding of subcircuits that need to \"go right\" for an induction head."],"url":"http://arxiv.org/abs/2404.07129v1","category":"cs.LG"}
{"created":"2024-04-10 15:41:53","title":"Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs","abstract":"Large language models (LLMs), while exhibiting exceptional performance, suffer from hallucinations, especially on knowledge-intensive tasks. Existing works propose to augment LLMs with individual text units retrieved from external knowledge corpora to alleviate the issue. However, in many domains, texts are interconnected (e.g., academic papers in a bibliographic graph are linked by citations and co-authorships) which form a (text-attributed) graph. The knowledge in such graphs is encoded not only in single texts/nodes but also in their associated connections. To facilitate the research of augmenting LLMs with graphs, we manually construct a Graph Reasoning Benchmark dataset called GRBench, containing 1,740 questions that can be answered with the knowledge from 10 domain graphs. Then, we propose a simple and effective framework called Graph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging LLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of three sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We conduct systematic experiments with three LLM backbones on GRBench, where Graph-CoT outperforms the baselines consistently. The code is available at https://github.com/PeterGriffinJin/Graph-CoT.","sentences":["Large language models (LLMs), while exhibiting exceptional performance, suffer from hallucinations, especially on knowledge-intensive tasks.","Existing works propose to augment LLMs with individual text units retrieved from external knowledge corpora to alleviate the issue.","However, in many domains, texts are interconnected (e.g., academic papers in a bibliographic graph are linked by citations and co-authorships) which form a (text-attributed) graph.","The knowledge in such graphs is encoded not only in single texts/nodes but also in their associated connections.","To facilitate the research of augmenting LLMs with graphs, we manually construct a Graph Reasoning Benchmark dataset called GRBench, containing 1,740 questions that can be answered with the knowledge from 10 domain graphs.","Then, we propose a simple and effective framework called Graph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging LLMs to reason on the graph iteratively.","Each Graph-CoT iteration consists of three sub-steps: LLM reasoning, LLM-graph interaction, and graph execution.","We conduct systematic experiments with three LLM backbones on GRBench, where Graph-CoT outperforms the baselines consistently.","The code is available at https://github.com/PeterGriffinJin/Graph-CoT."],"url":"http://arxiv.org/abs/2404.07103v1","category":"cs.CL"}
{"created":"2024-04-10 15:03:48","title":"Multiscale structure-property discovery via active learning in scanning tunneling microscopy","abstract":"Atomic arrangements and local sub-structures fundamentally influence emergent material functionalities. The local structures are conventionally probed using spatially resolved studies and the property correlations are usually deciphered by a researcher based on sequential explorations and auxiliary information, thus limiting the throughput efficiency. Here we demonstrate a Bayesian deep learning based framework that automatically correlates material structure with its electronic properties using scanning tunneling microscopy (STM) measurements in real-time. Its predictions are used to autonomously direct exploration toward regions of the sample that optimize a given material property. This autonomous method is deployed on the low-temperature ultra-high vacuum STM to understand the structure-property relationship in a europium-based semimetal, EuZn2As2, one of the promising candidates for studying the magnetism-driven topological properties. The framework employs a sparse sampling approach to efficiently construct the scalar-property space using a minimal number of measurements, about 1 - 10 % of the data required in standard hyperspectral imaging methods. We further demonstrate a target-property-guided active learning of structures within a multiscale framework. This is implemented across length scales in a hierarchical fashion for the autonomous discovery of structural origins for an observed material property. This framework offers the choice to select and derive a suitable scalar property from the spectroscopic data to steer exploration across the sample space. Our findings reveal correlations of the electronic properties unique to surface terminations, local defect density, and point defects.","sentences":["Atomic arrangements and local sub-structures fundamentally influence emergent material functionalities.","The local structures are conventionally probed using spatially resolved studies and the property correlations are usually deciphered by a researcher based on sequential explorations and auxiliary information, thus limiting the throughput efficiency.","Here we demonstrate a Bayesian deep learning based framework that automatically correlates material structure with its electronic properties using scanning tunneling microscopy (STM) measurements in real-time.","Its predictions are used to autonomously direct exploration toward regions of the sample that optimize a given material property.","This autonomous method is deployed on the low-temperature ultra-high vacuum STM to understand the structure-property relationship in a europium-based semimetal, EuZn2As2, one of the promising candidates for studying the magnetism-driven topological properties.","The framework employs a sparse sampling approach to efficiently construct the scalar-property space using a minimal number of measurements, about 1 - 10 % of the data required in standard hyperspectral imaging methods.","We further demonstrate a target-property-guided active learning of structures within a multiscale framework.","This is implemented across length scales in a hierarchical fashion for the autonomous discovery of structural origins for an observed material property.","This framework offers the choice to select and derive a suitable scalar property from the spectroscopic data to steer exploration across the sample space.","Our findings reveal correlations of the electronic properties unique to surface terminations, local defect density, and point defects."],"url":"http://arxiv.org/abs/2404.07074v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-10 13:12:07","title":"Quiver Laplacians and Feature Selection","abstract":"The challenge of selecting the most relevant features of a given dataset arises ubiquitously in data analysis and dimensionality reduction. However, features found to be of high importance for the entire dataset may not be relevant to subsets of interest, and vice versa. Given a feature selector and a fixed decomposition of the data into subsets, we describe a method for identifying selected features which are compatible with the decomposition into subsets. We achieve this by re-framing the problem of finding compatible features to one of finding sections of a suitable quiver representation. In order to approximate such sections, we then introduce a Laplacian operator for quiver representations valued in Hilbert spaces. We provide explicit bounds on how the spectrum of a quiver Laplacian changes when the representation and the underlying quiver are modified in certain natural ways. Finally, we apply this machinery to the study of peak-calling algorithms which measure chromatin accessibility in single-cell data. We demonstrate that eigenvectors of the associated quiver Laplacian yield locally and globally compatible features.","sentences":["The challenge of selecting the most relevant features of a given dataset arises ubiquitously in data analysis and dimensionality reduction.","However, features found to be of high importance for the entire dataset may not be relevant to subsets of interest, and vice versa.","Given a feature selector and a fixed decomposition of the data into subsets, we describe a method for identifying selected features which are compatible with the decomposition into subsets.","We achieve this by re-framing the problem of finding compatible features to one of finding sections of a suitable quiver representation.","In order to approximate such sections, we then introduce a Laplacian operator for quiver representations valued in Hilbert spaces.","We provide explicit bounds on how the spectrum of a quiver Laplacian changes when the representation and the underlying quiver are modified in certain natural ways.","Finally, we apply this machinery to the study of peak-calling algorithms which measure chromatin accessibility in single-cell data.","We demonstrate that eigenvectors of the associated quiver Laplacian yield locally and globally compatible features."],"url":"http://arxiv.org/abs/2404.06993v1","category":"stat.ML"}
{"created":"2024-04-10 13:08:07","title":"On Fixing the Right Problems in Predictive Analytics: AUC Is Not the Problem","abstract":"Recently, ACM FAccT published an article by Kwegyir-Aggrey and colleagues (2023), critiquing the use of AUC ROC in predictive analytics in several domains. In this article, we offer a critique of that article. Specifically, we highlight technical inaccuracies in that paper's comparison of metrics, mis-specification of the interpretation and goals of AUC ROC, the article's use of the accuracy metric as a gold standard for comparison to AUC ROC, and the article's application of critiques solely to AUC ROC for concerns that would apply to the use of any metric. We conclude with a re-framing of the very valid concerns raised in that article, and discuss how the use of AUC ROC can remain a valid and appropriate practice in a well-informed predictive analytics approach taking those concerns into account. We conclude by discussing the combined use of multiple metrics, including machine learning bias metrics, and AUC ROC's place in such an approach. Like broccoli, AUC ROC is healthy, but also like broccoli, researchers and practitioners in our field shouldn't eat a diet of only AUC ROC.","sentences":["Recently, ACM FAccT published an article by Kwegyir-Aggrey and colleagues (2023), critiquing the use of AUC ROC in predictive analytics in several domains.","In this article, we offer a critique of that article.","Specifically, we highlight technical inaccuracies in that paper's comparison of metrics, mis-specification of the interpretation and goals of AUC ROC, the article's use of the accuracy metric as a gold standard for comparison to AUC ROC, and the article's application of critiques solely to AUC ROC for concerns that would apply to the use of any metric.","We conclude with a re-framing of the very valid concerns raised in that article, and discuss how the use of AUC ROC can remain a valid and appropriate practice in a well-informed predictive analytics approach taking those concerns into account.","We conclude by discussing the combined use of multiple metrics, including machine learning bias metrics, and AUC ROC's place in such an approach.","Like broccoli, AUC ROC is healthy, but also like broccoli, researchers and practitioners in our field shouldn't eat a diet of only AUC ROC."],"url":"http://arxiv.org/abs/2404.06989v1","category":"cs.LG"}
{"created":"2024-04-10 13:05:39","title":"Quantum Network Tomography via Learning Isometries on Stiefel Manifold","abstract":"Explicit mathematical reconstructions of quantum networks play a significant role in developing quantum information science. However, tremendous parameter requirements and physical constraint implementations have become computationally non-ignorable encumbrances. In this work, we propose an efficient method for quantum network tomography by learning isometries on the Stiefel manifold. Tasks of reconstructing quantum networks are tackled by solving a series of unconstrained optimization problems with significantly less parameters. The step-wise isometry estimation shows the capability for providing information of the truncated quantum comb while processing the tomography. Remarkably, this method enables the compressive QCT by specifying the dimensions of isometries. As a result, our proposed method exhibits high accuracy and efficiency.","sentences":["Explicit mathematical reconstructions of quantum networks play a significant role in developing quantum information science.","However, tremendous parameter requirements and physical constraint implementations have become computationally non-ignorable encumbrances.","In this work, we propose an efficient method for quantum network tomography by learning isometries on the Stiefel manifold.","Tasks of reconstructing quantum networks are tackled by solving a series of unconstrained optimization problems with significantly less parameters.","The step-wise isometry estimation shows the capability for providing information of the truncated quantum comb while processing the tomography.","Remarkably, this method enables the compressive QCT by specifying the dimensions of isometries.","As a result, our proposed method exhibits high accuracy and efficiency."],"url":"http://arxiv.org/abs/2404.06988v1","category":"quant-ph"}
{"created":"2024-04-10 12:38:38","title":"Deep Reinforcement Learning for Mobile Robot Path Planning","abstract":"Path planning is an important problem with the the applications in many aspects, such as video games, robotics etc. This paper proposes a novel method to address the problem of Deep Reinforcement Learning (DRL) based path planning for a mobile robot. We design DRL-based algorithms, including reward functions, and parameter optimization, to avoid time-consuming work in a 2D environment. We also designed an Two-way search hybrid A* algorithm to improve the quality of local path planning. We transferred the designed algorithm to a simple embedded environment to test the computational load of the algorithm when running on a mobile robot. Experiments show that when deployed on a robot platform, the DRL-based algorithm in this article can achieve better planning results and consume less computing resources.","sentences":["Path planning is an important problem with the the applications in many aspects, such as video games, robotics etc.","This paper proposes a novel method to address the problem of Deep Reinforcement Learning (DRL) based path planning for a mobile robot.","We design DRL-based algorithms, including reward functions, and parameter optimization, to avoid time-consuming work in a 2D environment.","We also designed an Two-way search hybrid A* algorithm to improve the quality of local path planning.","We transferred the designed algorithm to a simple embedded environment to test the computational load of the algorithm when running on a mobile robot.","Experiments show that when deployed on a robot platform, the DRL-based algorithm in this article can achieve better planning results and consume less computing resources."],"url":"http://arxiv.org/abs/2404.06974v1","category":"cs.RO"}
{"created":"2024-04-10 11:47:51","title":"Accelerating Cardiac MRI Reconstruction with CMRatt: An Attention-Driven Approach","abstract":"Cine cardiac magnetic resonance (CMR) imaging is recognised as the benchmark modality for the comprehensive assessment of cardiac function. Nevertheless, the acquisition process of cine CMR is considered as an impediment due to its prolonged scanning time. One commonly used strategy to expedite the acquisition process is through k-space undersampling, though it comes with a drawback of introducing aliasing effects in the reconstructed image. Lately, deep learning-based methods have shown remarkable results over traditional approaches in rapidly achieving precise CMR reconstructed images. This study aims to explore the untapped potential of attention mechanisms incorporated with a deep learning model within the context of the CMR reconstruction problem. We are motivated by the fact that attention has proven beneficial in downstream tasks such as image classification and segmentation, but has not been systematically analysed in the context of CMR reconstruction. Our primary goal is to identify the strengths and potential limitations of attention algorithms when integrated with a convolutional backbone model such as a U-Net. To achieve this, we benchmark different state-of-the-art spatial and channel attention mechanisms on the CMRxRecon dataset and quantitatively evaluate the quality of reconstruction using objective metrics. Furthermore, inspired by the best performing attention mechanism, we propose a new, simple yet effective, attention pipeline specifically optimised for the task of cardiac image reconstruction that outperforms other state-of-the-art attention methods. The layer and model code will be made publicly available.","sentences":["Cine cardiac magnetic resonance (CMR) imaging is recognised as the benchmark modality for the comprehensive assessment of cardiac function.","Nevertheless, the acquisition process of cine CMR is considered as an impediment due to its prolonged scanning time.","One commonly used strategy to expedite the acquisition process is through k-space undersampling, though it comes with a drawback of introducing aliasing effects in the reconstructed image.","Lately, deep learning-based methods have shown remarkable results over traditional approaches in rapidly achieving precise CMR reconstructed images.","This study aims to explore the untapped potential of attention mechanisms incorporated with a deep learning model within the context of the CMR reconstruction problem.","We are motivated by the fact that attention has proven beneficial in downstream tasks such as image classification and segmentation, but has not been systematically analysed in the context of CMR reconstruction.","Our primary goal is to identify the strengths and potential limitations of attention algorithms when integrated with a convolutional backbone model such as a U-Net.","To achieve this, we benchmark different state-of-the-art spatial and channel attention mechanisms on the CMRxRecon dataset and quantitatively evaluate the quality of reconstruction using objective metrics.","Furthermore, inspired by the best performing attention mechanism, we propose a new, simple yet effective, attention pipeline specifically optimised for the task of cardiac image reconstruction that outperforms other state-of-the-art attention methods.","The layer and model code will be made publicly available."],"url":"http://arxiv.org/abs/2404.06941v1","category":"eess.IV"}
{"created":"2024-04-10 11:04:24","title":"Set-Encoder: Permutation-Invariant Inter-Passage Attention for Listwise Passage Re-Ranking with Cross-Encoders","abstract":"Cross-encoders are effective passage re-rankers. But when re-rank\\-ing multiple passages at once, existing cross-encoders inefficiently optimize the output ranking over several input permutations, as their passage interactions are not permutation-invariant. Moreover, their high memory footprint constrains the number of passages during listwise training. To tackle these issues, we propose the Set-Encoder, a new cross-encoder architecture that (1) introduces inter-passage attention with parallel passage processing to ensure permutation invariance between input passages, and that (2) uses fused-attention kernels to enable training with more passages at a time. In experiments on TREC Deep Learning and TIREx, the Set-Encoder is more effective than previous cross-encoders with a similar number of parameters. Compared to larger models, the Set-Encoder is more efficient and either on par or even more effective.","sentences":["Cross-encoders are effective passage re-rankers.","But when re-rank\\-ing multiple passages at once, existing cross-encoders inefficiently optimize the output ranking over several input permutations, as their passage interactions are not permutation-invariant.","Moreover, their high memory footprint constrains the number of passages during listwise training.","To tackle these issues, we propose the Set-Encoder, a new cross-encoder architecture that (1) introduces inter-passage attention with parallel passage processing to ensure permutation invariance between input passages, and that (2) uses fused-attention kernels to enable training with more passages at a time.","In experiments on TREC Deep Learning and TIREx, the Set-Encoder is more effective than previous cross-encoders with a similar number of parameters.","Compared to larger models, the Set-Encoder is more efficient and either on par or even more effective."],"url":"http://arxiv.org/abs/2404.06912v1","category":"cs.IR"}
{"created":"2024-04-10 10:49:43","title":"Vision-Language Model-based Physical Reasoning for Robot Liquid Perception","abstract":"There is a growing interest in applying large language models (LLMs) in robotic tasks, due to their remarkable reasoning ability and extensive knowledge learned from vast training corpora. Grounding LLMs in the physical world remains an open challenge as they can only process textual input. Recent advancements in large vision-language models (LVLMs) have enabled a more comprehensive understanding of the physical world by incorporating visual input, which provides richer contextual information than language alone. In this work, we proposed a novel paradigm that leveraged GPT-4V(ision), the state-of-the-art LVLM by OpenAI, to enable embodied agents to perceive liquid objects via image-based environmental feedback. Specifically, we exploited the physical understanding of GPT-4V to interpret the visual representation (e.g., time-series plot) of non-visual feedback (e.g., F/T sensor data), indirectly enabling multimodal perception beyond vision and language using images as proxies. We evaluated our method using 10 common household liquids with containers of various geometry and material. Without any training or fine-tuning, we demonstrated that our method can enable the robot to indirectly perceive the physical response of liquids and estimate their viscosity. We also showed that by jointly reasoning over the visual and physical attributes learned through interactions, our method could recognize liquid objects in the absence of strong visual cues (e.g., container labels with legible text or symbols), increasing the accuracy from 69.0% -- achieved by the best-performing vision-only variant -- to 86.0%.","sentences":["There is a growing interest in applying large language models (LLMs) in robotic tasks, due to their remarkable reasoning ability and extensive knowledge learned from vast training corpora.","Grounding LLMs in the physical world remains an open challenge as they can only process textual input.","Recent advancements in large vision-language models (LVLMs) have enabled a more comprehensive understanding of the physical world by incorporating visual input, which provides richer contextual information than language alone.","In this work, we proposed a novel paradigm that leveraged GPT-4V(ision), the state-of-the-art LVLM by OpenAI, to enable embodied agents to perceive liquid objects via image-based environmental feedback.","Specifically, we exploited the physical understanding of GPT-4V to interpret the visual representation (e.g., time-series plot) of non-visual feedback (e.g., F/T sensor data), indirectly enabling multimodal perception beyond vision and language using images as proxies.","We evaluated our method using 10 common household liquids with containers of various geometry and material.","Without any training or fine-tuning, we demonstrated that our method can enable the robot to indirectly perceive the physical response of liquids and estimate their viscosity.","We also showed that by jointly reasoning over the visual and physical attributes learned through interactions, our method could recognize liquid objects in the absence of strong visual cues (e.g., container labels with legible text or symbols), increasing the accuracy from 69.0% -- achieved by the best-performing vision-only variant -- to 86.0%."],"url":"http://arxiv.org/abs/2404.06904v1","category":"cs.RO"}
{"created":"2024-04-10 07:34:37","title":"Extracting Clean and Balanced Subset for Noisy Long-tailed Classification","abstract":"Real-world datasets usually are class-imbalanced and corrupted by label noise. To solve the joint issue of long-tailed distribution and label noise, most previous works usually aim to design a noise detector to distinguish the noisy and clean samples. Despite their effectiveness, they may be limited in handling the joint issue effectively in a unified way. In this work, we develop a novel pseudo labeling method using class prototypes from the perspective of distribution matching, which can be solved with optimal transport (OT). By setting a manually-specific probability measure and using a learned transport plan to pseudo-label the training samples, the proposed method can reduce the side-effects of noisy and long-tailed data simultaneously. Then we introduce a simple yet effective filter criteria by combining the observed labels and pseudo labels to obtain a more balanced and less noisy subset for a robust model training. Extensive experiments demonstrate that our method can extract this class-balanced subset with clean labels, which brings effective performance gains for long-tailed classification with label noise.","sentences":["Real-world datasets usually are class-imbalanced and corrupted by label noise.","To solve the joint issue of long-tailed distribution and label noise, most previous works usually aim to design a noise detector to distinguish the noisy and clean samples.","Despite their effectiveness, they may be limited in handling the joint issue effectively in a unified way.","In this work, we develop a novel pseudo labeling method using class prototypes from the perspective of distribution matching, which can be solved with optimal transport (OT).","By setting a manually-specific probability measure and using a learned transport plan to pseudo-label the training samples, the proposed method can reduce the side-effects of noisy and long-tailed data simultaneously.","Then we introduce a simple yet effective filter criteria by combining the observed labels and pseudo labels to obtain a more balanced and less noisy subset for a robust model training.","Extensive experiments demonstrate that our method can extract this class-balanced subset with clean labels, which brings effective performance gains for long-tailed classification with label noise."],"url":"http://arxiv.org/abs/2404.06795v1","category":"cs.LG"}
{"created":"2024-04-10 03:11:10","title":"Scaling Multi-Camera 3D Object Detection through Weak-to-Strong Eliciting","abstract":"The emergence of Multi-Camera 3D Object Detection (MC3D-Det), facilitated by bird's-eye view (BEV) representation, signifies a notable progression in 3D object detection. Scaling MC3D-Det training effectively accommodates varied camera parameters and urban landscapes, paving the way for the MC3D-Det foundation model. However, the multi-view fusion stage of the MC3D-Det method relies on the ill-posed monocular perception during training rather than surround refinement ability, leading to what we term \"surround refinement degradation\". To this end, our study presents a weak-to-strong eliciting framework aimed at enhancing surround refinement while maintaining robust monocular perception. Specifically, our framework employs weakly tuned experts trained on distinct subsets, and each is inherently biased toward specific camera configurations and scenarios. These biased experts can learn the perception of monocular degeneration, which can help the multi-view fusion stage to enhance surround refinement abilities. Moreover, a composite distillation strategy is proposed to integrate the universal knowledge of 2D foundation models and task-specific information. Finally, for MC3D-Det joint training, the elaborate dataset merge strategy is designed to solve the problem of inconsistent camera numbers and camera parameters. We set up a multiple dataset joint training benchmark for MC3D-Det and adequately evaluated existing methods. Further, we demonstrate the proposed framework brings a generalized and significant boost over multiple baselines. Our code is at \\url{https://github.com/EnVision-Research/Scale-BEV}.","sentences":["The emergence of Multi-Camera 3D Object Detection (MC3D-Det), facilitated by bird's-eye view (BEV) representation, signifies a notable progression in 3D object detection.","Scaling MC3D-Det training effectively accommodates varied camera parameters and urban landscapes, paving the way for the MC3D-Det foundation model.","However, the multi-view fusion stage of the MC3D-Det method relies on the ill-posed monocular perception during training rather than surround refinement ability, leading to what we term \"surround refinement degradation\".","To this end, our study presents a weak-to-strong eliciting framework aimed at enhancing surround refinement while maintaining robust monocular perception.","Specifically, our framework employs weakly tuned experts trained on distinct subsets, and each is inherently biased toward specific camera configurations and scenarios.","These biased experts can learn the perception of monocular degeneration, which can help the multi-view fusion stage to enhance surround refinement abilities.","Moreover, a composite distillation strategy is proposed to integrate the universal knowledge of 2D foundation models and task-specific information.","Finally, for MC3D-Det joint training, the elaborate dataset merge strategy is designed to solve the problem of inconsistent camera numbers and camera parameters.","We set up a multiple dataset joint training benchmark for MC3D-Det and adequately evaluated existing methods.","Further, we demonstrate the proposed framework brings a generalized and significant boost over multiple baselines.","Our code is at \\url{https://github.com/EnVision-Research/Scale-BEV}."],"url":"http://arxiv.org/abs/2404.06700v1","category":"cs.CV"}
{"created":"2024-04-10 03:04:40","title":"Bayesian Model Selection with Latent Group-Based Effects and Variances with the R Package slgf","abstract":"Linear modeling is ubiquitous, but performance can suffer when the model is misspecified. We have recently demonstrated that latent groupings in the levels of categorical predictors can complicate inference in a variety of fields including bioinformatics, agriculture, industry, engineering, and medicine. Here we present the R package slgf which enables the user to easily implement our recently-developed approach to detect group-based regression effects, latent interactions, and/or heteroscedastic error variance through Bayesian model selection. We focus on the scenario in which the levels of a categorical predictor exhibit two latent groups. We treat the detection of this grouping structure as an unsupervised learning problem by searching the space of possible groupings of factor levels. First we review the suspected latent grouping factor (SLGF) method. Next, using both observational and experimental data, we illustrate the usage of slgf in the context of several common linear model layouts: one-way analysis of variance (ANOVA), analysis of covariance (ANCOVA), a two-way replicated layout, and a two-way unreplicated layout. We have selected data that reveal the shortcomings of classical analyses to emphasize the advantage our method can provide when a latent grouping structure is present.","sentences":["Linear modeling is ubiquitous, but performance can suffer when the model is misspecified.","We have recently demonstrated that latent groupings in the levels of categorical predictors can complicate inference in a variety of fields including bioinformatics, agriculture, industry, engineering, and medicine.","Here we present the R package slgf which enables the user to easily implement our recently-developed approach to detect group-based regression effects, latent interactions, and/or heteroscedastic error variance through Bayesian model selection.","We focus on the scenario in which the levels of a categorical predictor exhibit two latent groups.","We treat the detection of this grouping structure as an unsupervised learning problem by searching the space of possible groupings of factor levels.","First we review the suspected latent grouping factor (SLGF) method.","Next, using both observational and experimental data, we illustrate the usage of slgf in the context of several common linear model layouts: one-way analysis of variance (ANOVA), analysis of covariance (ANCOVA), a two-way replicated layout, and a two-way unreplicated layout.","We have selected data that reveal the shortcomings of classical analyses to emphasize the advantage our method can provide when a latent grouping structure is present."],"url":"http://arxiv.org/abs/2404.06698v1","category":"stat.ME"}
{"created":"2024-04-10 03:02:25","title":"Dual Ensemble Kalman Filter for Stochastic Optimal Control","abstract":"In this paper, stochastic optimal control problems in continuous time and space are considered. In recent years, such problems have received renewed attention from the lens of reinforcement learning (RL) which is also one of our motivation. The main contribution is a simulation-based algorithm -- dual ensemble Kalman filter (EnKF) -- to numerically approximate the solution of these problems. The paper extends our previous work where the dual EnKF was applied in deterministic settings of the problem. The theoretical results and algorithms are illustrated with numerical experiments.","sentences":["In this paper, stochastic optimal control problems in continuous time and space are considered.","In recent years, such problems have received renewed attention from the lens of reinforcement learning (RL) which is also one of our motivation.","The main contribution is a simulation-based algorithm -- dual ensemble Kalman filter (EnKF) -- to numerically approximate the solution of these problems.","The paper extends our previous work where the dual EnKF was applied in deterministic settings of the problem.","The theoretical results and algorithms are illustrated with numerical experiments."],"url":"http://arxiv.org/abs/2404.06696v1","category":"eess.SY"}
{"created":"2024-04-10 03:01:28","title":"Spiral Scanning and Self-Supervised Image Reconstruction Enable Ultra-Sparse Sampling Multispectral Photoacoustic Tomography","abstract":"Multispectral photoacoustic tomography (PAT) is an imaging modality that utilizes the photoacoustic effect to achieve non-invasive and high-contrast imaging of internal tissues. However, the hardware cost and computational demand of a multispectral PAT system consisting of up to thousands of detectors are huge. To address this challenge, we propose an ultra-sparse spiral sampling strategy for multispectral PAT, which we named U3S-PAT. Our strategy employs a sparse ring-shaped transducer that, when switching excitation wavelengths, simultaneously rotates and translates. This creates a spiral scanning pattern with multispectral angle-interlaced sampling. To solve the highly ill-conditioned image reconstruction problem, we propose a self-supervised learning method that is able to introduce structural information shared during spiral scanning. We simulate the proposed U3S-PAT method on a commercial PAT system and conduct in vivo animal experiments to verify its performance. The results show that even with a sparse sampling rate as low as 1/30, our U3S-PAT strategy achieves similar reconstruction and spectral unmixing accuracy as non-spiral dense sampling. Given its ability to dramatically reduce the time required for three-dimensional multispectral scanning, our U3S-PAT strategy has the potential to perform volumetric molecular imaging of dynamic biological activities.","sentences":["Multispectral photoacoustic tomography (PAT) is an imaging modality that utilizes the photoacoustic effect to achieve non-invasive and high-contrast imaging of internal tissues.","However, the hardware cost and computational demand of a multispectral PAT system consisting of up to thousands of detectors are huge.","To address this challenge, we propose an ultra-sparse spiral sampling strategy for multispectral PAT, which we named U3S-PAT.","Our strategy employs a sparse ring-shaped transducer that, when switching excitation wavelengths, simultaneously rotates and translates.","This creates a spiral scanning pattern with multispectral angle-interlaced sampling.","To solve the highly ill-conditioned image reconstruction problem, we propose a self-supervised learning method that is able to introduce structural information shared during spiral scanning.","We simulate the proposed U3S-PAT method on a commercial PAT system and conduct in vivo animal experiments to verify its performance.","The results show that even with a sparse sampling rate as low as 1/30, our U3S-PAT strategy achieves similar reconstruction and spectral unmixing accuracy as non-spiral dense sampling.","Given its ability to dramatically reduce the time required for three-dimensional multispectral scanning, our U3S-PAT strategy has the potential to perform volumetric molecular imaging of dynamic biological activities."],"url":"http://arxiv.org/abs/2404.06695v1","category":"eess.IV"}
{"created":"2024-04-10 02:40:17","title":"Perception-Oriented Video Frame Interpolation via Asymmetric Blending","abstract":"Previous methods for Video Frame Interpolation (VFI) have encountered challenges, notably the manifestation of blur and ghosting effects. These issues can be traced back to two pivotal factors: unavoidable motion errors and misalignment in supervision. In practice, motion estimates often prove to be error-prone, resulting in misaligned features. Furthermore, the reconstruction loss tends to bring blurry results, particularly in misaligned regions. To mitigate these challenges, we propose a new paradigm called PerVFI (Perception-oriented Video Frame Interpolation). Our approach incorporates an Asymmetric Synergistic Blending module (ASB) that utilizes features from both sides to synergistically blend intermediate features. One reference frame emphasizes primary content, while the other contributes complementary information. To impose a stringent constraint on the blending process, we introduce a self-learned sparse quasi-binary mask which effectively mitigates ghosting and blur artifacts in the output. Additionally, we employ a normalizing flow-based generator and utilize the negative log-likelihood loss to learn the conditional distribution of the output, which further facilitates the generation of clear and fine details. Experimental results validate the superiority of PerVFI, demonstrating significant improvements in perceptual quality compared to existing methods. Codes are available at \\url{https://github.com/mulns/PerVFI}","sentences":["Previous methods for Video Frame Interpolation (VFI) have encountered challenges, notably the manifestation of blur and ghosting effects.","These issues can be traced back to two pivotal factors: unavoidable motion errors and misalignment in supervision.","In practice, motion estimates often prove to be error-prone, resulting in misaligned features.","Furthermore, the reconstruction loss tends to bring blurry results, particularly in misaligned regions.","To mitigate these challenges, we propose a new paradigm called PerVFI (Perception-oriented Video Frame Interpolation).","Our approach incorporates an Asymmetric Synergistic Blending module (ASB) that utilizes features from both sides to synergistically blend intermediate features.","One reference frame emphasizes primary content, while the other contributes complementary information.","To impose a stringent constraint on the blending process, we introduce a self-learned sparse quasi-binary mask which effectively mitigates ghosting and blur artifacts in the output.","Additionally, we employ a normalizing flow-based generator and utilize the negative log-likelihood loss to learn the conditional distribution of the output, which further facilitates the generation of clear and fine details.","Experimental results validate the superiority of PerVFI, demonstrating significant improvements in perceptual quality compared to existing methods.","Codes are available at \\url{https://github.com/mulns/PerVFI}"],"url":"http://arxiv.org/abs/2404.06692v1","category":"cs.CV"}
{"created":"2024-04-10 02:03:14","title":"Unsupervised Visible-Infrared ReID via Pseudo-label Correction and Modality-level Alignment","abstract":"Unsupervised visible-infrared person re-identification (UVI-ReID) has recently gained great attention due to its potential for enhancing human detection in diverse environments without labeling. Previous methods utilize intra-modality clustering and cross-modality feature matching to achieve UVI-ReID. However, there exist two challenges: 1) noisy pseudo labels might be generated in the clustering process, and 2) the cross-modality feature alignment via matching the marginal distribution of visible and infrared modalities may misalign the different identities from two modalities. In this paper, we first conduct a theoretic analysis where an interpretable generalization upper bound is introduced. Based on the analysis, we then propose a novel unsupervised cross-modality person re-identification framework (PRAISE). Specifically, to address the first challenge, we propose a pseudo-label correction strategy that utilizes a Beta Mixture Model to predict the probability of mis-clustering based network's memory effect and rectifies the correspondence by adding a perceptual term to contrastive learning. Next, we introduce a modality-level alignment strategy that generates paired visible-infrared latent features and reduces the modality gap by aligning the labeling function of visible and infrared features to learn identity discriminative and modality-invariant features. Experimental results on two benchmark datasets demonstrate that our method achieves state-of-the-art performance than the unsupervised visible-ReID methods.","sentences":["Unsupervised visible-infrared person re-identification (UVI-ReID) has recently gained great attention due to its potential for enhancing human detection in diverse environments without labeling.","Previous methods utilize intra-modality clustering and cross-modality feature matching to achieve UVI-ReID.","However, there exist two challenges: 1) noisy pseudo labels might be generated in the clustering process, and 2) the cross-modality feature alignment via matching the marginal distribution of visible and infrared modalities may misalign the different identities from two modalities.","In this paper, we first conduct a theoretic analysis where an interpretable generalization upper bound is introduced.","Based on the analysis, we then propose a novel unsupervised cross-modality person re-identification framework (PRAISE).","Specifically, to address the first challenge, we propose a pseudo-label correction strategy that utilizes a Beta Mixture Model to predict the probability of mis-clustering based network's memory effect and rectifies the correspondence by adding a perceptual term to contrastive learning.","Next, we introduce a modality-level alignment strategy that generates paired visible-infrared latent features and reduces the modality gap by aligning the labeling function of visible and infrared features to learn identity discriminative and modality-invariant features.","Experimental results on two benchmark datasets demonstrate that our method achieves state-of-the-art performance than the unsupervised visible-ReID methods."],"url":"http://arxiv.org/abs/2404.06683v1","category":"cs.CV"}
{"created":"2024-04-10 02:02:51","title":"Learning Multidimensional Disentangled Representations of Instrumental Sounds for Musical Similarity Assessment","abstract":"To achieve a flexible recommendation and retrieval system, it is desirable to calculate music similarity by focusing on multiple partial elements of musical pieces and allowing the users to select the element they want to focus on. A previous study proposed using multiple individual networks for calculating music similarity based on each instrumental sound, but it is impractical to use each signal as a query in search systems. Using separated instrumental sounds alternatively resulted in less accuracy due to artifacts. In this paper, we propose a method to compute similarities focusing on each instrumental sound with a single network that takes mixed sounds as input instead of individual instrumental sounds. Specifically, we design a single similarity embedding space with disentangled dimensions for each instrument, extracted by Conditional Similarity Networks, which is trained by the triplet loss using masks. Experimental results have shown that (1) the proposed method can obtain more accurate feature representation than using individual networks using separated sounds as input, (2) each sub-embedding space can hold the characteristics of the corresponding instrument, and (3) the selection of similar musical pieces focusing on each instrumental sound by the proposed method can obtain human consent, especially in drums and guitar.","sentences":["To achieve a flexible recommendation and retrieval system, it is desirable to calculate music similarity by focusing on multiple partial elements of musical pieces and allowing the users to select the element they want to focus on.","A previous study proposed using multiple individual networks for calculating music similarity based on each instrumental sound, but it is impractical to use each signal as a query in search systems.","Using separated instrumental sounds alternatively resulted in less accuracy due to artifacts.","In this paper, we propose a method to compute similarities focusing on each instrumental sound with a single network that takes mixed sounds as input instead of individual instrumental sounds.","Specifically, we design a single similarity embedding space with disentangled dimensions for each instrument, extracted by Conditional Similarity Networks, which is trained by the triplet loss using masks.","Experimental results have shown that (1) the proposed method can obtain more accurate feature representation than using individual networks using separated sounds as input, (2) each sub-embedding space can hold the characteristics of the corresponding instrument, and (3) the selection of similar musical pieces focusing on each instrumental sound by the proposed method can obtain human consent, especially in drums and guitar."],"url":"http://arxiv.org/abs/2404.06682v1","category":"cs.SD"}
{"created":"2024-04-10 01:37:41","title":"Topological Feature Search Method for Multichannel EEG: Application in ADHD classification","abstract":"In recent years, the preliminary diagnosis of Attention Deficit Hyperactivity Disorder (ADHD) using electroencephalography (EEG) has garnered attention from researchers. EEG, known for its expediency and efficiency, plays a pivotal role in the diagnosis and treatment of ADHD. However, the non-stationarity of EEG signals and inter-subject variability pose challenges to the diagnostic and classification processes. Topological Data Analysis (TDA) offers a novel perspective for ADHD classification, diverging from traditional time-frequency domain features. Yet, conventional TDA models are restricted to single-channel time series and are susceptible to noise, leading to the loss of topological features in persistence diagrams.This paper presents an enhanced TDA approach applicable to multi-channel EEG in ADHD. Initially, optimal input parameters for multi-channel EEG are determined. Subsequently, each channel's EEG undergoes phase space reconstruction (PSR) followed by the utilization of k-Power Distance to Measure (k-PDTM) for approximating ideal point clouds. Then, multi-dimensional time series are re-embedded, and TDA is applied to obtain topological feature information. Gaussian function-based Multivariate Kernel Density Estimation (MKDE) is employed in the merger persistence diagram to filter out desired topological feature mappings. Finally, persistence image (PI) method is utilized to extract topological features, and the influence of various weighting functions on the results is discussed.The effectiveness of our method is evaluated using the IEEE ADHD dataset. Results demonstrate that the accuracy, sensitivity, and specificity reach 85.60%, 83.61%, and 88.33%, respectively. Compared to traditional TDA methods, our method was effectively improved and outperforms typical nonlinear descriptors. These findings indicate that our method exhibits higher precision and robustness.","sentences":["In recent years, the preliminary diagnosis of Attention Deficit Hyperactivity Disorder (ADHD) using electroencephalography (EEG) has garnered attention from researchers.","EEG, known for its expediency and efficiency, plays a pivotal role in the diagnosis and treatment of ADHD.","However, the non-stationarity of EEG signals and inter-subject variability pose challenges to the diagnostic and classification processes.","Topological Data Analysis (TDA) offers a novel perspective for ADHD classification, diverging from traditional time-frequency domain features.","Yet, conventional TDA models are restricted to single-channel time series and are susceptible to noise, leading to the loss of topological features in persistence diagrams.","This paper presents an enhanced TDA approach applicable to multi-channel EEG in ADHD.","Initially, optimal input parameters for multi-channel EEG are determined.","Subsequently, each channel's EEG undergoes phase space reconstruction (PSR) followed by the utilization of k-Power Distance to Measure (k-PDTM) for approximating ideal point clouds.","Then, multi-dimensional time series are re-embedded, and TDA is applied to obtain topological feature information.","Gaussian function-based Multivariate Kernel Density Estimation (MKDE) is employed in the merger persistence diagram to filter out desired topological feature mappings.","Finally, persistence image (PI) method is utilized to extract topological features, and the influence of various weighting functions on the results is discussed.","The effectiveness of our method is evaluated using the IEEE ADHD dataset.","Results demonstrate that the accuracy, sensitivity, and specificity reach 85.60%, 83.61%, and 88.33%, respectively.","Compared to traditional TDA methods, our method was effectively improved and outperforms typical nonlinear descriptors.","These findings indicate that our method exhibits higher precision and robustness."],"url":"http://arxiv.org/abs/2404.06676v1","category":"cs.LG"}
{"created":"2024-04-10 01:35:17","title":"Toward Cross-Layer Energy Optimizations in Machine Learning Systems","abstract":"The enormous energy consumption of machine learning (ML) and generative AI workloads shows no sign of waning, taking a toll on operating costs, power delivery, and environmental sustainability. Despite a long line of research on energy-efficient hardware, we found that software plays a critical role in ML energy optimization through two recent works: Zeus and Perseus. This is especially true for large language models (LLMs) because their model sizes and, therefore, energy demands are growing faster than hardware efficiency improvements. Therefore, we advocate for a cross-layer approach for energy optimizations in ML systems, where hardware provides architectural support that pushes energy-efficient software further, while software leverages and abstracts the hardware to develop techniques that bring hardware-agnostic energy-efficiency gains.","sentences":["The enormous energy consumption of machine learning (ML) and generative AI workloads shows no sign of waning, taking a toll on operating costs, power delivery, and environmental sustainability.","Despite a long line of research on energy-efficient hardware, we found that software plays a critical role in ML energy optimization through two recent works: Zeus and Perseus.","This is especially true for large language models (LLMs) because their model sizes and, therefore, energy demands are growing faster than hardware efficiency improvements.","Therefore, we advocate for a cross-layer approach for energy optimizations in ML systems, where hardware provides architectural support that pushes energy-efficient software further, while software leverages and abstracts the hardware to develop techniques that bring hardware-agnostic energy-efficiency gains."],"url":"http://arxiv.org/abs/2404.06675v1","category":"cs.LG"}
{"created":"2024-04-10 01:14:12","title":"What's Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs","abstract":"Best practices for high conflict conversations like counseling or customer support almost always include recommendations to paraphrase the previous speaker. Although paraphrase classification has received widespread attention in NLP, paraphrases are usually considered independent from context, and common models and datasets are not applicable to dialog settings. In this work, we investigate paraphrases in dialog (e.g., Speaker 1: \"That book is mine.\" becomes Speaker 2: \"That book is yours.\"). We provide an operationalization of context-dependent paraphrases, and develop a training for crowd-workers to classify paraphrases in dialog. We introduce a dataset with utterance pairs from NPR and CNN news interviews annotated for context-dependent paraphrases. To enable analyses on label variation, the dataset contains 5,581 annotations on 600 utterance pairs. We present promising results with in-context learning and with token classification models for automatic paraphrase detection in dialog.","sentences":["Best practices for high conflict conversations like counseling or customer support almost always include recommendations to paraphrase the previous speaker.","Although paraphrase classification has received widespread attention in NLP, paraphrases are usually considered independent from context, and common models and datasets are not applicable to dialog settings.","In this work, we investigate paraphrases in dialog (e.g., Speaker 1: \"That book is mine.\"","becomes Speaker 2: \"That book is yours.\").","We provide an operationalization of context-dependent paraphrases, and develop a training for crowd-workers to classify paraphrases in dialog.","We introduce a dataset with utterance pairs from NPR and CNN news interviews annotated for context-dependent paraphrases.","To enable analyses on label variation, the dataset contains 5,581 annotations on 600 utterance pairs.","We present promising results with in-context learning and with token classification models for automatic paraphrase detection in dialog."],"url":"http://arxiv.org/abs/2404.06670v1","category":"cs.CL"}
{"created":"2024-04-09 23:47:53","title":"Res-U2Net: Untrained Deep Learning for Phase Retrieval and Image Reconstruction","abstract":"Conventional deep learning-based image reconstruction methods require a large amount of training data which can be hard to obtain in practice. Untrained deep learning methods overcome this limitation by training a network to invert a physical model of the image formation process. Here we present a novel untrained Res-U2Net model for phase retrieval. We use the extracted phase information to determine changes in an object's surface and generate a mesh representation of its 3D structure. We compare the performance of Res-U2Net phase retrieval against UNet and U2Net using images from the GDXRAY dataset.","sentences":["Conventional deep learning-based image reconstruction methods require a large amount of training data which can be hard to obtain in practice.","Untrained deep learning methods overcome this limitation by training a network to invert a physical model of the image formation process.","Here we present a novel untrained Res-U2Net model for phase retrieval.","We use the extracted phase information to determine changes in an object's surface and generate a mesh representation of its 3D structure.","We compare the performance of Res-U2Net phase retrieval against UNet and U2Net using images from the GDXRAY dataset."],"url":"http://arxiv.org/abs/2404.06657v1","category":"eess.IV"}
{"created":"2024-04-09 23:24:19","title":"FlameFinder: Illuminating Obscured Fire through Smoke with Attentive Deep Metric Learning","abstract":"FlameFinder is a deep metric learning (DML) framework designed to accurately detect flames, even when obscured by smoke, using thermal images from firefighter drones during wildfire monitoring. Traditional RGB cameras struggle in such conditions, but thermal cameras can capture smoke-obscured flame features. However, they lack absolute thermal reference points, leading to false positives.To address this issue, FlameFinder utilizes paired thermal-RGB images for training. By learning latent flame features from smoke-free samples, the model becomes less biased towards relative thermal gradients. In testing, it identifies flames in smoky patches by analyzing their equivalent thermal-domain distribution. This method improves performance using both supervised and distance-based clustering metrics.The framework incorporates a flame segmentation method and a DML-aided detection framework. This includes utilizing center loss (CL), triplet center loss (TCL), and triplet cosine center loss (TCCL) to identify optimal cluster representatives for classification. However, the dominance of center loss over the other losses leads to the model missing features sensitive to them. To address this limitation, an attention mechanism is proposed. This mechanism allows for non-uniform feature contribution, amplifying the critical role of cosine and triplet loss in the DML framework. Additionally, it improves interpretability, class discrimination, and decreases intra-class variance. As a result, the proposed model surpasses the baseline by 4.4% in the FLAME2 dataset and 7% in the FLAME3 dataset for unobscured flame detection accuracy. Moreover, it demonstrates enhanced class separation in obscured scenarios compared to VGG19, ResNet18, and three backbone models tailored for flame detection.","sentences":["FlameFinder is a deep metric learning (DML) framework designed to accurately detect flames, even when obscured by smoke, using thermal images from firefighter drones during wildfire monitoring.","Traditional RGB cameras struggle in such conditions, but thermal cameras can capture smoke-obscured flame features.","However, they lack absolute thermal reference points, leading to false positives.","To address this issue, FlameFinder utilizes paired thermal-RGB images for training.","By learning latent flame features from smoke-free samples, the model becomes less biased towards relative thermal gradients.","In testing, it identifies flames in smoky patches by analyzing their equivalent thermal-domain distribution.","This method improves performance using both supervised and distance-based clustering metrics.","The framework incorporates a flame segmentation method and a DML-aided detection framework.","This includes utilizing center loss (CL), triplet center loss (TCL), and triplet cosine center loss (TCCL) to identify optimal cluster representatives for classification.","However, the dominance of center loss over the other losses leads to the model missing features sensitive to them.","To address this limitation, an attention mechanism is proposed.","This mechanism allows for non-uniform feature contribution, amplifying the critical role of cosine and triplet loss in the DML framework.","Additionally, it improves interpretability, class discrimination, and decreases intra-class variance.","As a result, the proposed model surpasses the baseline by 4.4% in the FLAME2 dataset and 7% in the FLAME3 dataset for unobscured flame detection accuracy.","Moreover, it demonstrates enhanced class separation in obscured scenarios compared to VGG19, ResNet18, and three backbone models tailored for flame detection."],"url":"http://arxiv.org/abs/2404.06653v1","category":"cs.CV"}
{"created":"2024-04-09 18:46:56","title":"Detecting Refactoring Commits in Machine Learning Python Projects: A Machine Learning-Based Approach","abstract":"Refactoring enhances software quality without altering its functional behaviors. Understanding the refactoring activities of developers is crucial to improving software maintainability. With the increasing use of machine learning (ML) libraries and frameworks, maximizing their maintainability is crucial. Due to the data-driven nature of ML projects, they often undergo different refactoring operations (e.g., data manipulation), for which existing refactoring tools lack ML-specific detection capabilities. Furthermore, a large number of ML libraries are written in Python, which has limited tools for refactoring detection. PyRef, a rule-based and state-of-the-art tool for Python refactoring detection, can identify 11 types of refactoring operations. In comparison, Rminer can detect 99 types of refactoring for Java projects. We introduce MLRefScanner, a prototype tool that applies machine-learning techniques to detect refactoring commits in ML Python projects. MLRefScanner identifies commits with both ML-specific and general refactoring operations. Evaluating MLRefScanner on 199 ML projects demonstrates its superior performance compared to state-of-the-art approaches, achieving an overall 94% precision and 82% recall. Combining it with PyRef further boosts performance to 95% precision and 99% recall. Our study highlights the potential of ML-driven approaches in detecting refactoring across diverse programming languages and technical domains, addressing the limitations of rule-based detection methods.","sentences":["Refactoring enhances software quality without altering its functional behaviors.","Understanding the refactoring activities of developers is crucial to improving software maintainability.","With the increasing use of machine learning (ML) libraries and frameworks, maximizing their maintainability is crucial.","Due to the data-driven nature of ML projects, they often undergo different refactoring operations (e.g., data manipulation), for which existing refactoring tools lack ML-specific detection capabilities.","Furthermore, a large number of ML libraries are written in Python, which has limited tools for refactoring detection.","PyRef, a rule-based and state-of-the-art tool for Python refactoring detection, can identify 11 types of refactoring operations.","In comparison, Rminer can detect 99 types of refactoring for Java projects.","We introduce MLRefScanner, a prototype tool that applies machine-learning techniques to detect refactoring commits in ML Python projects.","MLRefScanner identifies commits with both ML-specific and general refactoring operations.","Evaluating MLRefScanner on 199 ML projects demonstrates its superior performance compared to state-of-the-art approaches, achieving an overall 94% precision and 82% recall.","Combining it with PyRef further boosts performance to 95% precision and 99% recall.","Our study highlights the potential of ML-driven approaches in detecting refactoring across diverse programming languages and technical domains, addressing the limitations of rule-based detection methods."],"url":"http://arxiv.org/abs/2404.06572v1","category":"cs.SE"}
{"created":"2024-04-09 18:00:01","title":"Learning to rank quantum circuits for hardware-optimized performance enhancement","abstract":"We introduce and experimentally test a machine-learning-based method for ranking logically equivalent quantum circuits based on expected performance estimates derived from a training procedure conducted on real hardware. We apply our method to the problem of layout selection, in which abstracted qubits are assigned to physical qubits on a given device. Circuit measurements performed on IBM hardware indicate that the maximum and median fidelities of logically equivalent layouts can differ by an order of magnitude. We introduce a circuit score used for ranking that is parameterized in terms of a physics-based, phenomenological error model whose parameters are fit by training a ranking-loss function over a measured dataset. The dataset consists of quantum circuits exhibiting a diversity of structures and executed on IBM hardware, allowing the model to incorporate the contextual nature of real device noise and errors without the need to perform an exponentially costly tomographic protocol. We perform model training and execution on the 16-qubit ibmq_guadalupe device and compare our method to two common approaches: random layout selection and a publicly available baseline called Mapomatic. Our model consistently outperforms both approaches, predicting layouts that exhibit lower noise and higher performance. In particular, we find that our best model leads to a $1.8\\times$ reduction in selection error when compared to the baseline approach and a $3.2\\times$ reduction when compared to random selection. Beyond delivering a new form of predictive quantum characterization, verification, and validation, our results reveal the specific way in which context-dependent and coherent gate errors appear to dominate the divergence from performance estimates extrapolated from simple proxy measures.","sentences":["We introduce and experimentally test a machine-learning-based method for ranking logically equivalent quantum circuits based on expected performance estimates derived from a training procedure conducted on real hardware.","We apply our method to the problem of layout selection, in which abstracted qubits are assigned to physical qubits on a given device.","Circuit measurements performed on IBM hardware indicate that the maximum and median fidelities of logically equivalent layouts can differ by an order of magnitude.","We introduce a circuit score used for ranking that is parameterized in terms of a physics-based, phenomenological error model whose parameters are fit by training a ranking-loss function over a measured dataset.","The dataset consists of quantum circuits exhibiting a diversity of structures and executed on IBM hardware, allowing the model to incorporate the contextual nature of real device noise and errors without the need to perform an exponentially costly tomographic protocol.","We perform model training and execution on the 16-qubit ibmq_guadalupe device and compare our method to two common approaches: random layout selection and a publicly available baseline called Mapomatic.","Our model consistently outperforms both approaches, predicting layouts that exhibit lower noise and higher performance.","In particular, we find that our best model leads to a $1.8\\times$ reduction in selection error when compared to the baseline approach and a $3.2\\times$ reduction when compared to random selection.","Beyond delivering a new form of predictive quantum characterization, verification, and validation, our results reveal the specific way in which context-dependent and coherent gate errors appear to dominate the divergence from performance estimates extrapolated from simple proxy measures."],"url":"http://arxiv.org/abs/2404.06535v1","category":"quant-ph"}
{"created":"2024-04-09 17:34:19","title":"RhythmMamba: Fast Remote Physiological Measurement with Arbitrary Length Videos","abstract":"Remote photoplethysmography (rPPG) is a non-contact method for detecting physiological signals from facial videos, holding great potential in various applications such as healthcare, affective computing, and anti-spoofing. Existing deep learning methods struggle to address two core issues of rPPG simultaneously: extracting weak rPPG signals from video segments with large spatiotemporal redundancy and understanding the periodic patterns of rPPG among long contexts. This represents a trade-off between computational complexity and the ability to capture long-range dependencies, posing a challenge for rPPG that is suitable for deployment on mobile devices. Based on the in-depth exploration of Mamba's comprehension of spatial and temporal information, this paper introduces RhythmMamba, an end-to-end Mamba-based method that employs multi-temporal Mamba to constrain both periodic patterns and short-term trends, coupled with frequency domain feed-forward to enable Mamba to robustly understand the quasi-periodic patterns of rPPG. Extensive experiments show that RhythmMamba achieves state-of-the-art performance with reduced parameters and lower computational complexity. The proposed RhythmMamba can be applied to video segments of any length without performance degradation. The codes are available at https://github.com/zizheng-guo/RhythmMamba.","sentences":["Remote photoplethysmography (rPPG) is a non-contact method for detecting physiological signals from facial videos, holding great potential in various applications such as healthcare, affective computing, and anti-spoofing.","Existing deep learning methods struggle to address two core issues of rPPG simultaneously: extracting weak rPPG signals from video segments with large spatiotemporal redundancy and understanding the periodic patterns of rPPG among long contexts.","This represents a trade-off between computational complexity and the ability to capture long-range dependencies, posing a challenge for rPPG that is suitable for deployment on mobile devices.","Based on the in-depth exploration of Mamba's comprehension of spatial and temporal information, this paper introduces RhythmMamba, an end-to-end Mamba-based method that employs multi-temporal Mamba to constrain both periodic patterns and short-term trends, coupled with frequency domain feed-forward to enable Mamba to robustly understand the quasi-periodic patterns of rPPG.","Extensive experiments show that RhythmMamba achieves state-of-the-art performance with reduced parameters and lower computational complexity.","The proposed RhythmMamba can be applied to video segments of any length without performance degradation.","The codes are available at https://github.com/zizheng-guo/RhythmMamba."],"url":"http://arxiv.org/abs/2404.06483v1","category":"cs.CV"}
{"created":"2024-04-09 17:31:18","title":"GeoDirDock: Guiding Docking Along Geodesic Paths","abstract":"This work introduces GeoDirDock (GDD), a novel approach to molecular docking that enhances the accuracy and physical plausibility of ligand docking predictions. GDD guides the denoising process of a diffusion model along geodesic paths within multiple spaces representing translational, rotational, and torsional degrees of freedom. Our method leverages expert knowledge to direct the generative modeling process, specifically targeting desired protein-ligand interaction regions. We demonstrate that GDD significantly outperforms existing blind docking methods in terms of RMSD accuracy and physicochemical pose realism. Our results indicate that incorporating domain expertise into the diffusion process leads to more biologically relevant docking predictions. Additionally, we explore the potential of GDD for lead optimization in drug discovery through angle transfer in maximal common substructure (MCS) docking, showcasing its capability to predict ligand orientations for chemically similar compounds accurately.","sentences":["This work introduces GeoDirDock (GDD), a novel approach to molecular docking that enhances the accuracy and physical plausibility of ligand docking predictions.","GDD guides the denoising process of a diffusion model along geodesic paths within multiple spaces representing translational, rotational, and torsional degrees of freedom.","Our method leverages expert knowledge to direct the generative modeling process, specifically targeting desired protein-ligand interaction regions.","We demonstrate that GDD significantly outperforms existing blind docking methods in terms of RMSD accuracy and physicochemical pose realism.","Our results indicate that incorporating domain expertise into the diffusion process leads to more biologically relevant docking predictions.","Additionally, we explore the potential of GDD for lead optimization in drug discovery through angle transfer in maximal common substructure (MCS) docking, showcasing its capability to predict ligand orientations for chemically similar compounds accurately."],"url":"http://arxiv.org/abs/2404.06481v1","category":"q-bio.BM"}
{"created":"2024-04-09 17:14:41","title":"Hyperparameter Selection in Continual Learning","abstract":"In continual learning (CL) -- where a learner trains on a stream of data -- standard hyperparameter optimisation (HPO) cannot be applied, as a learner does not have access to all of the data at the same time. This has prompted the development of CL-specific HPO frameworks. The most popular way to tune hyperparameters in CL is to repeatedly train over the whole data stream with different hyperparameter settings. However, this end-of-training HPO is unrealistic as in practice a learner can only see the stream once. Hence, there is an open question: what HPO framework should a practitioner use for a CL problem in reality? This paper answers this question by evaluating several realistic HPO frameworks. We find that all the HPO frameworks considered, including end-of-training HPO, perform similarly. We therefore advocate using the realistic and most computationally efficient method: fitting the hyperparameters on the first task and then fixing them throughout training.","sentences":["In continual learning (CL) -- where a learner trains on a stream of data -- standard hyperparameter optimisation (HPO) cannot be applied, as a learner does not have access to all of the data at the same time.","This has prompted the development of CL-specific HPO frameworks.","The most popular way to tune hyperparameters in CL is to repeatedly train over the whole data stream with different hyperparameter settings.","However, this end-of-training HPO is unrealistic as in practice a learner can only see the stream once.","Hence, there is an open question: what HPO framework should a practitioner use for a CL problem in reality?","This paper answers this question by evaluating several realistic HPO frameworks.","We find that all the HPO frameworks considered, including end-of-training HPO, perform similarly.","We therefore advocate using the realistic and most computationally efficient method: fitting the hyperparameters on the first task and then fixing them throughout training."],"url":"http://arxiv.org/abs/2404.06466v1","category":"cs.LG"}
{"created":"2024-04-09 15:07:25","title":"Model Generation from Requirements with LLMs: an Exploratory Study","abstract":"Complementing natural language (NL) requirements with graphical models can improve stakeholders' communication and provide directions for system design. However, creating models from requirements involves manual effort. The advent of generative large language models (LLMs), ChatGPT being a notable example, offers promising avenues for automated assistance in model generation. This paper investigates the capability of ChatGPT to generate a specific type of model, i.e., UML sequence diagrams, from NL requirements. We conduct a qualitative study in which we examine the sequence diagrams generated by ChatGPT for 28 requirements documents of various types and from different domains. Observations from the analysis of the generated diagrams have systematically been captured through evaluation logs, and categorized through thematic analysis. Our results indicate that, although the models generally conform to the standard and exhibit a reasonable level of understandability, their completeness and correctness with respect to the specified requirements often present challenges. This issue is particularly pronounced in the presence of requirements smells, such as ambiguity and inconsistency. The insights derived from this study can influence the practical utilization of LLMs in the RE process, and open the door to novel RE-specific prompting strategies targeting effective model generation.","sentences":["Complementing natural language (NL) requirements with graphical models can improve stakeholders' communication and provide directions for system design.","However, creating models from requirements involves manual effort.","The advent of generative large language models (LLMs), ChatGPT being a notable example, offers promising avenues for automated assistance in model generation.","This paper investigates the capability of ChatGPT to generate a specific type of model, i.e., UML sequence diagrams, from NL requirements.","We conduct a qualitative study in which we examine the sequence diagrams generated by ChatGPT for 28 requirements documents of various types and from different domains.","Observations from the analysis of the generated diagrams have systematically been captured through evaluation logs, and categorized through thematic analysis.","Our results indicate that, although the models generally conform to the standard and exhibit a reasonable level of understandability, their completeness and correctness with respect to the specified requirements often present challenges.","This issue is particularly pronounced in the presence of requirements smells, such as ambiguity and inconsistency.","The insights derived from this study can influence the practical utilization of LLMs in the RE process, and open the door to novel RE-specific prompting strategies targeting effective model generation."],"url":"http://arxiv.org/abs/2404.06371v1","category":"cs.SE"}
{"created":"2024-04-10 15:53:41","title":"Classical simulation and quantum resource theory of non-Gaussian optics","abstract":"We propose efficient algorithms for classically simulating Gaussian unitaries and measurements applied to non-Gaussian initial states. The constructions are based on decomposing the non-Gaussian states into linear combinations of Gaussian states. We use an extension of the covariance matrix formalism to efficiently track relative phases in the superpositions of Gaussian states. We get an exact simulation that scales quadratically with the number of Gaussian states required to represent the initial state and an approximate simulation algorithm that scales linearly with the degree. We define measures of non-Gaussianty quantifying this simulation cost, which we call the Gaussian rank and the Gaussian extent. From the perspective of quantum resource theories, we investigate the properties of this type of non-Gaussianity measure and compute optimal decomposition for states relevant to continuous-variable quantum computing.","sentences":["We propose efficient algorithms for classically simulating Gaussian unitaries and measurements applied to non-Gaussian initial states.","The constructions are based on decomposing the non-Gaussian states into linear combinations of Gaussian states.","We use an extension of the covariance matrix formalism to efficiently track relative phases in the superpositions of Gaussian states.","We get an exact simulation that scales quadratically with the number of Gaussian states required to represent the initial state and an approximate simulation algorithm that scales linearly with the degree.","We define measures of non-Gaussianty quantifying this simulation cost, which we call the Gaussian rank and the Gaussian extent.","From the perspective of quantum resource theories, we investigate the properties of this type of non-Gaussianity measure and compute optimal decomposition for states relevant to continuous-variable quantum computing."],"url":"http://arxiv.org/abs/2404.07115v1","category":"quant-ph"}
{"created":"2024-04-10 14:04:06","title":"POD Suboptimal Control of Evolution Problems: Theory and Applications","abstract":"The work is organized as follows. First an introduction is given in Chapter 1. In Chapter 2 we introduce the POD method in finite and infinite-dimensional Hilbert spaces and discuss various applications. Chapter 3 is devoted to to POD-based Galerkin schemes for evolution problems. Mainly, we study linear problems taking different discretization methods into account. We provide a certified a-priori and a-posteriori error analysis. Furthermore, the numerical realizations are explained and illustrated by test examples. Quadratic programming problems governed by liner evolution problems are investigated in Chapter 4. As in Chapter 3 we present a certified a-priori and a-posteriori error analysis. Moreover, we discuss basis update strategies. In Chapter 5 we give an outlook to further directions in reduced-order modeling in optimal control and optimization. More precisely, a nonlinear optimal control problem is studied. Moreover, state-constrained optimization problems are solved by a tailored combination of primal-dual active set methods and POD-aesed reduced-order modeling. Furthermore, POD Galerkin methods for multiobjective optimal control problems are investigated. Finally, some required theoretical foundations are summarized in the appendix.","sentences":["The work is organized as follows.","First an introduction is given in Chapter 1.","In Chapter 2 we introduce the POD method in finite and infinite-dimensional Hilbert spaces and discuss various applications.","Chapter 3 is devoted to to POD-based Galerkin schemes for evolution problems.","Mainly, we study linear problems taking different discretization methods into account.","We provide a certified a-priori and a-posteriori error analysis.","Furthermore, the numerical realizations are explained and illustrated by test examples.","Quadratic programming problems governed by liner evolution problems are investigated in Chapter 4.","As in Chapter 3 we present a certified a-priori and a-posteriori error analysis.","Moreover, we discuss basis update strategies.","In Chapter 5 we give an outlook to further directions in reduced-order modeling in optimal control and optimization.","More precisely, a nonlinear optimal control problem is studied.","Moreover, state-constrained optimization problems are solved by a tailored combination of primal-dual active set methods and POD-aesed reduced-order modeling.","Furthermore, POD Galerkin methods for multiobjective optimal control problems are investigated.","Finally, some required theoretical foundations are summarized in the appendix."],"url":"http://arxiv.org/abs/2404.07015v1","category":"math.OC"}
{"created":"2024-04-10 09:38:13","title":"A scalable 2-local architecture for quantum annealing of all-to-all Ising models","abstract":"Achieving dense connectivities is a challenge for most quantum computing platforms today, and a particularly crucial one for the case of quantum annealing applications. In this context, we present a scalable architecture for quantum annealers defined on a graph of degree $d=3$ and containing exclusively 2-local interactions to realize an all-to-all connected Ising model. This amounts to an efficient braiding of logical chains of qubits which can be derived from a description of the problem in terms of triangles. We also devise strategies to address the challenges of scalable architectures, such as the faster shrinking of the gap due to the larger physical Hilbert space, based on driver Hamiltonians more suited to the symmetries of the logical solution space. We thus show an alternative route to scale up devices dedicated to classical optimization tasks within the quantum annealing paradigm.","sentences":["Achieving dense connectivities is a challenge for most quantum computing platforms today, and a particularly crucial one for the case of quantum annealing applications.","In this context, we present a scalable architecture for quantum annealers defined on a graph of degree $d=3$ and containing exclusively 2-local interactions to realize an all-to-all connected Ising model.","This amounts to an efficient braiding of logical chains of qubits which can be derived from a description of the problem in terms of triangles.","We also devise strategies to address the challenges of scalable architectures, such as the faster shrinking of the gap due to the larger physical Hilbert space, based on driver Hamiltonians more suited to the symmetries of the logical solution space.","We thus show an alternative route to scale up devices dedicated to classical optimization tasks within the quantum annealing paradigm."],"url":"http://arxiv.org/abs/2404.06861v1","category":"quant-ph"}
{"created":"2024-04-10 06:09:53","title":"A mid-infrared Brillouin laser using ultra-high-Q on-chip resonators","abstract":"Ultra-high-Q optical resonators have facilitated recent advancements in on-chip photonics by effectively harnessing nonlinear phenomena providing useful functionalities. While these breakthroughs, primarily focused on the near-infrared region, have extended interest to longer wavelengths holding importance for monitoring and manipulating molecules, the absence of ultra-high-Q resonators in this region remains a significant challenge. Here, we have developed on-chip microresonators with a remarkable Q-factor of 38 million, surpassing previous mid-infrared records by over 30 times. Employing innovative fabrication techniques, including the spontaneous formation of light-guiding geometries during material deposition, resonators with internal multilayer structures have been seamlessly created and passivated with chalcogenide glasses within a single chamber. Major loss factors, especially airborne-chemical absorption, were thoroughly investigated and mitigated by extensive optimization of resonator geometries and fabrication procedures. This allowed us to access the fundamental loss performance offered by doubly purified chalcogenide glass sources, as demonstrated in their fiber form. Exploiting this ultra-high-Q resonator, we successfully demonstrated Brillouin lasing on a chip for the first time in the mid-infrared, with a threshold power of 91.9 {\\mu}W and a theoretical Schawlow-Townes linewidth of 83.45 Hz, far surpassing carrier phase noise. Our results showcase the effective integration of cavity-enhanced optical nonlinearities into on-chip mid-infrared photonics.","sentences":["Ultra-high-Q optical resonators have facilitated recent advancements in on-chip photonics by effectively harnessing nonlinear phenomena providing useful functionalities.","While these breakthroughs, primarily focused on the near-infrared region, have extended interest to longer wavelengths holding importance for monitoring and manipulating molecules, the absence of ultra-high-Q resonators in this region remains a significant challenge.","Here, we have developed on-chip microresonators with a remarkable Q-factor of 38 million, surpassing previous mid-infrared records by over 30 times.","Employing innovative fabrication techniques, including the spontaneous formation of light-guiding geometries during material deposition, resonators with internal multilayer structures have been seamlessly created and passivated with chalcogenide glasses within a single chamber.","Major loss factors, especially airborne-chemical absorption, were thoroughly investigated and mitigated by extensive optimization of resonator geometries and fabrication procedures.","This allowed us to access the fundamental loss performance offered by doubly purified chalcogenide glass sources, as demonstrated in their fiber form.","Exploiting this ultra-high-Q resonator, we successfully demonstrated Brillouin lasing on a chip for the first time in the mid-infrared, with a threshold power of 91.9 {\\mu}W and a theoretical Schawlow-Townes linewidth of 83.45 Hz, far surpassing carrier phase noise.","Our results showcase the effective integration of cavity-enhanced optical nonlinearities into on-chip mid-infrared photonics."],"url":"http://arxiv.org/abs/2404.06764v1","category":"physics.optics"}
{"created":"2024-04-10 04:56:27","title":"The Physics of Antimicrobial Activity of Ionic Liquids","abstract":"The bactericidal potency of ionic liquids (ILs) is well-established, yet their precise mechanism of action remains elusive. Here, we show evidence that the bactericidal action of ILs primarily involves permeabilizing the bacterial cell membrane. Our findings reveal that ILs exert their effects by directly interacting with the lipid bilayer and enhancing the membrane dynamics. Lateral lipid diffusion is accelerated which in turn augments membrane permeability, ultimately leading to bacterial death. Furthermore, our results establish a significant connection: an increase in the alkyl chain length of ILs correlates with a notable enhancement in both lipid lateral diffusion and antimicrobial potency. This underscores a compelling correlation between membrane dynamics and antimicrobial effectiveness, providing valuable insights for the rational design and optimization of IL-based antimicrobial agents in healthcare applications.","sentences":["The bactericidal potency of ionic liquids (ILs) is well-established, yet their precise mechanism of action remains elusive.","Here, we show evidence that the bactericidal action of ILs primarily involves permeabilizing the bacterial cell membrane.","Our findings reveal that ILs exert their effects by directly interacting with the lipid bilayer and enhancing the membrane dynamics.","Lateral lipid diffusion is accelerated which in turn augments membrane permeability, ultimately leading to bacterial death.","Furthermore, our results establish a significant connection: an increase in the alkyl chain length of ILs correlates with a notable enhancement in both lipid lateral diffusion and antimicrobial potency.","This underscores a compelling correlation between membrane dynamics and antimicrobial effectiveness, providing valuable insights for the rational design and optimization of IL-based antimicrobial agents in healthcare applications."],"url":"http://arxiv.org/abs/2404.06739v1","category":"cond-mat.soft"}
{"created":"2024-04-10 02:22:38","title":"Fast and Accurate Relative Motion Tracking for Two Industrial Robots","abstract":"Industrial robotic applications such as spraying, welding, and additive manufacturing frequently require fast, accurate, and uniform motion along a 3D spatial curve. To increase process throughput, some manufacturers propose a dual-robot setup to overcome the speed limitation of a single robot. Industrial robot motion is programmed through waypoints connected by motion primitives (Cartesian linear and circular paths and linear joint paths at constant Cartesian speed). The actual robot motion is affected by the blending between these motion primitives and the pose of the robot (an outstretched/close to singularity pose tends to have larger path-tracking errors). Choosing the waypoints and the speed along each motion segment to achieve the performance requirement is challenging. At present, there is no automated solution, and laborious manual tuning by robot experts is needed to approach the desired performance. In this paper, we present a systematic three-step approach to designing and programming a dual-robot system to optimize system performance. The first step is to select the relative placement between the two robots based on the specified relative motion path. The second step is to select the relative waypoints and the motion primitives. The final step is to update the waypoints iteratively based on the actual relative motion. Waypoint iteration is first executed in simulation and then completed using the actual robots. For performance measures, we use the mean path speed subject to the relative position and orientation constraints and the path speed uniformity constraint. We have demonstrated the effectiveness of this method with ABB and FANUC robots on two challenging test curves. The performance improvement over the current industrial practice baseline is over 300%. Compared to the optimized single-arm case that we have previously reported, the improvement is over 14%.","sentences":["Industrial robotic applications such as spraying, welding, and additive manufacturing frequently require fast, accurate, and uniform motion along a 3D spatial curve.","To increase process throughput, some manufacturers propose a dual-robot setup to overcome the speed limitation of a single robot.","Industrial robot motion is programmed through waypoints connected by motion primitives (Cartesian linear and circular paths and linear joint paths at constant Cartesian speed).","The actual robot motion is affected by the blending between these motion primitives and the pose of the robot (an outstretched/close to singularity pose tends to have larger path-tracking errors).","Choosing the waypoints and the speed along each motion segment to achieve the performance requirement is challenging.","At present, there is no automated solution, and laborious manual tuning by robot experts is needed to approach the desired performance.","In this paper, we present a systematic three-step approach to designing and programming a dual-robot system to optimize system performance.","The first step is to select the relative placement between the two robots based on the specified relative motion path.","The second step is to select the relative waypoints and the motion primitives.","The final step is to update the waypoints iteratively based on the actual relative motion.","Waypoint iteration is first executed in simulation and then completed using the actual robots.","For performance measures, we use the mean path speed subject to the relative position and orientation constraints and the path speed uniformity constraint.","We have demonstrated the effectiveness of this method with ABB and FANUC robots on two challenging test curves.","The performance improvement over the current industrial practice baseline is over 300%.","Compared to the optimized single-arm case that we have previously reported, the improvement is over 14%."],"url":"http://arxiv.org/abs/2404.06687v1","category":"cs.RO"}
{"created":"2024-04-09 22:34:38","title":"On Frobenius-Schur exponent bounds","abstract":"Here we study bounds on the Frobenius-Schur exponent of spherical fusion categories based on their global dimension generalizing bounds from the representation theory of finite-dimensional quasi-Hopf algebras. Our main result is that if the Frobenius-Schur exponent of a modular fusion category is a prime power for some prime integer $p$, then it is bounded by the norm of its global dimension when $p$ is odd, and four times the norm of its global dimension when $p=2$; these bounds are optimal. If one assumes in addition pseudounitarity, the categories achieving the optimal bound are completely described and we attain similar bounds and classifications for arbitrary spherical fusion categories. This proof includes an explicit classification of modular fusion categories of Frobenius-Perron dimension $p^5$; all examples which are not pointed are constructed explicitly from the Ising categories and the representation theory of extraspecial $p$-groups.","sentences":["Here we study bounds on the Frobenius-Schur exponent of spherical fusion categories based on their global dimension generalizing bounds from the representation theory of finite-dimensional quasi-Hopf algebras.","Our main result is that if the Frobenius-Schur exponent of a modular fusion category is a prime power for some prime integer $p$, then it is bounded by the norm of its global dimension when $p$ is odd, and four times the norm of its global dimension when $p=2$; these bounds are optimal.","If one assumes in addition pseudounitarity, the categories achieving the optimal bound are completely described and we attain similar bounds and classifications for arbitrary spherical fusion categories.","This proof includes an explicit classification of modular fusion categories of Frobenius-Perron dimension $p^5$; all examples which are not pointed are constructed explicitly from the Ising categories and the representation theory of extraspecial $p$-groups."],"url":"http://arxiv.org/abs/2404.06643v1","category":"math.QA"}
{"created":"2024-04-09 22:24:55","title":"Modification of Jet Velocities in an Explosively Loaded Copper Target with a Conical Defect","abstract":"In this work, the design and execution of an experiment with the goal of demonstrating control over the evolution of a copper jet is described. Simulations show that when using simple multi-material buffers placed between a copper target with a conical defect and a cylinder of high-explosive, a variety of jetting behaviors occur based on material placement, including both jet velocity augmentation and mitigation. A parameter sweep was performed to determine optimal buffer designs in two configurations. Experiments using the optimal buffer designs verified the effectiveness of the buffer and validated the modeling.","sentences":["In this work, the design and execution of an experiment with the goal of demonstrating control over the evolution of a copper jet is described.","Simulations show that when using simple multi-material buffers placed between a copper target with a conical defect and a cylinder of high-explosive, a variety of jetting behaviors occur based on material placement, including both jet velocity augmentation and mitigation.","A parameter sweep was performed to determine optimal buffer designs in two configurations.","Experiments using the optimal buffer designs verified the effectiveness of the buffer and validated the modeling."],"url":"http://arxiv.org/abs/2404.06640v1","category":"physics.app-ph"}
{"created":"2024-04-09 21:52:12","title":"Relative entropy bounds for sampling with and without replacement","abstract":"Sharp, nonasymptotic bounds are obtained for the relative entropy between the distributions of sampling with and without replacement from an urn with balls of $c\\geq 2$ colors. Our bounds are asymptotically tight in certain regimes and, unlike previous results, they depend on the number of balls of each colour in the urn. The connection of these results with finite de Finetti-style theorems is explored, and it is observed that a sampling bound due to Stam (1978) combined with the convexity of relative entropy yield a new finite de Finetti bound in relative entropy, which achieves the optimal asymptotic convergence rate.","sentences":["Sharp, nonasymptotic bounds are obtained for the relative entropy between the distributions of sampling with and without replacement from an urn with balls of $c\\geq 2$ colors.","Our bounds are asymptotically tight in certain regimes and, unlike previous results, they depend on the number of balls of each colour in the urn.","The connection of these results with finite de Finetti-style theorems is explored, and it is observed that a sampling bound due to Stam (1978) combined with the convexity of relative entropy yield a new finite de Finetti bound in relative entropy, which achieves the optimal asymptotic convergence rate."],"url":"http://arxiv.org/abs/2404.06632v1","category":"math.PR"}
{"created":"2024-04-09 19:18:54","title":"NotNets: Accelerating Microservices by Bypassing the Network","abstract":"Remote procedure calls are the workhorse of distributed systems. However, as software engineering trends, such as micro-services and serverless computing, push applications towards ever finer-grained decompositions, the overhead of RPC-based communication is becoming too great to bear. In this paper, we argue that point solutions that attempt to optimize one aspect of RPC logic are unlikely to mitigate these ballooning communication costs. Rather, we need a dramatic reappraisal of how we provide communication. Towards this end, we propose to emulate message-passing RPCs by sharing message payloads and metadata on CXL 3.0-backed far memory. We provide initial evidence of feasibility and analyze the expected benefits.","sentences":["Remote procedure calls are the workhorse of distributed systems.","However, as software engineering trends, such as micro-services and serverless computing, push applications towards ever finer-grained decompositions, the overhead of RPC-based communication is becoming too great to bear.","In this paper, we argue that point solutions that attempt to optimize one aspect of RPC logic are unlikely to mitigate these ballooning communication costs.","Rather, we need a dramatic reappraisal of how we provide communication.","Towards this end, we propose to emulate message-passing RPCs by sharing message payloads and metadata on CXL 3.0-backed far memory.","We provide initial evidence of feasibility and analyze the expected benefits."],"url":"http://arxiv.org/abs/2404.06581v1","category":"cs.DC"}
{"created":"2024-04-09 19:06:26","title":"Replication-based quantum annealing error mitigation","abstract":"Quantum annealers like those from D-Wave Systems implement adiabatic quantum computing to solve optimization problems, but their analog nature and limited control functionalities present challenges to correcting or mitigating errors. As quantum computing advances towards applications, effective error suppression is an important research goal. We propose a new approach called replication based mitigation (RBM) based on parallel quantum annealing. In RBM, physical qubits representing the same logical qubit are dispersed across different copies of the problem embedded in the hardware. This mitigates hardware biases, is compatible with limited qubit connectivity in current annealers, and is suited for available noisy intermediate-scale quantum (NISQ) annealers. Our experimental analysis shows that RBM provides solution quality on par with previous methods while being compatible with a much wider range of hardware connectivity patterns. In comparisons against standard quantum annealing without error mitigation, RBM consistently improves the energies and ground state probabilities across parameterized problem sets.","sentences":["Quantum annealers like those from D-Wave Systems implement adiabatic quantum computing to solve optimization problems, but their analog nature and limited control functionalities present challenges to correcting or mitigating errors.","As quantum computing advances towards applications, effective error suppression is an important research goal.","We propose a new approach called replication based mitigation (RBM) based on parallel quantum annealing.","In RBM, physical qubits representing the same logical qubit are dispersed across different copies of the problem embedded in the hardware.","This mitigates hardware biases, is compatible with limited qubit connectivity in current annealers, and is suited for available noisy intermediate-scale quantum (NISQ) annealers.","Our experimental analysis shows that RBM provides solution quality on par with previous methods while being compatible with a much wider range of hardware connectivity patterns.","In comparisons against standard quantum annealing without error mitigation, RBM consistently improves the energies and ground state probabilities across parameterized problem sets."],"url":"http://arxiv.org/abs/2404.06580v1","category":"quant-ph"}
{"created":"2024-04-09 18:35:21","title":"On Test Sequence Generation using Multi-Objective Particle Swarm Optimization","abstract":"Software testing is an important and essential part of the software development life cycle and accounts for almost one-third of system development costs. In the software industry, testing costs can account for about 35% to 40% of the total cost of a software project. Therefore, providing efficient ways to test software is critical to reduce cost, time, and effort. Black-box testing and White-box testing are two essential components of software testing. Black-box testing focuses on the software's functionality, while White-box testing examines its internal structure. These tests contribute significantly to ensuring program coverage, which remains one of the main goals of the software testing paradigm. One of the main problems in this area is the identification of appropriate paths for program coverage, which are referred to as test sequences. Creating an automated and effective test sequence is a challenging task in the software testing process. In the proposed methodology, the challenge of \"test sequence generation\" is considered a multi-objective optimization problem that includes the Oracle cost and the path, both of which are optimized in a symmetrical manner to achieve optimal software testing. Multi-Objective Particle Swarm Optimization (MOPSO) is used to represent the test sequences with the highest priority and the lowest Oracle cost as optimal. The performance of the implemented approach is compared with the Multi-Objective Firefly Algorithm (MOFA) for generating test sequences. The MOPSO-based solution outperforms the MOFA-based approach and simultaneously provides the optimal solution for both objectives.","sentences":["Software testing is an important and essential part of the software development life cycle and accounts for almost one-third of system development costs.","In the software industry, testing costs can account for about 35% to 40% of the total cost of a software project.","Therefore, providing efficient ways to test software is critical to reduce cost, time, and effort.","Black-box testing and White-box testing are two essential components of software testing.","Black-box testing focuses on the software's functionality, while White-box testing examines its internal structure.","These tests contribute significantly to ensuring program coverage, which remains one of the main goals of the software testing paradigm.","One of the main problems in this area is the identification of appropriate paths for program coverage, which are referred to as test sequences.","Creating an automated and effective test sequence is a challenging task in the software testing process.","In the proposed methodology, the challenge of \"test sequence generation\" is considered a multi-objective optimization problem that includes the Oracle cost and the path, both of which are optimized in a symmetrical manner to achieve optimal software testing.","Multi-Objective Particle Swarm Optimization (MOPSO) is used to represent the test sequences with the highest priority and the lowest Oracle cost as optimal.","The performance of the implemented approach is compared with the Multi-Objective Firefly Algorithm (MOFA) for generating test sequences.","The MOPSO-based solution outperforms the MOFA-based approach and simultaneously provides the optimal solution for both objectives."],"url":"http://arxiv.org/abs/2404.06568v1","category":"cs.SE"}
{"created":"2024-04-09 18:00:00","title":"JADES Data Release 3 -- NIRSpec/MSA spectroscopy for 4,000 galaxies in the GOODS fields","abstract":"We present the third data release of JADES, the JWST Advanced Deep Extragalactic Survey, providing both imaging and spectroscopy in the two GOODS fields. Spectroscopy consists of medium-depth and deep NIRSpec/MSA spectra of 4,000 targets, covering the spectral range 0.6-5.3 $\\mu$m and observed with both the low-dispersion prism (R=30-300) and all three medium-resolution gratings (R=500-1,500). We describe the observations, data reduction, sample selection, and target allocation. We measured 2,375 redshifts (2,053 from multiple emission lines); our targets span the range from z=0.5 up to z=13, including 404 at z>5. The data release includes 2-d and 1-d fully reduced spectra, with slit-loss corrections and background subtraction optimized for point sources. We also provide redshifts and S/N>5 emission-line flux catalogs for the prism and grating spectra, and concise guidelines on how to use these data products. Alongside spectroscopy, we are also publishing fully calibrated NIRCam imaging, which enables studying the JADES sample with the combined power of imaging and spectroscopy. Together, these data provide the largest statistical sample to date to characterize the properties of galaxy populations in the first billion years after the Big Bang.","sentences":["We present the third data release of JADES, the JWST Advanced Deep Extragalactic Survey, providing both imaging and spectroscopy in the two GOODS fields.","Spectroscopy consists of medium-depth and deep NIRSpec/MSA spectra of 4,000 targets, covering the spectral range 0.6-5.3 $\\mu$m and observed with both the low-dispersion prism (R=30-300) and all three medium-resolution gratings (R=500-1,500).","We describe the observations, data reduction, sample selection, and target allocation.","We measured 2,375 redshifts (2,053 from multiple emission lines); our targets span the range from z=0.5 up to z=13, including 404 at z>5.","The data release includes 2-d and 1-d fully reduced spectra, with slit-loss corrections and background subtraction optimized for point sources.","We also provide redshifts and S/N>5 emission-line flux catalogs for the prism and grating spectra, and concise guidelines on how to use these data products.","Alongside spectroscopy, we are also publishing fully calibrated NIRCam imaging, which enables studying the JADES sample with the combined power of imaging and spectroscopy.","Together, these data provide the largest statistical sample to date to characterize the properties of galaxy populations in the first billion years after the Big Bang."],"url":"http://arxiv.org/abs/2404.06531v1","category":"astro-ph.GA"}
{"created":"2024-04-09 17:54:27","title":"Characterizing visual cortical magnification with topological smoothing and optimal transportation","abstract":"Human vision has different concentration on visual fields. Cortical magnification factor (CMF) is a popular measurement on visual acuity and cortex concentration. In order to achieve thorough measurement of CMF across the whole visual field, we propose a method to measure planar CMF upon retinotopic maps generated by pRF decoding, with help of our proposed methods: optimal transportation and topological smoothing. The optimal transportation re-calculates vertex location in retinotopic mapping, and topological smoothing guarantees topological conditions in retinotopic maps, which allow us to calculate planar CMF with the proposed 1-ring patch method. The pipeline was applied to the HCP 7T dataset, giving new planar results on CMF measurement across all 181 subjects, which illustrate novel concentration behavior on visual fields and their individual difference.","sentences":["Human vision has different concentration on visual fields.","Cortical magnification factor (CMF) is a popular measurement on visual acuity and cortex concentration.","In order to achieve thorough measurement of CMF across the whole visual field, we propose a method to measure planar CMF upon retinotopic maps generated by pRF decoding, with help of our proposed methods: optimal transportation and topological smoothing.","The optimal transportation re-calculates vertex location in retinotopic mapping, and topological smoothing guarantees topological conditions in retinotopic maps, which allow us to calculate planar CMF with the proposed 1-ring patch method.","The pipeline was applied to the HCP 7T dataset, giving new planar results on CMF measurement across all 181 subjects, which illustrate novel concentration behavior on visual fields and their individual difference."],"url":"http://arxiv.org/abs/2404.06505v1","category":"q-bio.NC"}
{"created":"2024-04-09 17:29:53","title":"Laue Indexing with Optimal Transport","abstract":"Laue tomography experiments retrieve the positions and orientations of crystal grains in a polycrystalline samples from diffraction patterns recorded at multiple viewing angles. The use of a broad wavelength spectrum beam can greatly reduce the experimental time, but poses a difficult challenge for the indexing of diffraction peaks in polycrystalline samples; the information about the wavelength of these Bragg peaks is absent and the diffraction patterns from multiple grains are superimposed. To date, no algorithms exist capable of indexing samples with more than about 500 grains efficiently. To address this need we present a novel method: Laue indexing with Optimal Transport (LaueOT). We create a probabilistic description of the multi-grain indexing problem and propose a solution based on Sinkhorn Expectation-Maximization method, which allows to efficiently find the maximum of the likelihood thanks to the assignments being calculated using Optimal Transport. This is a non-convex optimization problem, where the orientations and positions of grains are optimized simultaneously with grain-to-spot assignments, while robustly handling the outliers. The selection of initial prototype grains to consider in the optimization problem are also calculated within the Optimal Transport framework. LaueOT can rapidly and effectively index up to 1000 grains on a single large memory GPU within less than 30 minutes. We demonstrate the performance of LaueOT on simulations with variable numbers of grains, spot position measurement noise levels, and outlier fractions. The algorithm recovers the correct number of grains even for high noise levels and up to 70% outliers in our experiments. We compare the results of indexing with LaueOT to existing algorithms both on synthetic and real neutron diffraction data from well-characterized samples.","sentences":["Laue tomography experiments retrieve the positions and orientations of crystal grains in a polycrystalline samples from diffraction patterns recorded at multiple viewing angles.","The use of a broad wavelength spectrum beam can greatly reduce the experimental time, but poses a difficult challenge for the indexing of diffraction peaks in polycrystalline samples; the information about the wavelength of these Bragg peaks is absent and the diffraction patterns from multiple grains are superimposed.","To date, no algorithms exist capable of indexing samples with more than about 500 grains efficiently.","To address this need we present a novel method:","Laue indexing with Optimal Transport (LaueOT).","We create a probabilistic description of the multi-grain indexing problem and propose a solution based on Sinkhorn Expectation-Maximization method, which allows to efficiently find the maximum of the likelihood thanks to the assignments being calculated using Optimal Transport.","This is a non-convex optimization problem, where the orientations and positions of grains are optimized simultaneously with grain-to-spot assignments, while robustly handling the outliers.","The selection of initial prototype grains to consider in the optimization problem are also calculated within the Optimal Transport framework.","LaueOT can rapidly and effectively index up to 1000 grains on a single large memory GPU within less than 30 minutes.","We demonstrate the performance of LaueOT on simulations with variable numbers of grains, spot position measurement noise levels, and outlier fractions.","The algorithm recovers the correct number of grains even for high noise levels and up to 70% outliers in our experiments.","We compare the results of indexing with LaueOT to existing algorithms both on synthetic and real neutron diffraction data from well-characterized samples."],"url":"http://arxiv.org/abs/2404.06478v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 16:55:56","title":"Sharp Propagation of Chaos for the Ensemble Langevin Sampler","abstract":"The aim of this note is to revisit propagation of chaos for a Langevin-type interacting particle system used for sampling probability measures. The interacting particle system we consider coincides, in the setting of a log-quadratic target distribution, with the ensemble Kalman sampler, for which propagation of chaos was first proved by Ding and Li. Like these authors, we prove propagation of chaos using a synchronous coupling as a starting point, as in Sznitman's classical argument. Instead of relying on a boostrapping argument, however, we use a technique based on stopping times in order to handle the presence of the empirical covariance in the coefficients of the dynamics. This approach originates from numerical analysis and was recently employed to prove mean field limits for consensus-based optimization and related interacting particle systems. In the context of ensemble Langevin sampling, it enables proving pathwise propagation of chaos with optimal rate, whereas previous results were optimal only up to a positive epsilon.","sentences":["The aim of this note is to revisit propagation of chaos for a Langevin-type interacting particle system used for sampling probability measures.","The interacting particle system we consider coincides, in the setting of a log-quadratic target distribution, with the ensemble Kalman sampler, for which propagation of chaos was first proved by Ding and Li.","Like these authors, we prove propagation of chaos using a synchronous coupling as a starting point, as in Sznitman's classical argument.","Instead of relying on a boostrapping argument, however, we use a technique based on stopping times in order to handle the presence of the empirical covariance in the coefficients of the dynamics.","This approach originates from numerical analysis and was recently employed to prove mean field limits for consensus-based optimization and related interacting particle systems.","In the context of ensemble Langevin sampling, it enables proving pathwise propagation of chaos with optimal rate, whereas previous results were optimal only up to a positive epsilon."],"url":"http://arxiv.org/abs/2404.06456v1","category":"math.PR"}
{"created":"2024-04-09 16:25:13","title":"Software-based Security Framework for Edge and Mobile IoT","abstract":"With the proliferation of Internet of Things (IoT) devices, ensuring secure communications has become imperative. Due to their low cost and embedded nature, many of these devices operate with computational and energy constraints, neglecting the potential security vulnerabilities that they may bring. This work-in-progress is focused on designing secure communication among remote servers and embedded IoT devices to balance security robustness and energy efficiency. The proposed approach uses lightweight cryptography, optimizing device performance and security without overburdening their limited resources. Our architecture stands out for integrating Edge servers and a central Name Server, allowing secure and decentralized authentication and efficient connection transitions between different Edge servers. This architecture enhances the scalability of the IoT network and reduces the load on each server, distributing the responsibility for authentication and key management.","sentences":["With the proliferation of Internet of Things (IoT) devices, ensuring secure communications has become imperative.","Due to their low cost and embedded nature, many of these devices operate with computational and energy constraints, neglecting the potential security vulnerabilities that they may bring.","This work-in-progress is focused on designing secure communication among remote servers and embedded IoT devices to balance security robustness and energy efficiency.","The proposed approach uses lightweight cryptography, optimizing device performance and security without overburdening their limited resources.","Our architecture stands out for integrating Edge servers and a central Name Server, allowing secure and decentralized authentication and efficient connection transitions between different Edge servers.","This architecture enhances the scalability of the IoT network and reduces the load on each server, distributing the responsibility for authentication and key management."],"url":"http://arxiv.org/abs/2404.06435v1","category":"cs.CR"}
{"created":"2024-04-09 16:07:00","title":"Radon-Hurwitz Grassmannian codes","abstract":"Every equi-isoclinic tight fusion frame (EITFF) is a type of optimal code in a Grassmannian, consisting of subspaces of a finite-dimensional Hilbert space for which the smallest principal angle between any pair of them is as large as possible. EITFFs yield dictionaries with minimal block coherence and so are ideal for certain types of compressed sensing. By refining classical arguments of Lemmens and Seidel that rely upon Radon-Hurwitz theory, we fully characterize EITFFs in the special case where the dimension of the subspaces is exactly one-half of that of the ambient space. We moreover show that each such \"Radon-Hurwitz EITFF\" is highly symmetric.","sentences":["Every equi-isoclinic tight fusion frame (EITFF) is a type of optimal code in a Grassmannian, consisting of subspaces of a finite-dimensional Hilbert space for which the smallest principal angle between any pair of them is as large as possible.","EITFFs yield dictionaries with minimal block coherence and so are ideal for certain types of compressed sensing.","By refining classical arguments of Lemmens and Seidel that rely upon Radon-Hurwitz theory, we fully characterize EITFFs in the special case where the dimension of the subspaces is exactly one-half of that of the ambient space.","We moreover show that each such \"Radon-Hurwitz EITFF\" is highly symmetric."],"url":"http://arxiv.org/abs/2404.06417v1","category":"cs.IT"}
{"created":"2024-04-09 15:28:18","title":"Toward Reliable Dipole Moments without Single Excitations: The Role of Orbital Rotations and Dynamical Correlation","abstract":"The dipole moment is a crucial molecular property linked to a molecular system's bond polarity and overall electronic structure. To that end, the electronic dipole moment, which results from the electron density of a system, is often used to assess the accuracy and reliability of new electronic structure methods. This work analyses electronic dipole moments computed with the pair coupled cluster doubles (pCCD) ansatz and its linearized coupled cluster (pCCD-LCC) corrections using the canonical Hartree--Fock and pCCD-optimized (localized) orbital bases. The accuracy of pCCD-based dipole moments is assessed against experimental and CCSD(T) reference values using relaxed and unrelaxed density matrices and different basis set sizes. Our test set comprises molecules of various bonding patterns and electronic structures, exposing pCCD-based methods to a wide range of electron correlation effects. Additionally, we investigate the performance of pCCD-in-DFT dipole moments of some model complexes. Finally, our work indicates the importance of orbital relaxation in the pCCD model and shows the limitations of the linearized couple cluster corrections in predicting electronic dipole moments of multiple-bonded systems. Most importantly, pCCD with a linearized CCD correction can reproduce the dipole moment surfaces in singly-bonded molecules, which are comparable to the multi-reference ones.","sentences":["The dipole moment is a crucial molecular property linked to a molecular system's bond polarity and overall electronic structure.","To that end, the electronic dipole moment, which results from the electron density of a system, is often used to assess the accuracy and reliability of new electronic structure methods.","This work analyses electronic dipole moments computed with the pair coupled cluster doubles (pCCD) ansatz and its linearized coupled cluster (pCCD-LCC) corrections using the canonical Hartree--Fock and pCCD-optimized (localized) orbital bases.","The accuracy of pCCD-based dipole moments is assessed against experimental and CCSD(T) reference values using relaxed and unrelaxed density matrices and different basis set sizes.","Our test set comprises molecules of various bonding patterns and electronic structures, exposing pCCD-based methods to a wide range of electron correlation effects.","Additionally, we investigate the performance of pCCD-in-DFT dipole moments of some model complexes.","Finally, our work indicates the importance of orbital relaxation in the pCCD model and shows the limitations of the linearized couple cluster corrections in predicting electronic dipole moments of multiple-bonded systems.","Most importantly, pCCD with a linearized CCD correction can reproduce the dipole moment surfaces in singly-bonded molecules, which are comparable to the multi-reference ones."],"url":"http://arxiv.org/abs/2404.06385v1","category":"physics.chem-ph"}
{"created":"2024-04-09 15:13:50","title":"Enhancing Pharmaceutical Cold Supply Chain: Integrating Medication Synchronization and Diverse Delivery Modes","abstract":"The significance of last-mile logistics in the healthcare supply chain is growing steadily, especially in pharmacies where the growing prevalence of medication delivery to patients' homes is remarkable. This paper proposes a novel mathematical model for the last-mile logistics of the pharmaceutical supply chain and optimizes a pharmacy's logistical financial outcome while considering medication synchronization, different delivery modes, and temperature requirements of medicines. We propose a mathematical formulation of the problem using Mixed Integer Linear Programming (MILP) evolved from the actual problem of an outpatient pharmacy of a Dutch hospital. We create a case study by gathering, preparing, processing, and analyzing the associated data. We find the optimal solution, using Python MIP package and the Gurobi solver, which indicates the number of order batches, the composition of these batches, and the number of staff related to the preparation of the order batches. Our results show that our optimal solution increases the pharmacy's logistical financial outcome by 34 percent. Moreover, we propose other model variations and perform extensive scenario analysis to provide managerial insights applicable to other pharmacies and distributors in the last step of cold supply chains. Based on our scenario analysis, we conclude that improving medication synchronization can significantly enhance the pharmacy's logistical financial outcome.","sentences":["The significance of last-mile logistics in the healthcare supply chain is growing steadily, especially in pharmacies where the growing prevalence of medication delivery to patients' homes is remarkable.","This paper proposes a novel mathematical model for the last-mile logistics of the pharmaceutical supply chain and optimizes a pharmacy's logistical financial outcome while considering medication synchronization, different delivery modes, and temperature requirements of medicines.","We propose a mathematical formulation of the problem using Mixed Integer Linear Programming (MILP) evolved from the actual problem of an outpatient pharmacy of a Dutch hospital.","We create a case study by gathering, preparing, processing, and analyzing the associated data.","We find the optimal solution, using Python MIP package and the Gurobi solver, which indicates the number of order batches, the composition of these batches, and the number of staff related to the preparation of the order batches.","Our results show that our optimal solution increases the pharmacy's logistical financial outcome by 34 percent.","Moreover, we propose other model variations and perform extensive scenario analysis to provide managerial insights applicable to other pharmacies and distributors in the last step of cold supply chains.","Based on our scenario analysis, we conclude that improving medication synchronization can significantly enhance the pharmacy's logistical financial outcome."],"url":"http://arxiv.org/abs/2404.06373v1","category":"math.OC"}
{"created":"2024-04-09 14:53:59","title":"Towards Practical Meshlet Compression","abstract":"We propose a codec specifically designed for meshlet compression, optimized for rapid data-parallel GPU decompression within a mesh shader. Our compression strategy orders triangles in optimal generalized triangle strips (GTSs), which we generate by formulating the creation as a mixed integer linear program (MILP). Our method achieves index buffer compression rates of 16:1 compared to the vertex pipeline and crack-free vertex attribute quantization based on user preference. The 15.5 million triangles of our teaser image decompress and render in 0.59 ms on an AMD Radeon RX 7900 XTX.","sentences":["We propose a codec specifically designed for meshlet compression, optimized for rapid data-parallel GPU decompression within a mesh shader.","Our compression strategy orders triangles in optimal generalized triangle strips (GTSs), which we generate by formulating the creation as a mixed integer linear program (MILP).","Our method achieves index buffer compression rates of 16:1 compared to the vertex pipeline and crack-free vertex attribute quantization based on user preference.","The 15.5 million triangles of our teaser image decompress and render in 0.59 ms on an AMD Radeon RX 7900 XTX."],"url":"http://arxiv.org/abs/2404.06359v1","category":"cs.GR"}
{"created":"2024-04-09 13:45:59","title":"Compensating slice emittance growth in high brightness photoinjectors using sacrificial charge","abstract":"Achieving maximum electron beam brightness in photoinjectors requires detailed control of the 3D bunch shape and precise tuning of the beam focusing. Even in state-of-the-art designs, slice emittance growth due to nonlinear space charge forces and partial nonlaminarity often remains non-negligible. In this work we introduce a new means to linearize the transverse slice phase space: a sacrificial portion of the bunch's own charge distribution, formed into a wavebroken shock front by highly nonlinear space charge forces within the gun, whose downstream purpose is to dynamically linearize the desired bunch core. We show that linearization of an appropriately prepared bunch can be achieved via strongly nonlaminar focusing of the sacrificial shock front, while the inner core focuses laminarly. This leads to a natural spatial separation of the two distributions: a dense core surrounded by a diffuse halo of sacrificial charge that can be collimated. Multi-objective genetic algorithm optimizations of the ultra-compact x-ray free electron laser (UCXFEL) injector employ this concept, and we interpret it with an analytic model that agrees well with the simulations. In simulation we demonstrate a final bunch charge of 100 pC, peak current $\\sim 30$ A, and a sacrificial charge of 150 pC (250 pC total emitted from cathode) with normalized emittance growth of $<20$ nm-rad due to space charge. This implies a maximum achievable brightness approximately an order of magnitude greater than existing FEL injector designs.","sentences":["Achieving maximum electron beam brightness in photoinjectors requires detailed control of the 3D bunch shape and precise tuning of the beam focusing.","Even in state-of-the-art designs, slice emittance growth due to nonlinear space charge forces and partial nonlaminarity often remains non-negligible.","In this work we introduce a new means to linearize the transverse slice phase space: a sacrificial portion of the bunch's own charge distribution, formed into a wavebroken shock front by highly nonlinear space charge forces within the gun, whose downstream purpose is to dynamically linearize the desired bunch core.","We show that linearization of an appropriately prepared bunch can be achieved via strongly nonlaminar focusing of the sacrificial shock front, while the inner core focuses laminarly.","This leads to a natural spatial separation of the two distributions: a dense core surrounded by a diffuse halo of sacrificial charge that can be collimated.","Multi-objective genetic algorithm optimizations of the ultra-compact x-ray free electron laser (UCXFEL) injector employ this concept, and we interpret it with an analytic model that agrees well with the simulations.","In simulation we demonstrate a final bunch charge of 100 pC, peak current $\\sim 30$ A, and a sacrificial charge of 150 pC (250 pC total emitted from cathode) with normalized emittance growth of $<20$ nm-rad due to space charge.","This implies a maximum achievable brightness approximately an order of magnitude greater than existing FEL injector designs."],"url":"http://arxiv.org/abs/2404.06312v1","category":"physics.acc-ph"}
{"created":"2024-04-09 13:19:43","title":"Fortifying Fully Convolutional Generative Adversarial Networks for Image Super-Resolution Using Divergence Measures","abstract":"Super-Resolution (SR) is a time-hallowed image processing problem that aims to improve the quality of a Low-Resolution (LR) sample up to the standard of its High-Resolution (HR) counterpart. We aim to address this by introducing Super-Resolution Generator (SuRGe), a fully-convolutional Generative Adversarial Network (GAN)-based architecture for SR. We show that distinct convolutional features obtained at increasing depths of a GAN generator can be optimally combined by a set of learnable convex weights to improve the quality of generated SR samples. In the process, we employ the Jensen-Shannon and the Gromov-Wasserstein losses respectively between the SR-HR and LR-SR pairs of distributions to further aid the generator of SuRGe to better exploit the available information in an attempt to improve SR. Moreover, we train the discriminator of SuRGe with the Wasserstein loss with gradient penalty, to primarily prevent mode collapse. The proposed SuRGe, as an end-to-end GAN workflow tailor-made for super-resolution, offers improved performance while maintaining low inference time. The efficacy of SuRGe is substantiated by its superior performance compared to 18 state-of-the-art contenders on 10 benchmark datasets.","sentences":["Super-Resolution (SR) is a time-hallowed image processing problem that aims to improve the quality of a Low-Resolution (LR) sample up to the standard of its High-Resolution (HR) counterpart.","We aim to address this by introducing Super-Resolution Generator (SuRGe), a fully-convolutional Generative Adversarial Network (GAN)-based architecture for SR.","We show that distinct convolutional features obtained at increasing depths of a GAN generator can be optimally combined by a set of learnable convex weights to improve the quality of generated SR samples.","In the process, we employ the Jensen-Shannon and the Gromov-Wasserstein losses respectively between the SR-HR and LR-SR pairs of distributions to further aid the generator of SuRGe to better exploit the available information in an attempt to improve SR.","Moreover, we train the discriminator of SuRGe with the Wasserstein loss with gradient penalty, to primarily prevent mode collapse.","The proposed SuRGe, as an end-to-end GAN workflow tailor-made for super-resolution, offers improved performance while maintaining low inference time.","The efficacy of SuRGe is substantiated by its superior performance compared to 18 state-of-the-art contenders on 10 benchmark datasets."],"url":"http://arxiv.org/abs/2404.06294v1","category":"eess.IV"}
{"created":"2024-04-09 12:38:14","title":"From Stochastic Hamiltonian to Quantum Simulation: Exploring Memory Effects in Exciton Dynamics","abstract":"The unraveling of open quantum system dynamics in terms of stochastic quantum trajectories offers a picture of open system dynamics that consistently considers memory effects stemming from the finite correlation time of environment fluctuations. These fluctuations significantly influence the coherence and energy transport properties of excitonic systems. When their correlation time is comparable to the timescale of the Hamiltonian evolution, it leads to the departure of open system dynamics from the Markovian limit. In this work, we leverage the unraveling of exciton dynamics through stochastic Hamiltonian propagators to design quantum circuits that simulate exciton transport, capturing finite memory effects. In addition to enabling the synthesis of parametrizable quantum circuits, stochastic unitary propagators provide a transparent framework for investigating non-Markovian effects on exciton transport. Our analysis reveals a nuanced relationship between environment correlation time and transport efficiency, identifying a regime of \"memory-assisted\" quantum transport where time-correlated fluctuations allow the system to reach higher efficiency. However, this property is not universal and can only be realized in conjunction with specific features of the system Hamiltonian.","sentences":["The unraveling of open quantum system dynamics in terms of stochastic quantum trajectories offers a picture of open system dynamics that consistently considers memory effects stemming from the finite correlation time of environment fluctuations.","These fluctuations significantly influence the coherence and energy transport properties of excitonic systems.","When their correlation time is comparable to the timescale of the Hamiltonian evolution, it leads to the departure of open system dynamics from the Markovian limit.","In this work, we leverage the unraveling of exciton dynamics through stochastic Hamiltonian propagators to design quantum circuits that simulate exciton transport, capturing finite memory effects.","In addition to enabling the synthesis of parametrizable quantum circuits, stochastic unitary propagators provide a transparent framework for investigating non-Markovian effects on exciton transport.","Our analysis reveals a nuanced relationship between environment correlation time and transport efficiency, identifying a regime of \"memory-assisted\" quantum transport where time-correlated fluctuations allow the system to reach higher efficiency.","However, this property is not universal and can only be realized in conjunction with specific features of the system Hamiltonian."],"url":"http://arxiv.org/abs/2404.06264v1","category":"quant-ph"}
{"created":"2024-04-09 12:29:11","title":"A Large-Scale Simulation Method for Neuromorphic Circuits","abstract":"Splitting algorithms are well-established in convex optimization and are designed to solve large-scale problems. Using such algorithms to simulate the behavior of nonlinear circuit networks provides scalable methods for the simulation and design of neuromorphic systems. For circuits made of linear capacitors and inductors with nonlinear resistive elements, we propose a splitting that breaks the network into its LTI lossless component and its static resistive component. This splitting has both physical and algorithmic advantages and allows for separate calculations in the time domain and in the frequency domain. To demonstrate the scalability of this approach, a network made from one hundred neurons modeled by the well-known FitzHugh-Nagumo circuit with all-to-all diffusive coupling is simulated.","sentences":["Splitting algorithms are well-established in convex optimization and are designed to solve large-scale problems.","Using such algorithms to simulate the behavior of nonlinear circuit networks provides scalable methods for the simulation and design of neuromorphic systems.","For circuits made of linear capacitors and inductors with nonlinear resistive elements, we propose a splitting that breaks the network into its LTI lossless component and its static resistive component.","This splitting has both physical and algorithmic advantages and allows for separate calculations in the time domain and in the frequency domain.","To demonstrate the scalability of this approach, a network made from one hundred neurons modeled by the well-known FitzHugh-Nagumo circuit with all-to-all diffusive coupling is simulated."],"url":"http://arxiv.org/abs/2404.06255v1","category":"eess.SY"}
{"created":"2024-04-09 10:47:01","title":"Further Understanding of a Local Gaussian Process Approximation: Characterising Convergence in the Finite Regime","abstract":"We show that common choices of kernel functions for a highly accurate and massively scalable nearest-neighbour based GP regression model (GPnn: \\cite{GPnn}) exhibit gradual convergence to asymptotic behaviour as dataset-size $n$ increases. For isotropic kernels such as Mat\\'{e}rn and squared-exponential, an upper bound on the predictive MSE can be obtained as $O(n^{-\\frac{p}{d}})$ for input dimension $d$, $p$ dictated by the kernel (and $d>p$) and fixed number of nearest-neighbours $m$ with minimal assumptions on the input distribution. Similar bounds can be found under model misspecification and combined to give overall rates of convergence of both MSE and an important calibration metric. We show that lower bounds on $n$ can be given in terms of $m$, $l$, $p$, $d$, a tolerance $\\varepsilon$ and a probability $\\delta$. When $m$ is chosen to be $O(n^{\\frac{p}{p+d}})$ minimax optimal rates of convergence are attained. Finally, we demonstrate empirical performance and show that in many cases convergence occurs faster than the upper bounds given here.","sentences":["We show that common choices of kernel functions for a highly accurate and massively scalable nearest-neighbour based GP regression model (GPnn: \\cite{GPnn}) exhibit gradual convergence to asymptotic behaviour as dataset-size $n$ increases.","For isotropic kernels such as Mat\\'{e}rn and squared-exponential, an upper bound on the predictive MSE can be obtained as $O(n^{-\\frac{p}{d}})$ for input dimension $d$, $p$ dictated by the kernel (and $d>p$) and fixed number of nearest-neighbours $m$ with minimal assumptions on the input distribution.","Similar bounds can be found under model misspecification and combined to give overall rates of convergence of both MSE and an important calibration metric.","We show that lower bounds on $n$ can be given in terms of $m$, $l$, $p$, $d$, a tolerance $\\varepsilon$ and a probability $\\delta$.","When $m$ is chosen to be $O(n^{\\frac{p}{p+d}})$ minimax optimal rates of convergence are attained.","Finally, we demonstrate empirical performance and show that in many cases convergence occurs faster than the upper bounds given here."],"url":"http://arxiv.org/abs/2404.06200v1","category":"math.ST"}
{"created":"2024-04-09 10:40:26","title":"SUPPLY: Sustainable multi-UAV Performance-aware Placement Algorithm for Flying Networks","abstract":"Unmanned Aerial Vehicles (UAVs) are used for a wide range of applications. Due to characteristics such as the ability to hover and carry cargo on-board, rotary-wing UAVs have been considered suitable platforms for carrying communications nodes, including Wi-Fi Access Points and cellular Base Stations. This gave rise to the concept of Flying Networks (FNs), now making part of the so-called Non-Terrestrial Networks (NTNs) defined in 3GPP. In scenarios where the deployment of terrestrial networks is not feasible, the use of FNs has emerged as a solution to provide wireless connectivity. However, the management of the communications resources in FNs imposes significant challenges, especially regarding the positioning of the UAVs so that the Quality of Service (QoS) offered to the Ground Users (GUs) and devices is maximized. Moreover, unlike terrestrial networks that are directly connected to the power grid, UAVs typically rely on on-board batteries that need to be recharged. In order to maximize the UAVs' flying time, the energy consumed by the UAVs needs to be minimized. When it comes to multi-UAV placement, most state-of-the-art solutions focus on maximizing the coverage area and assume that the UAVs keep hovering in a fixed position while serving GUs. Also, they do not address the energy-aware multi-UAV placement problem in networking scenarios where the GUs may have different QoS requirements and may not be uniformly distributed across the area of interest. In this work, we propose the Sustainable multi-UAV Performance-aware Placement (SUPPLY) algorithm. SUPPLY defines the energy and performance-aware positioning of multiple UAVs in an FN. To accomplish this, SUPPLY defines trajectories that minimize UAVs' energy consumption, while ensuring the targeted QoS levels. The obtained results show up to 25% energy consumption reduction with minimal impact on throughput and delay.","sentences":["Unmanned Aerial Vehicles (UAVs) are used for a wide range of applications.","Due to characteristics such as the ability to hover and carry cargo on-board, rotary-wing UAVs have been considered suitable platforms for carrying communications nodes, including Wi-Fi Access Points and cellular Base Stations.","This gave rise to the concept of Flying Networks (FNs), now making part of the so-called Non-Terrestrial Networks (NTNs) defined in 3GPP.","In scenarios where the deployment of terrestrial networks is not feasible, the use of FNs has emerged as a solution to provide wireless connectivity.","However, the management of the communications resources in FNs imposes significant challenges, especially regarding the positioning of the UAVs so that the Quality of Service (QoS) offered to the Ground Users (GUs) and devices is maximized.","Moreover, unlike terrestrial networks that are directly connected to the power grid, UAVs typically rely on on-board batteries that need to be recharged.","In order to maximize the UAVs' flying time, the energy consumed by the UAVs needs to be minimized.","When it comes to multi-UAV placement, most state-of-the-art solutions focus on maximizing the coverage area and assume that the UAVs keep hovering in a fixed position while serving GUs.","Also, they do not address the energy-aware multi-UAV placement problem in networking scenarios where the GUs may have different QoS requirements and may not be uniformly distributed across the area of interest.","In this work, we propose the Sustainable multi-UAV Performance-aware Placement (SUPPLY) algorithm.","SUPPLY defines the energy and performance-aware positioning of multiple UAVs in an FN.","To accomplish this, SUPPLY defines trajectories that minimize UAVs' energy consumption, while ensuring the targeted QoS levels.","The obtained results show up to 25% energy consumption reduction with minimal impact on throughput and delay."],"url":"http://arxiv.org/abs/2404.06197v1","category":"cs.NI"}
{"created":"2024-04-09 10:38:10","title":"Permissible extensions of classical to quantum games combining three strategies","abstract":"We study the extension of classical games to the quantum domain, generated by the addition of one unitary strategy to two classical strategies of each player. The conditions that need to be met by unitary operations to ensure that the extended game is invariant with respect to the isomorphic transformations of the input game are determined. It has been shown that there are three types of these extensions, two of them are purely quantum. On the other hand, it has been demonstrated that the extensions of two versions of the same classical game by a unitary operator that does not meet these conditions may result in quantum games that are non-equivalent, e.g. having different Nash equilibria. We use the obtained results to extend the classical Prisoner's Dilemma game to a quantum game that has a unique Nash equilibrium closer to Pareto-optimal solutions than the original one.","sentences":["We study the extension of classical games to the quantum domain, generated by the addition of one unitary strategy to two classical strategies of each player.","The conditions that need to be met by unitary operations to ensure that the extended game is invariant with respect to the isomorphic transformations of the input game are determined.","It has been shown that there are three types of these extensions, two of them are purely quantum.","On the other hand, it has been demonstrated that the extensions of two versions of the same classical game by a unitary operator that does not meet these conditions may result in quantum games that are non-equivalent, e.g. having different Nash equilibria.","We use the obtained results to extend the classical Prisoner's Dilemma game to a quantum game that has a unique Nash equilibrium closer to Pareto-optimal solutions than the original one."],"url":"http://arxiv.org/abs/2404.06196v1","category":"quant-ph"}
{"created":"2024-04-09 10:22:13","title":"Transport resistance strikes back: unveiling its impact on fill factor losses in organic solar cells","abstract":"The fill factor ($FF$) is a critical parameter for solar cell efficiency, yet its analytical description is challenging due to the interplay between recombination and charge extraction processes. An often overlooked yet significant factor contributing to $FF$ losses, beyond recombination, is the influence of charge transport. In most state-of-the-art organic solar cells, the primary limitation of the $FF$ arises not from recombination but rather from low conductivity, highlighting the need for refined models to predict the $FF$ accurately. Here, we extend the analytical model for transport resistance to a more general case. Drawing from a large set of experimental current-voltage and light intensity-dependent open-circuit voltage data, we systematically incorporate crucial details previously omitted in the model. Consequently, we introduce a straightforward set of equations to predict the $FF$ of a solar cell, enabling the differentiation of losses attributed to recombination and transport resistance. Our study provides valuable insights into strategies for mitigating $FF$ losses based on the experimentally validated analytical model, guiding the development of more efficient solar cell designs and optimization strategies.","sentences":["The fill factor ($FF$) is a critical parameter for solar cell efficiency, yet its analytical description is challenging due to the interplay between recombination and charge extraction processes.","An often overlooked yet significant factor contributing to $FF$ losses, beyond recombination, is the influence of charge transport.","In most state-of-the-art organic solar cells, the primary limitation of the $FF$ arises not from recombination but rather from low conductivity, highlighting the need for refined models to predict the $FF$ accurately.","Here, we extend the analytical model for transport resistance to a more general case.","Drawing from a large set of experimental current-voltage and light intensity-dependent open-circuit voltage data, we systematically incorporate crucial details previously omitted in the model.","Consequently, we introduce a straightforward set of equations to predict the $FF$ of a solar cell, enabling the differentiation of losses attributed to recombination and transport resistance.","Our study provides valuable insights into strategies for mitigating $FF$ losses based on the experimentally validated analytical model, guiding the development of more efficient solar cell designs and optimization strategies."],"url":"http://arxiv.org/abs/2404.06190v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 10:21:32","title":"MLatom software ecosystem for surface hopping dynamics in Python with quantum mechanical and machine learning methods","abstract":"We present an open-source MLatom@XACS software ecosystem for on-the-fly surface hopping nonadiabatic dynamics based on the Landau-Zener-Belyaev-Lebedev (LZBL) algorithm. The dynamics can be performed via Python API with a wide range of quantum mechanical (QM) and machine learning (ML) methods, including ab initio QM (CASSCF and ADC(2)), semi-empirical QM methods (e.g., AM1, PM3, OMx, and ODMx), and many types of machine learning potentials (e.g., KREG, ANI, and MACE). Combinations of QM and ML methods can also be used. While the user can build their own combinations, we provide AIQM1, which is based on {\\Delta}-learning and can be used out of the box. We showcase how AIQM1 reproduces the isomerization quantum yield of trans-azobenzene at a low cost. We provide example scripts that, in a dozen lines, enable the user to obtain the final population plots by simply providing the initial geometry of a molecule. Thus, those scripts perform geometry optimization, normal mode calculations, initial condition sampling, parallel trajectories propagation, population analysis, and final result plotting. Given the capabilities of MLatom to be used for training different ML models, this ecosystem can be seamlessly integrated into the protocols building ML models for nonadiabatic dynamics. In the future, a deeper and more efficient integration of MLatom with Newton-X will enable vast range of functionalities for surface hopping dynamics, such as fewest-switches surface hopping, to facilitate similar workflows via the Python API.","sentences":["We present an open-source MLatom@XACS software ecosystem for on-the-fly surface hopping nonadiabatic dynamics based on the Landau-Zener-Belyaev-Lebedev (LZBL) algorithm.","The dynamics can be performed via Python API with a wide range of quantum mechanical (QM) and machine learning (ML) methods, including ab initio QM (CASSCF and ADC(2)), semi-empirical QM methods (e.g., AM1, PM3, OMx, and ODMx), and many types of machine learning potentials (e.g., KREG, ANI, and MACE).","Combinations of QM and ML methods can also be used.","While the user can build their own combinations, we provide AIQM1, which is based on {\\Delta}-learning and can be used out of the box.","We showcase how AIQM1 reproduces the isomerization quantum yield of trans-azobenzene at a low cost.","We provide example scripts that, in a dozen lines, enable the user to obtain the final population plots by simply providing the initial geometry of a molecule.","Thus, those scripts perform geometry optimization, normal mode calculations, initial condition sampling, parallel trajectories propagation, population analysis, and final result plotting.","Given the capabilities of MLatom to be used for training different ML models, this ecosystem can be seamlessly integrated into the protocols building ML models for nonadiabatic dynamics.","In the future, a deeper and more efficient integration of MLatom with Newton-X will enable vast range of functionalities for surface hopping dynamics, such as fewest-switches surface hopping, to facilitate similar workflows via the Python API."],"url":"http://arxiv.org/abs/2404.06189v1","category":"physics.chem-ph"}
{"created":"2024-04-09 10:13:41","title":"High-Fidelity CZ Gates in Double Quantum Dot -- Circuit QED Systems Beyond the Rotating-Wave Approximation","abstract":"Semiconductor double quantum dot (DQD) qubits coupled via superconducting microwave resonators provide a powerful means of long-range manipulation of the qubits' spin and charge degrees of freedom. Quantum gates can be implemented by parametrically driving the qubits while their transition frequencies are detuned from the resonator frequency. Long-range two-qubit CZ gates have been proposed for the DQD spin qubit within the rotating-wave approximation (RWA). Rapid gates demand strong coupling, but RWA breaks down when coupling strengths become significant relative to system frequencies. Therefore, understanding the detrimental impact of time-dependent terms ignored by RWA is critical for high-fidelity operation. Here, we go beyond RWA to study CZ gate fidelity for both DQD spin and charge qubits. We propose a novel parametric drive on the charge qubit that produces fewer time-dependent terms and show that it outperforms its spin counterpart. We find that drive amplitude - a parameter dropped in RWA - is critical for optimizing fidelity and map out high-fidelity regimes. Our results demonstrate the necessity of going beyond RWA in understanding how long-range gates can be realized in DQD qubits, with charge qubits offering considerable advantages in high-fidelity operation.","sentences":["Semiconductor double quantum dot (DQD) qubits coupled via superconducting microwave resonators provide a powerful means of long-range manipulation of the qubits' spin and charge degrees of freedom.","Quantum gates can be implemented by parametrically driving the qubits while their transition frequencies are detuned from the resonator frequency.","Long-range two-qubit CZ gates have been proposed for the DQD spin qubit within the rotating-wave approximation (RWA).","Rapid gates demand strong coupling, but RWA breaks down when coupling strengths become significant relative to system frequencies.","Therefore, understanding the detrimental impact of time-dependent terms ignored by RWA is critical for high-fidelity operation.","Here, we go beyond RWA to study CZ gate fidelity for both DQD spin and charge qubits.","We propose a novel parametric drive on the charge qubit that produces fewer time-dependent terms and show that it outperforms its spin counterpart.","We find that drive amplitude - a parameter dropped in RWA - is critical for optimizing fidelity and map out high-fidelity regimes.","Our results demonstrate the necessity of going beyond RWA in understanding how long-range gates can be realized in DQD qubits, with charge qubits offering considerable advantages in high-fidelity operation."],"url":"http://arxiv.org/abs/2404.06187v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-09 10:03:35","title":"AI-MOLE: Autonomous Iterative Motion Learning for Unknown Nonlinear Dynamics with Extensive Experimental Validation","abstract":"This work proposes Autonomous Iterative Motion Learning (AI-MOLE), a method that enables systems with unknown, nonlinear dynamics to autonomously learn to solve reference tracking tasks. The method iteratively applies an input trajectory to the unknown dynamics, trains a Gaussian process model based on the experimental data, and utilizes the model to update the input trajectory until desired tracking performance is achieved. Unlike existing approaches, the proposed method determines necessary parameters automatically, i.e., AI-MOLE works plug-and-play and without manual parameter tuning. Furthermore, AI-MOLE only requires input/output information, but can also exploit available state information to accelerate learning.   While other approaches are typically only validated in simulation or on a single real-world testbed using manually tuned parameters, we present the unprecedented result of validating the proposed method on three different real-world robots and a total of nine different reference tracking tasks without requiring any a priori model information or manual parameter tuning. Over all systems and tasks, AI-MOLE rapidly learns to track the references without requiring any manual parameter tuning at all, even if only input/output information is available.","sentences":["This work proposes Autonomous Iterative Motion Learning (AI-MOLE), a method that enables systems with unknown, nonlinear dynamics to autonomously learn to solve reference tracking tasks.","The method iteratively applies an input trajectory to the unknown dynamics, trains a Gaussian process model based on the experimental data, and utilizes the model to update the input trajectory until desired tracking performance is achieved.","Unlike existing approaches, the proposed method determines necessary parameters automatically, i.e., AI-MOLE works plug-and-play and without manual parameter tuning.","Furthermore, AI-MOLE only requires input/output information, but can also exploit available state information to accelerate learning.   ","While other approaches are typically only validated in simulation or on a single real-world testbed using manually tuned parameters, we present the unprecedented result of validating the proposed method on three different real-world robots and a total of nine different reference tracking tasks without requiring any a priori model information or manual parameter tuning.","Over all systems and tasks, AI-MOLE rapidly learns to track the references without requiring any manual parameter tuning at all, even if only input/output information is available."],"url":"http://arxiv.org/abs/2404.06179v1","category":"cs.RO"}
{"created":"2024-04-09 09:54:59","title":"A quantum information theoretic analysis of reinforcement learning-assisted quantum architecture search","abstract":"In the field of quantum computing, variational quantum algorithms (VQAs) represent a pivotal category of quantum solutions across a broad spectrum of applications. These algorithms demonstrate significant potential for realising quantum computational advantage. A fundamental aspect of VQAs involves formulating expressive and efficient quantum circuits (namely ansatz) and automating the search of such ansatz is known as quantum architecture search (QAS). RL-QAS involves optimising QAS using reinforcement learning techniques. This study investigates RL-QAS for crafting ansatzes tailored to the variational quantum state diagonalization problem. Our investigation includes a comprehensive analysis of various dimensions, such as the entanglement thresholds of the resultant states, the impact of initial conditions on the performance of RL-agent, the phase change behavior of correlation in concurrence bounds, and the discrete contributions of qubits in deducing eigenvalues through conditional entropy metrics. We leverage these insights to devise an optimal, admissible QAS to diagonalize random quantum states. Furthermore, the methodologies presented herein offer a generalised framework for constructing reward functions within RL-QAS applicable to variational quantum algorithms.","sentences":["In the field of quantum computing, variational quantum algorithms (VQAs) represent a pivotal category of quantum solutions across a broad spectrum of applications.","These algorithms demonstrate significant potential for realising quantum computational advantage.","A fundamental aspect of VQAs involves formulating expressive and efficient quantum circuits (namely ansatz) and automating the search of such ansatz is known as quantum architecture search (QAS).","RL-QAS involves optimising QAS using reinforcement learning techniques.","This study investigates RL-QAS for crafting ansatzes tailored to the variational quantum state diagonalization problem.","Our investigation includes a comprehensive analysis of various dimensions, such as the entanglement thresholds of the resultant states, the impact of initial conditions on the performance of RL-agent, the phase change behavior of correlation in concurrence bounds, and the discrete contributions of qubits in deducing eigenvalues through conditional entropy metrics.","We leverage these insights to devise an optimal, admissible QAS to diagonalize random quantum states.","Furthermore, the methodologies presented herein offer a generalised framework for constructing reward functions within RL-QAS applicable to variational quantum algorithms."],"url":"http://arxiv.org/abs/2404.06174v1","category":"quant-ph"}
{"created":"2024-04-09 09:43:39","title":"Classical and quantum field theory in a box with moving boundaries: A numerical study of the Dynamical Casimir Effect","abstract":"We present a detailed description of a quantum scalar field theory within a flat spacetime confined to a cavity with perfectly reflecting moving boundaries. Moreover, we establish an equivalence between this time-dependent setting and a field theory on an acoustic metric with static Dirichlet boundary conditions. We discuss the classical and quantum aspects of the theory from the latter perspective, accompanied by the introduction of novel numerical techniques designed for the (nonperturbative) computation of particle production attributed to the Dynamical Casimir effect, applicable to arbitrary boundary trajectories. As an illustrative example of these methodologies, we compute the particle production for a massless field in 1+1 dimensions. Notably, our approaches readily extend to encompass scenarios involving massive fields and higher dimensions","sentences":["We present a detailed description of a quantum scalar field theory within a flat spacetime confined to a cavity with perfectly reflecting moving boundaries.","Moreover, we establish an equivalence between this time-dependent setting and a field theory on an acoustic metric with static Dirichlet boundary conditions.","We discuss the classical and quantum aspects of the theory from the latter perspective, accompanied by the introduction of novel numerical techniques designed for the (nonperturbative) computation of particle production attributed to the Dynamical Casimir effect, applicable to arbitrary boundary trajectories.","As an illustrative example of these methodologies, we compute the particle production for a massless field in 1+1 dimensions.","Notably, our approaches readily extend to encompass scenarios involving massive fields and higher dimensions"],"url":"http://arxiv.org/abs/2404.06166v1","category":"quant-ph"}
{"created":"2024-04-09 09:29:06","title":"WaSP: Warp Scheduling to Mimic Prefetching in Graphics Workloads","abstract":"Contemporary GPUs are designed to handle long-latency operations effectively; however, challenges such as core occupancy (number of warps in a core) and pipeline width can impede their latency management. This is particularly evident in Tile-Based Rendering (TBR) GPUs, where core occupancy remains low for extended durations. To address this challenge, we introduce WaSP, a lightweight warp scheduler tailored for GPUs in graphics applications. WaSP strategically mimics prefetching by initiating a select subset of warps, termed priority warps, early in execution to reduce memory latency for subsequent warps. This optimization taps into the inherent but underutilized memory parallelism within the GPU core. This underutilization is a consequence of a baseline scheduler that evenly spaces misses throughout execution to exploit the inherent spatial locality in graphics workloads. WaSP improves on this by reducing average memory latency while maintaining locality for the majority of warps. While maximizing memory parallelism utilization, WaSP prevents saturating the caches with misses to avoid filling up the MSHRs (Miss Status Holding Registers). This approach reduces cache stalls that halt further accesses to the cache. Overall, WaSP yields a significant 3.9% performance speedup. Importantly, WaSP accomplishes these enhancements with a negligible overhead, positioning it as a promising solution for enhancing the efficiency of GPUs in managing latency challenges.","sentences":["Contemporary GPUs are designed to handle long-latency operations effectively; however, challenges such as core occupancy (number of warps in a core) and pipeline width can impede their latency management.","This is particularly evident in Tile-Based Rendering (TBR) GPUs, where core occupancy remains low for extended durations.","To address this challenge, we introduce WaSP, a lightweight warp scheduler tailored for GPUs in graphics applications.","WaSP strategically mimics prefetching by initiating a select subset of warps, termed priority warps, early in execution to reduce memory latency for subsequent warps.","This optimization taps into the inherent but underutilized memory parallelism within the GPU core.","This underutilization is a consequence of a baseline scheduler that evenly spaces misses throughout execution to exploit the inherent spatial locality in graphics workloads.","WaSP improves on this by reducing average memory latency while maintaining locality for the majority of warps.","While maximizing memory parallelism utilization, WaSP prevents saturating the caches with misses to avoid filling up the MSHRs (Miss Status Holding Registers).","This approach reduces cache stalls that halt further accesses to the cache.","Overall, WaSP yields a significant 3.9% performance speedup.","Importantly, WaSP accomplishes these enhancements with a negligible overhead, positioning it as a promising solution for enhancing the efficiency of GPUs in managing latency challenges."],"url":"http://arxiv.org/abs/2404.06156v1","category":"cs.AR"}
{"created":"2024-04-09 09:08:24","title":"Highly reflective and high-$Q$ thin resonant subwavelength gratings","abstract":"We theoretically investigate the design of thin subwavelength gratings possessing high-reflectivity and high-$Q$ resonances when illuminated at normal incidence by a Gaussian beam. We compare the performances of single-period and dual-period rectangular gratings using Finite Element Method-based optimization and predict one to two orders of magnitude improvement in their transmission loss-linewidth product, which is the relevant figure of merit for e.g. resonant mirror-based microcavity applications.","sentences":["We theoretically investigate the design of thin subwavelength gratings possessing high-reflectivity and high-$Q$ resonances when illuminated at normal incidence by a Gaussian beam.","We compare the performances of single-period and dual-period rectangular gratings using Finite Element Method-based optimization and predict one to two orders of magnitude improvement in their transmission loss-linewidth product, which is the relevant figure of merit for e.g. resonant mirror-based microcavity applications."],"url":"http://arxiv.org/abs/2404.06143v1","category":"physics.optics"}
{"created":"2024-04-09 09:03:06","title":"Inexact Policy Iteration Methods for Large-Scale Markov Decision Processes","abstract":"We consider inexact policy iteration methods for large-scale infinite-horizon discounted MDPs with finite spaces, a variant of policy iteration where the policy evaluation step is implemented inexactly using an iterative solver for linear systems. In the classical dynamic programming literature, a similar principle is deployed in optimistic policy iteration, where an a-priori fixed-number of iterations of value iteration is used to inexactly solve the policy evaluation step. Inspired by the connection between policy iteration and semismooth Newton's method, we investigate a class of iPI methods that mimic the inexact variants of semismooth Newton's method by adopting a parametric stopping condition to regulate the level of inexactness of the policy evaluation step. For this class of methods we discuss local and global convergence properties and derive a practical range of values for the stopping-condition parameter that provide contraction guarantees. Our analysis is general and therefore encompasses a variety of iterative solvers for policy evaluation, including the standard value iteration as well as more sophisticated ones such as GMRES. As underlined by our analysis, the selection of the inner solver is of fundamental importance for the performance of the overall method. We therefore consider different iterative methods to solve the policy evaluation step and analyze their applicability and contraction properties when used for policy evaluation. We show that the contraction properties of these methods tend to be enhanced by the specific structure of policy evaluation and that there is margin for substantial improvement in terms of convergence rate. Finally, we study the numerical performance of different instances of inexact policy iteration on large-scale MDPs for the design of health policies to control the spread of infectious diseases in epidemiology.","sentences":["We consider inexact policy iteration methods for large-scale infinite-horizon discounted MDPs with finite spaces, a variant of policy iteration where the policy evaluation step is implemented inexactly using an iterative solver for linear systems.","In the classical dynamic programming literature, a similar principle is deployed in optimistic policy iteration, where an a-priori fixed-number of iterations of value iteration is used to inexactly solve the policy evaluation step.","Inspired by the connection between policy iteration and semismooth Newton's method, we investigate a class of iPI methods that mimic the inexact variants of semismooth Newton's method by adopting a parametric stopping condition to regulate the level of inexactness of the policy evaluation step.","For this class of methods we discuss local and global convergence properties and derive a practical range of values for the stopping-condition parameter that provide contraction guarantees.","Our analysis is general and therefore encompasses a variety of iterative solvers for policy evaluation, including the standard value iteration as well as more sophisticated ones such as GMRES.","As underlined by our analysis, the selection of the inner solver is of fundamental importance for the performance of the overall method.","We therefore consider different iterative methods to solve the policy evaluation step and analyze their applicability and contraction properties when used for policy evaluation.","We show that the contraction properties of these methods tend to be enhanced by the specific structure of policy evaluation and that there is margin for substantial improvement in terms of convergence rate.","Finally, we study the numerical performance of different instances of inexact policy iteration on large-scale MDPs for the design of health policies to control the spread of infectious diseases in epidemiology."],"url":"http://arxiv.org/abs/2404.06136v1","category":"math.OC"}
{"created":"2024-04-09 08:49:41","title":"Learning Model Predictive Control Parameters via Bayesian Optimization for Battery Fast Charging","abstract":"Tuning parameters in model predictive control (MPC) presents significant challenges, particularly when there is a notable discrepancy between the controller's predictions and the actual behavior of the closed-loop plant. This mismatch may stem from factors like substantial model-plant differences, limited prediction horizons that do not cover the entire time of interest, or unforeseen system disturbances. Such mismatches can jeopardize both performance and safety, including constraint satisfaction. Traditional methods address this issue by modifying the finite horizon cost function to better reflect the overall operational cost, learning parts of the prediction model from data, or implementing robust MPC strategies, which might be either computationally intensive or overly cautious. As an alternative, directly optimizing or learning the controller parameters to enhance closed-loop performance has been proposed. We apply Bayesian optimization for efficient learning of unknown model parameters and parameterized constraint backoff terms, aiming to improve closed-loop performance of battery fast charging. This approach establishes a hierarchical control framework where Bayesian optimization directly fine-tunes closed-loop behavior towards a global and long-term objective, while MPC handles lower-level, short-term control tasks. For lithium-ion battery fast charging, we show that the learning approach not only ensures safe operation but also maximizes closed-loop performance. This includes maintaining the battery's operation below its maximum terminal voltage and reducing charging times, all achieved using a standard nominal MPC model with a short horizon and notable initial model-plant mismatch.","sentences":["Tuning parameters in model predictive control (MPC) presents significant challenges, particularly when there is a notable discrepancy between the controller's predictions and the actual behavior of the closed-loop plant.","This mismatch may stem from factors like substantial model-plant differences, limited prediction horizons that do not cover the entire time of interest, or unforeseen system disturbances.","Such mismatches can jeopardize both performance and safety, including constraint satisfaction.","Traditional methods address this issue by modifying the finite horizon cost function to better reflect the overall operational cost, learning parts of the prediction model from data, or implementing robust MPC strategies, which might be either computationally intensive or overly cautious.","As an alternative, directly optimizing or learning the controller parameters to enhance closed-loop performance has been proposed.","We apply Bayesian optimization for efficient learning of unknown model parameters and parameterized constraint backoff terms, aiming to improve closed-loop performance of battery fast charging.","This approach establishes a hierarchical control framework where Bayesian optimization directly fine-tunes closed-loop behavior towards a global and long-term objective, while MPC handles lower-level, short-term control tasks.","For lithium-ion battery fast charging, we show that the learning approach not only ensures safe operation but also maximizes closed-loop performance.","This includes maintaining the battery's operation below its maximum terminal voltage and reducing charging times, all achieved using a standard nominal MPC model with a short horizon and notable initial model-plant mismatch."],"url":"http://arxiv.org/abs/2404.06125v1","category":"eess.SY"}
{"created":"2024-04-09 07:48:49","title":"EVE: Enabling Anyone to Train Robot using Augmented Reality","abstract":"The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities. However, training a robot to automate a task typically requires physical robots and expensive demonstration data from trained human annotators. Consequently, only those with access to physical robots produce demonstrations to train robots. To mitigate this issue, we introduce EVE, an iOS app that enables everyday users to train robots using intuitive augmented reality visualizations without needing a physical robot. With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories. In a user study ($N=14$, $D=30$) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching-physically moving a real robot-in completion time, usability, motion intent communication, enjoyment, and preference ($mean_{p}=0.30$). We conclude by enumerating limitations and design considerations for future AR-based demonstration collection systems for robotics.","sentences":["The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities.","However, training a robot to automate a task typically requires physical robots and expensive demonstration data from trained human annotators.","Consequently, only those with access to physical robots produce demonstrations to train robots.","To mitigate this issue, we introduce EVE, an iOS app that enables everyday users to train robots using intuitive augmented reality visualizations without needing a physical robot.","With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories.","In a user study ($N=14$, $D=30$) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching-physically moving a real robot-in completion time, usability, motion intent communication, enjoyment, and preference ($mean_{p}=0.30$).","We conclude by enumerating limitations and design considerations for future AR-based demonstration collection systems for robotics."],"url":"http://arxiv.org/abs/2404.06089v1","category":"cs.HC"}
{"created":"2024-04-09 07:45:45","title":"Binary Cyclic Transversal Polytopes","abstract":"With every family of finitely many subsets of a finite-dimensional vector space over the Galois-field with two elements we associate a cyclic transversal polytope. It turns out that those polytopes generalize several well-known polytopes that are relevant in combinatorial optimization, among them cut polytopes as well as stable set and matching polytopes. We introduce the class of lifted odd-set inequalities and prove results demonstrating their strength. In particular, we show that they suffice to describe cyclic transversal polytopes if the union of the sets in the family has rank at most two. We also describe extended formulations for cyclic transversal polytopes and introduce a special relaxation hierarchy for them.","sentences":["With every family of finitely many subsets of a finite-dimensional vector space over the Galois-field with two elements we associate a cyclic transversal polytope.","It turns out that those polytopes generalize several well-known polytopes that are relevant in combinatorial optimization, among them cut polytopes as well as stable set and matching polytopes.","We introduce the class of lifted odd-set inequalities and prove results demonstrating their strength.","In particular, we show that they suffice to describe cyclic transversal polytopes if the union of the sets in the family has rank at most two.","We also describe extended formulations for cyclic transversal polytopes and introduce a special relaxation hierarchy for them."],"url":"http://arxiv.org/abs/2404.06088v1","category":"math.CO"}
{"created":"2024-04-09 07:45:06","title":"The Overlap Gap Property limits limit swapping in QAOA","abstract":"The Quantum Approximate Optimization Algorithm (QAOA) is a quantum algorithm designed for combinatorial optimization problem. We show that under the assumption that the Overlap Gap Property (OGP) in the solution space for the Max-$q$-XORSAT is a monotonic increasing function, the swapping of limits in QAOA leads to suboptimal results limited by the OGP. Furthermore, since the performance of QAOA for the pure $q$-spin model matches asymptotically for Max-$q$-XORSAT on large-girth regular hypergraph, we show that the average-case value obtained by QAOA for the pure $q$-spin model for even $q\\ge 4$ is bounded away from optimality even when the algorithm runs indefinitely. This suggests that a necessary condition for the validity of limit swapping in QAOA is the absence of OGP in a given combinatorial optimization problem. A corollary of this is that the spectral gap of a Hamiltonian exhibiting the OGP will close in the thermodynamic limit resulting in a limitation of the quantum adiabatic theorem and efficient optimization of QAOA parameters. Furthermore, the results suggests that even when sub-optimised, the performance of QAOA on spin glass is equal in performance to Montanari's classical algorithm in solving the mean field spin glass problem, the best known classical algorithm.","sentences":["The Quantum Approximate Optimization Algorithm (QAOA) is a quantum algorithm designed for combinatorial optimization problem.","We show that under the assumption that the Overlap Gap Property (OGP) in the solution space for the Max-$q$-XORSAT is a monotonic increasing function, the swapping of limits in QAOA leads to suboptimal results limited by the OGP.","Furthermore, since the performance of QAOA for the pure $q$-spin model matches asymptotically for Max-$q$-XORSAT on large-girth regular hypergraph, we show that the average-case value obtained by QAOA for the pure $q$-spin model for even $q\\ge 4$ is bounded away from optimality even when the algorithm runs indefinitely.","This suggests that a necessary condition for the validity of limit swapping in QAOA is the absence of OGP in a given combinatorial optimization problem.","A corollary of this is that the spectral gap of a Hamiltonian exhibiting the OGP will close in the thermodynamic limit resulting in a limitation of the quantum adiabatic theorem and efficient optimization of QAOA parameters.","Furthermore, the results suggests that even when sub-optimised, the performance of QAOA on spin glass is equal in performance to Montanari's classical algorithm in solving the mean field spin glass problem, the best known classical algorithm."],"url":"http://arxiv.org/abs/2404.06087v1","category":"quant-ph"}
{"created":"2024-04-09 07:39:33","title":"Energetic bounds on gyrokinetic instabilities. Part 4. Bounce-averaged electrons","abstract":"Upper bounds on the growth of instabilities in gyrokinetic systems have recently been derived by considering the optimal perturbations that maximise the growth of a chosen energy norm. This technique has previously been applied to two-species gyrokinetic systems with fully kinetic ions and electrons. However, in tokamaks and stellarators, the expectation from linear instability analyses is that the most important kinetic-electron contribution to ion-scale modes comes from the trapped electrons, which bounce faster than the timescale upon which instabilities evolve. As a result, a fully-kinetic electron response is not required to describe unstable modes in most cases. Here, we apply the optimal mode analysis to a reduced two-species system that consists of fully gyrokinetic ions and bounce-averaged electrons with the aim of finding a tighter bound on ion-scale instabilities in toroidal geometry. This analysis yields bounds that are greatly reduced in comparison to the earlier two-species result. Moreover, if the energy norm is properly chosen, wave-particle resonance effects can be captured, reproducing the stabilisation of density-gradient-driven instabilities in maximum-$J$ devices. The optimal mode analysis also reveals that the maximum-$J$ property has an additional stabilising effect on ion-temperature-gradient-driven instabilities, even in the absence of an electron-free energy source. This effect is explained in terms of the concept of mode inertia, making it distinct from other mechanisms.","sentences":["Upper bounds on the growth of instabilities in gyrokinetic systems have recently been derived by considering the optimal perturbations that maximise the growth of a chosen energy norm.","This technique has previously been applied to two-species gyrokinetic systems with fully kinetic ions and electrons.","However, in tokamaks and stellarators, the expectation from linear instability analyses is that the most important kinetic-electron contribution to ion-scale modes comes from the trapped electrons, which bounce faster than the timescale upon which instabilities evolve.","As a result, a fully-kinetic electron response is not required to describe unstable modes in most cases.","Here, we apply the optimal mode analysis to a reduced two-species system that consists of fully gyrokinetic ions and bounce-averaged electrons with the aim of finding a tighter bound on ion-scale instabilities in toroidal geometry.","This analysis yields bounds that are greatly reduced in comparison to the earlier two-species result.","Moreover, if the energy norm is properly chosen, wave-particle resonance effects can be captured, reproducing the stabilisation of density-gradient-driven instabilities in maximum-$J$ devices.","The optimal mode analysis also reveals that the maximum-$J$ property has an additional stabilising effect on ion-temperature-gradient-driven instabilities, even in the absence of an electron-free energy source.","This effect is explained in terms of the concept of mode inertia, making it distinct from other mechanisms."],"url":"http://arxiv.org/abs/2404.06081v1","category":"physics.plasm-ph"}
