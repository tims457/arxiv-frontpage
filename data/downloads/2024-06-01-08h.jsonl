{"created":"2024-05-29 17:59:20","title":"LLMs Meet Multimodal Generation and Editing: A Survey","abstract":"With the recent advancement in large language models (LLMs), there is a growing interest in combining LLMs with multimodal learning. Previous surveys of multimodal large language models (MLLMs) mainly focus on understanding. This survey elaborates on multimodal generation across different domains, including image, video, 3D, and audio, where we highlight the notable advancements with milestone works in these fields. Specifically, we exhaustively investigate the key technical components behind methods and multimodal datasets utilized in these studies. Moreover, we dig into tool-augmented multimodal agents that can use existing generative models for human-computer interaction. Lastly, we also comprehensively discuss the advancement in AI safety and investigate emerging applications as well as future prospects. Our work provides a systematic and insightful overview of multimodal generation, which is expected to advance the development of Artificial Intelligence for Generative Content (AIGC) and world models. A curated list of all related papers can be found at https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation","sentences":["With the recent advancement in large language models (LLMs), there is a growing interest in combining LLMs with multimodal learning.","Previous surveys of multimodal large language models (MLLMs) mainly focus on understanding.","This survey elaborates on multimodal generation across different domains, including image, video, 3D, and audio, where we highlight the notable advancements with milestone works in these fields.","Specifically, we exhaustively investigate the key technical components behind methods and multimodal datasets utilized in these studies.","Moreover, we dig into tool-augmented multimodal agents that can use existing generative models for human-computer interaction.","Lastly, we also comprehensively discuss the advancement in AI safety and investigate emerging applications as well as future prospects.","Our work provides a systematic and insightful overview of multimodal generation, which is expected to advance the development of Artificial Intelligence for Generative Content (AIGC) and world models.","A curated list of all related papers can be found at https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation"],"url":"http://arxiv.org/abs/2405.19334v1","category":"cs.AI"}
{"created":"2024-05-29 17:57:30","title":"Normative Modules: A Generative Agent Architecture for Learning Norms that Supports Multi-Agent Cooperation","abstract":"Generative agents, which implement behaviors using a large language model (LLM) to interpret and evaluate an environment, has demonstrated the capacity to solve complex tasks across many social and technological domains. However, when these agents interact with other agents and humans in presence of social structures such as existing norms, fostering cooperation between them is a fundamental challenge. In this paper, we develop the framework of a 'Normative Module': an architecture designed to enhance cooperation by enabling agents to recognize and adapt to the normative infrastructure of a given environment. We focus on the equilibrium selection aspect of the cooperation problem and inform our agent design based on the existence of classification institutions that implement correlated equilibrium to provide effective resolution of the equilibrium selection problem. Specifically, the normative module enables agents to learn through peer interactions which of multiple candidate institutions in the environment, does a group treat as authoritative. By enabling normative competence in this sense, agents gain ability to coordinate their sanctioning behaviour; coordinated sanctioning behaviour in turn shapes primary behaviour within a social environment, leading to higher average welfare. We design a new environment that supports institutions and evaluate the proposed framework based on two key criteria derived from agent interactions with peers and institutions: (i) the agent's ability to disregard non-authoritative institutions and (ii) the agent's ability to identify authoritative institutions among several options. We show that these capabilities allow the agent to achieve more stable cooperative outcomes compared to baseline agents without the normative module, paving the way for research in a new avenue of designing environments and agents that account for normative infrastructure.","sentences":["Generative agents, which implement behaviors using a large language model (LLM) to interpret and evaluate an environment, has demonstrated the capacity to solve complex tasks across many social and technological domains.","However, when these agents interact with other agents and humans in presence of social structures such as existing norms, fostering cooperation between them is a fundamental challenge.","In this paper, we develop the framework of a 'Normative Module': an architecture designed to enhance cooperation by enabling agents to recognize and adapt to the normative infrastructure of a given environment.","We focus on the equilibrium selection aspect of the cooperation problem and inform our agent design based on the existence of classification institutions that implement correlated equilibrium to provide effective resolution of the equilibrium selection problem.","Specifically, the normative module enables agents to learn through peer interactions which of multiple candidate institutions in the environment, does a group treat as authoritative.","By enabling normative competence in this sense, agents gain ability to coordinate their sanctioning behaviour; coordinated sanctioning behaviour in turn shapes primary behaviour within a social environment, leading to higher average welfare.","We design a new environment that supports institutions and evaluate the proposed framework based on two key criteria derived from agent interactions with peers and institutions: (i) the agent's ability to disregard non-authoritative institutions and (ii) the agent's ability to identify authoritative institutions among several options.","We show that these capabilities allow the agent to achieve more stable cooperative outcomes compared to baseline agents without the normative module, paving the way for research in a new avenue of designing environments and agents that account for normative infrastructure."],"url":"http://arxiv.org/abs/2405.19328v1","category":"cs.MA"}
{"created":"2024-05-29 17:43:13","title":"Adaptive Generalized Neyman Allocation: Local Asymptotic Minimax Optimal Best Arm Identification","abstract":"This study investigates a local asymptotic minimax optimal strategy for fixed-budget best arm identification (BAI). We propose the Adaptive Generalized Neyman Allocation (AGNA) strategy and show that its worst-case upper bound of the probability of misidentifying the best arm aligns with the worst-case lower bound under the small-gap regime, where the gap between the expected outcomes of the best and suboptimal arms is small. Our strategy corresponds to a generalization of the Neyman allocation for two-armed bandits (Neyman, 1934; Kaufmann et al., 2016) and a refinement of existing strategies such as the ones proposed by Glynn & Juneja (2004) and Shin et al. (2018). Compared to Komiyama et al. (2022), which proposes a minimax rate-optimal strategy, our proposed strategy has a tighter upper bound that exactly matches the lower bound, including the constant terms, by restricting the class of distributions to the class of small-gap distributions. Our result contributes to the longstanding open issue about the existence of asymptotically optimal strategies in fixed-budget BAI, by presenting the local asymptotic minimax optimal strategy.","sentences":["This study investigates a local asymptotic minimax optimal strategy for fixed-budget best arm identification (BAI).","We propose the Adaptive Generalized Neyman Allocation (AGNA) strategy and show that its worst-case upper bound of the probability of misidentifying the best arm aligns with the worst-case lower bound under the small-gap regime, where the gap between the expected outcomes of the best and suboptimal arms is small.","Our strategy corresponds to a generalization of the Neyman allocation for two-armed bandits (Neyman, 1934; Kaufmann et al., 2016) and a refinement of existing strategies such as the ones proposed by Glynn & Juneja (2004) and Shin et al.","(2018).","Compared to Komiyama et al. (2022), which proposes a minimax rate-optimal strategy, our proposed strategy has a tighter upper bound that exactly matches the lower bound, including the constant terms, by restricting the class of distributions to the class of small-gap distributions.","Our result contributes to the longstanding open issue about the existence of asymptotically optimal strategies in fixed-budget BAI, by presenting the local asymptotic minimax optimal strategy."],"url":"http://arxiv.org/abs/2405.19317v1","category":"cs.LG"}
{"created":"2024-05-29 17:39:42","title":"Matryoshka Query Transformer for Large Vision-Language Models","abstract":"Large Vision-Language Models (LVLMs) typically encode an image into a fixed number of visual tokens (e.g., 576) and process these tokens with a language model. Despite their strong performance, LVLMs face challenges in adapting to varying computational constraints. This raises the question: can we achieve flexibility in the number of visual tokens to suit different tasks and computational resources? We answer this with an emphatic yes. Inspired by Matryoshka Representation Learning, we introduce the Matryoshka Query Transformer (MQT), capable of encoding an image into m visual tokens during inference, where m can be any number up to a predefined maximum. This is achieved by employing a query transformer with M latent query tokens to compress the visual embeddings. During each training step, we randomly select m <= M latent query tokens and train the model using only these first m tokens, discarding the rest. Combining MQT with LLaVA, we train a single model once, and flexibly and drastically reduce the number of inference-time visual tokens while maintaining similar or better performance compared to training independent models for each number of tokens. Our model, MQT-LLAVA, matches LLaVA-1.5 performance across 11 benchmarks using a maximum of 256 tokens instead of LLaVA's fixed 576. Reducing to 16 tokens (8x less TFLOPs) only sacrifices the performance by 2.4 points on MMBench. On certain tasks such as ScienceQA and MMMU, we can even go down to only 2 visual tokens with performance drops of just 3% and 6% each. Our exploration of the trade-off between the accuracy and computational cost brought about by the number of visual tokens facilitates future research to achieve the best of both worlds.","sentences":["Large Vision-Language Models (LVLMs) typically encode an image into a fixed number of visual tokens (e.g., 576) and process these tokens with a language model.","Despite their strong performance, LVLMs face challenges in adapting to varying computational constraints.","This raises the question: can we achieve flexibility in the number of visual tokens to suit different tasks and computational resources?","We answer this with an emphatic yes.","Inspired by Matryoshka Representation Learning, we introduce the Matryoshka Query Transformer (MQT), capable of encoding an image into m visual tokens during inference, where m can be any number up to a predefined maximum.","This is achieved by employing a query transformer with M latent query tokens to compress the visual embeddings.","During each training step, we randomly select m <= M latent query tokens and train the model using only these first m tokens, discarding the rest.","Combining MQT with LLaVA, we train a single model once, and flexibly and drastically reduce the number of inference-time visual tokens while maintaining similar or better performance compared to training independent models for each number of tokens.","Our model, MQT-LLAVA, matches LLaVA-1.5 performance across 11 benchmarks using a maximum of 256 tokens instead of LLaVA's fixed 576.","Reducing to 16 tokens (8x less TFLOPs) only sacrifices the performance by 2.4 points on MMBench.","On certain tasks such as ScienceQA and MMMU, we can even go down to only 2 visual tokens with performance drops of just 3% and 6% each.","Our exploration of the trade-off between the accuracy and computational cost brought about by the number of visual tokens facilitates future research to achieve the best of both worlds."],"url":"http://arxiv.org/abs/2405.19315v1","category":"cs.CV"}
{"created":"2024-05-29 17:26:09","title":"Adaptive Image Quality Assessment via Teaching Large Multimodal Model to Compare","abstract":"While recent advancements in large multimodal models (LMMs) have significantly improved their abilities in image quality assessment (IQA) relying on absolute quality rating, how to transfer reliable relative quality comparison outputs to continuous perceptual quality scores remains largely unexplored. To address this gap, we introduce Compare2Score-an all-around LMM-based no-reference IQA (NR-IQA) model, which is capable of producing qualitatively comparative responses and effectively translating these discrete comparative levels into a continuous quality score. Specifically, during training, we present to generate scaled-up comparative instructions by comparing images from the same IQA dataset, allowing for more flexible integration of diverse IQA datasets. Utilizing the established large-scale training corpus, we develop a human-like visual quality comparator. During inference, moving beyond binary choices, we propose a soft comparison method that calculates the likelihood of the test image being preferred over multiple predefined anchor images. The quality score is further optimized by maximum a posteriori estimation with the resulting probability matrix. Extensive experiments on nine IQA datasets validate that the Compare2Score effectively bridges text-defined comparative levels during training with converted single image quality score for inference, surpassing state-of-the-art IQA models across diverse scenarios. Moreover, we verify that the probability-matrix-based inference conversion not only improves the rating accuracy of Compare2Score but also zero-shot general-purpose LMMs, suggesting its intrinsic effectiveness.","sentences":["While recent advancements in large multimodal models (LMMs) have significantly improved their abilities in image quality assessment (IQA) relying on absolute quality rating, how to transfer reliable relative quality comparison outputs to continuous perceptual quality scores remains largely unexplored.","To address this gap, we introduce Compare2Score-an all-around LMM-based no-reference IQA (NR-IQA) model, which is capable of producing qualitatively comparative responses and effectively translating these discrete comparative levels into a continuous quality score.","Specifically, during training, we present to generate scaled-up comparative instructions by comparing images from the same IQA dataset, allowing for more flexible integration of diverse IQA datasets.","Utilizing the established large-scale training corpus, we develop a human-like visual quality comparator.","During inference, moving beyond binary choices, we propose a soft comparison method that calculates the likelihood of the test image being preferred over multiple predefined anchor images.","The quality score is further optimized by maximum a posteriori estimation with the resulting probability matrix.","Extensive experiments on nine IQA datasets validate that the Compare2Score effectively bridges text-defined comparative levels during training with converted single image quality score for inference, surpassing state-of-the-art IQA models across diverse scenarios.","Moreover, we verify that the probability-matrix-based inference conversion not only improves the rating accuracy of Compare2Score but also zero-shot general-purpose LMMs, suggesting its intrinsic effectiveness."],"url":"http://arxiv.org/abs/2405.19298v1","category":"cs.CV"}
{"created":"2024-05-29 17:19:04","title":"Integrating Multi-scale Contextualized Information for Byte-based Neural Machine Translation","abstract":"Subword tokenization is a common method for vocabulary building in Neural Machine Translation (NMT) models. However, increasingly complex tasks have revealed its disadvantages. First, a vocabulary cannot be modified once it is learned, making it hard to adapt to new words. Second, in multilingual translation, the imbalance in data volumes across different languages spreads to the vocabulary, exacerbating translations involving low-resource languages. While byte-based tokenization addresses these issues, byte-based models struggle with the low information density inherent in UTF-8 byte sequences. Previous works enhance token semantics through local contextualization but fail to select an appropriate contextualizing scope based on the input. Consequently, we propose the Multi-Scale Contextualization (MSC) method, which learns contextualized information of varying scales across different hidden state dimensions. It then leverages the attention module to dynamically integrate the multi-scale contextualized information. Experiments show that MSC significantly outperforms subword-based and other byte-based methods in both multilingual and out-of-domain scenarios. Code can be found in https://github.com/ictnlp/Multiscale-Contextualization.","sentences":["Subword tokenization is a common method for vocabulary building in Neural Machine Translation (NMT) models.","However, increasingly complex tasks have revealed its disadvantages.","First, a vocabulary cannot be modified once it is learned, making it hard to adapt to new words.","Second, in multilingual translation, the imbalance in data volumes across different languages spreads to the vocabulary, exacerbating translations involving low-resource languages.","While byte-based tokenization addresses these issues, byte-based models struggle with the low information density inherent in UTF-8 byte sequences.","Previous works enhance token semantics through local contextualization but fail to select an appropriate contextualizing scope based on the input.","Consequently, we propose the Multi-Scale Contextualization (MSC) method, which learns contextualized information of varying scales across different hidden state dimensions.","It then leverages the attention module to dynamically integrate the multi-scale contextualized information.","Experiments show that MSC significantly outperforms subword-based and other byte-based methods in both multilingual and out-of-domain scenarios.","Code can be found in https://github.com/ictnlp/Multiscale-Contextualization."],"url":"http://arxiv.org/abs/2405.19290v1","category":"cs.CL"}
{"created":"2024-05-29 16:59:38","title":"PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications","abstract":"Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline. In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation. Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models. After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses. In the parameter-efficient secondary SFT phase, a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery. Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct doctor downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs. Our model and dataset will be open-source for community development.","sentences":["Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce.","Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures.","To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands.","Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline.","In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation.","Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models.","After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses.","In the parameter-efficient secondary SFT phase, a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery.","Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct doctor downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs.","Our model and dataset will be open-source for community development."],"url":"http://arxiv.org/abs/2405.19266v1","category":"cs.CL"}
{"created":"2024-05-29 16:41:42","title":"Weak Generative Sampler to Efficiently Sample Invariant Distribution of Stochastic Differential Equation","abstract":"Sampling invariant distributions from an Ito diffusion process presents a significant challenge in stochastic simulation. Traditional numerical solvers for stochastic differential equations require both a fine step size and a lengthy simulation period, resulting in both biased and correlated samples. Current deep learning-based method solves the stationary Fokker--Planck equation to determine the invariant probability density function in form of deep neural networks, but they generally do not directly address the problem of sampling from the computed density function. In this work, we introduce a framework that employs a weak generative sampler (WGS) to directly generate independent and identically distributed (iid) samples induced by a transformation map derived from the stationary Fokker--Planck equation. Our proposed loss function is based on the weak form of the Fokker--Planck equation, integrating normalizing flows to characterize the invariant distribution and facilitate sample generation from the base distribution. Our randomized test function circumvents the need for mini-max optimization in the traditional weak formulation. Distinct from conventional generative models, our method neither necessitates the computationally intensive calculation of the Jacobian determinant nor the invertibility of the transformation map. A crucial component of our framework is the adaptively chosen family of test functions in the form of Gaussian kernel functions with centres selected from the generated data samples. Experimental results on several benchmark examples demonstrate the effectiveness of our method, which offers both low computational costs and excellent capability in exploring multiple metastable states.","sentences":["Sampling invariant distributions from an Ito diffusion process presents a significant challenge in stochastic simulation.","Traditional numerical solvers for stochastic differential equations require both a fine step size and a lengthy simulation period, resulting in both biased and correlated samples.","Current deep learning-based method solves the stationary Fokker--Planck equation to determine the invariant probability density function in form of deep neural networks, but they generally do not directly address the problem of sampling from the computed density function.","In this work, we introduce a framework that employs a weak generative sampler (WGS) to directly generate independent and identically distributed (iid) samples induced by a transformation map derived from the stationary Fokker--Planck equation.","Our proposed loss function is based on the weak form of the Fokker--Planck equation, integrating normalizing flows to characterize the invariant distribution and facilitate sample generation from the base distribution.","Our randomized test function circumvents the need for mini-max optimization in the traditional weak formulation.","Distinct from conventional generative models, our method neither necessitates the computationally intensive calculation of the Jacobian determinant nor the invertibility of the transformation map.","A crucial component of our framework is the adaptively chosen family of test functions in the form of Gaussian kernel functions with centres selected from the generated data samples.","Experimental results on several benchmark examples demonstrate the effectiveness of our method, which offers both low computational costs and excellent capability in exploring multiple metastable states."],"url":"http://arxiv.org/abs/2405.19256v1","category":"cs.LG"}
{"created":"2024-05-29 16:31:55","title":"Uniform Inviscid Damping and Inviscid Limit of the 2D Navier-Stokes equation with Navier Boundary Conditions","abstract":"We consider the 2D, incompressible Navier-Stokes equations near the Couette flow, $\\omega^{(NS)} = 1 + \\epsilon \\omega$, set on the channel $\\mathbb{T} \\times [-1, 1]$, supplemented with Navier boundary conditions on the perturbation, $\\omega|_{y = \\pm 1} = 0$. We are simultaneously interested in two asymptotic regimes that are classical in hydrodynamic stability: the long time, $t \\rightarrow \\infty$, stability of background shear flows, and the inviscid limit, $\\nu \\rightarrow 0$ in the presence of boundaries. Given small ($\\epsilon \\ll 1$, but independent of $\\nu$) Gevrey 2- datum, $\\omega_0^{(\\nu)}(x, y)$, that is supported away from the boundaries $y = \\pm 1$, we prove the following results: \\begin{align*} & \\|\\omega^{(\\nu)}(t) - \\frac{1}{2\\pi}\\int \\omega^{(\\nu)}(t) dx \\|_{L^2} \\lesssim \\epsilon e^{-\\delta \\nu^{1/3} t}, & \\text{(Enhanced Dissipation)} \\\\ & \\langle t \\rangle \\|u_1^{(\\nu)}(t) - \\frac{1}{2\\pi} \\int u_1^{(\\nu)}(t) dx\\|_{L^2} + \\langle t \\rangle^2 \\|u_2^{(\\nu)}(t)\\|_{L^2} \\lesssim \\epsilon e^{-\\delta \\nu^{1/3} t}, & \\text{(Inviscid Damping)} \\\\ &\\| \\omega^{(\\nu)} - \\omega^{(0)} \\|_{L^\\infty} \\lesssim \\epsilon \\nu t^{3+\\eta}, \\quad\\quad t \\lesssim \\nu^{-1/(3+\\eta)} & \\text{(Long-time Inviscid Limit)} \\end{align*} This is the first nonlinear asymptotic stability result of its type, which combines three important physical phenomena at the nonlinear level: inviscid damping, enhanced dissipation, and long-time inviscid limit in the presence of boundaries. The techniques we develop represent a major departure from prior works on nonlinear inviscid damping as physical space techniques necessarily play a central role. In this paper, we focus on the primary nonlinear result, while tools for handling the linearized parabolic and elliptic equations are developed in our separate, companion work.","sentences":["We consider the 2D, incompressible Navier-Stokes equations near the Couette flow, $\\omega^{(NS)} = 1 + \\epsilon \\omega$, set on the channel $\\mathbb{T} \\times","[-1, 1]$, supplemented with Navier boundary conditions on the perturbation, $\\omega|_{y = \\pm 1} = 0$.","We are simultaneously interested in two asymptotic regimes that are classical in hydrodynamic stability: the long time, $t \\rightarrow \\infty$, stability of background shear flows, and the inviscid limit, $\\nu \\rightarrow 0$ in the presence of boundaries.","Given small ($\\epsilon \\ll 1$, but independent of $\\nu$)","Gevrey 2- datum, $\\omega_0^{(\\nu)}(x, y)$, that is supported away from the boundaries $y = \\pm 1$, we prove the following results: \\begin{align*} & \\|\\omega^{(\\nu)}(t) - \\frac{1}{2\\pi}\\int \\omega^{(\\nu)}(t) dx \\|_{L^2} \\lesssim \\epsilon e^{-\\delta \\nu^{1/3} t}, & \\text{(Enhanced Dissipation)} \\\\ & \\langle t \\rangle \\|u_1^{(\\nu)}(t) - \\frac{1}{2\\pi} \\int u_1^{(\\nu)}(t) dx\\|_{L^2} +","\\langle t \\rangle^2 \\|u_2^{(\\nu)}(t)\\|_{L^2} \\lesssim \\epsilon e^{-\\delta \\nu^{1/3} t}, & \\text{(Inviscid Damping)} \\\\ &\\| \\omega^{(\\nu)} - \\omega^{(0)} \\|_{L^\\infty} \\lesssim \\epsilon \\nu t^{3+\\eta}, \\quad\\quad t \\lesssim \\nu^{-1/(3+\\eta)} & \\text{(Long-time Inviscid Limit)} \\end{align*} This is the first nonlinear asymptotic stability result of its type, which combines three important physical phenomena at the nonlinear level: inviscid damping, enhanced dissipation, and long-time inviscid limit in the presence of boundaries.","The techniques we develop represent a major departure from prior works on nonlinear inviscid damping as physical space techniques necessarily play a central role.","In this paper, we focus on the primary nonlinear result, while tools for handling the linearized parabolic and elliptic equations are developed in our separate, companion work."],"url":"http://arxiv.org/abs/2405.19249v1","category":"math.AP"}
{"created":"2024-05-29 16:26:57","title":"Efficient Optimal Control of Open Quantum Systems","abstract":"The optimal control problem for open quantum systems can be formulated as a time-dependent Lindbladian that is parameterized by a number of time-dependent control variables. Given an observable and an initial state, the goal is to tune the control variables so that the expected value of some observable with respect to the final state is maximized. In this paper, we present algorithms for solving this optimal control problem efficiently, i.e., having a poly-logarithmic dependency on the system dimension, which is exponentially faster than best-known classical algorithms. Our algorithms are hybrid, consisting of both quantum and classical components. The quantum procedure simulates time-dependent Lindblad evolution that drives the initial state to the final state, and it also provides access to the gradients of the objective function via quantum gradient estimation. The classical procedure uses the gradient information to update the control variables.   At the technical level, we provide the first (to the best of our knowledge) simulation algorithm for time-dependent Lindbladians with an $\\ell_1$-norm dependence. As an alternative, we also present a simulation algorithm in the interaction picture to improve the algorithm for the cases where the time-independent component of a Lindbladian dominates the time-dependent part. On the classical side, we heavily adapt the state-of-the-art classical optimization analysis to interface with the quantum part of our algorithms. Both the quantum simulation techniques and the classical optimization analyses might be of independent interest.","sentences":["The optimal control problem for open quantum systems can be formulated as a time-dependent Lindbladian that is parameterized by a number of time-dependent control variables.","Given an observable and an initial state, the goal is to tune the control variables so that the expected value of some observable with respect to the final state is maximized.","In this paper, we present algorithms for solving this optimal control problem efficiently, i.e., having a poly-logarithmic dependency on the system dimension, which is exponentially faster than best-known classical algorithms.","Our algorithms are hybrid, consisting of both quantum and classical components.","The quantum procedure simulates time-dependent Lindblad evolution that drives the initial state to the final state, and it also provides access to the gradients of the objective function via quantum gradient estimation.","The classical procedure uses the gradient information to update the control variables.   ","At the technical level, we provide the first (to the best of our knowledge) simulation algorithm for time-dependent Lindbladians with an $\\ell_1$-norm dependence.","As an alternative, we also present a simulation algorithm in the interaction picture to improve the algorithm for the cases where the time-independent component of a Lindbladian dominates the time-dependent part.","On the classical side, we heavily adapt the state-of-the-art classical optimization analysis to interface with the quantum part of our algorithms.","Both the quantum simulation techniques and the classical optimization analyses might be of independent interest."],"url":"http://arxiv.org/abs/2405.19245v1","category":"quant-ph"}
{"created":"2024-05-29 16:17:19","title":"Exploring the impact of traffic signal control and connected and automated vehicles on intersections safety: A deep reinforcement learning approach","abstract":"In transportation networks, intersections pose significant risks of collisions due to conflicting movements of vehicles approaching from different directions. To address this issue, various tools can exert influence on traffic safety both directly and indirectly. This study focuses on investigating the impact of adaptive signal control and connected and automated vehicles (CAVs) on intersection safety using a deep reinforcement learning approach. The objective is to assess the individual and combined effects of CAVs and adaptive traffic signal control on traffic safety, considering rear-end and crossing conflicts. The study employs a Deep Q Network (DQN) to regulate traffic signals and driving behaviors of both CAVs and Human Drive Vehicles (HDVs), and uses Time To Collision (TTC) metric to evaluate safety. The findings demonstrate a significant reduction in rear-end and crossing conflicts through the combined implementation of CAVs and DQNs-based traffic signal control. Additionally, the long-term positive effects of CAVs on safety are similar to the short-term effects of combined CAVs and DQNs-based traffic signal control. Overall, the study emphasizes the potential benefits of integrating CAVs and adaptive traffic signal control approaches in order to enhance traffic safety. The findings of this study could provide valuable insights for city officials and transportation authorities in developing effective strategies to improve safety at signalized intersections.","sentences":["In transportation networks, intersections pose significant risks of collisions due to conflicting movements of vehicles approaching from different directions.","To address this issue, various tools can exert influence on traffic safety both directly and indirectly.","This study focuses on investigating the impact of adaptive signal control and connected and automated vehicles (CAVs) on intersection safety using a deep reinforcement learning approach.","The objective is to assess the individual and combined effects of CAVs and adaptive traffic signal control on traffic safety, considering rear-end and crossing conflicts.","The study employs a Deep Q Network (DQN) to regulate traffic signals and driving behaviors of both CAVs and Human Drive Vehicles (HDVs), and uses Time To Collision (TTC) metric to evaluate safety.","The findings demonstrate a significant reduction in rear-end and crossing conflicts through the combined implementation of CAVs and DQNs-based traffic signal control.","Additionally, the long-term positive effects of CAVs on safety are similar to the short-term effects of combined CAVs and DQNs-based traffic signal control.","Overall, the study emphasizes the potential benefits of integrating CAVs and adaptive traffic signal control approaches in order to enhance traffic safety.","The findings of this study could provide valuable insights for city officials and transportation authorities in developing effective strategies to improve safety at signalized intersections."],"url":"http://arxiv.org/abs/2405.19236v1","category":"cs.AI"}
{"created":"2024-05-29 16:13:43","title":"Pseudo-Gevrey Smoothing for the Passive Scalar Equations near Couette","abstract":"In this article, we study the regularity theory for two linear equations that are important in fluid dynamics: the passive scalar equation for (time-varying) shear flows close to Couette in $\\mathbb T \\times [-1,1]$ with vanishing diffusivity $\\nu \\to 0$ and the Poisson equation with right-hand side behaving in similar function spaces to such a passive scalar. The primary motivation for this work is to develop some of the main technical tools required for our treatment of the (nonlinear) 2D Navier-Stokes equations, carried out in our companion work. Both equations are studied with homogeneous Dirichlet conditions (the analogue of a Navier slip-type boundary condition) and the initial condition is taken to be compactly supported away from the walls. We develop smoothing estimates with the following three features:   [1] Uniform-in-$\\nu$ regularity is with respect to $\\partial_x$ and a time-dependent adapted vector-field $\\Gamma$ which approximately commutes with the passive scalar equation (as opposed to `flat' derivatives), and a scaled gradient $\\sqrt{\\nu} \\nabla$;   [2] $(\\partial_x, \\Gamma)$-regularity estimates are performed in Gevrey spaces with regularity that depends on the spatial coordinate, $y$ (what we refer to as `pseudo-Gevrey');   [3] The regularity of these pseudo-Gevrey spaces degenerates to finite regularity near the center of the channel and hence standard Gevrey product rules and other amenable properties do not hold.   Nonlinear analysis in such a delicate functional setting is one of the key ingredients to our companion paper, \\cite{BHIW24a}, which proves the full nonlinear asymptotic stability of the Couette flow with slip boundary conditions. The present article introduces new estimates for the associated linear problems in these degenerate pseudo-Gevrey spaces, which is of independent interest.","sentences":["In this article, we study the regularity theory for two linear equations that are important in fluid dynamics: the passive scalar equation for (time-varying) shear flows close to Couette in $\\mathbb T \\times","[-1,1]$ with vanishing diffusivity $\\nu \\to 0$ and the Poisson equation with right-hand side behaving in similar function spaces to such a passive scalar.","The primary motivation for this work is to develop some of the main technical tools required for our treatment of the (nonlinear) 2D Navier-Stokes equations, carried out in our companion work.","Both equations are studied with homogeneous Dirichlet conditions (the analogue of a Navier slip-type boundary condition) and the initial condition is taken to be compactly supported away from the walls.","We develop smoothing estimates with the following three features:   ","[1] Uniform-in-$\\nu$ regularity is with respect to $\\partial_x$ and a time-dependent adapted vector-field $\\Gamma$ which approximately commutes with the passive scalar equation (as opposed to `flat' derivatives), and a scaled gradient $\\sqrt{\\nu} \\nabla$;   [2] $(\\partial_x, \\Gamma)$-regularity estimates are performed in Gevrey spaces with regularity that depends on the spatial coordinate, $y$ (what we refer to as `pseudo-Gevrey');   ","[3] The regularity of these pseudo-Gevrey spaces degenerates to finite regularity near the center of the channel and hence standard Gevrey product rules and other amenable properties do not hold.   ","Nonlinear analysis in such a delicate functional setting is one of the key ingredients to our companion paper, \\cite{BHIW24a}, which proves the full nonlinear asymptotic stability of the Couette flow with slip boundary conditions.","The present article introduces new estimates for the associated linear problems in these degenerate pseudo-Gevrey spaces, which is of independent interest."],"url":"http://arxiv.org/abs/2405.19233v1","category":"math.AP"}
{"created":"2024-05-29 16:12:14","title":"DiPPeST: Diffusion-based Path Planner for Synthesizing Trajectories Applied on Quadruped Robots","abstract":"We present DiPPeST, a novel image and goal conditioned diffusion-based trajectory generator for quadrupedal robot path planning. DiPPeST is a zero-shot adaptation of our previously introduced diffusion-based 2D global trajectory generator (DiPPeR). The introduced system incorporates a novel strategy for local real-time path refinements, that is reactive to camera input, without requiring any further training, image processing, or environment interpretation techniques. DiPPeST achieves 92% success rate in obstacle avoidance for nominal environments and an average of 88% success rate when tested in environments that are up to 3.5 times more complex in pixel variation than DiPPeR. A visual-servoing framework is developed to allow for real-world execution, tested on the quadruped robot, achieving 80% success rate in different environments and showcasing improved behavior than complex state-of-the-art local planners, in narrow environments.","sentences":["We present DiPPeST, a novel image and goal conditioned diffusion-based trajectory generator for quadrupedal robot path planning.","DiPPeST is a zero-shot adaptation of our previously introduced diffusion-based 2D global trajectory generator (DiPPeR).","The introduced system incorporates a novel strategy for local real-time path refinements, that is reactive to camera input, without requiring any further training, image processing, or environment interpretation techniques.","DiPPeST achieves 92% success rate in obstacle avoidance for nominal environments and an average of 88% success rate when tested in environments that are up to 3.5 times more complex in pixel variation than DiPPeR.","A visual-servoing framework is developed to allow for real-world execution, tested on the quadruped robot, achieving 80% success rate in different environments and showcasing improved behavior than complex state-of-the-art local planners, in narrow environments."],"url":"http://arxiv.org/abs/2405.19232v1","category":"cs.RO"}
{"created":"2024-05-29 16:08:15","title":"Covariate Shift Corrected Conditional Randomization Test","abstract":"Conditional independence tests are crucial across various disciplines in determining the independence of an outcome variable $Y$ from a treatment variable $X$, conditioning on a set of confounders $Z$. The Conditional Randomization Test (CRT) offers a powerful framework for such testing by assuming known distributions of $X \\mid Z$; it controls the Type-I error exactly, allowing for the use of flexible, black-box test statistics. In practice, testing for conditional independence often involves using data from a source population to draw conclusions about a target population. This can be challenging due to covariate shift -- differences in the distribution of $X$, $Z$, and surrogate variables, which can affect the conditional distribution of $Y \\mid X, Z$ -- rendering traditional CRT approaches invalid. To address this issue, we propose a novel Covariate Shift Corrected Pearson Chi-squared Conditional Randomization (csPCR) test. This test adapts to covariate shifts by integrating importance weights and employing the control variates method to reduce variance in the test statistics and thus enhance power. Theoretically, we establish that the csPCR test controls the Type-I error asymptotically. Empirically, through simulation studies, we demonstrate that our method not only maintains control over Type-I errors but also exhibits superior power, confirming its efficacy and practical utility in real-world scenarios where covariate shifts are prevalent. Finally, we apply our methodology to a real-world dataset to assess the impact of a COVID-19 treatment on the 90-day mortality rate among patients.","sentences":["Conditional independence tests are crucial across various disciplines in determining the independence of an outcome variable $Y$ from a treatment variable $X$, conditioning on a set of confounders $Z$. The Conditional Randomization Test (CRT) offers a powerful framework for such testing by assuming known distributions of $X \\mid Z$; it controls the Type-I error exactly, allowing for the use of flexible, black-box test statistics.","In practice, testing for conditional independence often involves using data from a source population to draw conclusions about a target population.","This can be challenging due to covariate shift -- differences in the distribution of $X$, $Z$, and surrogate variables, which can affect the conditional distribution of $Y \\mid X, Z$ -- rendering traditional CRT approaches invalid.","To address this issue, we propose a novel Covariate Shift Corrected Pearson Chi-squared Conditional Randomization (csPCR) test.","This test adapts to covariate shifts by integrating importance weights and employing the control variates method to reduce variance in the test statistics and thus enhance power.","Theoretically, we establish that the csPCR test controls the Type-I error asymptotically.","Empirically, through simulation studies, we demonstrate that our method not only maintains control over Type-I errors but also exhibits superior power, confirming its efficacy and practical utility in real-world scenarios where covariate shifts are prevalent.","Finally, we apply our methodology to a real-world dataset to assess the impact of a COVID-19 treatment on the 90-day mortality rate among patients."],"url":"http://arxiv.org/abs/2405.19231v1","category":"stat.ME"}
{"created":"2024-05-29 16:06:21","title":"ContextBLIP: Doubly Contextual Alignment for Contrastive Image Retrieval from Linguistically Complex Descriptions","abstract":"Image retrieval from contextual descriptions (IRCD) aims to identify an image within a set of minimally contrastive candidates based on linguistically complex text. Despite the success of VLMs, they still significantly lag behind human performance in IRCD. The main challenges lie in aligning key contextual cues in two modalities, where these subtle cues are concealed in tiny areas of multiple contrastive images and within the complex linguistics of textual descriptions. This motivates us to propose ContextBLIP, a simple yet effective method that relies on a doubly contextual alignment scheme for challenging IRCD. Specifically, 1) our model comprises a multi-scale adapter, a matching loss, and a text-guided masking loss. The adapter learns to capture fine-grained visual cues. The two losses enable iterative supervision for the adapter, gradually highlighting the focal patches of a single image to the key textual cues. We term such a way as intra-contextual alignment. 2) Then, ContextBLIP further employs an inter-context encoder to learn dependencies among candidates, facilitating alignment between the text to multiple images. We term this step as inter-contextual alignment. Consequently, the nuanced cues concealed in each modality can be effectively aligned. Experiments on two benchmarks show the superiority of our method. We observe that ContextBLIP can yield comparable results with GPT-4V, despite involving about 7,500 times fewer parameters.","sentences":["Image retrieval from contextual descriptions (IRCD) aims to identify an image within a set of minimally contrastive candidates based on linguistically complex text.","Despite the success of VLMs, they still significantly lag behind human performance in IRCD.","The main challenges lie in aligning key contextual cues in two modalities, where these subtle cues are concealed in tiny areas of multiple contrastive images and within the complex linguistics of textual descriptions.","This motivates us to propose ContextBLIP, a simple yet effective method that relies on a doubly contextual alignment scheme for challenging IRCD.","Specifically, 1) our model comprises a multi-scale adapter, a matching loss, and a text-guided masking loss.","The adapter learns to capture fine-grained visual cues.","The two losses enable iterative supervision for the adapter, gradually highlighting the focal patches of a single image to the key textual cues.","We term such a way as intra-contextual alignment.","2) Then, ContextBLIP further employs an inter-context encoder to learn dependencies among candidates, facilitating alignment between the text to multiple images.","We term this step as inter-contextual alignment.","Consequently, the nuanced cues concealed in each modality can be effectively aligned.","Experiments on two benchmarks show the superiority of our method.","We observe that ContextBLIP can yield comparable results with GPT-4V, despite involving about 7,500 times fewer parameters."],"url":"http://arxiv.org/abs/2405.19226v1","category":"cs.CV"}
{"created":"2024-05-29 16:01:15","title":"Domain adaptation in small-scale and heterogeneous biological datasets","abstract":"Machine learning techniques are steadily becoming more important in modern biology, and are used to build predictive models, discover patterns, and investigate biological problems. However, models trained on one dataset are often not generalizable to other datasets from different cohorts or laboratories, due to differences in the statistical properties of these datasets. These could stem from technical differences, such as the measurement technique used, or from relevant biological differences between the populations studied. Domain adaptation, a type of transfer learning, can alleviate this problem by aligning the statistical distributions of features and samples among different datasets so that similar models can be applied across them. However, a majority of state-of-the-art domain adaptation methods are designed to work with large-scale data, mostly text and images, while biological datasets often suffer from small sample sizes, and possess complexities such as heterogeneity of the feature space. This Review aims to synthetically discuss domain adaptation methods in the context of small-scale and highly heterogeneous biological data. We describe the benefits and challenges of domain adaptation in biological research and critically discuss some of its objectives, strengths, and weaknesses through key representative methodologies. We argue for the incorporation of domain adaptation techniques to the computational biologist's toolkit, with further development of customized approaches.","sentences":["Machine learning techniques are steadily becoming more important in modern biology, and are used to build predictive models, discover patterns, and investigate biological problems.","However, models trained on one dataset are often not generalizable to other datasets from different cohorts or laboratories, due to differences in the statistical properties of these datasets.","These could stem from technical differences, such as the measurement technique used, or from relevant biological differences between the populations studied.","Domain adaptation, a type of transfer learning, can alleviate this problem by aligning the statistical distributions of features and samples among different datasets so that similar models can be applied across them.","However, a majority of state-of-the-art domain adaptation methods are designed to work with large-scale data, mostly text and images, while biological datasets often suffer from small sample sizes, and possess complexities such as heterogeneity of the feature space.","This Review aims to synthetically discuss domain adaptation methods in the context of small-scale and highly heterogeneous biological data.","We describe the benefits and challenges of domain adaptation in biological research and critically discuss some of its objectives, strengths, and weaknesses through key representative methodologies.","We argue for the incorporation of domain adaptation techniques to the computational biologist's toolkit, with further development of customized approaches."],"url":"http://arxiv.org/abs/2405.19221v1","category":"q-bio.QM"}
{"created":"2024-05-29 15:56:36","title":"Compactly supported anomalous weak solutions for 2D Euler equations with vorticity in Hardy spaces","abstract":"In a previous work (arXiv:2306.05948), we constructed by convex integration examples of energy dissipating solutions to the 2D Euler equations on $\\mathbb{R}^2$ with vorticity in the real Hardy space $H^p(\\mathbb{R}^2)$. In the present paper, we develop tools that significantly improve that result in two ways: Firstly, we achieve vorticities in $H^p(\\mathbb{R}^2)$ in the optimal range $p\\in (0,1)$ compared to $(2/3,1)$ in our previous work. Secondly, the solutions constructed here possess compact support and in particular preserve linear and angular momenta.","sentences":["In a previous work (arXiv:2306.05948), we constructed by convex integration examples of energy dissipating solutions to the 2D Euler equations on $\\mathbb{R}^2$ with vorticity in the real Hardy space $H^p(\\mathbb{R}^2)$. In the present paper, we develop tools that significantly improve that result in two ways: Firstly, we achieve vorticities in $H^p(\\mathbb{R}^2)$ in the optimal range $p\\in (0,1)$ compared to $(2/3,1)$ in our previous work.","Secondly, the solutions constructed here possess compact support and in particular preserve linear and angular momenta."],"url":"http://arxiv.org/abs/2405.19214v1","category":"math.AP"}
{"created":"2024-05-29 15:49:09","title":"VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos","abstract":"Video-language understanding tasks have focused on short video clips, often struggling with long-form video understanding tasks. Recently, many long video-language understanding approaches have leveraged the reasoning capabilities of Large Language Models (LLMs) to perform long video QA, transforming videos into densely sampled frame captions, and asking LLMs to respond to text queries over captions. However, the frames used for captioning are often redundant and contain irrelevant information, making dense sampling inefficient, and ignoring the fact that video QA requires varying levels of granularity, with some video segments being highly relevant to the question (needing more fine-grained detail) while others being less relevant. Thus, these LLM-based approaches are prone to missing information and operate on large numbers of irrelevant captions, lowering both performance and efficiency. To address these issues, we introduce VideoTree, a query-adaptive and hierarchical framework for long-video understanding with LLMs. VideoTree dynamically extracts query-related information from a video and builds a tree-based representation for LLM reasoning. First, VideoTree adaptively selects frames for captioning by iteratively clustering frames based on their visual features and scoring clusters using their relevance to the query. Second, it organizes visual clusters into a query-adaptive and hierarchical tree structure; the tree encodes varying levels of granularity, with higher resolution on relevant segments. Finally, VideoTree produces an answer by traversing the tree's keyframes and passing their captions to an LLM answerer. Our method improves both reasoning accuracy and efficiency compared to existing methods: VideoTree achieves a 7.0%, 2.2%, and 2.7% accuracy gain over baselines on the EgoSchema, NExT-QA, and IntentQA benchmarks, respectively, while reducing inference time by 40%.","sentences":["Video-language understanding tasks have focused on short video clips, often struggling with long-form video understanding tasks.","Recently, many long video-language understanding approaches have leveraged the reasoning capabilities of Large Language Models (LLMs) to perform long video QA, transforming videos into densely sampled frame captions, and asking LLMs to respond to text queries over captions.","However, the frames used for captioning are often redundant and contain irrelevant information, making dense sampling inefficient, and ignoring the fact that video QA requires varying levels of granularity, with some video segments being highly relevant to the question (needing more fine-grained detail) while others being less relevant.","Thus, these LLM-based approaches are prone to missing information and operate on large numbers of irrelevant captions, lowering both performance and efficiency.","To address these issues, we introduce VideoTree, a query-adaptive and hierarchical framework for long-video understanding with LLMs.","VideoTree dynamically extracts query-related information from a video and builds a tree-based representation for LLM reasoning.","First, VideoTree adaptively selects frames for captioning by iteratively clustering frames based on their visual features and scoring clusters using their relevance to the query.","Second, it organizes visual clusters into a query-adaptive and hierarchical tree structure; the tree encodes varying levels of granularity, with higher resolution on relevant segments.","Finally, VideoTree produces an answer by traversing the tree's keyframes and passing their captions to an LLM answerer.","Our method improves both reasoning accuracy and efficiency compared to existing methods: VideoTree achieves a 7.0%, 2.2%, and 2.7% accuracy gain over baselines on the EgoSchema, NExT-QA, and IntentQA benchmarks, respectively, while reducing inference time by 40%."],"url":"http://arxiv.org/abs/2405.19209v1","category":"cs.CV"}
{"created":"2024-05-29 15:44:51","title":"Contrastive-Adversarial and Diffusion: Exploring pre-training and fine-tuning strategies for sulcal identification","abstract":"In the last decade, computer vision has witnessed the establishment of various training and learning approaches. Techniques like adversarial learning, contrastive learning, diffusion denoising learning, and ordinary reconstruction learning have become standard, representing state-of-the-art methods extensively employed for fully training or pre-training networks across various vision tasks. The exploration of fine-tuning approaches has emerged as a current focal point, addressing the need for efficient model tuning with reduced GPU memory usage and time costs while enhancing overall performance, as exemplified by methodologies like low-rank adaptation (LoRA). Key questions arise: which pre-training technique yields optimal results - adversarial, contrastive, reconstruction, or diffusion denoising? How does the performance of these approaches vary as the complexity of fine-tuning is adjusted? This study aims to elucidate the advantages of pre-training techniques and fine-tuning strategies to enhance the learning process of neural networks in independent identical distribution (IID) cohorts. We underscore the significance of fine-tuning by examining various cases, including full tuning, decoder tuning, top-level tuning, and fine-tuning of linear parameters using LoRA. Systematic summaries of model performance and efficiency are presented, leveraging metrics such as accuracy, time cost, and memory efficiency. To empirically demonstrate our findings, we focus on a multi-task segmentation-classification challenge involving the paracingulate sulcus (PCS) using different 3D Convolutional Neural Network (CNN) architectures by using the TOP-OSLO cohort comprising 596 subjects.","sentences":["In the last decade, computer vision has witnessed the establishment of various training and learning approaches.","Techniques like adversarial learning, contrastive learning, diffusion denoising learning, and ordinary reconstruction learning have become standard, representing state-of-the-art methods extensively employed for fully training or pre-training networks across various vision tasks.","The exploration of fine-tuning approaches has emerged as a current focal point, addressing the need for efficient model tuning with reduced GPU memory usage and time costs while enhancing overall performance, as exemplified by methodologies like low-rank adaptation (LoRA).","Key questions arise: which pre-training technique yields optimal results - adversarial, contrastive, reconstruction, or diffusion denoising?","How does the performance of these approaches vary as the complexity of fine-tuning is adjusted?","This study aims to elucidate the advantages of pre-training techniques and fine-tuning strategies to enhance the learning process of neural networks in independent identical distribution (IID) cohorts.","We underscore the significance of fine-tuning by examining various cases, including full tuning, decoder tuning, top-level tuning, and fine-tuning of linear parameters using LoRA.","Systematic summaries of model performance and efficiency are presented, leveraging metrics such as accuracy, time cost, and memory efficiency.","To empirically demonstrate our findings, we focus on a multi-task segmentation-classification challenge involving the paracingulate sulcus (PCS) using different 3D Convolutional Neural Network (CNN) architectures by using the TOP-OSLO cohort comprising 596 subjects."],"url":"http://arxiv.org/abs/2405.19204v1","category":"eess.IV"}
{"created":"2024-05-29 15:14:28","title":"Greedy Kernel Methods for Approximating Breakthrough Curves for Reactive Flow from 3D Porous Geometry Data","abstract":"We address the challenging application of 3D pore scale reactive flow under varying geometry parameters. The task is to predict time-dependent integral quantities, i.e., breakthrough curves, from the given geometries. As the 3D reactive flow simulation is highly complex and computationally expensive, we are interested in data-based surrogates that can give a rapid prediction of the target quantities of interest. This setting is an example of an application with scarce data, i.e., only having available few data samples, while the input and output dimensions are high. In this scarce data setting, standard machine learning methods are likely to ail. Therefore, we resort to greedy kernel approximation schemes that have shown to be efficient meshless approximation techniques for multivariate functions. We demonstrate that such methods can efficiently be used in the high-dimensional input/output case under scarce data. Especially, we show that the vectorial kernel orthogonal greedy approximation (VKOGA) procedure with a data-adapted two-layer kernel yields excellent predictors for learning from 3D geometry voxel data via both morphological descriptors or principal component analysis.","sentences":["We address the challenging application of 3D pore scale reactive flow under varying geometry parameters.","The task is to predict time-dependent integral quantities, i.e., breakthrough curves, from the given geometries.","As the 3D reactive flow simulation is highly complex and computationally expensive, we are interested in data-based surrogates that can give a rapid prediction of the target quantities of interest.","This setting is an example of an application with scarce data, i.e., only having available few data samples, while the input and output dimensions are high.","In this scarce data setting, standard machine learning methods are likely to ail.","Therefore, we resort to greedy kernel approximation schemes that have shown to be efficient meshless approximation techniques for multivariate functions.","We demonstrate that such methods can efficiently be used in the high-dimensional input/output case under scarce data.","Especially, we show that the vectorial kernel orthogonal greedy approximation (VKOGA) procedure with a data-adapted two-layer kernel yields excellent predictors for learning from 3D geometry voxel data via both morphological descriptors or principal component analysis."],"url":"http://arxiv.org/abs/2405.19170v1","category":"math.NA"}
{"created":"2024-05-29 14:40:24","title":"A quantum implementation of high-order power method for estimating geometric entanglement of pure states","abstract":"Entanglement is one of the fundamental properties of a quantum state and is a crucial differentiator between classical and quantum computation. There are many ways to define entanglement and its measure, depending on the problem or application under consideration. Each of these measures may be computed or approximated by multiple methods. However, hardly any of these methods can be run on near-term quantum hardware. This work presents a quantum adaptation of the iterative higher-order power method for estimating the geometric measure of entanglement of multi-qubit pure states using rank-1 tensor approximation. This method is executable on current (hybrid) quantum hardware and does not depend on quantum memory. We study the effect of noise on the algorithm using a simple theoretical model based on the standard depolarising channel. This model allows us to post hoc mitigate the effects of noise on the results of the computation.","sentences":["Entanglement is one of the fundamental properties of a quantum state and is a crucial differentiator between classical and quantum computation.","There are many ways to define entanglement and its measure, depending on the problem or application under consideration.","Each of these measures may be computed or approximated by multiple methods.","However, hardly any of these methods can be run on near-term quantum hardware.","This work presents a quantum adaptation of the iterative higher-order power method for estimating the geometric measure of entanglement of multi-qubit pure states using rank-1 tensor approximation.","This method is executable on current (hybrid) quantum hardware and does not depend on quantum memory.","We study the effect of noise on the algorithm using a simple theoretical model based on the standard depolarising channel.","This model allows us to post hoc mitigate the effects of noise on the results of the computation."],"url":"http://arxiv.org/abs/2405.19134v1","category":"quant-ph"}
{"created":"2024-05-29 14:37:48","title":"Learning Interpretable Scheduling Algorithms for Data Processing Clusters","abstract":"Workloads in data processing clusters are often represented in the form of DAG (Directed Acyclic Graph) jobs. Scheduling DAG jobs is challenging. Simple heuristic scheduling algorithms are often adopted in practice in production data centres. There is much room for scheduling performance optimisation for cost saving. Recently, reinforcement learning approaches (like decima) have been attempted to optimise DAG job scheduling and demonstrate clear performance gain in comparison to traditional algorithms. However, reinforcement learning (RL) approaches face their own problems in real-world deployment. In particular, their black-box decision making processes and generalizability in unseen workloads may add a non-trivial burden to the cluster administrators. Moreover, adapting RL models on unseen workloads often requires significant amount of training data, which leaves edge cases run in a sub-optimal mode. To fill the gap, we propose a new method to distill a simple scheduling policy based on observations of the behaviours of a complex deep learning model. The simple model not only provides interpretability of scheduling decisions, but also adaptive to edge cases easily through tuning. We show that our method achieves high fidelity to the decisions made by deep learning models and outperforms these models when additional heuristics are taken into account.","sentences":["Workloads in data processing clusters are often represented in the form of DAG (Directed Acyclic Graph) jobs.","Scheduling DAG jobs is challenging.","Simple heuristic scheduling algorithms are often adopted in practice in production data centres.","There is much room for scheduling performance optimisation for cost saving.","Recently, reinforcement learning approaches (like decima) have been attempted to optimise DAG job scheduling and demonstrate clear performance gain in comparison to traditional algorithms.","However, reinforcement learning (RL) approaches face their own problems in real-world deployment.","In particular, their black-box decision making processes and generalizability in unseen workloads may add a non-trivial burden to the cluster administrators.","Moreover, adapting RL models on unseen workloads often requires significant amount of training data, which leaves edge cases run in a sub-optimal mode.","To fill the gap, we propose a new method to distill a simple scheduling policy based on observations of the behaviours of a complex deep learning model.","The simple model not only provides interpretability of scheduling decisions, but also adaptive to edge cases easily through tuning.","We show that our method achieves high fidelity to the decisions made by deep learning models and outperforms these models when additional heuristics are taken into account."],"url":"http://arxiv.org/abs/2405.19131v1","category":"cs.DC"}
{"created":"2024-05-29 14:31:39","title":"Early Detection of Critical Urban Events using Mobile Phone Network Data","abstract":"Network Signalling Data (NSD) have the potential to provide continuous spatio-temporal information about the presence, mobility, and usage patterns of cell phone services by individuals. Such information is invaluable for monitoring large urban areas and supporting the implementation of decision-making services. When analyzed in real time, NSD can enable the early detection of critical urban events, including fires, large accidents, stampedes, terrorist attacks, and sports and leisure gatherings, especially if these events significantly impact mobile phone network activity in the affected areas. This paper presents empirical evidence that advanced NSD can detect anomalies in mobile traffic service consumption, attributable to critical urban events, with fine spatial and temporal resolutions. We introduce two methodologies for real-time anomaly detection from multivariate time series extracted from large-scale NSD, utilizing a range of algorithms adapted from the state-of-the-art in unsupervised machine learning techniques for anomaly detection. Our research includes a comprehensive quantitative evaluation of these algorithms on a large-scale dataset of NSD service consumption for the Paris region. The evaluation uses an original dataset of documented critical or unusual urban events. This dataset has been built as a ground truth basis for assessing the algorithms performance. The obtained results demonstrate that our framework can detect unusual events almost instantaneously and locate the affected areas with high precision, largely outperforming random classifiers. This efficiency and effectiveness underline the potential of NSD-based anomaly detection in significantly enhancing emergency response strategies and urban planning.","sentences":["Network Signalling Data (NSD) have the potential to provide continuous spatio-temporal information about the presence, mobility, and usage patterns of cell phone services by individuals.","Such information is invaluable for monitoring large urban areas and supporting the implementation of decision-making services.","When analyzed in real time, NSD can enable the early detection of critical urban events, including fires, large accidents, stampedes, terrorist attacks, and sports and leisure gatherings, especially if these events significantly impact mobile phone network activity in the affected areas.","This paper presents empirical evidence that advanced NSD can detect anomalies in mobile traffic service consumption, attributable to critical urban events, with fine spatial and temporal resolutions.","We introduce two methodologies for real-time anomaly detection from multivariate time series extracted from large-scale NSD, utilizing a range of algorithms adapted from the state-of-the-art in unsupervised machine learning techniques for anomaly detection.","Our research includes a comprehensive quantitative evaluation of these algorithms on a large-scale dataset of NSD service consumption for the Paris region.","The evaluation uses an original dataset of documented critical or unusual urban events.","This dataset has been built as a ground truth basis for assessing the algorithms performance.","The obtained results demonstrate that our framework can detect unusual events almost instantaneously and locate the affected areas with high precision, largely outperforming random classifiers.","This efficiency and effectiveness underline the potential of NSD-based anomaly detection in significantly enhancing emergency response strategies and urban planning."],"url":"http://arxiv.org/abs/2405.19125v1","category":"cs.CY"}
{"created":"2024-05-29 14:28:17","title":"Apparent horizon tracking in supercritical solutions of the Einstein-scalar field equations in spherical symmetry in affine-null coordinates","abstract":"Choptuik's critical phenomena in general relativity is revisited in the affine-null metric formulation of Einstein's equations for a massless scalar field in spherical symmetry. Numerical solutions are obtained by evolution of initial data using pseudo-spectral methods. The underlying system consists of differential equations along the outgoing null rays which can be solved in sequential form. A new two-parameter family of initial data is presented for which these equations can be integrated analytically. Specific choices of the initial data parameters correspond to either an asymptotically flat null cone, a black hole event horizon or the singular interior of a black hole. Our main focus is on the interior features of a black hole, for which the affine-null system is especially well adapted. We present both analytic and numerical results describing the geometric properties of the apparent horizon and final singularity. Using a re-gridding technique for the affine parameter, numerical evolution of initially asymptotically flat supercritical data can be continued inside the event horizon and track the apparent horizon up to the formation of the final singularity.","sentences":["Choptuik's critical phenomena in general relativity is revisited in the affine-null metric formulation of Einstein's equations for a massless scalar field in spherical symmetry.","Numerical solutions are obtained by evolution of initial data using pseudo-spectral methods.","The underlying system consists of differential equations along the outgoing null rays which can be solved in sequential form.","A new two-parameter family of initial data is presented for which these equations can be integrated analytically.","Specific choices of the initial data parameters correspond to either an asymptotically flat null cone, a black hole event horizon or the singular interior of a black hole.","Our main focus is on the interior features of a black hole, for which the affine-null system is especially well adapted.","We present both analytic and numerical results describing the geometric properties of the apparent horizon and final singularity.","Using a re-gridding technique for the affine parameter, numerical evolution of initially asymptotically flat supercritical data can be continued inside the event horizon and track the apparent horizon up to the formation of the final singularity."],"url":"http://arxiv.org/abs/2405.19122v1","category":"gr-qc"}
{"created":"2024-05-29 14:24:35","title":"MHD simulations of the space weather in Proxima b: Habitability conditions and radio emission","abstract":"The habitability of exoplanets hosted by M-dwarf stars dramatically depends on their space weather. We present 3D magneto-hydrodynamic simulations to characterise the magneto-plasma environment and thus the habitability of the Earth-like planet Proxima b when it is subject to both calm and extreme (CME-like) space weather conditions. We study the role of the stellar wind and planetary magnetic field, and determine the radio emission arising from the interaction between the stellar wind of Proxima and the magnetosphere of its planet Proxima b. We find that if Prox b has a magnetic field similar to that of the Earth ($B_{\\rm p} = B_\\oplus \\approx 0.32$ G) or larger, the magnetopause standoff distance is large enough to shield the surface from the stellar wind for essentially any planetary tilt but the most extreme values (close to $90^{\\circ} $), under a calm space weather. Even if Proxima b is subject to more extreme space weather conditions, the planet is well shielded by an Earth-like magnetosphere ($B_{\\rm p} \\approx B_\\oplus$; $ \\approx 23.5^{\\circ}$), or if it has tilt smaller than that of the Earth. For calm space weather conditions, the radio emission caused by the day-side reconnection regions can be as high as 7$\\times10^{19}$ erg s$^{-1}$ in the super-Alfv\\'enic regime, and is on average almost an order of magnitude larger than the radio emission in the sub-Alfv\\'enic cases, due to the much larger contribution of the bow shock. We also find that the energy dissipation at the bow shock is independent of the angle between the planet's magnetic dipole and the incident stellar wind flow. If Prox b is subject to extreme space weather conditions, the radio emission is more than two orders of magnitude larger than under calm space weather conditions. This result yields expectations for a direct detection--from Earth--in radio of giant planets in close-in orbits.","sentences":["The habitability of exoplanets hosted by M-dwarf stars dramatically depends on their space weather.","We present 3D magneto-hydrodynamic simulations to characterise the magneto-plasma environment and thus the habitability of the Earth-like planet Proxima b when it is subject to both calm and extreme (CME-like) space weather conditions.","We study the role of the stellar wind and planetary magnetic field, and determine the radio emission arising from the interaction between the stellar wind of Proxima and the magnetosphere of its planet Proxima b.","We find that if Prox b has a magnetic field similar to that of the Earth ($B_{\\rm p} =","B_\\oplus","\\approx 0.32$ G) or larger, the magnetopause standoff distance is large enough to shield the surface from the stellar wind for essentially any planetary tilt but the most extreme values (close to $90^{\\circ} $), under a calm space weather.","Even if Proxima b is subject to more extreme space weather conditions, the planet is well shielded by an Earth-like magnetosphere ($B_{\\rm p} \\approx B_\\oplus$; $ \\approx 23.5^{\\circ}$), or if it has tilt smaller than that of the Earth.","For calm space weather conditions, the radio emission caused by the day-side reconnection regions can be as high as 7$\\times10^{19}$ erg s$^{-1}$ in the super-Alfv\\'enic regime, and is on average almost an order of magnitude larger than the radio emission in the sub-Alfv\\'enic cases, due to the much larger contribution of the bow shock.","We also find that the energy dissipation at the bow shock is independent of the angle between the planet's magnetic dipole and the incident stellar wind flow.","If Prox b is subject to extreme space weather conditions, the radio emission is more than two orders of magnitude larger than under calm space weather conditions.","This result yields expectations for a direct detection--from Earth--in radio of giant planets in close-in orbits."],"url":"http://arxiv.org/abs/2405.19116v1","category":"astro-ph.EP"}
{"created":"2024-05-29 14:07:44","title":"Voice Jailbreak Attacks Against GPT-4o","abstract":"Recently, the concept of artificial assistants has evolved from science fiction into real-world applications. GPT-4o, the newest multimodal large language model (MLLM) across audio, vision, and text, has further blurred the line between fiction and reality by enabling more natural human-computer interactions. However, the advent of GPT-4o's voice mode may also introduce a new attack surface. In this paper, we present the first systematic measurement of jailbreak attacks against the voice mode of GPT-4o. We show that GPT-4o demonstrates good resistance to forbidden questions and text jailbreak prompts when directly transferring them to voice mode. This resistance is primarily due to GPT-4o's internal safeguards and the difficulty of adapting text jailbreak prompts to voice mode. Inspired by GPT-4o's human-like behaviors, we propose VoiceJailbreak, a novel voice jailbreak attack that humanizes GPT-4o and attempts to persuade it through fictional storytelling (setting, character, and plot). VoiceJailbreak is capable of generating simple, audible, yet effective jailbreak prompts, which significantly increases the average attack success rate (ASR) from 0.033 to 0.778 in six forbidden scenarios. We also conduct extensive experiments to explore the impacts of interaction steps, key elements of fictional writing, and different languages on VoiceJailbreak's effectiveness and further enhance the attack performance with advanced fictional writing techniques. We hope our study can assist the research community in building more secure and well-regulated MLLMs.","sentences":["Recently, the concept of artificial assistants has evolved from science fiction into real-world applications.","GPT-4o, the newest multimodal large language model (MLLM) across audio, vision, and text, has further blurred the line between fiction and reality by enabling more natural human-computer interactions.","However, the advent of GPT-4o's voice mode may also introduce a new attack surface.","In this paper, we present the first systematic measurement of jailbreak attacks against the voice mode of GPT-4o.","We show that GPT-4o demonstrates good resistance to forbidden questions and text jailbreak prompts when directly transferring them to voice mode.","This resistance is primarily due to GPT-4o's internal safeguards and the difficulty of adapting text jailbreak prompts to voice mode.","Inspired by GPT-4o's human-like behaviors, we propose VoiceJailbreak, a novel voice jailbreak attack that humanizes GPT-4o and attempts to persuade it through fictional storytelling (setting, character, and plot).","VoiceJailbreak is capable of generating simple, audible, yet effective jailbreak prompts, which significantly increases the average attack success rate (ASR) from 0.033 to 0.778 in six forbidden scenarios.","We also conduct extensive experiments to explore the impacts of interaction steps, key elements of fictional writing, and different languages on VoiceJailbreak's effectiveness and further enhance the attack performance with advanced fictional writing techniques.","We hope our study can assist the research community in building more secure and well-regulated MLLMs."],"url":"http://arxiv.org/abs/2405.19103v1","category":"cs.CR"}
{"created":"2024-05-29 14:05:16","title":"Efficient Black-box Adversarial Attacks via Bayesian Optimization Guided by a Function Prior","abstract":"This paper studies the challenging black-box adversarial attack that aims to generate adversarial examples against a black-box model by only using output feedback of the model to input queries. Some previous methods improve the query efficiency by incorporating the gradient of a surrogate white-box model into query-based attacks due to the adversarial transferability. However, the localized gradient is not informative enough, making these methods still query-intensive. In this paper, we propose a Prior-guided Bayesian Optimization (P-BO) algorithm that leverages the surrogate model as a global function prior in black-box adversarial attacks. As the surrogate model contains rich prior information of the black-box one, P-BO models the attack objective with a Gaussian process whose mean function is initialized as the surrogate model's loss. Our theoretical analysis on the regret bound indicates that the performance of P-BO may be affected by a bad prior. Therefore, we further propose an adaptive integration strategy to automatically adjust a coefficient on the function prior by minimizing the regret bound. Extensive experiments on image classifiers and large vision-language models demonstrate the superiority of the proposed algorithm in reducing queries and improving attack success rates compared with the state-of-the-art black-box attacks. Code is available at https://github.com/yibo-miao/PBO-Attack.","sentences":["This paper studies the challenging black-box adversarial attack that aims to generate adversarial examples against a black-box model by only using output feedback of the model to input queries.","Some previous methods improve the query efficiency by incorporating the gradient of a surrogate white-box model into query-based attacks due to the adversarial transferability.","However, the localized gradient is not informative enough, making these methods still query-intensive.","In this paper, we propose a Prior-guided Bayesian Optimization (P-BO) algorithm that leverages the surrogate model as a global function prior in black-box adversarial attacks.","As the surrogate model contains rich prior information of the black-box one, P-BO models the attack objective with a Gaussian process whose mean function is initialized as the surrogate model's loss.","Our theoretical analysis on the regret bound indicates that the performance of P-BO may be affected by a bad prior.","Therefore, we further propose an adaptive integration strategy to automatically adjust a coefficient on the function prior by minimizing the regret bound.","Extensive experiments on image classifiers and large vision-language models demonstrate the superiority of the proposed algorithm in reducing queries and improving attack success rates compared with the state-of-the-art black-box attacks.","Code is available at https://github.com/yibo-miao/PBO-Attack."],"url":"http://arxiv.org/abs/2405.19098v1","category":"cs.LG"}
{"created":"2024-05-29 13:49:44","title":"MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors","abstract":"Model editing aims to efficiently alter the behavior of Large Language Models (LLMs) within a desired scope, while ensuring no adverse impact on other inputs. Recent years have witnessed various model editing methods been proposed. However, these methods either exhibit poor overall performance or struggle to strike a balance between generalization and locality. We propose MOMoE, a model editing adapter utilizing a Mixture of Experts (MoE) architecture with a knowledge anchor routing strategy. MOMoE updates knowledge using a bypass MoE structure, keeping the original parameters unchanged to preserve the general ability of LLMs. And, the knowledge anchor routing ensures that inputs requiring similar knowledge are routed to the same expert, thereby enhancing the generalization of the updated knowledge. Experimental results show the superiority of our approach over both batch editing and sequential batch editing tasks, exhibiting exceptional overall performance alongside outstanding balance between generalization and locality. Our code will be available.","sentences":["Model editing aims to efficiently alter the behavior of Large Language Models (LLMs) within a desired scope, while ensuring no adverse impact on other inputs.","Recent years have witnessed various model editing methods been proposed.","However, these methods either exhibit poor overall performance or struggle to strike a balance between generalization and locality.","We propose MOMoE, a model editing adapter utilizing a Mixture of Experts (MoE) architecture with a knowledge anchor routing strategy.","MOMoE updates knowledge using a bypass MoE structure, keeping the original parameters unchanged to preserve the general ability of LLMs.","And, the knowledge anchor routing ensures that inputs requiring similar knowledge are routed to the same expert, thereby enhancing the generalization of the updated knowledge.","Experimental results show the superiority of our approach over both batch editing and sequential batch editing tasks, exhibiting exceptional overall performance alongside outstanding balance between generalization and locality.","Our code will be available."],"url":"http://arxiv.org/abs/2405.19086v1","category":"cs.CL"}
{"created":"2024-05-29 13:47:32","title":"Patch-enhanced Mask Encoder Prompt Image Generation","abstract":"Artificial Intelligence Generated Content(AIGC), known for its superior visual results, represents a promising mitigation method for high-cost advertising applications. Numerous approaches have been developed to manipulate generated content under different conditions. However, a crucial limitation lies in the accurate description of products in advertising applications. Applying previous methods directly may lead to considerable distortion and deformation of advertised products, primarily due to oversimplified content control conditions. Hence, in this work, we propose a patch-enhanced mask encoder approach to ensure accurate product descriptions while preserving diverse backgrounds. Our approach consists of three components Patch Flexible Visibility, Mask Encoder Prompt Adapter and an image Foundation Model. Patch Flexible Visibility is used for generating a more reasonable background image. Mask Encoder Prompt Adapter enables region-controlled fusion. We also conduct an analysis of the structure and operational mechanisms of the Generation Module. Experimental results show our method can achieve the highest visual results and FID scores compared with other methods.","sentences":["Artificial Intelligence Generated Content(AIGC), known for its superior visual results, represents a promising mitigation method for high-cost advertising applications.","Numerous approaches have been developed to manipulate generated content under different conditions.","However, a crucial limitation lies in the accurate description of products in advertising applications.","Applying previous methods directly may lead to considerable distortion and deformation of advertised products, primarily due to oversimplified content control conditions.","Hence, in this work, we propose a patch-enhanced mask encoder approach to ensure accurate product descriptions while preserving diverse backgrounds.","Our approach consists of three components Patch Flexible Visibility, Mask Encoder Prompt Adapter and an image Foundation Model.","Patch Flexible Visibility is used for generating a more reasonable background image.","Mask Encoder Prompt Adapter enables region-controlled fusion.","We also conduct an analysis of the structure and operational mechanisms of the Generation Module.","Experimental results show our method can achieve the highest visual results and FID scores compared with other methods."],"url":"http://arxiv.org/abs/2405.19085v1","category":"cs.AI"}
{"created":"2024-05-29 13:36:36","title":"OMPO: A Unified Framework for RL under Policy and Dynamics Shifts","abstract":"Training reinforcement learning policies using environment interaction data collected from varying policies or dynamics presents a fundamental challenge. Existing works often overlook the distribution discrepancies induced by policy or dynamics shifts, or rely on specialized algorithms with task priors, thus often resulting in suboptimal policy performances and high learning variances. In this paper, we identify a unified strategy for online RL policy learning under diverse settings of policy and dynamics shifts: transition occupancy matching. In light of this, we introduce a surrogate policy learning objective by considering the transition occupancy discrepancies and then cast it into a tractable min-max optimization problem through dual reformulation. Our method, dubbed Occupancy-Matching Policy Optimization (OMPO), features a specialized actor-critic structure equipped with a distribution discriminator and a small-size local buffer. We conduct extensive experiments based on the OpenAI Gym, Meta-World, and Panda Robots environments, encompassing policy shifts under stationary and nonstationary dynamics, as well as domain adaption. The results demonstrate that OMPO outperforms the specialized baselines from different categories in all settings. We also find that OMPO exhibits particularly strong performance when combined with domain randomization, highlighting its potential in RL-based robotics applications","sentences":["Training reinforcement learning policies using environment interaction data collected from varying policies or dynamics presents a fundamental challenge.","Existing works often overlook the distribution discrepancies induced by policy or dynamics shifts, or rely on specialized algorithms with task priors, thus often resulting in suboptimal policy performances and high learning variances.","In this paper, we identify a unified strategy for online RL policy learning under diverse settings of policy and dynamics shifts: transition occupancy matching.","In light of this, we introduce a surrogate policy learning objective by considering the transition occupancy discrepancies and then cast it into a tractable min-max optimization problem through dual reformulation.","Our method, dubbed Occupancy-Matching Policy Optimization (OMPO), features a specialized actor-critic structure equipped with a distribution discriminator and a small-size local buffer.","We conduct extensive experiments based on the OpenAI Gym, Meta-World, and Panda Robots environments, encompassing policy shifts under stationary and nonstationary dynamics, as well as domain adaption.","The results demonstrate that OMPO outperforms the specialized baselines from different categories in all settings.","We also find that OMPO exhibits particularly strong performance when combined with domain randomization, highlighting its potential in RL-based robotics applications"],"url":"http://arxiv.org/abs/2405.19080v1","category":"cs.LG"}
{"created":"2024-05-29 13:25:30","title":"Computational bounds on randomized algorithms for online bin stretching","abstract":"A frequently studied performance measure in online optimization is competitive analysis. It corresponds to the worst-case ratio, over all possible inputs of an algorithm, between the performance of the algorithm and the optimal offline performance. However, this analysis may be too pessimistic to give valuable insight on a problem. Several workarounds exist, such as randomized algorithms. This paper aims to propose computational methods to construct randomized algorithms and to bound their performance on the classical online bin stretching problem. A game theory method is adapted to construct lower bounds on the performance of randomized online algorithms via linear programming. Another computational method is then proposed to construct randomized algorithms which perform better than the best deterministic algorithms known. Finally, another lower bound method for a restricted class of randomized algorithm for this problem is proposed.","sentences":["A frequently studied performance measure in online optimization is competitive analysis.","It corresponds to the worst-case ratio, over all possible inputs of an algorithm, between the performance of the algorithm and the optimal offline performance.","However, this analysis may be too pessimistic to give valuable insight on a problem.","Several workarounds exist, such as randomized algorithms.","This paper aims to propose computational methods to construct randomized algorithms and to bound their performance on the classical online bin stretching problem.","A game theory method is adapted to construct lower bounds on the performance of randomized online algorithms via linear programming.","Another computational method is then proposed to construct randomized algorithms which perform better than the best deterministic algorithms known.","Finally, another lower bound method for a restricted class of randomized algorithm for this problem is proposed."],"url":"http://arxiv.org/abs/2405.19071v1","category":"math.OC"}
{"created":"2024-05-29 13:20:50","title":"Implementing arbitrary multi-mode continuous-variable quantum gates with fixed non-Gaussian states and adaptive linear optics","abstract":"Non-Gaussian quantum gates are essential components for optical quantum information processing. However, the efficient implementation of practically important multi-mode higher-order non-Gaussian gates has not been comprehensively studied. We propose a measurement-based method to directly implement general, multi-mode, and higher-order non-Gaussian gates using only fixed non-Gaussian ancillary states and adaptive linear optics. Compared to existing methods, our method allows for a more resource-efficient and experimentally feasible implementation of multi-mode gates that are important for various applications in optical quantum technology, such as the two-mode cubic quantum non-demolition gate or the three-mode continuous-variable Toffoli gate, and their higher-order extensions. Our results will expedite the progress toward fault-tolerant universal quantum computing with light.","sentences":["Non-Gaussian quantum gates are essential components for optical quantum information processing.","However, the efficient implementation of practically important multi-mode higher-order non-Gaussian gates has not been comprehensively studied.","We propose a measurement-based method to directly implement general, multi-mode, and higher-order non-Gaussian gates using only fixed non-Gaussian ancillary states and adaptive linear optics.","Compared to existing methods, our method allows for a more resource-efficient and experimentally feasible implementation of multi-mode gates that are important for various applications in optical quantum technology, such as the two-mode cubic quantum non-demolition gate or the three-mode continuous-variable Toffoli gate, and their higher-order extensions.","Our results will expedite the progress toward fault-tolerant universal quantum computing with light."],"url":"http://arxiv.org/abs/2405.19067v1","category":"quant-ph"}
{"created":"2024-05-29 13:08:50","title":"Fracture metamaterials with on-demand crack paths enabled by bending","abstract":"In many scenarios -- when we bite food or during a crash -- fracture is inevitable. Finding solutions to steer fracture to mitigate its impact or turn it into a purposeful functionality, is therefore crucial. Strategies using composites, changes in chemical composition or crystal orientation, have proven to be very efficient, but the crack path control remains limited and has not been achieved in load-bearing structures. Here, we introduce fracture metamaterials consisting of slender elements whose bending enables large elastic deformation as fracture propagates. This interplay between bending and fracture enables tunable energy dissipation and the design of on-demand crack paths of arbitrary complexity. To this end, we use topology optimisation to create unit cells with anisotropic fracture energy, which we then tile up to realize fracture metamaterials with uniform density that we 3D-print. The thin ligaments that constitute the unit cells confer them a strikingly distinct response in tension and shear, and we show that by controlling the orientation and layout of the unit cells the sequential progress of the crack can be controlled, making the fracture path arbitrarily tortuous. This tortuosity increases the energy dissipation of the metamaterial without changing its stiffness. Using bespoke arrangements of unit cells, metamaterials can have on-demand fracture paths of arbitrary complexity. Our findings bring a new perspective on inelastic deformations in mechanical metamaterials, with potential applications in areas as diverse as the food industry, structural design, and for shock and impact damping.","sentences":["In many scenarios -- when we bite food or during a crash -- fracture is inevitable.","Finding solutions to steer fracture to mitigate its impact or turn it into a purposeful functionality, is therefore crucial.","Strategies using composites, changes in chemical composition or crystal orientation, have proven to be very efficient, but the crack path control remains limited and has not been achieved in load-bearing structures.","Here, we introduce fracture metamaterials consisting of slender elements whose bending enables large elastic deformation as fracture propagates.","This interplay between bending and fracture enables tunable energy dissipation and the design of on-demand crack paths of arbitrary complexity.","To this end, we use topology optimisation to create unit cells with anisotropic fracture energy, which we then tile up to realize fracture metamaterials with uniform density that we 3D-print.","The thin ligaments that constitute the unit cells confer them a strikingly distinct response in tension and shear, and we show that by controlling the orientation and layout of the unit cells the sequential progress of the crack can be controlled, making the fracture path arbitrarily tortuous.","This tortuosity increases the energy dissipation of the metamaterial without changing its stiffness.","Using bespoke arrangements of unit cells, metamaterials can have on-demand fracture paths of arbitrary complexity.","Our findings bring a new perspective on inelastic deformations in mechanical metamaterials, with potential applications in areas as diverse as the food industry, structural design, and for shock and impact damping."],"url":"http://arxiv.org/abs/2405.19061v1","category":"cond-mat.soft"}
{"created":"2024-05-29 13:06:10","title":"New perspectives on the optimal placement of detectors for suicide bombers using metaheuristics","abstract":"We consider an operational model of suicide bombing attacks -- an increasingly prevalent form of terrorism -- against specific targets, and the use of protective countermeasures based on the deployment of detectors over the area under threat. These detectors have to be carefully located in order to minimize the expected number of casualties or the economic damage suffered, resulting in a hard optimization problem for which different metaheuristics have been proposed. Rather than assuming random decisions by the attacker, the problem is approached by considering different models of the latter, whereby he takes informed decisions on which objective must be targeted and through which path it has to be reached based on knowledge on the importance or value of the objectives or on the defensive strategy of the defender (a scenario that can be regarded as an adversarial game). We consider four different algorithms, namely a greedy heuristic, a hill climber, tabu search and an evolutionary algorithm, and study their performance on a broad collection of problem instances trying to resemble different realistic settings such as a coastal area, a modern urban area, and the historic core of an old town. It is shown that the adversarial scenario is harder for all techniques, and that the evolutionary algorithm seems to adapt better to the complexity of the resulting search landscape.","sentences":["We consider an operational model of suicide bombing attacks -- an increasingly prevalent form of terrorism -- against specific targets, and the use of protective countermeasures based on the deployment of detectors over the area under threat.","These detectors have to be carefully located in order to minimize the expected number of casualties or the economic damage suffered, resulting in a hard optimization problem for which different metaheuristics have been proposed.","Rather than assuming random decisions by the attacker, the problem is approached by considering different models of the latter, whereby he takes informed decisions on which objective must be targeted and through which path it has to be reached based on knowledge on the importance or value of the objectives or on the defensive strategy of the defender (a scenario that can be regarded as an adversarial game).","We consider four different algorithms, namely a greedy heuristic, a hill climber, tabu search and an evolutionary algorithm, and study their performance on a broad collection of problem instances trying to resemble different realistic settings such as a coastal area, a modern urban area, and the historic core of an old town.","It is shown that the adversarial scenario is harder for all techniques, and that the evolutionary algorithm seems to adapt better to the complexity of the resulting search landscape."],"url":"http://arxiv.org/abs/2405.19060v1","category":"cs.NE"}
{"created":"2024-05-29 12:44:41","title":"Statistical Context Detection for Deep Lifelong Reinforcement Learning","abstract":"Context detection involves labeling segments of an online stream of data as belonging to different tasks. Task labels are used in lifelong learning algorithms to perform consolidation or other procedures that prevent catastrophic forgetting. Inferring task labels from online experiences remains a challenging problem. Most approaches assume finite and low-dimension observation spaces or a preliminary training phase during which task labels are learned. Moreover, changes in the transition or reward functions can be detected only in combination with a policy, and therefore are more difficult to detect than changes in the input distribution. This paper presents an approach to learning both policies and labels in an online deep reinforcement learning setting. The key idea is to use distance metrics, obtained via optimal transport methods, i.e., Wasserstein distance, on suitable latent action-reward spaces to measure distances between sets of data points from past and current streams. Such distances can then be used for statistical tests based on an adapted Kolmogorov-Smirnov calculation to assign labels to sequences of experiences. A rollback procedure is introduced to learn multiple policies by ensuring that only the appropriate data is used to train the corresponding policy. The combination of task detection and policy deployment allows for the optimization of lifelong reinforcement learning agents without an oracle that provides task labels. The approach is tested using two benchmarks and the results show promising performance when compared with related context detection algorithms. The results suggest that optimal transport statistical methods provide an explainable and justifiable procedure for online context detection and reward optimization in lifelong reinforcement learning.","sentences":["Context detection involves labeling segments of an online stream of data as belonging to different tasks.","Task labels are used in lifelong learning algorithms to perform consolidation or other procedures that prevent catastrophic forgetting.","Inferring task labels from online experiences remains a challenging problem.","Most approaches assume finite and low-dimension observation spaces or a preliminary training phase during which task labels are learned.","Moreover, changes in the transition or reward functions can be detected only in combination with a policy, and therefore are more difficult to detect than changes in the input distribution.","This paper presents an approach to learning both policies and labels in an online deep reinforcement learning setting.","The key idea is to use distance metrics, obtained via optimal transport methods, i.e., Wasserstein distance, on suitable latent action-reward spaces to measure distances between sets of data points from past and current streams.","Such distances can then be used for statistical tests based on an adapted Kolmogorov-Smirnov calculation to assign labels to sequences of experiences.","A rollback procedure is introduced to learn multiple policies by ensuring that only the appropriate data is used to train the corresponding policy.","The combination of task detection and policy deployment allows for the optimization of lifelong reinforcement learning agents without an oracle that provides task labels.","The approach is tested using two benchmarks and the results show promising performance when compared with related context detection algorithms.","The results suggest that optimal transport statistical methods provide an explainable and justifiable procedure for online context detection and reward optimization in lifelong reinforcement learning."],"url":"http://arxiv.org/abs/2405.19047v1","category":"cs.LG"}
{"created":"2024-05-29 12:43:39","title":"Continual Collaborative Distillation for Recommender System","abstract":"Knowledge distillation (KD) has emerged as a promising technique for addressing the computational challenges associated with deploying large-scale recommender systems. KD transfers the knowledge of a massive teacher system to a compact student model, to reduce the huge computational burdens for inference while retaining high accuracy. The existing KD studies primarily focus on one-time distillation in static environments, leaving a substantial gap in their applicability to real-world scenarios dealing with continuously incoming users, items, and their interactions. In this work, we delve into a systematic approach to operating the teacher-student KD in a non-stationary data stream. Our goal is to enable efficient deployment through a compact student, which preserves the high performance of the massive teacher, while effectively adapting to continuously incoming data. We propose Continual Collaborative Distillation (CCD) framework, where both the teacher and the student continually and collaboratively evolve along the data stream. CCD facilitates the student in effectively adapting to new data, while also enabling the teacher to fully leverage accumulated knowledge. We validate the effectiveness of CCD through extensive quantitative, ablative, and exploratory experiments on two real-world datasets. We expect this research direction to contribute to narrowing the gap between existing KD studies and practical applications, thereby enhancing the applicability of KD in real-world systems.","sentences":["Knowledge distillation (KD) has emerged as a promising technique for addressing the computational challenges associated with deploying large-scale recommender systems.","KD transfers the knowledge of a massive teacher system to a compact student model, to reduce the huge computational burdens for inference while retaining high accuracy.","The existing KD studies primarily focus on one-time distillation in static environments, leaving a substantial gap in their applicability to real-world scenarios dealing with continuously incoming users, items, and their interactions.","In this work, we delve into a systematic approach to operating the teacher-student KD in a non-stationary data stream.","Our goal is to enable efficient deployment through a compact student, which preserves the high performance of the massive teacher, while effectively adapting to continuously incoming data.","We propose Continual Collaborative Distillation (CCD) framework, where both the teacher and the student continually and collaboratively evolve along the data stream.","CCD facilitates the student in effectively adapting to new data, while also enabling the teacher to fully leverage accumulated knowledge.","We validate the effectiveness of CCD through extensive quantitative, ablative, and exploratory experiments on two real-world datasets.","We expect this research direction to contribute to narrowing the gap between existing KD studies and practical applications, thereby enhancing the applicability of KD in real-world systems."],"url":"http://arxiv.org/abs/2405.19046v1","category":"cs.IR"}
{"created":"2024-05-29 12:38:01","title":"On adaptive stochastic extended iterative methods for solving least squares","abstract":"In this paper, we propose a novel adaptive stochastic extended iterative method, which can be viewed as an improved extension of the randomized extended Kaczmarz (REK) method, for finding the unique minimum Euclidean norm least-squares solution of a given linear system. In particular, we introduce three equivalent stochastic reformulations of the linear least-squares problem: stochastic unconstrained and constrained optimization problems, and the stochastic multiobjective optimization problem. We then alternately employ the adaptive variants of the stochastic heavy ball momentum (SHBM) method, which utilize iterative information to update the parameters, to solve the stochastic reformulations. We prove that our method converges linearly in expectation, addressing an open problem in the literature related to designing theoretically supported adaptive SHBM methods. Numerical experiments show that our adaptive stochastic extended iterative method has strong advantages over the non-adaptive one.","sentences":["In this paper, we propose a novel adaptive stochastic extended iterative method, which can be viewed as an improved extension of the randomized extended Kaczmarz (REK) method, for finding the unique minimum Euclidean norm least-squares solution of a given linear system.","In particular, we introduce three equivalent stochastic reformulations of the linear least-squares problem: stochastic unconstrained and constrained optimization problems, and the stochastic multiobjective optimization problem.","We then alternately employ the adaptive variants of the stochastic heavy ball momentum (SHBM) method, which utilize iterative information to update the parameters, to solve the stochastic reformulations.","We prove that our method converges linearly in expectation, addressing an open problem in the literature related to designing theoretically supported adaptive SHBM methods.","Numerical experiments show that our adaptive stochastic extended iterative method has strong advantages over the non-adaptive one."],"url":"http://arxiv.org/abs/2405.19044v1","category":"math.NA"}
{"created":"2024-05-29 12:32:08","title":"BLSP-KD: Bootstrapping Language-Speech Pre-training via Knowledge Distillation","abstract":"Recent end-to-end approaches have shown promise in extending large language models (LLMs) to speech inputs, but face limitations in directly assessing and optimizing alignment quality and fail to achieve fine-grained alignment due to speech-text length mismatch. We introduce BLSP-KD, a novel approach for Bootstrapping Language-Speech Pretraining via Knowledge Distillation, which addresses these limitations through two key techniques. First, it optimizes speech-text alignment by minimizing the divergence between the LLM's next-token prediction distributions for speech and text inputs using knowledge distillation. Second, it employs a continuous-integrate-andfire strategy to segment speech into tokens that correspond one-to-one with text tokens, enabling fine-grained alignment. We also introduce Partial LoRA (PLoRA), a new adaptation method supporting LLM finetuning for speech inputs under knowledge distillation. Quantitative evaluation shows that BLSP-KD outperforms previous end-to-end baselines and cascaded systems with comparable scale of parameters, facilitating general instruction-following capabilities for LLMs with speech inputs. This approach provides new possibilities for extending LLMs to spoken language interactions.","sentences":["Recent end-to-end approaches have shown promise in extending large language models (LLMs) to speech inputs, but face limitations in directly assessing and optimizing alignment quality and fail to achieve fine-grained alignment due to speech-text length mismatch.","We introduce BLSP-KD, a novel approach for Bootstrapping Language-Speech Pretraining via Knowledge Distillation, which addresses these limitations through two key techniques.","First, it optimizes speech-text alignment by minimizing the divergence between the LLM's next-token prediction distributions for speech and text inputs using knowledge distillation.","Second, it employs a continuous-integrate-andfire strategy to segment speech into tokens that correspond one-to-one with text tokens, enabling fine-grained alignment.","We also introduce Partial LoRA (PLoRA), a new adaptation method supporting LLM finetuning for speech inputs under knowledge distillation.","Quantitative evaluation shows that BLSP-KD outperforms previous end-to-end baselines and cascaded systems with comparable scale of parameters, facilitating general instruction-following capabilities for LLMs with speech inputs.","This approach provides new possibilities for extending LLMs to spoken language interactions."],"url":"http://arxiv.org/abs/2405.19041v1","category":"cs.CL"}
{"created":"2024-05-29 11:57:04","title":"Adaptive posterior concentration rates for sparse high-dimensional linear regression with random design and unknown error variance","abstract":"This paper investigates sparse high-dimensional linear regression, particularly examining the properties of the posterior under conditions of random design and unknown error variance. We provide consistency results for the posterior and analyze its concentration rates, demonstrating adaptiveness to the unknown sparsity level of the regression coefficient vector. Furthermore, we extend our investigation to establish concentration outcomes for parameter estimation using specific distance measures. These findings are in line with recent discoveries in frequentist studies. Additionally, by employing techniques to address model misspecification through a fractional posterior, we broaden our analysis through oracle inequalities to encompass the critical aspect of model misspecification for the regular posterior. Our novel findings are demonstrated using two different types of sparsity priors: a shrinkage prior and a spike-and-slab prior.","sentences":["This paper investigates sparse high-dimensional linear regression, particularly examining the properties of the posterior under conditions of random design and unknown error variance.","We provide consistency results for the posterior and analyze its concentration rates, demonstrating adaptiveness to the unknown sparsity level of the regression coefficient vector.","Furthermore, we extend our investigation to establish concentration outcomes for parameter estimation using specific distance measures.","These findings are in line with recent discoveries in frequentist studies.","Additionally, by employing techniques to address model misspecification through a fractional posterior, we broaden our analysis through oracle inequalities to encompass the critical aspect of model misspecification for the regular posterior.","Our novel findings are demonstrated using two different types of sparsity priors: a shrinkage prior and a spike-and-slab prior."],"url":"http://arxiv.org/abs/2405.19016v1","category":"math.ST"}
{"created":"2024-05-29 11:53:07","title":"Trust the Model Where It Trusts Itself -- Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption","abstract":"Dyna-style model-based reinforcement learning (MBRL) combines model-free agents with predictive transition models through model-based rollouts. This combination raises a critical question: 'When to trust your model?'; i.e., which rollout length results in the model providing useful data? Janner et al. (2019) address this question by gradually increasing rollout lengths throughout the training. While theoretically tempting, uniform model accuracy is a fallacy that collapses at the latest when extrapolating. Instead, we propose asking the question 'Where to trust your model?'. Using inherent model uncertainty to consider local accuracy, we obtain the Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption (MACURA) algorithm. We propose an easy-to-tune rollout mechanism and demonstrate substantial improvements in data efficiency and performance compared to state-of-the-art deep MBRL methods on the MuJoCo benchmark.","sentences":["Dyna-style model-based reinforcement learning (MBRL) combines model-free agents with predictive transition models through model-based rollouts.","This combination raises a critical question: 'When to trust your model?'; i.e., which rollout length results in the model providing useful data?","Janner et al. (2019) address this question by gradually increasing rollout lengths throughout the training.","While theoretically tempting, uniform model accuracy is a fallacy that collapses at the latest when extrapolating.","Instead, we propose asking the question 'Where to trust your model?'.","Using inherent model uncertainty to consider local accuracy, we obtain the Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption (MACURA) algorithm.","We propose an easy-to-tune rollout mechanism and demonstrate substantial improvements in data efficiency and performance compared to state-of-the-art deep MBRL methods on the MuJoCo benchmark."],"url":"http://arxiv.org/abs/2405.19014v1","category":"cs.LG"}
{"created":"2024-05-29 11:52:53","title":"On Dissipativity of Cross-Entropy Loss in Training ResNets","abstract":"The training of ResNets and neural ODEs can be formulated and analyzed from the perspective of optimal control. This paper proposes a dissipative formulation of the training of ResNets and neural ODEs for classification problems by including a variant of the cross-entropy as a regularization in the stage cost. Based on the dissipative formulation of the training, we prove that the trained ResNet exhibit the turnpike phenomenon. We then illustrate that the training exhibits the turnpike phenomenon by training on the two spirals and MNIST datasets. This can be used to find very shallow networks suitable for a given classification task.","sentences":["The training of ResNets and neural ODEs can be formulated and analyzed from the perspective of optimal control.","This paper proposes a dissipative formulation of the training of ResNets and neural ODEs for classification problems by including a variant of the cross-entropy as a regularization in the stage cost.","Based on the dissipative formulation of the training, we prove that the trained ResNet exhibit the turnpike phenomenon.","We then illustrate that the training exhibits the turnpike phenomenon by training on the two spirals and MNIST datasets.","This can be used to find very shallow networks suitable for a given classification task."],"url":"http://arxiv.org/abs/2405.19013v1","category":"cs.LG"}
{"created":"2024-05-29 11:51:33","title":"Implicit Neural Image Field for Biological Microscopy Image Compression","abstract":"The rapid pace of innovation in biological microscopy imaging has led to large images, putting pressure on data storage and impeding efficient sharing, management, and visualization. This necessitates the development of efficient compression solutions. Traditional CODEC methods struggle to adapt to the diverse bioimaging data and often suffer from sub-optimal compression. In this study, we propose an adaptive compression workflow based on Implicit Neural Representation (INR). This approach permits application-specific compression objectives, capable of compressing images of any shape and arbitrary pixel-wise decompression. We demonstrated on a wide range of microscopy images from real applications that our workflow not only achieved high, controllable compression ratios (e.g., 512x) but also preserved detailed information critical for downstream analysis.","sentences":["The rapid pace of innovation in biological microscopy imaging has led to large images, putting pressure on data storage and impeding efficient sharing, management, and visualization.","This necessitates the development of efficient compression solutions.","Traditional CODEC methods struggle to adapt to the diverse bioimaging data and often suffer from sub-optimal compression.","In this study, we propose an adaptive compression workflow based on Implicit Neural Representation (INR).","This approach permits application-specific compression objectives, capable of compressing images of any shape and arbitrary pixel-wise decompression.","We demonstrated on a wide range of microscopy images from real applications that our workflow not only achieved high, controllable compression ratios (e.g., 512x) but also preserved detailed information critical for downstream analysis."],"url":"http://arxiv.org/abs/2405.19012v1","category":"cs.AI"}
{"created":"2024-05-29 11:42:02","title":"Auto-selected Knowledge Adapters for Lifelong Person Re-identification","abstract":"Lifelong Person Re-Identification (LReID) extends traditional ReID by requiring systems to continually learn from non-overlapping datasets across different times and locations, adapting to new identities while preserving knowledge of previous ones. Existing approaches, either rehearsal-free or rehearsal-based, still suffer from the problem of catastrophic forgetting since they try to cram diverse knowledge into one fixed model. To overcome this limitation, we introduce a novel framework AdalReID, that adopts knowledge adapters and a parameter-free auto-selection mechanism for lifelong learning. Concretely, we incrementally build distinct adapters to learn domain-specific knowledge at each step, which can effectively learn and preserve knowledge across different datasets. Meanwhile, the proposed auto-selection strategy adaptively calculates the knowledge similarity between the input set and the adapters. On the one hand, the appropriate adapters are selected for the inputs to process ReID, and on the other hand, the knowledge interaction and fusion between adapters are enhanced to improve the generalization ability of the model. Extensive experiments are conducted to demonstrate the superiority of our AdalReID, which significantly outperforms SOTAs by about 10$\\sim$20\\% mAP on both seen and unseen domains.","sentences":["Lifelong Person Re-Identification (LReID) extends traditional ReID by requiring systems to continually learn from non-overlapping datasets across different times and locations, adapting to new identities while preserving knowledge of previous ones.","Existing approaches, either rehearsal-free or rehearsal-based, still suffer from the problem of catastrophic forgetting since they try to cram diverse knowledge into one fixed model.","To overcome this limitation, we introduce a novel framework AdalReID, that adopts knowledge adapters and a parameter-free auto-selection mechanism for lifelong learning.","Concretely, we incrementally build distinct adapters to learn domain-specific knowledge at each step, which can effectively learn and preserve knowledge across different datasets.","Meanwhile, the proposed auto-selection strategy adaptively calculates the knowledge similarity between the input set and the adapters.","On the one hand, the appropriate adapters are selected for the inputs to process ReID, and on the other hand, the knowledge interaction and fusion between adapters are enhanced to improve the generalization ability of the model.","Extensive experiments are conducted to demonstrate the superiority of our AdalReID, which significantly outperforms SOTAs by about 10$\\sim$20\\% mAP on both seen and unseen domains."],"url":"http://arxiv.org/abs/2405.19005v2","category":"cs.CV"}
{"created":"2024-05-29 11:28:06","title":"FedMAP: Unlocking Potential in Personalized Federated Learning through Bi-Level MAP Optimization","abstract":"Federated Learning (FL) enables collaborative training of machine learning models on decentralized data while preserving data privacy. However, data across clients often differs significantly due to class imbalance, feature distribution skew, sample size imbalance, and other phenomena. Leveraging information from these not identically distributed (non-IID) datasets poses substantial challenges. FL methods based on a single global model cannot effectively capture the variations in client data and underperform in non-IID settings. Consequently, Personalized FL (PFL) approaches that adapt to each client's data distribution but leverage other clients' data are essential but currently underexplored. We propose a novel Bayesian PFL framework using bi-level optimization to tackle the data heterogeneity challenges. Our proposed framework utilizes the global model as a prior distribution within a Maximum A Posteriori (MAP) estimation of personalized client models. This approach facilitates PFL by integrating shared knowledge from the prior, thereby enhancing local model performance, generalization ability, and communication efficiency. We extensively evaluated our bi-level optimization approach on real-world and synthetic datasets, demonstrating significant improvements in model accuracy compared to existing methods while reducing communication overhead. This study contributes to PFL by establishing a solid theoretical foundation for the proposed method and offering a robust, ready-to-use framework that effectively addresses the challenges posed by non-IID data in FL.","sentences":["Federated Learning (FL) enables collaborative training of machine learning models on decentralized data while preserving data privacy.","However, data across clients often differs significantly due to class imbalance, feature distribution skew, sample size imbalance, and other phenomena.","Leveraging information from these not identically distributed (non-IID) datasets poses substantial challenges.","FL methods based on a single global model cannot effectively capture the variations in client data and underperform in non-IID settings.","Consequently, Personalized FL (PFL) approaches that adapt to each client's data distribution but leverage other clients' data are essential but currently underexplored.","We propose a novel Bayesian PFL framework using bi-level optimization to tackle the data heterogeneity challenges.","Our proposed framework utilizes the global model as a prior distribution within a Maximum A Posteriori (MAP) estimation of personalized client models.","This approach facilitates PFL by integrating shared knowledge from the prior, thereby enhancing local model performance, generalization ability, and communication efficiency.","We extensively evaluated our bi-level optimization approach on real-world and synthetic datasets, demonstrating significant improvements in model accuracy compared to existing methods while reducing communication overhead.","This study contributes to PFL by establishing a solid theoretical foundation for the proposed method and offering a robust, ready-to-use framework that effectively addresses the challenges posed by non-IID data in FL."],"url":"http://arxiv.org/abs/2405.19000v1","category":"cs.LG"}
{"created":"2024-05-29 11:11:07","title":"EasyAnimate: A High-Performance Long Video Generation Method based on Transformer Architecture","abstract":"This paper presents EasyAnimate, an advanced method for video generation that leverages the power of transformer architecture for high-performance outcomes. We have expanded the DiT framework originally designed for 2D image synthesis to accommodate the complexities of 3D video generation by incorporating a motion module block. It is used to capture temporal dynamics, thereby ensuring the production of consistent frames and seamless motion transitions. The motion module can be adapted to various DiT baseline methods to generate video with different styles. It can also generate videos with different frame rates and resolutions during both training and inference phases, suitable for both images and videos. Moreover, we introduce slice VAE, a novel approach to condense the temporal axis, facilitating the generation of long duration videos. Currently, EasyAnimate exhibits the proficiency to generate videos with 144 frames. We provide a holistic ecosystem for video production based on DiT, encompassing aspects such as data pre-processing, VAE training, DiT models training (both the baseline model and LoRA model), and end-to-end video inference. Code is available at: https://github.com/aigc-apps/EasyAnimate. We are continuously working to enhance the performance of our method.","sentences":["This paper presents EasyAnimate, an advanced method for video generation that leverages the power of transformer architecture for high-performance outcomes.","We have expanded the DiT framework originally designed for 2D image synthesis to accommodate the complexities of 3D video generation by incorporating a motion module block.","It is used to capture temporal dynamics, thereby ensuring the production of consistent frames and seamless motion transitions.","The motion module can be adapted to various DiT baseline methods to generate video with different styles.","It can also generate videos with different frame rates and resolutions during both training and inference phases, suitable for both images and videos.","Moreover, we introduce slice VAE, a novel approach to condense the temporal axis, facilitating the generation of long duration videos.","Currently, EasyAnimate exhibits the proficiency to generate videos with 144 frames.","We provide a holistic ecosystem for video production based on DiT, encompassing aspects such as data pre-processing, VAE training, DiT models training (both the baseline model and LoRA model), and end-to-end video inference.","Code is available at: https://github.com/aigc-apps/EasyAnimate.","We are continuously working to enhance the performance of our method."],"url":"http://arxiv.org/abs/2405.18991v1","category":"cs.CV"}
{"created":"2024-05-29 10:34:44","title":"Federated Learning with Bilateral Curation for Partially Class-Disjoint Data","abstract":"Partially class-disjoint data (PCDD), a common yet under-explored data formation where each client contributes a part of classes (instead of all classes) of samples, severely challenges the performance of federated algorithms. Without full classes, the local objective will contradict the global objective, yielding the angle collapse problem for locally missing classes and the space waste problem for locally existing classes. As far as we know, none of the existing methods can intrinsically mitigate PCDD challenges to achieve holistic improvement in the bilateral views (both global view and local view) of federated learning. To address this dilemma, we are inspired by the strong generalization of simplex Equiangular Tight Frame~(ETF) on the imbalanced data, and propose a novel approach called FedGELA where the classifier is globally fixed as a simplex ETF while locally adapted to the personal distributions. Globally, FedGELA provides fair and equal discrimination for all classes and avoids inaccurate updates of the classifier, while locally it utilizes the space of locally missing classes for locally existing classes. We conduct extensive experiments on a range of datasets to demonstrate that our FedGELA achieves promising performance~(averaged improvement of 3.9% to FedAvg and 1.5% to best baselines) and provide both local and global convergence guarantees. Source code is available at:https://github.com/MediaBrain-SJTU/FedGELA.git.","sentences":["Partially class-disjoint data (PCDD), a common yet under-explored data formation where each client contributes a part of classes (instead of all classes) of samples, severely challenges the performance of federated algorithms.","Without full classes, the local objective will contradict the global objective, yielding the angle collapse problem for locally missing classes and the space waste problem for locally existing classes.","As far as we know, none of the existing methods can intrinsically mitigate PCDD challenges to achieve holistic improvement in the bilateral views (both global view and local view) of federated learning.","To address this dilemma, we are inspired by the strong generalization of simplex Equiangular Tight Frame~(ETF) on the imbalanced data, and propose a novel approach called FedGELA where the classifier is globally fixed as a simplex ETF while locally adapted to the personal distributions.","Globally, FedGELA provides fair and equal discrimination for all classes and avoids inaccurate updates of the classifier, while locally it utilizes the space of locally missing classes for locally existing classes.","We conduct extensive experiments on a range of datasets to demonstrate that our FedGELA achieves promising performance~(averaged improvement of 3.9% to FedAvg and 1.5% to best baselines) and provide both local and global convergence guarantees.","Source code is available at:https://github.com/MediaBrain-SJTU/FedGELA.git."],"url":"http://arxiv.org/abs/2405.18972v1","category":"cs.LG"}
{"created":"2024-05-29 10:31:53","title":"Mitigate Position Bias with Coupled Ranking Bias on CTR Prediction","abstract":"Position bias, i.e., users' preference of an item is affected by its placing position, is well studied in the recommender system literature. However, most existing methods ignore the widely coupled ranking bias, which is also related to the placing position of the item. Using both synthetic and industrial datasets, we first show how this widely coexisted ranking bias deteriorates the performance of the existing position bias estimation methods. To mitigate the position bias with the presence of the ranking bias, we propose a novel position bias estimation method, namely gradient interpolation, which fuses two estimation methods using a fusing weight. We further propose an adaptive method to automatically determine the optimal fusing weight. Extensive experiments on both synthetic and industrial datasets demonstrate the superior performance of the proposed methods.","sentences":["Position bias, i.e., users' preference of an item is affected by its placing position, is well studied in the recommender system literature.","However, most existing methods ignore the widely coupled ranking bias, which is also related to the placing position of the item.","Using both synthetic and industrial datasets, we first show how this widely coexisted ranking bias deteriorates the performance of the existing position bias estimation methods.","To mitigate the position bias with the presence of the ranking bias, we propose a novel position bias estimation method, namely gradient interpolation, which fuses two estimation methods using a fusing weight.","We further propose an adaptive method to automatically determine the optimal fusing weight.","Extensive experiments on both synthetic and industrial datasets demonstrate the superior performance of the proposed methods."],"url":"http://arxiv.org/abs/2405.18971v1","category":"cs.IR"}
{"created":"2024-05-29 09:56:00","title":"Predicting Many Properties of Crystals by a Single Deep Learning Model","abstract":"The use of machine learning methods for predicting the properties of crystalline materials encounters significant challenges, primarily related to input encoding, output versatility, and interpretability. Here, we introduce CrystalBERT, an adaptable transformer-based framework with novel structure that integrates space group, elemental, and unit cell information. The method's adaptability lies not only in its ability to seamlessly combine diverse features but also in its capability to accurately predict a wide range of physically important properties, including topological properties, superconducting transition temperatures, dielectric constants, and more. CrystalBERT also provides insightful physical interpretations regarding the features that most significantly influence the target properties. Our findings indicate that space group and elemental information are more important for predicting topological and superconducting properties, in contrast to some properties that primarily depend on the unit cell information. This underscores the intricate nature of topological and superconducting properties. By incorporating all these features, we achieve a high accuracy of 91% in topological classification, surpassing prior studies and identifying previously misclassified topological materials, further demonstrating the effectiveness of our model.","sentences":["The use of machine learning methods for predicting the properties of crystalline materials encounters significant challenges, primarily related to input encoding, output versatility, and interpretability.","Here, we introduce CrystalBERT, an adaptable transformer-based framework with novel structure that integrates space group, elemental, and unit cell information.","The method's adaptability lies not only in its ability to seamlessly combine diverse features but also in its capability to accurately predict a wide range of physically important properties, including topological properties, superconducting transition temperatures, dielectric constants, and more.","CrystalBERT also provides insightful physical interpretations regarding the features that most significantly influence the target properties.","Our findings indicate that space group and elemental information are more important for predicting topological and superconducting properties, in contrast to some properties that primarily depend on the unit cell information.","This underscores the intricate nature of topological and superconducting properties.","By incorporating all these features, we achieve a high accuracy of 91% in topological classification, surpassing prior studies and identifying previously misclassified topological materials, further demonstrating the effectiveness of our model."],"url":"http://arxiv.org/abs/2405.18944v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-29 09:13:30","title":"Exploring Human-in-the-Loop Test-Time Adaptation by Synergizing Active Learning and Model Selection","abstract":"Existing test-time adaptation (TTA) approaches often adapt models with the unlabeled testing data stream. A recent attempt relaxed the assumption by introducing limited human annotation, referred to as Human-In-the-Loop Test-Time Adaptation (HILTTA) in this study. The focus of existing HILTTA lies on selecting the most informative samples to label, a.k.a. active learning. In this work, we are motivated by a pitfall of TTA, i.e. sensitive to hyper-parameters, and propose to approach HILTTA by synergizing active learning and model selection. Specifically, we first select samples for human annotation (active learning) and then use the labeled data to select optimal hyper-parameters (model selection). A sample selection strategy is tailored for choosing samples by considering the balance between active learning and model selection purposes. We demonstrate on 4 TTA datasets that the proposed HILTTA approach is compatible with off-the-shelf TTA methods which outperform the state-of-the-art HILTTA methods and stream-based active learning methods. Importantly, our proposed method can always prevent choosing the worst hyper-parameters on all off-the-shelf TTA methods. The source code will be released upon publication.","sentences":["Existing test-time adaptation (TTA) approaches often adapt models with the unlabeled testing data stream.","A recent attempt relaxed the assumption by introducing limited human annotation, referred to as Human-In-the-Loop Test-Time Adaptation (HILTTA) in this study.","The focus of existing HILTTA lies on selecting the most informative samples to label, a.k.a. active learning.","In this work, we are motivated by a pitfall of TTA, i.e. sensitive to hyper-parameters, and propose to approach HILTTA by synergizing active learning and model selection.","Specifically, we first select samples for human annotation (active learning) and then use the labeled data to select optimal hyper-parameters (model selection).","A sample selection strategy is tailored for choosing samples by considering the balance between active learning and model selection purposes.","We demonstrate on 4 TTA datasets that the proposed HILTTA approach is compatible with off-the-shelf TTA methods which outperform the state-of-the-art HILTTA methods and stream-based active learning methods.","Importantly, our proposed method can always prevent choosing the worst hyper-parameters on all off-the-shelf TTA methods.","The source code will be released upon publication."],"url":"http://arxiv.org/abs/2405.18911v1","category":"cs.CV"}
{"created":"2024-05-29 09:09:00","title":"Language Generation with Strictly Proper Scoring Rules","abstract":"Language generation based on maximum likelihood estimation (MLE) has become the fundamental approach for text generation. Maximum likelihood estimation is typically performed by minimizing the log-likelihood loss, also known as the logarithmic score in statistical decision theory. The logarithmic score is strictly proper in the sense that it encourages honest forecasts, where the expected score is maximized only when the model reports true probabilities. Although many strictly proper scoring rules exist, the logarithmic score is the only local scoring rule among them that depends exclusively on the probability of the observed sample, making it capable of handling the exponentially large sample space of natural text. In this work, we propose a straightforward strategy for adapting scoring rules to language generation, allowing for language modeling with any non-local scoring rules. Leveraging this strategy, we train language generation models using two classic strictly proper scoring rules, the Brier score and the Spherical score, as alternatives to the logarithmic score. Experimental results indicate that simply substituting the loss function, without adjusting other hyperparameters, can yield substantial improvements in model's generation capabilities. Moreover, these improvements can scale up to large language models (LLMs) such as LLaMA-7B and LLaMA-13B. Source code: \\url{https://github.com/shaochenze/ScoringRulesLM}.","sentences":["Language generation based on maximum likelihood estimation (MLE) has become the fundamental approach for text generation.","Maximum likelihood estimation is typically performed by minimizing the log-likelihood loss, also known as the logarithmic score in statistical decision theory.","The logarithmic score is strictly proper in the sense that it encourages honest forecasts, where the expected score is maximized only when the model reports true probabilities.","Although many strictly proper scoring rules exist, the logarithmic score is the only local scoring rule among them that depends exclusively on the probability of the observed sample, making it capable of handling the exponentially large sample space of natural text.","In this work, we propose a straightforward strategy for adapting scoring rules to language generation, allowing for language modeling with any non-local scoring rules.","Leveraging this strategy, we train language generation models using two classic strictly proper scoring rules, the Brier score and the Spherical score, as alternatives to the logarithmic score.","Experimental results indicate that simply substituting the loss function, without adjusting other hyperparameters, can yield substantial improvements in model's generation capabilities.","Moreover, these improvements can scale up to large language models (LLMs) such as LLaMA-7B and LLaMA-13B. Source code: \\url{https://github.com/shaochenze/ScoringRulesLM}."],"url":"http://arxiv.org/abs/2405.18906v1","category":"cs.CL"}
{"created":"2024-05-29 08:57:23","title":"MLAE: Masked LoRA Experts for Parameter-Efficient Fine-Tuning","abstract":"In response to the challenges posed by the extensive parameter updates required for full fine-tuning of large-scale pre-trained models, parameter-efficient fine-tuning (PEFT) methods, exemplified by Low-Rank Adaptation (LoRA), have emerged. LoRA simplifies the fine-tuning process but may still struggle with a certain level of redundancy in low-rank matrices and limited effectiveness from merely increasing their rank. To address these issues, a natural idea is to enhance the independence and diversity of the learning process for the low-rank matrices. Therefore, we propose Masked LoRA Experts (MLAE), an innovative approach that applies the concept of masking to PEFT. Our method incorporates a cellular decomposition strategy that transforms a low-rank matrix into independent rank-1 submatrices, or ``experts'', thus enhancing independence. Additionally, we introduce a binary mask matrix that selectively activates these experts during training to promote more diverse and anisotropic learning, based on expert-level dropout strategies. Our investigations reveal that this selective activation not only enhances performance but also fosters a more diverse acquisition of knowledge with a marked decrease in parameter similarity among MLAE, significantly boosting the quality of the model while barely increasing the parameter count. Remarkably, MLAE achieves new SOTA performance with an average accuracy score of 78.8% on the VTAB-1k benchmark and 90.9% on the FGVC benchmark, demonstrating superior performance. Our code is available at https://github.com/jie040109/MLAE.","sentences":["In response to the challenges posed by the extensive parameter updates required for full fine-tuning of large-scale pre-trained models, parameter-efficient fine-tuning (PEFT) methods, exemplified by Low-Rank Adaptation (LoRA), have emerged.","LoRA simplifies the fine-tuning process but may still struggle with a certain level of redundancy in low-rank matrices and limited effectiveness from merely increasing their rank.","To address these issues, a natural idea is to enhance the independence and diversity of the learning process for the low-rank matrices.","Therefore, we propose Masked LoRA Experts (MLAE), an innovative approach that applies the concept of masking to PEFT.","Our method incorporates a cellular decomposition strategy that transforms a low-rank matrix into independent rank-1 submatrices, or ``experts'', thus enhancing independence.","Additionally, we introduce a binary mask matrix that selectively activates these experts during training to promote more diverse and anisotropic learning, based on expert-level dropout strategies.","Our investigations reveal that this selective activation not only enhances performance but also fosters a more diverse acquisition of knowledge with a marked decrease in parameter similarity among MLAE, significantly boosting the quality of the model while barely increasing the parameter count.","Remarkably, MLAE achieves new SOTA performance with an average accuracy score of 78.8% on the VTAB-1k benchmark and 90.9% on the FGVC benchmark, demonstrating superior performance.","Our code is available at https://github.com/jie040109/MLAE."],"url":"http://arxiv.org/abs/2405.18897v1","category":"cs.CV"}
{"created":"2024-05-29 08:42:30","title":"Compressing Large Language Models using Low Rank and Low Precision Decomposition","abstract":"The prohibitive sizes of Large Language Models (LLMs) today make it difficult to deploy them on memory-constrained edge devices. This work introduces $\\rm CALDERA$ -- a new post-training LLM compression algorithm that harnesses the inherent low-rank structure of a weight matrix $\\mathbf{W}$ by approximating it via a low-rank, low-precision decomposition as $\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$. Here, $\\mathbf{L}$ and $\\mathbf{R}$ are low rank factors, and the entries of $\\mathbf{Q}$, $\\mathbf{L}$ and $\\mathbf{R}$ are quantized. The model is compressed by substituting each layer with its $\\mathbf{Q} + \\mathbf{L}\\mathbf{R}$ decomposition, and the zero-shot performance of the compressed model is evaluated. Additionally, $\\mathbf{L}$ and $\\mathbf{R}$ are readily amenable to low-rank adaptation, consequently enhancing the zero-shot performance. $\\rm CALDERA$ obtains this decomposition by formulating it as an optimization problem $\\min_{\\mathbf{Q},\\mathbf{L},\\mathbf{R}}\\lVert(\\mathbf{Q} + \\mathbf{L}\\mathbf{R} - \\mathbf{W})\\mathbf{X}^\\top\\rVert_{\\rm F}^2$, where $\\mathbf{X}$ is the calibration data, and $\\mathbf{Q}, \\mathbf{L}, \\mathbf{R}$ are constrained to be representable using low-precision formats. Theoretical upper bounds on the approximation error of $\\rm CALDERA$ are established using a rank-constrained regression framework, and the tradeoff between compression ratio and model performance is studied by analyzing the impact of target rank and quantization bit budget. Results illustrate that compressing LlaMa-$2$ $7$B/$70$B and LlaMa-$3$ $8$B models obtained using $\\rm CALDERA$ outperforms existing post-training LLM compression techniques in the regime of less than $2.5$ bits per parameter. The implementation is available at: \\href{https://github.com/pilancilab/caldera}{https://github.com/pilancilab/caldera}.","sentences":["The prohibitive sizes of Large Language Models (LLMs) today make it difficult to deploy them on memory-constrained edge devices.","This work introduces $\\rm CALDERA$ -- a new post-training LLM compression algorithm that harnesses the inherent low-rank structure of a weight matrix $\\mathbf{W}$ by approximating it via a low-rank, low-precision decomposition as $\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$. Here, $\\mathbf{L}$ and $\\mathbf{R}$ are low rank factors, and the entries of $\\mathbf{Q}$, $\\mathbf{L}$ and $\\mathbf{R}$ are quantized.","The model is compressed by substituting each layer with its $\\mathbf{Q} + \\mathbf{L}\\mathbf{R}$ decomposition, and the zero-shot performance of the compressed model is evaluated.","Additionally, $\\mathbf{L}$ and $\\mathbf{R}$ are readily amenable to low-rank adaptation, consequently enhancing the zero-shot performance.","$\\rm CALDERA$ obtains this decomposition by formulating it as an optimization problem $\\min_{\\mathbf{Q},\\mathbf{L},\\mathbf{R}}\\lVert(\\mathbf{Q} + \\mathbf{L}\\mathbf{R} - \\mathbf{W})\\mathbf{X}^\\top\\rVert_{\\rm F}^2$, where $\\mathbf{X}$ is the calibration data, and $\\mathbf{Q}, \\mathbf{L}, \\mathbf{R}$ are constrained to be representable using low-precision formats.","Theoretical upper bounds on the approximation error of $\\rm CALDERA$ are established using a rank-constrained regression framework, and the tradeoff between compression ratio and model performance is studied by analyzing the impact of target rank and quantization bit budget.","Results illustrate that compressing LlaMa-$2$ $7$B/$70$B and LlaMa-$3$ $8$B models obtained using $\\rm CALDERA$ outperforms existing post-training LLM compression techniques in the regime of less than $2.5$ bits per parameter.","The implementation is available at: \\href{https://github.com/pilancilab/caldera}{https://github.com/pilancilab/caldera}."],"url":"http://arxiv.org/abs/2405.18886v1","category":"cs.LG"}
{"created":"2024-05-29 08:39:31","title":"EventZoom: A Progressive Approach to Event-Based Data Augmentation for Enhanced Neuromorphic Vision","abstract":"Event data captured by Dynamic Vision Sensors (DVS) offers a unique approach to visual processing that differs from traditional video capture, showcasing its efficiency in dynamic and real-time scenarios. Despite advantages such as high temporal resolution and low energy consumption, the application of event data faces challenges due to limited dataset size and diversity. To address this, we developed EventZoom -- a data augmentation strategy specifically designed for event data. EventZoom employs a progressive temporal strategy that intelligently blends time and space to enhance the diversity and complexity of the data while maintaining its authenticity. This method aims to improve the quality of data for model training and enhance the adaptability and robustness of algorithms in handling complex dynamic scenes. We have experimentally validated EventZoom across various supervised learning frameworks, including supervised, semi-supervised, and unsupervised learning. Our results demonstrate that EventZoom consistently outperforms other data augmentation methods, confirming its effectiveness and applicability as a powerful event-based data augmentation tool in diverse learning settings.","sentences":["Event data captured by Dynamic Vision Sensors (DVS) offers a unique approach to visual processing that differs from traditional video capture, showcasing its efficiency in dynamic and real-time scenarios.","Despite advantages such as high temporal resolution and low energy consumption, the application of event data faces challenges due to limited dataset size and diversity.","To address this, we developed EventZoom -- a data augmentation strategy specifically designed for event data.","EventZoom employs a progressive temporal strategy that intelligently blends time and space to enhance the diversity and complexity of the data while maintaining its authenticity.","This method aims to improve the quality of data for model training and enhance the adaptability and robustness of algorithms in handling complex dynamic scenes.","We have experimentally validated EventZoom across various supervised learning frameworks, including supervised, semi-supervised, and unsupervised learning.","Our results demonstrate that EventZoom consistently outperforms other data augmentation methods, confirming its effectiveness and applicability as a powerful event-based data augmentation tool in diverse learning settings."],"url":"http://arxiv.org/abs/2405.18880v1","category":"cs.CV"}
{"created":"2024-05-29 17:58:09","title":"NPGA: Neural Parametric Gaussian Avatars","abstract":"The creation of high-fidelity, digital versions of human heads is an important stepping stone in the process of further integrating virtual components into our everyday lives. Constructing such avatars is a challenging research problem, due to a high demand for photo-realism and real-time rendering performance. In this work, we propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings. We build our method around 3D Gaussian Splatting for its highly efficient rendering and to inherit the topological flexibility of point clouds. In contrast to previous work, we condition our avatars' dynamics on the rich expression space of neural parametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we distill the backward deformation field of our underlying NPHM into forward deformations which are compatible with rasterization-based rendering. All remaining fine-scale, expression-dependent details are learned from the multi-view videos. To increase the representational capacity of our avatars, we augment the canonical Gaussian point cloud using per-primitive latent features which govern its dynamic behavior. To regularize this increased dynamic expressivity, we propose Laplacian terms on the latent features and predicted dynamics. We evaluate our method on the public NeRSemble dataset, demonstrating that NPGA significantly outperforms the previous state-of-the-art avatars on the self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate animation capabilities from real-world monocular videos.","sentences":["The creation of high-fidelity, digital versions of human heads is an important stepping stone in the process of further integrating virtual components into our everyday lives.","Constructing such avatars is a challenging research problem, due to a high demand for photo-realism and real-time rendering performance.","In this work, we propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings.","We build our method around 3D Gaussian Splatting for its highly efficient rendering and to inherit the topological flexibility of point clouds.","In contrast to previous work, we condition our avatars' dynamics on the rich expression space of neural parametric head models (NPHM), instead of mesh-based 3DMMs.","To this end, we distill the backward deformation field of our underlying NPHM into forward deformations which are compatible with rasterization-based rendering.","All remaining fine-scale, expression-dependent details are learned from the multi-view videos.","To increase the representational capacity of our avatars, we augment the canonical Gaussian point cloud using per-primitive latent features which govern its dynamic behavior.","To regularize this increased dynamic expressivity, we propose Laplacian terms on the latent features and predicted dynamics.","We evaluate our method on the public NeRSemble dataset, demonstrating that NPGA significantly outperforms the previous state-of-the-art avatars on the self-reenactment task by 2.6 PSNR.","Furthermore, we demonstrate accurate animation capabilities from real-world monocular videos."],"url":"http://arxiv.org/abs/2405.19331v1","category":"cs.CV"}
{"created":"2024-05-29 17:44:22","title":"Cosmology Based on Finsler and Finsler-like Metric Structure of Gravitational Field","abstract":"In this article, we review some aspects of gravitational field and cosmology based on Finsler and Finsler-like generalized metric structures. The geometrical framework of these spaces allows further investigation of locally-anisotropic phenomena related to the gravitational field and cosmological considerations, e.g the extracted geodesics, deflection of light, Finsler-Einstein gravitational field equations , the Friedmann equations and the Raychaudhuri equations include extra anisotropic terms that in the Riemannian framework of General Relativity (GR) are not interpreted. This approach gives us the opportunity to extend the research with more degrees of freedom on the tangent bundle of a spacetime manifold. In the above mentioned generalizations omitting the extra anisotropic terms we recover the framework of GR. In addition, we study the gravitational Magnus effect in a generalized metric framework.   Based on this approach, we describe further properties of Finsler-Randers (FR) and Schwarzschild Finsler Randers (SFR) cosmological models which are useful for the description and evolution of the universe.","sentences":["In this article, we review some aspects of gravitational field and cosmology based on Finsler and Finsler-like generalized metric structures.","The geometrical framework of these spaces allows further investigation of locally-anisotropic phenomena related to the gravitational field and cosmological considerations, e.g the extracted geodesics, deflection of light, Finsler-Einstein gravitational field equations , the Friedmann equations and the Raychaudhuri equations include extra anisotropic terms that in the Riemannian framework of General Relativity (GR) are not interpreted.","This approach gives us the opportunity to extend the research with more degrees of freedom on the tangent bundle of a spacetime manifold.","In the above mentioned generalizations omitting the extra anisotropic terms we recover the framework of GR.","In addition, we study the gravitational Magnus effect in a generalized metric framework.   ","Based on this approach, we describe further properties of Finsler-Randers (FR) and Schwarzschild Finsler Randers (SFR) cosmological models which are useful for the description and evolution of the universe."],"url":"http://arxiv.org/abs/2405.19318v1","category":"gr-qc"}
{"created":"2024-05-29 17:33:34","title":"SDPRLayers: Certifiable Backpropagation Through Polynomial Optimization Problems in Robotics","abstract":"Differentiable optimization is a powerful new paradigm capable of reconciling model-based and learning-based approaches in robotics. However, the majority of robotics optimization problems are non-convex and current differentiable optimization techniques are therefore prone to convergence to local minima. When this occurs, the gradients provided by these existing solvers can be wildly inaccurate and will ultimately corrupt the training process. On the other hand, any non-convex robotics problems can be framed as polynomial optimization problems and, in turn, admit convex relaxations that can be used to recover a global solution via so-called certifiably correct methods. We present SDPRLayers, an approach that leverages these methods as well as state-of-the-art convex implicit differentiation techniques to provide certifiably correct gradients throughout the training process. We introduce this approach and showcase theoretical results that provide conditions under which correctness of the gradients is guaranteed. We demonstrate our approach on two simple-but-demonstrative simulated examples, which expose the potential pitfalls of existing, state-of-the-art, differentiable optimization methods. We apply our method in a real-world application: we train a deep neural network to detect image keypoints for robot localization in challenging lighting conditions. An open-source, PyTorch implementation of SDPRLayers will be made available upon paper acceptance.","sentences":["Differentiable optimization is a powerful new paradigm capable of reconciling model-based and learning-based approaches in robotics.","However, the majority of robotics optimization problems are non-convex and current differentiable optimization techniques are therefore prone to convergence to local minima.","When this occurs, the gradients provided by these existing solvers can be wildly inaccurate and will ultimately corrupt the training process.","On the other hand, any non-convex robotics problems can be framed as polynomial optimization problems and, in turn, admit convex relaxations that can be used to recover a global solution via so-called certifiably correct methods.","We present SDPRLayers, an approach that leverages these methods as well as state-of-the-art convex implicit differentiation techniques to provide certifiably correct gradients throughout the training process.","We introduce this approach and showcase theoretical results that provide conditions under which correctness of the gradients is guaranteed.","We demonstrate our approach on two simple-but-demonstrative simulated examples, which expose the potential pitfalls of existing, state-of-the-art, differentiable optimization methods.","We apply our method in a real-world application: we train a deep neural network to detect image keypoints for robot localization in challenging lighting conditions.","An open-source, PyTorch implementation of SDPRLayers will be made available upon paper acceptance."],"url":"http://arxiv.org/abs/2405.19309v1","category":"cs.RO"}
{"created":"2024-05-29 17:30:06","title":"Uniform-in-time estimates on the size of chaos for interacting Brownian particles","abstract":"We consider a system of classical Brownian particles interacting via a smooth long-range potential in the mean-field regime, and we analyze the propagation of chaos in form of sharp, uniform-in-time estimates on many-particle correlation functions. Our results cover both the kinetic Langevin setting and the corresponding overdamped Brownian dynamics. The approach is mainly based on so-called Lions expansions, which we combine with new diagrammatic tools to capture many-particle cancellations, as well as with fine ergodic estimates on the linearized mean-field equation, and with discrete stochastic calculus with respect to initial data. In the process, we derive some new ergodic estimates for the linearized Vlasov-Fokker-Planck kinetic equation that are of independent interest. Our analysis also leads to uniform-in-time concentration estimates and to a uniform-in-time quantitative central limit theorem for the empirical measure associated with the particle dynamics.","sentences":["We consider a system of classical Brownian particles interacting via a smooth long-range potential in the mean-field regime, and we analyze the propagation of chaos in form of sharp, uniform-in-time estimates on many-particle correlation functions.","Our results cover both the kinetic Langevin setting and the corresponding overdamped Brownian dynamics.","The approach is mainly based on so-called Lions expansions, which we combine with new diagrammatic tools to capture many-particle cancellations, as well as with fine ergodic estimates on the linearized mean-field equation, and with discrete stochastic calculus with respect to initial data.","In the process, we derive some new ergodic estimates for the linearized Vlasov-Fokker-Planck kinetic equation that are of independent interest.","Our analysis also leads to uniform-in-time concentration estimates and to a uniform-in-time quantitative central limit theorem for the empirical measure associated with the particle dynamics."],"url":"http://arxiv.org/abs/2405.19306v1","category":"math.AP"}
{"created":"2024-05-29 17:29:54","title":"Set Descriptive Complexity of Solvable Functions","abstract":"In a recent article, we introduced and studied a precise class of dynamical systems called solvable systems. These systems present a dynamic ruled by discontinuous ordinary differential equations with solvable right-hand terms and unique evolution. They correspond to a class of systems for which a transfinite method exist to compute the solution. We also presented several examples including a nontrivial one whose solution yields, at an integer time, a real encoding of the halting set for Turing machines; therefore showcasing that the behavior of solvable systems might describe ordinal Turing computations. In the current article, we study in more depth solvable systems, using tools from descriptive set theory. By establishing a correspondence with the class of well-founded trees, we construct a coanalytic ranking over the set of solvable functions and discuss its relation with other existing rankings for differentiable functions, in particular with the Kechris-Woodin, Denjoy and Zalcwasser ranking. We prove that our ranking is unbounded below the first uncountable ordinal.","sentences":["In a recent article, we introduced and studied a precise class of dynamical systems called solvable systems.","These systems present a dynamic ruled by discontinuous ordinary differential equations with solvable right-hand terms and unique evolution.","They correspond to a class of systems for which a transfinite method exist to compute the solution.","We also presented several examples including a nontrivial one whose solution yields, at an integer time, a real encoding of the halting set for Turing machines; therefore showcasing that the behavior of solvable systems might describe ordinal Turing computations.","In the current article, we study in more depth solvable systems, using tools from descriptive set theory.","By establishing a correspondence with the class of well-founded trees, we construct a coanalytic ranking over the set of solvable functions and discuss its relation with other existing rankings for differentiable functions, in particular with the Kechris-Woodin, Denjoy and Zalcwasser ranking.","We prove that our ranking is unbounded below the first uncountable ordinal."],"url":"http://arxiv.org/abs/2405.19304v1","category":"cs.CC"}
{"created":"2024-05-29 17:24:25","title":"Neural Isometries: Taming Transformations for Equivariant ML","abstract":"Real-world geometry and 3D vision tasks are replete with challenging symmetries that defy tractable analytical expression. In this paper, we introduce Neural Isometries, an autoencoder framework which learns to map the observation space to a general-purpose latent space wherein encodings are related by isometries whenever their corresponding observations are geometrically related in world space. Specifically, we regularize the latent space such that maps between encodings preserve a learned inner product and commute with a learned functional operator, in the same manner as rigid-body transformations commute with the Laplacian. This approach forms an effective backbone for self-supervised representation learning, and we demonstrate that a simple off-the-shelf equivariant network operating in the pre-trained latent space can achieve results on par with meticulously-engineered, handcrafted networks designed to handle complex, nonlinear symmetries. Furthermore, isometric maps capture information about the respective transformations in world space, and we show that this allows us to regress camera poses directly from the coefficients of the maps between encodings of adjacent views of a scene.","sentences":["Real-world geometry and 3D vision tasks are replete with challenging symmetries that defy tractable analytical expression.","In this paper, we introduce Neural Isometries, an autoencoder framework which learns to map the observation space to a general-purpose latent space wherein encodings are related by isometries whenever their corresponding observations are geometrically related in world space.","Specifically, we regularize the latent space such that maps between encodings preserve a learned inner product and commute with a learned functional operator, in the same manner as rigid-body transformations commute with the Laplacian.","This approach forms an effective backbone for self-supervised representation learning, and we demonstrate that a simple off-the-shelf equivariant network operating in the pre-trained latent space can achieve results on par with meticulously-engineered, handcrafted networks designed to handle complex, nonlinear symmetries.","Furthermore, isometric maps capture information about the respective transformations in world space, and we show that this allows us to regress camera poses directly from the coefficients of the maps between encodings of adjacent views of a scene."],"url":"http://arxiv.org/abs/2405.19296v1","category":"cs.CV"}
{"created":"2024-05-29 17:23:51","title":"3D Neural Edge Reconstruction","abstract":"Real-world objects and environments are predominantly composed of edge features, including straight lines and curves. Such edges are crucial elements for various applications, such as CAD modeling, surface meshing, lane mapping, etc. However, existing traditional methods only prioritize lines over curves for simplicity in geometric modeling. To this end, we introduce EMAP, a new method for learning 3D edge representations with a focus on both lines and curves. Our method implicitly encodes 3D edge distance and direction in Unsigned Distance Functions (UDF) from multi-view edge maps. On top of this neural representation, we propose an edge extraction algorithm that robustly abstracts parametric 3D edges from the inferred edge points and their directions. Comprehensive evaluations demonstrate that our method achieves better 3D edge reconstruction on multiple challenging datasets. We further show that our learned UDF field enhances neural surface reconstruction by capturing more details.","sentences":["Real-world objects and environments are predominantly composed of edge features, including straight lines and curves.","Such edges are crucial elements for various applications, such as CAD modeling, surface meshing, lane mapping, etc.","However, existing traditional methods only prioritize lines over curves for simplicity in geometric modeling.","To this end, we introduce EMAP, a new method for learning 3D edge representations with a focus on both lines and curves.","Our method implicitly encodes 3D edge distance and direction in Unsigned Distance Functions (UDF) from multi-view edge maps.","On top of this neural representation, we propose an edge extraction algorithm that robustly abstracts parametric 3D edges from the inferred edge points and their directions.","Comprehensive evaluations demonstrate that our method achieves better 3D edge reconstruction on multiple challenging datasets.","We further show that our learned UDF field enhances neural surface reconstruction by capturing more details."],"url":"http://arxiv.org/abs/2405.19295v1","category":"cs.CV"}
{"created":"2024-05-29 17:18:51","title":"Genuine topological Anderson insulator from impurity induced chirality reversal","abstract":"We investigate a model of Dirac fermions with Haldane type mass impurities which open a global topological gap even in the dilute limit. Surprisingly, we find that the chirality of this mass term, i.e., the sign of the Chern number, can be reversed by tuning the magnitude of the single-impurity scattering. Consequently, the disorder induces a phase disconnected from the clean topological phase, i.e., a genuine topological Anderson insulator. In seeming contradiction to the expectation that mass disorder is an irrelevant perturbation to the clean integer quantum Hall transition, the tri-critical point separating these two Chern insulating phases and a thermal metal phase is located at zero impurity density and connected to the appearance of a zero energy bound state in the continuum corresponding to a divergent Haldane mass impurity. Our conclusions based on the T-matrix expansion are substantiated by large scale Chebyshev-Polynomial-Green-Function numerics. We discuss possible experimental platforms.","sentences":["We investigate a model of Dirac fermions with Haldane type mass impurities which open a global topological gap even in the dilute limit.","Surprisingly, we find that the chirality of this mass term, i.e., the sign of the Chern number, can be reversed by tuning the magnitude of the single-impurity scattering.","Consequently, the disorder induces a phase disconnected from the clean topological phase, i.e., a genuine topological Anderson insulator.","In seeming contradiction to the expectation that mass disorder is an irrelevant perturbation to the clean integer quantum Hall transition, the tri-critical point separating these two Chern insulating phases and a thermal metal phase is located at zero impurity density and connected to the appearance of a zero energy bound state in the continuum corresponding to a divergent Haldane mass impurity.","Our conclusions based on the T-matrix expansion are substantiated by large scale Chebyshev-Polynomial-Green-Function numerics.","We discuss possible experimental platforms."],"url":"http://arxiv.org/abs/2405.19289v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-29 17:12:10","title":"Parametric satellites and connected-sums in the space of Legendrian embeddings","abstract":"This article introduces two new constructions at the higher homotopy level in the space of Legendrian embeddings in $(\\mathbb{R}^3, \\xi_{\\operatorname{std}})$. We first introduce the parametric Legendrian satellite construction, showing that the satellite operation works for parametric families of Legendrian embeddings. This yields new invariants at the higher-order homotopy level.   We then introduce the parametric connected-sum construction. This operation takes as inputs two $n$-spheres based at Legendrian embeddings $K_1$ and $K_2$, respectively, and produces a new $n$-sphere based at $K_1\\# K_2$. As a main application we construct new infinite families of loops of Legendrian embeddings with non-trivial LCH monodromy invariant.","sentences":["This article introduces two new constructions at the higher homotopy level in the space of Legendrian embeddings in $(\\mathbb{R}^3, \\xi_{\\operatorname{std}})$. We first introduce the parametric Legendrian satellite construction, showing that the satellite operation works for parametric families of Legendrian embeddings.","This yields new invariants at the higher-order homotopy level.   ","We then introduce the parametric connected-sum construction.","This operation takes as inputs two $n$-spheres based at Legendrian embeddings $K_1$ and $K_2$, respectively, and produces a new $n$-sphere based at $K_1\\# K_2$.","As a main application we construct new infinite families of loops of Legendrian embeddings with non-trivial LCH monodromy invariant."],"url":"http://arxiv.org/abs/2405.19280v1","category":"math.SG"}
{"created":"2024-05-29 17:11:28","title":"Understanding and Minimising Outlier Features in Neural Network Training","abstract":"Outlier Features (OF) are neurons whose activation magnitudes significantly exceed the average over a neural network's (NN) width. They are well known to emerge during standard transformer training and have the undesirable effect of hindering quantisation in afflicted models. Despite their practical importance, little is known behind why OFs emerge during training, nor how one can minimise them.   Our work focuses on the above questions, first identifying several quantitative metrics, such as the kurtosis over neuron activation norms, to measure OFs. With these metrics, we study how architectural and optimisation choices influence OFs, and provide practical insights to minimise OFs during training. As highlights, we emphasise the importance of controlling signal propagation throughout training, and propose the Outlier Protected transformer block, which removes standard Pre-Norm layers to mitigate OFs, without loss of convergence speed or training stability. Overall, our findings shed new light on our understanding of, our ability to prevent, and the complexity of this important facet in NN training dynamics.","sentences":["Outlier Features (OF) are neurons whose activation magnitudes significantly exceed the average over a neural network's (NN) width.","They are well known to emerge during standard transformer training and have the undesirable effect of hindering quantisation in afflicted models.","Despite their practical importance, little is known behind why OFs emerge during training, nor how one can minimise them.   ","Our work focuses on the above questions, first identifying several quantitative metrics, such as the kurtosis over neuron activation norms, to measure OFs.","With these metrics, we study how architectural and optimisation choices influence OFs, and provide practical insights to minimise OFs during training.","As highlights, we emphasise the importance of controlling signal propagation throughout training, and propose the Outlier Protected transformer block, which removes standard Pre-Norm layers to mitigate OFs, without loss of convergence speed or training stability.","Overall, our findings shed new light on our understanding of, our ability to prevent, and the complexity of this important facet in NN training dynamics."],"url":"http://arxiv.org/abs/2405.19279v1","category":"cs.LG"}
{"created":"2024-05-29 17:07:33","title":"Deep Latent Variable Modeling of Physiological Signals","abstract":"A deep latent variable model is a powerful method for capturing complex distributions. These models assume that underlying structures, but unobserved, are present within the data. In this dissertation, we explore high-dimensional problems related to physiological monitoring using latent variable models. First, we present a novel deep state-space model to generate electrical waveforms of the heart using optically obtained signals as inputs. This can bring about clinical diagnoses of heart disease via simple assessment through wearable devices. Second, we present a brain signal modeling scheme that combines the strengths of probabilistic graphical models and deep adversarial learning. The structured representations can provide interpretability and encode inductive biases to reduce the data complexity of neural oscillations. The efficacy of the learned representations is further studied in epilepsy seizure detection formulated as an unsupervised learning problem. Third, we propose a framework for the joint modeling of physiological measures and behavior. Existing methods to combine multiple sources of brain data provided are limited. Direct analysis of the relationship between different types of physiological measures usually does not involve behavioral data. Our method can identify the unique and shared contributions of brain regions to behavior and can be used to discover new functions of brain regions. The success of these innovative computational methods would allow the translation of biomarker findings across species and provide insight into neurocognitive analysis in numerous biological studies and clinical diagnoses, as well as emerging consumer applications.","sentences":["A deep latent variable model is a powerful method for capturing complex distributions.","These models assume that underlying structures, but unobserved, are present within the data.","In this dissertation, we explore high-dimensional problems related to physiological monitoring using latent variable models.","First, we present a novel deep state-space model to generate electrical waveforms of the heart using optically obtained signals as inputs.","This can bring about clinical diagnoses of heart disease via simple assessment through wearable devices.","Second, we present a brain signal modeling scheme that combines the strengths of probabilistic graphical models and deep adversarial learning.","The structured representations can provide interpretability and encode inductive biases to reduce the data complexity of neural oscillations.","The efficacy of the learned representations is further studied in epilepsy seizure detection formulated as an unsupervised learning problem.","Third, we propose a framework for the joint modeling of physiological measures and behavior.","Existing methods to combine multiple sources of brain data provided are limited.","Direct analysis of the relationship between different types of physiological measures usually does not involve behavioral data.","Our method can identify the unique and shared contributions of brain regions to behavior and can be used to discover new functions of brain regions.","The success of these innovative computational methods would allow the translation of biomarker findings across species and provide insight into neurocognitive analysis in numerous biological studies and clinical diagnoses, as well as emerging consumer applications."],"url":"http://arxiv.org/abs/2405.19277v2","category":"cs.LG"}
{"created":"2024-05-29 17:07:24","title":"A Recipe for Charge Density Prediction","abstract":"In density functional theory, charge density is the core attribute of atomic systems from which all chemical properties can be derived. Machine learning methods are promising in significantly accelerating charge density prediction, yet existing approaches either lack accuracy or scalability. We propose a recipe that can achieve both. In particular, we identify three key ingredients: (1) representing the charge density with atomic and virtual orbitals (spherical fields centered at atom/virtual coordinates); (2) using expressive and learnable orbital basis sets (basis function for the spherical fields); and (3) using high-capacity equivariant neural network architecture. Our method achieves state-of-the-art accuracy while being more than an order of magnitude faster than existing methods. Furthermore, our method enables flexible efficiency-accuracy trade-offs by adjusting the model/basis sizes.","sentences":["In density functional theory, charge density is the core attribute of atomic systems from which all chemical properties can be derived.","Machine learning methods are promising in significantly accelerating charge density prediction, yet existing approaches either lack accuracy or scalability.","We propose a recipe that can achieve both.","In particular, we identify three key ingredients: (1) representing the charge density with atomic and virtual orbitals (spherical fields centered at atom/virtual coordinates); (2) using expressive and learnable orbital basis sets (basis function for the spherical fields); and (3) using high-capacity equivariant neural network architecture.","Our method achieves state-of-the-art accuracy while being more than an order of magnitude faster than existing methods.","Furthermore, our method enables flexible efficiency-accuracy trade-offs by adjusting the model/basis sizes."],"url":"http://arxiv.org/abs/2405.19276v1","category":"physics.comp-ph"}
{"created":"2024-05-29 17:03:31","title":"Mitigating Disparate Impact of Differential Privacy in Federated Learning through Robust Clustering","abstract":"Federated Learning (FL) is a decentralized machine learning (ML) approach that keeps data localized and often incorporates Differential Privacy (DP) to enhance privacy guarantees. Similar to previous work on DP in ML, we observed that differentially private federated learning (DPFL) introduces performance disparities, particularly affecting minority groups. Recent work has attempted to address performance fairness in vanilla FL through clustering, but this method remains sensitive and prone to errors, which are further exacerbated by the DP noise in DPFL. To fill this gap, in this paper, we propose a novel clustered DPFL algorithm designed to effectively identify clients' clusters in highly heterogeneous settings while maintaining high accuracy with DP guarantees. To this end, we propose to cluster clients based on both their model updates and training loss values. Our proposed approach also addresses the server's uncertainties in clustering clients' model updates by employing larger batch sizes along with Gaussian Mixture Model (GMM) to alleviate the impact of noise and potential clustering errors, especially in privacy-sensitive scenarios. We provide theoretical analysis of the effectiveness of our proposed approach. We also extensively evaluate our approach across diverse data distributions and privacy budgets and show its effectiveness in mitigating the disparate impact of DP in FL settings with a small computational cost.","sentences":["Federated Learning (FL) is a decentralized machine learning (ML) approach that keeps data localized and often incorporates Differential Privacy (DP) to enhance privacy guarantees.","Similar to previous work on DP in ML, we observed that differentially private federated learning (DPFL) introduces performance disparities, particularly affecting minority groups.","Recent work has attempted to address performance fairness in vanilla FL through clustering, but this method remains sensitive and prone to errors, which are further exacerbated by the DP noise in DPFL.","To fill this gap, in this paper, we propose a novel clustered DPFL algorithm designed to effectively identify clients' clusters in highly heterogeneous settings while maintaining high accuracy with DP guarantees.","To this end, we propose to cluster clients based on both their model updates and training loss values.","Our proposed approach also addresses the server's uncertainties in clustering clients' model updates by employing larger batch sizes along with Gaussian Mixture Model (GMM) to alleviate the impact of noise and potential clustering errors, especially in privacy-sensitive scenarios.","We provide theoretical analysis of the effectiveness of our proposed approach.","We also extensively evaluate our approach across diverse data distributions and privacy budgets and show its effectiveness in mitigating the disparate impact of DP in FL settings with a small computational cost."],"url":"http://arxiv.org/abs/2405.19272v1","category":"cs.LG"}
{"created":"2024-05-29 16:44:09","title":"Hybrid-Parallel: Achieving High Performance and Energy Efficient Distributed Inference on Robots","abstract":"The rapid advancements in machine learning techniques have led to significant achievements in various real-world robotic tasks. These tasks heavily rely on fast and energy-efficient inference of deep neural network (DNN) models when deployed on robots. To enhance inference performance, distributed inference has emerged as a promising approach, parallelizing inference across multiple powerful GPU devices in modern data centers using techniques such as data parallelism, tensor parallelism, and pipeline parallelism. However, when deployed on real-world robots, existing parallel methods fail to provide low inference latency and meet the energy requirements due to the limited bandwidth of robotic IoT. We present Hybrid-Parallel, a high-performance distributed inference system optimized for robotic IoT. Hybrid-Parallel employs a fine-grained approach to parallelize inference at the granularity of local operators within DNN layers (i.e., operators that can be computed independently with the partial input, such as the convolution kernel in the convolution layer). By doing so, Hybrid-Parallel enables different operators of different layers to be computed and transmitted concurrently, and overlap the computation and transmission phases within the same inference task. The evaluation demonstrate that Hybrid-Parallel reduces inference time by 14.9% ~41.1% and energy consumption per inference by up to 35.3% compared to the state-of-the-art baselines.","sentences":["The rapid advancements in machine learning techniques have led to significant achievements in various real-world robotic tasks.","These tasks heavily rely on fast and energy-efficient inference of deep neural network (DNN) models when deployed on robots.","To enhance inference performance, distributed inference has emerged as a promising approach, parallelizing inference across multiple powerful GPU devices in modern data centers using techniques such as data parallelism, tensor parallelism, and pipeline parallelism.","However, when deployed on real-world robots, existing parallel methods fail to provide low inference latency and meet the energy requirements due to the limited bandwidth of robotic IoT. We present Hybrid-Parallel, a high-performance distributed inference system optimized for robotic IoT. Hybrid-Parallel employs a fine-grained approach to parallelize inference at the granularity of local operators within DNN layers (i.e., operators that can be computed independently with the partial input, such as the convolution kernel in the convolution layer).","By doing so, Hybrid-Parallel enables different operators of different layers to be computed and transmitted concurrently, and overlap the computation and transmission phases within the same inference task.","The evaluation demonstrate that Hybrid-Parallel reduces inference time by 14.9% ~41.1% and energy consumption per inference by up to 35.3% compared to the state-of-the-art baselines."],"url":"http://arxiv.org/abs/2405.19257v1","category":"cs.RO"}
{"created":"2024-05-29 16:39:01","title":"Causal Fermion Systems as an Effective Collapse Theory","abstract":"It is shown that, in the non-relativistic limit, causal fermion systems give rise to an effective collapse theory. The nonlinear and stochastic correction terms to the Schr\\\"odinger equation are derived from the causal action principle. The dynamics of the statistical operator is described by a deterministic equation of Kossakowski-Lindblad form. Moreover, the quantum state undergoes a dynamical collapse compatible with Born's rule. The effective model has similarities with the continuous spontaneous localization model, but differs from it by a conservation law for the probability integral as well as a non-locality in time on a microscopic length scale $\\ell_{\\min}$.","sentences":["It is shown that, in the non-relativistic limit, causal fermion systems give rise to an effective collapse theory.","The nonlinear and stochastic correction terms to the Schr\\\"odinger equation are derived from the causal action principle.","The dynamics of the statistical operator is described by a deterministic equation of Kossakowski-Lindblad form.","Moreover, the quantum state undergoes a dynamical collapse compatible with Born's rule.","The effective model has similarities with the continuous spontaneous localization model, but differs from it by a conservation law for the probability integral as well as a non-locality in time on a microscopic length scale $\\ell_{\\min}$."],"url":"http://arxiv.org/abs/2405.19254v1","category":"math-ph"}
{"created":"2024-05-29 16:34:54","title":"Dark matter admixed neutron stars with a realistic nuclear equation of state from chiral nuclear interactions","abstract":"We study the effects of dark matter on the structural properties of neutron stars. In particular we investigate how the presence of a dark matter component influences the mass-radius relation, the value of the maximum mass of a neutron star and others stellar properties. To model ordinary matter we use a state-of-the-art equation of state of $\\beta$-stable nuclear matter obtained using the Brueckner-Hartree-Fock quantum many-body approach starting from two-body and three-body nuclear interactions derived from chiral effective field theory. The dark matter component of the star is modeled as a non-self-annihilating system of spin $1/2$ fermions and its equation of state as an ideal relativistic Fermi gas. The equilibrium configurations of these dark matter admixed neutron stars (DANS) are calculated by solving a generalization of the Tolman-Oppenheimer-Volkoff equations to the case where the system consists of two perfect fluids interacting solely through gravity. We find that, depending on the dark matter particle mass $m_\\chi$, one can have somehow opposite effects on the stellar properties. In the case $m_\\chi = 1\\, \\mathrm{GeV}$, the stellar gravitational maximum mass $M_{max}$ decreases, whereas in the case $m_\\chi = 0.1\\, \\mathrm{GeV}$, $M_{max}$ increases with respect to the maximum mass of ordinary neutron stars. We also show that the presence of dark matter has indirect sizeable effect on the proton fraction in the ordinary matter fluid and, in the case $m_\\chi = 1\\, \\mathrm{GeV}$, results in a decrease of the threshold gravitational mass $M_{tot}^{durca}$ for having direct URCA processes and fast stellar cooling. Finally we study the stability of dark matter admixed neutron stars with respect to radial perturbations.","sentences":["We study the effects of dark matter on the structural properties of neutron stars.","In particular we investigate how the presence of a dark matter component influences the mass-radius relation, the value of the maximum mass of a neutron star and others stellar properties.","To model ordinary matter we use a state-of-the-art equation of state of $\\beta$-stable nuclear matter obtained using the Brueckner-Hartree-Fock quantum many-body approach starting from two-body and three-body nuclear interactions derived from chiral effective field theory.","The dark matter component of the star is modeled as a non-self-annihilating system of spin $1/2$ fermions and its equation of state as an ideal relativistic Fermi gas.","The equilibrium configurations of these dark matter admixed neutron stars (DANS) are calculated by solving a generalization of the Tolman-Oppenheimer-Volkoff equations to the case where the system consists of two perfect fluids interacting solely through gravity.","We find that, depending on the dark matter particle mass $m_\\chi$, one can have somehow opposite effects on the stellar properties.","In the case $m_\\chi = 1\\, \\mathrm{GeV}$, the stellar gravitational maximum mass $M_{max}$ decreases, whereas in the case $m_\\chi = 0.1\\, \\mathrm{GeV}$, $M_{max}$ increases with respect to the maximum mass of ordinary neutron stars.","We also show that the presence of dark matter has indirect sizeable effect on the proton fraction in the ordinary matter fluid and, in the case $m_\\chi = 1\\, \\mathrm{GeV}$, results in a decrease of the threshold gravitational mass $M_{tot}^{durca}$ for having direct URCA processes and fast stellar cooling.","Finally we study the stability of dark matter admixed neutron stars with respect to radial perturbations."],"url":"http://arxiv.org/abs/2405.19251v1","category":"astro-ph.HE"}
{"created":"2024-05-29 16:21:43","title":"On geometric invariants of singular plane curves","abstract":"Given a germ of a smooth plane curve $(\\{f(x,y)=0\\},0)\\subset (\\mathbb K^2,0), \\mathbb K=\\mathbb R, \\mathbb C$, with an isolated singularity, we define two invariants $I_f$ and $V_f\\in \\mathbb N\\cup\\{\\infty\\}$ which count the number of inflections and vertices (suitably interpreted in the complex case) concentrated at the singular point; the first is an affine invariant and the second is invariant under similarities of $\\mathbb R^2$, and their analogue for $\\mathbb C^2$. We show that for almost all representations of $f$, in the sense that their complement is of infinite codimension, these invariants are finite. Indeed when the curve has no smooth components they are always finite and bounded and we can be much more explicit about the values they can attain; the set of possible values is of course an analytic invariant of $f$. We illustrate our results by computing these invariants for Arnold's $\\mathcal K$-simple singularities as well as singularities that have ${\\mathcal A}$-simple parametrisations. We also obtain a relationship between these invariants, the Milnor number of $f$ and the contact of the curve germ with its osculating circle.","sentences":["Given a germ of a smooth plane curve $(\\{f(x,y)=0\\},0)\\subset (\\mathbb K^2,0), \\mathbb K=\\mathbb R, \\mathbb C$, with an isolated singularity, we define two invariants $I_f$ and $V_f\\in \\mathbb N\\cup\\{\\infty\\}$ which count the number of inflections and vertices (suitably interpreted in the complex case) concentrated at the singular point; the first is an affine invariant and the second is invariant under similarities of $\\mathbb R^2$, and their analogue for $\\mathbb C^2$. We show that for almost all representations of $f$, in the sense that their complement is of infinite codimension, these invariants are finite.","Indeed when the curve has no smooth components they are always finite and bounded and we can be much more explicit about the values they can attain; the set of possible values is of course an analytic invariant of $f$. We illustrate our results by computing these invariants for Arnold's $\\mathcal K$-simple singularities as well as singularities that have ${\\mathcal A}$-simple parametrisations.","We also obtain a relationship between these invariants, the Milnor number of $f$ and the contact of the curve germ with its osculating circle."],"url":"http://arxiv.org/abs/2405.19239v1","category":"math.DG"}
{"created":"2024-05-29 16:13:54","title":"Forward-Backward Knowledge Distillation for Continual Clustering","abstract":"Unsupervised Continual Learning (UCL) is a burgeoning field in machine learning, focusing on enabling neural networks to sequentially learn tasks without explicit label information. Catastrophic Forgetting (CF), where models forget previously learned tasks upon learning new ones, poses a significant challenge in continual learning, especially in UCL, where labeled information of data is not accessible. CF mitigation strategies, such as knowledge distillation and replay buffers, often face memory inefficiency and privacy issues. Although current research in UCL has endeavored to refine data representations and address CF in streaming data contexts, there is a noticeable lack of algorithms specifically designed for unsupervised clustering. To fill this gap, in this paper, we introduce the concept of Unsupervised Continual Clustering (UCC). We propose Forward-Backward Knowledge Distillation for unsupervised Continual Clustering (FBCC) to counteract CF within the context of UCC. FBCC employs a single continual learner (the ``teacher'') with a cluster projector, along with multiple student models, to address the CF issue. The proposed method consists of two phases: Forward Knowledge Distillation, where the teacher learns new clusters while retaining knowledge from previous tasks with guidance from specialized student models, and Backward Knowledge Distillation, where a student model mimics the teacher's behavior to retain task-specific knowledge, aiding the teacher in subsequent tasks. FBCC marks a pioneering approach to UCC, demonstrating enhanced performance and memory efficiency in clustering across various tasks, outperforming the application of clustering algorithms to the latent space of state-of-the-art UCL algorithms.","sentences":["Unsupervised Continual Learning (UCL) is a burgeoning field in machine learning, focusing on enabling neural networks to sequentially learn tasks without explicit label information.","Catastrophic Forgetting (CF), where models forget previously learned tasks upon learning new ones, poses a significant challenge in continual learning, especially in UCL, where labeled information of data is not accessible.","CF mitigation strategies, such as knowledge distillation and replay buffers, often face memory inefficiency and privacy issues.","Although current research in UCL has endeavored to refine data representations and address CF in streaming data contexts, there is a noticeable lack of algorithms specifically designed for unsupervised clustering.","To fill this gap, in this paper, we introduce the concept of Unsupervised Continual Clustering (UCC).","We propose Forward-Backward Knowledge Distillation for unsupervised Continual Clustering (FBCC) to counteract CF within the context of UCC.","FBCC employs a single continual learner (the ``teacher'') with a cluster projector, along with multiple student models, to address the CF issue.","The proposed method consists of two phases: Forward Knowledge Distillation, where the teacher learns new clusters while retaining knowledge from previous tasks with guidance from specialized student models, and Backward Knowledge Distillation, where a student model mimics the teacher's behavior to retain task-specific knowledge, aiding the teacher in subsequent tasks.","FBCC marks a pioneering approach to UCC, demonstrating enhanced performance and memory efficiency in clustering across various tasks, outperforming the application of clustering algorithms to the latent space of state-of-the-art UCL algorithms."],"url":"http://arxiv.org/abs/2405.19234v1","category":"cs.LG"}
{"created":"2024-05-29 16:07:39","title":"Valid Conformal Prediction for Dynamic GNNs","abstract":"Graph neural networks (GNNs) are powerful black-box models which have shown impressive empirical performance. However, without any form of uncertainty quantification, it can be difficult to trust such models in high-risk scenarios. Conformal prediction aims to address this problem, however, an assumption of exchangeability is required for its validity which has limited its applicability to static graphs and transductive regimes. We propose to use unfolding, which allows any existing static GNN to output a dynamic graph embedding with exchangeability properties. Using this, we extend the validity of conformal prediction to dynamic GNNs in both transductive and semi-inductive regimes. We provide a theoretical guarantee of valid conformal prediction in these cases and demonstrate the empirical validity, as well as the performance gains, of unfolded GNNs against standard GNN architectures on both simulated and real datasets.","sentences":["Graph neural networks (GNNs) are powerful black-box models which have shown impressive empirical performance.","However, without any form of uncertainty quantification, it can be difficult to trust such models in high-risk scenarios.","Conformal prediction aims to address this problem, however, an assumption of exchangeability is required for its validity which has limited its applicability to static graphs and transductive regimes.","We propose to use unfolding, which allows any existing static GNN to output a dynamic graph embedding with exchangeability properties.","Using this, we extend the validity of conformal prediction to dynamic GNNs in both transductive and semi-inductive regimes.","We provide a theoretical guarantee of valid conformal prediction in these cases and demonstrate the empirical validity, as well as the performance gains, of unfolded GNNs against standard GNN architectures on both simulated and real datasets."],"url":"http://arxiv.org/abs/2405.19230v1","category":"stat.ML"}
{"created":"2024-05-29 17:59:07","title":"Self-Exploring Language Models: Active Preference Elicitation for Online Alignment","abstract":"Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions. Unlike offline alignment with a fixed dataset, online feedback collection from humans or AI on model generations typically leads to more capable reward models and better-aligned LLMs through an iterative process. However, achieving a globally accurate reward model requires systematic exploration to generate diverse responses that span the vast space of natural language. Random sampling from standard reward-maximizing LLMs alone is insufficient to fulfill this requirement. To address this issue, we propose a bilevel objective optimistically biased towards potentially high-reward responses to actively explore out-of-distribution regions. By solving the inner-level problem with the reparameterized reward function, the resulting algorithm, named Self-Exploring Language Models (SELM), eliminates the need for a separate RM and iteratively updates the LLM with a straightforward objective. Compared to Direct Preference Optimization (DPO), the SELM objective reduces indiscriminate favor of unseen extrapolations and enhances exploration efficiency. Our experimental results demonstrate that when finetuned on Zephyr-7B-SFT and Llama-3-8B-Instruct models, SELM significantly boosts the performance on instruction-following benchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard academic benchmarks in different settings. Our code and models are available at https://github.com/shenao-zhang/SELM.","sentences":["Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions.","Unlike offline alignment with a fixed dataset, online feedback collection from humans or AI on model generations typically leads to more capable reward models and better-aligned LLMs through an iterative process.","However, achieving a globally accurate reward model requires systematic exploration to generate diverse responses that span the vast space of natural language.","Random sampling from standard reward-maximizing LLMs alone is insufficient to fulfill this requirement.","To address this issue, we propose a bilevel objective optimistically biased towards potentially high-reward responses to actively explore out-of-distribution regions.","By solving the inner-level problem with the reparameterized reward function, the resulting algorithm, named Self-Exploring Language Models (SELM), eliminates the need for a separate RM and iteratively updates the LLM with a straightforward objective.","Compared to Direct Preference Optimization (DPO), the SELM objective reduces indiscriminate favor of unseen extrapolations and enhances exploration efficiency.","Our experimental results demonstrate that when finetuned on Zephyr-7B-SFT and Llama-3-8B-Instruct models, SELM significantly boosts the performance on instruction-following benchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard academic benchmarks in different settings.","Our code and models are available at https://github.com/shenao-zhang/SELM."],"url":"http://arxiv.org/abs/2405.19332v1","category":"cs.LG"}
{"created":"2024-05-29 17:57:16","title":"MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series","abstract":"Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks. However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details. Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs. However, only the model's weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.) being undisclosed. To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided. These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks. However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes. To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs. Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided. Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs.","sentences":["Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks.","However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details.","Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs.","However, only the model's weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.)","being undisclosed.","To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided.","These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks.","However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes.","To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens.","Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs.","Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided.","Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs."],"url":"http://arxiv.org/abs/2405.19327v2","category":"cs.CL"}
{"created":"2024-05-29 17:52:22","title":"DGD: Dynamic 3D Gaussians Distillation","abstract":"We tackle the task of learning dynamic 3D semantic radiance fields given a single monocular video as input. Our learned semantic radiance field captures per-point semantics as well as color and geometric properties for a dynamic 3D scene, enabling the generation of novel views and their corresponding semantics. This enables the segmentation and tracking of a diverse set of 3D semantic entities, specified using a simple and intuitive interface that includes a user click or a text prompt. To this end, we present DGD, a unified 3D representation for both the appearance and semantics of a dynamic 3D scene, building upon the recently proposed dynamic 3D Gaussians representation. Our representation is optimized over time with both color and semantic information. Key to our method is the joint optimization of the appearance and semantic attributes, which jointly affect the geometric properties of the scene. We evaluate our approach in its ability to enable dense semantic 3D object tracking and demonstrate high-quality results that are fast to render, for a diverse set of scenes. Our project webpage is available on https://isaaclabe.github.io/DGD-Website/","sentences":["We tackle the task of learning dynamic 3D semantic radiance fields given a single monocular video as input.","Our learned semantic radiance field captures per-point semantics as well as color and geometric properties for a dynamic 3D scene, enabling the generation of novel views and their corresponding semantics.","This enables the segmentation and tracking of a diverse set of 3D semantic entities, specified using a simple and intuitive interface that includes a user click or a text prompt.","To this end, we present DGD, a unified 3D representation for both the appearance and semantics of a dynamic 3D scene, building upon the recently proposed dynamic 3D Gaussians representation.","Our representation is optimized over time with both color and semantic information.","Key to our method is the joint optimization of the appearance and semantic attributes, which jointly affect the geometric properties of the scene.","We evaluate our approach in its ability to enable dense semantic 3D object tracking and demonstrate high-quality results that are fast to render, for a diverse set of scenes.","Our project webpage is available on https://isaaclabe.github.io/DGD-Website/"],"url":"http://arxiv.org/abs/2405.19321v1","category":"cs.CV"}
{"created":"2024-05-29 17:51:42","title":"Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF","abstract":"Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference. Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected. While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations.   In this paper, we introduce a unified approach to online and offline RLHF -- value-incentivized preference optimization (VPO) -- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a $\\textit{sign}$ to indicate whether the optimism or pessimism is chosen. VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization. Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts. Moreover, experiments on text summarization and dialog verify the practicality and effectiveness of VPO.","sentences":["Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference.","Depending on the availability of preference data, both online and offline RLHF are active areas of investigation.","A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected.","While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations.   ","In this paper, we introduce a unified approach to online and offline RLHF -- value-incentivized preference optimization (VPO) -- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a $\\textit{sign}$ to indicate whether the optimism or pessimism is chosen.","VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization.","Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts.","Moreover, experiments on text summarization and dialog verify the practicality and effectiveness of VPO."],"url":"http://arxiv.org/abs/2405.19320v1","category":"cs.LG"}
{"created":"2024-05-29 17:39:48","title":"Robust Preference Optimization through Reward Model Distillation","abstract":"Language model (LM) post-training (or alignment) involves maximizing a reward function that is derived from preference annotations. Direct Preference Optimization (DPO) is a popular offline alignment method that trains a policy directly on preference data without the need to train a reward model or apply reinforcement learning. However, typical preference datasets have only a single, or at most a few, annotation per preference pair, which causes DPO to overconfidently assign rewards that trend towards infinite magnitude. This frequently leads to degenerate policies, sometimes causing even the probabilities of the preferred generations to go to zero. In this work, we analyze this phenomenon and propose distillation to get a better proxy for the true preference distribution over generation pairs: we train the LM to produce probabilities that match the distribution induced by a reward model trained on the preference data. Moreover, to account for uncertainty in the reward model we are distilling from, we optimize against a family of reward models that, as a whole, is likely to include at least one reasonable proxy for the preference distribution. Our results show that distilling from such a family of reward models leads to improved robustness to distribution shift in preference annotations, while preserving the simple supervised nature of DPO.","sentences":["Language model (LM) post-training (or alignment) involves maximizing a reward function that is derived from preference annotations.","Direct Preference Optimization (DPO) is a popular offline alignment method that trains a policy directly on preference data without the need to train a reward model or apply reinforcement learning.","However, typical preference datasets have only a single, or at most a few, annotation per preference pair, which causes DPO to overconfidently assign rewards that trend towards infinite magnitude.","This frequently leads to degenerate policies, sometimes causing even the probabilities of the preferred generations to go to zero.","In this work, we analyze this phenomenon and propose distillation to get a better proxy for the true preference distribution over generation pairs: we train the LM to produce probabilities that match the distribution induced by a reward model trained on the preference data.","Moreover, to account for uncertainty in the reward model we are distilling from, we optimize against a family of reward models that, as a whole, is likely to include at least one reasonable proxy for the preference distribution.","Our results show that distilling from such a family of reward models leads to improved robustness to distribution shift in preference annotations, while preserving the simple supervised nature of DPO."],"url":"http://arxiv.org/abs/2405.19316v1","category":"cs.LG"}
{"created":"2024-05-29 17:27:50","title":"Multi-qubit circuit synthesis and Hermitian lattices","abstract":"We present new optimal and heuristic algorithms for exact synthesis of multi-qubit unitaries and isometries. For example, our algorithms find Clifford and T circuits for unitaries with entries in $\\mathbb{Z}[i,1/\\sqrt{2}]$. The optimal algorithms are the A* search instantiated with a new data structure for graph vertices and new consistent heuristic functions. We also prove that for some gate sets, best-first search synthesis relying on the same heuristic is efficient. For example, for two-qubit Clifford and T circuits, our best-first search runtime is proportional to the T-count of the unitary. Our algorithms rely on Hermite and Smith Normal Forms of matrices with entries in a ring of integers of a number field, and we leverage the theory of and algorithms for Hermitian lattices over number fields to prove efficiency. These new techniques are of independent interest for future work on multi-qubit exact circuit synthesis and related questions.","sentences":["We present new optimal and heuristic algorithms for exact synthesis of multi-qubit unitaries and isometries.","For example, our algorithms find Clifford and T circuits for unitaries with entries in $\\mathbb{Z}[i,1/\\sqrt{2}]$. The optimal algorithms are the A* search instantiated with a new data structure for graph vertices and new consistent heuristic functions.","We also prove that for some gate sets, best-first search synthesis relying on the same heuristic is efficient.","For example, for two-qubit Clifford and T circuits, our best-first search runtime is proportional to the T-count of the unitary.","Our algorithms rely on Hermite and Smith Normal Forms of matrices with entries in a ring of integers of a number field, and we leverage the theory of and algorithms for Hermitian lattices over number fields to prove efficiency.","These new techniques are of independent interest for future work on multi-qubit exact circuit synthesis and related questions."],"url":"http://arxiv.org/abs/2405.19302v1","category":"quant-ph"}
{"created":"2024-05-29 17:27:30","title":"Safe and Efficient Estimation for Robotics through the Optimal Use of Resources","abstract":"In order to operate in and interact with the physical world, robots need to have estimates of the current and future state of the environment. We thus equip robots with sensors and build models and algorithms that, given some measurements, produce estimates of the current or future states. Environments can be unpredictable and sensors are not perfect. Therefore, it is important to both use all information available, and to do so optimally: making sure that we get the best possible answer from the amount of information we have. However, in prevalent research, uncommon sensors, such as sound or radio-frequency signals, are commonly ignored for state estimation; and the most popular solvers employed to produce state estimates are only of local nature, meaning they may produce suboptimal estimates for the typically non-convex estimation problems. My research aims to use resources more optimally, by building on 1) multi-modality: using ubiquitous RF transceivers and microphones to support state estimation, 2) building certifiably optimal solvers and 3) learning and improving adequate models from data.","sentences":["In order to operate in and interact with the physical world, robots need to have estimates of the current and future state of the environment.","We thus equip robots with sensors and build models and algorithms that, given some measurements, produce estimates of the current or future states.","Environments can be unpredictable and sensors are not perfect.","Therefore, it is important to both use all information available, and to do so optimally: making sure that we get the best possible answer from the amount of information we have.","However, in prevalent research, uncommon sensors, such as sound or radio-frequency signals, are commonly ignored for state estimation; and the most popular solvers employed to produce state estimates are only of local nature, meaning they may produce suboptimal estimates for the typically non-convex estimation problems.","My research aims to use resources more optimally, by building on 1) multi-modality: using ubiquitous RF transceivers and microphones to support state estimation, 2) building certifiably optimal solvers and 3) learning and improving adequate models from data."],"url":"http://arxiv.org/abs/2405.19301v1","category":"cs.RO"}
{"created":"2024-05-29 17:23:05","title":"GPU-accelerated Higher Representations of Wilson Fermions with HiRep","abstract":"We are improving one of the available lattice software packages HiRep by adding GPU acceleration supporting highly-optimized simulations on both NVIDIA and AMD GPUs. HiRep allows lattice simulations of theories with fermions in higher representations and a variable number of colors in the gauge group. The development is accompanied by an overall software quality improvement in the build system, testing, and documentation, adding features for both CPUs and GPUs. The software is available under https://github.com/claudiopica/HiRep","sentences":["We are improving one of the available lattice software packages HiRep by adding GPU acceleration supporting highly-optimized simulations on both NVIDIA and AMD GPUs.","HiRep allows lattice simulations of theories with fermions in higher representations and a variable number of colors in the gauge group.","The development is accompanied by an overall software quality improvement in the build system, testing, and documentation, adding features for both CPUs and GPUs.","The software is available under https://github.com/claudiopica/HiRep"],"url":"http://arxiv.org/abs/2405.19294v1","category":"hep-lat"}
{"created":"2024-05-29 17:21:25","title":"Act Natural! Projecting Autonomous System Trajectories Into Naturalistic Behavior Sets","abstract":"Autonomous agents operating around human actors must consider how their behaviors might affect those humans, even when not directly interacting with them. To this end, it is often beneficial to be predictable and appear naturalistic. Existing methods to address this problem use human actor intent modeling or imitation learning techniques, but these approaches rarely capture all possible motivations for human behavior or require significant amounts of data. In contrast, we propose a technique for modeling naturalistic behavior as a set of convex hulls computed over a relatively small dataset of human behavior. Given this set, we design an optimization-based filter which projects arbitrary trajectories into it to make them more naturalistic for autonomous agents to execute while also satisfying dynamics constraints. We demonstrate our methods on real-world human driving data from the inD intersection dataset (Bock et al., 2020).","sentences":["Autonomous agents operating around human actors must consider how their behaviors might affect those humans, even when not directly interacting with them.","To this end, it is often beneficial to be predictable and appear naturalistic.","Existing methods to address this problem use human actor intent modeling or imitation learning techniques, but these approaches rarely capture all possible motivations for human behavior or require significant amounts of data.","In contrast, we propose a technique for modeling naturalistic behavior as a set of convex hulls computed over a relatively small dataset of human behavior.","Given this set, we design an optimization-based filter which projects arbitrary trajectories into it to make them more naturalistic for autonomous agents to execute while also satisfying dynamics constraints.","We demonstrate our methods on real-world human driving data from the inD intersection dataset (Bock et al., 2020)."],"url":"http://arxiv.org/abs/2405.19292v1","category":"cs.MA"}
