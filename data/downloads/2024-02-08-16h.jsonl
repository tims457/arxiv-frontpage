{"created":"2024-02-06 18:59:08","title":"HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal","abstract":"Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.","sentences":["Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods.","To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming.","We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria.","Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights.","We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses.","We open source HarmBench at https://github.com/centerforaisafety/HarmBench."],"url":"http://arxiv.org/abs/2402.04249v1","category":"cs.LG"}
{"created":"2024-02-06 18:54:07","title":"Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science","abstract":"Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents and advocate for the development of improved models, robust benchmarks, and comprehensive regulations to address these issues effectively.","sentences":["Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines.","While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety.","However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities.","This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures.","We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment.","Then, we delve into the origins of these vulnerabilities and provide a scoping review of the limited existing works.","Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks.","Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents and advocate for the development of improved models, robust benchmarks, and comprehensive regulations to address these issues effectively."],"url":"http://arxiv.org/abs/2402.04247v2","category":"cs.CY"}
{"created":"2024-02-06 18:49:39","title":"Novel IMU-based Adaptive Estimator of the Center of Rotation of Joints for Movement Analysis","abstract":"The location of the center of rotation (COR) of joints is a key parameter in multiple applications of human motion analysis. The aim of this work was to propose a novel real-time estimator of the center of fixed joints using an inertial measurement unit (IMU). Since the distance to this center commonly varies during the joint motion due to soft tissue artifacts (STA), our approach is aimed at adapting to these small variations when the COR is fixed. Our proposal, called ArVEd, to the best of our knowledge, is the first real-time estimator of the IMU-joint center vector based on one IMU. Previous works are off-line and require a complete measurement batch to be solved and most of them are not tested on the real scenario. The algorithm is based on an Extended Kalman Filter (EKF) that provides an adaptive vector to STA motion variations at each time instant, without requiring a pre-processing stage to reduce the level of noise. ArVEd has been tested through different experiments, including synthetic and real data. The synthetic data are obtained from a simulated spherical pendulum whose COR is fixed, considering both a constant and a variable IMU-joint vector, that simulates translational IMU motions due to STA. The results prove that ArVEd is adapted to obtain a vector per sample with an accuracy of 6.8$\\pm$3.9 on the synthetic data, that means an error lower than 3.5% of the simulated IMU-joint vector. Its accuracy is also tested on the real scenario estimating the COR of the hip of 5 volunteers using as reference the results from an optical system. In this case, ArVEd gets an average error of 9.5% of the real vector value. In all the experiments, ArVEd outperforms the published results of the reference algorithms.","sentences":["The location of the center of rotation (COR) of joints is a key parameter in multiple applications of human motion analysis.","The aim of this work was to propose a novel real-time estimator of the center of fixed joints using an inertial measurement unit (IMU).","Since the distance to this center commonly varies during the joint motion due to soft tissue artifacts (STA), our approach is aimed at adapting to these small variations when the COR is fixed.","Our proposal, called ArVEd, to the best of our knowledge, is the first real-time estimator of the IMU-joint center vector based on one IMU.","Previous works are off-line and require a complete measurement batch to be solved and most of them are not tested on the real scenario.","The algorithm is based on an Extended Kalman Filter (EKF) that provides an adaptive vector to STA motion variations at each time instant, without requiring a pre-processing stage to reduce the level of noise.","ArVEd has been tested through different experiments, including synthetic and real data.","The synthetic data are obtained from a simulated spherical pendulum whose COR is fixed, considering both a constant and a variable IMU-joint vector, that simulates translational IMU motions due to STA.","The results prove that ArVEd is adapted to obtain a vector per sample with an accuracy of 6.8$\\pm$3.9 on the synthetic data, that means an error lower than 3.5% of the simulated IMU-joint vector.","Its accuracy is also tested on the real scenario estimating the COR of the hip of 5 volunteers using as reference the results from an optical system.","In this case, ArVEd gets an average error of 9.5% of the real vector value.","In all the experiments, ArVEd outperforms the published results of the reference algorithms."],"url":"http://arxiv.org/abs/2402.04240v1","category":"eess.SP"}
{"created":"2024-02-06 18:39:43","title":"Can Generative Agents Predict Emotion?","abstract":"Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the perception of the new event. Finally, the new experience is then added to the agents memory to be used in the creation of future norms. By creating multiple experiences in natural language from emotionally charged situations, we test the proposed architecture on a wide range of scenarios. The mixed results suggests that introducing context can occasionally improve the emotional alignment of the agent, but further study and comparison with human evaluators is necessary. We hope that this paper is another step towards the alignment of generative agents.","sentences":["Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans.","In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories.","Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation.","First, the agent perceives new experiences as time series text data.","After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm.","Through this comparison we can analyse how the agent reacts to the new experience in context.","The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the perception of the new event.","Finally, the new experience is then added to the agents memory to be used in the creation of future norms.","By creating multiple experiences in natural language from emotionally charged situations, we test the proposed architecture on a wide range of scenarios.","The mixed results suggests that introducing context can occasionally improve the emotional alignment of the agent, but further study and comparison with human evaluators is necessary.","We hope that this paper is another step towards the alignment of generative agents."],"url":"http://arxiv.org/abs/2402.04232v2","category":"cs.AI"}
{"created":"2024-02-06 18:36:52","title":"MusicRL: Aligning Music Generation to Human Preferences","abstract":"We propose MusicRL, the first music generation system finetuned from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as \"upbeat work-out music\" can map to a retro guitar solo or a techno pop beat). Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning. MusicRL is a pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete audio tokens finetuned with reinforcement learning to maximise sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human feedback at scale. Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU combines the two approaches and results in the best model according to human raters. Ablation studies shed light on the musical attributes influencing human preferences, indicating that text adherence and quality only account for a part of it. This underscores the prevalence of subjectivity in musical appreciation and calls for further involvement of human listeners in the finetuning of music generation models.","sentences":["We propose MusicRL, the first music generation system finetuned from human feedback.","Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as \"upbeat work-out music\" can map to a retro guitar solo or a techno pop beat).","Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning.","MusicRL is a pretrained autoregressive MusicLM","(Agostinelli et al., 2023) model of discrete audio tokens finetuned with reinforcement learning to maximise sequence-level rewards.","We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences.","Using Reinforcement Learning from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human feedback at scale.","Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the baseline.","Ultimately, MusicRL-RU combines the two approaches and results in the best model according to human raters.","Ablation studies shed light on the musical attributes influencing human preferences, indicating that text adherence and quality only account for a part of it.","This underscores the prevalence of subjectivity in musical appreciation and calls for further involvement of human listeners in the finetuning of music generation models."],"url":"http://arxiv.org/abs/2402.04229v1","category":"cs.LG"}
{"created":"2024-02-06 18:36:44","title":"Intelligent Collective Escape of Swarm Robots Based on a Novel Fish-inspired Self-adaptive Approach with Neurodynamic Models","abstract":"Fish schools present high-efficiency group behaviors through simple individual interactions to collective migration and dynamic escape from the predator. The school behavior of fish is usually a good inspiration to design control architecture for swarm robots. In this paper, a novel fish-inspired self-adaptive approach is proposed for collective escape for the swarm robots. In addition, a bio-inspired neural network (BINN) is introduced to generate collision-free escape robot trajectories through the combination of attractive and repulsive forces. Furthermore, to cope with dynamic environments, a neurodynamics-based self-adaptive mechanism is proposed to improve the self-adaptive performance of the swarm robots in the changing environment. Similar to fish escape maneuvers, simulation and experimental results show that the swarm robots are capable of collectively leaving away from the threats. Several comparison studies demonstrated that the proposed approach can significantly improve the effectiveness and efficiency of system performance, and the flexibility and robustness in complex environments.","sentences":["Fish schools present high-efficiency group behaviors through simple individual interactions to collective migration and dynamic escape from the predator.","The school behavior of fish is usually a good inspiration to design control architecture for swarm robots.","In this paper, a novel fish-inspired self-adaptive approach is proposed for collective escape for the swarm robots.","In addition, a bio-inspired neural network (BINN) is introduced to generate collision-free escape robot trajectories through the combination of attractive and repulsive forces.","Furthermore, to cope with dynamic environments, a neurodynamics-based self-adaptive mechanism is proposed to improve the self-adaptive performance of the swarm robots in the changing environment.","Similar to fish escape maneuvers, simulation and experimental results show that the swarm robots are capable of collectively leaving away from the threats.","Several comparison studies demonstrated that the proposed approach can significantly improve the effectiveness and efficiency of system performance, and the flexibility and robustness in complex environments."],"url":"http://arxiv.org/abs/2402.04228v1","category":"cs.RO"}
{"created":"2024-02-06 18:34:19","title":"Growth rate of self-sustained QED cascades induced by intense lasers","abstract":"It was suggested [A. R. Bell & J. G. Kirk, PRL 101, 200403 (2008)] that an avalanche of electron-positron pairs can be triggered in the laboratory by a standing wave generated by intense laser fields. Here, we present a general solution to the long-standing problem of the avalanche growth rate calculation. We provide a simple formula that we apply to the case of the standing wave created by two circularly polarized lasers and demonstrate that it allows to predict the particle yield for the full range of intensity able to generate an avalanche. We account for the damping of the growth rate due to pair migration from the region of prolific generation and show that above a threshold in intensity, this effect is negligible. The growth rate calculation allows us to predict when abundant pair production will induce a back-reaction on the generating field due to plasma collective effects and screening. Our model shows excellent agreement with self-consistent PIC simulations and can be applied to study the generation of electron-positron pair avalanches in realistic field configurations to plan future experiments at ultra-high-intensity laser facilities.","sentences":["It was suggested [A. R. Bell & J. G. Kirk, PRL 101, 200403 (2008)] that an avalanche of electron-positron pairs can be triggered in the laboratory by a standing wave generated by intense laser fields.","Here, we present a general solution to the long-standing problem of the avalanche growth rate calculation.","We provide a simple formula that we apply to the case of the standing wave created by two circularly polarized lasers and demonstrate that it allows to predict the particle yield for the full range of intensity able to generate an avalanche.","We account for the damping of the growth rate due to pair migration from the region of prolific generation and show that above a threshold in intensity, this effect is negligible.","The growth rate calculation allows us to predict when abundant pair production will induce a back-reaction on the generating field due to plasma collective effects and screening.","Our model shows excellent agreement with self-consistent PIC simulations and can be applied to study the generation of electron-positron pair avalanches in realistic field configurations to plan future experiments at ultra-high-intensity laser facilities."],"url":"http://arxiv.org/abs/2402.04225v1","category":"physics.plasm-ph"}
{"created":"2024-02-06 18:07:43","title":"\"Task Success\" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors","abstract":"Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences. Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback. In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met. Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations). However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving. This begs the question: when no sound verifier is available, can we use large vision and language models (VLMs), which are approximately omniscient, as scalable Behavior Critics to catch undesirable robot behaviors in videos? To answer this, we first construct a benchmark that contains diverse cases of goal-reaching yet undesirable robot policies. Then, we comprehensively evaluate VLM critics to gain a deeper understanding of their strengths and failure modes. Based on the evaluation, we provide guidelines on how to effectively utilize VLM critiques and showcase a practical way to integrate the feedback into an iterative process of policy refinement. The dataset and codebase are released at: https://guansuns.github.io/pages/vlm-critic.","sentences":["Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences.","Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback.","In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met.","Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations).","However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving.","This begs the question: when no sound verifier is available, can we use large vision and language models (VLMs), which are approximately omniscient, as scalable Behavior Critics to catch undesirable robot behaviors in videos?","To answer this, we first construct a benchmark that contains diverse cases of goal-reaching yet undesirable robot policies.","Then, we comprehensively evaluate VLM critics to gain a deeper understanding of their strengths and failure modes.","Based on the evaluation, we provide guidelines on how to effectively utilize VLM critiques and showcase a practical way to integrate the feedback into an iterative process of policy refinement.","The dataset and codebase are released at: https://guansuns.github.io/pages/vlm-critic."],"url":"http://arxiv.org/abs/2402.04210v1","category":"cs.AI"}
{"created":"2024-02-06 18:05:30","title":"Acute kidney injury prediction for non-critical care patients: a retrospective external and internal validation study","abstract":"Background: Acute kidney injury (AKI), the decline of kidney excretory function, occurs in up to 18% of hospitalized admissions. Progression of AKI may lead to irreversible kidney damage. Methods: This retrospective cohort study includes adult patients admitted to a non-intensive care unit at the University of Pittsburgh Medical Center (UPMC) (n = 46,815) and University of Florida Health (UFH) (n = 127,202). We developed and compared deep learning and conventional machine learning models to predict progression to Stage 2 or higher AKI within the next 48 hours. We trained local models for each site (UFH Model trained on UFH, UPMC Model trained on UPMC) and a separate model with a development cohort of patients from both sites (UFH-UPMC Model). We internally and externally validated the models on each site and performed subgroup analyses across sex and race. Results: Stage 2 or higher AKI occurred in 3% (n=3,257) and 8% (n=2,296) of UFH and UPMC patients, respectively. Area under the receiver operating curve values (AUROC) for the UFH test cohort ranged between 0.77 (UPMC Model) and 0.81 (UFH Model), while AUROC values ranged between 0.79 (UFH Model) and 0.83 (UPMC Model) for the UPMC test cohort. UFH-UPMC Model achieved an AUROC of 0.81 (95% confidence interval [CI] [0.80, 0.83]) for UFH and 0.82 (95% CI [0.81,0.84]) for UPMC test cohorts; an area under the precision recall curve values (AUPRC) of 0.6 (95% CI, [0.05, 0.06]) for UFH and 0.13 (95% CI, [0.11,0.15]) for UPMC test cohorts. Kinetic estimated glomerular filtration rate, nephrotoxic drug burden and blood urea nitrogen remained the top three features with the highest influence across the models and health centers. Conclusion: Locally developed models displayed marginally reduced discrimination when tested on another institution, while the top set of influencing features remained the same across the models and sites.","sentences":["Background: Acute kidney injury (AKI), the decline of kidney excretory function, occurs in up to 18% of hospitalized admissions.","Progression of AKI may lead to irreversible kidney damage.","Methods: This retrospective cohort study includes adult patients admitted to a non-intensive care unit at the University of Pittsburgh Medical Center (UPMC) (n = 46,815) and University of Florida Health (UFH) (n = 127,202).","We developed and compared deep learning and conventional machine learning models to predict progression to Stage 2 or higher AKI within the next 48 hours.","We trained local models for each site (UFH Model trained on UFH, UPMC Model trained on UPMC) and a separate model with a development cohort of patients from both sites (UFH-UPMC Model).","We internally and externally validated the models on each site and performed subgroup analyses across sex and race.","Results: Stage 2 or higher AKI occurred in 3% (n=3,257) and 8% (n=2,296) of UFH and UPMC patients, respectively.","Area under the receiver operating curve values (AUROC) for the UFH test cohort ranged between 0.77 (UPMC Model) and 0.81 (UFH Model), while AUROC values ranged between 0.79 (UFH Model) and 0.83 (UPMC Model) for the UPMC test cohort.","UFH-UPMC Model achieved an AUROC of 0.81 (95% confidence interval","[CI]","[0.80, 0.83]) for UFH and 0.82 (95% CI","[0.81,0.84]) for UPMC test cohorts; an area under the precision recall curve values (AUPRC) of 0.6 (95% CI, [0.05, 0.06]) for UFH and 0.13 (95% CI, [0.11,0.15]) for UPMC test cohorts.","Kinetic estimated glomerular filtration rate, nephrotoxic drug burden and blood urea nitrogen remained the top three features with the highest influence across the models and health centers.","Conclusion: Locally developed models displayed marginally reduced discrimination when tested on another institution, while the top set of influencing features remained the same across the models and sites."],"url":"http://arxiv.org/abs/2402.04209v1","category":"cs.LG"}
{"created":"2024-02-06 17:59:46","title":"Human-Like Geometric Abstraction in Large Pre-trained Neural Networks","abstract":"Humans possess a remarkable capacity to recognize and manipulate abstract structure, which is especially apparent in the domain of geometry. Recent research in cognitive science suggests neural networks do not share this capacity, concluding that human geometric abilities come from discrete symbolic structure in human mental representations. However, progress in artificial intelligence (AI) suggests that neural networks begin to demonstrate more human-like reasoning after scaling up standard architectures in both model size and amount of training data. In this study, we revisit empirical results in cognitive science on geometric visual processing and identify three key biases in geometric visual processing: a sensitivity towards complexity, regularity, and the perception of parts and relations. We test tasks from the literature that probe these biases in humans and find that large pre-trained neural network models used in AI demonstrate more human-like abstract geometric processing.","sentences":["Humans possess a remarkable capacity to recognize and manipulate abstract structure, which is especially apparent in the domain of geometry.","Recent research in cognitive science suggests neural networks do not share this capacity, concluding that human geometric abilities come from discrete symbolic structure in human mental representations.","However, progress in artificial intelligence (AI) suggests that neural networks begin to demonstrate more human-like reasoning after scaling up standard architectures in both model size and amount of training data.","In this study, we revisit empirical results in cognitive science on geometric visual processing and identify three key biases in geometric visual processing: a sensitivity towards complexity, regularity, and the perception of parts and relations.","We test tasks from the literature that probe these biases in humans and find that large pre-trained neural network models used in AI demonstrate more human-like abstract geometric processing."],"url":"http://arxiv.org/abs/2402.04203v1","category":"cs.AI"}
{"created":"2024-02-06 17:45:16","title":"Tropical Geometry of Rado Matroids","abstract":"In this note, we characterize the products of simplicial generators for the Chow ring of a loopless matroid, extending a result of Backman, Eur, and Simpson. We prove that the stable intersection of a collection of tropical hyperplanes centered at the origin with the Bergman fan of a matroid is the Bergman fan of the dual of a certain Rado matroid.","sentences":["In this note, we characterize the products of simplicial generators for the Chow ring of a loopless matroid, extending a result of Backman, Eur, and Simpson.","We prove that the stable intersection of a collection of tropical hyperplanes centered at the origin with the Bergman fan of a matroid is the Bergman fan of the dual of a certain Rado matroid."],"url":"http://arxiv.org/abs/2402.04186v1","category":"math.CO"}
{"created":"2024-02-06 17:43:27","title":"Incivility in Open Source Projects: A Comprehensive Annotated Dataset of Locked GitHub Issue Threads","abstract":"In the dynamic landscape of open source software (OSS) development, understanding and addressing incivility within issue discussions is crucial for fostering healthy and productive collaborations. This paper presents a curated dataset of 404 locked GitHub issue discussion threads and 5961 individual comments, collected from 213 OSS projects. We annotated the comments with various categories of incivility using Tone Bearing Discussion Features (TBDFs), and, for each issue thread, we annotated the triggers, targets, and consequences of incivility. We observed that Bitter frustration, Impatience, and Mocking are the most prevalent TBDFs exhibited in our dataset. The most common triggers, targets, and consequences of incivility include Failed use of tool/code or error messages, People, and Discontinued further discussion, respectively. This dataset can serve as a valuable resource for analyzing incivility in OSS and improving automated tools to detect and mitigate such behavior.","sentences":["In the dynamic landscape of open source software (OSS) development, understanding and addressing incivility within issue discussions is crucial for fostering healthy and productive collaborations.","This paper presents a curated dataset of 404 locked GitHub issue discussion threads and 5961 individual comments, collected from 213 OSS projects.","We annotated the comments with various categories of incivility using Tone Bearing Discussion Features (TBDFs), and, for each issue thread, we annotated the triggers, targets, and consequences of incivility.","We observed that Bitter frustration, Impatience, and Mocking are the most prevalent TBDFs exhibited in our dataset.","The most common triggers, targets, and consequences of incivility include Failed use of tool/code or error messages, People, and Discontinued further discussion, respectively.","This dataset can serve as a valuable resource for analyzing incivility in OSS and improving automated tools to detect and mitigate such behavior."],"url":"http://arxiv.org/abs/2402.04183v1","category":"cs.SE"}
{"created":"2024-02-06 17:27:12","title":"COPS: A Compact On-device Pipeline for real-time Smishing detection","abstract":"Smartphones have become indispensable in our daily lives and can do almost everything, from communication to online shopping. However, with the increased usage, cybercrime aimed at mobile devices is rocketing. Smishing attacks, in particular, have observed a significant upsurge in recent years. This problem is further exacerbated by the perpetrator creating new deceptive websites daily, with an average life cycle of under 15 hours. This renders the standard practice of keeping a database of malicious URLs ineffective. To this end, we propose a novel on-device pipeline: COPS that intelligently identifies features of fraudulent messages and URLs to alert the user in real-time. COPS is a lightweight pipeline with a detection module based on the Disentangled Variational Autoencoder of size 3.46MB for smishing and URL phishing detection, and we benchmark it on open datasets. We achieve an accuracy of 98.15% and 99.5%, respectively, for both tasks, with a false negative and false positive rate of a mere 0.037 and 0.015, outperforming previous works with the added advantage of ensuring real-time alerts on resource-constrained devices.","sentences":["Smartphones have become indispensable in our daily lives and can do almost everything, from communication to online shopping.","However, with the increased usage, cybercrime aimed at mobile devices is rocketing.","Smishing attacks, in particular, have observed a significant upsurge in recent years.","This problem is further exacerbated by the perpetrator creating new deceptive websites daily, with an average life cycle of under 15 hours.","This renders the standard practice of keeping a database of malicious URLs ineffective.","To this end, we propose a novel on-device pipeline: COPS that intelligently identifies features of fraudulent messages and URLs to alert the user in real-time.","COPS is a lightweight pipeline with a detection module based on the Disentangled Variational Autoencoder of size 3.46MB for smishing and URL phishing detection, and we benchmark it on open datasets.","We achieve an accuracy of 98.15% and 99.5%, respectively, for both tasks, with a false negative and false positive rate of a mere 0.037 and 0.015, outperforming previous works with the added advantage of ensuring real-time alerts on resource-constrained devices."],"url":"http://arxiv.org/abs/2402.04173v1","category":"cs.CR"}
{"created":"2024-02-06 17:22:45","title":"Mind the Gap: Securely modeling cyber risk based on security deviations from a peer group","abstract":"There are two strategic and longstanding questions about cyber risk that organizations largely have been unable to answer: What is an organization's estimated risk exposure and how does its security compare with peers? Answering both requires industry-wide data on security posture, incidents, and losses that, until recently, have been too sensitive for organizations to share. Now, privacy enhancing technologies (PETs) such as cryptographic computing can enable the secure computation of aggregate cyber risk metrics from a peer group of organizations while leaving sensitive input data undisclosed. As these new aggregate data become available, analysts need ways to integrate them into cyber risk models that can produce more reliable risk assessments and allow comparison to a peer group. This paper proposes a new framework for benchmarking cyber posture against peers and estimating cyber risk within specific economic sectors using the new variables emerging from secure computations. We introduce a new top-line variable called the Defense Gap Index representing the weighted security gap between an organization and its peers that can be used to forecast an organization's own security risk based on historical industry data. We apply this approach in a specific sector using data collected from 25 large firms, in partnership with an industry ISAO, to build an industry risk model and provide tools back to participants to estimate their own risk exposure and privately compare their security posture with their peers.","sentences":["There are two strategic and longstanding questions about cyber risk that organizations largely have been unable to answer: What is an organization's estimated risk exposure and how does its security compare with peers?","Answering both requires industry-wide data on security posture, incidents, and losses that, until recently, have been too sensitive for organizations to share.","Now, privacy enhancing technologies (PETs) such as cryptographic computing can enable the secure computation of aggregate cyber risk metrics from a peer group of organizations while leaving sensitive input data undisclosed.","As these new aggregate data become available, analysts need ways to integrate them into cyber risk models that can produce more reliable risk assessments and allow comparison to a peer group.","This paper proposes a new framework for benchmarking cyber posture against peers and estimating cyber risk within specific economic sectors using the new variables emerging from secure computations.","We introduce a new top-line variable called the Defense Gap Index representing the weighted security gap between an organization and its peers that can be used to forecast an organization's own security risk based on historical industry data.","We apply this approach in a specific sector using data collected from 25 large firms, in partnership with an industry ISAO, to build an industry risk model and provide tools back to participants to estimate their own risk exposure and privately compare their security posture with their peers."],"url":"http://arxiv.org/abs/2402.04166v1","category":"cs.CR"}
{"created":"2024-02-06 17:09:25","title":"Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction","abstract":"Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a \"read-to-play\" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set of multimodal game instructions to incorporate instruction tuning into a decision transformer.Experimental results demonstrate that incorporating multimodal game instructions significantly enhances the decision transformer's multitasking and generalization capabilities.","sentences":["Developing a generalist agent is a longstanding objective in artificial intelligence.","Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.","However, these works encounter challenges in extending their capabilities to new tasks.","Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.","However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.","This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a \"read-to-play\" capability.","Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set of multimodal game instructions to incorporate instruction tuning into a decision transformer.","Experimental results demonstrate that incorporating multimodal game instructions significantly enhances the decision transformer's multitasking and generalization capabilities."],"url":"http://arxiv.org/abs/2402.04154v1","category":"cs.AI"}
{"created":"2024-02-06 16:54:59","title":"Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian Process","abstract":"With the advent of artificial intelligence (AI) and machine learning (ML), various domains of science and engineering communites has leveraged data-driven surrogates to model complex systems from numerous sources of information (data). The proliferation has led to significant reduction in cost and time involved in development of superior systems designed to perform specific functionalities. A high proposition of such surrogates are built extensively fusing multiple sources of data, may it be published papers, patents, open repositories, or other resources. However, not much attention has been paid to the differences in quality and comprehensiveness of the known and unknown underlying physical parameters of the information sources that could have downstream implications during system optimization. Towards resolving this issue, a multi-source data fusion framework based on Latent Variable Gaussian Process (LVGP) is proposed. The individual data sources are tagged as a characteristic categorical variable that are mapped into a physically interpretable latent space, allowing the development of source-aware data fusion modeling. Additionally, a dissimilarity metric based on the latent variables of LVGP is introduced to study and understand the differences in the sources of data. The proposed approach is demonstrated on and analyzed through two mathematical (representative parabola problem, 2D Ackley function) and two materials science (design of FeCrAl and SmCoFe alloys) case studies. From the case studies, it is observed that compared to using single-source and source unaware ML models, the proposed multi-source data fusion framework can provide better predictions for sparse-data problems, interpretability regarding the sources, and enhanced modeling capabilities by taking advantage of the correlations and relationships among different sources.","sentences":["With the advent of artificial intelligence (AI) and machine learning (ML), various domains of science and engineering communites has leveraged data-driven surrogates to model complex systems from numerous sources of information (data).","The proliferation has led to significant reduction in cost and time involved in development of superior systems designed to perform specific functionalities.","A high proposition of such surrogates are built extensively fusing multiple sources of data, may it be published papers, patents, open repositories, or other resources.","However, not much attention has been paid to the differences in quality and comprehensiveness of the known and unknown underlying physical parameters of the information sources that could have downstream implications during system optimization.","Towards resolving this issue, a multi-source data fusion framework based on Latent Variable Gaussian Process (LVGP) is proposed.","The individual data sources are tagged as a characteristic categorical variable that are mapped into a physically interpretable latent space, allowing the development of source-aware data fusion modeling.","Additionally, a dissimilarity metric based on the latent variables of LVGP is introduced to study and understand the differences in the sources of data.","The proposed approach is demonstrated on and analyzed through two mathematical (representative parabola problem, 2D Ackley function) and two materials science (design of FeCrAl and SmCoFe alloys) case studies.","From the case studies, it is observed that compared to using single-source and source unaware ML models, the proposed multi-source data fusion framework can provide better predictions for sparse-data problems, interpretability regarding the sources, and enhanced modeling capabilities by taking advantage of the correlations and relationships among different sources."],"url":"http://arxiv.org/abs/2402.04146v1","category":"stat.ML"}
{"created":"2024-02-06 16:48:50","title":"Multi-line AI-assisted Code Authoring","abstract":"CodeCompose is an AI-assisted code authoring tool powered by large language models (LLMs) that provides inline suggestions to 10's of thousands of developers at Meta. In this paper, we present how we scaled the product from displaying single-line suggestions to multi-line suggestions. This evolution required us to overcome several unique challenges in improving the usability of these suggestions for developers.   First, we discuss how multi-line suggestions can have a 'jarring' effect, as the LLM's suggestions constantly move around the developer's existing code, which would otherwise result in decreased productivity and satisfaction.   Second, multi-line suggestions take significantly longer to generate; hence we present several innovative investments we made to reduce the perceived latency for users. These model-hosting optimizations sped up multi-line suggestion latency by 2.5x.   Finally, we conduct experiments on 10's of thousands of engineers to understand how multi-line suggestions impact the user experience and contrast this with single-line suggestions. Our experiments reveal that (i) multi-line suggestions account for 42% of total characters accepted (despite only accounting for 16% for displayed suggestions) (ii) multi-line suggestions almost doubled the percentage of keystrokes saved for users from 9% to 17%. Multi-line CodeCompose has been rolled out to all engineers at Meta, and less than 1% of engineers have opted out of multi-line suggestions.","sentences":["CodeCompose is an AI-assisted code authoring tool powered by large language models (LLMs) that provides inline suggestions to 10's of thousands of developers at Meta.","In this paper, we present how we scaled the product from displaying single-line suggestions to multi-line suggestions.","This evolution required us to overcome several unique challenges in improving the usability of these suggestions for developers.   ","First, we discuss how multi-line suggestions can have a 'jarring' effect, as the LLM's suggestions constantly move around the developer's existing code, which would otherwise result in decreased productivity and satisfaction.   ","Second, multi-line suggestions take significantly longer to generate; hence we present several innovative investments we made to reduce the perceived latency for users.","These model-hosting optimizations sped up multi-line suggestion latency by 2.5x.   ","Finally, we conduct experiments on 10's of thousands of engineers to understand how multi-line suggestions impact the user experience and contrast this with single-line suggestions.","Our experiments reveal that (i) multi-line suggestions account for 42% of total characters accepted (despite only accounting for 16% for displayed suggestions) (ii) multi-line suggestions almost doubled the percentage of keystrokes saved for users from 9% to 17%.","Multi-line CodeCompose has been rolled out to all engineers at Meta, and less than 1% of engineers have opted out of multi-line suggestions."],"url":"http://arxiv.org/abs/2402.04141v1","category":"cs.SE"}
{"created":"2024-02-06 16:47:34","title":"Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs)","abstract":"This study consists of a novel approach toward the analysis of court judgments spanning five countries, including the United States, the United Kingdom, Rwanda, Sweden and Hong Kong. This study also explores the intersection of the latest advancements in artificial intelligence (AI) and legal analysis, emphasizing the role of AI (specifically generative AI) in identifying human biases and facilitating automated, valid, and coherent multisided argumentation of court judgments with the goal of ensuring consistent application of laws in and across various jurisdictions. By incorporating Advanced Language Models (ALMs) and a newly introduced human-AI collaborative framework, this paper seeks to analyze Grounded Theory-based research design with Advanced Language Models (ALMs) in the practice of law. SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT technology), focusing on detecting logical inconsistencies and biases across various legal decisions. SHIRLEY analysis is aggregated and is accompanied by a comparison-oriented AI-based application called SAM (also an ALM) to identify relative deviations in SHIRLEY bias detections. Further, a CRITIC is generated within semi-autonomous arbitration process via the ALM, SARA. A novel approach is introduced in the utilization of an AI arbitrator to critically evaluate biases and qualitative-in-nature nuances identified by the aforementioned AI applications (SAM in concert with SHIRLEY), based on the Hague Rules on Business and Human Rights Arbitration. This Semi-Automated Arbitration Process (SAAP) aims to uphold the integrity and fairness of legal judgments by ensuring a nuanced debate-resultant \"understanding\" through a hybrid system of AI and human-based collaborative analysis.","sentences":["This study consists of a novel approach toward the analysis of court judgments spanning five countries, including the United States, the United Kingdom, Rwanda, Sweden and Hong Kong.","This study also explores the intersection of the latest advancements in artificial intelligence (AI) and legal analysis, emphasizing the role of AI (specifically generative AI) in identifying human biases and facilitating automated, valid, and coherent multisided argumentation of court judgments with the goal of ensuring consistent application of laws in and across various jurisdictions.","By incorporating Advanced Language Models (ALMs) and a newly introduced human-AI collaborative framework, this paper seeks to analyze Grounded Theory-based research design with Advanced Language Models (ALMs) in the practice of law.","SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT technology), focusing on detecting logical inconsistencies and biases across various legal decisions.","SHIRLEY analysis is aggregated and is accompanied by a comparison-oriented AI-based application called SAM (also an ALM) to identify relative deviations in SHIRLEY bias detections.","Further, a CRITIC is generated within semi-autonomous arbitration process via the ALM, SARA.","A novel approach is introduced in the utilization of an AI arbitrator to critically evaluate biases and qualitative-in-nature nuances identified by the aforementioned AI applications (SAM in concert with SHIRLEY), based on the Hague Rules on Business and Human Rights Arbitration.","This Semi-Automated Arbitration Process (SAAP) aims to uphold the integrity and fairness of legal judgments by ensuring a nuanced debate-resultant \"understanding\" through a hybrid system of AI and human-based collaborative analysis."],"url":"http://arxiv.org/abs/2402.04140v2","category":"cs.AI"}
{"created":"2024-02-06 16:02:17","title":"Hierarchical Delay Attribution Classification using Unstructured Text in Train Management Systems","abstract":"EU directives stipulate a systematic follow-up of train delays. In Sweden, the Swedish Transport Administration registers and assigns an appropriate delay attribution code. However, this delay attribution code is assigned manually, which is a complex task. In this paper, a machine learning-based decision support for assigning delay attribution codes based on event descriptions is investigated. The text is transformed using TF-IDF, and two models, Random Forest and Support Vector Machine, are evaluated against a random uniform classifier and the classification performance of the Swedish Transport Administration. Further, the problem is modeled as both a hierarchical and flat approach. The results indicate that a hierarchical approach performs better than a flat approach. Both approaches perform better than the random uniform classifier but perform worse than the manual classification.","sentences":["EU directives stipulate a systematic follow-up of train delays.","In Sweden, the Swedish Transport Administration registers and assigns an appropriate delay attribution code.","However, this delay attribution code is assigned manually, which is a complex task.","In this paper, a machine learning-based decision support for assigning delay attribution codes based on event descriptions is investigated.","The text is transformed using TF-IDF, and two models, Random Forest and Support Vector Machine, are evaluated against a random uniform classifier and the classification performance of the Swedish Transport Administration.","Further, the problem is modeled as both a hierarchical and flat approach.","The results indicate that a hierarchical approach performs better than a flat approach.","Both approaches perform better than the random uniform classifier but perform worse than the manual classification."],"url":"http://arxiv.org/abs/2402.04108v1","category":"cs.LG"}
{"created":"2024-02-06 15:58:14","title":"An Exploration of Clustering Algorithms for Customer Segmentation in the UK Retail Market","abstract":"Recently, peoples awareness of online purchases has significantly risen. This has given rise to online retail platforms and the need for a better understanding of customer purchasing behaviour. Retail companies are pressed with the need to deal with a high volume of customer purchases, which requires sophisticated approaches to perform more accurate and efficient customer segmentation. Customer segmentation is a marketing analytical tool that aids customer-centric service and thus enhances profitability. In this paper, we aim to develop a customer segmentation model to improve decision-making processes in the retail market industry. To achieve this, we employed a UK-based online retail dataset obtained from the UCI machine learning repository. The retail dataset consists of 541,909 customer records and eight features. Our study adopted the RFM (recency, frequency, and monetary) framework to quantify customer values. Thereafter, we compared several state-of-the-art (SOTA) clustering algorithms, namely, K-means clustering, the Gaussian mixture model (GMM), density-based spatial clustering of applications with noise (DBSCAN), agglomerative clustering, and balanced iterative reducing and clustering using hierarchies (BIRCH). The results showed the GMM outperformed other approaches, with a Silhouette Score of 0.80.","sentences":["Recently, peoples awareness of online purchases has significantly risen.","This has given rise to online retail platforms and the need for a better understanding of customer purchasing behaviour.","Retail companies are pressed with the need to deal with a high volume of customer purchases, which requires sophisticated approaches to perform more accurate and efficient customer segmentation.","Customer segmentation is a marketing analytical tool that aids customer-centric service and thus enhances profitability.","In this paper, we aim to develop a customer segmentation model to improve decision-making processes in the retail market industry.","To achieve this, we employed a UK-based online retail dataset obtained from the UCI machine learning repository.","The retail dataset consists of 541,909 customer records and eight features.","Our study adopted the RFM (recency, frequency, and monetary) framework to quantify customer values.","Thereafter, we compared several state-of-the-art (SOTA) clustering algorithms, namely, K-means clustering, the Gaussian mixture model (GMM), density-based spatial clustering of applications with noise (DBSCAN), agglomerative clustering, and balanced iterative reducing and clustering using hierarchies (BIRCH).","The results showed the GMM outperformed other approaches, with a Silhouette Score of 0.80."],"url":"http://arxiv.org/abs/2402.04103v1","category":"cs.LG"}
{"created":"2024-02-06 15:57:08","title":"Use of Multi-CNNs for Section Analysis in Static Malware Detection","abstract":"Existing research on malware detection focuses almost exclusively on the detection rate. However, in some cases, it is also important to understand the results of our algorithm, or to obtain more information, such as where to investigate in the file for an analyst. In this aim, we propose a new model to analyze Portable Executable files. Our method consists in splitting the files in different sections, then transform each section into an image, in order to train convolutional neural networks to treat specifically each identified section. Then we use all these scores returned by CNNs to compute a final detection score, using models that enable us to improve our analysis of the importance of each section in the final score.","sentences":["Existing research on malware detection focuses almost exclusively on the detection rate.","However, in some cases, it is also important to understand the results of our algorithm, or to obtain more information, such as where to investigate in the file for an analyst.","In this aim, we propose a new model to analyze Portable Executable files.","Our method consists in splitting the files in different sections, then transform each section into an image, in order to train convolutional neural networks to treat specifically each identified section.","Then we use all these scores returned by CNNs to compute a final detection score, using models that enable us to improve our analysis of the importance of each section in the final score."],"url":"http://arxiv.org/abs/2402.04102v1","category":"cs.CR"}
{"created":"2024-02-06 15:46:31","title":"The Use of a Large Language Model for Cyberbullying Detection","abstract":"The dominance of social media has added to the channels of bullying for perpetrators. Unfortunately, cyberbullying (CB) is the most prevalent phenomenon in todays cyber world, and is a severe threat to the mental and physical health of citizens. This opens the need to develop a robust system to prevent bullying content from online forums, blogs, and social media platforms to manage the impact in our society. Several machine learning (ML) algorithms have been proposed for this purpose. However, their performances are not consistent due to high class imbalance and generalisation issues. In recent years, large language models (LLMs) like BERT and RoBERTa have achieved state-of-the-art (SOTA) results in several natural language processing (NLP) tasks. Unfortunately, the LLMs have not been applied extensively for CB detection. In our paper, we explored the use of these models for cyberbullying (CB) detection. We have prepared a new dataset (D2) from existing studies (Formspring and Twitter). Our experimental results for dataset D1 and D2 showed that RoBERTa outperformed other models.","sentences":["The dominance of social media has added to the channels of bullying for perpetrators.","Unfortunately, cyberbullying (CB) is the most prevalent phenomenon in todays cyber world, and is a severe threat to the mental and physical health of citizens.","This opens the need to develop a robust system to prevent bullying content from online forums, blogs, and social media platforms to manage the impact in our society.","Several machine learning (ML) algorithms have been proposed for this purpose.","However, their performances are not consistent due to high class imbalance and generalisation issues.","In recent years, large language models (LLMs) like BERT and RoBERTa have achieved state-of-the-art (SOTA) results in several natural language processing (NLP) tasks.","Unfortunately, the LLMs have not been applied extensively for CB detection.","In our paper, we explored the use of these models for cyberbullying (CB) detection.","We have prepared a new dataset (D2) from existing studies (Formspring and Twitter).","Our experimental results for dataset D1 and D2 showed that RoBERTa outperformed other models."],"url":"http://arxiv.org/abs/2402.04088v1","category":"cs.CL"}
{"created":"2024-02-06 15:45:27","title":"A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation","abstract":"Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP. Extensive results on 17 datasets validate that our method surpasses or achieves comparable results with state-of-the-art methods on few-shot classification, imbalanced learning, and out-of-distribution generalization. In addition, we extend our method to base-to-new generalization and unsupervised learning, once again demonstrating its superiority over competing approaches. Our code is publicly available at \\url{https://github.com/mrflogs/ICLR24}.","sentences":["Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity.","Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks.","However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources.","In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP.","Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance.","By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training.","To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP.","Extensive results on 17 datasets validate that our method surpasses or achieves comparable results with state-of-the-art methods on few-shot classification, imbalanced learning, and out-of-distribution generalization.","In addition, we extend our method to base-to-new generalization and unsupervised learning, once again demonstrating its superiority over competing approaches.","Our code is publicly available at \\url{https://github.com/mrflogs/ICLR24}."],"url":"http://arxiv.org/abs/2402.04087v1","category":"cs.CV"}
{"created":"2024-02-06 15:36:06","title":"An Optimal House Price Prediction Algorithm: XGBoost","abstract":"An accurate prediction of house prices is a fundamental requirement for various sectors including real estate and mortgage lending. It is widely recognized that a property value is not solely determined by its physical attributes but is significantly influenced by its surrounding neighbourhood. Meeting the diverse housing needs of individuals while balancing budget constraints is a primary concern for real estate developers. To this end, we addressed the house price prediction problem as a regression task and thus employed various machine learning techniques capable of expressing the significance of independent variables. We made use of the housing dataset of Ames City in Iowa, USA to compare support vector regressor, random forest regressor, XGBoost, multilayer perceptron and multiple linear regression algorithms for house price prediction. Afterwards, we identified the key factors that influence housing costs. Our results show that XGBoost is the best performing model for house price prediction.","sentences":["An accurate prediction of house prices is a fundamental requirement for various sectors including real estate and mortgage lending.","It is widely recognized that a property value is not solely determined by its physical attributes but is significantly influenced by its surrounding neighbourhood.","Meeting the diverse housing needs of individuals while balancing budget constraints is a primary concern for real estate developers.","To this end, we addressed the house price prediction problem as a regression task and thus employed various machine learning techniques capable of expressing the significance of independent variables.","We made use of the housing dataset of Ames City in Iowa, USA to compare support vector regressor, random forest regressor, XGBoost, multilayer perceptron and multiple linear regression algorithms for house price prediction.","Afterwards, we identified the key factors that influence housing costs.","Our results show that XGBoost is the best performing model for house price prediction."],"url":"http://arxiv.org/abs/2402.04082v1","category":"cs.LG"}
{"created":"2024-02-06 15:34:44","title":"Improved Generalization of Weight Space Networks via Augmentations","abstract":"Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks. Unfortunately, weight space models tend to suffer from substantial overfitting. We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets. While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object. To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces. We demonstrate the effectiveness of these methods in two setups. In classification, they improve performance similarly to having up to 10 times more data. In self-supervised contrastive learning, they yield substantial 5-10% gains in downstream classification.","sentences":["Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks.","Unfortunately, weight space models tend to suffer from substantial overfitting.","We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets.","While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object.","To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces.","We demonstrate the effectiveness of these methods in two setups.","In classification, they improve performance similarly to having up to 10 times more data.","In self-supervised contrastive learning, they yield substantial 5-10% gains in downstream classification."],"url":"http://arxiv.org/abs/2402.04081v1","category":"cs.LG"}
{"created":"2024-02-06 15:16:27","title":"Hermitian stochastic methodology for X-ray superfluorescence","abstract":"A recently introduced theoretical framework for modeling the dynamics of X-ray amplified spontaneous emission is based on stochastic sampling of the density matrix of quantum emitters and the radiation field, similarly to other phase-space sampling techniques. While based on first principles and providing valuable theoretical insights, the original stochastic differential equations exhibit divergences and numerical instabilities. Here, we resolve this issue by accounting the stochastic components perturbatively. The refined formalism accurately reproduces the properties of spontaneous emission and proves universally applicable for describing all stages of collective X-ray emission in paraxial geometry, including spontaneous emission, amplified spontaneous emission, and the non-linear regime. Through numerical examples, we analyze key features of superfluorescence in one-dimensional approximation. Importantly, single realizations of the underlying stochastic equations can be fully interpreted as individual experimental observations of superfluorescence.","sentences":["A recently introduced theoretical framework for modeling the dynamics of X-ray amplified spontaneous emission is based on stochastic sampling of the density matrix of quantum emitters and the radiation field, similarly to other phase-space sampling techniques.","While based on first principles and providing valuable theoretical insights, the original stochastic differential equations exhibit divergences and numerical instabilities.","Here, we resolve this issue by accounting the stochastic components perturbatively.","The refined formalism accurately reproduces the properties of spontaneous emission and proves universally applicable for describing all stages of collective X-ray emission in paraxial geometry, including spontaneous emission, amplified spontaneous emission, and the non-linear regime.","Through numerical examples, we analyze key features of superfluorescence in one-dimensional approximation.","Importantly, single realizations of the underlying stochastic equations can be fully interpreted as individual experimental observations of superfluorescence."],"url":"http://arxiv.org/abs/2402.04069v1","category":"physics.optics"}
{"created":"2024-02-06 15:09:50","title":"Multi-class Road Defect Detection and Segmentation using Spatial and Channel-wise Attention for Autonomous Road Repairing","abstract":"Road pavement detection and segmentation are critical for developing autonomous road repair systems. However, developing an instance segmentation method that simultaneously performs multi-class defect detection and segmentation is challenging due to the textural simplicity of road pavement image, the diversity of defect geometries, and the morphological ambiguity between classes. We propose a novel end-to-end method for multi-class road defect detection and segmentation. The proposed method comprises multiple spatial and channel-wise attention blocks available to learn global representations across spatial and channel-wise dimensions. Through these attention blocks, more globally generalised representations of morphological information (spatial characteristics) of road defects and colour and depth information of images can be learned. To demonstrate the effectiveness of our framework, we conducted various ablation studies and comparisons with prior methods on a newly collected dataset annotated with nine road defect classes. The experiments show that our proposed method outperforms existing state-of-the-art methods for multi-class road defect detection and segmentation methods.","sentences":["Road pavement detection and segmentation are critical for developing autonomous road repair systems.","However, developing an instance segmentation method that simultaneously performs multi-class defect detection and segmentation is challenging due to the textural simplicity of road pavement image, the diversity of defect geometries, and the morphological ambiguity between classes.","We propose a novel end-to-end method for multi-class road defect detection and segmentation.","The proposed method comprises multiple spatial and channel-wise attention blocks available to learn global representations across spatial and channel-wise dimensions.","Through these attention blocks, more globally generalised representations of morphological information (spatial characteristics) of road defects and colour and depth information of images can be learned.","To demonstrate the effectiveness of our framework, we conducted various ablation studies and comparisons with prior methods on a newly collected dataset annotated with nine road defect classes.","The experiments show that our proposed method outperforms existing state-of-the-art methods for multi-class road defect detection and segmentation methods."],"url":"http://arxiv.org/abs/2402.04064v1","category":"cs.CV"}
{"created":"2024-02-06 15:05:40","title":"Link Prediction with Relational Hypergraphs","abstract":"Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications. Nonetheless, it remains challenging to transfer the success of these architectures to link prediction with relational hypergraphs. The presence of relational hyperedges makes link prediction a task between $k$ nodes for varying choices of $k$, which is substantially harder than link prediction with knowledge graphs, where every relation is binary ($k=2$). In this paper, we propose two frameworks for link prediction with relational hypergraphs and conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms, and also via some natural logical formalisms. Through extensive empirical analysis, we validate the power of the proposed model architectures on various relational hypergraph benchmarks. The resulting model architectures substantially outperform every baseline for inductive link prediction, and lead to state-of-the-art results for transductive link prediction. Our study therefore unlocks applications of graph neural networks to fully relational structures.","sentences":["Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications.","Nonetheless, it remains challenging to transfer the success of these architectures to link prediction with relational hypergraphs.","The presence of relational hyperedges makes link prediction a task between $k$ nodes for varying choices of $k$, which is substantially harder than link prediction with knowledge graphs, where every relation is binary ($k=2$).","In this paper, we propose two frameworks for link prediction with relational hypergraphs and conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms, and also via some natural logical formalisms.","Through extensive empirical analysis, we validate the power of the proposed model architectures on various relational hypergraph benchmarks.","The resulting model architectures substantially outperform every baseline for inductive link prediction, and lead to state-of-the-art results for transductive link prediction.","Our study therefore unlocks applications of graph neural networks to fully relational structures."],"url":"http://arxiv.org/abs/2402.04062v1","category":"cs.LG"}
{"created":"2024-02-06 15:03:53","title":"Deep Learning for Multivariate Time Series Imputation: A Survey","abstract":"The ubiquitous missing values cause the multivariate time series data to be partially observed, destroying the integrity of time series and hindering the effective time series data analysis. Recently deep learning imputation methods have demonstrated remarkable success in elevating the quality of corrupted time series data, subsequently enhancing performance in downstream tasks. In this paper, we conduct a comprehensive survey on the recently proposed deep learning imputation methods. First, we propose a taxonomy for the reviewed methods, and then provide a structured review of these methods by highlighting their strengths and limitations. We also conduct empirical experiments to study different methods and compare their enhancement for downstream tasks. Finally, the open issues for future research on multivariate time series imputation are pointed out. All code and configurations of this work, including a regularly maintained multivariate time series imputation paper list, can be found in the GitHub repository~\\url{https://github.com/WenjieDu/Awesome\\_Imputation}.","sentences":["The ubiquitous missing values cause the multivariate time series data to be partially observed, destroying the integrity of time series and hindering the effective time series data analysis.","Recently deep learning imputation methods have demonstrated remarkable success in elevating the quality of corrupted time series data, subsequently enhancing performance in downstream tasks.","In this paper, we conduct a comprehensive survey on the recently proposed deep learning imputation methods.","First, we propose a taxonomy for the reviewed methods, and then provide a structured review of these methods by highlighting their strengths and limitations.","We also conduct empirical experiments to study different methods and compare their enhancement for downstream tasks.","Finally, the open issues for future research on multivariate time series imputation are pointed out.","All code and configurations of this work, including a regularly maintained multivariate time series imputation paper list, can be found in the GitHub repository~\\url{https://github.com/WenjieDu/Awesome\\_Imputation}."],"url":"http://arxiv.org/abs/2402.04059v1","category":"cs.LG"}
{"created":"2024-02-06 14:53:19","title":"Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models","abstract":"With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \\textbf{C}ollabo\\textbf{ra}tive \\textbf{F}ine-\\textbf{T}uning (\\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algorithm. Extensive experiments on few-shot classification over 15 datasets demonstrate the superiority of CraFT. The results show that CraFT achieves a decent gain of about 12\\% with 16-shot datasets and only 8,000 queries. Moreover, CraFT trains faster and uses only about 1/80 of the memory footprint for deployment, while sacrificing only 1.62\\% compared to the white-box method.","sentences":["With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks.","Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership.","This paper proposes a \\textbf{C}ollabo\\textbf{ra}tive \\textbf{F}ine-\\textbf{T}uning (\\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model.","CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style.","Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules.","These modules are optimized by a novel collaborative training algorithm.","Extensive experiments on few-shot classification over 15 datasets demonstrate the superiority of CraFT.","The results show that CraFT achieves a decent gain of about 12\\% with 16-shot datasets and only 8,000 queries.","Moreover, CraFT trains faster and uses only about 1/80 of the memory footprint for deployment, while sacrificing only 1.62\\% compared to the white-box method."],"url":"http://arxiv.org/abs/2402.04050v1","category":"cs.LG"}
{"created":"2024-02-06 14:51:55","title":"Systematic Biases in LLM Simulations of Debates","abstract":"Recent advancements in natural language processing, especially the emergence of Large Language Models (LLMs), have opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations.","sentences":["Recent advancements in natural language processing, especially the emergence of Large Language Models (LLMs), have opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately.","However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors.","In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates.","Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives.","This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans.","We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases.","These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations."],"url":"http://arxiv.org/abs/2402.04049v1","category":"cs.CL"}
{"created":"2024-02-06 14:48:34","title":"Generative Modeling of Graphs via Joint Diffusion of Node and Edge Attributes","abstract":"Graph generation is integral to various engineering and scientific disciplines. Nevertheless, existing methodologies tend to overlook the generation of edge attributes. However, we identify critical applications where edge attributes are essential, making prior methods potentially unsuitable in such contexts. Moreover, while trivial adaptations are available, empirical investigations reveal their limited efficacy as they do not properly model the interplay among graph components. To address this, we propose a joint score-based model of nodes and edges for graph generation that considers all graph components. Our approach offers two key novelties: (i) node and edge attributes are combined in an attention module that generates samples based on the two ingredients; and (ii) node, edge and adjacency information are mutually dependent during the graph diffusion process. We evaluate our method on challenging benchmarks involving real-world and synthetic datasets in which edge features are crucial. Additionally, we introduce a new synthetic dataset that incorporates edge values. Furthermore, we propose a novel application that greatly benefits from the method due to its nature: the generation of traffic scenes represented as graphs. Our method outperforms other graph generation methods, demonstrating a significant advantage in edge-related measures.","sentences":["Graph generation is integral to various engineering and scientific disciplines.","Nevertheless, existing methodologies tend to overlook the generation of edge attributes.","However, we identify critical applications where edge attributes are essential, making prior methods potentially unsuitable in such contexts.","Moreover, while trivial adaptations are available, empirical investigations reveal their limited efficacy as they do not properly model the interplay among graph components.","To address this, we propose a joint score-based model of nodes and edges for graph generation that considers all graph components.","Our approach offers two key novelties: (i) node and edge attributes are combined in an attention module that generates samples based on the two ingredients; and (ii) node, edge and adjacency information are mutually dependent during the graph diffusion process.","We evaluate our method on challenging benchmarks involving real-world and synthetic datasets in which edge features are crucial.","Additionally, we introduce a new synthetic dataset that incorporates edge values.","Furthermore, we propose a novel application that greatly benefits from the method due to its nature: the generation of traffic scenes represented as graphs.","Our method outperforms other graph generation methods, demonstrating a significant advantage in edge-related measures."],"url":"http://arxiv.org/abs/2402.04046v1","category":"cs.SI"}
{"created":"2024-02-06 14:43:31","title":"Mission Planning and Safety Assessment for Pipeline Inspection Using Autonomous Underwater Vehicles: A Framework based on Behavior Trees","abstract":"The recent advance in autonomous underwater robotics facilitates autonomous inspection tasks of offshore infrastructure. However, current inspection missions rely on predefined plans created offline, hampering the flexibility and autonomy of the inspection vehicle and the mission's success in case of unexpected events. In this work, we address these challenges by proposing a framework encompassing the modeling and verification of mission plans through Behavior Trees (BTs). This framework leverages the modularity of BTs to model onboard reactive behaviors, thus enabling autonomous plan executions, and uses BehaVerify to verify the mission's safety. Moreover, as a use case of this framework, we present a novel AI-enabled algorithm that aims for efficient, autonomous pipeline camera data collection. In a simulated environment, we demonstrate the framework's application to our proposed pipeline inspection algorithm. Our framework marks a significant step forward in the field of autonomous underwater robotics, promising to enhance the safety and success of underwater missions in practical, real-world applications. https://github.com/remaro-network/pipe_inspection_mission","sentences":["The recent advance in autonomous underwater robotics facilitates autonomous inspection tasks of offshore infrastructure.","However, current inspection missions rely on predefined plans created offline, hampering the flexibility and autonomy of the inspection vehicle and the mission's success in case of unexpected events.","In this work, we address these challenges by proposing a framework encompassing the modeling and verification of mission plans through Behavior Trees (BTs).","This framework leverages the modularity of BTs to model onboard reactive behaviors, thus enabling autonomous plan executions, and uses BehaVerify to verify the mission's safety.","Moreover, as a use case of this framework, we present a novel AI-enabled algorithm that aims for efficient, autonomous pipeline camera data collection.","In a simulated environment, we demonstrate the framework's application to our proposed pipeline inspection algorithm.","Our framework marks a significant step forward in the field of autonomous underwater robotics, promising to enhance the safety and success of underwater missions in practical, real-world applications.","https://github.com/remaro-network/pipe_inspection_mission"],"url":"http://arxiv.org/abs/2402.04045v1","category":"cs.RO"}
{"created":"2024-02-06 14:26:22","title":"HEAM : Hashed Embedding Acceleration using Processing-In-Memory","abstract":"In today's data centers, personalized recommendation systems face challenges such as the need for large memory capacity and high bandwidth, especially when performing embedding operations. Previous approaches have relied on DIMM-based near-memory processing techniques or introduced 3D-stacked DRAM to address memory-bound issues and expand memory bandwidth. However, these solutions fall short when dealing with the expanding size of personalized recommendation systems. Recommendation models have grown to sizes exceeding tens of terabytes, making them challenging to run efficiently on traditional single-node inference servers. Although various algorithmic methods have been proposed to reduce embedding table capacity, they often result in increased memory access or inefficient utilization of memory resources. This paper introduces HEAM, a heterogeneous memory architecture that integrates 3D-stacked DRAM with DIMM to accelerate recommendation systems in which compositional embedding is utilized-a technique aimed at reducing the size of embedding tables. The architecture is organized into a three-tier memory hierarchy consisting of conventional DIMM, 3D-stacked DRAM with a base die-level Processing-In-Memory (PIM), and a bank group-level PIM incorporating a Look-Up-Table. This setup is specifically designed to accommodate the unique aspects of compositional embedding, such as temporal locality and embedding table capacity. This design effectively reduces bank access, improves access efficiency, and enhances overall throughput, resulting in a 6.3 times speedup and 58.9% energy savings compared to the baseline.","sentences":["In today's data centers, personalized recommendation systems face challenges such as the need for large memory capacity and high bandwidth, especially when performing embedding operations.","Previous approaches have relied on DIMM-based near-memory processing techniques or introduced 3D-stacked DRAM to address memory-bound issues and expand memory bandwidth.","However, these solutions fall short when dealing with the expanding size of personalized recommendation systems.","Recommendation models have grown to sizes exceeding tens of terabytes, making them challenging to run efficiently on traditional single-node inference servers.","Although various algorithmic methods have been proposed to reduce embedding table capacity, they often result in increased memory access or inefficient utilization of memory resources.","This paper introduces HEAM, a heterogeneous memory architecture that integrates 3D-stacked DRAM with DIMM to accelerate recommendation systems in which compositional embedding is utilized-a technique aimed at reducing the size of embedding tables.","The architecture is organized into a three-tier memory hierarchy consisting of conventional DIMM, 3D-stacked DRAM with a base die-level Processing-In-Memory (PIM), and a bank group-level PIM incorporating a Look-Up-Table.","This setup is specifically designed to accommodate the unique aspects of compositional embedding, such as temporal locality and embedding table capacity.","This design effectively reduces bank access, improves access efficiency, and enhances overall throughput, resulting in a 6.3 times speedup and 58.9% energy savings compared to the baseline."],"url":"http://arxiv.org/abs/2402.04032v1","category":"cs.AR"}
{"created":"2024-02-06 14:24:28","title":"AlbNews: A Corpus of Headlines for Topic Modeling in Albanian","abstract":"The scarcity of available text corpora for low-resource languages like Albanian is a serious hurdle for research in natural language processing tasks. This paper introduces AlbNews, a collection of 600 topically labeled news headlines and 2600 unlabeled ones in Albanian. The data can be freely used for conducting topic modeling research. We report the initial classification scores of some traditional machine learning classifiers trained with the AlbNews samples. These results show that basic models outrun the ensemble learning ones and can serve as a baseline for future experiments.","sentences":["The scarcity of available text corpora for low-resource languages like Albanian is a serious hurdle for research in natural language processing tasks.","This paper introduces AlbNews, a collection of 600 topically labeled news headlines and 2600 unlabeled ones in Albanian.","The data can be freely used for conducting topic modeling research.","We report the initial classification scores of some traditional machine learning classifiers trained with the AlbNews samples.","These results show that basic models outrun the ensemble learning ones and can serve as a baseline for future experiments."],"url":"http://arxiv.org/abs/2402.04028v1","category":"cs.CL"}
{"created":"2024-02-06 14:03:15","title":"Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning","abstract":"In finetuning a large pretrained model to downstream tasks, parameter-efficient fine-tuning (PEFT) methods can effectively finetune pretrained models with few trainable parameters, but suffer from high GPU memory consumption and slow training speed. Because learnable parameters from these methods are entangled with the pretrained model, gradients related to the frozen pretrained model's parameters have to be computed and stored during finetuning. We propose Low-rank Attention Side-Tuning (LAST), which disentangles the trainable module from the pretrained model by freezing not only parameters but also outputs of the pretrained network. LAST trains a side-network composed of only low-rank self-attention modules. By viewing the pretrained model as a frozen feature extractor, the side-network takes intermediate output from the pretrained model and focus on learning task-specific knowledge. We also show that LAST can be highly parallel across multiple optimization objectives, making it very efficient in downstream task adaptation, for example, in finding optimal hyperparameters. LAST outperforms previous state-of-the-art methods on VTAB-1K and other visual adaptation tasks with roughly only 30\\% of GPU memory footprint and 60\\% of training time compared to existing PEFT methods, but achieves significantly higher accuracy.","sentences":["In finetuning a large pretrained model to downstream tasks, parameter-efficient fine-tuning (PEFT) methods can effectively finetune pretrained models with few trainable parameters, but suffer from high GPU memory consumption and slow training speed.","Because learnable parameters from these methods are entangled with the pretrained model, gradients related to the frozen pretrained model's parameters have to be computed and stored during finetuning.","We propose Low-rank Attention Side-Tuning (LAST), which disentangles the trainable module from the pretrained model by freezing not only parameters but also outputs of the pretrained network.","LAST trains a side-network composed of only low-rank self-attention modules.","By viewing the pretrained model as a frozen feature extractor, the side-network takes intermediate output from the pretrained model and focus on learning task-specific knowledge.","We also show that LAST can be highly parallel across multiple optimization objectives, making it very efficient in downstream task adaptation, for example, in finding optimal hyperparameters.","LAST outperforms previous state-of-the-art methods on VTAB-1K and other visual adaptation tasks with roughly only 30\\% of GPU memory footprint and 60\\% of training time compared to existing PEFT methods, but achieves significantly higher accuracy."],"url":"http://arxiv.org/abs/2402.04009v1","category":"cs.CV"}
{"created":"2024-02-06 13:31:45","title":"YOLOPoint Joint Keypoint and Object Detection","abstract":"Intelligent vehicles of the future must be capable of understanding and navigating safely through their surroundings. Camera-based vehicle systems can use keypoints as well as objects as low- and high-level landmarks for GNSS-independent SLAM and visual odometry. To this end we propose YOLOPoint, a convolutional neural network model that simultaneously detects keypoints and objects in an image by combining YOLOv5 and SuperPoint to create a single forward-pass network that is both real-time capable and accurate. By using a shared backbone and a light-weight network structure, YOLOPoint is able to perform competitively on both the HPatches and KITTI benchmarks.","sentences":["Intelligent vehicles of the future must be capable of understanding and navigating safely through their surroundings.","Camera-based vehicle systems can use keypoints as well as objects as low- and high-level landmarks for GNSS-independent SLAM and visual odometry.","To this end we propose YOLOPoint, a convolutional neural network model that simultaneously detects keypoints and objects in an image by combining YOLOv5 and SuperPoint to create a single forward-pass network that is both real-time capable and accurate.","By using a shared backbone and a light-weight network structure, YOLOPoint is able to perform competitively on both the HPatches and KITTI benchmarks."],"url":"http://arxiv.org/abs/2402.03989v1","category":"cs.CV"}
{"created":"2024-02-06 13:24:36","title":"Tail-Erasure-Correcting Codes","abstract":"The increasing demand for data storage has prompted the exploration of new techniques, with molecular data storage being a promising alternative. In this work, we develop coding schemes for a new storage paradigm that can be represented as a collection of two-dimensional arrays. Motivated by error patterns observed in recent prototype architectures, our study focuses on correcting erasures in the last few symbols of each row, and also correcting arbitrary deletions across rows. We present code constructions and explicit encoders and decoders that are shown to be nearly optimal in many scenarios. We show that the new coding schemes are capable of effectively mitigating these errors, making these emerging storage platforms potentially promising solutions.","sentences":["The increasing demand for data storage has prompted the exploration of new techniques, with molecular data storage being a promising alternative.","In this work, we develop coding schemes for a new storage paradigm that can be represented as a collection of two-dimensional arrays.","Motivated by error patterns observed in recent prototype architectures, our study focuses on correcting erasures in the last few symbols of each row, and also correcting arbitrary deletions across rows.","We present code constructions and explicit encoders and decoders that are shown to be nearly optimal in many scenarios.","We show that the new coding schemes are capable of effectively mitigating these errors, making these emerging storage platforms potentially promising solutions."],"url":"http://arxiv.org/abs/2402.03987v1","category":"cs.IT"}
{"created":"2024-02-06 13:02:00","title":"Joint Intrinsic Motivation for Coordinated Exploration in Multi-Agent Deep Reinforcement Learning","abstract":"Multi-agent deep reinforcement learning (MADRL) problems often encounter the challenge of sparse rewards. This challenge becomes even more pronounced when coordination among agents is necessary. As performance depends not only on one agent's behavior but rather on the joint behavior of multiple agents, finding an adequate solution becomes significantly harder. In this context, a group of agents can benefit from actively exploring different joint strategies in order to determine the most efficient one. In this paper, we propose an approach for rewarding strategies where agents collectively exhibit novel behaviors. We present JIM (Joint Intrinsic Motivation), a multi-agent intrinsic motivation method that follows the centralized learning with decentralized execution paradigm. JIM rewards joint trajectories based on a centralized measure of novelty designed to function in continuous environments. We demonstrate the strengths of this approach both in a synthetic environment designed to reveal shortcomings of state-of-the-art MADRL methods, and in simulated robotic tasks. Results show that joint exploration is crucial for solving tasks where the optimal strategy requires a high level of coordination.","sentences":["Multi-agent deep reinforcement learning (MADRL) problems often encounter the challenge of sparse rewards.","This challenge becomes even more pronounced when coordination among agents is necessary.","As performance depends not only on one agent's behavior but rather on the joint behavior of multiple agents, finding an adequate solution becomes significantly harder.","In this context, a group of agents can benefit from actively exploring different joint strategies in order to determine the most efficient one.","In this paper, we propose an approach for rewarding strategies where agents collectively exhibit novel behaviors.","We present JIM (Joint Intrinsic Motivation), a multi-agent intrinsic motivation method that follows the centralized learning with decentralized execution paradigm.","JIM rewards joint trajectories based on a centralized measure of novelty designed to function in continuous environments.","We demonstrate the strengths of this approach both in a synthetic environment designed to reveal shortcomings of state-of-the-art MADRL methods, and in simulated robotic tasks.","Results show that joint exploration is crucial for solving tasks where the optimal strategy requires a high level of coordination."],"url":"http://arxiv.org/abs/2402.03972v1","category":"cs.MA"}
{"created":"2024-02-06 12:59:02","title":"Tabular Data: Is Attention All You Need?","abstract":"Deep Learning has revolutionized the field of AI and led to remarkable achievements in applications involving image and text data. Unfortunately, there is inconclusive evidence on the merits of neural networks for structured tabular data. In this paper, we introduce a large-scale empirical study comparing neural networks against gradient-boosted decision trees on tabular data, but also transformer-based architectures against traditional multi-layer perceptrons (MLP) with residual connections. In contrast to prior work, our empirical findings indicate that neural networks are competitive against decision trees. Furthermore, we assess that transformer-based architectures do not outperform simpler variants of traditional MLP architectures on tabular datasets. As a result, this paper helps the research and practitioner communities make informed choices on deploying neural networks on future tabular data applications.","sentences":["Deep Learning has revolutionized the field of AI and led to remarkable achievements in applications involving image and text data.","Unfortunately, there is inconclusive evidence on the merits of neural networks for structured tabular data.","In this paper, we introduce a large-scale empirical study comparing neural networks against gradient-boosted decision trees on tabular data, but also transformer-based architectures against traditional multi-layer perceptrons (MLP) with residual connections.","In contrast to prior work, our empirical findings indicate that neural networks are competitive against decision trees.","Furthermore, we assess that transformer-based architectures do not outperform simpler variants of traditional MLP architectures on tabular datasets.","As a result, this paper helps the research and practitioner communities make informed choices on deploying neural networks on future tabular data applications."],"url":"http://arxiv.org/abs/2402.03970v1","category":"cs.LG"}
{"created":"2024-02-06 12:55:25","title":"Almost Perfect Mutually Unbiased Bases that are Sparse","abstract":"In dimension $d$, Mutually Unbiased Bases (MUBs) are a collection of orthonormal bases over $\\mathbb{C}^d$ such that for any two vectors $v_1, v_2$ belonging to different bases, the dot or scalar product $|\\braket{v_1|v_2}| = \\frac{1}{\\sqrt{d}}$. The upper bound on the number of such bases is $d+1$. Construction methods to achieve this bound are known for cases when $d$ is some power of prime. The situation is more restrictive in other cases and also when we consider the results over real rather than complex. Thus, certain relaxations of this model are considered in literature and consequently Approximate MUBs (AMUB) are studied. This enables one to construct potentially large number of such objects for $\\mathbb{C}^d$ as well as in $\\mathbb{R}^d$. In this regard, we propose the concept of Almost Perfect MUBs (APMUB), where we restrict the absolute value of inner product $|\\braket{v_1|v_2}|$ to be two-valued, one being 0 and the other $ \\leq \\frac{1+\\mathcal{O}(d^{-\\lambda})}{\\sqrt{d}}$, such that $\\lambda > 0$ and the numerator $1 + \\mathcal{O}(d^{-\\lambda}) \\leq 2$. Each such vector constructed, has an important feature that large number of its components are zero and the non-zero components are of equal magnitude. Our techniques are based on combinatorial structures related to Resolvable Block Designs (RBDs). We show that for several composite dimensions $d$, one can construct $\\mathcal{O}(\\sqrt{d})$ many APMUBs, in which cases the number of MUBs are significantly small. To be specific, this result works for $d$ of the form $(q-e)(q+f), \\ q, e, f \\in \\mathbb{N}$, with the conditions $0 \\leq f \\leq e$ for constant $e, f$ and $q$ some power of prime. We also show that such APMUBs provide sets of Bi-angular vectors which are of the order of $\\mathcal{O}(d^{3/2})$ in numbers, having high angular distances among them.","sentences":["In dimension $d$, Mutually Unbiased Bases (MUBs) are a collection of orthonormal bases over $\\mathbb{C}^d$ such that for any two vectors $v_1, v_2$ belonging to different bases, the dot or scalar product $|\\braket{v_1|v_2}| = \\frac{1}{\\sqrt{d}}$. The upper bound on the number of such bases is $d+1$. Construction methods to achieve this bound are known for cases when $d$ is some power of prime.","The situation is more restrictive in other cases and also when we consider the results over real rather than complex.","Thus, certain relaxations of this model are considered in literature and consequently Approximate MUBs (AMUB) are studied.","This enables one to construct potentially large number of such objects for $\\mathbb{C}^d$ as well as in $\\mathbb{R}^d$.","In this regard, we propose the concept of Almost Perfect MUBs (APMUB), where we restrict the absolute value of inner product $|\\braket{v_1|v_2}|$ to be two-valued, one being 0 and the other $ \\leq \\frac{1+\\mathcal{O}(d^{-\\lambda})}{\\sqrt{d}}$, such that $\\lambda > 0$ and the numerator $1 + \\mathcal{O}(d^{-\\lambda})","\\leq 2$.","Each such vector constructed, has an important feature that large number of its components are zero and the non-zero components are of equal magnitude.","Our techniques are based on combinatorial structures related to Resolvable Block Designs (RBDs).","We show that for several composite dimensions $d$, one can construct $\\mathcal{O}(\\sqrt{d})$ many APMUBs, in which cases the number of MUBs are significantly small.","To be specific, this result works for $d$ of the form $(q-e)(q+f), \\ q, e, f \\in \\mathbb{N}$, with the conditions $0","\\leq f \\leq e$ for constant $e, f$ and $q$ some power of prime.","We also show that such APMUBs provide sets of Bi-angular vectors which are of the order of $\\mathcal{O}(d^{3/2})$ in numbers, having high angular distances among them."],"url":"http://arxiv.org/abs/2402.03964v1","category":"cs.DM"}
{"created":"2024-02-06 12:23:14","title":"Boosting Adversarial Transferability across Model Genus by Deformation-Constrained Warping","abstract":"Adversarial examples generated by a surrogate model typically exhibit limited transferability to unknown target systems. To address this problem, many transferability enhancement approaches (e.g., input transformation and model augmentation) have been proposed. However, they show poor performances in attacking systems having different model genera from the surrogate model. In this paper, we propose a novel and generic attacking strategy, called Deformation-Constrained Warping Attack (DeCoWA), that can be effectively applied to cross model genus attack. Specifically, DeCoWA firstly augments input examples via an elastic deformation, namely Deformation-Constrained Warping (DeCoW), to obtain rich local details of the augmented input. To avoid severe distortion of global semantics led by random deformation, DeCoW further constrains the strength and direction of the warping transformation by a novel adaptive control strategy. Extensive experiments demonstrate that the transferable examples crafted by our DeCoWA on CNN surrogates can significantly hinder the performance of Transformers (and vice versa) on various tasks, including image classification, video action recognition, and audio recognition. Code is made available at https://github.com/LinQinLiang/DeCoWA.","sentences":["Adversarial examples generated by a surrogate model typically exhibit limited transferability to unknown target systems.","To address this problem, many transferability enhancement approaches (e.g., input transformation and model augmentation) have been proposed.","However, they show poor performances in attacking systems having different model genera from the surrogate model.","In this paper, we propose a novel and generic attacking strategy, called Deformation-Constrained Warping Attack (DeCoWA), that can be effectively applied to cross model genus attack.","Specifically, DeCoWA firstly augments input examples via an elastic deformation, namely Deformation-Constrained Warping (DeCoW), to obtain rich local details of the augmented input.","To avoid severe distortion of global semantics led by random deformation, DeCoW further constrains the strength and direction of the warping transformation by a novel adaptive control strategy.","Extensive experiments demonstrate that the transferable examples crafted by our DeCoWA on CNN surrogates can significantly hinder the performance of Transformers (and vice versa) on various tasks, including image classification, video action recognition, and audio recognition.","Code is made available at https://github.com/LinQinLiang/DeCoWA."],"url":"http://arxiv.org/abs/2402.03951v1","category":"cs.CV"}
{"created":"2024-02-06 12:19:46","title":"Using metaheuristics for the location of bicycle stations","abstract":"In this work, we solve the problem of finding the best locations to place stations for depositing/collecting shared bicycles. To do this, we model the problem as the p-median problem, that is a major existing localization problem in optimization. The p-median problem seeks to place a set of facilities (bicycle stations) in a way that minimizes the distance between a set of clients (citizens) and their closest facility (bike station). We have used a genetic algorithm, iterated local search, particle swarm optimization, simulated annealing, and variable neighbourhood search, to find the best locations for the bicycle stations and study their comparative advantages. We use irace to parameterize each algorithm automatically, to contribute with a methodology to fine-tune algorithms automatically. We have also studied different real data (distance and weights) from diverse open data sources from a real city, Malaga (Spain), hopefully leading to a final smart city application. We have compared our results with the implemented solution in Malaga. Finally, we have analyzed how we can use our proposal to improve the existing system in the city by adding more stations.","sentences":["In this work, we solve the problem of finding the best locations to place stations for depositing/collecting shared bicycles.","To do this, we model the problem as the p-median problem, that is a major existing localization problem in optimization.","The p-median problem seeks to place a set of facilities (bicycle stations) in a way that minimizes the distance between a set of clients (citizens) and their closest facility (bike station).","We have used a genetic algorithm, iterated local search, particle swarm optimization, simulated annealing, and variable neighbourhood search, to find the best locations for the bicycle stations and study their comparative advantages.","We use irace to parameterize each algorithm automatically, to contribute with a methodology to fine-tune algorithms automatically.","We have also studied different real data (distance and weights) from diverse open data sources from a real city, Malaga (Spain), hopefully leading to a final smart city application.","We have compared our results with the implemented solution in Malaga.","Finally, we have analyzed how we can use our proposal to improve the existing system in the city by adding more stations."],"url":"http://arxiv.org/abs/2402.03945v1","category":"cs.NE"}
{"created":"2024-02-06 12:18:54","title":"Discovery of the Hidden World with Large Language Models","abstract":"Science originates with discovering new causal knowledge from a combination of known facts and observations. Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. However, the causal variables are usually unavailable in a wide range of real-world applications. The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data. Therefore, we introduce COAT: Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer that extracts the potential causal factors from unstructured data. Moreover, LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data. The annotated data will be fed to a causal learning module (e.g., the FCI algorithm) that provides both rigorous explanations of the data, as well as useful feedback to further improve the extraction of causal factors by LLMs. We verify the effectiveness of COAT in uncovering the underlying causal system with two case studies of review rating analysis and neuropathic diagnosis.","sentences":["Science originates with discovering new causal knowledge from a combination of known facts and observations.","Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations.","However, the causal variables are usually unavailable in a wide range of real-world applications.","The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data.","Therefore, we introduce COAT:","Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer that extracts the potential causal factors from unstructured data.","Moreover, LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data.","The annotated data will be fed to a causal learning module (e.g., the FCI algorithm) that provides both rigorous explanations of the data, as well as useful feedback to further improve the extraction of causal factors by LLMs.","We verify the effectiveness of COAT in uncovering the underlying causal system with two case studies of review rating analysis and neuropathic diagnosis."],"url":"http://arxiv.org/abs/2402.03941v1","category":"cs.LG"}
{"created":"2024-02-06 11:54:23","title":"Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs","abstract":"Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of \\emph{indirect} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been globally exposed to $\\sim$4.7M samples from 263 benchmarks. At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues. We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts.","sentences":["Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source.","The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers.","Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error.","Additionally, they overlook the problem of \\emph{indirect} data leaking, where models are iteratively improved by using data coming from users.","In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination.","By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release.","We report that these models have been globally exposed to $\\sim$4.7M samples from 263 benchmarks.","At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues.","We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts."],"url":"http://arxiv.org/abs/2402.03927v1","category":"cs.CL"}
{"created":"2024-02-06 11:44:06","title":"Large Language Models to Enhance Bayesian Optimization","abstract":"Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance still remains a delicate process. In this light, we present \\texttt{LLAMBO}, a novel approach that integrates the capabilities of large language models (LLM) within BO. At a high level, we frame the BO problem in natural language terms, enabling LLMs to iteratively propose promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can enhance various components of model-based BO. Our findings illustrate that \\texttt{LLAMBO} is effective at zero-shot warmstarting, and improves surrogate modeling and candidate sampling, especially in the early stages of search when observations are sparse. Our approach is performed in context and does not require LLM finetuning. Additionally, it is modular by design, allowing individual components to be integrated into existing BO frameworks, or function cohesively as an end-to-end method. We empirically validate \\texttt{LLAMBO}'s efficacy on the problem of hyperparameter tuning, highlighting strong empirical performance across a range of diverse benchmarks, proprietary, and synthetic tasks.","sentences":["Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions.","Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation.","While there has been substantial progress in BO methods, striking this balance still remains a delicate process.","In this light, we present \\texttt{LLAMBO}, a novel approach that integrates the capabilities of large language models (LLM) within BO.","At a high level, we frame the BO problem in natural language terms, enabling LLMs to iteratively propose promising solutions conditioned on historical evaluations.","More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can enhance various components of model-based BO.","Our findings illustrate that \\texttt{LLAMBO} is effective at zero-shot warmstarting, and improves surrogate modeling and candidate sampling, especially in the early stages of search when observations are sparse.","Our approach is performed in context and does not require LLM finetuning.","Additionally, it is modular by design, allowing individual components to be integrated into existing BO frameworks, or function cohesively as an end-to-end method.","We empirically validate \\texttt{LLAMBO}'s efficacy on the problem of hyperparameter tuning, highlighting strong empirical performance across a range of diverse benchmarks, proprietary, and synthetic tasks."],"url":"http://arxiv.org/abs/2402.03921v1","category":"cs.LG"}
{"created":"2024-02-06 11:31:04","title":"Learning Metrics that Maximise Power for Accelerated A/B-Tests","abstract":"Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies. A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior. North Star metrics are typically delayed and insensitive. As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent.   We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star. We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the $p$-values a metric would have produced on a log of past experiments. We collect such datasets from two social media applications with over 160 million Monthly Active Users each, totalling over 153 A/B-pairs. Empirical results show that we are able to increase statistical power by up to 78% when using our learnt metrics stand-alone, and by up to 210% when used in tandem with the North Star. Alternatively, we can obtain constant statistical power at a sample size that is down to 12% of what the North Star requires, significantly reducing the cost of experimentation.","sentences":["Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies.","A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior.","North Star metrics are typically delayed and insensitive.","As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent.   ","We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star.","We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the $p$-values a metric would have produced on a log of past experiments.","We collect such datasets from two social media applications with over 160 million Monthly Active Users each, totalling over 153 A/B-pairs.","Empirical results show that we are able to increase statistical power by up to 78% when using our learnt metrics stand-alone, and by up to 210% when used in tandem with the North Star.","Alternatively, we can obtain constant statistical power at a sample size that is down to 12% of what the North Star requires, significantly reducing the cost of experimentation."],"url":"http://arxiv.org/abs/2402.03915v1","category":"cs.LG"}
{"created":"2024-02-06 11:19:40","title":"Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy","abstract":"Recent developments in computer graphics, hardware, artificial intelligence (AI), and human-computer interaction likely lead to extended reality (XR) devices and setups being more pervasive. While these devices and setups provide users with interactive, engaging, and immersive experiences with different sensing modalities, such as eye and hand trackers, many non-player characters are utilized in a pre-scripted way or by conventional AI techniques. In this paper, we argue for using large language models (LLMs) in XR by embedding them in virtual avatars or as narratives to facilitate more inclusive experiences through prompt engineering according to user profiles and fine-tuning the LLMs for particular purposes. We argue that such inclusion will facilitate diversity for XR use. In addition, we believe that with the versatile conversational capabilities of LLMs, users will engage more with XR environments, which might help XR be more used in everyday life. Lastly, we speculate that combining the information provided to LLM-powered environments by the users and the biometric data obtained through the sensors might lead to novel privacy invasions. While studying such possible privacy invasions, user privacy concerns and preferences should also be investigated. In summary, despite some challenges, embedding LLMs into XR is a promising and novel research area with several opportunities.","sentences":["Recent developments in computer graphics, hardware, artificial intelligence (AI), and human-computer interaction likely lead to extended reality (XR) devices and setups being more pervasive.","While these devices and setups provide users with interactive, engaging, and immersive experiences with different sensing modalities, such as eye and hand trackers, many non-player characters are utilized in a pre-scripted way or by conventional AI techniques.","In this paper, we argue for using large language models (LLMs) in XR by embedding them in virtual avatars or as narratives to facilitate more inclusive experiences through prompt engineering according to user profiles and fine-tuning the LLMs for particular purposes.","We argue that such inclusion will facilitate diversity for XR use.","In addition, we believe that with the versatile conversational capabilities of LLMs, users will engage more with XR environments, which might help XR be more used in everyday life.","Lastly, we speculate that combining the information provided to LLM-powered environments by the users and the biometric data obtained through the sensors might lead to novel privacy invasions.","While studying such possible privacy invasions, user privacy concerns and preferences should also be investigated.","In summary, despite some challenges, embedding LLMs into XR is a promising and novel research area with several opportunities."],"url":"http://arxiv.org/abs/2402.03907v1","category":"cs.HC"}
{"created":"2024-02-06 11:10:35","title":"DistiLLM: Towards Streamlined Distillation for Large Language Models","abstract":"Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing student models while achieving up to 4.3$\\times$ speedup compared to recent KD methods.","sentences":["Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities.","However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function.","Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs.","To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models.","DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs.","Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing student models while achieving up to 4.3$\\times$ speedup compared to recent KD methods."],"url":"http://arxiv.org/abs/2402.03898v1","category":"cs.CL"}
{"created":"2024-02-06 11:10:32","title":"Robust Data-EnablEd Predictive Leading Cruise Control via Reachability Analysis","abstract":"Data-driven predictive control promises modelfree wave-dampening strategies for Connected and Autonomous Vehicles (CAVs) in mixed traffic flow. However, the performance suffers from unknown noise and disturbances, which could occur in offline data collection and online predictive control. In this paper, we propose a Robust Data-EnablEd Predictive Leading Cruise Control (RDeeP-LCC) method based on reachability analysis, aiming to achieve safe and optimal control of CAVs under bounded process noise and external disturbances. Precisely, we decouple the mixed platoon system into an error system and a nominal system, and tighten the constraint via the data-driven reachable set technique. Then, the enhanced safety constraint is integrated with the data-driven predictive control formulation to achieve stronger robust control performance for CAVs. Simulations validate the effectiveness of the proposed method in mitigating traffic waves with better robustness.","sentences":["Data-driven predictive control promises modelfree wave-dampening strategies for Connected and Autonomous Vehicles (CAVs) in mixed traffic flow.","However, the performance suffers from unknown noise and disturbances, which could occur in offline data collection and online predictive control.","In this paper, we propose a Robust Data-EnablEd Predictive Leading Cruise Control (RDeeP-LCC) method based on reachability analysis, aiming to achieve safe and optimal control of CAVs under bounded process noise and external disturbances.","Precisely, we decouple the mixed platoon system into an error system and a nominal system, and tighten the constraint via the data-driven reachable set technique.","Then, the enhanced safety constraint is integrated with the data-driven predictive control formulation to achieve stronger robust control performance for CAVs.","Simulations validate the effectiveness of the proposed method in mitigating traffic waves with better robustness."],"url":"http://arxiv.org/abs/2402.03897v1","category":"cs.SY"}
{"created":"2024-02-06 10:58:13","title":"Prediction Horizon Requirements for Automated Driving: Optimizing Safety, Comfort, and Efficiency","abstract":"Predicting the movement of other road users is beneficial for improving automated vehicle (AV) performance. However, the relationship between the time horizon associated with these predictions and AV performance remains unclear. Despite the existence of numerous trajectory prediction algorithms, no studies have been conducted on how varying prediction lengths affect AV safety and other vehicle performance metrics, resulting in undefined horizon requirements for prediction methods. Our study addresses this gap by examining the effects of different prediction horizons on AV performance, focusing on safety, comfort, and efficiency. Through multiple experiments using a state-of-the-art, risk-based predictive trajectory planner, we simulated predictions with horizons up to 20 seconds. Based on our simulations, we propose a framework for specifying the minimum required and optimal prediction horizons based on specific AV performance criteria and application needs. Our results indicate that a horizon of 1.6 seconds is required to prevent collisions with crossing pedestrians, horizons of 7-8 seconds yield the best efficiency, and horizons up to 15 seconds improve passenger comfort. We conclude that prediction horizon requirements are application-dependent, and recommend aiming for a prediction horizon of 11.8 seconds as a general guideline for applications involving crossing pedestrians.","sentences":["Predicting the movement of other road users is beneficial for improving automated vehicle (AV) performance.","However, the relationship between the time horizon associated with these predictions and AV performance remains unclear.","Despite the existence of numerous trajectory prediction algorithms, no studies have been conducted on how varying prediction lengths affect AV safety and other vehicle performance metrics, resulting in undefined horizon requirements for prediction methods.","Our study addresses this gap by examining the effects of different prediction horizons on AV performance, focusing on safety, comfort, and efficiency.","Through multiple experiments using a state-of-the-art, risk-based predictive trajectory planner, we simulated predictions with horizons up to 20 seconds.","Based on our simulations, we propose a framework for specifying the minimum required and optimal prediction horizons based on specific AV performance criteria and application needs.","Our results indicate that a horizon of 1.6 seconds is required to prevent collisions with crossing pedestrians, horizons of 7-8 seconds yield the best efficiency, and horizons up to 15 seconds improve passenger comfort.","We conclude that prediction horizon requirements are application-dependent, and recommend aiming for a prediction horizon of 11.8 seconds as a general guideline for applications involving crossing pedestrians."],"url":"http://arxiv.org/abs/2402.03893v1","category":"cs.RO"}
{"created":"2024-02-06 10:55:17","title":"The Emergence of Cooperation in the well-mixed Prisoner's Dilemma: Memory Couples Individual and Group Strategies","abstract":"Exploration of mechanisms underlying the emergence of collective cooperation remains a focal point in field of evolution of cooperation. Prevailing studies often neglect historical information, relying on the latest rewards as the primary criterion for individual decision-making-a method incongruent with human cognition and decision-making modes. This limitation impedes a comprehensive understanding of the spontaneous emergence of cooperation. Integrating memory factors into evolutionary game models to formulate decision criteria with delayed effects has shown potential in unraveling cooperation mechanisms. However, this comes at the significant cost of heightened computational complexity. In this paper, we propose an experiential decision-making method based on reinforcement learning. Utilizing this method, we construct a multi-agent system to engage in the evolutionary Prisoner's Dilemma game. Simulation results indicate that memory establishes a coupling relationship between individual and group strategies, fostering periodic oscillation between cooperation and defection in a well-mixed group. Specifically, defection loses its payoff advantage over cooperation as the group cooperation rate decreases. Conversely, the cooperative behavior gains reinforcement with an increase in the group cooperation rate, overcoming defection as the dominant strategy for individuals. This coupling between individual and group strategies fundamentally bridges the gap between individual and group interests, integrating a multitude of known factors and elucidating the fundamental mechanism of cooperation emergence in the face of social dilemmas.","sentences":["Exploration of mechanisms underlying the emergence of collective cooperation remains a focal point in field of evolution of cooperation.","Prevailing studies often neglect historical information, relying on the latest rewards as the primary criterion for individual decision-making-a method incongruent with human cognition and decision-making modes.","This limitation impedes a comprehensive understanding of the spontaneous emergence of cooperation.","Integrating memory factors into evolutionary game models to formulate decision criteria with delayed effects has shown potential in unraveling cooperation mechanisms.","However, this comes at the significant cost of heightened computational complexity.","In this paper, we propose an experiential decision-making method based on reinforcement learning.","Utilizing this method, we construct a multi-agent system to engage in the evolutionary Prisoner's Dilemma game.","Simulation results indicate that memory establishes a coupling relationship between individual and group strategies, fostering periodic oscillation between cooperation and defection in a well-mixed group.","Specifically, defection loses its payoff advantage over cooperation as the group cooperation rate decreases.","Conversely, the cooperative behavior gains reinforcement with an increase in the group cooperation rate, overcoming defection as the dominant strategy for individuals.","This coupling between individual and group strategies fundamentally bridges the gap between individual and group interests, integrating a multitude of known factors and elucidating the fundamental mechanism of cooperation emergence in the face of social dilemmas."],"url":"http://arxiv.org/abs/2402.03890v1","category":"physics.soc-ph"}
{"created":"2024-02-06 10:48:46","title":"MOMENT: A Family of Open Time-series Foundation Models","abstract":"We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis. Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning. Finally, we present several interesting empirical observations about large pre-trained time-series models. Our code is available anonymously at anonymous.4open.science/r/BETT-773F/.","sentences":["We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis.","Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous.","Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages.","To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training.","Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings.","Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning.","Finally, we present several interesting empirical observations about large pre-trained time-series models.","Our code is available anonymously at anonymous.4open.science/r/BETT-773F/."],"url":"http://arxiv.org/abs/2402.03885v1","category":"cs.LG"}
{"created":"2024-02-06 10:37:21","title":"Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models","abstract":"Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored. We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning. Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue. This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations.","sentences":["Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored.","We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning.","Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas.","LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements.","To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue.","This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations."],"url":"http://arxiv.org/abs/2402.03877v1","category":"cs.CL"}
{"created":"2024-02-06 10:06:13","title":"Position Paper: Toward New Frameworks for Studying Model Representations","abstract":"Mechanistic interpretability (MI) aims to understand AI models by reverse-engineering the exact algorithms neural networks learn. Most works in MI so far have studied behaviors and capabilities that are trivial and token-aligned. However, most capabilities are not that trivial, which advocates for the study of hidden representations inside these networks as the unit of analysis. We do a literature review, formalize representations for features and behaviors, highlight their importance and evaluation, and perform some basic exploration in the mechanistic interpretability of representations. With discussion and exploratory results, we justify our position that studying representations is an important and under-studied field, and that currently established methods in MI are not sufficient to understand representations, thus pushing for the research community to work toward new frameworks for studying representations.","sentences":["Mechanistic interpretability (MI) aims to understand AI models by reverse-engineering the exact algorithms neural networks learn.","Most works in MI so far have studied behaviors and capabilities that are trivial and token-aligned.","However, most capabilities are not that trivial, which advocates for the study of hidden representations inside these networks as the unit of analysis.","We do a literature review, formalize representations for features and behaviors, highlight their importance and evaluation, and perform some basic exploration in the mechanistic interpretability of representations.","With discussion and exploratory results, we justify our position that studying representations is an important and under-studied field, and that currently established methods in MI are not sufficient to understand representations, thus pushing for the research community to work toward new frameworks for studying representations."],"url":"http://arxiv.org/abs/2402.03855v1","category":"cs.LG"}
{"created":"2024-02-06 09:50:08","title":"ANLS* -- A Universal Document Processing Metric for Generative Large Language Models","abstract":"Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and is still compatible with previously reported ANLS scores. An evaluation of 7 different datasets and 3 different GLLMs using the ANLS* metric is also provided, demonstrating the importance of the proposed metric. We also benchmark a novel approach to generate prompts for documents, called SFT, against other prompting techniques such as LATIN. In 15 out of 21 cases, SFT outperforms other techniques and improves the state-of-the-art, sometimes by as much as $15$ percentage points.   Sources are available at https://github.com/deepopinion/anls_star_metric","sentences":["Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction.","These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score.","However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning.","However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs.","This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks.","The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and is still compatible with previously reported ANLS scores.","An evaluation of 7 different datasets and 3 different GLLMs using the ANLS* metric is also provided, demonstrating the importance of the proposed metric.","We also benchmark a novel approach to generate prompts for documents, called SFT, against other prompting techniques such as LATIN.","In 15 out of 21 cases, SFT outperforms other techniques and improves the state-of-the-art, sometimes by as much as $15$ percentage points.   ","Sources are available at https://github.com/deepopinion/anls_star_metric"],"url":"http://arxiv.org/abs/2402.03848v1","category":"cs.CL"}
{"created":"2024-02-06 09:39:05","title":"A new method for optical steel rope non-destructive damage detection","abstract":"This paper presents a novel algorithm for non-destructive damage detection for steel ropes in high-altitude environments (aerial ropeway). The algorithm comprises two key components: First, a segmentation model named RGBD-UNet is designed to accurately extract steel ropes from complex backgrounds. This model is equipped with the capability to process and combine color and depth information through the proposed CMA module. Second, a detection model named VovNetV3.5 is developed to differentiate between normal and abnormal steel ropes. It integrates the VovNet architecture with a DBB module to enhance performance. Besides, a novel background augmentation method is proposed to enhance the generalization ability of the segmentation model. Datasets containing images of steel ropes in different scenarios are created for the training and testing of both the segmentation and detection models. Experiments demonstrate a significant improvement over baseline models. On the proposed dataset, the highest accuracy achieved by the detection model reached 0.975, and the maximum F-measure achieved by the segmentation model reached 0.948.","sentences":["This paper presents a novel algorithm for non-destructive damage detection for steel ropes in high-altitude environments (aerial ropeway).","The algorithm comprises two key components: First, a segmentation model named RGBD-UNet is designed to accurately extract steel ropes from complex backgrounds.","This model is equipped with the capability to process and combine color and depth information through the proposed CMA module.","Second, a detection model named VovNetV3.5 is developed to differentiate between normal and abnormal steel ropes.","It integrates the VovNet architecture with a DBB module to enhance performance.","Besides, a novel background augmentation method is proposed to enhance the generalization ability of the segmentation model.","Datasets containing images of steel ropes in different scenarios are created for the training and testing of both the segmentation and detection models.","Experiments demonstrate a significant improvement over baseline models.","On the proposed dataset, the highest accuracy achieved by the detection model reached 0.975, and the maximum F-measure achieved by the segmentation model reached 0.948."],"url":"http://arxiv.org/abs/2402.03843v1","category":"cs.CV"}
{"created":"2024-02-06 09:19:44","title":"OASim: an Open and Adaptive Simulator based on Neural Rendering for Autonomous Driving","abstract":"With deep learning and computer vision technology development, autonomous driving provides new solutions to improve traffic safety and efficiency. The importance of building high-quality datasets is self-evident, especially with the rise of end-to-end autonomous driving algorithms in recent years. Data plays a core role in the algorithm closed-loop system. However, collecting real-world data is expensive, time-consuming, and unsafe. With the development of implicit rendering technology and in-depth research on using generative models to produce data at scale, we propose OASim, an open and adaptive simulator and autonomous driving data generator based on implicit neural rendering. It has the following characteristics: (1) High-quality scene reconstruction through neural implicit surface reconstruction technology. (2) Trajectory editing of the ego vehicle and participating vehicles. (3) Rich vehicle model library that can be freely selected and inserted into the scene. (4) Rich sensors model library where you can select specified sensors to generate data. (5) A highly customizable data generation system can generate data according to user needs. We demonstrate the high quality and fidelity of the generated data through perception performance evaluation on the Carla simulator and real-world data acquisition. Code is available at https://github.com/PJLab-ADG/OASim.","sentences":["With deep learning and computer vision technology development, autonomous driving provides new solutions to improve traffic safety and efficiency.","The importance of building high-quality datasets is self-evident, especially with the rise of end-to-end autonomous driving algorithms in recent years.","Data plays a core role in the algorithm closed-loop system.","However, collecting real-world data is expensive, time-consuming, and unsafe.","With the development of implicit rendering technology and in-depth research on using generative models to produce data at scale, we propose OASim, an open and adaptive simulator and autonomous driving data generator based on implicit neural rendering.","It has the following characteristics: (1) High-quality scene reconstruction through neural implicit surface reconstruction technology.","(2) Trajectory editing of the ego vehicle and participating vehicles.","(3) Rich vehicle model library that can be freely selected and inserted into the scene.","(4) Rich sensors model library where you can select specified sensors to generate data.","(5) A highly customizable data generation system can generate data according to user needs.","We demonstrate the high quality and fidelity of the generated data through perception performance evaluation on the Carla simulator and real-world data acquisition.","Code is available at https://github.com/PJLab-ADG/OASim."],"url":"http://arxiv.org/abs/2402.03830v1","category":"cs.CV"}
{"created":"2024-02-06 09:17:32","title":"Precise Measurement of Born Cross Sections for $e^+e^-\\to D\\bar{D}$ and Observation of One Structure between $\\sqrt{s} = 3.80-4.95$ GeV","abstract":"Using data samples collected with the BESIII detector at the BEPCII collider at center-of-mass energies ranging from 3.80 to 4.95 GeV, corresponding to an integrated luminosity of 20 fb$^{-1}$, a measurement of Born cross sections for the $e^+e^-\\to D^{0}\\bar{D}^{0}$ and $D^{+}D^{-}$ processes is presented with unprecedented precision. By performing a simultaneous fit to the dressed cross sections for both processes, one possible new structure around 3.9 GeV/$c^2$ is observed for the first time, in addition to seven known resonances $\\psi(3770)$, $\\psi(4040)$, $\\psi(4160)$, $Y(4230)$, $Y(4360)$, $\\psi(4415)$, and $Y(4660)$. These results offer crucial experimental insights into the nature of hadron production in the open charm region.","sentences":["Using data samples collected with the BESIII detector at the BEPCII collider at center-of-mass energies ranging from 3.80 to 4.95 GeV, corresponding to an integrated luminosity of 20 fb$^{-1}$, a measurement of Born cross sections for the $e^+e^-\\to D^{0}\\bar{D}^{0}$ and $D^{+}D^{-}$ processes is presented with unprecedented precision.","By performing a simultaneous fit to the dressed cross sections for both processes, one possible new structure around 3.9 GeV/$c^2$ is observed for the first time, in addition to seven known resonances $\\psi(3770)$, $\\psi(4040)$, $\\psi(4160)$, $Y(4230)$, $Y(4360)$, $\\psi(4415)$, and $Y(4660)$. These results offer crucial experimental insights into the nature of hadron production in the open charm region."],"url":"http://arxiv.org/abs/2402.03829v1","category":"hep-ex"}
{"created":"2024-02-06 09:17:07","title":"Estimating Barycenters of Distributions with Neural Optimal Transport","abstract":"Given a collection of probability measures, a practitioner sometimes needs to find an \"average\" distribution which adequately aggregates reference distributions. A theoretically appealing notion of such an average is the Wasserstein barycenter, which is the primal focus of our work. By building upon the dual formulation of Optimal Transport (OT), we propose a new scalable approach for solving the Wasserstein barycenter problem. Our methodology is based on the recent Neural OT solver: it has bi-level adversarial learning objective and works for general cost functions. These are key advantages of our method, since the typical adversarial algorithms leveraging barycenter tasks utilize tri-level optimization and focus mostly on quadratic cost. We also establish theoretical error bounds for our proposed approach and showcase its applicability and effectiveness on illustrative scenarios and image data setups.","sentences":["Given a collection of probability measures, a practitioner sometimes needs to find an \"average\" distribution which adequately aggregates reference distributions.","A theoretically appealing notion of such an average is the Wasserstein barycenter, which is the primal focus of our work.","By building upon the dual formulation of Optimal Transport (OT), we propose a new scalable approach for solving the Wasserstein barycenter problem.","Our methodology is based on the recent Neural OT solver: it has bi-level adversarial learning objective and works for general cost functions.","These are key advantages of our method, since the typical adversarial algorithms leveraging barycenter tasks utilize tri-level optimization and focus mostly on quadratic cost.","We also establish theoretical error bounds for our proposed approach and showcase its applicability and effectiveness on illustrative scenarios and image data setups."],"url":"http://arxiv.org/abs/2402.03828v1","category":"cs.LG"}
{"created":"2024-02-06 09:11:20","title":"A call for embodied AI","abstract":"We propose Embodied AI as the next fundamental step in the pursuit of Artificial General Intelligence, juxtaposing it against current AI advancements, particularly Large Language Models. We traverse the evolution of the embodiment concept across diverse fields - philosophy, psychology, neuroscience, and robotics - to highlight how EAI distinguishes itself from the classical paradigm of static learning. By broadening the scope of Embodied AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent. This framework is aligned with Friston's active inference principle, offering a comprehensive approach to EAI development. Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist. Our discussion lays down a foundational guideline for future Embodied AI research. Highlighting the importance of creating Embodied AI agents capable of seamless communication, collaboration, and coexistence with humans and other intelligent entities within real-world environments, we aim to steer the AI community towards addressing the multifaceted challenges and seizing the opportunities that lie ahead in the quest for AGI.","sentences":["We propose Embodied AI as the next fundamental step in the pursuit of Artificial General Intelligence, juxtaposing it against current AI advancements, particularly Large Language Models.","We traverse the evolution of the embodiment concept across diverse fields - philosophy, psychology, neuroscience, and robotics - to highlight how EAI distinguishes itself from the classical paradigm of static learning.","By broadening the scope of Embodied AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent.","This framework is aligned with Friston's active inference principle, offering a comprehensive approach to EAI development.","Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist.","Our discussion lays down a foundational guideline for future Embodied AI research.","Highlighting the importance of creating Embodied AI agents capable of seamless communication, collaboration, and coexistence with humans and other intelligent entities within real-world environments, we aim to steer the AI community towards addressing the multifaceted challenges and seizing the opportunities that lie ahead in the quest for AGI."],"url":"http://arxiv.org/abs/2402.03824v1","category":"cs.AI"}
{"created":"2024-02-06 09:10:35","title":"RevOrder: A Novel Method for Enhanced Arithmetic in Language Models","abstract":"This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks. Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\\mathcal{O}(1)$, a new metric we introduce to assess equation complexity. Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle. Implementation of RevOrder is cost-effective for both training and inference phases. Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4.","sentences":["This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks.","Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\\mathcal{O}(1)$, a new metric we introduce to assess equation complexity.","Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle.","Implementation of RevOrder is cost-effective for both training and inference phases.","Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4."],"url":"http://arxiv.org/abs/2402.03822v1","category":"cs.AI"}
{"created":"2024-02-06 08:56:22","title":"Using Perspective-n-Point Algorithms for a Local Positioning System Based on LEDs and a QADA Receiver","abstract":"The research interest on location-based services has increased during the last years ever since 3D centimetre accuracy inside intelligent environments could be confronted with. This work proposes an indoor local positioning system based on LED lighting, transmitted from a set of beacons to a receiver.The receiver is based on a quadrant photodiode angular diversity aperture (QADA) plus an aperture placed over it.This configuration can be modelled as a perspective camera, where the image position of the transmitters can be used to recover the receiver's 3D pose. This process is known as the perspective-n-point (PnP) problem, which is well known in computer vision and photogrammetry. This work investigates the use of different state-of-the-art PnP algorithms to localize the receiver in a large space based on four co-planar transmitters and with a distance from transmitters to receiver of 3.4 m. Encoding techniques are used to permit the simultaneous emission of all the transmitted signals and their processing in the receiver. In addition, correlation techniques are used to determine the image points projected from each emitter on the QADA. This work uses Monte Carlo simulations to characterize the absolute errors for a grid of test points under noisy measurements, as well as the robustness of the system when varying the 3D location of one transmitter. The IPPE algorithm obtained the best performance in this configuration. The proposal has also been experimentally evaluated in a real setup. The estimation of the receiver's position at three points using the IPPE algorithm achieves average absolute errors of 4.33cm, 3.51cm and 28.90cm in the coordinates x, y and z, respectively. These positioning results are in line with those obtained in previous work using triangulation techniques but with the addition that the complete pose of the receiver is obtained in this proposal.","sentences":["The research interest on location-based services has increased during the last years ever since 3D centimetre accuracy inside intelligent environments could be confronted with.","This work proposes an indoor local positioning system based on LED lighting, transmitted from a set of beacons to a receiver.","The receiver is based on a quadrant photodiode angular diversity aperture (QADA) plus an aperture placed over it.","This configuration can be modelled as a perspective camera, where the image position of the transmitters can be used to recover the receiver's 3D pose.","This process is known as the perspective-n-point (PnP) problem, which is well known in computer vision and photogrammetry.","This work investigates the use of different state-of-the-art PnP algorithms to localize the receiver in a large space based on four co-planar transmitters and with a distance from transmitters to receiver of 3.4 m. Encoding techniques are used to permit the simultaneous emission of all the transmitted signals and their processing in the receiver.","In addition, correlation techniques are used to determine the image points projected from each emitter on the QADA.","This work uses Monte Carlo simulations to characterize the absolute errors for a grid of test points under noisy measurements, as well as the robustness of the system when varying the 3D location of one transmitter.","The IPPE algorithm obtained the best performance in this configuration.","The proposal has also been experimentally evaluated in a real setup.","The estimation of the receiver's position at three points using the IPPE algorithm achieves average absolute errors of 4.33cm, 3.51cm and 28.90cm in the coordinates x, y and z, respectively.","These positioning results are in line with those obtained in previous work using triangulation techniques but with the addition that the complete pose of the receiver is obtained in this proposal."],"url":"http://arxiv.org/abs/2402.03811v1","category":"eess.SP"}
{"created":"2024-02-06 08:53:57","title":"On Erd\u0151s covering systems in global function fields","abstract":"A covering system of the integers is a finite collection of arithmetic progressions whose union is the set of integers. A well-known problem on covering systems is the minimum modulus problem posed by Erd\\H{o}s in 1950, who asked whether the minimum modulus in such systems with distinct moduli is arbitrarily large. This problem was resolved by Hough in 2015, showing that the minimum modulus is at most $10^{16}$. In 2022, Balister, Bollob\\'as, Morris, Sahasrabudhe and Tiba reduced Hough's bound to $616,000$ by developing Hough's method. They call it the distortion method. In this paper, by applying this method, we mainly prove that there does not exist any covering system of multiplicity $s$ in any global function field of genus $g$ over $\\mathbb{F}_q$ for $q\\geq (1.14+0.16g)e^{6.5+0.97g}s^2$. In particular, there is no covering system of $\\mathbb{F}_q[x]$ of distinct moduli for $q\\geq 759$.","sentences":["A covering system of the integers is a finite collection of arithmetic progressions whose union is the set of integers.","A well-known problem on covering systems is the minimum modulus problem posed by Erd\\H{o}s in 1950, who asked whether the minimum modulus in such systems with distinct moduli is arbitrarily large.","This problem was resolved by Hough in 2015, showing that the minimum modulus is at most $10^{16}$. In 2022, Balister, Bollob\\'as, Morris, Sahasrabudhe and Tiba reduced Hough's bound to $616,000$ by developing Hough's method.","They call it the distortion method.","In this paper, by applying this method, we mainly prove that there does not exist any covering system of multiplicity $s$ in any global function field of genus $g$ over $\\mathbb{F}_q$ for $q\\geq (1.14+0.16g)e^{6.5+0.97g}s^2$. In particular, there is no covering system of $\\mathbb{F}_q[x]$ of distinct moduli for $q\\geq 759$."],"url":"http://arxiv.org/abs/2402.03810v1","category":"math.NT"}
{"created":"2024-02-06 08:48:01","title":"SEABO: A Simple Search-Based Method for Offline Imitation Learning","abstract":"Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment. Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels. In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient. To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data. To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO. SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner. Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL algorithms with ground-truth rewards, given only a single expert trajectory, and can outperform prior reward learning and offline IL methods across many tasks. Moreover, we demonstrate that SEABO also works well if the expert demonstrations contain only observations. Our code is publicly available at https://github.com/dmksjfl/SEABO.","sentences":["Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment.","Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels.","In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient.","To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data.","To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO.","SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner.","Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL algorithms with ground-truth rewards, given only a single expert trajectory, and can outperform prior reward learning and offline IL methods across many tasks.","Moreover, we demonstrate that SEABO also works well if the expert demonstrations contain only observations.","Our code is publicly available at https://github.com/dmksjfl/SEABO."],"url":"http://arxiv.org/abs/2402.03807v1","category":"cs.LG"}
{"created":"2024-02-06 08:47:16","title":"Explainable Automated Machine Learning for Credit Decisions: Enhancing Human Artificial Intelligence Collaboration in Financial Engineering","abstract":"This paper explores the integration of Explainable Automated Machine Learning (AutoML) in the realm of financial engineering, specifically focusing on its application in credit decision-making. The rapid evolution of Artificial Intelligence (AI) in finance has necessitated a balance between sophisticated algorithmic decision-making and the need for transparency in these systems. The focus is on how AutoML can streamline the development of robust machine learning models for credit scoring, while Explainable AI (XAI) methods, particularly SHapley Additive exPlanations (SHAP), provide insights into the models' decision-making processes. This study demonstrates how the combination of AutoML and XAI not only enhances the efficiency and accuracy of credit decisions but also fosters trust and collaboration between humans and AI systems. The findings underscore the potential of explainable AutoML in improving the transparency and accountability of AI-driven financial decisions, aligning with regulatory requirements and ethical considerations.","sentences":["This paper explores the integration of Explainable Automated Machine Learning (AutoML) in the realm of financial engineering, specifically focusing on its application in credit decision-making.","The rapid evolution of Artificial Intelligence (AI) in finance has necessitated a balance between sophisticated algorithmic decision-making and the need for transparency in these systems.","The focus is on how AutoML can streamline the development of robust machine learning models for credit scoring, while Explainable AI (XAI) methods, particularly SHapley Additive exPlanations (SHAP), provide insights into the models' decision-making processes.","This study demonstrates how the combination of AutoML and XAI not only enhances the efficiency and accuracy of credit decisions but also fosters trust and collaboration between humans and AI systems.","The findings underscore the potential of explainable AutoML in improving the transparency and accountability of AI-driven financial decisions, aligning with regulatory requirements and ethical considerations."],"url":"http://arxiv.org/abs/2402.03806v1","category":"q-fin.RM"}
{"created":"2024-02-06 08:45:51","title":"ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs","abstract":"Sparse computation offers a compelling solution for the inference of Large Language Models (LLMs) in low-resource scenarios by dynamically skipping the computation of inactive neurons. While traditional approaches focus on ReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of sparse LLMs beyond zero activation values. We introduce a general method that defines neuron activation through neuron output magnitudes and a tailored magnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse activation. To find the most efficient activation function for sparse computation, we propose a systematic framework to examine the sparsity of LLMs from three aspects: the trade-off between sparsity and performance, the predictivity of sparsity, and the hardware affinity. We conduct thorough experiments on LLMs utilizing different activation functions, including ReLU, SwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing ReLU$^2$ excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse LLMs. We will release the code to facilitate future research.","sentences":["Sparse computation offers a compelling solution for the inference of Large Language Models (LLMs) in low-resource scenarios by dynamically skipping the computation of inactive neurons.","While traditional approaches focus on ReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of sparse LLMs beyond zero activation values.","We introduce a general method that defines neuron activation through neuron output magnitudes and a tailored magnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse activation.","To find the most efficient activation function for sparse computation, we propose a systematic framework to examine the sparsity of LLMs from three aspects: the trade-off between sparsity and performance, the predictivity of sparsity, and the hardware affinity.","We conduct thorough experiments on LLMs utilizing different activation functions, including ReLU, SwiGLU, ReGLU, and ReLU$^2$.","The results indicate that models employing ReLU$^2$ excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse LLMs.","We will release the code to facilitate future research."],"url":"http://arxiv.org/abs/2402.03804v1","category":"cs.LG"}
{"created":"2024-02-06 18:53:58","title":"Theory of Supervibronic Transitions via Casimir Polaritons","abstract":"A remote energy transfer pathway from electronic to vibrational degrees of freedom is identified inside an infrared optical cavity under vibrational strong coupling conditions. This mechanism relies on the dynamical Casimir effect, whereby real infrared photons are generated due to a sudden electronic transition of molecules. Moreover, the formation of vibrational polaritons enables the excited photon energy to be transferred to the vibrational degrees of freedom before any dissipation occurs. Both analytic solutions and numerical simulations reveal that the magnitude of this electronic to vibrational energy transfer depends quadratically on the number of molecules and resonantly on the vibration-cavity detuning. During this \"supervibronic\" transition process, because the vibrational energy gain per molecule can be meaningful in the macroscopic limit, this process may potentially be observed using conventional vibrational strong coupling devices at room temperature.","sentences":["A remote energy transfer pathway from electronic to vibrational degrees of freedom is identified inside an infrared optical cavity under vibrational strong coupling conditions.","This mechanism relies on the dynamical Casimir effect, whereby real infrared photons are generated due to a sudden electronic transition of molecules.","Moreover, the formation of vibrational polaritons enables the excited photon energy to be transferred to the vibrational degrees of freedom before any dissipation occurs.","Both analytic solutions and numerical simulations reveal that the magnitude of this electronic to vibrational energy transfer depends quadratically on the number of molecules and resonantly on the vibration-cavity detuning.","During this \"supervibronic\" transition process, because the vibrational energy gain per molecule can be meaningful in the macroscopic limit, this process may potentially be observed using conventional vibrational strong coupling devices at room temperature."],"url":"http://arxiv.org/abs/2402.04246v1","category":"quant-ph"}
{"created":"2024-02-06 18:09:05","title":"Variational Shapley Network: A Probabilistic Approach to Self-Explaining Shapley values with Uncertainty Quantification","abstract":"Shapley values have emerged as a foundational tool in machine learning (ML) for elucidating model decision-making processes. Despite their widespread adoption and unique ability to satisfy essential explainability axioms, computational challenges persist in their estimation when ($i$) evaluating a model over all possible subset of input feature combinations, ($ii$) estimating model marginals, and ($iii$) addressing variability in explanations. We introduce a novel, self-explaining method that simplifies the computation of Shapley values significantly, requiring only a single forward pass. Recognizing the deterministic treatment of Shapley values as a limitation, we explore incorporating a probabilistic framework to capture the inherent uncertainty in explanations. Unlike alternatives, our technique does not rely directly on the observed data space to estimate marginals; instead, it uses adaptable baseline values derived from a latent, feature-specific embedding space, generated by a novel masked neural network architecture. Evaluations on simulated and real datasets underscore our technique's robust predictive and explanatory performance.","sentences":["Shapley values have emerged as a foundational tool in machine learning (ML) for elucidating model decision-making processes.","Despite their widespread adoption and unique ability to satisfy essential explainability axioms, computational challenges persist in their estimation when ($i$) evaluating a model over all possible subset of input feature combinations, ($ii$) estimating model marginals, and ($iii$) addressing variability in explanations.","We introduce a novel, self-explaining method that simplifies the computation of Shapley values significantly, requiring only a single forward pass.","Recognizing the deterministic treatment of Shapley values as a limitation, we explore incorporating a probabilistic framework to capture the inherent uncertainty in explanations.","Unlike alternatives, our technique does not rely directly on the observed data space to estimate marginals; instead, it uses adaptable baseline values derived from a latent, feature-specific embedding space, generated by a novel masked neural network architecture.","Evaluations on simulated and real datasets underscore our technique's robust predictive and explanatory performance."],"url":"http://arxiv.org/abs/2402.04211v1","category":"cs.LG"}
{"created":"2024-02-06 17:50:39","title":"Observation of the double quantum spin Hall phase in moir\u00e9 WSe2","abstract":"Quantum spin Hall (QSH) insulators are a topologically protected phase of matter in two dimensions that can support non-dissipative spin transport. A hallmark of the phase is a pair of helical edge states surrounding an insulating bulk. A higher (even) number of helical edge state pairs is usually not possible in real materials because spin mixing would gap out the edge states. Multiple pairs of helical edge states have been proposed in materials with spin conservation symmetry and high spin Chern bands, but remained experimentally elusive. Here, we demonstrate a QSH phase with one and two pairs of helical edge states in twisted bilayer WSe2 at moir\\'e hole filling factor {\\nu}= 2 and 4, respectively. We observe nearly quantized conductance or resistance plateaus of h/({\\nu}e^2 ) at {\\nu} = 2 and 4 while the bulk is insulating. The conductance is nearly independent of out-of-plane magnetic field and decreases under an in-plane magnetic field. We also observe nonlocal transport, which is sensitive only to the in-plane magnetic field. The results agree with quantum transport of helical edge states protected by Ising spin conservation symmetry and open a promising platform for low-power spintronics.","sentences":["Quantum spin Hall (QSH) insulators are a topologically protected phase of matter in two dimensions that can support non-dissipative spin transport.","A hallmark of the phase is a pair of helical edge states surrounding an insulating bulk.","A higher (even) number of helical edge state pairs is usually not possible in real materials because spin mixing would gap out the edge states.","Multiple pairs of helical edge states have been proposed in materials with spin conservation symmetry and high spin Chern bands, but remained experimentally elusive.","Here, we demonstrate a QSH phase with one and two pairs of helical edge states in twisted bilayer WSe2 at moir\\'e hole filling factor {\\nu}= 2 and 4, respectively.","We observe nearly quantized conductance or resistance plateaus of h/({\\nu}e^2 ) at {\\nu} = 2 and 4 while the bulk is insulating.","The conductance is nearly independent of out-of-plane magnetic field and decreases under an in-plane magnetic field.","We also observe nonlocal transport, which is sensitive only to the in-plane magnetic field.","The results agree with quantum transport of helical edge states protected by Ising spin conservation symmetry and open a promising platform for low-power spintronics."],"url":"http://arxiv.org/abs/2402.04196v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-06 17:48:07","title":"Strain Functionals: A Complete and Symmetry-adapted Set of Descriptors to Characterize Atomistic Configurations","abstract":"Extracting relevant information from atomistic simulations relies on a complete and accurate characterization of atomistic configurations. We present a framework for characterizing atomistic configurations in terms of a complete and symmetry-adapted basis, referred to as strain functionals. In this approach a Gaussian kernel is used to map discrete atomic quantities, such as number density, velocities, and forces, to continuous fields. The local atomic configurations are then characterized using nth order central moments of the local number density. The initial Cartesian moments are recast unitarily into a Solid Harmonic Polynomial basis using SO(3) decompositions. Rotationally invariant metrics, referred to as Strain Functional Descriptors (SFDs), are constructed from the terms in the SO(3) decomposition using Clebsch-Gordan coupling. A key distinction compared to related methods is that a minimal but complete set of descriptors is identified. These descriptors characterize the local geometries numerically in terms of shape, size, and orientation descriptors that recognize n-fold symmetry axes and net shapes such as trigonal, cubic, hexagonal, etc. They can easily distinguish between most different crystal symmetries using n = 4, identify defects (such as dislocations and stacking faults), measure local deformation, and can be used in conjunction with machine learning techniques for in situ analysis of finite temperature atomistic simulation data and quantification of defect dynamics.","sentences":["Extracting relevant information from atomistic simulations relies on a complete and accurate characterization of atomistic configurations.","We present a framework for characterizing atomistic configurations in terms of a complete and symmetry-adapted basis, referred to as strain functionals.","In this approach a Gaussian kernel is used to map discrete atomic quantities, such as number density, velocities, and forces, to continuous fields.","The local atomic configurations are then characterized using nth order central moments of the local number density.","The initial Cartesian moments are recast unitarily into a Solid Harmonic Polynomial basis using SO(3) decompositions.","Rotationally invariant metrics, referred to as Strain Functional Descriptors (SFDs), are constructed from the terms in the SO(3) decomposition using Clebsch-Gordan coupling.","A key distinction compared to related methods is that a minimal but complete set of descriptors is identified.","These descriptors characterize the local geometries numerically in terms of shape, size, and orientation descriptors that recognize n-fold symmetry axes and net shapes such as trigonal, cubic, hexagonal, etc.","They can easily distinguish between most different crystal symmetries using n = 4, identify defects (such as dislocations and stacking faults), measure local deformation, and can be used in conjunction with machine learning techniques for in situ analysis of finite temperature atomistic simulation data and quantification of defect dynamics."],"url":"http://arxiv.org/abs/2402.04191v1","category":"physics.app-ph"}
{"created":"2024-02-06 17:22:52","title":"Estimates for oscillatory integrals with phase having $D$ type singularities","abstract":"In this paper, we consider estimates for the two-dimensional oscillatory integrals. The phase function of the oscillatory integrals is the linear perturbation of a function having $D$ type singularities. We consider estimates for the oscillatory integrals in terms of the Randol's type maximal functions. We obtain a sharp $L^p_{loc}$ estimates for the Randol's maximal functions. Moreover, we investigate the sharp exponent $p$ depending on whether, the phase function has linearly adapted coordinates system or not.","sentences":["In this paper, we consider estimates for the two-dimensional oscillatory integrals.","The phase function of the oscillatory integrals is the linear perturbation of a function having $D$ type singularities.","We consider estimates for the oscillatory integrals in terms of the Randol's type maximal functions.","We obtain a sharp $L^p_{loc}$ estimates for the Randol's maximal functions.","Moreover, we investigate the sharp exponent $p$ depending on whether, the phase function has linearly adapted coordinates system or not."],"url":"http://arxiv.org/abs/2402.04167v1","category":"math.CA"}
{"created":"2024-02-06 17:18:25","title":"Harnessing the Plug-and-Play Controller by Prompting","abstract":"Controllable text generation is a growing field within natural language generation (NLG) that focuses on producing text that meets specific constraints in real-world applications. Previous approaches, such as plug-and-play controllers (PPCs), aimed to steer the properties of generated text in a flexible manner. However, these methods often compromised the integrity of the language model's decoding process, resulting in less smooth text generation. Alternatively, other techniques utilized multiple attribute prompts to align the generated text with desired attributes, but this approach required prompt design for each attribute and was dependent on the size of the language model. This paper introduces a novel method for flexible attribute control in text generation using pre-trained language models (PLMs). The proposed approach aims to enhance the fluency of generated text by guiding the generation process with PPCs. The key idea is to dynamically adjust the distribution of generated text by modifying prompts, effectively constraining the output space of the language model and influencing the desired attribute. To enable smooth cooperation between the PLM and the PPC, our work innovatively proposes a new model fine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback (RLDAF).This fine-tuning process adapts a small subset of the language model's parameters based on the generating actions taken during the PPC control process. The resulting harmonious collaboration between the PLM and PPC leads to improved smoothness in text generation during inference. Extensive experiments were conducted on the SST2 dataset, and the proposed method outperformed previous approaches in various evaluation metrics, including text fluency and attribute consistency.","sentences":["Controllable text generation is a growing field within natural language generation (NLG) that focuses on producing text that meets specific constraints in real-world applications.","Previous approaches, such as plug-and-play controllers (PPCs), aimed to steer the properties of generated text in a flexible manner.","However, these methods often compromised the integrity of the language model's decoding process, resulting in less smooth text generation.","Alternatively, other techniques utilized multiple attribute prompts to align the generated text with desired attributes, but this approach required prompt design for each attribute and was dependent on the size of the language model.","This paper introduces a novel method for flexible attribute control in text generation using pre-trained language models (PLMs).","The proposed approach aims to enhance the fluency of generated text by guiding the generation process with PPCs.","The key idea is to dynamically adjust the distribution of generated text by modifying prompts, effectively constraining the output space of the language model and influencing the desired attribute.","To enable smooth cooperation between the PLM and the PPC, our work innovatively proposes a new model fine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback (RLDAF).This fine-tuning process adapts a small subset of the language model's parameters based on the generating actions taken during the PPC control process.","The resulting harmonious collaboration between the PLM and PPC leads to improved smoothness in text generation during inference.","Extensive experiments were conducted on the SST2 dataset, and the proposed method outperformed previous approaches in various evaluation metrics, including text fluency and attribute consistency."],"url":"http://arxiv.org/abs/2402.04160v1","category":"cs.CL"}
{"created":"2024-02-06 16:40:52","title":"Reducing two-level system dissipations in 3D superconducting Niobium resonators by atomic layer deposition and high temperature heat treatment","abstract":"Superconducting qubits have arisen as a leading technology platform for quantum computing which is on the verge of revolutionizing the world's calculation capacities. Nonetheless, the fabrication of computationally reliable qubit circuits requires increasing the quantum coherence lifetimes, which are predominantly limited by the dissipations of two-level system (TLS) defects present in the thin superconducting film and the adjacent dielectric regions. In this paper, we demonstrate the reduction of two-level system losses in three-dimensional superconducting radio frequency (SRF) niobium resonators by atomic layer deposition (ALD) of a 10 nm aluminum oxide Al2O3 thin films followed by a high vacuum (HV) heat treatment at 650 {\\deg}C for few hours. By probing the effect of several heat treatments on Al2O3-coated niobium samples by X-ray photoelectron spectroscopy (XPS) plus scanning and conventional high resolution transmission electron microscopy (STEM/HRTEM) coupled with electron energy loss spectroscopy (EELS) and (EDX) , we witness a dissolution of niobium native oxides and the modification of the Al2O3-Nb interface, which correlates with the enhancement of the quality factor at low fields of two 1.3 GHz niobium cavities coated with 10 nm of Al2O3.","sentences":["Superconducting qubits have arisen as a leading technology platform for quantum computing which is on the verge of revolutionizing the world's calculation capacities.","Nonetheless, the fabrication of computationally reliable qubit circuits requires increasing the quantum coherence lifetimes, which are predominantly limited by the dissipations of two-level system (TLS) defects present in the thin superconducting film and the adjacent dielectric regions.","In this paper, we demonstrate the reduction of two-level system losses in three-dimensional superconducting radio frequency (SRF) niobium resonators by atomic layer deposition (ALD) of a 10 nm aluminum oxide Al2O3 thin films followed by a high vacuum (HV) heat treatment at 650 {\\deg}C for few hours.","By probing the effect of several heat treatments on Al2O3-coated niobium samples by X-ray photoelectron spectroscopy (XPS) plus scanning and conventional high resolution transmission electron microscopy (STEM/HRTEM) coupled with electron energy loss spectroscopy (EELS) and (EDX) , we witness a dissolution of niobium native oxides and the modification of the Al2O3-Nb interface, which correlates with the enhancement of the quality factor at low fields of two 1.3 GHz niobium cavities coated with 10 nm of Al2O3."],"url":"http://arxiv.org/abs/2402.04137v1","category":"physics.app-ph"}
{"created":"2024-02-06 15:46:48","title":"Acceleration and energy consumption optimization in cascading classifiers for face detection on low-cost ARM big.LITTLE asymmetric architectures","abstract":"This paper proposes a mechanism to accelerate and optimize the energy consumption of a face detection software based on Haar-like cascading classifiers, taking advantage of the features of low-cost Asymmetric Multicore Processors (AMPs) with limited power budget. A modelling and task scheduling/allocation is proposed in order to efficiently make use of the existing features on big.LITTLE ARM processors, including: (I) source-code adaptation for parallel computing, which enables code acceleration by applying the OmpSs programming model, a task-based programming model that handles data-dependencies between tasks in a transparent fashion; (II) different OmpSs task allocation policies which take into account the processor asymmetry and can dynamically set processing resources in a more efficient way based on their particular features. The proposed mechanism can be efficiently applied to take advantage of the processing elements existing on low-cost and low-energy multi-core embedded devices executing object detection algorithms based on cascading classifiers. Although these classifiers yield the best results for detection algorithms in the field of computer vision, their high computational requirements prevent them from being used on these devices under real-time requirements. Finally, we compare the energy efficiency of a heterogeneous architecture based on asymmetric multicore processors with a suitable task scheduling, with that of a homogeneous symmetric architecture.","sentences":["This paper proposes a mechanism to accelerate and optimize the energy consumption of a face detection software based on Haar-like cascading classifiers, taking advantage of the features of low-cost Asymmetric Multicore Processors (AMPs) with limited power budget.","A modelling and task scheduling/allocation is proposed in order to efficiently make use of the existing features on big.","LITTLE ARM processors, including: (I) source-code adaptation for parallel computing, which enables code acceleration by applying the OmpSs programming model, a task-based programming model that handles data-dependencies between tasks in a transparent fashion; (II) different OmpSs task allocation policies which take into account the processor asymmetry and can dynamically set processing resources in a more efficient way based on their particular features.","The proposed mechanism can be efficiently applied to take advantage of the processing elements existing on low-cost and low-energy multi-core embedded devices executing object detection algorithms based on cascading classifiers.","Although these classifiers yield the best results for detection algorithms in the field of computer vision, their high computational requirements prevent them from being used on these devices under real-time requirements.","Finally, we compare the energy efficiency of a heterogeneous architecture based on asymmetric multicore processors with a suitable task scheduling, with that of a homogeneous symmetric architecture."],"url":"http://arxiv.org/abs/2402.04090v1","category":"cs.PF"}
{"created":"2024-02-06 15:13:17","title":"Retrieve to Explain: Evidence-driven Predictions with Language Models","abstract":"Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.","sentences":["Machine learning models, particularly language models, are notoriously difficult to introspect.","Black-box models can mask both issues in model training and harmful biases.","For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively.","To address these issues, we introduce Retrieve to Explain (R2E).","R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction.","R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language.","We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes."],"url":"http://arxiv.org/abs/2402.04068v1","category":"cs.LG"}
{"created":"2024-02-06 15:05:25","title":"TopoNav: Topological Navigation for Efficient Exploration in Sparse Reward Environments","abstract":"Autonomous robots exploring unknown areas face a significant challenge -- navigating effectively without prior maps and with limited external feedback. This challenge intensifies in sparse reward environments, where traditional exploration techniques often fail. In this paper, we introduce TopoNav, a novel framework that empowers robots to overcome these constraints and achieve efficient, adaptable, and goal-oriented exploration. TopoNav's fundamental building blocks are active topological mapping, intrinsic reward mechanisms, and hierarchical objective prioritization. Throughout its exploration, TopoNav constructs a dynamic topological map that captures key locations and pathways. It utilizes intrinsic rewards to guide the robot towards designated sub-goals within this map, fostering structured exploration even in sparse reward settings. To ensure efficient navigation, TopoNav employs the Hierarchical Objective-Driven Active Topologies framework, enabling the robot to prioritize immediate tasks like obstacle avoidance while maintaining focus on the overall goal. We demonstrate TopoNav's effectiveness in simulated environments that replicate real-world conditions. Our results reveal significant improvements in exploration efficiency, navigational accuracy, and adaptability to unforeseen obstacles, showcasing its potential to revolutionize autonomous exploration in a wide range of applications, including search and rescue, environmental monitoring, and planetary exploration.","sentences":["Autonomous robots exploring unknown areas face a significant challenge -- navigating effectively without prior maps and with limited external feedback.","This challenge intensifies in sparse reward environments, where traditional exploration techniques often fail.","In this paper, we introduce TopoNav, a novel framework that empowers robots to overcome these constraints and achieve efficient, adaptable, and goal-oriented exploration.","TopoNav's fundamental building blocks are active topological mapping, intrinsic reward mechanisms, and hierarchical objective prioritization.","Throughout its exploration, TopoNav constructs a dynamic topological map that captures key locations and pathways.","It utilizes intrinsic rewards to guide the robot towards designated sub-goals within this map, fostering structured exploration even in sparse reward settings.","To ensure efficient navigation, TopoNav employs the Hierarchical Objective-Driven Active Topologies framework, enabling the robot to prioritize immediate tasks like obstacle avoidance while maintaining focus on the overall goal.","We demonstrate TopoNav's effectiveness in simulated environments that replicate real-world conditions.","Our results reveal significant improvements in exploration efficiency, navigational accuracy, and adaptability to unforeseen obstacles, showcasing its potential to revolutionize autonomous exploration in a wide range of applications, including search and rescue, environmental monitoring, and planetary exploration."],"url":"http://arxiv.org/abs/2402.04061v1","category":"cs.RO"}
{"created":"2024-02-06 15:01:06","title":"Teleparallel Geroch geometry","abstract":"We construct the teleparallel dynamics for extended geometry where the structure algebra is (an extension of) an untwisted affine Kac-Moody algebra. This provides a geometrisation of the Geroch symmetry appearing on dimensional reduction of a gravitational theory to two dimensions. The formalism is adapted to the underlying tensor hierarchy algebra, and will serve as a stepping stone towards the geometrisation of other infinite-dimensional, e.g. hyperbolic, symmetries.","sentences":["We construct the teleparallel dynamics for extended geometry where the structure algebra is (an extension of) an untwisted affine Kac-Moody algebra.","This provides a geometrisation of the Geroch symmetry appearing on dimensional reduction of a gravitational theory to two dimensions.","The formalism is adapted to the underlying tensor hierarchy algebra, and will serve as a stepping stone towards the geometrisation of other infinite-dimensional, e.g. hyperbolic, symmetries."],"url":"http://arxiv.org/abs/2402.04055v1","category":"hep-th"}
{"created":"2024-02-06 14:22:47","title":"NNLO+PS predictions for Higgs production through bottom-quark annihilation with MINNLO$_{\\text{PS}}$","abstract":"We consider Higgs production through bottom-quark annihilation at hadron colliders and we calculate next-to-next-to-leading-order (NNLO) corrections in QCD perturbation theory matched to parton showers (NNLO+PS). To this end, we have adapted the MINNLO$_{\\text{PS}}$ method to account for the extra scale dependence induced by an overall Yukawa coupling that is $\\overline{\\rm MS}$ renormalized. We compare our results against state-of-the-art fixed-order predictions at NNLO as well as resummed predictions at next-to-next-to-leading-logarithmic (NNLL) accuracy.","sentences":["We consider Higgs production through bottom-quark annihilation at hadron colliders and we calculate next-to-next-to-leading-order (NNLO) corrections in QCD perturbation theory matched to parton showers (NNLO+PS).","To this end, we have adapted the MINNLO$_{\\text{PS}}$ method to account for the extra scale dependence induced by an overall Yukawa coupling that is $\\overline{\\rm MS}$ renormalized.","We compare our results against state-of-the-art fixed-order predictions at NNLO as well as resummed predictions at next-to-next-to-leading-logarithmic (NNLL) accuracy."],"url":"http://arxiv.org/abs/2402.04025v1","category":"hep-ph"}
{"created":"2024-02-06 13:19:26","title":"On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions","abstract":"The Adaptive Momentum Estimation (Adam) algorithm is highly effective in training various deep learning tasks. Despite this, there's limited theoretical understanding for Adam, especially when focusing on its vanilla form in non-convex smooth scenarios with potential unbounded gradients and affine variance noise. In this paper, we study vanilla Adam under these challenging conditions. We introduce a comprehensive noise model which governs affine variance noise, bounded noise and sub-Gaussian noise. We show that Adam can find a stationary point with a $\\mathcal{O}(\\text{poly}(\\log T)/\\sqrt{T})$ rate in high probability under this general noise model where $T$ denotes total number iterations, matching the lower rate of stochastic first-order algorithms up to logarithm factors. More importantly, we reveal that Adam is free of tuning step-sizes with any problem-parameters, yielding a better adaptation property than the Stochastic Gradient Descent under the same conditions. We also provide a probabilistic convergence result for Adam under a generalized smooth condition which allows unbounded smoothness parameters and has been illustrated empirically to more accurately capture the smooth property of many practical objective functions.","sentences":["The Adaptive Momentum Estimation (Adam) algorithm is highly effective in training various deep learning tasks.","Despite this, there's limited theoretical understanding for Adam, especially when focusing on its vanilla form in non-convex smooth scenarios with potential unbounded gradients and affine variance noise.","In this paper, we study vanilla Adam under these challenging conditions.","We introduce a comprehensive noise model which governs affine variance noise, bounded noise and sub-Gaussian noise.","We show that Adam can find a stationary point with a $\\mathcal{O}(\\text{poly}(\\log T)/\\sqrt{T})$ rate in high probability under this general noise model where $T$ denotes total number iterations, matching the lower rate of stochastic first-order algorithms up to logarithm factors.","More importantly, we reveal that Adam is free of tuning step-sizes with any problem-parameters, yielding a better adaptation property than the Stochastic Gradient Descent under the same conditions.","We also provide a probabilistic convergence result for Adam under a generalized smooth condition which allows unbounded smoothness parameters and has been illustrated empirically to more accurately capture the smooth property of many practical objective functions."],"url":"http://arxiv.org/abs/2402.03982v1","category":"math.OC"}
{"created":"2024-02-06 13:01:21","title":"The CMS Fast Beam Condition Monitor for HL-LHC","abstract":"The high-luminosity upgrade of the LHC brings unprecedented requirements for real-time and precision bunch-by-bunch online luminosity measurement and beam-induced background monitoring. A key component of the CMS Beam Radiation, Instrumentation and Luminosity system is a stand-alone luminometer, the Fast Beam Condition Monitor (FBCM), which is fully independent from the CMS central trigger and data acquisition services and able to operate at all times with a triggerless readout. FBCM utilizes a dedicated front-end application-specific integrated circuit (ASIC) to amplify the signals from CO$_2$-cooled silicon-pad sensors with a timing resolution of a few nanoseconds, which enables the measurement of the beam-induced background. FBCM uses a modular design with two half-disks of twelve modules at each end of CMS, with four service modules placed close to the outer edge to reduce radiation-induced aging. The electronics system design adapts several components from the CMS Tracker for power, control and read-out functionalities. The dedicated FBCM23 ASIC contains six channels and adjustable shaping time to optimize the noise with regards to sensor leakage current. Each ASIC channel outputs a single binary high-speed asynchronous signal carrying time-of-arrival and time-over-threshold information. The chip output signal is digitized, encoded and sent via a radiation-hard gigabit transceiver and an optical link to the back-end electronics for analysis. This paper reports on the updated design of the FBCM detector and the ongoing testing program.","sentences":["The high-luminosity upgrade of the LHC brings unprecedented requirements for real-time and precision bunch-by-bunch online luminosity measurement and beam-induced background monitoring.","A key component of the CMS Beam Radiation, Instrumentation and Luminosity system is a stand-alone luminometer, the Fast Beam Condition Monitor (FBCM), which is fully independent from the CMS central trigger and data acquisition services and able to operate at all times with a triggerless readout.","FBCM utilizes a dedicated front-end application-specific integrated circuit (ASIC) to amplify the signals from CO$_2$-cooled silicon-pad sensors with a timing resolution of a few nanoseconds, which enables the measurement of the beam-induced background.","FBCM uses a modular design with two half-disks of twelve modules at each end of CMS, with four service modules placed close to the outer edge to reduce radiation-induced aging.","The electronics system design adapts several components from the CMS Tracker for power, control and read-out functionalities.","The dedicated FBCM23 ASIC contains six channels and adjustable shaping time to optimize the noise with regards to sensor leakage current.","Each ASIC channel outputs a single binary high-speed asynchronous signal carrying time-of-arrival and time-over-threshold information.","The chip output signal is digitized, encoded and sent via a radiation-hard gigabit transceiver and an optical link to the back-end electronics for analysis.","This paper reports on the updated design of the FBCM detector and the ongoing testing program."],"url":"http://arxiv.org/abs/2402.03971v1","category":"physics.ins-det"}
{"created":"2024-02-06 12:58:38","title":"In-context learning agents are asymmetric belief updaters","abstract":"We study the in-context learning dynamics of large language models (LLMs) using three instrumental learning tasks adapted from cognitive psychology. We find that LLMs update their beliefs in an asymmetric manner and learn more from better-than-expected outcomes than from worse-than-expected ones. Furthermore, we show that this effect reverses when learning about counterfactual feedback and disappears when no agency is implied. We corroborate these findings by investigating idealized in-context learning agents derived through meta-reinforcement learning, where we observe similar patterns. Taken together, our results contribute to our understanding of how in-context learning works by highlighting that the framing of a problem significantly influences how learning occurs, a phenomenon also observed in human cognition.","sentences":["We study the in-context learning dynamics of large language models (LLMs) using three instrumental learning tasks adapted from cognitive psychology.","We find that LLMs update their beliefs in an asymmetric manner and learn more from better-than-expected outcomes than from worse-than-expected ones.","Furthermore, we show that this effect reverses when learning about counterfactual feedback and disappears when no agency is implied.","We corroborate these findings by investigating idealized in-context learning agents derived through meta-reinforcement learning, where we observe similar patterns.","Taken together, our results contribute to our understanding of how in-context learning works by highlighting that the framing of a problem significantly influences how learning occurs, a phenomenon also observed in human cognition."],"url":"http://arxiv.org/abs/2402.03969v1","category":"cs.LG"}
{"created":"2024-02-06 12:28:27","title":"A linear dissipativity approach to incremental input-to-state stability for a class of positive Lur'e systems","abstract":"Incremental stability properties are considered for certain systems of forced, nonlinear differential equations with a particular positivity structure. An incremental stability estimate is derived for pairs of input/state/output trajectories of the Lur'e systems under consideration, from which a number of consequences are obtained, including the incremental exponential input-to-state stability property and certain input-output stability concepts with linear gain. Incremental stability estimates provide a basis for an investigation into the response to convergent and (almost) periodic forcing terms, and is treated presently. Our results show that an incremental version of the real Aizerman conjecture is true for positive Lur'e systems when an incremental gain condition is imposed on the nonlinear term, as we describe. Our argumentation is underpinned by linear dissipativity theory -- a property of positive linear control systems.","sentences":["Incremental stability properties are considered for certain systems of forced, nonlinear differential equations with a particular positivity structure.","An incremental stability estimate is derived for pairs of input/state/output trajectories of the Lur'e systems under consideration, from which a number of consequences are obtained, including the incremental exponential input-to-state stability property and certain input-output stability concepts with linear gain.","Incremental stability estimates provide a basis for an investigation into the response to convergent and (almost) periodic forcing terms, and is treated presently.","Our results show that an incremental version of the real Aizerman conjecture is true for positive Lur'e systems when an incremental gain condition is imposed on the nonlinear term, as we describe.","Our argumentation is underpinned by linear dissipativity theory -- a property of positive linear control systems."],"url":"http://arxiv.org/abs/2402.03955v1","category":"eess.SY"}
{"created":"2024-02-06 12:20:10","title":"BioNet-XR: Biological Network Visualization Framework for Virtual Reality and Mixed Reality Environments","abstract":"Protein-protein interaction networks (PPIN) enable the study of cellular processes in organisms. Visualizing PPINs in extended reality (XR), including virtual reality (VR) and mixed reality (MR), is crucial for exploring subnetworks, evaluating protein positions, and collaboratively analyzing and discussing on networks with the help of recent technological advancements. Here, we present BioNet-XR, a 3D visualization framework, to visualize PPINs in VR and MR environments. BioNet-XR was developed with the Unity3D game engine. Our framework provides state-of-the-art methods and visualization features including teleportation between nodes, general and first-person view to explore the network, subnetwork construction via PageRank, Steiner tree, and all-pair shortest path algorithms for a given set of initial nodes. We used usability tests to gather feedback from both specialists (bioinformaticians) and generalists (multidisciplinary groups), addressing the need for usability evaluations of visualization tools. In the MR version of BioNet-XR, users can seamlessly transition to real-world environments and interact with protein interaction networks. BioNet-XR is highly modular and adaptable for visualization of other biological networks, such as metabolic and regulatory networks, and extension with additional network methods.","sentences":["Protein-protein interaction networks (PPIN) enable the study of cellular processes in organisms.","Visualizing PPINs in extended reality (XR), including virtual reality (VR) and mixed reality (MR), is crucial for exploring subnetworks, evaluating protein positions, and collaboratively analyzing and discussing on networks with the help of recent technological advancements.","Here, we present BioNet-XR, a 3D visualization framework, to visualize PPINs in VR and MR environments.","BioNet-XR was developed with the Unity3D game engine.","Our framework provides state-of-the-art methods and visualization features including teleportation between nodes, general and first-person view to explore the network, subnetwork construction via PageRank, Steiner tree, and all-pair shortest path algorithms for a given set of initial nodes.","We used usability tests to gather feedback from both specialists (bioinformaticians) and generalists (multidisciplinary groups), addressing the need for usability evaluations of visualization tools.","In the MR version of BioNet-XR, users can seamlessly transition to real-world environments and interact with protein interaction networks.","BioNet-XR is highly modular and adaptable for visualization of other biological networks, such as metabolic and regulatory networks, and extension with additional network methods."],"url":"http://arxiv.org/abs/2402.03946v1","category":"cs.MM"}
{"created":"2024-02-06 10:18:30","title":"AED: Adaptable Error Detection for Few-shot Imitation Policy","abstract":"We study how to report few-shot imitation (FSI) policies' behavior errors in novel environments, a novel task named adaptable error detection (AED). The potential to cause serious damage to surrounding areas limits the application of FSI policies in real-world scenarios. Thus, a robust system is necessary to notify operators when FSI policies are inconsistent with the intent of demonstrations. We develop a cross-domain benchmark for the challenging AED task, consisting of 329 base and 158 novel environments. This task introduces three challenges, including (1) detecting behavior errors in novel environments, (2) behavior errors occurring without revealing notable changes, and (3) lacking complete temporal information of the rollout due to the necessity of online detection. To address these challenges, we propose Pattern Observer (PrObe) to parse discernible patterns in the policy feature representations of normal or error states, whose effectiveness is verified in the proposed benchmark. Through our comprehensive evaluation, PrObe consistently surpasses strong baselines and demonstrates a robust capability to identify errors arising from a wide range of FSI policies. Moreover, we conduct comprehensive ablations and experiments (error correction, demonstration quality, etc.) to validate the practicality of our proposed task and methodology.","sentences":["We study how to report few-shot imitation (FSI) policies' behavior errors in novel environments, a novel task named adaptable error detection (AED).","The potential to cause serious damage to surrounding areas limits the application of FSI policies in real-world scenarios.","Thus, a robust system is necessary to notify operators when FSI policies are inconsistent with the intent of demonstrations.","We develop a cross-domain benchmark for the challenging AED task, consisting of 329 base and 158 novel environments.","This task introduces three challenges, including (1) detecting behavior errors in novel environments, (2) behavior errors occurring without revealing notable changes, and (3) lacking complete temporal information of the rollout due to the necessity of online detection.","To address these challenges, we propose Pattern Observer (PrObe) to parse discernible patterns in the policy feature representations of normal or error states, whose effectiveness is verified in the proposed benchmark.","Through our comprehensive evaluation, PrObe consistently surpasses strong baselines and demonstrates a robust capability to identify errors arising from a wide range of FSI policies.","Moreover, we conduct comprehensive ablations and experiments (error correction, demonstration quality, etc.) to validate the practicality of our proposed task and methodology."],"url":"http://arxiv.org/abs/2402.03860v1","category":"cs.RO"}
{"created":"2024-02-06 09:26:46","title":"Enhanced Security and Efficiency in Blockchain with Aggregated Zero-Knowledge Proof Mechanisms","abstract":"Blockchain technology has emerged as a revolutionary tool in ensuring data integrity and security in digital transactions. However, the current approaches to data verification in blockchain systems, particularly in Ethereum, face challenges in terms of efficiency and computational overhead. The traditional use of Merkle Trees and cryptographic hash functions, while effective, leads to significant resource consumption, especially for large datasets. This highlights a gap in existing research: the need for more efficient methods of data verification in blockchain networks. Our study addresses this gap by proposing an innovative aggregation scheme for Zero-Knowledge Proofs within the structure of Merkle Trees. We develop a system that significantly reduces the size of the proof and the computational resources needed for its generation and verification. Our approach represents a paradigm shift in blockchain data verification, balancing security with efficiency. We conducted extensive experimental evaluations using real Ethereum block data to validate the effectiveness of our proposed scheme. The results demonstrate a drastic reduction in proof size and computational requirements compared to traditional methods, making the verification process more efficient and economically viable. Our contribution fills a critical research void, offering a scalable and secure solution for blockchain data verification. The implications of our work are far-reaching, enhancing the overall performance and adaptability of blockchain technology in various applications, from financial transactions to supply chain management.","sentences":["Blockchain technology has emerged as a revolutionary tool in ensuring data integrity and security in digital transactions.","However, the current approaches to data verification in blockchain systems, particularly in Ethereum, face challenges in terms of efficiency and computational overhead.","The traditional use of Merkle Trees and cryptographic hash functions, while effective, leads to significant resource consumption, especially for large datasets.","This highlights a gap in existing research: the need for more efficient methods of data verification in blockchain networks.","Our study addresses this gap by proposing an innovative aggregation scheme for Zero-Knowledge Proofs within the structure of Merkle Trees.","We develop a system that significantly reduces the size of the proof and the computational resources needed for its generation and verification.","Our approach represents a paradigm shift in blockchain data verification, balancing security with efficiency.","We conducted extensive experimental evaluations using real Ethereum block data to validate the effectiveness of our proposed scheme.","The results demonstrate a drastic reduction in proof size and computational requirements compared to traditional methods, making the verification process more efficient and economically viable.","Our contribution fills a critical research void, offering a scalable and secure solution for blockchain data verification.","The implications of our work are far-reaching, enhancing the overall performance and adaptability of blockchain technology in various applications, from financial transactions to supply chain management."],"url":"http://arxiv.org/abs/2402.03834v1","category":"cs.CR"}
{"created":"2024-02-06 09:10:44","title":"Learning immune receptor representations with protein language models","abstract":"Protein language models (PLMs) learn contextual representations from protein sequences and are profoundly impacting various scientific disciplines spanning protein design, drug discovery, and structural predictions. One particular research area where PLMs have gained considerable attention is adaptive immune receptors, whose tremendous sequence diversity dictates the functional recognition of the adaptive immune system. The self-supervised nature underlying the training of PLMs has been recently leveraged to implement a variety of immune receptor-specific PLMs. These models have demonstrated promise in tasks such as predicting antigen-specificity and structure, computationally engineering therapeutic antibodies, and diagnostics. However, challenges including insufficient training data and considerations related to model architecture, training strategies, and data and model availability must be addressed before fully unlocking the potential of PLMs in understanding, translating, and engineering immune receptors.","sentences":["Protein language models (PLMs) learn contextual representations from protein sequences and are profoundly impacting various scientific disciplines spanning protein design, drug discovery, and structural predictions.","One particular research area where PLMs have gained considerable attention is adaptive immune receptors, whose tremendous sequence diversity dictates the functional recognition of the adaptive immune system.","The self-supervised nature underlying the training of PLMs has been recently leveraged to implement a variety of immune receptor-specific PLMs.","These models have demonstrated promise in tasks such as predicting antigen-specificity and structure, computationally engineering therapeutic antibodies, and diagnostics.","However, challenges including insufficient training data and considerations related to model architecture, training strategies, and data and model availability must be addressed before fully unlocking the potential of PLMs in understanding, translating, and engineering immune receptors."],"url":"http://arxiv.org/abs/2402.03823v1","category":"q-bio.QM"}
{"created":"2024-02-06 08:48:39","title":"SDEMG: Score-based Diffusion Model for Surface Electromyographic Signal Denoising","abstract":"Surface electromyography (sEMG) recordings can be influenced by electrocardiogram (ECG) signals when the muscle being monitored is close to the heart. Several existing methods use signal-processing-based approaches, such as high-pass filter and template subtraction, while some derive mapping functions to restore clean sEMG signals from noisy sEMG (sEMG with ECG interference). Recently, the score-based diffusion model, a renowned generative model, has been introduced to generate high-quality and accurate samples with noisy input data. In this study, we proposed a novel approach, termed SDEMG, as a score-based diffusion model for sEMG signal denoising. To evaluate the proposed SDEMG approach, we conduct experiments to reduce noise in sEMG signals, employing data from an openly accessible source, the Non-Invasive Adaptive Prosthetics database, along with ECG signals from the MIT-BIH Normal Sinus Rhythm Database. The experiment result indicates that SDEMG outperformed comparative methods and produced high-quality sEMG samples. The source code of SDEMG the framework is available at: https://github.com/tonyliu0910/SDEMG","sentences":["Surface electromyography (sEMG) recordings can be influenced by electrocardiogram (ECG) signals when the muscle being monitored is close to the heart.","Several existing methods use signal-processing-based approaches, such as high-pass filter and template subtraction, while some derive mapping functions to restore clean sEMG signals from noisy sEMG","(sEMG with ECG interference).","Recently, the score-based diffusion model, a renowned generative model, has been introduced to generate high-quality and accurate samples with noisy input data.","In this study, we proposed a novel approach, termed SDEMG, as a score-based diffusion model for sEMG signal denoising.","To evaluate the proposed SDEMG approach, we conduct experiments to reduce noise in sEMG signals, employing data from an openly accessible source, the Non-Invasive Adaptive Prosthetics database, along with ECG signals from the MIT-BIH Normal Sinus Rhythm Database.","The experiment result indicates that SDEMG outperformed comparative methods and produced high-quality sEMG samples.","The source code of SDEMG the framework is available at: https://github.com/tonyliu0910/SDEMG"],"url":"http://arxiv.org/abs/2402.03808v1","category":"eess.SP"}
{"created":"2024-02-06 08:27:49","title":"Energy-based Domain-Adaptive Segmentation with Depth Guidance","abstract":"Recent endeavors have been made to leverage self-supervised depth estimation as guidance in unsupervised domain adaptation (UDA) for semantic segmentation. Prior arts, however, overlook the discrepancy between semantic and depth features, as well as the reliability of feature fusion, thus leading to suboptimal segmentation performance. To address this issue, we propose a novel UDA framework called SMART (croSs doMain semAntic segmentation based on eneRgy esTimation) that utilizes Energy-Based Models (EBMs) to obtain task-adaptive features and achieve reliable feature fusion for semantic segmentation with self-supervised depth estimates. Our framework incorporates two novel components: energy-based feature fusion (EB2F) and energy-based reliable fusion Assessment (RFA) modules. The EB2F module produces task-adaptive semantic and depth features by explicitly measuring and reducing their discrepancy using Hopfield energy for better feature fusion. The RFA module evaluates the reliability of the feature fusion using an energy score to improve the effectiveness of depth guidance. Extensive experiments on two datasets demonstrate that our method achieves significant performance gains over prior works, validating the effectiveness of our energy-based learning approach.","sentences":["Recent endeavors have been made to leverage self-supervised depth estimation as guidance in unsupervised domain adaptation (UDA) for semantic segmentation.","Prior arts, however, overlook the discrepancy between semantic and depth features, as well as the reliability of feature fusion, thus leading to suboptimal segmentation performance.","To address this issue, we propose a novel UDA framework called SMART (croSs doMain semAntic segmentation based on eneRgy esTimation) that utilizes Energy-Based Models (EBMs) to obtain task-adaptive features and achieve reliable feature fusion for semantic segmentation with self-supervised depth estimates.","Our framework incorporates two novel components: energy-based feature fusion (EB2F) and energy-based reliable fusion Assessment (RFA) modules.","The EB2F module produces task-adaptive semantic and depth features by explicitly measuring and reducing their discrepancy using Hopfield energy for better feature fusion.","The RFA module evaluates the reliability of the feature fusion using an energy score to improve the effectiveness of depth guidance.","Extensive experiments on two datasets demonstrate that our method achieves significant performance gains over prior works, validating the effectiveness of our energy-based learning approach."],"url":"http://arxiv.org/abs/2402.03795v1","category":"cs.CV"}
{"created":"2024-02-06 08:14:56","title":"Adaptive Blockwise Task-interleaved Pipeline Parallelism","abstract":"Efficient distributed training serves as a powerful catalyst and an essential foundation for the development of large-scale neural networks. In distributed training scenarios, various pipeline parallelism methods are cleverly designed and widely employed. In this paper, we propose ZeroPP, a highly efficient and flexible pipeline parallelism method that trades off pipeline bubbles, memory usage, and communication through adaptive scheduling units. ZeroPP achieves minimal pipeline bubbles by carefully staggering the computation tasks of forward, input gradient, and weight gradient within a scheduling unit. Additionally, ZeroPP optimizes the combination of pipeline parallelism and fully sharded data parallelism using a blockwise schedule. We conduct experiments with popular GPT-style models and observe up to a 30% increase in throughput compared to the state-of-the-art breath-first pipeline parallelism. Besides, our evaluation also demonstrates up to a 68% increase in throughput and a 10% reduction in memory consumption compared to the memory-efficient 1F1B method.","sentences":["Efficient distributed training serves as a powerful catalyst and an essential foundation for the development of large-scale neural networks.","In distributed training scenarios, various pipeline parallelism methods are cleverly designed and widely employed.","In this paper, we propose ZeroPP, a highly efficient and flexible pipeline parallelism method that trades off pipeline bubbles, memory usage, and communication through adaptive scheduling units.","ZeroPP achieves minimal pipeline bubbles by carefully staggering the computation tasks of forward, input gradient, and weight gradient within a scheduling unit.","Additionally, ZeroPP optimizes the combination of pipeline parallelism and fully sharded data parallelism using a blockwise schedule.","We conduct experiments with popular GPT-style models and observe up to a 30% increase in throughput compared to the state-of-the-art breath-first pipeline parallelism.","Besides, our evaluation also demonstrates up to a 68% increase in throughput and a 10% reduction in memory consumption compared to the memory-efficient 1F1B method."],"url":"http://arxiv.org/abs/2402.03791v1","category":"cs.DC"}
{"created":"2024-02-06 07:52:30","title":"Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More","abstract":"Soft Prompt Tuning (SPT) is a parameter-efficient method for adapting pre-trained language models (PLMs) to specific tasks by inserting learnable embeddings, or soft prompts, at the input layer of the PLM, without modifying its parameters. This paper investigates the potential of SPT for cross-lingual transfer. Unlike previous studies on SPT for cross-lingual transfer that often fine-tune both the soft prompt and the model parameters, we adhere to the original intent of SPT by keeping the model parameters frozen and only training the soft prompt. This does not only reduce the computational cost and storage overhead of full-model fine-tuning, but we also demonstrate that this very parameter efficiency intrinsic to SPT can enhance cross-lingual transfer performance to linguistically distant languages. Moreover, we explore how different factors related to the prompt, such as the length or its reparameterization, affect cross-lingual transfer performance.","sentences":["Soft Prompt Tuning (SPT) is a parameter-efficient method for adapting pre-trained language models (PLMs) to specific tasks by inserting learnable embeddings, or soft prompts, at the input layer of the PLM, without modifying its parameters.","This paper investigates the potential of SPT for cross-lingual transfer.","Unlike previous studies on SPT for cross-lingual transfer that often fine-tune both the soft prompt and the model parameters, we adhere to the original intent of SPT by keeping the model parameters frozen and only training the soft prompt.","This does not only reduce the computational cost and storage overhead of full-model fine-tuning, but we also demonstrate that this very parameter efficiency intrinsic to SPT can enhance cross-lingual transfer performance to linguistically distant languages.","Moreover, we explore how different factors related to the prompt, such as the length or its reparameterization, affect cross-lingual transfer performance."],"url":"http://arxiv.org/abs/2402.03782v1","category":"cs.CL"}
{"created":"2024-02-06 07:50:27","title":"EERO: Early Exit with Reject Option for Efficient Classification with limited budget","abstract":"The increasing complexity of advanced machine learning models requires innovative approaches to manage computational resources effectively. One such method is the Early Exit strategy, which allows for adaptive computation by providing a mechanism to shorten the processing path for simpler data instances. In this paper, we propose EERO, a new methodology to translate the problem of early exiting to a problem of using multiple classifiers with reject option in order to better select the exiting head for each instance. We calibrate the probabilities of exiting at the different heads using aggregation with exponential weights to guarantee a fixed budget .We consider factors such as Bayesian risk, budget constraints, and head-specific budget consumption. Experimental results, conducted using a ResNet-18 model and a ConvNext architecture on Cifar and ImageNet datasets, demonstrate that our method not only effectively manages budget allocation but also enhances accuracy in overthinking scenarios.","sentences":["The increasing complexity of advanced machine learning models requires innovative approaches to manage computational resources effectively.","One such method is the Early Exit strategy, which allows for adaptive computation by providing a mechanism to shorten the processing path for simpler data instances.","In this paper, we propose EERO, a new methodology to translate the problem of early exiting to a problem of using multiple classifiers with reject option in order to better select the exiting head for each instance.","We calibrate the probabilities of exiting at the different heads using aggregation with exponential weights to guarantee a fixed budget .We","consider factors such as Bayesian risk, budget constraints, and head-specific budget consumption.","Experimental results, conducted using a ResNet-18 model and a ConvNext architecture on Cifar and ImageNet datasets, demonstrate that our method not only effectively manages budget allocation but also enhances accuracy in overthinking scenarios."],"url":"http://arxiv.org/abs/2402.03779v1","category":"stat.ML"}
{"created":"2024-02-06 07:40:53","title":"Learning a Decision Tree Algorithm with Transformers","abstract":"Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data. Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree. However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization. To address this, we introduce MetaTree, which trains a transformer-based model on filtered outputs from classical algorithms to produce strong decision trees for classification. Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets. We then train MetaTree to produce the trees that achieve strong generalization performance. This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance.","sentences":["Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data.","Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree.","However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization.","To address this, we introduce MetaTree, which trains a transformer-based model on filtered outputs from classical algorithms to produce strong decision trees for classification.","Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets.","We then train MetaTree to produce the trees that achieve strong generalization performance.","This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance."],"url":"http://arxiv.org/abs/2402.03774v1","category":"cs.LG"}
{"created":"2024-02-06 07:26:44","title":"Reinforcement Learning from Bagged Reward: A Transformer-based Approach for Instance-Level Reward Redistribution","abstract":"In reinforcement Learning (RL), an instant reward signal is generated for each action of the agent, such that the agent learns to maximize the cumulative reward to obtain the optimal policy. However, in many real-world applications, the instant reward signals are not obtainable by the agent. Instead, the learner only obtains rewards at the ends of bags, where a bag is defined as a partial sequence of a complete trajectory. In this situation, the learner has to face the significant difficulty of exploring the unknown instant rewards in the bags, which could not be addressed by existing approaches, including those trajectory-based approaches that consider only complete trajectories and ignore the inner reward distributions. To formally study this situation, we introduce a novel RL setting termed Reinforcement Learning from Bagged Rewards (RLBR), where only the bagged rewards of sequences can be obtained. We provide the theoretical study to establish the connection between RLBR and standard RL in Markov Decision Processes (MDPs). To effectively explore the reward distributions within the bagged rewards, we propose a Transformer-based reward model, the Reward Bag Transformer (RBT), which uses the self-attention mechanism for interpreting the contextual nuances and temporal dependencies within each bag. Extensive experimental analyses demonstrate the superiority of our method, particularly in its ability to mimic the original MDP's reward distribution, highlighting its proficiency in contextual understanding and adaptability to environmental dynamics.","sentences":["In reinforcement Learning (RL), an instant reward signal is generated for each action of the agent, such that the agent learns to maximize the cumulative reward to obtain the optimal policy.","However, in many real-world applications, the instant reward signals are not obtainable by the agent.","Instead, the learner only obtains rewards at the ends of bags, where a bag is defined as a partial sequence of a complete trajectory.","In this situation, the learner has to face the significant difficulty of exploring the unknown instant rewards in the bags, which could not be addressed by existing approaches, including those trajectory-based approaches that consider only complete trajectories and ignore the inner reward distributions.","To formally study this situation, we introduce a novel RL setting termed Reinforcement Learning from Bagged Rewards (RLBR), where only the bagged rewards of sequences can be obtained.","We provide the theoretical study to establish the connection between RLBR and standard RL in Markov Decision Processes (MDPs).","To effectively explore the reward distributions within the bagged rewards, we propose a Transformer-based reward model, the Reward Bag Transformer (RBT), which uses the self-attention mechanism for interpreting the contextual nuances and temporal dependencies within each bag.","Extensive experimental analyses demonstrate the superiority of our method, particularly in its ability to mimic the original MDP's reward distribution, highlighting its proficiency in contextual understanding and adaptability to environmental dynamics."],"url":"http://arxiv.org/abs/2402.03771v1","category":"cs.LG"}
{"created":"2024-02-06 06:49:04","title":"Virtual Classification: Modulating Domain-Specific Knowledge for Multidomain Crowd Counting","abstract":"Multidomain crowd counting aims to learn a general model for multiple diverse datasets. However, deep networks prefer modeling distributions of the dominant domains instead of all domains, which is known as domain bias. In this study, we propose a simple-yet-effective Modulating Domain-specific Knowledge Network (MDKNet) to handle the domain bias issue in multidomain crowd counting. MDKNet is achieved by employing the idea of `modulating', enabling deep network balancing and modeling different distributions of diverse datasets with little bias. Specifically, we propose an Instance-specific Batch Normalization (IsBN) module, which serves as a base modulator to refine the information flow to be adaptive to domain distributions. To precisely modulating the domain-specific information, the Domain-guided Virtual Classifier (DVC) is then introduced to learn a domain-separable latent space. This space is employed as an input guidance for the IsBN modulator, such that the mixture distributions of multiple datasets can be well treated. Extensive experiments performed on popular benchmarks, including Shanghai-tech A/B, QNRF and NWPU, validate the superiority of MDKNet in tackling multidomain crowd counting and the effectiveness for multidomain learning. Code is available at \\url{https://github.com/csguomy/MDKNet}.","sentences":["Multidomain crowd counting aims to learn a general model for multiple diverse datasets.","However, deep networks prefer modeling distributions of the dominant domains instead of all domains, which is known as domain bias.","In this study, we propose a simple-yet-effective Modulating Domain-specific Knowledge Network (MDKNet) to handle the domain bias issue in multidomain crowd counting.","MDKNet is achieved by employing the idea of `modulating', enabling deep network balancing and modeling different distributions of diverse datasets with little bias.","Specifically, we propose an Instance-specific Batch Normalization (IsBN) module, which serves as a base modulator to refine the information flow to be adaptive to domain distributions.","To precisely modulating the domain-specific information, the Domain-guided Virtual Classifier (DVC) is then introduced to learn a domain-separable latent space.","This space is employed as an input guidance for the IsBN modulator, such that the mixture distributions of multiple datasets can be well treated.","Extensive experiments performed on popular benchmarks, including Shanghai-tech A/B, QNRF and NWPU, validate the superiority of MDKNet in tackling multidomain crowd counting and the effectiveness for multidomain learning.","Code is available at \\url{https://github.com/csguomy/MDKNet}."],"url":"http://arxiv.org/abs/2402.03758v1","category":"cs.CV"}
{"created":"2024-02-06 06:46:46","title":"Intensive Vision-guided Network for Radiology Report Generation","abstract":"Automatic radiology report generation is booming due to its huge application potential for the healthcare industry. However, existing computer vision and natural language processing approaches to tackle this problem are limited in two aspects. First, when extracting image features, most of them neglect multi-view reasoning in vision and model single-view structure of medical images, such as space-view or channel-view. However, clinicians rely on multi-view imaging information for comprehensive judgment in daily clinical diagnosis. Second, when generating reports, they overlook context reasoning with multi-modal information and focus on pure textual optimization utilizing retrieval-based methods. We aim to address these two issues by proposing a model that better simulates clinicians' perspectives and generates more accurate reports. Given the above limitation in feature extraction, we propose a Globally-intensive Attention (GIA) module in the medical image encoder to simulate and integrate multi-view vision perception. GIA aims to learn three types of vision perception: depth view, space view, and pixel view. On the other hand, to address the above problem in report generation, we explore how to involve multi-modal signals to generate precisely matched reports, i.e., how to integrate previously predicted words with region-aware visual content in next word prediction. Specifically, we design a Visual Knowledge-guided Decoder (VKGD), which can adaptively consider how much the model needs to rely on visual information and previously predicted text to assist next word prediction. Hence, our final Intensive Vision-guided Network (IVGN) framework includes a GIA-guided Visual Encoder and the VKGD. Experiments on two commonly-used datasets IU X-Ray and MIMIC-CXR demonstrate the superior ability of our method compared with other state-of-the-art approaches.","sentences":["Automatic radiology report generation is booming due to its huge application potential for the healthcare industry.","However, existing computer vision and natural language processing approaches to tackle this problem are limited in two aspects.","First, when extracting image features, most of them neglect multi-view reasoning in vision and model single-view structure of medical images, such as space-view or channel-view.","However, clinicians rely on multi-view imaging information for comprehensive judgment in daily clinical diagnosis.","Second, when generating reports, they overlook context reasoning with multi-modal information and focus on pure textual optimization utilizing retrieval-based methods.","We aim to address these two issues by proposing a model that better simulates clinicians' perspectives and generates more accurate reports.","Given the above limitation in feature extraction, we propose a Globally-intensive Attention (GIA) module in the medical image encoder to simulate and integrate multi-view vision perception.","GIA aims to learn three types of vision perception: depth view, space view, and pixel view.","On the other hand, to address the above problem in report generation, we explore how to involve multi-modal signals to generate precisely matched reports, i.e., how to integrate previously predicted words with region-aware visual content in next word prediction.","Specifically, we design a Visual Knowledge-guided Decoder (VKGD), which can adaptively consider how much the model needs to rely on visual information and previously predicted text to assist next word prediction.","Hence, our final Intensive Vision-guided Network (IVGN) framework includes a GIA-guided Visual Encoder and the VKGD.","Experiments on two commonly-used datasets IU X-Ray and MIMIC-CXR demonstrate the superior ability of our method compared with other state-of-the-art approaches."],"url":"http://arxiv.org/abs/2402.03754v1","category":"cs.CV"}
{"created":"2024-02-06 06:30:34","title":"Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models","abstract":"Recent advancements in large language models have sparked interest in their extraordinary and near-superhuman capabilities, leading researchers to explore methods for evaluating and optimizing these abilities, which is called superalignment. In this context, our paper delves into the realm of vision foundation models, focusing on the concept of weak-to-strong generalization, which involves using a weaker model to supervise a stronger one, aiming to enhance the latter's capabilities beyond the former's limits. We introduce a novel and adaptively adjustable loss function for weak-to-strong supervision. Our comprehensive experiments span various scenarios, including few-shot learning, transfer learning, noisy label learning, and common knowledge distillation settings. The results are striking: our approach not only exceeds the performance benchmarks set by strong-to-strong generalization but also surpasses the outcomes of fine-tuning strong models with whole datasets. This compelling evidence underscores the significant potential of weak-to-strong generalization, showcasing its capability to substantially elevate the performance of vision foundation models. The code is available at https://github.com/ggjy/vision_weak_to_strong.","sentences":["Recent advancements in large language models have sparked interest in their extraordinary and near-superhuman capabilities, leading researchers to explore methods for evaluating and optimizing these abilities, which is called superalignment.","In this context, our paper delves into the realm of vision foundation models, focusing on the concept of weak-to-strong generalization, which involves using a weaker model to supervise a stronger one, aiming to enhance the latter's capabilities beyond the former's limits.","We introduce a novel and adaptively adjustable loss function for weak-to-strong supervision.","Our comprehensive experiments span various scenarios, including few-shot learning, transfer learning, noisy label learning, and common knowledge distillation settings.","The results are striking: our approach not only exceeds the performance benchmarks set by strong-to-strong generalization but also surpasses the outcomes of fine-tuning strong models with whole datasets.","This compelling evidence underscores the significant potential of weak-to-strong generalization, showcasing its capability to substantially elevate the performance of vision foundation models.","The code is available at https://github.com/ggjy/vision_weak_to_strong."],"url":"http://arxiv.org/abs/2402.03749v1","category":"cs.CV"}
{"created":"2024-02-06 05:53:21","title":"Theory of parametric resonance for discrete time crystals in fully-connected spin-cavity systems","abstract":"We pinpoint the conditions necessary for discrete time crystal formation in fully-connected spin-cavity systems from the perspective of parametric resonance by mapping these systems onto oscillator-like models. We elucidate the role of nonlinearity and dissipation by mapping the periodically driven open Dicke model (DM) onto effective linear and nonlinear oscillator models, while we analyze the effect of global symmetry breaking using the Lipkin-Meshkov-Glick (LMG) model with tunable anisotropy. We show that the system's nonlinearity restrains the dynamics from becoming unbounded when driven resonantly. On the other hand, dissipation keeps the oscillation amplitude of the period-doubling instability fixed, which is a key feature of DTCs. The presence of global symmetry breaking in the absence of driving is found to be crucial in the parametric resonant activation of period-doubling response. We provide analytic predictions for the resonant frequencies and amplitudes leading to DTC formation for both systems using their respective oscillator models.","sentences":["We pinpoint the conditions necessary for discrete time crystal formation in fully-connected spin-cavity systems from the perspective of parametric resonance by mapping these systems onto oscillator-like models.","We elucidate the role of nonlinearity and dissipation by mapping the periodically driven open Dicke model (DM) onto effective linear and nonlinear oscillator models, while we analyze the effect of global symmetry breaking using the Lipkin-Meshkov-Glick (LMG) model with tunable anisotropy.","We show that the system's nonlinearity restrains the dynamics from becoming unbounded when driven resonantly.","On the other hand, dissipation keeps the oscillation amplitude of the period-doubling instability fixed, which is a key feature of DTCs.","The presence of global symmetry breaking in the absence of driving is found to be crucial in the parametric resonant activation of period-doubling response.","We provide analytic predictions for the resonant frequencies and amplitudes leading to DTC formation for both systems using their respective oscillator models."],"url":"http://arxiv.org/abs/2402.03729v1","category":"quant-ph"}
{"created":"2024-02-06 05:04:24","title":"Adaptive Backstepping Control of a Bicopter in Pure Feedback Form with Dynamic Extension","abstract":"This paper presents a model-based, adaptive, nonlinear controller for the bicopter stabilization and trajectory-tracking problem. The nonlinear controller is designed using the backstepping technique. Due to the non-invertibility of the input map, the bicopter system is first dynamically extended. However, the resulting dynamically extended system is in the pure feedback form with the uncertainty appearing in the input map. The adaptive backstepping technique is then extended and applied to design the controller. The proposed controller is validated in simulation for a smooth and nonsmooth trajectory-tracking problem.","sentences":["This paper presents a model-based, adaptive, nonlinear controller for the bicopter stabilization and trajectory-tracking problem.","The nonlinear controller is designed using the backstepping technique.","Due to the non-invertibility of the input map, the bicopter system is first dynamically extended.","However, the resulting dynamically extended system is in the pure feedback form with the uncertainty appearing in the input map.","The adaptive backstepping technique is then extended and applied to design the controller.","The proposed controller is validated in simulation for a smooth and nonsmooth trajectory-tracking problem."],"url":"http://arxiv.org/abs/2402.03709v1","category":"math.OC"}
{"created":"2024-02-06 04:57:07","title":"MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature Drone Threats","abstract":"In response to the evolving challenges posed by small unmanned aerial vehicles (UAVs), which possess the potential to transport harmful payloads or independently cause damage, we introduce MMAUD: a comprehensive Multi-Modal Anti-UAV Dataset. MMAUD addresses a critical gap in contemporary threat detection methodologies by focusing on drone detection, UAV-type classification, and trajectory estimation. MMAUD stands out by combining diverse sensory inputs, including stereo vision, various Lidars, Radars, and audio arrays. It offers a unique overhead aerial detection vital for addressing real-world scenarios with higher fidelity than datasets captured on specific vantage points using thermal and RGB. Additionally, MMAUD provides accurate Leica-generated ground truth data, enhancing credibility and enabling confident refinement of algorithms and models, which has never been seen in other datasets. Most existing works do not disclose their datasets, making MMAUD an invaluable resource for developing accurate and efficient solutions. Our proposed modalities are cost-effective and highly adaptable, allowing users to experiment and implement new UAV threat detection tools. Our dataset closely simulates real-world scenarios by incorporating ambient heavy machinery sounds. This approach enhances the dataset's applicability, capturing the exact challenges faced during proximate vehicular operations. It is expected that MMAUD can play a pivotal role in advancing UAV threat detection, classification, trajectory estimation capabilities, and beyond. Our dataset, codes, and designs will be available in https://github.com/ntu-aris/MMAUD.","sentences":["In response to the evolving challenges posed by small unmanned aerial vehicles (UAVs), which possess the potential to transport harmful payloads or independently cause damage, we introduce MMAUD: a comprehensive Multi-Modal Anti-UAV Dataset.","MMAUD addresses a critical gap in contemporary threat detection methodologies by focusing on drone detection, UAV-type classification, and trajectory estimation.","MMAUD stands out by combining diverse sensory inputs, including stereo vision, various Lidars, Radars, and audio arrays.","It offers a unique overhead aerial detection vital for addressing real-world scenarios with higher fidelity than datasets captured on specific vantage points using thermal and RGB.","Additionally, MMAUD provides accurate Leica-generated ground truth data, enhancing credibility and enabling confident refinement of algorithms and models, which has never been seen in other datasets.","Most existing works do not disclose their datasets, making MMAUD an invaluable resource for developing accurate and efficient solutions.","Our proposed modalities are cost-effective and highly adaptable, allowing users to experiment and implement new UAV threat detection tools.","Our dataset closely simulates real-world scenarios by incorporating ambient heavy machinery sounds.","This approach enhances the dataset's applicability, capturing the exact challenges faced during proximate vehicular operations.","It is expected that MMAUD can play a pivotal role in advancing UAV threat detection, classification, trajectory estimation capabilities, and beyond.","Our dataset, codes, and designs will be available in https://github.com/ntu-aris/MMAUD."],"url":"http://arxiv.org/abs/2402.03706v1","category":"cs.RO"}
{"created":"2024-02-06 04:47:58","title":"WhisperFuzz: White-Box Fuzzing for Detecting and Locating Timing Vulnerabilities in Processors","abstract":"Timing vulnerabilities in processors have emerged as a potent threat. As processors are the foundation of any computing system, identifying these flaws is imperative. Recently fuzzing techniques, traditionally used for detecting software vulnerabilities, have shown promising results for uncovering vulnerabilities in large-scale hardware designs, such as processors. Researchers have adapted black-box or grey-box fuzzing to detect timing vulnerabilities in processors. However, they cannot identify the locations or root causes of these timing vulnerabilities, nor do they provide coverage feedback to enable the designer's confidence in the processor's security.   To address the deficiencies of the existing fuzzers, we present WhisperFuzz--the first white-box fuzzer with static analysis--aiming to detect and locate timing vulnerabilities in processors and evaluate the coverage of microarchitectural timing behaviors. WhisperFuzz uses the fundamental nature of processors' timing behaviors, microarchitectural state transitions, to localize timing vulnerabilities. WhisperFuzz automatically extracts microarchitectural state transitions from a processor design at the register-transfer level (RTL) and instruments the design to monitor the state transitions as coverage. Moreover, WhisperFuzz measures the time a design-under-test (DUT) takes to process tests, identifying any minor, abnormal variations that may hint at a timing vulnerability. WhisperFuzz detects 12 new timing vulnerabilities across advanced open-sourced RISC-V processors: BOOM, Rocket Core, and CVA6. Eight of these violate the zero latency requirements of the Zkt extension and are considered serious security vulnerabilities. Moreover, WhisperFuzz also pinpoints the locations of the new and the existing vulnerabilities.","sentences":["Timing vulnerabilities in processors have emerged as a potent threat.","As processors are the foundation of any computing system, identifying these flaws is imperative.","Recently fuzzing techniques, traditionally used for detecting software vulnerabilities, have shown promising results for uncovering vulnerabilities in large-scale hardware designs, such as processors.","Researchers have adapted black-box or grey-box fuzzing to detect timing vulnerabilities in processors.","However, they cannot identify the locations or root causes of these timing vulnerabilities, nor do they provide coverage feedback to enable the designer's confidence in the processor's security.   ","To address the deficiencies of the existing fuzzers, we present WhisperFuzz--the first white-box fuzzer with static analysis--aiming to detect and locate timing vulnerabilities in processors and evaluate the coverage of microarchitectural timing behaviors.","WhisperFuzz uses the fundamental nature of processors' timing behaviors, microarchitectural state transitions, to localize timing vulnerabilities.","WhisperFuzz automatically extracts microarchitectural state transitions from a processor design at the register-transfer level (RTL) and instruments the design to monitor the state transitions as coverage.","Moreover, WhisperFuzz measures the time a design-under-test (DUT) takes to process tests, identifying any minor, abnormal variations that may hint at a timing vulnerability.","WhisperFuzz detects 12 new timing vulnerabilities across advanced open-sourced RISC-V processors: BOOM, Rocket Core, and CVA6.","Eight of these violate the zero latency requirements of the Zkt extension and are considered serious security vulnerabilities.","Moreover, WhisperFuzz also pinpoints the locations of the new and the existing vulnerabilities."],"url":"http://arxiv.org/abs/2402.03704v1","category":"cs.CR"}
{"created":"2024-02-06 03:47:49","title":"ARGO: An Auto-Tuning Runtime System for Scalable GNN Training on Multi-Core Processor","abstract":"As Graph Neural Networks (GNNs) become popular, libraries like PyTorch-Geometric (PyG) and Deep Graph Library (DGL) are proposed; these libraries have emerged as the de facto standard for implementing GNNs because they provide graph-oriented APIs and are purposefully designed to manage the inherent sparsity and irregularity in graph structures. However, these libraries show poor scalability on multi-core processors, which under-utilizes the available platform resources and limits the performance. This is because GNN training is a resource-intensive workload with high volume of irregular data accessing, and existing libraries fail to utilize the memory bandwidth efficiently. To address this challenge, we propose ARGO, a novel runtime system for GNN training that offers scalable performance. ARGO exploits multi-processing and core-binding techniques to improve platform resource utilization. We further develop an auto-tuner that searches for the optimal configuration for multi-processing and core-binding. The auto-tuner works automatically, making it completely transparent from the user. Furthermore, the auto-tuner allows ARGO to adapt to various platforms, GNN models, datasets, etc. We evaluate ARGO on two representative GNN models and four widely-used datasets on two platforms. With the proposed autotuner, ARGO is able to select a near-optimal configuration by exploring only 5% of the design space. ARGO speeds up state-of-the-art GNN libraries by up to 5.06x and 4.54x on a four-socket Ice Lake machine with 112 cores and a two-socket Sapphire Rapids machine with 64 cores, respectively. Finally, ARGO can seamlessly integrate into widely-used GNN libraries (e.g., DGL, PyG) with few lines of code and speed up GNN training.","sentences":["As Graph Neural Networks (GNNs) become popular, libraries like PyTorch-Geometric (PyG) and Deep Graph Library (DGL) are proposed; these libraries have emerged as the de facto standard for implementing GNNs because they provide graph-oriented APIs and are purposefully designed to manage the inherent sparsity and irregularity in graph structures.","However, these libraries show poor scalability on multi-core processors, which under-utilizes the available platform resources and limits the performance.","This is because GNN training is a resource-intensive workload with high volume of irregular data accessing, and existing libraries fail to utilize the memory bandwidth efficiently.","To address this challenge, we propose ARGO, a novel runtime system for GNN training that offers scalable performance.","ARGO exploits multi-processing and core-binding techniques to improve platform resource utilization.","We further develop an auto-tuner that searches for the optimal configuration for multi-processing and core-binding.","The auto-tuner works automatically, making it completely transparent from the user.","Furthermore, the auto-tuner allows ARGO to adapt to various platforms, GNN models, datasets, etc.","We evaluate ARGO on two representative GNN models and four widely-used datasets on two platforms.","With the proposed autotuner, ARGO is able to select a near-optimal configuration by exploring only 5% of the design space.","ARGO speeds up state-of-the-art GNN libraries by up to 5.06x and 4.54x on a four-socket Ice Lake machine with 112 cores and a two-socket Sapphire Rapids machine with 64 cores, respectively.","Finally, ARGO can seamlessly integrate into widely-used GNN libraries (e.g., DGL, PyG) with few lines of code and speed up GNN training."],"url":"http://arxiv.org/abs/2402.03671v1","category":"cs.DC"}
{"created":"2024-02-06 03:39:44","title":"QuEST: Low-bit Diffusion Model Quantization via Efficient Selective Finetuning","abstract":"Diffusion models have achieved remarkable success in image generation tasks, yet their practical deployment is restrained by the high memory and time consumption. While quantization paves a way for diffusion model compression and acceleration, existing methods totally fail when the models are quantized to low-bits. In this paper, we unravel three properties in quantized diffusion models that compromise the efficacy of current methods: imbalanced activation distributions, imprecise temporal information, and vulnerability to perturbations of specific modules. To alleviate the intensified low-bit quantization difficulty stemming from the distribution imbalance, we propose finetuning the quantized model to better adapt to the activation distribution. Building on this idea, we identify two critical types of quantized layers: those holding vital temporal information and those sensitive to reduced bit-width, and finetune them to mitigate performance degradation with efficiency. We empirically verify that our approach modifies the activation distribution and provides meaningful temporal information, facilitating easier and more accurate quantization. Our method is evaluated over three high-resolution image generation tasks and achieves state-of-the-art performance under various bit-width settings, as well as being the first method to generate readable images on full 4-bit (i.e. W4A4) Stable Diffusion.","sentences":["Diffusion models have achieved remarkable success in image generation tasks, yet their practical deployment is restrained by the high memory and time consumption.","While quantization paves a way for diffusion model compression and acceleration, existing methods totally fail when the models are quantized to low-bits.","In this paper, we unravel three properties in quantized diffusion models that compromise the efficacy of current methods: imbalanced activation distributions, imprecise temporal information, and vulnerability to perturbations of specific modules.","To alleviate the intensified low-bit quantization difficulty stemming from the distribution imbalance, we propose finetuning the quantized model to better adapt to the activation distribution.","Building on this idea, we identify two critical types of quantized layers: those holding vital temporal information and those sensitive to reduced bit-width, and finetune them to mitigate performance degradation with efficiency.","We empirically verify that our approach modifies the activation distribution and provides meaningful temporal information, facilitating easier and more accurate quantization.","Our method is evaluated over three high-resolution image generation tasks and achieves state-of-the-art performance under various bit-width settings, as well as being the first method to generate readable images on full 4-bit (i.e. W4A4) Stable Diffusion."],"url":"http://arxiv.org/abs/2402.03666v1","category":"cs.CV"}
{"created":"2024-02-06 03:08:19","title":"PSO-Based Adaptive NMPC for Uranium Extraction-Scrubbing Operation in Spent Nuclear Fuel Treatment Process","abstract":"This paper addresses the particularities of adaptive optimal control of the uranium extraction-scrubbing operation in the PUREX process. The process dynamics are nonlinear, high dimensional, and have limited online measurements. In addition, analysis and developments are based on a qualified simulation program called PAREX, which was validated with laboratory and industrial data. The control objective is to stabilize the process at a desired solvent saturation level, guaranteeing constraints and handling disturbances. The developed control strategy relies on optimization-based methods for computing control inputs and estimates, i.e., Nonlinear Model Predictive Control (NMPC) and Nonlinear Moving Horizon Estimation (NMHE). The designs of these two associated algorithms are tailored for this process's particular dynamics and are implemented through an enhanced Particle Swarm Optimization (PSO) to guarantee constraint satisfaction. Software-in-the-loop simulations using PAREX show that the designed control scheme effectively satisfies control objectives and guarantees constraints during operation.","sentences":["This paper addresses the particularities of adaptive optimal control of the uranium extraction-scrubbing operation in the PUREX process.","The process dynamics are nonlinear, high dimensional, and have limited online measurements.","In addition, analysis and developments are based on a qualified simulation program called PAREX, which was validated with laboratory and industrial data.","The control objective is to stabilize the process at a desired solvent saturation level, guaranteeing constraints and handling disturbances.","The developed control strategy relies on optimization-based methods for computing control inputs and estimates, i.e., Nonlinear Model Predictive Control (NMPC) and Nonlinear Moving Horizon Estimation (NMHE).","The designs of these two associated algorithms are tailored for this process's particular dynamics and are implemented through an enhanced Particle Swarm Optimization (PSO) to guarantee constraint satisfaction.","Software-in-the-loop simulations using PAREX show that the designed control scheme effectively satisfies control objectives and guarantees constraints during operation."],"url":"http://arxiv.org/abs/2402.03656v1","category":"eess.SY"}
{"created":"2024-02-06 02:00:18","title":"CAT-SAM: Conditional Tuning Network for Few-Shot Adaptation of Segmentation Anything Model","abstract":"The recent Segment Anything Model (SAM) has demonstrated remarkable zero-shot capability and flexible geometric prompting in general image segmentation. However, SAM often struggles when handling various unconventional images, such as aerial, medical, and non-RGB images. This paper presents CAT-SAM, a ConditionAl Tuning network that adapts SAM toward various unconventional target tasks with just few-shot target samples. CAT-SAM freezes the entire SAM and adapts its mask decoder and image encoder simultaneously with a small number of learnable parameters. The core design is a prompt bridge structure that enables decoder-conditioned joint tuning of the heavyweight image encoder and the lightweight mask decoder. The bridging maps the prompt token of the mask decoder to the image encoder, fostering synergic adaptation of the encoder and the decoder with mutual benefits. We develop two representative tuning strategies for the image encoder which leads to two CAT-SAM variants: one injecting learnable prompt tokens in the input space and the other inserting lightweight adapter networks. Extensive experiments over 11 unconventional tasks show that both CAT-SAM variants achieve superior target segmentation performance consistently even under the very challenging one-shot adaptation setup. Project page: \\url{https://xiaoaoran.github.io/projects/CAT-SAM}","sentences":["The recent Segment Anything Model (SAM) has demonstrated remarkable zero-shot capability and flexible geometric prompting in general image segmentation.","However, SAM often struggles when handling various unconventional images, such as aerial, medical, and non-RGB images.","This paper presents CAT-SAM, a ConditionAl Tuning network that adapts SAM toward various unconventional target tasks with just few-shot target samples.","CAT-SAM freezes the entire SAM and adapts its mask decoder and image encoder simultaneously with a small number of learnable parameters.","The core design is a prompt bridge structure that enables decoder-conditioned joint tuning of the heavyweight image encoder and the lightweight mask decoder.","The bridging maps the prompt token of the mask decoder to the image encoder, fostering synergic adaptation of the encoder and the decoder with mutual benefits.","We develop two representative tuning strategies for the image encoder which leads to two CAT-SAM variants: one injecting learnable prompt tokens in the input space and the other inserting lightweight adapter networks.","Extensive experiments over 11 unconventional tasks show that both CAT-SAM variants achieve superior target segmentation performance consistently even under the very challenging one-shot adaptation setup.","Project page: \\url{https://xiaoaoran.github.io/projects/CAT-SAM}"],"url":"http://arxiv.org/abs/2402.03631v1","category":"cs.CV"}
{"created":"2024-02-06 01:02:14","title":"Vortex Stretching of Non-premixed, Diluted Hydrogen/Oxygen Flamelets","abstract":"A three-dimensional flamelet model considering vortex stretching with unitary Lewis number is used to simulate diluted hydrogen-oxygen diffusion flames. Non-reacting nitrogen is used as the diluent gas in the fuel stream. Unitary Lewis number provides a common thermal and mass diffusivity from which to create scalar dissipation rate. Both stable and unstable branches of flammability curves (S-curves) are calculated with three vorticity levels and plotted against multiple input and output parameters. The description of the three-dimensional flamelet structure, allowing vorticity and variable density to produce a centrifugal effect, is seen to be necessary for an accurate determination of the water burning rate. Both maximum temperature and integrated water burning rate nearly collapse to a single curve when plotted versus maximum scalar dissipation rate but do not collapse when plotted versus the local maximum strain rate or the ambient strain rate; however, local strain rate and scalar dissipation rate depend strongly on vorticity and ambient strain rate. It is argued that the controlling inputs and natural parameters for a flamelet embedded in a turbulent eddy are the ambient vorticity and strain rate which are thus the natural parameterizing variables as opposed to local strain rate or scalar dissipation rate within the flame zone. Furthermore, established practices of the scaling of velocity derivatives exist in the literature while scaling laws for scalar gradients are sparse and unsubstantiated, further supporting the use of ambient parameterizing quantities.","sentences":["A three-dimensional flamelet model considering vortex stretching with unitary Lewis number is used to simulate diluted hydrogen-oxygen diffusion flames.","Non-reacting nitrogen is used as the diluent gas in the fuel stream.","Unitary Lewis number provides a common thermal and mass diffusivity from which to create scalar dissipation rate.","Both stable and unstable branches of flammability curves (S-curves) are calculated with three vorticity levels and plotted against multiple input and output parameters.","The description of the three-dimensional flamelet structure, allowing vorticity and variable density to produce a centrifugal effect, is seen to be necessary for an accurate determination of the water burning rate.","Both maximum temperature and integrated water burning rate nearly collapse to a single curve when plotted versus maximum scalar dissipation rate but do not collapse when plotted versus the local maximum strain rate or the ambient strain rate; however, local strain rate and scalar dissipation rate depend strongly on vorticity and ambient strain rate.","It is argued that the controlling inputs and natural parameters for a flamelet embedded in a turbulent eddy are the ambient vorticity and strain rate which are thus the natural parameterizing variables as opposed to local strain rate or scalar dissipation rate within the flame zone.","Furthermore, established practices of the scaling of velocity derivatives exist in the literature while scaling laws for scalar gradients are sparse and unsubstantiated, further supporting the use of ambient parameterizing quantities."],"url":"http://arxiv.org/abs/2402.03615v1","category":"physics.flu-dyn"}
{"created":"2024-02-06 00:16:01","title":"A Review on Internet of Things for Defense and Public Safety","abstract":"The Internet of Things (IoT) is undeniably transforming the way that organizations communicate and organize everyday businesses and industrial procedures. Its adoption has proven well suited for sectors that manage a large number of assets and coordinate complex and distributed processes. This survey analyzes the great potential for applying IoT technologies (i.e., data-driven applications or embedded automation and intelligent adaptive systems) to revolutionize modern warfare and provide benefits similar to those in industry. It identifies scenarios where Defense and Public Safety (PS) could leverage better commercial IoT capabilities to deliver greater survivability to the warfighter or first responders, while reducing costs and increasing operation efficiency and effectiveness. This article reviews the main tactical requirements and the architecture, examining gaps and shortcomings in existing IoT systems across the military field and mission-critical scenarios. The review characterizes the open challenges for a broad deployment and presents a research roadmap for enabling an affordable IoT for defense and PS.","sentences":["The Internet of Things (IoT) is undeniably transforming the way that organizations communicate and organize everyday businesses and industrial procedures.","Its adoption has proven well suited for sectors that manage a large number of assets and coordinate complex and distributed processes.","This survey analyzes the great potential for applying IoT technologies (i.e., data-driven applications or embedded automation and intelligent adaptive systems) to revolutionize modern warfare and provide benefits similar to those in industry.","It identifies scenarios where Defense and Public Safety (PS) could leverage better commercial IoT capabilities to deliver greater survivability to the warfighter or first responders, while reducing costs and increasing operation efficiency and effectiveness.","This article reviews the main tactical requirements and the architecture, examining gaps and shortcomings in existing IoT systems across the military field and mission-critical scenarios.","The review characterizes the open challenges for a broad deployment and presents a research roadmap for enabling an affordable IoT for defense and PS."],"url":"http://arxiv.org/abs/2402.03599v1","category":"eess.SY"}
{"created":"2024-02-06 00:10:28","title":"Upgrading the GRAVITY fringe tracker for GRAVITY+: Tracking the white light fringe in the non-observable Optical Path Length state-space","abstract":"Aims. As part of the ongoing GRAVITY+ upgrade of the Very Large Telescope Interferometer infrastructure, we aim to improve the performance of the GRAVITY Fringe-Tracker, and to enable its use by other instruments. Methods. We modify the group delay controller to consistently maintain tracking in the white light fringe, characterised by a minimum group delay. Additionally, we introduce a novel approach in which fringe-tracking is performed in the non-observable Optical Path Length state-space, using a covariance-weighted Kalman filter and an auto-regressive model of the disturbance. We outline this new state-space representation, and the formalism we use to propagate the state-vector and generate the control signal. While our approach is presented specifically in the context of GRAVITY/GRAVITY+, it can easily be adapted to other instruments or interferometric facilities. Results. We successfully demonstrate phase delay tracking within a single fringe, with any spurious phase jumps detected and corrected in less than 100 ms. We also report a significant performance improvement, as evidenced by a reduction of about 30 to 40% in phase residuals, and a much better behaviour under sub-optimal atmospheric conditions. Compared to what was observed in 2019, the median residuals have decreased from 150 nm to 100 nm on the Auxiliary Telescopes and from 250 nm to 150 nm on the Unit Telescopes. Conclusions. The improved phase-delay tracking combined with whit light fringe tracking means that from now-on, the GRAVITY Fringe-Tracker can be used by other instruments operating in different wavebands. The only limitation remains the need for an optical path dispersion adjustment.","sentences":["Aims.","As part of the ongoing GRAVITY+ upgrade of the Very Large Telescope Interferometer infrastructure, we aim to improve the performance of the GRAVITY Fringe-Tracker, and to enable its use by other instruments.","Methods.","We modify the group delay controller to consistently maintain tracking in the white light fringe, characterised by a minimum group delay.","Additionally, we introduce a novel approach in which fringe-tracking is performed in the non-observable Optical Path Length state-space, using a covariance-weighted Kalman filter and an auto-regressive model of the disturbance.","We outline this new state-space representation, and the formalism we use to propagate the state-vector and generate the control signal.","While our approach is presented specifically in the context of GRAVITY/GRAVITY+, it can easily be adapted to other instruments or interferometric facilities.","Results.","We successfully demonstrate phase delay tracking within a single fringe, with any spurious phase jumps detected and corrected in less than 100 ms.","We also report a significant performance improvement, as evidenced by a reduction of about 30 to 40% in phase residuals, and a much better behaviour under sub-optimal atmospheric conditions.","Compared to what was observed in 2019, the median residuals have decreased from 150 nm to 100 nm on the Auxiliary Telescopes and from 250 nm to 150 nm on the Unit Telescopes.","Conclusions.","The improved phase-delay tracking combined with whit light fringe tracking means that from now-on, the GRAVITY Fringe-Tracker can be used by other instruments operating in different wavebands.","The only limitation remains the need for an optical path dispersion adjustment."],"url":"http://arxiv.org/abs/2402.03594v1","category":"astro-ph.IM"}
{"created":"2024-02-05 23:46:03","title":"Continual Domain Adversarial Adaptation via Double-Head Discriminators","abstract":"Domain adversarial adaptation in a continual setting poses a significant challenge due to the limitations on accessing previous source domain data. Despite extensive research in continual learning, the task of adversarial adaptation cannot be effectively accomplished using only a small number of stored source domain data, which is a standard setting in memory replay approaches. This limitation arises from the erroneous empirical estimation of $\\gH$-divergence with few source domain samples. To tackle this problem, we propose a double-head discriminator algorithm, by introducing an addition source-only domain discriminator that are trained solely on source learning phase. We prove that with the introduction of a pre-trained source-only domain discriminator, the empirical estimation error of $\\gH$-divergence related adversarial loss is reduced from the source domain side. Further experiments on existing domain adaptation benchmark show that our proposed algorithm achieves more than 2$\\%$ improvement on all categories of target domain adaptation task while significantly mitigating the forgetting on source domain.","sentences":["Domain adversarial adaptation in a continual setting poses a significant challenge due to the limitations on accessing previous source domain data.","Despite extensive research in continual learning, the task of adversarial adaptation cannot be effectively accomplished using only a small number of stored source domain data, which is a standard setting in memory replay approaches.","This limitation arises from the erroneous empirical estimation of $\\gH$-divergence with few source domain samples.","To tackle this problem, we propose a double-head discriminator algorithm, by introducing an addition source-only domain discriminator that are trained solely on source learning phase.","We prove that with the introduction of a pre-trained source-only domain discriminator, the empirical estimation error of $\\gH$-divergence related adversarial loss is reduced from the source domain side.","Further experiments on existing domain adaptation benchmark show that our proposed algorithm achieves more than 2$\\%$ improvement on all categories of target domain adaptation task while significantly mitigating the forgetting on source domain."],"url":"http://arxiv.org/abs/2402.03588v1","category":"cs.LG"}
{"created":"2024-02-05 23:13:06","title":"MINLP-based hybrid strategy for operating mode selection of TES-backed-up refrigeration systems","abstract":"This brief deals with the satisfaction of the daily cooling demand by a hybrid system that consists of a vapour-compression refrigeration cycle and a thermal energy storage (TES) unit, based on phase change materials. The addition of the TES tank to the original refrigeration plant allows to schedule the cooling production regardless of the instantaneous demand, given that the TES tank can store cold energy and release it whenever deemed appropriate. The scheduling problem is posed as an optimization problem based on mixed-integer non-linear programming (MINLP), since it includes both discrete and continuous variables. The latter corresponds to the references on the main cooling powers involved in the problem (cooling production at the evaporator and TES charging/discharging), whereas the discrete variables define the operating mode scheduling. Therefore, in addition to the hybrid features of the physical plant, a hybrid optimal control strategy is also proposed. A receding horizon approach is applied, similar to model predictive control (MPC) strategies, while economic criteria are imposed in the objective function, as well as feasibility issues. The TES state estimation is also addressed, since its instantaneous charge ratio is not measurable. The proposed strategy is applied in simulation to a challenging cooling demand profile and the main advantages of the MINLP-based strategy over a non-linear MPC-based scheduling strategy previously developed are highlighted, regarding operating cost, ease of tuning, and ability to adapt to cooling demand variations.","sentences":["This brief deals with the satisfaction of the daily cooling demand by a hybrid system that consists of a vapour-compression refrigeration cycle and a thermal energy storage (TES) unit, based on phase change materials.","The addition of the TES tank to the original refrigeration plant allows to schedule the cooling production regardless of the instantaneous demand, given that the TES tank can store cold energy and release it whenever deemed appropriate.","The scheduling problem is posed as an optimization problem based on mixed-integer non-linear programming (MINLP), since it includes both discrete and continuous variables.","The latter corresponds to the references on the main cooling powers involved in the problem (cooling production at the evaporator and TES charging/discharging), whereas the discrete variables define the operating mode scheduling.","Therefore, in addition to the hybrid features of the physical plant, a hybrid optimal control strategy is also proposed.","A receding horizon approach is applied, similar to model predictive control (MPC) strategies, while economic criteria are imposed in the objective function, as well as feasibility issues.","The TES state estimation is also addressed, since its instantaneous charge ratio is not measurable.","The proposed strategy is applied in simulation to a challenging cooling demand profile and the main advantages of the MINLP-based strategy over a non-linear MPC-based scheduling strategy previously developed are highlighted, regarding operating cost, ease of tuning, and ability to adapt to cooling demand variations."],"url":"http://arxiv.org/abs/2402.03580v2","category":"math.OC"}
{"created":"2024-02-05 22:29:51","title":"Breakpoint based online anomaly detection","abstract":"The goal of anomaly detection is to identify observations that are generated by a distribution that differs from the reference distribution that qualifies normal behavior. When examining a time series, the reference distribution may evolve over time. The anomaly detector must therefore be able to adapt to such changes. In the online context, it is particularly difficult to adapt to abrupt and unpredictable changes. Our solution to this problem is based on the detection of breakpoints in order to adapt in real time to the new reference behavior of the series and to increase the accuracy of the anomaly detection. This solution also provides a control of the False Discovery Rate by extending methods developed for stationary series.","sentences":["The goal of anomaly detection is to identify observations that are generated by a distribution that differs from the reference distribution that qualifies normal behavior.","When examining a time series, the reference distribution may evolve over time.","The anomaly detector must therefore be able to adapt to such changes.","In the online context, it is particularly difficult to adapt to abrupt and unpredictable changes.","Our solution to this problem is based on the detection of breakpoints in order to adapt in real time to the new reference behavior of the series and to increase the accuracy of the anomaly detection.","This solution also provides a control of the False Discovery Rate by extending methods developed for stationary series."],"url":"http://arxiv.org/abs/2402.03565v1","category":"stat.ME"}
{"created":"2024-02-05 22:20:19","title":"VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation","abstract":"Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with three proxy tasks: Masked Language Modeling, Instruction and Trajectory Matching, and Next Action Prediction, so as to learn temporally-aware and visually-aligned instruction representations. The learned instruction representation is adapted to the state-of-the-art navigator when fine-tuning on the Touchdown dataset. Empirical results demonstrate that VLN-Video significantly outperforms previous state-of-the-art models by 2.1% in task completion rate, achieving a new state-of-the-art on the Touchdown dataset.","sentences":["Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions.","The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data.","To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance.","VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models.","We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with three proxy tasks:","Masked Language Modeling, Instruction and Trajectory Matching, and Next Action Prediction, so as to learn temporally-aware and visually-aligned instruction representations.","The learned instruction representation is adapted to the state-of-the-art navigator when fine-tuning on the Touchdown dataset.","Empirical results demonstrate that VLN-Video significantly outperforms previous state-of-the-art models by 2.1% in task completion rate, achieving a new state-of-the-art on the Touchdown dataset."],"url":"http://arxiv.org/abs/2402.03561v2","category":"cs.CV"}
{"created":"2024-02-05 22:03:25","title":"Online Feature Updates Improve Online (Generalized) Label Shift Adaptation","abstract":"This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging. While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we explore the untapped potential of enhancing feature representations using unlabeled data at test-time. Our novel method, Online Label Shift adaptation with Online Feature Updates (OLS-OFU), leverages self-supervised learning to refine the feature extraction process, thereby improving the prediction model. Theoretical analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on self-supervised learning for feature refinement. Empirical studies on various datasets, under both online label shift and generalized label shift conditions, underscore the effectiveness and robustness of OLS-OFU, especially in cases of domain shifts.","sentences":["This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging.","While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we explore the untapped potential of enhancing feature representations using unlabeled data at test-time.","Our novel method, Online Label Shift adaptation with Online Feature Updates (OLS-OFU), leverages self-supervised learning to refine the feature extraction process, thereby improving the prediction model.","Theoretical analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on self-supervised learning for feature refinement.","Empirical studies on various datasets, under both online label shift and generalized label shift conditions, underscore the effectiveness and robustness of OLS-OFU, especially in cases of domain shifts."],"url":"http://arxiv.org/abs/2402.03545v1","category":"cs.LG"}
{"created":"2024-02-05 21:55:24","title":"HAMLET: Graph Transformer Neural Operator for Partial Differential Equations","abstract":"We present a novel graph transformer framework, HAMLET, designed to address the challenges in solving partial differential equations (PDEs) using neural networks. The framework uses graph transformers with modular input encoders to directly incorporate differential equation information into the solution process. This modularity enhances parameter correspondence control, making HAMLET adaptable to PDEs of arbitrary geometries and varied input formats. Notably, HAMLET scales effectively with increasing data complexity and noise, showcasing its robustness. HAMLET is not just tailored to a single type of physical simulation, but can be applied across various domains. Moreover, it boosts model resilience and performance, especially in scenarios with limited data. We demonstrate, through extensive experiments, that our framework is capable of outperforming current techniques for PDEs.","sentences":["We present a novel graph transformer framework, HAMLET, designed to address the challenges in solving partial differential equations (PDEs) using neural networks.","The framework uses graph transformers with modular input encoders to directly incorporate differential equation information into the solution process.","This modularity enhances parameter correspondence control, making HAMLET adaptable to PDEs of arbitrary geometries and varied input formats.","Notably, HAMLET scales effectively with increasing data complexity and noise, showcasing its robustness.","HAMLET is not just tailored to a single type of physical simulation, but can be applied across various domains.","Moreover, it boosts model resilience and performance, especially in scenarios with limited data.","We demonstrate, through extensive experiments, that our framework is capable of outperforming current techniques for PDEs."],"url":"http://arxiv.org/abs/2402.03541v1","category":"cs.LG"}
{"created":"2024-02-05 21:37:03","title":"Basic principles drive self-organization of brain-like connectivity structure","abstract":"The brain can be considered as a system that dynamically optimizes the structure of anatomical connections based on the efficiency requirements of functional connectivity. To illustrate the power of this principle in organizing the complexity of brain architecture, we portray the functional connectivity as diffusion on the current network structure. The diffusion drives adaptive rewiring, resulting in changes to the network to enhance its efficiency. This dynamic evolution of the network structure generates, and thus explains, modular small-worlds with rich club effects, f eatures commonly observed in neural anatomy. Taking wiring length and propagating waves into account leads to the morphogenesis of more specific neural structures that are stalwarts of the detailed brain functional anatomy, such as parallelism, divergence, convergence, super-rings, and super-chains. By showing how such structures emerge, largely independently of their specific biological realization, we offer a new conjecture on how natural and artificial brain-like structures can be physically implemented.","sentences":["The brain can be considered as a system that dynamically optimizes the structure of anatomical connections based on the efficiency requirements of functional connectivity.","To illustrate the power of this principle in organizing the complexity of brain architecture, we portray the functional connectivity as diffusion on the current network structure.","The diffusion drives adaptive rewiring, resulting in changes to the network to enhance its efficiency.","This dynamic evolution of the network structure generates, and thus explains, modular small-worlds with rich club effects, f eatures commonly observed in neural anatomy.","Taking wiring length and propagating waves into account leads to the morphogenesis of more specific neural structures that are stalwarts of the detailed brain functional anatomy, such as parallelism, divergence, convergence, super-rings, and super-chains.","By showing how such structures emerge, largely independently of their specific biological realization, we offer a new conjecture on how natural and artificial brain-like structures can be physically implemented."],"url":"http://arxiv.org/abs/2402.03529v1","category":"q-bio.NC"}
{"created":"2024-02-05 21:33:22","title":"Consistent Validation for Predictive Methods in Spatial Settings","abstract":"Spatial prediction tasks are key to weather forecasting, studying air pollution, and other scientific endeavors. Determining how much to trust predictions made by statistical or physical methods is essential for the credibility of scientific conclusions. Unfortunately, classical approaches for validation fail to handle mismatch between locations available for validation and (test) locations where we want to make predictions. This mismatch is often not an instance of covariate shift (as commonly formalized) because the validation and test locations are fixed (e.g., on a grid or at select points) rather than i.i.d. from two distributions. In the present work, we formalize a check on validation methods: that they become arbitrarily accurate as validation data becomes arbitrarily dense. We show that classical and covariate-shift methods can fail this check. We instead propose a method that builds from existing ideas in the covariate-shift literature, but adapts them to the validation data at hand. We prove that our proposal passes our check. And we demonstrate its advantages empirically on simulated and real data.","sentences":["Spatial prediction tasks are key to weather forecasting, studying air pollution, and other scientific endeavors.","Determining how much to trust predictions made by statistical or physical methods is essential for the credibility of scientific conclusions.","Unfortunately, classical approaches for validation fail to handle mismatch between locations available for validation and (test) locations where we want to make predictions.","This mismatch is often not an instance of covariate shift (as commonly formalized) because the validation and test locations are fixed (e.g., on a grid or at select points) rather than i.i.d.","from two distributions.","In the present work, we formalize a check on validation methods: that they become arbitrarily accurate as validation data becomes arbitrarily dense.","We show that classical and covariate-shift methods can fail this check.","We instead propose a method that builds from existing ideas in the covariate-shift literature, but adapts them to the validation data at hand.","We prove that our proposal passes our check.","And we demonstrate its advantages empirically on simulated and real data."],"url":"http://arxiv.org/abs/2402.03527v1","category":"stat.ML"}
{"created":"2024-02-05 21:01:01","title":"Video Super-Resolution for Optimized Bitrate and Green Online Streaming","abstract":"Conventional per-title encoding schemes strive to optimize encoding resolutions to deliver the utmost perceptual quality for each bitrate ladder representation. Nevertheless, maintaining encoding time within an acceptable threshold is equally imperative in online streaming applications. Furthermore, modern client devices are equipped with the capability for fast deep-learning-based video super-resolution (VSR) techniques, enhancing the perceptual quality of the decoded bitstream. This suggests that opting for lower resolutions in representations during the encoding process can curtail the overall energy consumption without substantially compromising perceptual quality. In this context, this paper introduces a video super-resolution-based latency-aware optimized bitrate encoding scheme (ViSOR) designed for online adaptive streaming applications. ViSOR determines the encoding resolution for each target bitrate, ensuring the highest achievable perceptual quality after VSR within the bound of a maximum acceptable latency. Random forest-based prediction models are trained to predict the perceptual quality after VSR and the encoding time for each resolution using the spatiotemporal features extracted for each video segment. Experimental results show that ViSOR targeting fast super-resolution convolutional neural network (FSRCNN) achieves an overall average bitrate reduction of 24.65 % and 32.70 % to maintain the same PSNR and VMAF, compared to the HTTP Live Streaming (HLS) bitrate ladder encoding of 4 s segments using the x265 encoder, when the maximum acceptable latency for each representation is set as two seconds. Considering a just noticeable difference (JND) of six VMAF points, the average cumulative storage consumption and encoding energy for each segment is reduced by 79.32 % and 68.21 %, respectively, contributing towards greener streaming.","sentences":["Conventional per-title encoding schemes strive to optimize encoding resolutions to deliver the utmost perceptual quality for each bitrate ladder representation.","Nevertheless, maintaining encoding time within an acceptable threshold is equally imperative in online streaming applications.","Furthermore, modern client devices are equipped with the capability for fast deep-learning-based video super-resolution (VSR) techniques, enhancing the perceptual quality of the decoded bitstream.","This suggests that opting for lower resolutions in representations during the encoding process can curtail the overall energy consumption without substantially compromising perceptual quality.","In this context, this paper introduces a video super-resolution-based latency-aware optimized bitrate encoding scheme (ViSOR) designed for online adaptive streaming applications.","ViSOR determines the encoding resolution for each target bitrate, ensuring the highest achievable perceptual quality after VSR within the bound of a maximum acceptable latency.","Random forest-based prediction models are trained to predict the perceptual quality after VSR and the encoding time for each resolution using the spatiotemporal features extracted for each video segment.","Experimental results show that ViSOR targeting fast super-resolution convolutional neural network (FSRCNN) achieves an overall average bitrate reduction of 24.65 % and 32.70 % to maintain the same PSNR and VMAF, compared to the HTTP Live Streaming (HLS) bitrate ladder encoding of 4 s segments using the x265 encoder, when the maximum acceptable latency for each representation is set as two seconds.","Considering a just noticeable difference (JND) of six VMAF points, the average cumulative storage consumption and encoding energy for each segment is reduced by 79.32 % and 68.21 %, respectively, contributing towards greener streaming."],"url":"http://arxiv.org/abs/2402.03513v1","category":"cs.MM"}
{"created":"2024-02-05 20:55:10","title":"Autopilot System for Depth and Pitch Control in Underwater Vehicles: Navigating Near-Surface Waves and Disturbances","abstract":"This paper introduces a framework for depth and pitch control of underwater vehicles in near-surface wave conditions. By effectively managing tail, sail plane angles and hover tank operations utilizing a Linear Quadratic Regulator controller and L1 Adaptive Autopilot augmentation, the system ensures balanced control input distribution and significantly attenuates wave disturbances. This development in underwater vehicle control systems offers potential for improved functionality across a range of marine applications. The proposed framework is demonstrated to be robust in a variety of wave conditions, enabling more precise navigation and improved safety in operational scenarios. The effectiveness of this control strategy is validated through extensive simulations using the Joubert BB2 model.","sentences":["This paper introduces a framework for depth and pitch control of underwater vehicles in near-surface wave conditions.","By effectively managing tail, sail plane angles and hover tank operations utilizing a Linear Quadratic Regulator controller and L1 Adaptive Autopilot augmentation, the system ensures balanced control input distribution and significantly attenuates wave disturbances.","This development in underwater vehicle control systems offers potential for improved functionality across a range of marine applications.","The proposed framework is demonstrated to be robust in a variety of wave conditions, enabling more precise navigation and improved safety in operational scenarios.","The effectiveness of this control strategy is validated through extensive simulations using the Joubert BB2 model."],"url":"http://arxiv.org/abs/2402.03510v1","category":"eess.SY"}
{"created":"2024-02-05 20:48:57","title":"Neural networks for abstraction and reasoning: Towards broad generalization in machines","abstract":"For half a century, artificial intelligence research has attempted to reproduce the human qualities of abstraction and reasoning - creating computer systems that can learn new concepts from a minimal set of examples, in settings where humans find this easy. While specific neural networks are able to solve an impressive range of problems, broad generalisation to situations outside their training data has proved elusive.In this work, we look at several novel approaches for solving the Abstraction & Reasoning Corpus (ARC), a dataset of abstract visual reasoning tasks introduced to test algorithms on broad generalization. Despite three international competitions with $100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks and rely on complex hand-crafted rules, without using machine learning at all. We revisit whether recent advances in neural networks allow progress on this task.   First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC. DreamCoder automatically writes programs in a bespoke domain-specific language to perform reasoning, using a neural network to mimic human intuition. We present the Perceptual Abstraction and Reasoning Language (PeARL) language, which allows DreamCoder to solve ARC tasks, and propose a new recognition model that allows us to significantly improve on the previous best implementation.We also propose a new encoding and augmentation scheme that allows large language models (LLMs) to solve ARC tasks, and find that the largest models can solve some ARC tasks. LLMs are able to solve a different group of problems to state-of-the-art solvers, and provide an interesting way to complement other approaches. We perform an ensemble analysis, combining models to achieve better results than any system alone. Finally, we publish the arckit Python library to make future research on ARC easier.","sentences":["For half a century, artificial intelligence research has attempted to reproduce the human qualities of abstraction and reasoning - creating computer systems that can learn new concepts from a minimal set of examples, in settings where humans find this easy.","While specific neural networks are able to solve an impressive range of problems, broad generalisation to situations outside their training data has proved elusive.","In this work, we look at several novel approaches for solving the Abstraction & Reasoning Corpus (ARC), a dataset of abstract visual reasoning tasks introduced to test algorithms on broad generalization.","Despite three international competitions with $100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks and rely on complex hand-crafted rules, without using machine learning at all.","We revisit whether recent advances in neural networks allow progress on this task.   ","First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC.","DreamCoder automatically writes programs in a bespoke domain-specific language to perform reasoning, using a neural network to mimic human intuition.","We present the Perceptual Abstraction and Reasoning Language (PeARL) language, which allows DreamCoder to solve ARC tasks, and propose a new recognition model that allows us to significantly improve on the previous best implementation.","We also propose a new encoding and augmentation scheme that allows large language models (LLMs) to solve ARC tasks, and find that the largest models can solve some ARC tasks.","LLMs are able to solve a different group of problems to state-of-the-art solvers, and provide an interesting way to complement other approaches.","We perform an ensemble analysis, combining models to achieve better results than any system alone.","Finally, we publish the arckit Python library to make future research on ARC easier."],"url":"http://arxiv.org/abs/2402.03507v1","category":"cs.AI"}
{"created":"2024-02-05 20:25:39","title":"An Analytic Solution for Kernel Adaptive Filtering","abstract":"Conventional kernel adaptive filtering (KAF) uses a prescribed, positive definite, nonlinear function to define the Reproducing Kernel Hilbert Space (RKHS), where the optimal solution for mean square error estimation is approximated using search techniques. Instead, this paper proposes to embed the full statistics of the input data in the kernel definition, obtaining the first analytical solution for nonlinear regression and nonlinear adaptive filtering applications. We call this solution the Functional Wiener Filter (FWF). Conceptually, the methodology is an extension of Parzen's work on the autocorrelation RKHS to nonlinear functional spaces. We provide an extended functional Wiener equation, and present a solution to this equation in an explicit, finite dimensional, data-dependent RKHS. We further explain the necessary requirements to compute the analytical solution in RKHS, which is beyond traditional methodologies based on the kernel trick. The FWF analytic solution to the nonlinear minimum mean square error problem has better accuracy than other kernel-based algorithms in synthetic, stationary data. In real world time series, it has comparable accuracy to KAF but displays constant complexity with respect to number of training samples. For evaluation, it is as computationally efficient as the Wiener solution (with a larger number of dimensions than the linear case). We also show how the difference equation learned by the FWF from data can be extracted leading to system identification applications, which extend the possible applications of the FWF beyond optimal nonlinear filtering.","sentences":["Conventional kernel adaptive filtering (KAF) uses a prescribed, positive definite, nonlinear function to define the Reproducing Kernel Hilbert Space (RKHS), where the optimal solution for mean square error estimation is approximated using search techniques.","Instead, this paper proposes to embed the full statistics of the input data in the kernel definition, obtaining the first analytical solution for nonlinear regression and nonlinear adaptive filtering applications.","We call this solution the Functional Wiener Filter (FWF).","Conceptually, the methodology is an extension of Parzen's work on the autocorrelation RKHS to nonlinear functional spaces.","We provide an extended functional Wiener equation, and present a solution to this equation in an explicit, finite dimensional, data-dependent RKHS.","We further explain the necessary requirements to compute the analytical solution in RKHS, which is beyond traditional methodologies based on the kernel trick.","The FWF analytic solution to the nonlinear minimum mean square error problem has better accuracy than other kernel-based algorithms in synthetic, stationary data.","In real world time series, it has comparable accuracy to KAF but displays constant complexity with respect to number of training samples.","For evaluation, it is as computationally efficient as the Wiener solution (with a larger number of dimensions than the linear case).","We also show how the difference equation learned by the FWF from data can be extracted leading to system identification applications, which extend the possible applications of the FWF beyond optimal nonlinear filtering."],"url":"http://arxiv.org/abs/2402.03497v1","category":"eess.SP"}
{"created":"2024-02-05 20:15:19","title":"Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective","abstract":"Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as transformers. Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root. While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference. In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation. Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to SGD on convolutional architectures, while maintaining their root-based counterpart's performance on transformers. The second-order perspective also has practical benefits for the development of adaptive methods with non-diagonal preconditioner. In contrast to root-based counterparts like Shampoo, they do not require numerically unstable matrix square roots and therefore work well in low precision, which we demonstrate empirically. This raises important questions regarding the currently overlooked role of adaptivity for the success of adaptive methods.","sentences":["Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as transformers.","Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root.","While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference.","In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation.","Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to SGD on convolutional architectures, while maintaining their root-based counterpart's performance on transformers.","The second-order perspective also has practical benefits for the development of adaptive methods with non-diagonal preconditioner.","In contrast to root-based counterparts like Shampoo, they do not require numerically unstable matrix square roots and therefore work well in low precision, which we demonstrate empirically.","This raises important questions regarding the currently overlooked role of adaptivity for the success of adaptive methods."],"url":"http://arxiv.org/abs/2402.03496v1","category":"cs.LG"}
{"created":"2024-02-05 19:47:45","title":"ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design","abstract":"Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when they share characteristics with the environments they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the implicit regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which have more control over the data generation mechanism. We find that existing UED methods can significantly shift the training distribution, which translates to low ZSG performance. To prevent both overfitting and distributional shift, we introduce in-context environment design (ICED). ICED generates levels using a variational autoencoder trained over an initial set of level parameters, reducing distributional shift, and achieves significant improvements in ZSG over adaptive level sampling strategies and UED methods.","sentences":["Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when they share characteristics with the environments they have encountered during training.","In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents.","We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data.","This provides a novel theoretical justification for the implicit regularisation achieved by certain adaptive sampling strategies.","We then turn our attention to unsupervised environment design (UED) methods, which have more control over the data generation mechanism.","We find that existing UED methods can significantly shift the training distribution, which translates to low ZSG performance.","To prevent both overfitting and distributional shift, we introduce in-context environment design (ICED).","ICED generates levels using a variational autoencoder trained over an initial set of level parameters, reducing distributional shift, and achieves significant improvements in ZSG over adaptive level sampling strategies and UED methods."],"url":"http://arxiv.org/abs/2402.03479v1","category":"cs.LG"}
{"created":"2024-02-05 19:09:16","title":"Median and Small Parsimony Problems on RNA trees","abstract":"Motivation: Non-coding RNAs (ncRNAs) express their functions by adopting molecular structures. Specifically, RNA secondary structures serve as a relatively stable intermediate step before tertiary structures, offering a reliable signature of molecular function. Consequently, within an RNA functional family, secondary structures are generally more evolutionarily conserved than sequences. Conversely, homologous RNA families grouped within an RNA clan share ancestors but typically exhibit structural differences. Inferring the evolution of RNA structures within RNA families and clans is crucial for gaining insights into functional adaptations over time and providing clues about the Ancient RNA World Hypothesis. Results: We introduce the median problem and the small parsimony problem for ncRNA families, where secondary structures are represented as leaf-labelled trees. We utilize the Robinson-Foulds (RF) tree distance, which corresponds to a specific edit distance between RNA trees, and a new metric called the Internal-Leafset (IL) distance. While the RF tree distance compares sets of leaves descending from internal nodes of two RNA trees, the IL distance compares the collection of leaf-children of internal nodes. The latter is better at capturing differences in structural elements of RNAs than the RF distance, which is more focused on base pairs. We also consider a more general tree edit distance that allows the mapping of base pairs that are not perfectly aligned. We study the theoretical complexity of the median problem and the small parsimony problem under the three distance metrics and various biologically-relevant constraints, and we present polynomial-time maximum parsimony algorithms for solving some versions of the problems. Our algorithms are applied to ncRNA families from the RFAM database, illustrating their practical utility","sentences":["Motivation: Non-coding RNAs (ncRNAs) express their functions by adopting molecular structures.","Specifically, RNA secondary structures serve as a relatively stable intermediate step before tertiary structures, offering a reliable signature of molecular function.","Consequently, within an RNA functional family, secondary structures are generally more evolutionarily conserved than sequences.","Conversely, homologous RNA families grouped within an RNA clan share ancestors but typically exhibit structural differences.","Inferring the evolution of RNA structures within RNA families and clans is crucial for gaining insights into functional adaptations over time and providing clues about the Ancient RNA World Hypothesis.","Results: We introduce the median problem and the small parsimony problem for ncRNA families, where secondary structures are represented as leaf-labelled trees.","We utilize the Robinson-Foulds (RF) tree distance, which corresponds to a specific edit distance between RNA trees, and a new metric called the Internal-Leafset (IL) distance.","While the RF tree distance compares sets of leaves descending from internal nodes of two RNA trees, the IL distance compares the collection of leaf-children of internal nodes.","The latter is better at capturing differences in structural elements of RNAs than the RF distance, which is more focused on base pairs.","We also consider a more general tree edit distance that allows the mapping of base pairs that are not perfectly aligned.","We study the theoretical complexity of the median problem and the small parsimony problem under the three distance metrics and various biologically-relevant constraints, and we present polynomial-time maximum parsimony algorithms for solving some versions of the problems.","Our algorithms are applied to ncRNA families from the RFAM database, illustrating their practical utility"],"url":"http://arxiv.org/abs/2402.03455v1","category":"cs.DS"}
{"created":"2024-02-05 19:00:05","title":"Quantum Decoherence Effects: a complete treatment","abstract":"Physical systems in real life are inextricably linked to their surroundings and never completely separated from them. Truly closed systems do not exist. The phenomenon of decoherence, which is brought about by the interaction with the environment, removes the relative phase of quantum states in superposition and makes them incoherent. In neutrino physics, decoherence, although extensively studied has only been analyzed thus far, exclusively in terms of its dissipative characteristics. While it is true that dissipation, or the exponential suppression, eventually is the main observable effect, the exchange of energy between the medium and the system, is an important factor that has been overlooked up until now. In this work, we introduce this term and analyze its consequences.","sentences":["Physical systems in real life are inextricably linked to their surroundings and never completely separated from them.","Truly closed systems do not exist.","The phenomenon of decoherence, which is brought about by the interaction with the environment, removes the relative phase of quantum states in superposition and makes them incoherent.","In neutrino physics, decoherence, although extensively studied has only been analyzed thus far, exclusively in terms of its dissipative characteristics.","While it is true that dissipation, or the exponential suppression, eventually is the main observable effect, the exchange of energy between the medium and the system, is an important factor that has been overlooked up until now.","In this work, we introduce this term and analyze its consequences."],"url":"http://arxiv.org/abs/2402.03438v1","category":"hep-ph"}
{"created":"2024-02-05 18:59:52","title":"Test-Time Adaptation for Depth Completion","abstract":"It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them. Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass. We first present a study on how the domain shift in each data modality affects model performance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During test time, sparse depth features are projected using this map as a proxy for source domain features and are used as guidance to train a set of auxiliary parameters (i.e., adaptation layer) to align image and sparse depth features from the target test domain to that of the source domain. We evaluate our method on indoor and outdoor scenarios and show that it improves over baselines by an average of 21.1%.","sentences":["It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them.","Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data.","We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass.","We first present a study on how the domain shift in each data modality affects model performance.","Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth.","During test time, sparse depth features are projected using this map as a proxy for source domain features and are used as guidance to train a set of auxiliary parameters (i.e., adaptation layer) to align image and sparse depth features from the target test domain to that of the source domain.","We evaluate our method on indoor and outdoor scenarios and show that it improves over baselines by an average of 21.1%."],"url":"http://arxiv.org/abs/2402.03312v1","category":"cs.CV"}
{"created":"2024-02-05 18:59:41","title":"HASSOD: Hierarchical Adaptive Self-Supervised Object Detection","abstract":"The human visual perception system demonstrates exceptional capabilities in learning without explicit supervision and understanding the part-to-whole composition of objects. Drawing inspiration from these two abilities, we propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a novel approach that learns to detect objects and understand their compositions without human supervision. HASSOD employs a hierarchical adaptive clustering strategy to group regions into object masks based on self-supervised visual representations, adaptively determining the number of objects per image. Furthermore, HASSOD identifies the hierarchical levels of objects in terms of composition, by analyzing coverage relations between masks and constructing tree structures. This additional self-supervised learning task leads to improved detection performance and enhanced interpretability. Lastly, we abandon the inefficient multi-round self-training process utilized in prior methods and instead adapt the Mean Teacher framework from semi-supervised learning, which leads to a smoother and more efficient training process. Through extensive experiments on prevalent image datasets, we demonstrate the superiority of HASSOD over existing methods, thereby advancing the state of the art in self-supervised object detection. Notably, we improve Mask AR from 20.2 to 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page: https://HASSOD-NeurIPS23.github.io.","sentences":["The human visual perception system demonstrates exceptional capabilities in learning without explicit supervision and understanding the part-to-whole composition of objects.","Drawing inspiration from these two abilities, we propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a novel approach that learns to detect objects and understand their compositions without human supervision.","HASSOD employs a hierarchical adaptive clustering strategy to group regions into object masks based on self-supervised visual representations, adaptively determining the number of objects per image.","Furthermore, HASSOD identifies the hierarchical levels of objects in terms of composition, by analyzing coverage relations between masks and constructing tree structures.","This additional self-supervised learning task leads to improved detection performance and enhanced interpretability.","Lastly, we abandon the inefficient multi-round self-training process utilized in prior methods and instead adapt the Mean Teacher framework from semi-supervised learning, which leads to a smoother and more efficient training process.","Through extensive experiments on prevalent image datasets, we demonstrate the superiority of HASSOD over existing methods, thereby advancing the state of the art in self-supervised object detection.","Notably, we improve Mask AR from 20.2 to 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page: https://HASSOD-NeurIPS23.github.io."],"url":"http://arxiv.org/abs/2402.03311v1","category":"cs.CV"}
{"created":"2024-02-05 18:54:43","title":"GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models","abstract":"The discovery of \"jailbreaks\" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses. In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly. We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD's versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities.","sentences":["The discovery of \"jailbreaks\" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures.","One major safety measure is to proactively test the LLMs with jailbreaks prior to the release.","Therefore, such testing will require a method that can generate jailbreaks massively and efficiently.","In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation.","We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks.","Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence.","We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve.","Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses.","In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly.","We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics).","We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT).","Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD's versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities."],"url":"http://arxiv.org/abs/2402.03299v1","category":"cs.LG"}
{"created":"2024-02-05 18:50:39","title":"Flora: Low-Rank Adapters Are Secretly Gradient Compressors","abstract":"Despite large neural networks demonstrating remarkable abilities to complete different tasks, they require excessive memory usage to store the optimization states for training. To alleviate this, the low-rank adaptation (LoRA) is proposed to reduce the optimization states by training fewer parameters. However, LoRA restricts overall weight update matrices to be low-rank, limiting the model performance. In this work, we investigate the dynamics of LoRA and identify that it can be approximated by a random projection. Based on this observation, we propose Flora, which is able to achieve high-rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states. We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach.","sentences":["Despite large neural networks demonstrating remarkable abilities to complete different tasks, they require excessive memory usage to store the optimization states for training.","To alleviate this, the low-rank adaptation (LoRA) is proposed to reduce the optimization states by training fewer parameters.","However, LoRA restricts overall weight update matrices to be low-rank, limiting the model performance.","In this work, we investigate the dynamics of LoRA and identify that it can be approximated by a random projection.","Based on this observation, we propose Flora, which is able to achieve high-rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states.","We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach."],"url":"http://arxiv.org/abs/2402.03293v1","category":"cs.LG"}
{"created":"2024-02-05 18:43:05","title":"A Lennard-Jones Layer for Distribution Normalization","abstract":"We introduce the Lennard-Jones layer (LJL) for the equalization of the density of 2D and 3D point clouds through systematically rearranging points without destroying their overall structure (distribution normalization). LJL simulates a dissipative process of repulsive and weakly attractive interactions between individual points by considering the nearest neighbor of each point at a given moment in time. This pushes the particles into a potential valley, reaching a well-defined stable configuration that approximates an equidistant sampling after the stabilization process. We apply LJLs to redistribute randomly generated point clouds into a randomized uniform distribution. Moreover, LJLs are embedded in the generation process of point cloud networks by adding them at later stages of the inference process. The improvements in 3D point cloud generation utilizing LJLs are evaluated qualitatively and quantitatively. Finally, we apply LJLs to improve the point distribution of a score-based 3D point cloud denoising network. In general, we demonstrate that LJLs are effective for distribution normalization which can be applied at negligible cost without retraining the given neural network.","sentences":["We introduce the Lennard-Jones layer (LJL) for the equalization of the density of 2D and 3D point clouds through systematically rearranging points without destroying their overall structure (distribution normalization).","LJL simulates a dissipative process of repulsive and weakly attractive interactions between individual points by considering the nearest neighbor of each point at a given moment in time.","This pushes the particles into a potential valley, reaching a well-defined stable configuration that approximates an equidistant sampling after the stabilization process.","We apply LJLs to redistribute randomly generated point clouds into a randomized uniform distribution.","Moreover, LJLs are embedded in the generation process of point cloud networks by adding them at later stages of the inference process.","The improvements in 3D point cloud generation utilizing LJLs are evaluated qualitatively and quantitatively.","Finally, we apply LJLs to improve the point distribution of a score-based 3D point cloud denoising network.","In general, we demonstrate that LJLs are effective for distribution normalization which can be applied at negligible cost without retraining the given neural network."],"url":"http://arxiv.org/abs/2402.03287v1","category":"cs.LG"}
{"created":"2024-02-05 18:35:40","title":"Wild orbits and generalised singularity modules: stratifications and quantisation","abstract":"We study isomorphism classes of untwisted irregular singular meromorphic connections on principal bundles over (wild) Riemann surfaces, for any complex reductive structure group $G$ and polar divisor. In particular we compute the stabilisers of suitable marked points on their principal part orbits, showing the stabilisers are connected and controlled by the corresponding filtration of (Levi factors of) nested parabolic subgroups of $G$; this uniquely determines the orbits as complex homogeneous manifolds for groups of jets of principal $G$-bundle automorphisms. Moreover, when the residue is semisimple we stratify the space of orbits by the stabilisers, relating this to local wild mapping class groups and generalising the Levi stratification of a Cartan subalgebra $\\mathfrak{t} \\subseteq \\mathfrak{g} = \\operatorname{Lie}(G)$: the dense stratum corresponds to the generic setting of irregular isomonodromic deformations \\`a la Jimbo--Miwa--Ueno.   Then we adapt a result of Alekseev--Lachowska to deformation-quantise nongeneric orbits: the $\\ast$-product involves affine-Lie-algebra modules, extending the generalised Verma modules (in the case of regular singularities) and the `singularity' modules of F.--R. (in the case of generic irregular singularities). As in the generic case, the modules contain Whittaker vectors for the Gaiotto--Teschner Virasoro pairs from irregular Liouville conformal field theory; but they now provide all the quotients which are obtained when the corresponding parameters leave the aforementioned dense strata. We also construct Shapovalov forms for the corresponding representations of truncated (holomorphic) current Lie algebras, leading to a conjectural irreducibility criterion. Finally, we use these representations to construct new flat vector bundles of vacua/covacua \\`a la Wess--Zumino--Novikov--Witten, equipped with connections \\`a la Knizhnik--Zamolodchikov.","sentences":["We study isomorphism classes of untwisted irregular singular meromorphic connections on principal bundles over (wild) Riemann surfaces, for any complex reductive structure group $G$ and polar divisor.","In particular we compute the stabilisers of suitable marked points on their principal part orbits, showing the stabilisers are connected and controlled by the corresponding filtration of (Levi factors of) nested parabolic subgroups of $G$; this uniquely determines the orbits as complex homogeneous manifolds for groups of jets of principal $G$-bundle automorphisms.","Moreover, when the residue is semisimple we stratify the space of orbits by the stabilisers, relating this to local wild mapping class groups and generalising the Levi stratification of a Cartan subalgebra $\\mathfrak{t} \\subseteq \\mathfrak{g} = \\operatorname{Lie}(G)$: the dense stratum corresponds to the generic setting of irregular isomonodromic deformations \\`a la Jimbo--Miwa--Ueno.   ","Then we adapt a result of Alekseev--Lachowska to deformation-quantise nongeneric orbits: the $\\ast$-product involves affine-Lie-algebra modules, extending the generalised Verma modules (in the case of regular singularities) and the `singularity' modules of F.--R. (in the case of generic irregular singularities).","As in the generic case, the modules contain Whittaker vectors for the Gaiotto--Teschner Virasoro pairs from irregular Liouville conformal field theory; but they now provide all the quotients which are obtained when the corresponding parameters leave the aforementioned dense strata.","We also construct Shapovalov forms for the corresponding representations of truncated (holomorphic) current Lie algebras, leading to a conjectural irreducibility criterion.","Finally, we use these representations to construct new flat vector bundles of vacua/covacua \\`a la Wess--Zumino--Novikov--Witten, equipped with connections \\`a la Knizhnik--Zamolodchikov."],"url":"http://arxiv.org/abs/2402.03278v1","category":"math.QA"}
{"created":"2024-02-05 18:09:48","title":"Fair Active Ranking from Pairwise Preferences","abstract":"We investigate the problem of probably approximately correct and fair (PACF) ranking of items by adaptively evoking pairwise comparisons. Given a set of $n$ items that belong to disjoint groups, our goal is to find an $(\\epsilon, \\delta)$-PACF-Ranking according to a fair objective function that we propose. We assume access to an oracle, wherein, for each query, the learner can choose a pair of items and receive stochastic winner feedback from the oracle. Our proposed objective function asks to minimize the $\\ell_q$ norm of the error of the groups, where the error of a group is the $\\ell_p$ norm of the error of all the items within that group, for $p, q \\geq 1$. This generalizes the objective function of $\\epsilon$-Best-Ranking, proposed by Saha & Gopalan (2019).   By adopting our objective function, we gain the flexibility to explore fundamental fairness concepts like equal or proportionate errors within a unified framework. Adjusting parameters $p$ and $q$ allows tailoring to specific fairness preferences. We present both group-blind and group-aware algorithms and analyze their sample complexity. We provide matching lower bounds up to certain logarithmic factors for group-blind algorithms. For a restricted class of group-aware algorithms, we show that we can get reasonable lower bounds. We conduct comprehensive experiments on both real-world and synthetic datasets to complement our theoretical findings.","sentences":["We investigate the problem of probably approximately correct and fair (PACF) ranking of items by adaptively evoking pairwise comparisons.","Given a set of $n$ items that belong to disjoint groups, our goal is to find an $(\\epsilon, \\delta)$-PACF-Ranking according to a fair objective function that we propose.","We assume access to an oracle, wherein, for each query, the learner can choose a pair of items and receive stochastic winner feedback from the oracle.","Our proposed objective function asks to minimize the $\\ell_q$ norm of the error of the groups, where the error of a group is the $\\ell_p$ norm of the error of all the items within that group, for $p, q \\geq 1$.","This generalizes the objective function of $\\epsilon$-Best-Ranking, proposed by Saha & Gopalan (2019).   ","By adopting our objective function, we gain the flexibility to explore fundamental fairness concepts like equal or proportionate errors within a unified framework.","Adjusting parameters $p$ and $q$ allows tailoring to specific fairness preferences.","We present both group-blind and group-aware algorithms and analyze their sample complexity.","We provide matching lower bounds up to certain logarithmic factors for group-blind algorithms.","For a restricted class of group-aware algorithms, we show that we can get reasonable lower bounds.","We conduct comprehensive experiments on both real-world and synthetic datasets to complement our theoretical findings."],"url":"http://arxiv.org/abs/2402.03252v1","category":"cs.LG"}
{"created":"2024-02-05 18:09:33","title":"CLIP Can Understand Depth","abstract":"Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts. In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment. By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth. With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin. Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework. Furthermore, an ablation study on mirror proves that the resulting model estimates depth utilizing knowledge not only from the image encoder but also text encoder despite not being given any prompt written in a human way. This research demonstrates that through minimal adjustments, the prior knowledge of vision-language foundation models, such as CLIP, can be generalized even to domains where learning during pretraining is challenging. We facilitate future works focused on methods to adjust suboptimal prior knowledge of vision-language models using non-human language prompts, achieving performance on par with task-specific state-of-the-art methodologies.","sentences":["Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts.","In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment.","By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth.","With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin.","Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework.","Furthermore, an ablation study on mirror proves that the resulting model estimates depth utilizing knowledge not only from the image encoder but also text encoder despite not being given any prompt written in a human way.","This research demonstrates that through minimal adjustments, the prior knowledge of vision-language foundation models, such as CLIP, can be generalized even to domains where learning during pretraining is challenging.","We facilitate future works focused on methods to adjust suboptimal prior knowledge of vision-language models using non-human language prompts, achieving performance on par with task-specific state-of-the-art methodologies."],"url":"http://arxiv.org/abs/2402.03251v1","category":"cs.CV"}
{"created":"2024-02-05 17:57:26","title":"JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching","abstract":"Recent approaches in skill matching, employing synthetic training data for classification or similarity model training, have shown promising results, reducing the need for time-consuming and expensive annotations. However, previous synthetic datasets have limitations, such as featuring only one skill per sentence and generally comprising short sentences. In this paper, we introduce JobSkape, a framework to generate synthetic data that tackles these limitations, specifically designed to enhance skill-to-taxonomy matching. Within this framework, we create SkillSkape, a comprehensive open-source synthetic dataset of job postings tailored for skill-matching tasks. We introduce several offline metrics that show that our dataset resembles real-world data. Additionally, we present a multi-step pipeline for skill extraction and matching tasks using large language models (LLMs), benchmarking against known supervised methodologies. We outline that the downstream evaluation results on real-world data can beat baselines, underscoring its efficacy and adaptability.","sentences":["Recent approaches in skill matching, employing synthetic training data for classification or similarity model training, have shown promising results, reducing the need for time-consuming and expensive annotations.","However, previous synthetic datasets have limitations, such as featuring only one skill per sentence and generally comprising short sentences.","In this paper, we introduce JobSkape, a framework to generate synthetic data that tackles these limitations, specifically designed to enhance skill-to-taxonomy matching.","Within this framework, we create SkillSkape, a comprehensive open-source synthetic dataset of job postings tailored for skill-matching tasks.","We introduce several offline metrics that show that our dataset resembles real-world data.","Additionally, we present a multi-step pipeline for skill extraction and matching tasks using large language models (LLMs), benchmarking against known supervised methodologies.","We outline that the downstream evaluation results on real-world data can beat baselines, underscoring its efficacy and adaptability."],"url":"http://arxiv.org/abs/2402.03242v1","category":"cs.CL"}
{"created":"2024-02-05 17:56:41","title":"FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition","abstract":"In this paper, we introduce FROSTER, an effective framework for open-vocabulary action recognition. The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability stemming from pretaining on massive image-text pairs. However, applying CLIP directly to the open-vocabulary action recognition task is challenging due to the absence of temporal information in CLIP's pretraining. Further, fine-tuning CLIP on action recognition datasets may lead to overfitting and hinder its generalizability, resulting in unsatisfactory results when dealing with unseen actions.   To address these issues, FROSTER employs a residual feature distillation approach to ensure that CLIP retains its generalization capability while effectively adapting to the action recognition task. Specifically, the residual feature distillation treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises the feature learning for the extraction of video-specific features to bridge the gap between images and videos. Meanwhile, it uses a residual sub-network for feature distillation to reach a balance between the two distinct objectives of learning generalizable and video-specific features.   We extensively evaluate FROSTER on open-vocabulary action recognition benchmarks under both base-to-novel and cross-dataset settings. FROSTER consistently achieves state-of-the-art performance on all datasets across the board. Project page: https://visual-ai.github.io/froster.","sentences":["In this paper, we introduce FROSTER, an effective framework for open-vocabulary action recognition.","The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability stemming from pretaining on massive image-text pairs.","However, applying CLIP directly to the open-vocabulary action recognition task is challenging due to the absence of temporal information in CLIP's pretraining.","Further, fine-tuning CLIP on action recognition datasets may lead to overfitting and hinder its generalizability, resulting in unsatisfactory results when dealing with unseen actions.   ","To address these issues, FROSTER employs a residual feature distillation approach to ensure that CLIP retains its generalization capability while effectively adapting to the action recognition task.","Specifically, the residual feature distillation treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises the feature learning for the extraction of video-specific features to bridge the gap between images and videos.","Meanwhile, it uses a residual sub-network for feature distillation to reach a balance between the two distinct objectives of learning generalizable and video-specific features.   ","We extensively evaluate FROSTER on open-vocabulary action recognition benchmarks under both base-to-novel and cross-dataset settings.","FROSTER consistently achieves state-of-the-art performance on all datasets across the board.","Project page: https://visual-ai.github.io/froster."],"url":"http://arxiv.org/abs/2402.03241v1","category":"cs.CV"}
{"created":"2024-02-05 17:53:16","title":"Declipping and the recovery of vectors from saturated measurements","abstract":"A frame $(x_j)_{j\\in J}$ for a Hilbert space $H$ allows for a linear and stable reconstruction of any vector $x\\in H$ from the linear measurements $(\\langle x,x_j\\rangle)_{j\\in J}$. However, there are many situations where some information in the frame coefficients is lost. In applications where one is using sensors with a fixed dynamic range, any measurement above that range is registered as the maximum, and any measurement below that range is registered as the minimum. Depending on the context, recovering a vector from such measurements is called either declipping or saturation recovery. We initiate a frame theoretic approach to saturation recovery in a similar way to what [BCE06] did for phase retrieval. We characterize when saturation recovery is possible, show optimal frames for use with saturation recovery correspond to minimal multi-fold packings in projective space, and prove that the classical frame algorithm may be adapted to this non-linear problem to provide a reconstruction algorithm.","sentences":["A frame $(x_j)_{j\\in J}$ for a Hilbert space $H$ allows for a linear and stable reconstruction of any vector $x\\in H$ from the linear measurements $(\\langle x,x_j\\rangle)_{j\\in J}$.","However, there are many situations where some information in the frame coefficients is lost.","In applications where one is using sensors with a fixed dynamic range, any measurement above that range is registered as the maximum, and any measurement below that range is registered as the minimum.","Depending on the context, recovering a vector from such measurements is called either declipping or saturation recovery.","We initiate a frame theoretic approach to saturation recovery in a similar way to what [BCE06] did for phase retrieval.","We characterize when saturation recovery is possible, show optimal frames for use with saturation recovery correspond to minimal multi-fold packings in projective space, and prove that the classical frame algorithm may be adapted to this non-linear problem to provide a reconstruction algorithm."],"url":"http://arxiv.org/abs/2402.03237v1","category":"math.FA"}
{"created":"2024-02-05 17:22:34","title":"Universal Gradient Methods for Stochastic Convex Optimization","abstract":"We develop universal gradient methods for Stochastic Convex Optimization (SCO). Our algorithms automatically adapt not only to the oracle's noise but also to the H\\\"older smoothness of the objective function without a priori knowledge of the particular setting. The key ingredient is a novel strategy for adjusting step-size coefficients in the Stochastic Gradient Method (SGD). Unlike AdaGrad, which accumulates gradient norms, our Universal Gradient Method accumulates appropriate combinations of gradient- and iterate differences. The resulting algorithm has state-of-the-art worst-case convergence rate guarantees for the entire H\\\"older class including, in particular, both nonsmooth functions and those with Lipschitz continuous gradient. We also present the Universal Fast Gradient Method for SCO enjoying optimal efficiency estimates.","sentences":["We develop universal gradient methods for Stochastic Convex Optimization (SCO).","Our algorithms automatically adapt not only to the oracle's noise but also to the H\\\"older smoothness of the objective function without a priori knowledge of the particular setting.","The key ingredient is a novel strategy for adjusting step-size coefficients in the Stochastic Gradient Method (SGD).","Unlike AdaGrad, which accumulates gradient norms, our Universal Gradient Method accumulates appropriate combinations of gradient- and iterate differences.","The resulting algorithm has state-of-the-art worst-case convergence rate guarantees for the entire H\\\"older class including, in particular, both nonsmooth functions and those with Lipschitz continuous gradient.","We also present the Universal Fast Gradient Method for SCO enjoying optimal efficiency estimates."],"url":"http://arxiv.org/abs/2402.03210v1","category":"math.OC"}
{"created":"2024-02-05 17:17:57","title":"Light and Optimal Schr\u00f6dinger Bridge Matching","abstract":"Schr\\\"odinger Bridges (SB) have recently gained the attention of the ML community as a promising extension of classic diffusion models which is also interconnected to the Entropic Optimal Transport (EOT). Recent solvers for SB exploit the pervasive bridge matching procedures. Such procedures aim to recover a stochastic process transporting the mass between distributions given only a transport plan between them. In particular, given the EOT plan, these procedures can be adapted to solve SB. This fact is heavily exploited by recent works giving rives to matching-based SB solvers. The cornerstone here is recovering the EOT plan: recent works either use heuristical approximations (e.g., the minibatch OT) or establish iterative matching procedures which by the design accumulate the error during the training. We address these limitations and propose a novel procedure to learn SB which we call the \\textbf{optimal Schr\\\"odinger bridge matching}. It exploits the optimal parameterization of the diffusion process and provably recovers the SB process \\textbf{(a)} with a single bridge matching step and \\textbf{(b)} with arbitrary transport plan as the input. Furthermore, we show that the optimal bridge matching objective coincides with the recently discovered energy-based modeling (EBM) objectives to learn EOT/SB. Inspired by this observation, we develop a light solver (which we call LightSB-M) to implement optimal matching in practice using the Gaussian mixture parameterization of the Schr\\\"odinger potential. We experimentally showcase the performance of our solver in a range of practical tasks. The code for the LightSB-M solver can be found at \\url{https://github.com/SKholkin/LightSB-Matching}.","sentences":["Schr\\\"odinger Bridges (SB) have recently gained the attention of the ML community as a promising extension of classic diffusion models which is also interconnected to the Entropic Optimal Transport (EOT).","Recent solvers for SB exploit the pervasive bridge matching procedures.","Such procedures aim to recover a stochastic process transporting the mass between distributions given only a transport plan between them.","In particular, given the EOT plan, these procedures can be adapted to solve SB.","This fact is heavily exploited by recent works giving rives to matching-based SB solvers.","The cornerstone here is recovering the EOT plan: recent works either use heuristical approximations (e.g., the minibatch OT) or establish iterative matching procedures which by the design accumulate the error during the training.","We address these limitations and propose a novel procedure to learn SB which we call the \\textbf{optimal Schr\\\"odinger bridge matching}.","It exploits the optimal parameterization of the diffusion process and provably recovers the SB process \\textbf{(a)} with a single bridge matching step and \\textbf{(b)} with arbitrary transport plan as the input.","Furthermore, we show that the optimal bridge matching objective coincides with the recently discovered energy-based modeling (EBM) objectives to learn EOT/SB.","Inspired by this observation, we develop a light solver (which we call LightSB-M) to implement optimal matching in practice using the Gaussian mixture parameterization of the Schr\\\"odinger potential.","We experimentally showcase the performance of our solver in a range of practical tasks.","The code for the LightSB-M solver can be found at \\url{https://github.com/SKholkin/LightSB-Matching}."],"url":"http://arxiv.org/abs/2402.03207v1","category":"cs.LG"}
{"created":"2024-02-05 17:13:31","title":"Right-censored models by the expectile method","abstract":"Based on the expectile loss function and the adaptive LASSO penalty, the paper proposes and studies the estimation methods for the accelerated failure time (AFT) model. In this approach, we need to estimate the survival function of the censoring variable by the Kaplan-Meier estimator. The AFT model parameters are first estimated by the expectile method and afterwards, when the number of explanatory variables can be large, by the adaptive LASSO expectile method which directly carries out the automatic selection of variables. We also obtain the convergence rate and asymptotic normality for the two estimators, while showing the sparsity property for the censored adaptive LASSO expectile estimator. A numerical study using Monte Carlo simulations confirms the theoretical results and demonstrates the competitive performance of the two proposed estimators. The usefulness of these estimators is illustrated by applying them to three survival data sets.","sentences":["Based on the expectile loss function and the adaptive LASSO penalty, the paper proposes and studies the estimation methods for the accelerated failure time (AFT) model.","In this approach, we need to estimate the survival function of the censoring variable by the Kaplan-Meier estimator.","The AFT model parameters are first estimated by the expectile method and afterwards, when the number of explanatory variables can be large, by the adaptive LASSO expectile method which directly carries out the automatic selection of variables.","We also obtain the convergence rate and asymptotic normality for the two estimators, while showing the sparsity property for the censored adaptive LASSO expectile estimator.","A numerical study using Monte Carlo simulations confirms the theoretical results and demonstrates the competitive performance of the two proposed estimators.","The usefulness of these estimators is illustrated by applying them to three survival data sets."],"url":"http://arxiv.org/abs/2402.03203v1","category":"math.ST"}
{"created":"2024-02-05 17:12:21","title":"Guidance with Spherical Gaussian Constraint for Conditional Diffusion","abstract":"Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training. While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes. This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed. We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance. To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions. DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps. Furthermore, we present a closed-form solution for DSG denoising with the Spherical Gaussian constraint. Notably, DSG can seamlessly integrate as a plugin module within existing training-free conditional diffusion methods. Implementing DSG merely involves a few lines of additional code with almost no extra computational overhead, yet it leads to significant performance improvements. Comprehensive experimental results in various conditional generation tasks validate the superiority and adaptability of DSG in terms of both sample quality and time efficiency.","sentences":["Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training.","While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes.","This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed.","We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance.","To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions.","DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps.","Furthermore, we present a closed-form solution for DSG denoising with the Spherical Gaussian constraint.","Notably, DSG can seamlessly integrate as a plugin module within existing training-free conditional diffusion methods.","Implementing DSG merely involves a few lines of additional code with almost no extra computational overhead, yet it leads to significant performance improvements.","Comprehensive experimental results in various conditional generation tasks validate the superiority and adaptability of DSG in terms of both sample quality and time efficiency."],"url":"http://arxiv.org/abs/2402.03201v1","category":"cs.LG"}
{"created":"2024-02-05 16:58:22","title":"Multiple testing using uniform filtering of ordered p-values","abstract":"We investigate the multiplicity model with m values of some test statistic independently drawn from a mixture of no effect (null) and positive effect (alternative), where we seek to identify, the alternative test results with a controlled error rate. We are interested in the case where the alternatives are rare. A number of multiple testing procedures filter the set of ordered p-values in order to eliminate the nulls. Such an approach can only work if the p-values originating from the alternatives form one or several identifiable clusters. The Benjamini and Hochberg (BH) method, for example, assumes that this cluster occurs in a small interval $(0,\\Delta)$ and filters out all or most of the ordered p-values $p_{(r)}$ above a linear threshold $s \\times r$. In repeated applications this filter controls the false discovery rate via the slope s. We propose a new adaptive filter that deletes the p-values from regions of uniform distribution. In cases where a single cluster remains, the p-values in an interval are declared alternatives, with the mid-point and the length of the interval chosen by controlling the data-dependent FDR at a desired level.","sentences":["We investigate the multiplicity model with m values of some test statistic independently drawn from a mixture of no effect (null) and positive effect (alternative), where we seek to identify, the alternative test results with a controlled error rate.","We are interested in the case where the alternatives are rare.","A number of multiple testing procedures filter the set of ordered p-values in order to eliminate the nulls.","Such an approach can only work if the p-values originating from the alternatives form one or several identifiable clusters.","The Benjamini and Hochberg (BH) method, for example, assumes that this cluster occurs in a small interval $(0,\\Delta)$ and filters out all or most of the ordered p-values $p_{(r)}$ above a linear threshold $s \\times r$.","In repeated applications this filter controls the false discovery rate via the slope s.","We propose a new adaptive filter that deletes the p-values from regions of uniform distribution.","In cases where a single cluster remains, the p-values in an interval are declared alternatives, with the mid-point and the length of the interval chosen by controlling the data-dependent FDR at a desired level."],"url":"http://arxiv.org/abs/2402.03192v1","category":"stat.ME"}
{"created":"2024-02-05 16:46:35","title":"Empowering Time Series Analysis with Large Language Models: A Survey","abstract":"Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks. However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training. Recent advances have shown that pre-trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications. In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis. Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of LLMs. Next, we summarize the general pipeline for LLM-based time series analysis, categorize existing methods into different groups (i.e., direct query, tokenization, prompt design, fine-tune, and model integration), and highlight the key ideas within each group. We also discuss the applications of LLMs for both general and spatial-temporal time series data, tailored to specific domains. Finally, we thoroughly discuss future research opportunities to empower time series analysis with LLMs.","sentences":["Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks.","However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training.","Recent advances have shown that pre-trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications.","In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis.","Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of LLMs.","Next, we summarize the general pipeline for LLM-based time series analysis, categorize existing methods into different groups (i.e., direct query, tokenization, prompt design, fine-tune, and model integration), and highlight the key ideas within each group.","We also discuss the applications of LLMs for both general and spatial-temporal time series data, tailored to specific domains.","Finally, we thoroughly discuss future research opportunities to empower time series analysis with LLMs."],"url":"http://arxiv.org/abs/2402.03182v1","category":"cs.LG"}
{"created":"2024-02-05 16:40:23","title":"Accurate and Well-Calibrated ICD Code Assignment Through Attention Over Diverse Label Embeddings","abstract":"Although the International Classification of Diseases (ICD) has been adopted worldwide, manually assigning ICD codes to clinical text is time-consuming, error-prone, and expensive, motivating the development of automated approaches. This paper describes a novel approach for automated ICD coding, combining several ideas from previous related work. We specifically employ a strong Transformer-based model as a text encoder and, to handle lengthy clinical narratives, we explored either (a) adapting the base encoder model into a Longformer, or (b) dividing the text into chunks and processing each chunk independently. The representations produced by the encoder are combined with a label embedding mechanism that explores diverse ICD code synonyms. Experiments with different splits of the MIMIC-III dataset show that the proposed approach outperforms the current state-of-the-art models in ICD coding, with the label embeddings significantly contributing to the good performance. Our approach also leads to properly calibrated classification results, which can effectively inform downstream tasks such as quantification.","sentences":["Although the International Classification of Diseases (ICD) has been adopted worldwide, manually assigning ICD codes to clinical text is time-consuming, error-prone, and expensive, motivating the development of automated approaches.","This paper describes a novel approach for automated ICD coding, combining several ideas from previous related work.","We specifically employ a strong Transformer-based model as a text encoder and, to handle lengthy clinical narratives, we explored either (a) adapting the base encoder model into a Longformer, or (b) dividing the text into chunks and processing each chunk independently.","The representations produced by the encoder are combined with a label embedding mechanism that explores diverse ICD code synonyms.","Experiments with different splits of the MIMIC-III dataset show that the proposed approach outperforms the current state-of-the-art models in ICD coding, with the label embeddings significantly contributing to the good performance.","Our approach also leads to properly calibrated classification results, which can effectively inform downstream tasks such as quantification."],"url":"http://arxiv.org/abs/2402.03172v1","category":"cs.CL"}
{"created":"2024-02-05 16:36:17","title":"Data-driven reconstruction of limit cycle position provides side information for improved model identification with SINDy","abstract":"Many important systems in nature are characterized by oscillations. To understand and interpret such behavior, researchers use the language of mathematical models, often in the form of differential equations. Nowadays, these equations can be derived using data-driven machine learning approaches, such as the white-box method 'Sparse Identification of Nonlinear Dynamics' (SINDy). In this paper, we show that to ensure the identification of sparse and meaningful models, it is crucial to identify the correct position of the system limit cycle in phase space. Therefore, we propose how the limit cycle position and the system's nullclines can be identified by applying SINDy to the data set with varying offsets, using three model evaluation criteria (complexity, coefficient of determination, generalization error). We successfully test the method on an oscillatory FitzHugh-Nagumo model and a more complex model consisting of two coupled cubic differential equations. Finally, we demonstrate that using this additional side information on the limit cycle in phase space can improve the success of model identification efforts in oscillatory systems.","sentences":["Many important systems in nature are characterized by oscillations.","To understand and interpret such behavior, researchers use the language of mathematical models, often in the form of differential equations.","Nowadays, these equations can be derived using data-driven machine learning approaches, such as the white-box method 'Sparse Identification of Nonlinear Dynamics' (SINDy).","In this paper, we show that to ensure the identification of sparse and meaningful models, it is crucial to identify the correct position of the system limit cycle in phase space.","Therefore, we propose how the limit cycle position and the system's nullclines can be identified by applying SINDy to the data set with varying offsets, using three model evaluation criteria (complexity, coefficient of determination, generalization error).","We successfully test the method on an oscillatory FitzHugh-Nagumo model and a more complex model consisting of two coupled cubic differential equations.","Finally, we demonstrate that using this additional side information on the limit cycle in phase space can improve the success of model identification efforts in oscillatory systems."],"url":"http://arxiv.org/abs/2402.03168v1","category":"nlin.AO"}
{"created":"2024-02-05 16:30:49","title":"Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization","abstract":"In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation. Our code and models will be available at https://video-lavit.github.io.","sentences":["In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos.","Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics.","In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions.","These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text.","At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content.","Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation.","Our code and models will be available at https://video-lavit.github.io."],"url":"http://arxiv.org/abs/2402.03161v2","category":"cs.CV"}
{"created":"2024-02-05 16:27:59","title":"Optimal and Near-Optimal Adaptive Vector Quantization","abstract":"Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.   We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.","sentences":["Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets.","The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case.","However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.   ","We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity.","We also present an even faster near-optimal algorithm for large inputs.","Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications."],"url":"http://arxiv.org/abs/2402.03158v1","category":"cs.LG"}
{"created":"2024-02-06 18:51:47","title":"Invariant Set Estimation for Piecewise Affine Dynamical Systems Using Piecewise Affine Barrier Function","abstract":"This paper introduces an algorithm for approximating the invariant set of closed-loop controlled dynamical systems identified using ReLU neural networks or piecewise affine PWA functions, particularly addressing the challenge of providing safety guarantees for ReLU networks commonly used in safety-critical applications. The invariant set of PWA dynamical system is estimated using ReLU networks or its equivalent PWA function. This method entails formulating the barrier function as a PWA function and converting the search process into a linear optimization problem using vertices. We incorporate a domain refinement strategy to increase flexibility in case the optimization does not find a valid barrier function. Moreover, the objective of optimization is to maximize the invariant set based on the current partition. Our experimental results demonstrate the effectiveness and efficiency of our approach, demonstrating its potential for ensuring the safety of PWA dynamical systems.","sentences":["This paper introduces an algorithm for approximating the invariant set of closed-loop controlled dynamical systems identified using ReLU neural networks or piecewise affine PWA functions, particularly addressing the challenge of providing safety guarantees for ReLU networks commonly used in safety-critical applications.","The invariant set of PWA dynamical system is estimated using ReLU networks or its equivalent PWA function.","This method entails formulating the barrier function as a PWA function and converting the search process into a linear optimization problem using vertices.","We incorporate a domain refinement strategy to increase flexibility in case the optimization does not find a valid barrier function.","Moreover, the objective of optimization is to maximize the invariant set based on the current partition.","Our experimental results demonstrate the effectiveness and efficiency of our approach, demonstrating its potential for ensuring the safety of PWA dynamical systems."],"url":"http://arxiv.org/abs/2402.04243v1","category":"eess.SY"}
{"created":"2024-02-06 18:49:51","title":"Algebraic identifiability of partial differential equation models","abstract":"Differential equation models are crucial to scientific processes. The values of model parameters are important for analyzing the behaviour of solutions. A parameter is called globally identifiable if its value can be uniquely determined from the input and output functions. To determine if a parameter estimation problem is well-posed for a given model, one must check if the model parameters are globally identifiable. This problem has been intensively studied for ordinary differential equation models, with theory and several efficient algorithms and software packages developed. A comprehensive theory of algebraic identifiability for PDEs has hitherto not been developed due to the complexity of initial and boundary conditions. Here, we provide theory and algorithms, based on differential algebra, for testing identifiability of polynomial PDE models. We showcase this approach on PDE models arising in the sciences.","sentences":["Differential equation models are crucial to scientific processes.","The values of model parameters are important for analyzing the behaviour of solutions.","A parameter is called globally identifiable if its value can be uniquely determined from the input and output functions.","To determine if a parameter estimation problem is well-posed for a given model, one must check if the model parameters are globally identifiable.","This problem has been intensively studied for ordinary differential equation models, with theory and several efficient algorithms and software packages developed.","A comprehensive theory of algebraic identifiability for PDEs has hitherto not been developed due to the complexity of initial and boundary conditions.","Here, we provide theory and algorithms, based on differential algebra, for testing identifiability of polynomial PDE models.","We showcase this approach on PDE models arising in the sciences."],"url":"http://arxiv.org/abs/2402.04241v1","category":"q-bio.QM"}
{"created":"2024-02-06 18:42:51","title":"LIPSTICK: Corruptibility-Aware and Explainable Graph Neural Network-based Oracle-Less Attack on Logic Locking","abstract":"In a zero-trust fabless paradigm, designers are increasingly concerned about hardware-based attacks on the semiconductor supply chain. Logic locking is a design-for-trust method that adds extra key-controlled gates in the circuits to prevent hardware intellectual property theft and overproduction. While attackers have traditionally relied on an oracle to attack logic-locked circuits, machine learning attacks have shown the ability to retrieve the secret key even without access to an oracle. In this paper, we first examine the limitations of state-of-the-art machine learning attacks and argue that the use of key hamming distance as the sole model-guiding structural metric is not always useful. Then, we develop, train, and test a corruptibility-aware graph neural network-based oracle-less attack on logic locking that takes into consideration both the structure and the behavior of the circuits. Our model is explainable in the sense that we analyze what the machine learning model has interpreted in the training process and how it can perform a successful attack. Chip designers may find this information beneficial in securing their designs while avoiding incremental fixes.","sentences":["In a zero-trust fabless paradigm, designers are increasingly concerned about hardware-based attacks on the semiconductor supply chain.","Logic locking is a design-for-trust method that adds extra key-controlled gates in the circuits to prevent hardware intellectual property theft and overproduction.","While attackers have traditionally relied on an oracle to attack logic-locked circuits, machine learning attacks have shown the ability to retrieve the secret key even without access to an oracle.","In this paper, we first examine the limitations of state-of-the-art machine learning attacks and argue that the use of key hamming distance as the sole model-guiding structural metric is not always useful.","Then, we develop, train, and test a corruptibility-aware graph neural network-based oracle-less attack on logic locking that takes into consideration both the structure and the behavior of the circuits.","Our model is explainable in the sense that we analyze what the machine learning model has interpreted in the training process and how it can perform a successful attack.","Chip designers may find this information beneficial in securing their designs while avoiding incremental fixes."],"url":"http://arxiv.org/abs/2402.04235v1","category":"cs.CR"}
{"created":"2024-02-06 18:01:05","title":"On the growing length scale in a replica-coupled glassforming liquid","abstract":"Computer simulations are used to study a three-dimensional polydisperse model glassformer in a replica-coupling setup where an attractive field $\\propto - \\varepsilon Q$ of strength $\\varepsilon$ can adjust the similarity of the system to a fixed reference configuration with the overlap parameter $Q$. The polydispersity in the model enables the efficient use of swap Monte Carlo in combination with molecular-dynamics simulation from which we obtain fully equilibrated liquid configurations at very low temperature, i.e., far below the critical temperature of mode-coupling theory, $T_{\\rm MCT}$. When the $\\varepsilon$-field is switched on, the fast dynamics with swaps allow relaxation to the stationary state at temperatures below $T_{\\rm MCT}$. In the stationary state, the overlap $Q$ has a finite value that increases with increasing $\\varepsilon$. For a given temperature $T$, fluctuations of the overlap around the average value become maximal at a critical field strength $\\varepsilon^\\star(T)$. With decreasing $T$ along this $\\varepsilon^\\star(T)$-line, overlap fluctuations increase and a transition from a unimodal overlap distribution to a bimodal shape occurs. We give evidence that these bimodal distributions are not due to first-order phase transitions. However, they reflect finite-size effects due to a rapidly growing length scale with decreasing temperature. We discuss the significance of this length scale for the understanding of the glass transition.","sentences":["Computer simulations are used to study a three-dimensional polydisperse model glassformer in a replica-coupling setup where an attractive field $\\propto - \\varepsilon Q$ of strength $\\varepsilon$ can adjust the similarity of the system to a fixed reference configuration with the overlap parameter $Q$. The polydispersity in the model enables the efficient use of swap Monte Carlo in combination with molecular-dynamics simulation from which we obtain fully equilibrated liquid configurations at very low temperature, i.e., far below the critical temperature of mode-coupling theory, $T_{\\rm MCT}$. When the $\\varepsilon$-field is switched on, the fast dynamics with swaps allow relaxation to the stationary state at temperatures below $T_{\\rm MCT}$. In the stationary state, the overlap $Q$ has a finite value that increases with increasing $\\varepsilon$. For a given temperature $T$, fluctuations of the overlap around the average value become maximal at a critical field strength $\\varepsilon^\\star(T)$. With decreasing $T$ along this $\\varepsilon^\\star(T)$-line, overlap fluctuations increase and a transition from a unimodal overlap distribution to a bimodal shape occurs.","We give evidence that these bimodal distributions are not due to first-order phase transitions.","However, they reflect finite-size effects due to a rapidly growing length scale with decreasing temperature.","We discuss the significance of this length scale for the understanding of the glass transition."],"url":"http://arxiv.org/abs/2402.04205v1","category":"cond-mat.soft"}
{"created":"2024-02-06 18:00:27","title":"Maximal regularity and optimal control for a non-local Cahn-Hilliard tumour growth model","abstract":"We consider a non-local tumour growth model of phase-field type, describing the evolution of tumour cells through proliferation in presence of a nutrient. The model consists of a coupled system, incorporating a non-local Cahn-Hilliard equation for the tumour phase variable and a reaction-diffusion equation for the nutrient. Non-local cell-to-cell adhesion effects are included through a convolution operator with appropriate spatial kernels. First, we establish novel regularity results for such model, by applying maximal regularity theory in weighted $L^p$ spaces. Such technique enables us to prove local existence and uniqueness of a regular solution in a quite general framework, which also includes chemotaxis effects. Then, by leveraging time-regularisation properties of the weighted spaces and global boundedness estimates, we further extend the solution to a global one, under some additional assumptions. These results provide the foundation for addressing an optimal distributed control problem, aimed at identifying a suitable therapy, capable of guiding the evolution of the tumour towards a predefined target. Specifically, we prove the existence of an optimal therapy and then, by studying the Fr\\'echet-differentiability of the control-to-state operator and introducing the adjoint system, we derive first-order necessary optimality conditions.","sentences":["We consider a non-local tumour growth model of phase-field type, describing the evolution of tumour cells through proliferation in presence of a nutrient.","The model consists of a coupled system, incorporating a non-local Cahn-Hilliard equation for the tumour phase variable and a reaction-diffusion equation for the nutrient.","Non-local cell-to-cell adhesion effects are included through a convolution operator with appropriate spatial kernels.","First, we establish novel regularity results for such model, by applying maximal regularity theory in weighted $L^p$ spaces.","Such technique enables us to prove local existence and uniqueness of a regular solution in a quite general framework, which also includes chemotaxis effects.","Then, by leveraging time-regularisation properties of the weighted spaces and global boundedness estimates, we further extend the solution to a global one, under some additional assumptions.","These results provide the foundation for addressing an optimal distributed control problem, aimed at identifying a suitable therapy, capable of guiding the evolution of the tumour towards a predefined target.","Specifically, we prove the existence of an optimal therapy and then, by studying the Fr\\'echet-differentiability of the control-to-state operator and introducing the adjoint system, we derive first-order necessary optimality conditions."],"url":"http://arxiv.org/abs/2402.04204v1","category":"math.AP"}
{"created":"2024-02-06 17:52:41","title":"From zero-mode intermittency to hidden symmetry in random scalar advection","abstract":"The statistical behavior of scalars passively advected by random flows exhibits intermittency in the form of anomalous multiscaling, in many ways similar to the patterns commonly observed in incompressible high-Reynolds fluids. This similarity suggests a generic dynamical mechanism underlying intermittency, though its specific nature remains unclear. Scalar turbulence is framed in a linear setting that points towards a zero-mode scenario connecting anomalous scaling to the presence of statistical conservation laws; the duality is fully substantiated within Kraichnan theory of random flows. However, extending the zero-mode scenario to nonlinear settings faces formidable technical challenges. Here, we revisit the scalar problem in the light of a hidden symmetry scenario introduced in recent deterministic turbulence studies addressing the Sabra shell model and the Navier-Stokes equations. Hidden symmetry uses a rescaling strategy based entirely on symmetry considerations, transforming the original dynamics into a rescaled (hidden) system; It ultimately identifies the scaling exponents as the eigenvalues of a Perron-Frobenius operator acting on invariant measures of the rescaled equations. Considering a minimal shell model of scalar advection of the Kraichnan type that was previously studied by Biferale & Wirth, the present work extends the hidden symmetry approach to a stochastic setting, in order to explicitly contrast it with the zero-mode scenario. Our study indicates that the zero-mode scenario represents only one facet of intermittency, here prescribing the scaling exponents of even-order correlators. Besides, we argue that hidden symmetry provides a more generic mechanism, fully prescribing intermittency in terms of scaling anomalies, but also in terms of its multiplicative random nature and fusion rules required to explicitly compute zero-modes from first principles.","sentences":["The statistical behavior of scalars passively advected by random flows exhibits intermittency in the form of anomalous multiscaling, in many ways similar to the patterns commonly observed in incompressible high-Reynolds fluids.","This similarity suggests a generic dynamical mechanism underlying intermittency, though its specific nature remains unclear.","Scalar turbulence is framed in a linear setting that points towards a zero-mode scenario connecting anomalous scaling to the presence of statistical conservation laws; the duality is fully substantiated within Kraichnan theory of random flows.","However, extending the zero-mode scenario to nonlinear settings faces formidable technical challenges.","Here, we revisit the scalar problem in the light of a hidden symmetry scenario introduced in recent deterministic turbulence studies addressing the Sabra shell model and the Navier-Stokes equations.","Hidden symmetry uses a rescaling strategy based entirely on symmetry considerations, transforming the original dynamics into a rescaled (hidden) system; It ultimately identifies the scaling exponents as the eigenvalues of a Perron-Frobenius operator acting on invariant measures of the rescaled equations.","Considering a minimal shell model of scalar advection of the Kraichnan type that was previously studied by Biferale & Wirth, the present work extends the hidden symmetry approach to a stochastic setting, in order to explicitly contrast it with the zero-mode scenario.","Our study indicates that the zero-mode scenario represents only one facet of intermittency, here prescribing the scaling exponents of even-order correlators.","Besides, we argue that hidden symmetry provides a more generic mechanism, fully prescribing intermittency in terms of scaling anomalies, but also in terms of its multiplicative random nature and fusion rules required to explicitly compute zero-modes from first principles."],"url":"http://arxiv.org/abs/2402.04198v1","category":"physics.flu-dyn"}
{"created":"2024-02-06 17:29:34","title":"Constrained curve fitting for semi-parametric models with radial basis function networks","abstract":"Common to many analysis pipelines in lattice gauge theory and the broader scientific discipline is the need to fit a semi-parametric model to data. We propose a fit method that utilizes a radial basis function network to approximate the non-parametric component of such models. The approximate parametric model is fit to data using the basin hopping global optimization algorithm. Parameter constraints are enforced through Gaussian priors. The viability of our method is tested by examining its use in a finite-size scaling analysis of the $q$-state Potts model and $p$-state clock model with $q=2,3$ and $p=4,\\infty$.","sentences":["Common to many analysis pipelines in lattice gauge theory and the broader scientific discipline is the need to fit a semi-parametric model to data.","We propose a fit method that utilizes a radial basis function network to approximate the non-parametric component of such models.","The approximate parametric model is fit to data using the basin hopping global optimization algorithm.","Parameter constraints are enforced through Gaussian priors.","The viability of our method is tested by examining its use in a finite-size scaling analysis of the $q$-state Potts model and $p$-state clock model with $q=2,3$ and $p=4,\\infty$."],"url":"http://arxiv.org/abs/2402.04175v1","category":"hep-lat"}
{"created":"2024-02-06 17:27:09","title":"An overview of existing and new nuclear and astrophysical constraints on the equation of state of neutron-rich dense matter","abstract":"Through continuous progress in nuclear theory and experiment and an increasing number of neutron-star observations, a multitude of information about the equation of state (EOS) for matter at extreme densities is available. Here, we apply these different pieces of data individually to a broad set of physics-agnostic candidate EOSs and analyze the resulting constraints. Specifically, we make use of information from chiral effective field theory, perturbative quantum chromodynamics, as well as data from heavy-ion collisions and the PREX-II and CREX experiments. We also investigate the impact of current mass and radius measurements of neutron stars, such as radio timing measurements of heavy pulsars, NICER data, and other X-ray observations. We augment these by reanalyses of the gravitational-wave (GW) signal GW170817, its associated kilonova AT2017gfo and gamma-ray burst afterglow, the GW signal GW190425, and the GRB211211A afterglow, where we use improved models for the tidal waveform and kilonova light curves. Additionally, we consider the postmerger fate of GW170817 and its consequences for the EOS. This large and diverse set of constraints is eventually combined in numerous ways to explore limits on quantities such as the typical neutron-star radius, the maximum neutron-star mass, the nuclear symmetry-energy parameters, and the speed of sound. Based on the priors from our EOS candidate set, we find the radius of the canonical 1.4 M$_\\odot$ neutron star to be $R_{1.4}= 12.27_{-0.94}^{+0.83}$ km and the TOV mass $M_{\\rm TOV}= 2.26_{-0.22}^{+0.45}$ M$_\\odot$ at 95% credibility, when including those constraints where systematic uncertainties are deemed small. A less conservative approach, combining all the presented constraints, similarly yields $R_{1.4}= 12.20_{-0.50}^{+0.53}$ km and $M_{\\rm TOV}= 2.31_{-0.20}^{+0.08}$ M$_\\odot$.","sentences":["Through continuous progress in nuclear theory and experiment and an increasing number of neutron-star observations, a multitude of information about the equation of state (EOS) for matter at extreme densities is available.","Here, we apply these different pieces of data individually to a broad set of physics-agnostic candidate EOSs and analyze the resulting constraints.","Specifically, we make use of information from chiral effective field theory, perturbative quantum chromodynamics, as well as data from heavy-ion collisions and the PREX-II and CREX experiments.","We also investigate the impact of current mass and radius measurements of neutron stars, such as radio timing measurements of heavy pulsars, NICER data, and other X-ray observations.","We augment these by reanalyses of the gravitational-wave (GW) signal GW170817, its associated kilonova AT2017gfo and gamma-ray burst afterglow, the GW signal GW190425, and the GRB211211A afterglow, where we use improved models for the tidal waveform and kilonova light curves.","Additionally, we consider the postmerger fate of GW170817 and its consequences for the EOS.","This large and diverse set of constraints is eventually combined in numerous ways to explore limits on quantities such as the typical neutron-star radius, the maximum neutron-star mass, the nuclear symmetry-energy parameters, and the speed of sound.","Based on the priors from our EOS candidate set, we find the radius of the canonical 1.4 M$_\\odot$ neutron star to be $R_{1.4}= 12.27_{-0.94}^{+0.83}$ km and the TOV mass $M_{\\rm TOV}=","2.26_{-0.22}^{+0.45}$ M$_\\odot$ at 95% credibility, when including those constraints where systematic uncertainties are deemed small.","A less conservative approach, combining all the presented constraints, similarly yields $R_{1.4}= 12.20_{-0.50}^{+0.53}$ km and $M_{\\rm TOV}= 2.31_{-0.20}^{+0.08}$ M$_\\odot$."],"url":"http://arxiv.org/abs/2402.04172v1","category":"astro-ph.HE"}
{"created":"2024-02-06 17:25:22","title":"Heavy Quarkonia, Heavy-Light Tetraquarks and the Chiral Quark-Soliton Model","abstract":"We apply the Chiral Quark-Soliton Model used previously to describe baryons with one heavy quark to the case of heavy tetraquarks. We argue, that the model is insenstive to the nature of the heavy object bound by the soliton, i.e. to its mass and spin. Therefore, a heavy quark can be replaced by an anti-diquark without modifying the soliton background. Diquark dynamics is taken into account by means of the nonrelarivistic Schr\\\"odinger equation with the Cornell potential. We fix the Cornell potential parameters from the charmonia and bottomia spectra. We first compute $B_c$ meson masses to check our fitting procedure, and then compute diquark masses by appropriately rescaling color factors in the Cornell potential. We ten compute tetraquark masses and confirm previous findings that only $bb$ tetraquarks are bound.","sentences":["We apply the Chiral Quark-Soliton Model used previously to describe baryons with one heavy quark to the case of heavy tetraquarks.","We argue, that the model is insenstive to the nature of the heavy object bound by the soliton, i.e. to its mass and spin.","Therefore, a heavy quark can be replaced by an anti-diquark without modifying the soliton background.","Diquark dynamics is taken into account by means of the nonrelarivistic Schr\\\"odinger equation with the Cornell potential.","We fix the Cornell potential parameters from the charmonia and bottomia spectra.","We first compute $B_c$ meson masses to check our fitting procedure, and then compute diquark masses by appropriately rescaling color factors in the Cornell potential.","We ten compute tetraquark masses and confirm previous findings that only $bb$ tetraquarks are bound."],"url":"http://arxiv.org/abs/2402.04169v1","category":"hep-ph"}
{"created":"2024-02-06 17:21:26","title":"A note on the persistence of multiplicity of eigenvalues of fractional Laplacian under perturbations","abstract":"We consider the eigenvalues problem for the the fractional Laplacian in a bounded domain Omega with Dirichlet boundary condition. A recent result by Fall, Ghimenti, Micheletti and Pistoia (CVPDE (2023)) states that under generic small perturbations of the coefficient of the equation or of the domain Omega all the eigenvalues are simple. In this paper we give a condition for which a perturbation of the coefficient or of the domain preserves the multiplicity of a given eigenvalue. Also, in the case of an eigenvalue of multiplicity 2 we prove that the set of perturbations of the coefficients which preserve the multiplicity is a smooth manifold of codimension $2$ in C^1(Omega).","sentences":["We consider the eigenvalues problem for the the fractional Laplacian in a bounded domain Omega with Dirichlet boundary condition.","A recent result by Fall, Ghimenti, Micheletti and Pistoia (CVPDE (2023)) states that under generic small perturbations of the coefficient of the equation or of the domain Omega all the eigenvalues are simple.","In this paper we give a condition for which a perturbation of the coefficient or of the domain preserves the multiplicity of a given eigenvalue.","Also, in the case of an eigenvalue of multiplicity 2 we prove that the set of perturbations of the coefficients which preserve the multiplicity is a smooth manifold of codimension $2$ in C^1(Omega)."],"url":"http://arxiv.org/abs/2402.04164v1","category":"math.AP"}
{"created":"2024-02-06 17:16:46","title":"Optimal transport in the frame of abstract Lax-Oleinik operator revisited","abstract":"This is our first paper on the extension of our recent work on the Lax-Oleinik commutators and its applications to the intrinsic approach of propagation of singularities of the viscosity solutions of Hamilton-Jacobi equations. We reformulate Kantorovich-Rubinstein duality theorem in the theory of optimal transport in terms of abstract Lax-Oleinik operators, and analyze the relevant optimal transport problem in the case the cost function $c(x,y)=h(t_1,t_2,x,y)$ is the fundamental solution of Hamilton-Jacobi equation. For further applications to the problem of cut locus and propagation of singularities in optimal transport, we introduce corresponding random Lax-Oleinik operators. We also study the problem of singularities for $c$-concave functions and its dynamical implication when $c$ is the fundamental solution with $t_2-t_1\\ll1$ and $t_2-t_1<\\infty$, and $c$ is the Peierls' barrier respectively.","sentences":["This is our first paper on the extension of our recent work on the Lax-Oleinik commutators and its applications to the intrinsic approach of propagation of singularities of the viscosity solutions of Hamilton-Jacobi equations.","We reformulate Kantorovich-Rubinstein duality theorem in the theory of optimal transport in terms of abstract Lax-Oleinik operators, and analyze the relevant optimal transport problem in the case the cost function $c(x,y)=h(t_1,t_2,x,y)$ is the fundamental solution of Hamilton-Jacobi equation.","For further applications to the problem of cut locus and propagation of singularities in optimal transport, we introduce corresponding random Lax-Oleinik operators.","We also study the problem of singularities for $c$-concave functions and its dynamical implication when $c$ is the fundamental solution with $t_2-t_1\\ll1$ and $t_2-t_1<\\infty$, and $c$ is the Peierls' barrier respectively."],"url":"http://arxiv.org/abs/2402.04159v1","category":"math.AP"}
{"created":"2024-02-06 16:26:18","title":"Multipass Quantum Process Tomography: Precision and Accuracy Enhancement","abstract":"We introduce a method to enhance the precision and accuracy of Quantum Process Tomography (QPT) by mitigating the errors caused by state preparation and measurement (SPAM), readout and shot noise. Instead of performing QPT solely on a single gate, we propose performing QPT on a sequence of multiple applications of the same gate. The method involves the measurement of the Pauli transfer matrix (PTM) by standard QPT of the multipass process, and then deduce the single-process PTM by two alternative approaches: an iterative approach which in theory delivers the exact result for small errors, and a linearized approach based on solving the Sylvester equation. We examine the efficiency of these two approaches through simulations on IBM Quantum using ibmq_qasm_simulator. Compared to the Randomized Benchmarking type of methods, the proposed method delivers the entire PTM rather than a single number (fidelity). Compared to standard QPT, our method delivers PTM with much higher accuracy and precision because it greatly reduces the SPAM, readout and shot noise errors. We use the proposed method to experimentally determine the PTM and the fidelity of the CNOT gate on the quantum processor ibmq_manila (Falcon r5.11L).","sentences":["We introduce a method to enhance the precision and accuracy of Quantum Process Tomography (QPT) by mitigating the errors caused by state preparation and measurement (SPAM), readout and shot noise.","Instead of performing QPT solely on a single gate, we propose performing QPT on a sequence of multiple applications of the same gate.","The method involves the measurement of the Pauli transfer matrix (PTM) by standard QPT of the multipass process, and then deduce the single-process PTM by two alternative approaches: an iterative approach which in theory delivers the exact result for small errors, and a linearized approach based on solving the Sylvester equation.","We examine the efficiency of these two approaches through simulations on IBM Quantum using ibmq_qasm_simulator.","Compared to the Randomized Benchmarking type of methods, the proposed method delivers the entire PTM rather than a single number (fidelity).","Compared to standard QPT, our method delivers PTM with much higher accuracy and precision because it greatly reduces the SPAM, readout and shot noise errors.","We use the proposed method to experimentally determine the PTM and the fidelity of the CNOT gate on the quantum processor ibmq_manila (Falcon r5.11L)."],"url":"http://arxiv.org/abs/2402.04128v1","category":"quant-ph"}
{"created":"2024-02-06 16:19:57","title":"Long time stability for cubic nonlinear Schr\u00f6dinger equations on non-rectangular flat tori","abstract":"We consider nonlinear Schr\\\"odinger equations on flat tori satisfying a simple and explicit Diophantine non-degeneracy condition. Provided that the nonlinearity contains a cubic term, we prove the almost global existence and stability of most of the small solutions in high regularity Sobolev spaces. To this end, we develop a normal form approach designed to handle general resonant Hamiltonian partial differential equations for which it is possible to modulate the frequencies by using the initial data.","sentences":["We consider nonlinear Schr\\\"odinger equations on flat tori satisfying a simple and explicit Diophantine non-degeneracy condition.","Provided that the nonlinearity contains a cubic term, we prove the almost global existence and stability of most of the small solutions in high regularity Sobolev spaces.","To this end, we develop a normal form approach designed to handle general resonant Hamiltonian partial differential equations for which it is possible to modulate the frequencies by using the initial data."],"url":"http://arxiv.org/abs/2402.04122v1","category":"math.AP"}
{"created":"2024-02-06 16:17:58","title":"Multivariable generalizations of bivariate means via invariance","abstract":"For a given $p$-variable mean $M \\colon I^p \\to I$ ($I$ is a subinterval of $\\mathbb{R}$), following (Horwitz, 2002) and (Lawson and Lim, 2008), we can define (under certain assumption) its $(p+1)$-variable $\\beta$-invariant extension as the unique solution $K \\colon I^{p+1} \\to I$ of the functional equation \\begin{align*} K\\big(M(x_2,\\dots,x_{p+1})&,M(x_1,x_3,\\dots,x_{p+1}),\\dots,M(x_1,\\dots,x_p)\\big)\\\\ &=K(x_1,\\dots,x_{p+1}), \\text{ for all }x_1,\\dots,x_{p+1} \\in I \\end{align*} in the family of means.   Applying this procedure iteratively we can obtain a mean which is defined for vectors of arbitrary lengths starting from the bivariate one. The aim of this paper is to study the properties of such extensions.","sentences":["For a given $p$-variable mean $M \\colon I^p \\to I$ ($I$ is a subinterval of $\\mathbb{R}$), following (Horwitz, 2002) and (Lawson and Lim, 2008), we can define (under certain assumption) its $(p+1)$-variable $\\beta$-invariant extension as the unique solution $K \\colon I^{p+1} \\to I$ of the functional equation \\begin{align*} K\\big(M(x_2,\\dots,x_{p+1})&,M(x_1,x_3,\\dots,x_{p+1}),\\dots,M(x_1,\\dots,x_p)\\big)\\\\ &=K(x_1,\\dots,x_{p+1}), \\text{ for all }x_1,\\dots,x_{p+1} \\in I \\end{align*} in the family of means.   ","Applying this procedure iteratively we can obtain a mean which is defined for vectors of arbitrary lengths starting from the bivariate one.","The aim of this paper is to study the properties of such extensions."],"url":"http://arxiv.org/abs/2402.04121v1","category":"math.DS"}
{"created":"2024-02-06 16:12:44","title":"Feynman rules and loop structure of Carrollian amplitude","abstract":"In this paper, we derive the Carrollian amplitude in the framework of bulk reduction. The Carrollian amplitude is shown to relate to the scattering amplitude by a Fourier transform in this method. We propose Feynman rules to calculate the Carrollian amplitude where the Fourier transforms emerge as the integral representation of the external lines in the Carrollian space. Then we study the four-point Carrollian amplitude at loop level in massless $\\Phi^4$ theory. As a consequence of Poincar\\'e invariance, the four-point Carrollian amplitude can be transformed to the amplitude that only depends on the cross ratio $z$ of the celestial sphere and a variable $\\chi$ invariant under translation. The four-point Carrollian amplitude is a polynomial of the two-point Carrollian amplitude whose argument is replaced with $\\chi$. The coefficients of the polynomial have branch cuts in the complex $z$ plane. We also show that the renormalized Carrollian amplitude obeys the Callan-Symanzik equation. Moreover, we initiate a generalized $\\Phi^4$ theory by designing the Feynman rules for more general Carrollian amplitude.","sentences":["In this paper, we derive the Carrollian amplitude in the framework of bulk reduction.","The Carrollian amplitude is shown to relate to the scattering amplitude by a Fourier transform in this method.","We propose Feynman rules to calculate the Carrollian amplitude where the Fourier transforms emerge as the integral representation of the external lines in the Carrollian space.","Then we study the four-point Carrollian amplitude at loop level in massless $\\Phi^4$ theory.","As a consequence of Poincar\\'e invariance, the four-point Carrollian amplitude can be transformed to the amplitude that only depends on the cross ratio $z$ of the celestial sphere and a variable $\\chi$ invariant under translation.","The four-point Carrollian amplitude is a polynomial of the two-point Carrollian amplitude whose argument is replaced with $\\chi$. The coefficients of the polynomial have branch cuts in the complex $z$ plane.","We also show that the renormalized Carrollian amplitude obeys the Callan-Symanzik equation.","Moreover, we initiate a generalized $\\Phi^4$ theory by designing the Feynman rules for more general Carrollian amplitude."],"url":"http://arxiv.org/abs/2402.04120v1","category":"hep-th"}
{"created":"2024-02-06 16:12:12","title":"An explicit Euler method for the continuity equation with Sobolev velocity fields","abstract":"We prove a stability estimate, in a suitable expected value, of the $1$-Wasserstein distance between the solution of the continuity equation under a Sobolev velocity field and a measure obtained by pushing forward Dirac deltas whose centers belong to a partition of the domain by a (sort of) explicit forward Euler method. The main tool is a $L^\\infty_t (L^p_x)$ estimate on the difference between the regular Lagrangian flow of the velocity field and an explicitly constructed approximation of such flow. Although our result only gives estimates in expected value, it has the advantage of being easily parallelizable and of not relying on any particular structure on the mesh. At the end, we also provide estimates with a logarithmic Wasserstein distance, already used in other works on this particular problem.","sentences":["We prove a stability estimate, in a suitable expected value, of the $1$-Wasserstein distance between the solution of the continuity equation under a Sobolev velocity field and a measure obtained by pushing forward Dirac deltas whose centers belong to a partition of the domain by a (sort of) explicit forward Euler method.","The main tool is a $L^\\infty_t (L^p_x)$ estimate on the difference between the regular Lagrangian flow of the velocity field and an explicitly constructed approximation of such flow.","Although our result only gives estimates in expected value, it has the advantage of being easily parallelizable and of not relying on any particular structure on the mesh.","At the end, we also provide estimates with a logarithmic Wasserstein distance, already used in other works on this particular problem."],"url":"http://arxiv.org/abs/2402.04118v1","category":"math.AP"}
{"created":"2024-02-06 16:05:18","title":"The open XYZ spin 1/2 chain: Separation of Variables and scalar products for boundary fields related by a constraint","abstract":"We consider the open XYZ spin chain with boundary fields. We solve the model by the new Separation of Variables approach introduced in arXiv:1904.00852. In this framework, the transfer matrix eigenstates are obtained as a particular sub-class of the class of so-called separate states. We consider the problem of computing scalar products of such separate states. As usual, they can be represented as determinants with rows labelled by the inhomogeneity parameters of the model. We notably focus on the special case in which the boundary parameters parametrising the two boundary fields satisfy one constraint, hence enabling for the description of part of the transfer matrix spectrum and eigenstates in terms of some elliptic polynomial Q-solution of a usual TQ-equation. In this case, we show how to transform the aforementioned determinant for the scalar product into some more convenient form for the consideration of the homogeneous and thermodynamic limits: as in the open XXX or XXZ cases, our result can be expressed as some generalisation of the so-called Slavnov determinant.","sentences":["We consider the open XYZ spin chain with boundary fields.","We solve the model by the new Separation of Variables approach introduced in arXiv:1904.00852.","In this framework, the transfer matrix eigenstates are obtained as a particular sub-class of the class of so-called separate states.","We consider the problem of computing scalar products of such separate states.","As usual, they can be represented as determinants with rows labelled by the inhomogeneity parameters of the model.","We notably focus on the special case in which the boundary parameters parametrising the two boundary fields satisfy one constraint, hence enabling for the description of part of the transfer matrix spectrum and eigenstates in terms of some elliptic polynomial Q-solution of a usual TQ-equation.","In this case, we show how to transform the aforementioned determinant for the scalar product into some more convenient form for the consideration of the homogeneous and thermodynamic limits: as in the open XXX or XXZ cases, our result can be expressed as some generalisation of the so-called Slavnov determinant."],"url":"http://arxiv.org/abs/2402.04112v1","category":"math-ph"}
{"created":"2024-02-06 15:51:10","title":"Stochastic theta methods for free stochastic differential equations","abstract":"We introduce free probability analogues of the stochastic theta methods for free stochastic differential equations, which generalize the free Euler-Maruyama method introduced by Schl\\\"{u}chtermann and Wibmer [27]. Under some mild conditions, we prove the strong convergence and exponential stability in mean square of the numerical solution. The free stochastic theta method with $\\theta=1$ can inherit the exponential stability of original equations for any given step size. Our method can offer better stability and efficiency than the free Euler-Maruyama method. Moreover, numerical results are reported to confirm these theoretical findings.","sentences":["We introduce free probability analogues of the stochastic theta methods for free stochastic differential equations, which generalize the free Euler-Maruyama method introduced by Schl\\\"{u}chtermann and Wibmer [27].","Under some mild conditions, we prove the strong convergence and exponential stability in mean square of the numerical solution.","The free stochastic theta method with $\\theta=1$ can inherit the exponential stability of original equations for any given step size.","Our method can offer better stability and efficiency than the free Euler-Maruyama method.","Moreover, numerical results are reported to confirm these theoretical findings."],"url":"http://arxiv.org/abs/2402.04094v1","category":"math.NA"}
{"created":"2024-02-06 15:34:30","title":"Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning","abstract":"This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets. To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at \\href{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}.","sentences":["This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL).","At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy.","We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets.","To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement.","By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks.","Code is available at \\href{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}."],"url":"http://arxiv.org/abs/2402.04080v1","category":"cs.LG"}
{"created":"2024-02-06 14:50:00","title":"A nodal ghost method based on variational formulation and regular square grid for elliptic problems on arbitrary domains in two space dimensions","abstract":"This paper focuses on the numerical solution of elliptic partial differential equations (PDEs) with Dirichlet and mixed boundary conditions, specifically addressing the challenges arising from irregular domains. Both finite element method (FEM) and finite difference method (FDM), face difficulties in dealing with arbitrary domains. The paper introduces a novel nodal symmetric ghost finite element method approach, which combines the advantages of FEM and FDM. The method employs bilinear finite elements on a structured mesh, and provides a detailed implementation description. A rigorous a priori convergence rate analysis is also presented. The convergence rates are validated with many numerical experiments, in both one and two space dimensions.","sentences":["This paper focuses on the numerical solution of elliptic partial differential equations (PDEs) with Dirichlet and mixed boundary conditions, specifically addressing the challenges arising from irregular domains.","Both finite element method (FEM) and finite difference method (FDM), face difficulties in dealing with arbitrary domains.","The paper introduces a novel nodal symmetric ghost finite element method approach, which combines the advantages of FEM and FDM.","The method employs bilinear finite elements on a structured mesh, and provides a detailed implementation description.","A rigorous a priori convergence rate analysis is also presented.","The convergence rates are validated with many numerical experiments, in both one and two space dimensions."],"url":"http://arxiv.org/abs/2402.04048v1","category":"math.NA"}
{"created":"2024-02-06 14:49:58","title":"Does $E=mc^2$ Require Relativity?","abstract":"It is universally believed that with his 1905 paper ``Does the inertia of a body depend on its energy content?\" Einstein first demonstrated the equivalence of mass and energy by making use of his new special theory of relativity. In the final step of that paper, however, Einstein equates the kinetic energy of a body to its Newtonian value, indicating that his result is at best a low-velocity approximation. Today, several characters debate whether a mid-nineteenth century physicist, employing only Galilean and pre-Maxwellian physics could plausibly arrive at the celebrated result. In other words, is Einsteinian relativity necessary to derive ${\\mathcal E}=mc^2$?","sentences":["It is universally believed that with his 1905 paper ``Does the inertia of a body depend on its energy content?\"","Einstein first demonstrated the equivalence of mass and energy by making use of his new special theory of relativity.","In the final step of that paper, however, Einstein equates the kinetic energy of a body to its Newtonian value, indicating that his result is at best a low-velocity approximation.","Today, several characters debate whether a mid-nineteenth century physicist, employing only Galilean and pre-Maxwellian physics could plausibly arrive at the celebrated result.","In other words, is Einsteinian relativity necessary to derive ${\\mathcal E}=mc^2$?"],"url":"http://arxiv.org/abs/2402.04047v1","category":"physics.hist-ph"}
{"created":"2024-02-06 14:42:03","title":"A residual-based non-orthogonality correction for force-balanced unstructured Volume-of-Fluid methods","abstract":"Non-orthogonality errors in unstructured Finite Volume methods for simulating incompressible two-phase flows may break the force-balanced discretization. We show that applying the same explicit non-orthogonality correction for all gradient terms in the context of segregated solution algorithms is not sufficient to achieve force balance. To ensure force balance, we introduce a straightforward and deterministic residual-based control of the non-orthogonality correction, which removes the number of non-orthogonality corrections as a free parameter from the simulation. Our method is directly applicable to different unstructured finite-volume two-phase flow simulation methods as long as they discretize the one-field formulation of incompressible two-phase Navier-Stokes equations. We demonstrate force balance for the surface tension force and the gravity force near linear solver tolerance for an algebraic and a geometric Volume-of-Fluid method using the stationary droplet and stationary water column verification cases on polyhedral unstructured meshes with varying levels of non-orthogonality.","sentences":["Non-orthogonality errors in unstructured Finite Volume methods for simulating incompressible two-phase flows may break the force-balanced discretization.","We show that applying the same explicit non-orthogonality correction for all gradient terms in the context of segregated solution algorithms is not sufficient to achieve force balance.","To ensure force balance, we introduce a straightforward and deterministic residual-based control of the non-orthogonality correction, which removes the number of non-orthogonality corrections as a free parameter from the simulation.","Our method is directly applicable to different unstructured finite-volume two-phase flow simulation methods as long as they discretize the one-field formulation of incompressible two-phase Navier-Stokes equations.","We demonstrate force balance for the surface tension force and the gravity force near linear solver tolerance for an algebraic and a geometric Volume-of-Fluid method using the stationary droplet and stationary water column verification cases on polyhedral unstructured meshes with varying levels of non-orthogonality."],"url":"http://arxiv.org/abs/2402.04043v1","category":"physics.comp-ph"}
{"created":"2024-02-06 14:34:17","title":"PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural Network","abstract":"Graph neural networks (GNNs) have gained popularity for various graph-related tasks. However, similar to deep neural networks, GNNs are also vulnerable to adversarial attacks. Empirical studies have shown that adversarially robust generalization has a pivotal role in establishing effective defense algorithms against adversarial attacks. In this paper, we contribute by providing adversarially robust generalization bounds for two kinds of popular GNNs, graph convolutional network (GCN) and message passing graph neural network, using the PAC-Bayesian framework. Our result reveals that spectral norm of the diffusion matrix on the graph and spectral norm of the weights as well as the perturbation factor govern the robust generalization bounds of both models. Our bounds are nontrivial generalizations of the results developed in (Liao et al., 2020) from the standard setting to adversarial setting while avoiding exponential dependence of the maximum node degree. As corollaries, we derive better PAC-Bayesian robust generalization bounds for GCN in the standard setting, which improve the bounds in (Liao et al., 2020) by avoiding exponential dependence on the maximum node degree.","sentences":["Graph neural networks (GNNs) have gained popularity for various graph-related tasks.","However, similar to deep neural networks, GNNs are also vulnerable to adversarial attacks.","Empirical studies have shown that adversarially robust generalization has a pivotal role in establishing effective defense algorithms against adversarial attacks.","In this paper, we contribute by providing adversarially robust generalization bounds for two kinds of popular GNNs, graph convolutional network (GCN) and message passing graph neural network, using the PAC-Bayesian framework.","Our result reveals that spectral norm of the diffusion matrix on the graph and spectral norm of the weights as well as the perturbation factor govern the robust generalization bounds of both models.","Our bounds are nontrivial generalizations of the results developed in (Liao et al., 2020) from the standard setting to adversarial setting while avoiding exponential dependence of the maximum node degree.","As corollaries, we derive better PAC-Bayesian robust generalization bounds for GCN in the standard setting, which improve the bounds in (Liao et al., 2020) by avoiding exponential dependence on the maximum node degree."],"url":"http://arxiv.org/abs/2402.04038v1","category":"stat.ML"}
{"created":"2024-02-06 14:26:22","title":"On provable privacy vulnerabilities of graph representations","abstract":"Graph representation learning (GRL) is critical for extracting insights from complex network structures, but it also raises security concerns due to potential privacy vulnerabilities in these representations. This paper investigates the structural vulnerabilities in graph neural models where sensitive topological information can be inferred through edge reconstruction attacks. Our research primarily addresses the theoretical underpinnings of cosine-similarity-based edge reconstruction attacks (COSERA), providing theoretical and empirical evidence that such attacks can perfectly reconstruct sparse Erdos Renyi graphs with independent random features as graph size increases. Conversely, we establish that sparsity is a critical factor for COSERA's effectiveness, as demonstrated through analysis and experiments on stochastic block models. Finally, we explore the resilience of (provably) private graph representations produced via noisy aggregation (NAG) mechanism against COSERA. We empirically delineate instances wherein COSERA demonstrates both efficacy and deficiency in its capacity to function as an instrument for elucidating the trade-off between privacy and utility.","sentences":["Graph representation learning (GRL) is critical for extracting insights from complex network structures, but it also raises security concerns due to potential privacy vulnerabilities in these representations.","This paper investigates the structural vulnerabilities in graph neural models where sensitive topological information can be inferred through edge reconstruction attacks.","Our research primarily addresses the theoretical underpinnings of cosine-similarity-based edge reconstruction attacks (COSERA), providing theoretical and empirical evidence that such attacks can perfectly reconstruct sparse Erdos Renyi graphs with independent random features as graph size increases.","Conversely, we establish that sparsity is a critical factor for COSERA's effectiveness, as demonstrated through analysis and experiments on stochastic block models.","Finally, we explore the resilience of (provably) private graph representations produced via noisy aggregation (NAG) mechanism against COSERA.","We empirically delineate instances wherein COSERA demonstrates both efficacy and deficiency in its capacity to function as an instrument for elucidating the trade-off between privacy and utility."],"url":"http://arxiv.org/abs/2402.04033v1","category":"cs.LG"}
{"created":"2024-02-06 14:25:09","title":"Reducing the Cost of Quantum Chemical Data By Backpropagating Through Density Functional Theory","abstract":"Density Functional Theory (DFT) accurately predicts the quantum chemical properties of molecules, but scales as $O(N_{\\text{electrons}}^3)$. Sch\\\"utt et al. (2019) successfully approximate DFT 1000x faster with Neural Networks (NN). Arguably, the biggest problem one faces when scaling to larger molecules is the cost of DFT labels. For example, it took years to create the PCQ dataset (Nakata & Shimazaki, 2017) on which subsequent NNs are trained within a week. DFT labels molecules by minimizing energy $E(\\cdot )$ as a \"loss function.\" We bypass dataset creation by directly training NNs with $E(\\cdot )$ as a loss function. For comparison, Sch\\\"utt et al. (2019) spent 626 hours creating a dataset on which they trained their NN for 160h, for a total of 786h; our method achieves comparable performance within 31h.","sentences":["Density Functional Theory (DFT) accurately predicts the quantum chemical properties of molecules, but scales as $O(N_{\\text{electrons}}^3)$. Sch\\\"utt et al.","(2019) successfully approximate DFT 1000x faster with Neural Networks (NN).","Arguably, the biggest problem one faces when scaling to larger molecules is the cost of DFT labels.","For example, it took years to create the PCQ dataset (Nakata & Shimazaki, 2017) on which subsequent NNs are trained within a week.","DFT labels molecules by minimizing energy $E(\\cdot )$ as a \"loss function.\"","We bypass dataset creation by directly training NNs with $E(\\cdot )$ as a loss function.","For comparison, Sch\\\"utt et al. (2019) spent 626 hours creating a dataset on which they trained their NN for 160h, for a total of 786h; our method achieves comparable performance within 31h."],"url":"http://arxiv.org/abs/2402.04030v1","category":"cs.LG"}
{"created":"2024-02-06 14:24:29","title":"Positive concave deep equilibrium models","abstract":"Deep equilibrium (DEQ) models are widely recognized as a memory efficient alternative to standard neural networks, achieving state-of-the-art performance in language modeling and computer vision tasks. These models solve a fixed point equation instead of explicitly computing the output, which sets them apart from standard neural networks. However, existing DEQ models often lack formal guarantees of the existence and uniqueness of the fixed point, and the convergence of the numerical scheme used for computing the fixed point is not formally established. As a result, DEQ models are potentially unstable in practice. To address these drawbacks, we introduce a novel class of DEQ models called positive concave deep equilibrium (pcDEQ) models. Our approach, which is based on nonlinear Perron-Frobenius theory, enforces nonnegative weights and activation functions that are concave on the positive orthant. By imposing these constraints, we can easily ensure the existence and uniqueness of the fixed point without relying on additional complex assumptions commonly found in the DEQ literature, such as those based on monotone operator theory in convex analysis. Furthermore, the fixed point can be computed with the standard fixed point algorithm, and we provide theoretical guarantees of geometric convergence, which, in particular, simplifies the training process. Experiments demonstrate the competitiveness of our pcDEQ models against other implicit models.","sentences":["Deep equilibrium (DEQ) models are widely recognized as a memory efficient alternative to standard neural networks, achieving state-of-the-art performance in language modeling and computer vision tasks.","These models solve a fixed point equation instead of explicitly computing the output, which sets them apart from standard neural networks.","However, existing DEQ models often lack formal guarantees of the existence and uniqueness of the fixed point, and the convergence of the numerical scheme used for computing the fixed point is not formally established.","As a result, DEQ models are potentially unstable in practice.","To address these drawbacks, we introduce a novel class of DEQ models called positive concave deep equilibrium (pcDEQ) models.","Our approach, which is based on nonlinear Perron-Frobenius theory, enforces nonnegative weights and activation functions that are concave on the positive orthant.","By imposing these constraints, we can easily ensure the existence and uniqueness of the fixed point without relying on additional complex assumptions commonly found in the DEQ literature, such as those based on monotone operator theory in convex analysis.","Furthermore, the fixed point can be computed with the standard fixed point algorithm, and we provide theoretical guarantees of geometric convergence, which, in particular, simplifies the training process.","Experiments demonstrate the competitiveness of our pcDEQ models against other implicit models."],"url":"http://arxiv.org/abs/2402.04029v1","category":"cs.LG"}
{"created":"2024-02-06 14:11:18","title":"ALE spaces and nodal curves","abstract":"We consider the twistor theory approach to Kronheimer's ALE metrics on resolutions of the quotient of C^2 by a finite subgroup of SU(2). The circle action on the 4-manifold induces a C^* action on a compactification of the twistor space and we identify the orbit of a generic twistor line as a nodal rational curve in a particular cohomology class of a projective rational surface. Using the results of N.Honda et al we identify this surface with the minitwistor space for the Einstein-Weyl structure on the 3-dimensional quotient of the ALE space by the circle action.","sentences":["We consider the twistor theory approach to Kronheimer's ALE metrics on resolutions of the quotient of C^2 by a finite subgroup of SU(2).","The circle action on the 4-manifold induces a C^* action on a compactification of the twistor space and we identify the orbit of a generic twistor line as a nodal rational curve in a particular cohomology class of a projective rational surface.","Using the results of N.Honda et al we identify this surface with the minitwistor space for the Einstein-Weyl structure on the 3-dimensional quotient of the ALE space by the circle action."],"url":"http://arxiv.org/abs/2402.04021v1","category":"math.DG"}
{"created":"2024-02-06 13:50:52","title":"Generalized almost-K\u00e4hler-Ricci solitons","abstract":"We generalize K\\\"ahler-Ricci solitons to the almost-K\\\"ahler setting as the zeros of Inoue's moment map \\cite{MR4017922}, and show that their existence is an obstruction to the existence of first-Chern-Einstein almost-K\\\"ahler metrics on compact symplectic Fano manifolds. We prove deformation results of such metrics in the $4$-dimensional case. Moreover, we study the Lie algebra of holomorphic vector fields on $2n$-dimensional compact symplectic Fano manifolds admitting generalized almost-K\\\"ahler-Ricci solitons. In particular, we partially extend Matsushima's theorem \\cite{MR0094478} to compact first-Chern-Einstein almost-K\\\"ahler manifolds.","sentences":["We generalize K\\\"ahler-Ricci solitons to the almost-K\\\"ahler setting as the zeros of Inoue's moment map \\cite{MR4017922}, and show that their existence is an obstruction to the existence of first-Chern-Einstein almost-K\\\"ahler metrics on compact symplectic Fano manifolds.","We prove deformation results of such metrics in the $4$-dimensional case.","Moreover, we study the Lie algebra of holomorphic vector fields on $2n$-dimensional compact symplectic Fano manifolds admitting generalized almost-K\\\"ahler-Ricci solitons.","In particular, we partially extend Matsushima's theorem \\cite{MR0094478} to compact first-Chern-Einstein almost-K\\\"ahler manifolds."],"url":"http://arxiv.org/abs/2402.03996v1","category":"math.DG"}
{"created":"2024-02-06 13:47:12","title":"Gradient Sketches for Training Data Attribution and Studying the Loss Landscape","abstract":"Random projections or sketches of gradients and Hessian vector products play an essential role in applications where one needs to store many such vectors while retaining accurate information about their relative geometry. Two important scenarios are training data attribution (tracing a model's behavior to the training data), where one needs to store a gradient for each training example, and the study of the spectrum of the Hessian (to analyze the training dynamics), where one needs to store multiple Hessian vector products. While sketches that use dense matrices are easy to implement, they are memory bound and cannot be scaled to modern neural networks. Motivated by work on the intrinsic dimension of neural networks, we propose and study a design space for scalable sketching algorithms. We demonstrate the efficacy of our approach in three applications: training data attribution, the analysis of the Hessian spectrum and the computation of the intrinsic dimension when fine-tuning pre-trained language models.","sentences":["Random projections or sketches of gradients and Hessian vector products play an essential role in applications where one needs to store many such vectors while retaining accurate information about their relative geometry.","Two important scenarios are training data attribution (tracing a model's behavior to the training data), where one needs to store a gradient for each training example, and the study of the spectrum of the Hessian (to analyze the training dynamics), where one needs to store multiple Hessian vector products.","While sketches that use dense matrices are easy to implement, they are memory bound and cannot be scaled to modern neural networks.","Motivated by work on the intrinsic dimension of neural networks, we propose and study a design space for scalable sketching algorithms.","We demonstrate the efficacy of our approach in three applications: training data attribution, the analysis of the Hessian spectrum and the computation of the intrinsic dimension when fine-tuning pre-trained language models."],"url":"http://arxiv.org/abs/2402.03994v1","category":"cs.LG"}
{"created":"2024-02-06 13:44:39","title":"Neural Rank Collapse: Weight Decay and Small Within-Class Variability Yield Low-Rank Bias","abstract":"Recent work in deep learning has shown strong empirical and theoretical evidence of an implicit low-rank bias: weight matrices in deep networks tend to be approximately low-rank and removing relatively small singular values during training or from available trained models may significantly reduce model size while maintaining or even improving model performance. However, the majority of the theoretical investigations around low-rank bias in neural networks deal with oversimplified deep linear networks. In this work, we consider general networks with nonlinear activations and the weight decay parameter, and we show the presence of an intriguing neural rank collapse phenomenon, connecting the low-rank bias of trained networks with networks' neural collapse properties: as the weight decay parameter grows, the rank of each layer in the network decreases proportionally to the within-class variability of the hidden-space embeddings of the previous layers. Our theoretical findings are supported by a range of experimental evaluations illustrating the phenomenon.","sentences":["Recent work in deep learning has shown strong empirical and theoretical evidence of an implicit low-rank bias: weight matrices in deep networks tend to be approximately low-rank and removing relatively small singular values during training or from available trained models may significantly reduce model size while maintaining or even improving model performance.","However, the majority of the theoretical investigations around low-rank bias in neural networks deal with oversimplified deep linear networks.","In this work, we consider general networks with nonlinear activations and the weight decay parameter, and we show the presence of an intriguing neural rank collapse phenomenon, connecting the low-rank bias of trained networks with networks' neural collapse properties: as the weight decay parameter grows, the rank of each layer in the network decreases proportionally to the within-class variability of the hidden-space embeddings of the previous layers.","Our theoretical findings are supported by a range of experimental evaluations illustrating the phenomenon."],"url":"http://arxiv.org/abs/2402.03991v1","category":"cs.LG"}
{"created":"2024-02-06 13:43:22","title":"Subsampling is not Magic: Why Large Batch Sizes Work for Differentially Private Stochastic Optimisation","abstract":"We study the effect of the batch size to the total gradient variance in differentially private stochastic gradient descent (DP-SGD), seeking a theoretical explanation for the usefulness of large batch sizes. As DP-SGD is the basis of modern DP deep learning, its properties have been widely studied, and recent works have empirically found large batch sizes to be beneficial. However, theoretical explanations of this benefit are currently heuristic at best. We first observe that the total gradient variance in DP-SGD can be decomposed into subsampling-induced and noise-induced variances. We then prove that in the limit of an infinite number of iterations, the effective noise-induced variance is invariant to the batch size. The remaining subsampling-induced variance decreases with larger batch sizes, so large batches reduce the effective total gradient variance. We confirm numerically that the asymptotic regime is relevant in practical settings when the batch size is not small, and find that outside the asymptotic regime, the total gradient variance decreases even more with large batch sizes. We also find a sufficient condition that implies that large batch sizes similarly reduce effective DP noise variance for one iteration of DP-SGD.","sentences":["We study the effect of the batch size to the total gradient variance in differentially private stochastic gradient descent (DP-SGD), seeking a theoretical explanation for the usefulness of large batch sizes.","As DP-SGD is the basis of modern DP deep learning, its properties have been widely studied, and recent works have empirically found large batch sizes to be beneficial.","However, theoretical explanations of this benefit are currently heuristic at best.","We first observe that the total gradient variance in DP-SGD can be decomposed into subsampling-induced and noise-induced variances.","We then prove that in the limit of an infinite number of iterations, the effective noise-induced variance is invariant to the batch size.","The remaining subsampling-induced variance decreases with larger batch sizes, so large batches reduce the effective total gradient variance.","We confirm numerically that the asymptotic regime is relevant in practical settings when the batch size is not small, and find that outside the asymptotic regime, the total gradient variance decreases even more with large batch sizes.","We also find a sufficient condition that implies that large batch sizes similarly reduce effective DP noise variance for one iteration of DP-SGD."],"url":"http://arxiv.org/abs/2402.03990v1","category":"stat.ML"}
{"created":"2024-02-06 18:59:24","title":"Computing Approximate Nash Equilibria for Integer Programming Games","abstract":"We propose a framework to compute approximate Nash equilibria in integer programming games with nonlinear payoffs, i.e., simultaneous and non-cooperative games where each player solves a parametrized mixed-integer nonlinear program. We prove that using absolute approximations of the players' objective functions and then computing its Nash equilibria is equivalent to computing approximate Nash equilibria where the approximation factor is doubled. In practice, we propose an algorithm to approximate the players' objective functions via piecewise linear approximations. Our numerical experiments on a cybersecurity investment game show the computational effectiveness of our approach.","sentences":["We propose a framework to compute approximate Nash equilibria in integer programming games with nonlinear payoffs, i.e., simultaneous and non-cooperative games where each player solves a parametrized mixed-integer nonlinear program.","We prove that using absolute approximations of the players' objective functions and then computing its Nash equilibria is equivalent to computing approximate Nash equilibria where the approximation factor is doubled.","In practice, we propose an algorithm to approximate the players' objective functions via piecewise linear approximations.","Our numerical experiments on a cybersecurity investment game show the computational effectiveness of our approach."],"url":"http://arxiv.org/abs/2402.04250v1","category":"math.OC"}
{"created":"2024-02-06 18:56:35","title":"Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks","abstract":"State-space models (SSMs), such as Mamba Gu & Dao (2034), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, \\variant, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently. Our findings suggest that hybrid architectures offer promising avenues for enhancing ICL in language models.","sentences":["State-space models (SSMs), such as Mamba Gu & Dao (2034), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention.","Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers.","In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks.","Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning.","However, SSMs fall short in tasks involving non-standard retrieval functionality.","To address these limitations, we introduce a hybrid model, \\variant, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently.","Our findings suggest that hybrid architectures offer promising avenues for enhancing ICL in language models."],"url":"http://arxiv.org/abs/2402.04248v1","category":"cs.LG"}
{"created":"2024-02-06 18:47:52","title":"CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers","abstract":"The Transformer architecture has shown to be a powerful tool for a wide range of tasks. It is based on the self-attention mechanism, which is an inherently computationally expensive operation with quadratic computational complexity: memory usage and compute time increase quadratically with the length of the input sequences, thus limiting the application of Transformers. In this work, we propose a novel Clustering self-Attention mechanism using Surrogate Tokens (CAST), to optimize the attention computation and achieve efficient transformers. CAST utilizes learnable surrogate tokens to construct a cluster affinity matrix, used to cluster the input sequence and generate novel cluster summaries. The self-attention from within each cluster is then combined with the cluster summaries of other clusters, enabling information flow across the entire input sequence. CAST improves efficiency by reducing the complexity from $O(N^2)$ to $O(\\alpha N)$ where N is the sequence length, and {\\alpha} is constant according to the number of clusters and samples per cluster. We show that CAST performs better than or comparable to the baseline Transformers on long-range sequence modeling tasks, while also achieving higher results on time and memory efficiency than other efficient transformers.","sentences":["The Transformer architecture has shown to be a powerful tool for a wide range of tasks.","It is based on the self-attention mechanism, which is an inherently computationally expensive operation with quadratic computational complexity: memory usage and compute time increase quadratically with the length of the input sequences, thus limiting the application of Transformers.","In this work, we propose a novel Clustering self-Attention mechanism using Surrogate Tokens (CAST), to optimize the attention computation and achieve efficient transformers.","CAST utilizes learnable surrogate tokens to construct a cluster affinity matrix, used to cluster the input sequence and generate novel cluster summaries.","The self-attention from within each cluster is then combined with the cluster summaries of other clusters, enabling information flow across the entire input sequence.","CAST improves efficiency by reducing the complexity from $O(N^2)$ to $O(\\alpha N)$ where N is the sequence length, and {\\alpha} is constant according to the number of clusters and samples per cluster.","We show that CAST performs better than or comparable to the baseline Transformers on long-range sequence modeling tasks, while also achieving higher results on time and memory efficiency than other efficient transformers."],"url":"http://arxiv.org/abs/2402.04239v1","category":"cs.LG"}
{"created":"2024-02-06 18:37:14","title":"Medium Resolution 0.97-5.3 micron spectra of Very Young Benchmark Brown Dwarfs with NIRSpec onboard the James Webb Space Telescope","abstract":"Spectra of young benchmark brown dwarfs with well-known ages are vital to characterize other brown dwarfs, for which ages are in general not known. These spectra are also crucial to test atmospheric models which have the potential to provide detailed information about the atmospheres of these objects. However, to optimally test atmospheric models, medium-resolution, long-wavelength coverage spectra with well-understood uncertainties are ideal, such as the spectra provided by the NIRSpec instrument onboard the James Webb Space Telescope. In this paper, we present the medium-resolution JWST/NIRSpec spectra of two young brown dwarfs, TWA 28 (M9.0) and TWA 27A (M9.0), and one planetary-mass object, TWA 27B (L6.0), members of the TW Hydrae Association (~10 Myr). We show the richness of the atomic lines and molecular bands present in the spectra. All objects show signs of a circumstellar disk, via near-infrared excess and/or via emission lines. We matched a set of cloudless atmospheric spectra (ATMO), and cloudy atmospheric spectra (BT-Settl) to our NIRSpec spectra, and analyzed which wavelength ranges and spectral features both models reproduce best. Both models derive consistent parameters for the three sources, and predict the existence of CH4 at 3.35 microns in TWA 27B. Nonetheless, in contrast to other slightly older objects with similar spectral type, like PSO 318.5-22 and VHS 1256b, this feature is not present in the spectrum of TWA 27B. The lack of the CH4 feature might suggest that the L/T transition of very young dwarfs starts at later spectral types than for older brown dwarfs.","sentences":["Spectra of young benchmark brown dwarfs with well-known ages are vital to characterize other brown dwarfs, for which ages are in general not known.","These spectra are also crucial to test atmospheric models which have the potential to provide detailed information about the atmospheres of these objects.","However, to optimally test atmospheric models, medium-resolution, long-wavelength coverage spectra with well-understood uncertainties are ideal, such as the spectra provided by the NIRSpec instrument onboard the James Webb Space Telescope.","In this paper, we present the medium-resolution JWST/NIRSpec spectra of two young brown dwarfs, TWA 28 (M9.0) and TWA 27A (M9.0), and one planetary-mass object, TWA 27B (L6.0), members of the TW Hydrae Association (~10 Myr).","We show the richness of the atomic lines and molecular bands present in the spectra.","All objects show signs of a circumstellar disk, via near-infrared excess and/or via emission lines.","We matched a set of cloudless atmospheric spectra (ATMO), and cloudy atmospheric spectra (BT-Settl) to our NIRSpec spectra, and analyzed which wavelength ranges and spectral features both models reproduce best.","Both models derive consistent parameters for the three sources, and predict the existence of CH4 at 3.35 microns in TWA 27B.","Nonetheless, in contrast to other slightly older objects with similar spectral type, like PSO 318.5-22 and VHS 1256b, this feature is not present in the spectrum of TWA 27B.","The lack of the CH4 feature might suggest that the L/T transition of very young dwarfs starts at later spectral types than for older brown dwarfs."],"url":"http://arxiv.org/abs/2402.04230v1","category":"astro-ph.SR"}
{"created":"2024-02-06 18:02:19","title":"Production-inventory games and pmas games: characterizations of the Owen point","abstract":"Production-inventory games were introduced in Guardiola et al. (2007) as a new class of totally balanced combinatorial optimization games. From among all core-allocations, the Owen point was proposed as a specifically appealing solution. In this paper we study some relationships of the class of production-inventory games and other classes of new and known games. In addition, we propose three axiomatic characterizations of the Owen point. We use eight axioms for these characterizations, among those, inessentiality and additivity of players' demands are used for the first time in this paper.","sentences":["Production-inventory games were introduced in Guardiola et al.","(2007) as a new class of totally balanced combinatorial optimization games.","From among all core-allocations, the Owen point was proposed as a specifically appealing solution.","In this paper we study some relationships of the class of production-inventory games and other classes of new and known games.","In addition, we propose three axiomatic characterizations of the Owen point.","We use eight axioms for these characterizations, among those, inessentiality and additivity of players' demands are used for the first time in this paper."],"url":"http://arxiv.org/abs/2402.04208v1","category":"cs.GT"}
{"created":"2024-02-06 17:24:06","title":"Informed Reinforcement Learning for Situation-Aware Traffic Rule Exceptions","abstract":"Reinforcement Learning is a highly active research field with promising advancements. In the field of autonomous driving, however, often very simple scenarios are being examined. Common approaches use non-interpretable control commands as the action space and unstructured reward designs which lack structure. In this work, we introduce Informed Reinforcement Learning, where a structured rulebook is integrated as a knowledge source. We learn trajectories and asses them with a situation-aware reward design, leading to a dynamic reward which allows the agent to learn situations which require controlled traffic rule exceptions. Our method is applicable to arbitrary RL models. We successfully demonstrate high completion rates of complex scenarios with recent model-based agents.","sentences":["Reinforcement Learning is a highly active research field with promising advancements.","In the field of autonomous driving, however, often very simple scenarios are being examined.","Common approaches use non-interpretable control commands as the action space and unstructured reward designs which lack structure.","In this work, we introduce Informed Reinforcement Learning, where a structured rulebook is integrated as a knowledge source.","We learn trajectories and asses them with a situation-aware reward design, leading to a dynamic reward which allows the agent to learn situations which require controlled traffic rule exceptions.","Our method is applicable to arbitrary RL models.","We successfully demonstrate high completion rates of complex scenarios with recent model-based agents."],"url":"http://arxiv.org/abs/2402.04168v1","category":"cs.LG"}
{"created":"2024-02-06 17:21:39","title":"Monthly GDP nowcasting with Machine Learning and Unstructured Data","abstract":"In the dynamic landscape of continuous change, Machine Learning (ML) \"nowcasting\" models offer a distinct advantage for informed decision-making in both public and private sectors. This study introduces ML-based GDP growth projection models for monthly rates in Peru, integrating structured macroeconomic indicators with high-frequency unstructured sentiment variables. Analyzing data from January 2007 to May 2023, encompassing 91 leading economic indicators, the study evaluates six ML algorithms to identify optimal predictors. Findings highlight the superior predictive capability of ML models using unstructured data, particularly Gradient Boosting Machine, LASSO, and Elastic Net, exhibiting a 20% to 25% reduction in prediction errors compared to traditional AR and Dynamic Factor Models (DFM). This enhanced performance is attributed to better handling of data of ML models in high-uncertainty periods, such as economic crises.","sentences":["In the dynamic landscape of continuous change, Machine Learning (ML) \"nowcasting\" models offer a distinct advantage for informed decision-making in both public and private sectors.","This study introduces ML-based GDP growth projection models for monthly rates in Peru, integrating structured macroeconomic indicators with high-frequency unstructured sentiment variables.","Analyzing data from January 2007 to May 2023, encompassing 91 leading economic indicators, the study evaluates six ML algorithms to identify optimal predictors.","Findings highlight the superior predictive capability of ML models using unstructured data, particularly Gradient Boosting Machine, LASSO, and Elastic Net, exhibiting a 20% to 25% reduction in prediction errors compared to traditional AR and Dynamic Factor Models (DFM).","This enhanced performance is attributed to better handling of data of ML models in high-uncertainty periods, such as economic crises."],"url":"http://arxiv.org/abs/2402.04165v1","category":"econ.EM"}
{"created":"2024-02-06 17:21:06","title":"Tempered Calculus for ML: Application to Hyperbolic Model Embedding","abstract":"Most mathematical distortions used in ML are fundamentally integral in nature: $f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances, etc. In this paper, we unveil a grounded theory and tools which can help improve these distortions to better cope with ML requirements. We start with a generalization of Riemann integration that also encapsulates functions that are not strictly additive but are, more generally, $t$-additive, as in nonextensive statistical mechanics. Notably, this recovers Volterra's product integral as a special case. We then generalize the Fundamental Theorem of calculus using an extension of the (Euclidean) derivative. This, along with a series of more specific Theorems, serves as a basis for results showing how one can specifically design, alter, or change fundamental properties of distortion measures in a simple way, with a special emphasis on geometric- and ML-related properties that are the metricity, hyperbolicity, and encoding. We show how to apply it to a problem that has recently gained traction in ML: hyperbolic embeddings with a \"cheap\" and accurate encoding along the hyperbolic vs Euclidean scale. We unveil a new application for which the Poincar\\'e disk model has very appealing features, and our theory comes in handy: \\textit{model} embeddings for boosted combinations of decision trees, trained using the log-loss (trees) and logistic loss (combinations).","sentences":["Most mathematical distortions used in ML are fundamentally integral in nature: $f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances, etc.","In this paper, we unveil a grounded theory and tools which can help improve these distortions to better cope with ML requirements.","We start with a generalization of Riemann integration that also encapsulates functions that are not strictly additive but are, more generally, $t$-additive, as in nonextensive statistical mechanics.","Notably, this recovers Volterra's product integral as a special case.","We then generalize the Fundamental Theorem of calculus using an extension of the (Euclidean) derivative.","This, along with a series of more specific Theorems, serves as a basis for results showing how one can specifically design, alter, or change fundamental properties of distortion measures in a simple way, with a special emphasis on geometric- and ML-related properties that are the metricity, hyperbolicity, and encoding.","We show how to apply it to a problem that has recently gained traction in ML: hyperbolic embeddings with a \"cheap\" and accurate encoding along the hyperbolic vs Euclidean scale.","We unveil a new application for which the Poincar\\'e disk model has very appealing features, and our theory comes in handy: \\textit{model} embeddings for boosted combinations of decision trees, trained using the log-loss (trees) and logistic loss (combinations)."],"url":"http://arxiv.org/abs/2402.04163v1","category":"cs.LG"}
{"created":"2024-02-06 17:13:51","title":"Controller synthesis for input-state data with measurement errors","abstract":"We consider the problem of designing a state-feedback controller for a linear system, based only on noisy input-state data. We focus on input-state data corrupted by additive measurement errors, which, albeit less investigated, are as relevant as process disturbances in applications. For energy and instantaneous bounds on these measurement errors, we derive linear matrix inequalities for controller design where the one for the energy bound is actually equivalent to robust stabilization of all systems consistent with the noisy data points.","sentences":["We consider the problem of designing a state-feedback controller for a linear system, based only on noisy input-state data.","We focus on input-state data corrupted by additive measurement errors, which, albeit less investigated, are as relevant as process disturbances in applications.","For energy and instantaneous bounds on these measurement errors, we derive linear matrix inequalities for controller design where the one for the energy bound is actually equivalent to robust stabilization of all systems consistent with the noisy data points."],"url":"http://arxiv.org/abs/2402.04157v1","category":"eess.SY"}
{"created":"2024-02-06 17:13:41","title":"Optimal weighted Wente's inequality","abstract":"We prove $L^\\infty$ and $W^{1,2}$ weighted Wente's inequalities. We prove in particular the critical case: for the $|x|^2$ weighted Wente's estimate the optimal weight is $|x|^2\\log|x|$.","sentences":["We prove $L^\\infty$ and $W^{1,2}$ weighted Wente's inequalities.","We prove in particular the critical case: for the $|x|^2$ weighted Wente's estimate the optimal weight is $|x|^2\\log|x|$."],"url":"http://arxiv.org/abs/2402.04156v1","category":"math.AP"}
{"created":"2024-02-06 17:03:01","title":"$L^\\infty$-optimal transport of anisotropic log-concave measures and exponential convergence in Fisher's infinitesimal model","abstract":"We prove upper bounds on the $L^\\infty$-Wasserstein distance from optimal transport between strongly log-concave probability densities and log-Lipschitz perturbations. In the simplest setting, such a bound amounts to a transport-information inequality involving the $L^\\infty$-Wasserstein metric and the relative $L^\\infty$-Fisher information. We show that this inequality can be sharpened significantly in situations where the involved densities are anisotropic. Our proof is based on probabilistic techniques using Langevin dynamics. As an application of these results, we obtain sharp exponential rates of convergence in Fisher's infinitesimal model from quantitative genetics, generalising recent results by Calvez, Poyato, and Santambrogio in dimension 1 to arbitrary dimensions.","sentences":["We prove upper bounds on the $L^\\infty$-Wasserstein distance from optimal transport between strongly log-concave probability densities and log-Lipschitz perturbations.","In the simplest setting, such a bound amounts to a transport-information inequality involving the $L^\\infty$-Wasserstein metric and the relative $L^\\infty$-Fisher information.","We show that this inequality can be sharpened significantly in situations where the involved densities are anisotropic.","Our proof is based on probabilistic techniques using Langevin dynamics.","As an application of these results, we obtain sharp exponential rates of convergence in Fisher's infinitesimal model from quantitative genetics, generalising recent results by Calvez, Poyato, and Santambrogio in dimension 1 to arbitrary dimensions."],"url":"http://arxiv.org/abs/2402.04151v1","category":"math.PR"}
{"created":"2024-02-06 17:01:10","title":"Dynamic Realization Games in Newsvendor Inventory Centralization","abstract":"Consider a set N of n (>1) stores with single-item and single-period nondeterministic demands like in a classic newsvendor setting with holding and penalty costs only. Assume a risk-pooling single-warehouse centralized inventory ordering option. Allocation of costs in the centralized inventory ordering corresponds to modelling it as a cooperative cost game whose players are the stores. It has been shown that when holding and penalty costs are identical for all subsets of stores, the game based on optimal expected costs has a non empty core (Hartman et. al., 2000, Muller \\textit{et. al.}, 2002). In this paper we examine a related inventory centralization game based on demand realizations that has, in general, an empty core even with identical penalty and holding costs (Hartman and Dror, 2005). We propose a repeated cost allocation scheme for dynamic realization games based on allocation processes introduced by Lehrer (2002a). We prove that the cost subsequences of the dynamic realization game process, based on Lehrer's rules, converge almost surely to either a least square value or the core of the expected game. We extend the above results to more general dynamic cost games and relax the independence hypothesis of the sequence of players' demands at different stages.","sentences":["Consider a set N of n (>1) stores with single-item and single-period nondeterministic demands like in a classic newsvendor setting with holding and penalty costs only.","Assume a risk-pooling single-warehouse centralized inventory ordering option.","Allocation of costs in the centralized inventory ordering corresponds to modelling it as a cooperative cost game whose players are the stores.","It has been shown that when holding and penalty costs are identical for all subsets of stores, the game based on optimal expected costs has a non empty core (Hartman et.","al., 2000, Muller \\textit{et.","al.}, 2002).","In this paper we examine a related inventory centralization game based on demand realizations that has, in general, an empty core even with identical penalty and holding costs (Hartman and Dror, 2005).","We propose a repeated cost allocation scheme for dynamic realization games based on allocation processes introduced by Lehrer (2002a).","We prove that the cost subsequences of the dynamic realization game process, based on Lehrer's rules, converge almost surely to either a least square value or the core of the expected game.","We extend the above results to more general dynamic cost games and relax the independence hypothesis of the sequence of players' demands at different stages."],"url":"http://arxiv.org/abs/2402.04149v1","category":"cs.GT"}
{"created":"2024-02-06 16:38:11","title":"A quasi-optimal lower bound for skew polynomial multiplication","abstract":"We establish a lower bound for the complexity of multiplying two skew polynomials. The lower bound coincides with the upper bound conjectured by Caruso and Borgne in 2017, up to a log factor. We present algorithms for three special cases, indicating that the aforementioned lower bound is quasi-optimal. In fact, our lower bound is quasi-optimal in the sense of bilinear complexity. In addition, we discuss the average bilinear complexity of simultaneous multiplication of skew polynomials and the complexity of skew polynomial multiplication in the case of towers of extensions.","sentences":["We establish a lower bound for the complexity of multiplying two skew polynomials.","The lower bound coincides with the upper bound conjectured by Caruso and Borgne in 2017, up to a log factor.","We present algorithms for three special cases, indicating that the aforementioned lower bound is quasi-optimal.","In fact, our lower bound is quasi-optimal in the sense of bilinear complexity.","In addition, we discuss the average bilinear complexity of simultaneous multiplication of skew polynomials and the complexity of skew polynomial multiplication in the case of towers of extensions."],"url":"http://arxiv.org/abs/2402.04134v1","category":"cs.CC"}
{"created":"2024-02-06 16:35:49","title":"Spectroscopic performance of Low-Gain Avalanche Diodes for different types of radiation","abstract":"Low-Gain Avalanche Diodes are a type of silicon Avalanche Photo-Diodes originally developed for the fast detection of minimum ionizing particles in high-energy physics experiments. Thanks to their fast timing performance, the Low-Gain Avalanche Diode paradigm enables detectors to accurately measure minimum ionizing particles with a timing resolution of a few tens of picoseconds. Such a performance is due to a thin substrate and the presence of a moderate signal gain. This internal gain of a few tens is enough to compensate for the reduced charge deposition in the thinner substrate and the noise of fast read-out systems. While Low-Gain Avalanche Diodes are optimized for the detection of minimum ionizing particles for high-energy particle detectors, it is critical to study their performance for the detection of different types of particle, such as X-rays, gamma-rays, or alphas. In this paper, we evaluate the gain of three types of Low-Gain Avalanche Diodes: two devices with different geometries and doping profiles fabricated by Brookhaven National Laboratory, and one fabricated by Hamamatsu Photonics with a different process.   Since the gain in LGADs depends on the bias voltage applied to the sensor, pulse-height spectra have been acquired for bias voltages spanning from the depletion voltage up to breakdown voltage. The signal-to-noise ratio of the generated signals and the shape of their spectra allow us to probe the underlying physics of the multiplication process.","sentences":["Low-Gain Avalanche Diodes are a type of silicon Avalanche Photo-Diodes originally developed for the fast detection of minimum ionizing particles in high-energy physics experiments.","Thanks to their fast timing performance, the Low-Gain Avalanche Diode paradigm enables detectors to accurately measure minimum ionizing particles with a timing resolution of a few tens of picoseconds.","Such a performance is due to a thin substrate and the presence of a moderate signal gain.","This internal gain of a few tens is enough to compensate for the reduced charge deposition in the thinner substrate and the noise of fast read-out systems.","While Low-Gain Avalanche Diodes are optimized for the detection of minimum ionizing particles for high-energy particle detectors, it is critical to study their performance for the detection of different types of particle, such as X-rays, gamma-rays, or alphas.","In this paper, we evaluate the gain of three types of Low-Gain Avalanche Diodes: two devices with different geometries and doping profiles fabricated by Brookhaven National Laboratory, and one fabricated by Hamamatsu Photonics with a different process.   ","Since the gain in LGADs depends on the bias voltage applied to the sensor, pulse-height spectra have been acquired for bias voltages spanning from the depletion voltage up to breakdown voltage.","The signal-to-noise ratio of the generated signals and the shape of their spectra allow us to probe the underlying physics of the multiplication process."],"url":"http://arxiv.org/abs/2402.04132v1","category":"physics.ins-det"}
{"created":"2024-02-06 16:12:11","title":"Optimization of Neumann Eigenvalues under convexity and geometric constraints","abstract":"In this paper we study optimization problems for Neumann eigenvalues $\\mu_k$ among convex domains with a constraint on the diameter or the perimeter. We work mainly in the plane, though some results are stated in higher dimension. We study the existence of an optimal domain in all considered cases. We also consider the case of the unit disk, giving values of the index $k$ for which it can be or cannot be extremal. We give some numerical examples for small values of $k$ that lead us to state some conjectures.","sentences":["In this paper we study optimization problems for Neumann eigenvalues $\\mu_k$ among convex domains with a constraint on the diameter or the perimeter.","We work mainly in the plane, though some results are stated in higher dimension.","We study the existence of an optimal domain in all considered cases.","We also consider the case of the unit disk, giving values of the index $k$ for which it can be or cannot be extremal.","We give some numerical examples for small values of $k$ that lead us to state some conjectures."],"url":"http://arxiv.org/abs/2402.04117v1","category":"math.AP"}
{"created":"2024-02-06 16:08:57","title":"Origami Nano-gap Electrodes for Reversible Nanoparticle Trapping","abstract":"We present a facile desktop fabrication method for origami-based nano-gap indium tin oxide (ITO) electrokinetic particle traps, providing a simplified approach compared to traditional lithographic techniques and effectively trapping of nanoparticles. Our approach involves bending ITO thin films on optically transparent polyethylene terephthalate (PET), creating an array of parallel nano-gaps. By strategically introducing weak points through cut-sharp edges, we successfully controlled the spread of nano-cracks. A single crack spanning the constriction width and splitting the conductive layers forms a nano-gap that can effectively trap small nanoparticles after applying an alternating electric potential across the nanogap. We analyze the conditions for reversible trapping and optimal performance of the nano-gap ITO electrodes with optical microscopy and electrokinetic impedance spectroscopy. Our findings highlight the potential of this facile fabrication method for the use of ITO at active electro-actuated traps in microfluidic systems.","sentences":["We present a facile desktop fabrication method for origami-based nano-gap indium tin oxide (ITO) electrokinetic particle traps, providing a simplified approach compared to traditional lithographic techniques and effectively trapping of nanoparticles.","Our approach involves bending ITO thin films on optically transparent polyethylene terephthalate (PET), creating an array of parallel nano-gaps.","By strategically introducing weak points through cut-sharp edges, we successfully controlled the spread of nano-cracks.","A single crack spanning the constriction width and splitting the conductive layers forms a nano-gap that can effectively trap small nanoparticles after applying an alternating electric potential across the nanogap.","We analyze the conditions for reversible trapping and optimal performance of the nano-gap ITO electrodes with optical microscopy and electrokinetic impedance spectroscopy.","Our findings highlight the potential of this facile fabrication method for the use of ITO at active electro-actuated traps in microfluidic systems."],"url":"http://arxiv.org/abs/2402.04116v1","category":"physics.app-ph"}
{"created":"2024-02-06 16:06:59","title":"SCAFFLSA: Quantifying and Eliminating Heterogeneity Bias in Federated Linear Stochastic Approximation and Temporal Difference Learning","abstract":"In this paper, we perform a non-asymptotic analysis of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the bias introduced by local training with heterogeneous agents, and investigate the sample complexity of the algorithm. We show that the communication complexity of FedLSA scales polynomially with the desired precision $\\epsilon$, which limits the benefits of federation. To overcome this, we propose SCAFFLSA, a novel variant of FedLSA, that uses control variates to correct the bias of local training, and prove its convergence without assumptions on statistical heterogeneity. We apply the proposed methodology to federated temporal difference learning with linear function approximation, and analyze the corresponding complexity improvements.","sentences":["In this paper, we perform a non-asymptotic analysis of the federated linear stochastic approximation (FedLSA) algorithm.","We explicitly quantify the bias introduced by local training with heterogeneous agents, and investigate the sample complexity of the algorithm.","We show that the communication complexity of FedLSA scales polynomially with the desired precision $\\epsilon$, which limits the benefits of federation.","To overcome this, we propose SCAFFLSA, a novel variant of FedLSA, that uses control variates to correct the bias of local training, and prove its convergence without assumptions on statistical heterogeneity.","We apply the proposed methodology to federated temporal difference learning with linear function approximation, and analyze the corresponding complexity improvements."],"url":"http://arxiv.org/abs/2402.04114v1","category":"stat.ML"}
{"created":"2024-02-06 15:52:23","title":"Analysis of Deep Image Prior and Exploiting Self-Guidance for Image Reconstruction","abstract":"The ability of deep image prior (DIP) to recover high-quality images from incomplete or corrupted measurements has made it popular in inverse problems in image restoration and medical imaging including magnetic resonance imaging (MRI). However, conventional DIP suffers from severe overfitting and spectral bias effects.In this work, we first provide an analysis of how DIP recovers information from undersampled imaging measurements by analyzing the training dynamics of the underlying networks in the kernel regime for different architectures.This study sheds light on important underlying properties for DIP-based recovery.Current research suggests that incorporating a reference image as network input can enhance DIP's performance in image reconstruction compared to using random inputs. However, obtaining suitable reference images requires supervision, and raises practical difficulties. In an attempt to overcome this obstacle, we further introduce a self-driven reconstruction process that concurrently optimizes both the network weights and the input while eliminating the need for training data. Our method incorporates a novel denoiser regularization term which enables robust and stable joint estimation of both the network input and reconstructed image.We demonstrate that our self-guided method surpasses both the original DIP and modern supervised methods in terms of MR image reconstruction performance and outperforms previous DIP-based schemes for image inpainting.","sentences":["The ability of deep image prior (DIP) to recover high-quality images from incomplete or corrupted measurements has made it popular in inverse problems in image restoration and medical imaging including magnetic resonance imaging (MRI).","However, conventional DIP suffers from severe overfitting and spectral bias effects.","In this work, we first provide an analysis of how DIP recovers information from undersampled imaging measurements by analyzing the training dynamics of the underlying networks in the kernel regime for different architectures.","This study sheds light on important underlying properties for DIP-based recovery.","Current research suggests that incorporating a reference image as network input can enhance DIP's performance in image reconstruction compared to using random inputs.","However, obtaining suitable reference images requires supervision, and raises practical difficulties.","In an attempt to overcome this obstacle, we further introduce a self-driven reconstruction process that concurrently optimizes both the network weights and the input while eliminating the need for training data.","Our method incorporates a novel denoiser regularization term which enables robust and stable joint estimation of both the network input and reconstructed image.","We demonstrate that our self-guided method surpasses both the original DIP and modern supervised methods in terms of MR image reconstruction performance and outperforms previous DIP-based schemes for image inpainting."],"url":"http://arxiv.org/abs/2402.04097v1","category":"cs.CV"}
{"created":"2024-02-06 15:51:56","title":"New Physical Mechanism for Lightning","abstract":"The article is devoted to electromagnetic phenomena in the atmosphere. The set of experimental data on the thunderstorm activity is analyzed. It helps to identify a possible physical mechanism of lightning flashes. This mechanism can involve the formation of metallic bonds in thunderclouds. The analysis of the problem is performed at a microphysical level within the framework of quantum mechanics. The mechanism of appearance of metallic conductivity includes the resonant tunneling of electrons along resonance-percolation trajectories. Such bonds allow the charges from the vast cloud charged subsystems concentrate quickly in lightning channel. The formation of metal bonds in the thunderstorm cloudiness is described as the second-order phase transition. A successive mechanism for the process of formation and development of the lightning channel is suggested. This mechanism is associated with the change in the orientation of crystals in growing electric field. Possible consequences of the quantum-mechanical mechanism under discussion are compared with the results of observations.","sentences":["The article is devoted to electromagnetic phenomena in the atmosphere.","The set of experimental data on the thunderstorm activity is analyzed.","It helps to identify a possible physical mechanism of lightning flashes.","This mechanism can involve the formation of metallic bonds in thunderclouds.","The analysis of the problem is performed at a microphysical level within the framework of quantum mechanics.","The mechanism of appearance of metallic conductivity includes the resonant tunneling of electrons along resonance-percolation trajectories.","Such bonds allow the charges from the vast cloud charged subsystems concentrate quickly in lightning channel.","The formation of metal bonds in the thunderstorm cloudiness is described as the second-order phase transition.","A successive mechanism for the process of formation and development of the lightning channel is suggested.","This mechanism is associated with the change in the orientation of crystals in growing electric field.","Possible consequences of the quantum-mechanical mechanism under discussion are compared with the results of observations."],"url":"http://arxiv.org/abs/2402.04096v1","category":"physics.plasm-ph"}
{"created":"2024-02-06 15:51:39","title":"Two-step growth mechanism of the solid electrolyte interphase in argyrodyte/Li-metal contacts","abstract":"The structure and growth of the Solid Electrolyte Interphase (SEI) region between an electrolyte and an electrode is one of the most fundamental, yet less-well understood phenomena in solid-state batteries. We present a parameter-free atomistic simulation of the SEI growth for one of the currently promising solid electrolytes (Li$_6$PS$_5$Cl), based on \\textit{ab initio} trained machine learning (ML) interatomic potentials, for over 30,000 atoms during 10 ns, well-beyond the capabilities of conventional MD. This unveils a two-step growth mechanism: Li-argyrodite chemical reaction leading to the formation of an amorphous phase, followed by a kinetically slower crystallization of the reaction products into a 5Li$_2$SLi$_3$PLiCl solid solution. The simulation results support the recent, experimentally founded hypothesis of an indirect pathway of electrolyte reduction. These findings shed light on the intricate processes governing SEI evolution, providing a valuable foundation for the design and optimization of next-generation solid-state batteries.","sentences":["The structure and growth of the Solid Electrolyte Interphase (SEI) region between an electrolyte and an electrode is one of the most fundamental, yet less-well understood phenomena in solid-state batteries.","We present a parameter-free atomistic simulation of the SEI growth for one of the currently promising solid electrolytes (Li$_6$PS$_5$Cl), based on \\textit{ab initio} trained machine learning (ML) interatomic potentials, for over 30,000 atoms during 10 ns, well-beyond the capabilities of conventional MD.","This unveils a two-step growth mechanism: Li-argyrodite chemical reaction leading to the formation of an amorphous phase, followed by a kinetically slower crystallization of the reaction products into a 5Li$_2$SLi$_3$PLiCl solid solution.","The simulation results support the recent, experimentally founded hypothesis of an indirect pathway of electrolyte reduction.","These findings shed light on the intricate processes governing SEI evolution, providing a valuable foundation for the design and optimization of next-generation solid-state batteries."],"url":"http://arxiv.org/abs/2402.04095v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-06 15:02:09","title":"Collaborative Deep Reinforcement Learning for Resource Optimization in Non-Terrestrial Networks","abstract":"Non-terrestrial networks (NTNs) with low-earth orbit (LEO) satellites have been regarded as promising remedies to support global ubiquitous wireless services. Due to the rapid mobility of LEO satellite, inter-beam/satellite handovers happen frequently for a specific user equipment (UE). To tackle this issue, earth-fixed cell scenarios have been under studied, in which the LEO satellite adjusts its beam direction towards a fixed area within its dwell duration, to maintain stable transmission performance for the UE. Therefore, it is required that the LEO satellite performs real-time resource allocation, which however is unaffordable by the LEO satellite with limited computing capability. To address this issue, in this paper, we propose a two-time-scale collaborative deep reinforcement learning (DRL) scheme for beam management and resource allocation in NTNs, in which LEO satellite and UE with different control cycles update their decision-making policies through a sequential manner. Specifically, UE updates its policy subject to improving the value functions of both the agents. Furthermore, the LEO satellite only makes decisions through finite-step rollouts with a reference decision trajectory received from the UE. Simulation results show that the proposed scheme can effectively balance the throughput performance and computational complexity over traditional greedy-searching schemes.","sentences":["Non-terrestrial networks (NTNs) with low-earth orbit (LEO) satellites have been regarded as promising remedies to support global ubiquitous wireless services.","Due to the rapid mobility of LEO satellite, inter-beam/satellite handovers happen frequently for a specific user equipment (UE).","To tackle this issue, earth-fixed cell scenarios have been under studied, in which the LEO satellite adjusts its beam direction towards a fixed area within its dwell duration, to maintain stable transmission performance for the UE.","Therefore, it is required that the LEO satellite performs real-time resource allocation, which however is unaffordable by the LEO satellite with limited computing capability.","To address this issue, in this paper, we propose a two-time-scale collaborative deep reinforcement learning (DRL) scheme for beam management and resource allocation in NTNs, in which LEO satellite and UE with different control cycles update their decision-making policies through a sequential manner.","Specifically, UE updates its policy subject to improving the value functions of both the agents.","Furthermore, the LEO satellite only makes decisions through finite-step rollouts with a reference decision trajectory received from the UE.","Simulation results show that the proposed scheme can effectively balance the throughput performance and computational complexity over traditional greedy-searching schemes."],"url":"http://arxiv.org/abs/2402.04056v1","category":"eess.SP"}
{"created":"2024-02-06 14:32:13","title":"Low-Distortion Clustering with Ordinal and Limited Cardinal Information","abstract":"Motivated by recent work in computational social choice, we extend the metric distortion framework to clustering problems. Given a set of $n$ agents located in an underlying metric space, our goal is to partition them into $k$ clusters, optimizing some social cost objective. The metric space is defined by a distance function $d$ between the agent locations. Information about $d$ is available only implicitly via $n$ rankings, through which each agent ranks all other agents in terms of their distance from her. Still, we would like to evaluate clustering algorithms in terms of social cost objectives that are defined using $d$. This is done using the notion of distortion, which measures how far from optimality a clustering can be, taking into account all underlying metrics that are consistent with the ordinal information available. Unfortunately, the most important clustering objectives do not admit algorithms with finite distortion. To sidestep this disappointing fact, we follow two alternative approaches: We first explore whether resource augmentation can be beneficial. We consider algorithms that use more than $k$ clusters but compare their social cost to that of the optimal $k$-clusterings. We show that using exponentially (in terms of $k$) many clusters, we can get low (constant or logarithmic) distortion for the $k$-center and $k$-median objectives. Interestingly, such an exponential blowup is shown to be necessary. More importantly, we explore whether limited cardinal information can be used to obtain better results. Somewhat surprisingly, for $k$-median and $k$-center, we show that a number of queries that is polynomial in $k$ and only logarithmic in $n$ (i.e., only sublinear in the number of agents for the most relevant scenarios in practice) is enough to get constant distortion.","sentences":["Motivated by recent work in computational social choice, we extend the metric distortion framework to clustering problems.","Given a set of $n$ agents located in an underlying metric space, our goal is to partition them into $k$ clusters, optimizing some social cost objective.","The metric space is defined by a distance function $d$ between the agent locations.","Information about $d$ is available only implicitly via $n$ rankings, through which each agent ranks all other agents in terms of their distance from her.","Still, we would like to evaluate clustering algorithms in terms of social cost objectives that are defined using $d$. This is done using the notion of distortion, which measures how far from optimality a clustering can be, taking into account all underlying metrics that are consistent with the ordinal information available.","Unfortunately, the most important clustering objectives do not admit algorithms with finite distortion.","To sidestep this disappointing fact, we follow two alternative approaches: We first explore whether resource augmentation can be beneficial.","We consider algorithms that use more than $k$ clusters but compare their social cost to that of the optimal $k$-clusterings.","We show that using exponentially (in terms of $k$) many clusters, we can get low (constant or logarithmic) distortion for the $k$-center and $k$-median objectives.","Interestingly, such an exponential blowup is shown to be necessary.","More importantly, we explore whether limited cardinal information can be used to obtain better results.","Somewhat surprisingly, for $k$-median and $k$-center, we show that a number of queries that is polynomial in $k$ and only logarithmic in $n$ (i.e., only sublinear in the number of agents for the most relevant scenarios in practice) is enough to get constant distortion."],"url":"http://arxiv.org/abs/2402.04035v1","category":"cs.GT"}
{"created":"2024-02-06 14:00:43","title":"Bayesian Uncertainty for Gradient Aggregation in Multi-Task Learning","abstract":"As machine learning becomes more prominent there is a growing demand to perform several inference tasks in parallel. Running a dedicated model for each task is computationally expensive and therefore there is a great interest in multi-task learning (MTL). MTL aims at learning a single model that solves several tasks efficiently. Optimizing MTL models is often achieved by computing a single gradient per task and aggregating them for obtaining a combined update direction. However, these approaches do not consider an important aspect, the sensitivity in the gradient dimensions. Here, we introduce a novel gradient aggregation approach using Bayesian inference. We place a probability distribution over the task-specific parameters, which in turn induce a distribution over the gradients of the tasks. This additional valuable information allows us to quantify the uncertainty in each of the gradients dimensions, which can then be factored in when aggregating them. We empirically demonstrate the benefits of our approach in a variety of datasets, achieving state-of-the-art performance.","sentences":["As machine learning becomes more prominent there is a growing demand to perform several inference tasks in parallel.","Running a dedicated model for each task is computationally expensive and therefore there is a great interest in multi-task learning (MTL).","MTL aims at learning a single model that solves several tasks efficiently.","Optimizing MTL models is often achieved by computing a single gradient per task and aggregating them for obtaining a combined update direction.","However, these approaches do not consider an important aspect, the sensitivity in the gradient dimensions.","Here, we introduce a novel gradient aggregation approach using Bayesian inference.","We place a probability distribution over the task-specific parameters, which in turn induce a distribution over the gradients of the tasks.","This additional valuable information allows us to quantify the uncertainty in each of the gradients dimensions, which can then be factored in when aggregating them.","We empirically demonstrate the benefits of our approach in a variety of datasets, achieving state-of-the-art performance."],"url":"http://arxiv.org/abs/2402.04005v1","category":"cs.LG"}
{"created":"2024-02-06 13:51:52","title":"Optimal partitions of the flat torus into parts of smaller diameter","abstract":"We consider the problem of partitioning a two-dimensional flat torus $T^2$ into $m$ sets in order to minimize the maximal diameter of a part. For $m \\leqslant 25$ we give numerical estimates for the maximal diameter $d_m(T^2)$ at which the partition exists. Several approaches are proposed to obtain such estimates. In particular, we use the search for mesh partitions via the SAT solver, the global optimization approach for polygonal partitions, and the optimization of periodic hexagonal tilings. For $m=3$, the exact estimate is proved using elementary topological reasoning.","sentences":["We consider the problem of partitioning a two-dimensional flat torus $T^2$ into $m$ sets in order to minimize the maximal diameter of a part.","For $m \\leqslant 25$ we give numerical estimates for the maximal diameter $d_m(T^2)$ at which the partition exists.","Several approaches are proposed to obtain such estimates.","In particular, we use the search for mesh partitions via the SAT solver, the global optimization approach for polygonal partitions, and the optimization of periodic hexagonal tilings.","For $m=3$, the exact estimate is proved using elementary topological reasoning."],"url":"http://arxiv.org/abs/2402.03997v1","category":"math.MG"}
{"created":"2024-02-06 13:16:54","title":"Controllable Diverse Sampling for Diffusion Based Motion Behavior Forecasting","abstract":"In autonomous driving tasks, trajectory prediction in complex traffic environments requires adherence to real-world context conditions and behavior multimodalities. Existing methods predominantly rely on prior assumptions or generative models trained on curated data to learn road agents' stochastic behavior bounded by scene constraints. However, they often face mode averaging issues due to data imbalance and simplistic priors, and could even suffer from mode collapse due to unstable training and single ground truth supervision. These issues lead the existing methods to a loss of predictive diversity and adherence to the scene constraints. To address these challenges, we introduce a novel trajectory generator named Controllable Diffusion Trajectory (CDT), which integrates map information and social interactions into a Transformer-based conditional denoising diffusion model to guide the prediction of future trajectories. To ensure multimodality, we incorporate behavioral tokens to direct the trajectory's modes, such as going straight, turning right or left. Moreover, we incorporate the predicted endpoints as an alternative behavioral token into the CDT model to facilitate the prediction of accurate trajectories. Extensive experiments on the Argoverse 2 benchmark demonstrate that CDT excels in generating diverse and scene-compliant trajectories in complex urban settings.","sentences":["In autonomous driving tasks, trajectory prediction in complex traffic environments requires adherence to real-world context conditions and behavior multimodalities.","Existing methods predominantly rely on prior assumptions or generative models trained on curated data to learn road agents' stochastic behavior bounded by scene constraints.","However, they often face mode averaging issues due to data imbalance and simplistic priors, and could even suffer from mode collapse due to unstable training and single ground truth supervision.","These issues lead the existing methods to a loss of predictive diversity and adherence to the scene constraints.","To address these challenges, we introduce a novel trajectory generator named Controllable Diffusion Trajectory (CDT), which integrates map information and social interactions into a Transformer-based conditional denoising diffusion model to guide the prediction of future trajectories.","To ensure multimodality, we incorporate behavioral tokens to direct the trajectory's modes, such as going straight, turning right or left.","Moreover, we incorporate the predicted endpoints as an alternative behavioral token into the CDT model to facilitate the prediction of accurate trajectories.","Extensive experiments on the Argoverse 2 benchmark demonstrate that CDT excels in generating diverse and scene-compliant trajectories in complex urban settings."],"url":"http://arxiv.org/abs/2402.03981v1","category":"cs.CV"}
{"created":"2024-02-06 13:16:52","title":"Large-time optimal observation domain for linear parabolic systems","abstract":"Given a well-posed linear evolution system settled on a domain $\\Omega$ of $\\mathbb{R}^d$, an observation subset $\\omega\\subset\\Omega$ and a time horizon $T$, the observability constant is defined as the largest possible nonnegative constant such that the observability inequality holds for the pair $(\\omega,T)$. In this article we investigate the large-time behavior of the observation domain that maximizes the observability constant over all possible measurable subsets of a given Lebesgue measure. We prove that it converges exponentially, as the time horizon goes to infinity, to a limit set that we characterize. The mathematical technique is new and relies on a quantitative version of the bathtub principle.","sentences":["Given a well-posed linear evolution system settled on a domain $\\Omega$ of $\\mathbb{R}^d$, an observation subset $\\omega\\subset\\Omega$ and a time horizon $T$, the observability constant is defined as the largest possible nonnegative constant such that the observability inequality holds for the pair $(\\omega,T)$.","In this article we investigate the large-time behavior of the observation domain that maximizes the observability constant over all possible measurable subsets of a given Lebesgue measure.","We prove that it converges exponentially, as the time horizon goes to infinity, to a limit set that we characterize.","The mathematical technique is new and relies on a quantitative version of the bathtub principle."],"url":"http://arxiv.org/abs/2402.03980v1","category":"math.AP"}
{"created":"2024-02-06 13:07:28","title":"Smoothed analysis of deterministic discounted and mean-payoff games","abstract":"We devise a policy-iteration algorithm for deterministic two-player discounted and mean-payoff games, that runs in polynomial time with high probability, on any input where each payoff is chosen independently from a sufficiently random distribution.   This includes the case where an arbitrary set of payoffs has been perturbed by a Gaussian, showing for the first time that deterministic two-player games can be solved efficiently, in the sense of smoothed analysis.   More generally, we devise a condition number for deterministic discounted and mean-payoff games, and show that our algorithm runs in time polynomial in this condition number.   Our result confirms a previous conjecture of Boros et al., which was claimed as a theorem and later retracted. It stands in contrast with a recent counter-example by Christ and Yannakakis, showing that Howard's policy-iteration algorithm does not run in smoothed polynomial time on stochastic single-player mean-payoff games.   Our approach is inspired by the analysis of random optimal assignment instances by Frieze and Sorkin, and the analysis of bias-induced policies for mean-payoff games by Akian, Gaubert and Hochart.","sentences":["We devise a policy-iteration algorithm for deterministic two-player discounted and mean-payoff games, that runs in polynomial time with high probability, on any input where each payoff is chosen independently from a sufficiently random distribution.   ","This includes the case where an arbitrary set of payoffs has been perturbed by a Gaussian, showing for the first time that deterministic two-player games can be solved efficiently, in the sense of smoothed analysis.   ","More generally, we devise a condition number for deterministic discounted and mean-payoff games, and show that our algorithm runs in time polynomial in this condition number.   ","Our result confirms a previous conjecture of Boros et al., which was claimed as a theorem and later retracted.","It stands in contrast with a recent counter-example by Christ and Yannakakis, showing that Howard's policy-iteration algorithm does not run in smoothed polynomial time on stochastic single-player mean-payoff games.   ","Our approach is inspired by the analysis of random optimal assignment instances by Frieze and Sorkin, and the analysis of bias-induced policies for mean-payoff games by Akian, Gaubert and Hochart."],"url":"http://arxiv.org/abs/2402.03975v1","category":"cs.GT"}
{"created":"2024-02-06 12:50:39","title":"A Novel Local and Hyper-Local Multicast Services Transmission Scheme for Beyond 5G Networks","abstract":"The efficiency of the broadcast network is impacted by the different types of services that may be transmitted over it. Global services serve users across the entire network, while local services cater to specific regions, and hyper-local services have even narrower coverage. Multimedia Broadcast over a Single-Frequency Network (MBSFN) is typically used for global service transmission while existing literature extensively discusses schemes for transmitting local or hyper-local services with or without Single Frequency Network (SFN) gain. However, these schemes fall short when network-wide requests for only local and hyper-local services are made, leading operators to scale down to either Single Cell-Point to Multipoint (SCPtM) or Multi-Frequency Network (MFN). SCPtM is highly susceptible to interference, and MFN requires substantial amounts of valuable spectrum. They both employ the Least Channel Gain (LCG) strategy for transmitting hyper-local services without SFN gain. Our proposed Local and Hyper-Local Services (LHS) transmission scheme utilizes the knowledge of user distribution and their corresponding radio link channel quality to schedule single or multi-resolution, local or hyper-local services within a three-cell cluster and aims to enhance spectral efficiency and maximize system throughput. It leverages Scalable Video Coding (SVC) in conjunction with Hierarchical Modulation (HM) for transmitting multi-resolution multimedia content to address the problem of heterogeneity amongst the multicast group users. The proposed scheme also employs macro-diversity combining with optimal HM parameters for each gNB catering to a local service area in order to minimize the service outage. System-level simulation results testify to the better performance achieved by the proposed LHS transmission scheme with respect to SCPtM.","sentences":["The efficiency of the broadcast network is impacted by the different types of services that may be transmitted over it.","Global services serve users across the entire network, while local services cater to specific regions, and hyper-local services have even narrower coverage.","Multimedia Broadcast over a Single-Frequency Network (MBSFN) is typically used for global service transmission while existing literature extensively discusses schemes for transmitting local or hyper-local services with or without Single Frequency Network (SFN) gain.","However, these schemes fall short when network-wide requests for only local and hyper-local services are made, leading operators to scale down to either Single Cell-Point to Multipoint (SCPtM) or Multi-Frequency Network (MFN).","SCPtM is highly susceptible to interference, and MFN requires substantial amounts of valuable spectrum.","They both employ the Least Channel Gain (LCG) strategy for transmitting hyper-local services without SFN gain.","Our proposed Local and Hyper-Local Services (LHS) transmission scheme utilizes the knowledge of user distribution and their corresponding radio link channel quality to schedule single or multi-resolution, local or hyper-local services within a three-cell cluster and aims to enhance spectral efficiency and maximize system throughput.","It leverages Scalable Video Coding (SVC) in conjunction with Hierarchical Modulation (HM) for transmitting multi-resolution multimedia content to address the problem of heterogeneity amongst the multicast group users.","The proposed scheme also employs macro-diversity combining with optimal HM parameters for each gNB catering to a local service area in order to minimize the service outage.","System-level simulation results testify to the better performance achieved by the proposed LHS transmission scheme with respect to SCPtM."],"url":"http://arxiv.org/abs/2402.03963v1","category":"eess.SP"}
{"created":"2024-02-06 12:19:08","title":"Wasserstein distributionally robust optimization and its tractable regularization formulations","abstract":"We study a variety of Wasserstein distributionally robust optimization (WDRO) problems where the distributions in the ambiguity set are chosen by constraining their Wasserstein discrepancies to the empirical distribution. Using the notion of weak Lipschitz property, we derive lower and upper bounds of the corresponding worst-case loss quantity and propose sufficient conditions under which this quantity coincides with its regularization scheme counterpart. Our constructive methodology and elementary analysis also directly characterize the closed-form of the approximate worst-case distribution. Extensive applications show that our theoretical results are applicable to various problems, including regression, classification and risk measure problems.","sentences":["We study a variety of Wasserstein distributionally robust optimization (WDRO) problems where the distributions in the ambiguity set are chosen by constraining their Wasserstein discrepancies to the empirical distribution.","Using the notion of weak Lipschitz property, we derive lower and upper bounds of the corresponding worst-case loss quantity and propose sufficient conditions under which this quantity coincides with its regularization scheme counterpart.","Our constructive methodology and elementary analysis also directly characterize the closed-form of the approximate worst-case distribution.","Extensive applications show that our theoretical results are applicable to various problems, including regression, classification and risk measure problems."],"url":"http://arxiv.org/abs/2402.03942v1","category":"math.OC"}
{"created":"2024-02-06 12:01:00","title":"Fully autonomous tuning of a spin qubit","abstract":"Spanning over two decades, the study of qubits in semiconductors for quantum computing has yielded significant breakthroughs. However, the development of large-scale semiconductor quantum circuits is still limited by challenges in efficiently tuning and operating these circuits. Identifying optimal operating conditions for these qubits is complex, involving the exploration of vast parameter spaces. This presents a real 'needle in the haystack' problem, which, until now, has resisted complete automation due to device variability and fabrication imperfections. In this study, we present the first fully autonomous tuning of a semiconductor qubit, from a grounded device to Rabi oscillations, a clear indication of successful qubit operation. We demonstrate this automation, achieved without human intervention, in a Ge/Si core/shell nanowire device. Our approach integrates deep learning, Bayesian optimization, and computer vision techniques. We expect this automation algorithm to apply to a wide range of semiconductor qubit devices, allowing for statistical studies of qubit quality metrics. As a demonstration of the potential of full automation, we characterise how the Rabi frequency and g-factor depend on barrier gate voltages for one of the qubits found by the algorithm. Twenty years after the initial demonstrations of spin qubit operation, this significant advancement is poised to finally catalyze the operation of large, previously unexplored quantum circuits.","sentences":["Spanning over two decades, the study of qubits in semiconductors for quantum computing has yielded significant breakthroughs.","However, the development of large-scale semiconductor quantum circuits is still limited by challenges in efficiently tuning and operating these circuits.","Identifying optimal operating conditions for these qubits is complex, involving the exploration of vast parameter spaces.","This presents a real 'needle in the haystack' problem, which, until now, has resisted complete automation due to device variability and fabrication imperfections.","In this study, we present the first fully autonomous tuning of a semiconductor qubit, from a grounded device to Rabi oscillations, a clear indication of successful qubit operation.","We demonstrate this automation, achieved without human intervention, in a Ge/Si core/shell nanowire device.","Our approach integrates deep learning, Bayesian optimization, and computer vision techniques.","We expect this automation algorithm to apply to a wide range of semiconductor qubit devices, allowing for statistical studies of qubit quality metrics.","As a demonstration of the potential of full automation, we characterise how the Rabi frequency and g-factor depend on barrier gate voltages for one of the qubits found by the algorithm.","Twenty years after the initial demonstrations of spin qubit operation, this significant advancement is poised to finally catalyze the operation of large, previously unexplored quantum circuits."],"url":"http://arxiv.org/abs/2402.03931v1","category":"cond-mat.mes-hall"}
