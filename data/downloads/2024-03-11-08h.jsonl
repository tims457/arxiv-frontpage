{"created":"2024-03-08 18:58:46","title":"Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos","abstract":"We introduce LaGTran, a novel framework that utilizes readily available or easily acquired text descriptions to guide robust transfer of discriminative knowledge from labeled source to unlabeled target data with domain shifts. While unsupervised adaptation methods have been established to address this problem, they show limitations in handling challenging domain shifts due to their exclusive operation within the pixel-space. Motivated by our observation that semantically richer text modality has more favorable transfer properties, we devise a transfer mechanism to use a source-trained text-classifier to generate predictions on the target text descriptions, and utilize these predictions as supervision for the corresponding images. Our approach driven by language guidance is surprisingly easy and simple, yet significantly outperforms all prior approaches on challenging datasets like GeoNet and DomainNet, validating its extreme effectiveness. To further extend the scope of our study beyond images, we introduce a new benchmark to study ego-exo transfer in videos and find that our language-aided LaGTran yields significant gains in this highly challenging and non-trivial transfer setting. Code, models, and proposed datasets are publicly available at https://tarun005.github.io/lagtran/.","sentences":["We introduce LaGTran, a novel framework that utilizes readily available or easily acquired text descriptions to guide robust transfer of discriminative knowledge from labeled source to unlabeled target data with domain shifts.","While unsupervised adaptation methods have been established to address this problem, they show limitations in handling challenging domain shifts due to their exclusive operation within the pixel-space.","Motivated by our observation that semantically richer text modality has more favorable transfer properties, we devise a transfer mechanism to use a source-trained text-classifier to generate predictions on the target text descriptions, and utilize these predictions as supervision for the corresponding images.","Our approach driven by language guidance is surprisingly easy and simple, yet significantly outperforms all prior approaches on challenging datasets like GeoNet and DomainNet, validating its extreme effectiveness.","To further extend the scope of our study beyond images, we introduce a new benchmark to study ego-exo transfer in videos and find that our language-aided LaGTran yields significant gains in this highly challenging and non-trivial transfer setting.","Code, models, and proposed datasets are publicly available at https://tarun005.github.io/lagtran/."],"url":"http://arxiv.org/abs/2403.05535v1","category":"cs.CV"}
{"created":"2024-03-08 18:57:00","title":"Tune without Validation: Searching for Learning Rate and Weight Decay on Training Sets","abstract":"We introduce Tune without Validation (Twin), a pipeline for tuning learning rate and weight decay without validation sets. We leverage a recent theoretical framework concerning learning phases in hypothesis space to devise a heuristic that predicts what hyper-parameter (HP) combinations yield better generalization. Twin performs a grid search of trials according to an early-/non-early-stopping scheduler and then segments the region that provides the best results in terms of training loss. Among these trials, the weight norm strongly correlates with predicting generalization. To assess the effectiveness of Twin, we run extensive experiments on 20 image classification datasets and train several families of deep networks, including convolutional, transformer, and feed-forward models. We demonstrate proper HP selection when training from scratch and fine-tuning, emphasizing small-sample scenarios.","sentences":["We introduce Tune without Validation (Twin), a pipeline for tuning learning rate and weight decay without validation sets.","We leverage a recent theoretical framework concerning learning phases in hypothesis space to devise a heuristic that predicts what hyper-parameter (HP) combinations yield better generalization.","Twin performs a grid search of trials according to an early-/non-early-stopping scheduler and then segments the region that provides the best results in terms of training loss.","Among these trials, the weight norm strongly correlates with predicting generalization.","To assess the effectiveness of Twin, we run extensive experiments on 20 image classification datasets and train several families of deep networks, including convolutional, transformer, and feed-forward models.","We demonstrate proper HP selection when training from scratch and fine-tuning, emphasizing small-sample scenarios."],"url":"http://arxiv.org/abs/2403.05532v1","category":"cs.LG"}
{"created":"2024-03-08 18:54:20","title":"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context","abstract":"In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.","sentences":["In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio.","Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks.","Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k).","Finally, we highlight surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content."],"url":"http://arxiv.org/abs/2403.05530v1","category":"cs.CL"}
{"created":"2024-03-08 18:50:19","title":"The Computational Complexity of Learning Gaussian Single-Index Models","abstract":"Single-Index Models are high-dimensional regression problems with planted structure, whereby labels depend on an unknown one-dimensional projection of the input via a generic, non-linear, and potentially non-deterministic transformation. As such, they encompass a broad class of statistical inference tasks, and provide a rich template to study statistical and computational trade-offs in the high-dimensional regime.   While the information-theoretic sample complexity to recover the hidden direction is linear in the dimension $d$, we show that computationally efficient algorithms, both within the Statistical Query (SQ) and the Low-Degree Polynomial (LDP) framework, necessarily require $\\Omega(d^{k^\\star/2})$ samples, where $k^\\star$ is a \"generative\" exponent associated with the model that we explicitly characterize. Moreover, we show that this sample complexity is also sufficient, by establishing matching upper bounds using a partial-trace algorithm. Therefore, our results provide evidence of a sharp computational-to-statistical gap (under both the SQ and LDP class) whenever $k^\\star>2$. To complete the study, we provide examples of smooth and Lipschitz deterministic target functions with arbitrarily large generative exponents $k^\\star$.","sentences":["Single-Index Models are high-dimensional regression problems with planted structure, whereby labels depend on an unknown one-dimensional projection of the input via a generic, non-linear, and potentially non-deterministic transformation.","As such, they encompass a broad class of statistical inference tasks, and provide a rich template to study statistical and computational trade-offs in the high-dimensional regime.   ","While the information-theoretic sample complexity to recover the hidden direction is linear in the dimension $d$, we show that computationally efficient algorithms, both within the Statistical Query (SQ) and the Low-Degree Polynomial (LDP) framework, necessarily require $\\Omega(d^{k^\\star/2})$ samples, where $k^\\star$ is a \"generative\" exponent associated with the model that we explicitly characterize.","Moreover, we show that this sample complexity is also sufficient, by establishing matching upper bounds using a partial-trace algorithm.","Therefore, our results provide evidence of a sharp computational-to-statistical gap (under both the SQ and LDP class) whenever $k^\\star>2$. To complete the study, we provide examples of smooth and Lipschitz deterministic target functions with arbitrarily large generative exponents $k^\\star$."],"url":"http://arxiv.org/abs/2403.05529v1","category":"cs.LG"}
{"created":"2024-03-08 18:48:30","title":"GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM","abstract":"Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quantization error, and a sparse matrix to remedy individual errors from outlier entries. By adeptly integrating three techniques, GEAR is able to fully exploit their synergistic potentials. Our experiments demonstrate that compared to alternatives, GEAR achieves near-lossless 4-bit KV cache compression with up to 2.38x throughput improvement, while reducing peak-memory size up to 2.29x. Our code is publicly available at https://github.com/HaoKang-Timmy/GEAR.","sentences":["Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference.","However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput.","Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly.","Such methods, however, often incur high approximation errors to represent the compressed matrices.","The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance.","To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression.","GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision.","It then employs a low rank matrix to approximate the quantization error, and a sparse matrix to remedy individual errors from outlier entries.","By adeptly integrating three techniques, GEAR is able to fully exploit their synergistic potentials.","Our experiments demonstrate that compared to alternatives, GEAR achieves near-lossless 4-bit KV cache compression with up to 2.38x throughput improvement, while reducing peak-memory size up to 2.29x.","Our code is publicly available at https://github.com/HaoKang-Timmy/GEAR."],"url":"http://arxiv.org/abs/2403.05527v1","category":"cs.LG"}
{"created":"2024-03-08 18:46:00","title":"DeepSeek-VL: Towards Real-World Vision-Language Understanding","abstract":"We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. Our approach is structured around three key dimensions:   We strive to ensure our data is diverse, scalable, and extensively covers real-world scenarios including web screenshots, PDFs, OCR, charts, and knowledge-based content, aiming for a comprehensive representation of practical contexts. Further, we create a use case taxonomy from real user scenarios and construct an instruction tuning dataset accordingly. The fine-tuning with this dataset substantially improves the model's user experience in practical applications. Considering efficiency and the demands of most real-world scenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently processes high-resolution images (1024 x 1024), while maintaining a relatively low computational overhead. This design choice ensures the model's ability to capture critical semantic and detailed information across various visual tasks. We posit that a proficient Vision-Language Model should, foremost, possess strong language abilities. To ensure the preservation of LLM capabilities during pretraining, we investigate an effective VL pretraining strategy by integrating LLM training from the beginning and carefully managing the competitive dynamics observed between vision and language modalities.   The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user experiences as a vision-language chatbot in real-world applications, achieving state-of-the-art or competitive performance across a wide range of visual-language benchmarks at the same model size while maintaining robust performance on language-centric benchmarks. We have made both 1.3B and 7B models publicly accessible to foster innovations based on this foundation model.","sentences":["We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications.","Our approach is structured around three key dimensions:   We strive to ensure our data is diverse, scalable, and extensively covers real-world scenarios including web screenshots, PDFs, OCR, charts, and knowledge-based content, aiming for a comprehensive representation of practical contexts.","Further, we create a use case taxonomy from real user scenarios and construct an instruction tuning dataset accordingly.","The fine-tuning with this dataset substantially improves the model's user experience in practical applications.","Considering efficiency and the demands of most real-world scenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently processes high-resolution images (1024 x 1024), while maintaining a relatively low computational overhead.","This design choice ensures the model's ability to capture critical semantic and detailed information across various visual tasks.","We posit that a proficient Vision-Language Model should, foremost, possess strong language abilities.","To ensure the preservation of LLM capabilities during pretraining, we investigate an effective VL pretraining strategy by integrating LLM training from the beginning and carefully managing the competitive dynamics observed between vision and language modalities.   ","The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user experiences as a vision-language chatbot in real-world applications, achieving state-of-the-art or competitive performance across a wide range of visual-language benchmarks at the same model size while maintaining robust performance on language-centric benchmarks.","We have made both 1.3B and 7B models publicly accessible to foster innovations based on this foundation model."],"url":"http://arxiv.org/abs/2403.05525v1","category":"cs.AI"}
{"created":"2024-03-08 18:44:57","title":"Spinning Particle Geometries in AdS$_3$/CFT$_2$","abstract":"We study spinning particle/defect geometries in the context of AdS$_3$/CFT$_2$. These solutions lie below the BTZ threshold, and can be obtained from identifications of AdS$_3$. We construct the Feynman propagator by solving the bulk equation of motion in the spinning particle geometry, summing over the modes of the fields and passing to the boundary. The quantization of the scalar fields becomes challenging when confined to the regions that are causally well-behaved. If the region containing closed timelike curves (CTCs) is included, the normalization of the scalar fields enjoys an analytical simplification and the propagator can be expressed as an infinite sum over image geodesics. In the dual CFT$_2$, the propagator can be recast as the HHLL four-point function, where by taking into account the $PSL (2,\\mathbb{Z})$ modular images, we recover the bulk computation. We comment on the casual behavior of bulk geometries associated with single-trace operators of spin scaling with the central charge below the BTZ threshold.","sentences":["We study spinning particle/defect geometries in the context of AdS$_3$/CFT$_2$. These solutions lie below the BTZ threshold, and can be obtained from identifications of AdS$_3$.","We construct the Feynman propagator by solving the bulk equation of motion in the spinning particle geometry, summing over the modes of the fields and passing to the boundary.","The quantization of the scalar fields becomes challenging when confined to the regions that are causally well-behaved.","If the region containing closed timelike curves (CTCs) is included, the normalization of the scalar fields enjoys an analytical simplification and the propagator can be expressed as an infinite sum over image geodesics.","In the dual CFT$_2$, the propagator can be recast as the HHLL four-point function, where by taking into account the $PSL (2,\\mathbb{Z})$ modular images, we recover the bulk computation.","We comment on the casual behavior of bulk geometries associated with single-trace operators of spin scaling with the central charge below the BTZ threshold."],"url":"http://arxiv.org/abs/2403.05524v1","category":"hep-th"}
{"created":"2024-03-08 18:44:23","title":"Beyond Finite Data: Towards Data-free Out-of-distribution Generalization via Extrapola","abstract":"Out-of-distribution (OOD) generalization is a favorable yet challenging property for deep neural networks. The core challenges lie in the limited availability of source domains that help models learn an invariant representation from the spurious features. Various domain augmentation have been proposed but largely rely on interpolating existing domains and frequently face difficulties in creating truly \"novel\" domains. Humans, on the other hand, can easily extrapolate novel domains, thus, an intriguing question arises: How can neural networks extrapolate like humans and achieve OOD generalization?   We introduce a novel approach to domain extrapolation that leverages reasoning ability and the extensive knowledge encapsulated within large language models (LLMs) to synthesize entirely new domains. Starting with the class of interest, we query the LLMs to extract relevant knowledge for these novel domains. We then bridge the gap between the text-centric knowledge derived from LLMs and the pixel input space of the model using text-to-image generation techniques. By augmenting the training set of domain generalization datasets with high-fidelity, photo-realistic images of these new domains, we achieve significant improvements over all existing methods, as demonstrated in both single and multi-domain generalization across various benchmarks.   With the ability to extrapolate any domains for any class, our method has the potential to learn a generalized model for any task without any data. To illustrate, we put forth a much more difficult setting termed, data-free domain generalization, that aims to learn a generalized model in the absence of any collected data. Our empirical findings support the above argument and our methods exhibit commendable performance in this setting, even surpassing the supervised setting by approximately 1-2\\% on datasets such as VLCS.","sentences":["Out-of-distribution (OOD) generalization is a favorable yet challenging property for deep neural networks.","The core challenges lie in the limited availability of source domains that help models learn an invariant representation from the spurious features.","Various domain augmentation have been proposed but largely rely on interpolating existing domains and frequently face difficulties in creating truly \"novel\" domains.","Humans, on the other hand, can easily extrapolate novel domains, thus, an intriguing question arises: How can neural networks extrapolate like humans and achieve OOD generalization?   ","We introduce a novel approach to domain extrapolation that leverages reasoning ability and the extensive knowledge encapsulated within large language models (LLMs) to synthesize entirely new domains.","Starting with the class of interest, we query the LLMs to extract relevant knowledge for these novel domains.","We then bridge the gap between the text-centric knowledge derived from LLMs and the pixel input space of the model using text-to-image generation techniques.","By augmenting the training set of domain generalization datasets with high-fidelity, photo-realistic images of these new domains, we achieve significant improvements over all existing methods, as demonstrated in both single and multi-domain generalization across various benchmarks.   ","With the ability to extrapolate any domains for any class, our method has the potential to learn a generalized model for any task without any data.","To illustrate, we put forth a much more difficult setting termed, data-free domain generalization, that aims to learn a generalized model in the absence of any collected data.","Our empirical findings support the above argument and our methods exhibit commendable performance in this setting, even surpassing the supervised setting by approximately 1-2\\% on datasets such as VLCS."],"url":"http://arxiv.org/abs/2403.05523v1","category":"cs.CV"}
{"created":"2024-03-08 18:44:05","title":"Anomalous Hall Crystals in Rhombohedral Multilayer Graphene II: General Mechanism and a Minimal Model","abstract":"We propose a minimal \"three-patch model\" for the anomalous Hall crystal (AHC), a topological electronic state that spontaneously breaks both time-reversal symmetry and continuous translation symmetry. The proposal for this state is inspired by the recently observed integer and fractional quantum Hall states in rhombohedral multilayer graphene at zero magnetic field. There, interaction effects appear to amplify the effects of a weak moir\\'e potential, leading to the formation of stable, isolated Chern bands. It has been further shown that Chern bands are stabilized in mean field calculations even without a moir\\'e potential, enabling a realization of the AHC state. Our model is built upon the dissection of the Brillouin zone into patches centered around high symmetry points. Within this model, the wavefunctions at high symmetry points fully determine the topology and energetics of the state. We extract two quantum geometrical phases of the non-interacting wavefunctions that control the stability of the topologically nontrivial AHC state. The model predicts that the AHC state wins over the topological trivial Wigner crystal in a wide range of parameters, and agrees very well with the results of full self-consistent Hartree-Fock calculations of the rhombohedral multilayer graphene Hamiltonian.","sentences":["We propose a minimal \"three-patch model\" for the anomalous Hall crystal (AHC), a topological electronic state that spontaneously breaks both time-reversal symmetry and continuous translation symmetry.","The proposal for this state is inspired by the recently observed integer and fractional quantum Hall states in rhombohedral multilayer graphene at zero magnetic field.","There, interaction effects appear to amplify the effects of a weak moir\\'e potential, leading to the formation of stable, isolated Chern bands.","It has been further shown that Chern bands are stabilized in mean field calculations even without a moir\\'e potential, enabling a realization of the AHC state.","Our model is built upon the dissection of the Brillouin zone into patches centered around high symmetry points.","Within this model, the wavefunctions at high symmetry points fully determine the topology and energetics of the state.","We extract two quantum geometrical phases of the non-interacting wavefunctions that control the stability of the topologically nontrivial AHC state.","The model predicts that the AHC state wins over the topological trivial Wigner crystal in a wide range of parameters, and agrees very well with the results of full self-consistent Hartree-Fock calculations of the rhombohedral multilayer graphene Hamiltonian."],"url":"http://arxiv.org/abs/2403.05522v1","category":"cond-mat.str-el"}
{"created":"2024-03-08 18:43:16","title":"Existence, regularity and asymptotic behavior of solutions for a nonlocal Chafee-Infante Problem via semigroup theory","abstract":"This article deals with the study of a non-local one-dimensional quasilinear problem with continuous forcing. We use a time-reparameterization to obtain a semilinear problem and study a more general equation using semigroup theory. The existence of mild solutions is established without uniqueness with the aid of the formula of variation of constants and asking only a suitable modulus of continuity on the nonlinearity this mild solution is shown to be strong. Comparison results are also established with the aid of the formula of variation of constants and using these comparison results, global existence is obtained with the additional requirement that the nonlinearity satisfy a structural condition. The existence of pullback attractor is also established for the associated multivalued process along with the uniform bounds given by the comparison results with the additional requirement that the nonlinearity be dissipative. As much as possible the results are abstract so that they can be also applied to other models.","sentences":["This article deals with the study of a non-local one-dimensional quasilinear problem with continuous forcing.","We use a time-reparameterization to obtain a semilinear problem and study a more general equation using semigroup theory.","The existence of mild solutions is established without uniqueness with the aid of the formula of variation of constants and asking only a suitable modulus of continuity on the nonlinearity this mild solution is shown to be strong.","Comparison results are also established with the aid of the formula of variation of constants and using these comparison results, global existence is obtained with the additional requirement that the nonlinearity satisfy a structural condition.","The existence of pullback attractor is also established for the associated multivalued process along with the uniform bounds given by the comparison results with the additional requirement that the nonlinearity be dissipative.","As much as possible the results are abstract so that they can be also applied to other models."],"url":"http://arxiv.org/abs/2403.05520v1","category":"math.AP"}
{"created":"2024-03-08 18:41:42","title":"Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought","abstract":"While chain-of-thought prompting (CoT) has the potential to improve the explainability of language model reasoning, it can systematically misrepresent the factors influencing models' behavior--for example, rationalizing answers in line with a user's opinion without mentioning this bias. To mitigate this biased reasoning problem, we introduce bias-augmented consistency training (BCT), an unsupervised fine-tuning scheme that trains models to give consistent reasoning across prompts with and without biasing features. We construct a suite testing nine forms of biased reasoning on seven question-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of biased reasoning by 86% on held-out tasks. Moreover, this model generalizes to other forms of bias, reducing biased reasoning on held-out biases by an average of 37%. As BCT generalizes to held-out biases and does not require gold labels, this method may hold promise for reducing biased reasoning from as-of-yet unknown biases and on tasks where supervision for ground truth reasoning is unavailable.","sentences":["While chain-of-thought prompting (CoT) has the potential to improve the explainability of language model reasoning, it can systematically misrepresent the factors influencing models' behavior--for example, rationalizing answers in line with a user's opinion without mentioning this bias.","To mitigate this biased reasoning problem, we introduce bias-augmented consistency training (BCT), an unsupervised fine-tuning scheme that trains models to give consistent reasoning across prompts with and without biasing features.","We construct a suite testing nine forms of biased reasoning on seven question-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of biased reasoning by 86% on held-out tasks.","Moreover, this model generalizes to other forms of bias, reducing biased reasoning on held-out biases by an average of 37%.","As BCT generalizes to held-out biases and does not require gold labels, this method may hold promise for reducing biased reasoning from as-of-yet unknown biases and on tasks where supervision for ground truth reasoning is unavailable."],"url":"http://arxiv.org/abs/2403.05518v1","category":"cs.CL"}
{"created":"2024-03-08 18:36:07","title":"Convex Geometry of Building Sets","abstract":"Building sets were introduced in the study of wonderful compactifications of hyperplane arrangement complements and were later generalized to meet-semilattices. Convex geometries, the duals of antimatroids, offer a robust combinatorial abstraction of convexity. Supersolvable convex geometries and antimatroids make appearances in the study of poset closure operators, Coxeter groups, and matroid activities. We prove that the building sets on a meet-semilattice form a supersolvable convex geometry.","sentences":["Building sets were introduced in the study of wonderful compactifications of hyperplane arrangement complements and were later generalized to meet-semilattices.","Convex geometries, the duals of antimatroids, offer a robust combinatorial abstraction of convexity.","Supersolvable convex geometries and antimatroids make appearances in the study of poset closure operators, Coxeter groups, and matroid activities.","We prove that the building sets on a meet-semilattice form a supersolvable convex geometry."],"url":"http://arxiv.org/abs/2403.05514v1","category":"math.CO"}
{"created":"2024-03-08 18:31:51","title":"Resilience of the slow component in timescale separated synchronized oscillators","abstract":"Physiological networks are usually made of a large number of biological oscillators evolving on a multitude of different timescales. Phase oscillators are particularly useful in the modelling of the synchronization dynamics of such systems. If the coupling is strong enough compared to the heterogeneity of the internal parameters, synchronized states might emerge where phase oscillators start to behave coherently. Here, we focus on the case where synchronized oscillators are divided into a fast and a slow component so that the two subsets evolve on separated timescales. We assess the resilience of the slow component by reducing the dynamics of the fast one and evaluating the variance of the phase deviations when the oscillators in the two components are subject to noise with possibly distinct correlation times. From the general expression for the variance, we consider specific network structures and show how the noise transmission between the fast and slow components is affected. Interestingly, we find that oscillators that are among the most robust when there is only a single timescale, might become the most vulnerable when the system separates into two timescales. We also find that layered networks seem to be insensitive to timescale separations when the noise has homogeneous correlation time.","sentences":["Physiological networks are usually made of a large number of biological oscillators evolving on a multitude of different timescales.","Phase oscillators are particularly useful in the modelling of the synchronization dynamics of such systems.","If the coupling is strong enough compared to the heterogeneity of the internal parameters, synchronized states might emerge where phase oscillators start to behave coherently.","Here, we focus on the case where synchronized oscillators are divided into a fast and a slow component so that the two subsets evolve on separated timescales.","We assess the resilience of the slow component by reducing the dynamics of the fast one and evaluating the variance of the phase deviations when the oscillators in the two components are subject to noise with possibly distinct correlation times.","From the general expression for the variance, we consider specific network structures and show how the noise transmission between the fast and slow components is affected.","Interestingly, we find that oscillators that are among the most robust when there is only a single timescale, might become the most vulnerable when the system separates into two timescales.","We also find that layered networks seem to be insensitive to timescale separations when the noise has homogeneous correlation time."],"url":"http://arxiv.org/abs/2403.05510v1","category":"nlin.AO"}
{"created":"2024-03-08 18:29:00","title":"On maximal nowhere dense sublocales","abstract":"The aim of this paper is to study some variants of nowhere dense sublocales called maximal nowhere dense and homogeneous maximal nowhere dense sublocales. These concepts were initially introduced by Veksler in classical topology. We give some general properties of these sublocales and further examine their relationship with both inaccessible sublocales and remote sublocales. It turns out that a locale has all of its non-void nowhere dense sublocales maximal nowhere dense precisely when all of its its non-void nowhere dense sublocales are inaccessible. We show that the Booleanization of a locale is inaccessible with respect to every dense and open sublocale. In connection to remote sublocales, we prove that, if the supplement of an open dense sublocale S is homogeneous maximal nowhere dense, then every $S^{\\#}$-remote sublocale is *-remote from S. Every open localic map that sends dense elements to dense elements preserves and reflects maximal nowhere dense sublocales. If such a localic map is further injective, then it sends homogeneous maximal nowhere dense sublocales back and forth.","sentences":["The aim of this paper is to study some variants of nowhere dense sublocales called maximal nowhere dense and homogeneous maximal nowhere dense sublocales.","These concepts were initially introduced by Veksler in classical topology.","We give some general properties of these sublocales and further examine their relationship with both inaccessible sublocales and remote sublocales.","It turns out that a locale has all of its non-void nowhere dense sublocales maximal nowhere dense precisely when all of its its non-void nowhere dense sublocales are inaccessible.","We show that the Booleanization of a locale is inaccessible with respect to every dense and open sublocale.","In connection to remote sublocales, we prove that, if the supplement of an open dense sublocale S is homogeneous maximal nowhere dense, then every $S^{\\#}$-remote sublocale is *-remote from S. Every open localic map that sends dense elements to dense elements preserves and reflects maximal nowhere dense sublocales.","If such a localic map is further injective, then it sends homogeneous maximal nowhere dense sublocales back and forth."],"url":"http://arxiv.org/abs/2403.05508v1","category":"math.GN"}
{"created":"2024-03-08 18:13:06","title":"Les Houches Lectures on Community Ecology: From Niche Theory to Statistical Mechanics","abstract":"Ecosystems are among the most interesting and well-studied examples of self-organized complex systems. Community ecology, the study of how species interact with each other and the environment, has a rich tradition. Over the last few years, there has been a growing theoretical and experimental interest in these problems from the physics and quantitative biology communities. Here, we give an overview of community ecology, highlighting the deep connections between ecology and statistical physics. We start by introducing the two classes of mathematical models that have served as the workhorses of community ecology: Consumer Resource Models (CRM) and the generalized Lotka-Volterra models (GLV). We place a special emphasis on graphical methods and general principles. We then review recent works showing a deep and surprising connection between ecological dynamics and constrained optimization. We then shift our focus by analyzing these same models in \"high-dimensions\" (i.e. in the limit where the number of species and resources in the ecosystem becomes large) and discuss how such complex ecosystems can be analyzed using methods from the statistical physics of disordered systems such as the cavity method and Random Matrix Theory.","sentences":["Ecosystems are among the most interesting and well-studied examples of self-organized complex systems.","Community ecology, the study of how species interact with each other and the environment, has a rich tradition.","Over the last few years, there has been a growing theoretical and experimental interest in these problems from the physics and quantitative biology communities.","Here, we give an overview of community ecology, highlighting the deep connections between ecology and statistical physics.","We start by introducing the two classes of mathematical models that have served as the workhorses of community ecology: Consumer Resource Models (CRM) and the generalized Lotka-Volterra models (GLV).","We place a special emphasis on graphical methods and general principles.","We then review recent works showing a deep and surprising connection between ecological dynamics and constrained optimization.","We then shift our focus by analyzing these same models in \"high-dimensions\" (i.e. in the limit where the number of species and resources in the ecosystem becomes large) and discuss how such complex ecosystems can be analyzed using methods from the statistical physics of disordered systems such as the cavity method and Random Matrix Theory."],"url":"http://arxiv.org/abs/2403.05497v1","category":"q-bio.PE"}
{"created":"2024-03-08 18:05:17","title":"Rediscovering the Mullins Effect With Deep Symbolic Regression","abstract":"The Mullins effect represents a softening phenomenon observed in rubber-like materials and soft biological tissues. It is usually accompanied by many other inelastic effects like for example residual strain and induced anisotropy. In spite of the long term research and many material models proposed in literature, accurate modeling and prediction of this complex phenomenon still remain a challenging task.   In this work, we present a novel approach using deep symbolic regression (DSR) to generate material models describing the Mullins effect in the context of nearly incompressible hyperelastic materials. The two step framework first identifies a strain energy function describing the primary loading. Subsequently, a damage function characterizing the softening behavior under cyclic loading is identified. The efficiency of the proposed approach is demonstrated through benchmark tests using the generalized the Mooney-Rivlin and the Ogden-Roxburgh model. The generalizability and robustness of the presented framework are thoroughly studied. In addition, the proposed methodology is extensively validated on a temperature-dependent data set, which demonstrates its versatile and reliable performance.","sentences":["The Mullins effect represents a softening phenomenon observed in rubber-like materials and soft biological tissues.","It is usually accompanied by many other inelastic effects like for example residual strain and induced anisotropy.","In spite of the long term research and many material models proposed in literature, accurate modeling and prediction of this complex phenomenon still remain a challenging task.   ","In this work, we present a novel approach using deep symbolic regression (DSR) to generate material models describing the Mullins effect in the context of nearly incompressible hyperelastic materials.","The two step framework first identifies a strain energy function describing the primary loading.","Subsequently, a damage function characterizing the softening behavior under cyclic loading is identified.","The efficiency of the proposed approach is demonstrated through benchmark tests using the generalized the Mooney-Rivlin and the Ogden-Roxburgh model.","The generalizability and robustness of the presented framework are thoroughly studied.","In addition, the proposed methodology is extensively validated on a temperature-dependent data set, which demonstrates its versatile and reliable performance."],"url":"http://arxiv.org/abs/2403.05495v1","category":"cs.CE"}
{"created":"2024-03-08 18:04:03","title":"To Err Is Human, but Llamas Can Learn It Too","abstract":"This study explores enhancing grammatical error correction (GEC) through artificial error generation (AEG) using language models (LMs). Specifically, we fine-tune Llama 2-based LMs for error generation and find that this approach yields synthetic errors akin to human errors. Next, we train GEC Llama models with the help of these artificial errors and outperform previous state-of-the-art error correction models, with gains ranging between 0.8 and 6 F0.5 points across all tested languages (German, Ukrainian, and Estonian). Moreover, we demonstrate that generating errors by fine-tuning smaller sequence-to-sequence models and prompting large commercial LMs (GPT-3.5 and GPT-4) also results in synthetic errors beneficially affecting error generation models.","sentences":["This study explores enhancing grammatical error correction (GEC) through artificial error generation (AEG) using language models (LMs).","Specifically, we fine-tune Llama 2-based LMs for error generation and find that this approach yields synthetic errors akin to human errors.","Next, we train GEC Llama models with the help of these artificial errors and outperform previous state-of-the-art error correction models, with gains ranging between 0.8 and 6 F0.5 points across all tested languages (German, Ukrainian, and Estonian).","Moreover, we demonstrate that generating errors by fine-tuning smaller sequence-to-sequence models and prompting large commercial LMs (GPT-3.5 and GPT-4) also results in synthetic errors beneficially affecting error generation models."],"url":"http://arxiv.org/abs/2403.05493v1","category":"cs.CL"}
{"created":"2024-03-08 17:58:34","title":"The Strong Lefschetz Property of Gorenstein Algebras Generated by Relative Invariants","abstract":"We prove the strong Lefschetz property for Artinian Gorenstein algebras generated by the relative invariants of prehomogeneous vector spaces of commutative parabolic type.","sentences":["We prove the strong Lefschetz property for Artinian Gorenstein algebras generated by the relative invariants of prehomogeneous vector spaces of commutative parabolic type."],"url":"http://arxiv.org/abs/2403.05492v1","category":"math.AC"}
{"created":"2024-03-08 17:55:41","title":"Poly-View Contrastive Learning","abstract":"Contrastive learning typically matches pairs of related views among a number of unrelated negative views. Views can be generated (e.g. by augmentations) or be observed. We investigate matching when there are more than two related views which we call poly-view tasks, and derive new representation learning objectives using information maximization and sufficient statistics. We show that with unlimited computation, one should maximize the number of related views, and with a fixed compute budget, it is beneficial to decrease the number of unique samples whilst increasing the number of views of those samples. In particular, poly-view contrastive models trained for 128 epochs with batch size 256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k, challenging the belief that contrastive models require large batch sizes and many training epochs.","sentences":["Contrastive learning typically matches pairs of related views among a number of unrelated negative views.","Views can be generated (e.g. by augmentations) or be observed.","We investigate matching when there are more than two related views which we call poly-view tasks, and derive new representation learning objectives using information maximization and sufficient statistics.","We show that with unlimited computation, one should maximize the number of related views, and with a fixed compute budget, it is beneficial to decrease the number of unique samples whilst increasing the number of views of those samples.","In particular, poly-view contrastive models trained for 128 epochs with batch size 256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k, challenging the belief that contrastive models require large batch sizes and many training epochs."],"url":"http://arxiv.org/abs/2403.05490v1","category":"cs.LG"}
{"created":"2024-03-08 17:50:41","title":"Subgroup Separability of Artin Groups II","abstract":"In this paper, we establish a new criterion for determining whether an Artin group is subgroup separable (LERF), building upon a criterion introduced in a previous work. Specifically, we prove an Artin group is LERF if and only if it does not contain certain induced subgraphs, thus providing a more direct generalization of the Metaftsis-Raptis criterion for RAAGs. As consequences, we prove an Artin group is ERF if and only if it is a free abelian group and we establish a link between subgroup separability and coherence for Artin groups.","sentences":["In this paper, we establish a new criterion for determining whether an Artin group is subgroup separable (LERF), building upon a criterion introduced in a previous work.","Specifically, we prove an Artin group is LERF if and only if it does not contain certain induced subgraphs, thus providing a more direct generalization of the Metaftsis-Raptis criterion for RAAGs.","As consequences, we prove an Artin group is ERF if and only if it is a free abelian group and we establish a link between subgroup separability and coherence for Artin groups."],"url":"http://arxiv.org/abs/2403.05483v1","category":"math.GR"}
{"created":"2024-03-08 17:50:13","title":"Quantum phenomena in attosecond science","abstract":"Attosecond science has opened up new frontiers in our understanding of processes happening on the intrinsic timescale of electrons. The ability to manipulate and observe phenomena at the attosecond level has yielded groundbreaking insights into processes such as electron dynamics and the behavior of matter under extreme conditions. This interdisciplinary field bridges various research areas such as quantum optics, quantum chemistry and quantum information science facilitating a cohesive understanding. However, despite many emerging successful applications, the discussion about intrinsic quantum effects has mainly been ignored. In this Perspective, we explore the latest advancements in quantum phenomena within attosecond science, encompassing both experimental and theoretical progress. Specifically, in the context of high-harmonic generation and above-threshold ionization, we focus on discerning genuinely quantum observations and distinguishing them from classical phenomena. Additionally, we illuminate the often overlooked yet significant role of entanglement in attosecond processes, elucidating its influence on experimental outcomes.","sentences":["Attosecond science has opened up new frontiers in our understanding of processes happening on the intrinsic timescale of electrons.","The ability to manipulate and observe phenomena at the attosecond level has yielded groundbreaking insights into processes such as electron dynamics and the behavior of matter under extreme conditions.","This interdisciplinary field bridges various research areas such as quantum optics, quantum chemistry and quantum information science facilitating a cohesive understanding.","However, despite many emerging successful applications, the discussion about intrinsic quantum effects has mainly been ignored.","In this Perspective, we explore the latest advancements in quantum phenomena within attosecond science, encompassing both experimental and theoretical progress.","Specifically, in the context of high-harmonic generation and above-threshold ionization, we focus on discerning genuinely quantum observations and distinguishing them from classical phenomena.","Additionally, we illuminate the often overlooked yet significant role of entanglement in attosecond processes, elucidating its influence on experimental outcomes."],"url":"http://arxiv.org/abs/2403.05482v1","category":"quant-ph"}
{"created":"2024-03-08 17:43:13","title":"The temperature affects the impact levels of synthetic insecticides on a parasitoid wasp used in the biological control of pentatomid pests in soybean crops","abstract":"The impact of climate change has led to growing global concern about the interaction of temperature and xenobiotics in agricultural toxicological studies. Thus, for the first time, we evaluated the lethal, sublethal and transgerational effects of six insecticides used in the management of stink bug complex in soybean crops on the different life stages of the parasitoid Telenomus podisi (Hymenoptera: Scelionidae) in three temperature levels (15, 25 and 30 {\\deg}C). Telenomus podisi adults (F0 generation), when exposed to insecticides based on acephate, spinosad and thiamethoxam + lambda-cyhalothrin, showed accumulated mortality of 100% at all temperature levels tested. On the other hand, methoxyfenozide + spinetoram caused average mortalities of 88.75% at 15 {\\deg}C and 38.75% at 25 and 30 {\\deg}C. In contrast, the mortality rates caused by chlorfenapyr at 15, 25 and 30 {\\deg}C were 1.25, 71.25 and 71.25%. On the other hand, surviving adults in lethal toxicity bioassay did not show differences in egg parasitism (F0 generation) and emergence of F1 generation in all temperature levels studied; however, the insecticide methoxyfenozide + spinetoram showed the lowest level of parasitism and emergence of T. podisi. In addition, our results demonstrated significant changes in the proportion of emerged males and females as the temperature increased; however, we did not find any differences when comparing the insecticides studied. Furthermore, we detected a significant interaction between insecticides and temperatures by contaminating the host's parasitized eggs (parasitoid pupal stage). Generally, the highest emergence reduction values were found at the highest temperature studied (30 {\\deg}C). Our results highlighted the temperature-dependent impact of synthetic insecticides on parasitoids, which should be considered in toxicological risk assessments and under predicted climate change scenarios.","sentences":["The impact of climate change has led to growing global concern about the interaction of temperature and xenobiotics in agricultural toxicological studies.","Thus, for the first time, we evaluated the lethal, sublethal and transgerational effects of six insecticides used in the management of stink bug complex in soybean crops on the different life stages of the parasitoid Telenomus podisi (Hymenoptera: Scelionidae) in three temperature levels (15, 25 and 30 {\\deg}C).","Telenomus podisi adults (F0 generation), when exposed to insecticides based on acephate, spinosad and thiamethoxam + lambda-cyhalothrin, showed accumulated mortality of 100% at all temperature levels tested.","On the other hand, methoxyfenozide + spinetoram caused average mortalities of 88.75% at 15 {\\deg}C and 38.75% at 25 and 30 {\\deg}C.","In contrast, the mortality rates caused by chlorfenapyr at 15, 25 and 30 {\\deg}C were 1.25, 71.25 and 71.25%.","On the other hand, surviving adults in lethal toxicity bioassay did not show differences in egg parasitism (F0 generation) and emergence of F1 generation in all temperature levels studied; however, the insecticide methoxyfenozide + spinetoram showed the lowest level of parasitism and emergence of T. podisi.","In addition, our results demonstrated significant changes in the proportion of emerged males and females as the temperature increased; however, we did not find any differences when comparing the insecticides studied.","Furthermore, we detected a significant interaction between insecticides and temperatures by contaminating the host's parasitized eggs (parasitoid pupal stage).","Generally, the highest emergence reduction values were found at the highest temperature studied (30 {\\deg}C).","Our results highlighted the temperature-dependent impact of synthetic insecticides on parasitoids, which should be considered in toxicological risk assessments and under predicted climate change scenarios."],"url":"http://arxiv.org/abs/2403.05479v1","category":"q-bio.QM"}
{"created":"2024-03-08 17:34:20","title":"Squeezing, trisqueezing, and quadsqueezing in a spin-oscillator system","abstract":"Quantum harmonic oscillators model a wide variety of phenomena ranging from electromagnetic fields to vibrations of atoms in molecules. Their excitations can be represented by bosons such as photons, single particles of light, or phonons, the quanta of vibrational energy. Linear interactions that only create and annihilate single bosons can generate coherent states of light or motion. Introducing nth-order nonlinear interactions, that instead involve n bosons, leads to increasingly complex quantum behaviour. For example, second-order interactions enable squeezing, used to enhance the precision of measurements beyond classical limits, while higher-order interactions create non-Gaussian states essential for continuous-variable quantum computation. However, generating nonlinear interactions is challenging, typically requiring higher-order derivatives of the driving field or specialized hardware. Hybrid systems, where linear interactions couple an oscillator to an additional spin, offer a solution and are readily available across many platforms. Here, using the spin of a single trapped ion coupled to its motion, we employ two linear interactions to demonstrate up to fourth-order bosonic interactions; we focus on generalised squeezing interactions and demonstrate squeezing, trisqueezing, and quadsqueezing. We characterise these interactions, including their spin dependence, and reconstruct the Wigner function of the resulting states. We also discuss the scaling of the interaction strength, where we drive the quadsqueezing interaction more than 100 times faster than using conventional techniques. Our method presents no fundamental limit in the interaction order n and applies to any platform supporting spin-dependent linear interactions. Strong higher-order nonlinear interactions unlock the study of fundamental quantum optics, quantum simulation, and computation in a hitherto unexplored regime.","sentences":["Quantum harmonic oscillators model a wide variety of phenomena ranging from electromagnetic fields to vibrations of atoms in molecules.","Their excitations can be represented by bosons such as photons, single particles of light, or phonons, the quanta of vibrational energy.","Linear interactions that only create and annihilate single bosons can generate coherent states of light or motion.","Introducing nth-order nonlinear interactions, that instead involve n bosons, leads to increasingly complex quantum behaviour.","For example, second-order interactions enable squeezing, used to enhance the precision of measurements beyond classical limits, while higher-order interactions create non-Gaussian states essential for continuous-variable quantum computation.","However, generating nonlinear interactions is challenging, typically requiring higher-order derivatives of the driving field or specialized hardware.","Hybrid systems, where linear interactions couple an oscillator to an additional spin, offer a solution and are readily available across many platforms.","Here, using the spin of a single trapped ion coupled to its motion, we employ two linear interactions to demonstrate up to fourth-order bosonic interactions; we focus on generalised squeezing interactions and demonstrate squeezing, trisqueezing, and quadsqueezing.","We characterise these interactions, including their spin dependence, and reconstruct the Wigner function of the resulting states.","We also discuss the scaling of the interaction strength, where we drive the quadsqueezing interaction more than 100 times faster than using conventional techniques.","Our method presents no fundamental limit in the interaction order n and applies to any platform supporting spin-dependent linear interactions.","Strong higher-order nonlinear interactions unlock the study of fundamental quantum optics, quantum simulation, and computation in a hitherto unexplored regime."],"url":"http://arxiv.org/abs/2403.05471v1","category":"quant-ph"}
{"created":"2024-03-08 17:30:41","title":"Will GPT-4 Run DOOM?","abstract":"We show that GPT-4's reasoning and planning capabilities extend to the 1993 first-person shooter Doom. This large language model (LLM) is able to run and play the game with only a few instructions, plus a textual description--generated by the model itself from screenshots--about the state of the game being observed. We find that GPT-4 can play the game to a passable degree: it is able to manipulate doors, combat enemies, and perform pathing. More complex prompting strategies involving multiple model calls provide better results. While further work is required to enable the LLM to play the game as well as its classical, reinforcement learning-based counterparts, we note that GPT-4 required no training, leaning instead on its own reasoning and observational capabilities. We hope our work pushes the boundaries on intelligent, LLM-based agents in video games. We conclude by discussing the ethical implications of our work.","sentences":["We show that GPT-4's reasoning and planning capabilities extend to the 1993 first-person shooter Doom.","This large language model (LLM) is able to run and play the game with only a few instructions, plus a textual description--generated by the model itself from screenshots--about the state of the game being observed.","We find that GPT-4 can play the game to a passable degree: it is able to manipulate doors, combat enemies, and perform pathing.","More complex prompting strategies involving multiple model calls provide better results.","While further work is required to enable the LLM to play the game as well as its classical, reinforcement learning-based counterparts, we note that GPT-4 required no training, leaning instead on its own reasoning and observational capabilities.","We hope our work pushes the boundaries on intelligent, LLM-based agents in video games.","We conclude by discussing the ethical implications of our work."],"url":"http://arxiv.org/abs/2403.05468v1","category":"cs.CL"}
{"created":"2024-03-08 17:29:51","title":"Grasping Trajectory Optimization with Point Clouds","abstract":"We introduce a new trajectory optimization method for robotic grasping based on a point-cloud representation of robots and task spaces. In our method, robots are represented by 3D points on their link surfaces. The task space of a robot is represented by a point cloud that can be obtained from depth sensors. Using the point-cloud representation, goal reaching in grasping can be formulated as point matching, while collision avoidance can be efficiently achieved by querying the signed distance values of the robot points in the signed distance field of the scene points. Consequently, a constrained non-linear optimization problem is formulated to solve the joint motion and grasp planning problem. The advantage of our method is that the point-cloud representation is general to be used with any robot in any environment. We demonstrate the effectiveness of our method by conducting experiments on a tabletop scene and a shelf scene for grasping with a Fetch mobile manipulator and a Franka Panda arm.","sentences":["We introduce a new trajectory optimization method for robotic grasping based on a point-cloud representation of robots and task spaces.","In our method, robots are represented by 3D points on their link surfaces.","The task space of a robot is represented by a point cloud that can be obtained from depth sensors.","Using the point-cloud representation, goal reaching in grasping can be formulated as point matching, while collision avoidance can be efficiently achieved by querying the signed distance values of the robot points in the signed distance field of the scene points.","Consequently, a constrained non-linear optimization problem is formulated to solve the joint motion and grasp planning problem.","The advantage of our method is that the point-cloud representation is general to be used with any robot in any environment.","We demonstrate the effectiveness of our method by conducting experiments on a tabletop scene and a shelf scene for grasping with a Fetch mobile manipulator and a Franka Panda arm."],"url":"http://arxiv.org/abs/2403.05466v1","category":"cs.RO"}
{"created":"2024-03-08 17:28:49","title":"Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference","abstract":"Traditional Deep Neural Network (DNN) quantization methods using integer, fixed-point, or floating-point data types struggle to capture diverse DNN parameter distributions at low precision, and often require large silicon overhead and intensive quantization-aware training. In this study, we introduce Logarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by posits that dynamically adapts to DNN weight/activation distributions by parameterizing LP bit fields. We also develop a novel genetic-algorithm based framework, LP Quantization (LPQ), to find optimal layer-wise LP parameters while reducing representational divergence between quantized and full-precision models through a novel global-local contrastive objective. Additionally, we design a unified mixed-precision LP accelerator (LPA) architecture comprising of processing elements (PEs) incorporating LP in the computational datapath. Our algorithm-hardware co-design demonstrates on average <1% drop in top-1 accuracy across various CNN and ViT models. It also achieves ~ 2x improvements in performance per unit area and 2.2x gains in energy efficiency compared to state-of-the-art quantization accelerators using different data types.","sentences":["Traditional Deep Neural Network (DNN) quantization methods using integer, fixed-point, or floating-point data types struggle to capture diverse DNN parameter distributions at low precision, and often require large silicon overhead and intensive quantization-aware training.","In this study, we introduce Logarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by posits that dynamically adapts to DNN weight/activation distributions by parameterizing LP bit fields.","We also develop a novel genetic-algorithm based framework, LP Quantization (LPQ), to find optimal layer-wise LP parameters while reducing representational divergence between quantized and full-precision models through a novel global-local contrastive objective.","Additionally, we design a unified mixed-precision LP accelerator (LPA) architecture comprising of processing elements (PEs) incorporating LP in the computational datapath.","Our algorithm-hardware co-design demonstrates on average <1% drop in top-1 accuracy across various CNN and ViT models.","It also achieves ~","2x improvements in performance per unit area and 2.2x gains in energy efficiency compared to state-of-the-art quantization accelerators using different data types."],"url":"http://arxiv.org/abs/2403.05465v1","category":"cs.AR"}
{"created":"2024-03-08 17:27:13","title":"Generalized Yang Poisson models on canonical phase space","abstract":"We discuss the generalized Yang-Poisson models. We costruct generalizations of the Yang Poisson algebra related to so(1, 5) algebra. The exact realizations of this generalized algebra on canonical phase space are presented and the corresponding differential equations are solved in simple case. Futhermore, we discuss a Poisson algebras related to so(3, 3) and so(2, 4) algebras.","sentences":["We discuss the generalized Yang-Poisson models.","We costruct generalizations of the Yang Poisson algebra related to so(1, 5) algebra.","The exact realizations of this generalized algebra on canonical phase space are presented and the corresponding differential equations are solved in simple case.","Futhermore, we discuss a Poisson algebras related to so(3, 3) and so(2, 4) algebras."],"url":"http://arxiv.org/abs/2403.05464v1","category":"math-ph"}
{"created":"2024-03-08 17:10:41","title":"Evaluating AI and Human Authorship Quality in Academic Writing through Physics Essays","abstract":"This study evaluates $n = 300$ short-form physics essay submissions, equally divided between student work submitted before the introduction of ChatGPT and those generated by OpenAI's GPT-4. In blinded evaluations conducted by five independent markers who were unaware of the origin of the essays, we observed no statistically significant differences in scores between essays authored by humans and those produced by AI (p-value $= 0.107$, $\\alpha$ = 0.05). Additionally, when the markers subsequently attempted to identify the authorship of the essays on a 4-point Likert scale - from `Definitely AI' to `Definitely Human' - their performance was only marginally better than random chance. This outcome not only underscores the convergence of AI and human authorship quality but also highlights the difficulty of discerning AI-generated content solely through human judgment. Furthermore, the effectiveness of five commercially available software tools for identifying essay authorship was evaluated. Among these, ZeroGPT was the most accurate, achieving a 98% accuracy rate and a precision score of 1.0 when its classifications were reduced to binary outcomes. This result is a source of potential optimism for maintaining assessment integrity. Finally, we propose that texts with $\\leq 50\\%$ AI-generated content should be considered the upper limit for classification as human-authored, a boundary inclusive of a future with ubiquitous AI assistance whilst also respecting human-authorship.","sentences":["This study evaluates $n = 300$ short-form physics essay submissions, equally divided between student work submitted before the introduction of ChatGPT and those generated by OpenAI's GPT-4.","In blinded evaluations conducted by five independent markers who were unaware of the origin of the essays, we observed no statistically significant differences in scores between essays authored by humans and those produced by AI (p-value $= 0.107$, $\\alpha$ = 0.05).","Additionally, when the markers subsequently attempted to identify the authorship of the essays on a 4-point Likert scale - from `Definitely AI' to `Definitely Human' - their performance was only marginally better than random chance.","This outcome not only underscores the convergence of AI and human authorship quality but also highlights the difficulty of discerning AI-generated content solely through human judgment.","Furthermore, the effectiveness of five commercially available software tools for identifying essay authorship was evaluated.","Among these, ZeroGPT was the most accurate, achieving a 98% accuracy rate and a precision score of 1.0 when its classifications were reduced to binary outcomes.","This result is a source of potential optimism for maintaining assessment integrity.","Finally, we propose that texts with $\\leq 50\\%$ AI-generated content should be considered the upper limit for classification as human-authored, a boundary inclusive of a future with ubiquitous AI assistance whilst also respecting human-authorship."],"url":"http://arxiv.org/abs/2403.05458v1","category":"physics.ed-ph"}
{"created":"2024-03-08 16:59:01","title":"Asymptotic Variation of Elementary Abelian p-Extensions over $P^1$","abstract":"In this paper, we prove that there exists a Zariski dense open subset $U$ in the parameter space of all elementary $p$-covers of the projective line that ramified at exactly one point, defined over the rationals, such that for every curve $X$ in $U(\\overline{Q})$ and for any prime $p$ large enough, the reduction of $X$ at all primes lying over $p$ achieves its generic Newton slopes.","sentences":["In this paper, we prove that there exists a Zariski dense open subset $U$ in the parameter space of all elementary $p$-covers of the projective line that ramified at exactly one point, defined over the rationals, such that for every curve $X$ in $U(\\overline{Q})$ and for any prime $p$ large enough, the reduction of $X$ at all primes lying over $p$ achieves its generic Newton slopes."],"url":"http://arxiv.org/abs/2403.05453v1","category":"math.NT"}
{"created":"2024-03-08 16:54:55","title":"Safe Execution of Learned Orientation Skills with Conic Control Barrier Functions","abstract":"In the field of Learning from Demonstration (LfD), Dynamical Systems (DSs) have gained significant attention due to their ability to generate real-time motions and reach predefined targets. However, the conventional convergence-centric behavior exhibited by DSs may fall short in safety-critical tasks, specifically, those requiring precise replication of demonstrated trajectories or strict adherence to constrained regions even in the presence of perturbations or human intervention. Moreover, existing DS research often assumes demonstrations solely in Euclidean space, overlooking the crucial aspect of orientation in various applications. To alleviate these shortcomings, we present an innovative approach geared toward ensuring the safe execution of learned orientation skills within constrained regions surrounding a reference trajectory. This involves learning a stable DS on SO(3), extracting time-varying conic constraints from the variability observed in expert demonstrations, and bounding the evolution of the DS with Conic Control Barrier Function (CCBF) to fulfill the constraints. We validated our approach through extensive evaluation in simulation and showcased its effectiveness for a cutting skill in the context of assisted teleoperation.","sentences":["In the field of Learning from Demonstration (LfD), Dynamical Systems (DSs) have gained significant attention due to their ability to generate real-time motions and reach predefined targets.","However, the conventional convergence-centric behavior exhibited by DSs may fall short in safety-critical tasks, specifically, those requiring precise replication of demonstrated trajectories or strict adherence to constrained regions even in the presence of perturbations or human intervention.","Moreover, existing DS research often assumes demonstrations solely in Euclidean space, overlooking the crucial aspect of orientation in various applications.","To alleviate these shortcomings, we present an innovative approach geared toward ensuring the safe execution of learned orientation skills within constrained regions surrounding a reference trajectory.","This involves learning a stable DS on SO(3), extracting time-varying conic constraints from the variability observed in expert demonstrations, and bounding the evolution of the DS with Conic Control Barrier Function (CCBF) to fulfill the constraints.","We validated our approach through extensive evaluation in simulation and showcased its effectiveness for a cutting skill in the context of assisted teleoperation."],"url":"http://arxiv.org/abs/2403.05447v1","category":"cs.RO"}
{"created":"2024-03-08 16:52:45","title":"Chlorine and zinc co-doping effects on the electronic structure and optical properties of \u03b3-CuI","abstract":"The effects of chlorine (Cl) and zinc (Zn) co-doping on the electronic structure and optical properties of the zinc blende ({\\gamma}) phase of copper iodide ({\\gamma}-CuI) scintillator material are investigated by using first-principles density functional theory calculations. The band structure, density of states, dielectric function, absorption coefficients, and reflectivity were analyzed before and after doping. Results show co-doping significantly modifies the band structure, reduces the band gap, and generates impurity energy levels. Cl doping enhances absorption in the high energy region while reducing visible light absorption. Zn doping induces a redshift in absorption and n-type conductivity at high concentrations. With suitable co-doping ratios, the absorption coefficient and reflectivity of {\\gamma}-CuI can be optimized in the visible range to improve scintillation light yield. The calculations provide guidance for co-doping {\\gamma}-CuI scintillators to achieve superior detection performance. The n-type conductivity also makes doped {\\gamma}-CuI promising for optoelectronic applications.","sentences":["The effects of chlorine (Cl) and zinc (Zn) co-doping on the electronic structure and optical properties of the zinc blende ({\\gamma}) phase of copper iodide ({\\gamma}-CuI) scintillator material are investigated by using first-principles density functional theory calculations.","The band structure, density of states, dielectric function, absorption coefficients, and reflectivity were analyzed before and after doping.","Results show co-doping significantly modifies the band structure, reduces the band gap, and generates impurity energy levels.","Cl doping enhances absorption in the high energy region while reducing visible light absorption.","Zn doping induces a redshift in absorption and n-type conductivity at high concentrations.","With suitable co-doping ratios, the absorption coefficient and reflectivity of {\\gamma}-CuI can be optimized in the visible range to improve scintillation light yield.","The calculations provide guidance for co-doping {\\gamma}-CuI scintillators to achieve superior detection performance.","The n-type conductivity also makes doped {\\gamma}-CuI promising for optoelectronic applications."],"url":"http://arxiv.org/abs/2403.05444v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-08 16:51:47","title":"Modelling Triatomic Biosignatures: Ozone and Isotopomers","abstract":"In this work we present a new approach to produce spectroscopic constants and model first-principles synthetic spectra for all molecules of astrophysical interest. We have generalized our previous diatomic molecule simulation framework, employing Transition-Optimised Shifted Hermite (TOSH) theory, thereby enabling the modelling of polyatomic rotational constants for molecules with three or more atoms. These capabilities, are now provided by our new code Epimetheus.   As a first validation of our approach, we confront our predictions and assess their accuracy against the well-studied triatomic molecule, ozone 666 ($^{16}$O$_3$), in addition to eight of its potential isotopomers: ozone 668 ($^{16}$O$^{16}$O$^{18}$O), 686 ($^{16}$O$^{18}$O$^{16}$O), 667 ($^{16}$O$^{16}$O$^{17}$O), 676 ($^{16}$O$^{17}$O$^{16}$O), 688 ($^{16}$O$^{18}$O$^{18}$O), 868 ($^{18}$O$^{16}$O$^{18}$O), 888 ($^{18}$O$_3$), and 777 ($^{17}$O$_3$). We then assess the accuracy of these rotational constants using the Epimetheus data in our code Pandora, and generate synthetic molecular spectra.   The ozone spectra presented here are purely infrared and not Raman. Epimetheus builds upon the work from our previous code Prometheus, which used the TOSH theory to account for anharmonicity for the fundamental $\\nu=0 \\rightarrow \\nu=1$ band, going further to now account for triatomic molecules. This is combined with thermal profile modeling for the rotational transitions. We have found that this extended method performs promisingly, typically approximating the spectroscopic constants and spectra well. Some issues do arise depending on the symmetry group of the ozone isotopomer. In general, we show that Epimetheus can provide the data to produce appreciable molecular spectra, to help drive future high-resolution studies.","sentences":["In this work we present a new approach to produce spectroscopic constants and model first-principles synthetic spectra for all molecules of astrophysical interest.","We have generalized our previous diatomic molecule simulation framework, employing Transition-Optimised Shifted Hermite (TOSH) theory, thereby enabling the modelling of polyatomic rotational constants for molecules with three or more atoms.","These capabilities, are now provided by our new code Epimetheus.   ","As a first validation of our approach, we confront our predictions and assess their accuracy against the well-studied triatomic molecule, ozone 666 ($^{16}$O$_3$), in addition to eight of its potential isotopomers: ozone 668 ($^{16}$O$^{16}$O$^{18}$O), 686 ($^{16}$O$^{18}$O$^{16}$O), 667 ($^{16}$O$^{16}$O$^{17}$O), 676 ($^{16}$O$^{17}$O$^{16}$O), 688 ($^{16}$O$^{18}$O$^{18}$O), 868 ($^{18}$O$^{16}$O$^{18}$O), 888 ($^{18}$O$_3$), and 777 ($^{17}$O$_3$).","We then assess the accuracy of these rotational constants using the Epimetheus data in our code Pandora, and generate synthetic molecular spectra.   ","The ozone spectra presented here are purely infrared and not Raman.","Epimetheus builds upon the work from our previous code Prometheus, which used the TOSH theory to account for anharmonicity for the fundamental $\\nu=0 \\rightarrow \\nu=1$ band, going further to now account for triatomic molecules.","This is combined with thermal profile modeling for the rotational transitions.","We have found that this extended method performs promisingly, typically approximating the spectroscopic constants and spectra well.","Some issues do arise depending on the symmetry group of the ozone isotopomer.","In general, we show that Epimetheus can provide the data to produce appreciable molecular spectra, to help drive future high-resolution studies."],"url":"http://arxiv.org/abs/2403.05442v1","category":"physics.chem-ph"}
{"created":"2024-03-08 16:48:04","title":"Scarf complexes of graphs and their powers","abstract":"Every multigraded free resolution of a monomial ideal I contains the Scarf multidegrees of I. We say I has a Scarf resolution if the Scarf multidegrees are sufficient to describe a minimal free resolution of I. The main question of this paper is which graphs G have edge ideal I(G) with a Scarf resolution? We show that I(G) has a Scarf resolution if and only if G is a gap-free forest. We also classify connected graphs for which all powers of I(G) have Scarf resolutions. Along the way, we give a concrete description of the Scarf complex of any forest. For a general graph, we give a recursive construction for its Scarf complex based on Scarf complexes of induced subgraphs.","sentences":["Every multigraded free resolution of a monomial ideal I contains the Scarf multidegrees of I.","We say I has a Scarf resolution if the Scarf multidegrees are sufficient to describe a minimal free resolution of I.","The main question of this paper is which graphs G have edge ideal I(G) with a Scarf resolution?","We show that I(G) has a Scarf resolution if and only if G is a gap-free forest.","We also classify connected graphs for which all powers of I(G) have Scarf resolutions.","Along the way, we give a concrete description of the Scarf complex of any forest.","For a general graph, we give a recursive construction for its Scarf complex based on Scarf complexes of induced subgraphs."],"url":"http://arxiv.org/abs/2403.05439v1","category":"math.AC"}
{"created":"2024-03-08 16:44:54","title":"VideoElevator: Elevating Video Generation Quality with Versatile Text-to-Image Diffusion Models","abstract":"Text-to-image diffusion models (T2I) have demonstrated unprecedented capabilities in creating realistic and aesthetic images. On the contrary, text-to-video diffusion models (T2V) still lag far behind in frame quality and text alignment, owing to insufficient quality and quantity of training videos. In this paper, we introduce VideoElevator, a training-free and plug-and-play method, which elevates the performance of T2V using superior capabilities of T2I. Different from conventional T2V sampling (i.e., temporal and spatial modeling), VideoElevator explicitly decomposes each sampling step into temporal motion refining and spatial quality elevating. Specifically, temporal motion refining uses encapsulated T2V to enhance temporal consistency, followed by inverting to the noise distribution required by T2I. Then, spatial quality elevating harnesses inflated T2I to directly predict less noisy latent, adding more photo-realistic details. We have conducted experiments in extensive prompts under the combination of various T2V and T2I. The results show that VideoElevator not only improves the performance of T2V baselines with foundational T2I, but also facilitates stylistic video synthesis with personalized T2I. Our code is available at https://github.com/YBYBZhang/VideoElevator.","sentences":["Text-to-image diffusion models (T2I) have demonstrated unprecedented capabilities in creating realistic and aesthetic images.","On the contrary, text-to-video diffusion models (T2V) still lag far behind in frame quality and text alignment, owing to insufficient quality and quantity of training videos.","In this paper, we introduce VideoElevator, a training-free and plug-and-play method, which elevates the performance of T2V using superior capabilities of T2I. Different from conventional T2V sampling (i.e., temporal and spatial modeling), VideoElevator explicitly decomposes each sampling step into temporal motion refining and spatial quality elevating.","Specifically, temporal motion refining uses encapsulated T2V to enhance temporal consistency, followed by inverting to the noise distribution required by T2I. Then, spatial quality elevating harnesses inflated T2I to directly predict less noisy latent, adding more photo-realistic details.","We have conducted experiments in extensive prompts under the combination of various T2V and T2I. The results show that VideoElevator not only improves the performance of T2V baselines with foundational T2I, but also facilitates stylistic video synthesis with personalized T2I. Our code is available at https://github.com/YBYBZhang/VideoElevator."],"url":"http://arxiv.org/abs/2403.05438v1","category":"cs.CV"}
{"created":"2024-03-08 16:42:18","title":"On the ergodicity breaking in well-behaved Generalized Langevin Equations","abstract":"The phenomenon of ergodicity breaking of stochastic dynamics governed by Generalized Langevin Equations (GLE) in the presence of well-behaved exponentially decaying dissipative memory kernels, recently investigated by many authors (Phys. Rev. E {\\bf 83} 062102 2011; Phys. Rev. E {\\bf 98} 062140 2018; Eur. Phys. J. B {\\bf 93} 184 2020),   finds, in the dynamic theory of GLE, its simple and natural explanation, related to the concept of dissipative stability. It is shown that the occurrence of ergodicity breakdown for well-behaved dissipative kernels falls, in general, ouside the region of stochastic realizability, and therefore it cannot be observed in physical systems.","sentences":["The phenomenon of ergodicity breaking of stochastic dynamics governed by Generalized Langevin Equations (GLE) in the presence of well-behaved exponentially decaying dissipative memory kernels, recently investigated by many authors (Phys. Rev. E {\\bf 83} 062102 2011; Phys.","Rev. E {\\bf 98} 062140 2018; Eur. Phys.","J. B {\\bf 93} 184 2020),   finds, in the dynamic theory of GLE, its simple and natural explanation, related to the concept of dissipative stability.","It is shown that the occurrence of ergodicity breakdown for well-behaved dissipative kernels falls, in general, ouside the region of stochastic realizability, and therefore it cannot be observed in physical systems."],"url":"http://arxiv.org/abs/2403.05437v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-08 16:38:11","title":"OmniCount: Multi-label Object Counting with Semantic-Geometric Priors","abstract":"Object counting is pivotal for understanding the composition of scenes. Previously, this task was dominated by class-specific methods, which have gradually evolved into more adaptable class-agnostic strategies. However, these strategies come with their own set of limitations, such as the need for manual exemplar input and multiple passes for multiple categories, resulting in significant inefficiencies. This paper introduces a new, more practical approach enabling simultaneous counting of multiple object categories using an open vocabulary framework. Our solution, OmniCount, stands out by using semantic and geometric insights from pre-trained models to count multiple categories of objects as specified by users, all without additional training. OmniCount distinguishes itself by generating precise object masks and leveraging point prompts via the Segment Anything Model for efficient counting. To evaluate OmniCount, we created the OmniCount-191 benchmark, a first-of-its-kind dataset with multi-label object counts, including points, bounding boxes, and VQA annotations. Our comprehensive evaluation in OmniCount-191, alongside other leading benchmarks, demonstrates OmniCount's exceptional performance, significantly outpacing existing solutions and heralding a new era in object counting technology.","sentences":["Object counting is pivotal for understanding the composition of scenes.","Previously, this task was dominated by class-specific methods, which have gradually evolved into more adaptable class-agnostic strategies.","However, these strategies come with their own set of limitations, such as the need for manual exemplar input and multiple passes for multiple categories, resulting in significant inefficiencies.","This paper introduces a new, more practical approach enabling simultaneous counting of multiple object categories using an open vocabulary framework.","Our solution, OmniCount, stands out by using semantic and geometric insights from pre-trained models to count multiple categories of objects as specified by users, all without additional training.","OmniCount distinguishes itself by generating precise object masks and leveraging point prompts via the Segment Anything Model for efficient counting.","To evaluate OmniCount, we created the OmniCount-191 benchmark, a first-of-its-kind dataset with multi-label object counts, including points, bounding boxes, and VQA annotations.","Our comprehensive evaluation in OmniCount-191, alongside other leading benchmarks, demonstrates OmniCount's exceptional performance, significantly outpacing existing solutions and heralding a new era in object counting technology."],"url":"http://arxiv.org/abs/2403.05435v1","category":"cs.CV"}
{"created":"2024-03-08 16:37:36","title":"Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs","abstract":"Large Language Models (LLMs) exhibit impressive zero/few-shot inference and generation quality for high-resource languages(HRLs). A few of them have been trained in low-resource languages (LRLs) and give decent performance. Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens. The number of tokens strongly depends on the script and language, as well as the LLM's sub-word vocabulary. We show that LRLs are at a pricing disadvantage, because the well-known LLMs produce more tokens for LRLs than HRLs. This is because most currently popular LLMs are optimized for HRL vocabularies. Our objective is to level the playing field: reduce the cost of processing LRLs in contemporary LLMs while ensuring that predictive and generative qualities are not compromised. As means to reduce the number of tokens processed by the LLM, we consider code-mixing, translation, and transliteration of LRLs to HRLs. We perform an extensive study using the IndicXTREME dataset, covering 15 Indian languages, while using GPT-4 (one of the costliest LLM services released so far) as a commercial LLM. We observe and analyze interesting patterns involving token count, cost,and quality across a multitude of languages and tasks. We show that choosing the best policy to interact with the LLM can reduce cost by 90% while giving better or comparable performance, compared to communicating with the LLM in the original LRL.","sentences":["Large Language Models (LLMs) exhibit impressive zero/few-shot inference and generation quality for high-resource languages(HRLs).","A few of them have been trained in low-resource languages (LRLs) and give decent performance.","Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens.","The number of tokens strongly depends on the script and language, as well as the LLM's sub-word vocabulary.","We show that LRLs are at a pricing disadvantage, because the well-known LLMs produce more tokens for LRLs than HRLs.","This is because most currently popular LLMs are optimized for HRL vocabularies.","Our objective is to level the playing field: reduce the cost of processing LRLs in contemporary LLMs while ensuring that predictive and generative qualities are not compromised.","As means to reduce the number of tokens processed by the LLM, we consider code-mixing, translation, and transliteration of LRLs to HRLs.","We perform an extensive study using the IndicXTREME dataset, covering 15 Indian languages, while using GPT-4 (one of the costliest LLM services released so far) as a commercial LLM.","We observe and analyze interesting patterns involving token count, cost,and quality across a multitude of languages and tasks.","We show that choosing the best policy to interact with the LLM can reduce cost by 90% while giving better or comparable performance, compared to communicating with the LLM in the original LRL."],"url":"http://arxiv.org/abs/2403.05434v1","category":"cs.CL"}
{"created":"2024-03-08 16:34:30","title":"Part-aware Personalized Segment Anything Model for Patient-Specific Segmentation","abstract":"Precision medicine, such as patient-adaptive treatments utilizing medical images, poses new challenges for image segmentation algorithms due to (1) the large variability across different patients and (2) the limited availability of annotated data for each patient. In this work, we propose a data-efficient segmentation method to address these challenges, namely Part-aware Personalized Segment Anything Model (P^2SAM). Without any model fine-tuning, P^2SAM enables seamless adaptation to any new patients relying only on one-shot patient-specific data. We introduce a novel part-aware prompt mechanism to select multiple-point prompts based on part-level features of the one-shot data. To further promote the robustness of the selected prompt, we propose a retrieval approach to handle outlier prompts. Extensive experiments demonstrate that P^2SAM improves the performance by +8.0% and +2.0% mean Dice score within two patient-specific segmentation settings, and exhibits impressive generality across different application domains, e.g., +6.4% mIoU on the PerSeg benchmark. Code will be released upon acceptance.","sentences":["Precision medicine, such as patient-adaptive treatments utilizing medical images, poses new challenges for image segmentation algorithms due to (1) the large variability across different patients and (2) the limited availability of annotated data for each patient.","In this work, we propose a data-efficient segmentation method to address these challenges, namely Part-aware Personalized Segment Anything Model (P^2SAM).","Without any model fine-tuning, P^2SAM enables seamless adaptation to any new patients relying only on one-shot patient-specific data.","We introduce a novel part-aware prompt mechanism to select multiple-point prompts based on part-level features of the one-shot data.","To further promote the robustness of the selected prompt, we propose a retrieval approach to handle outlier prompts.","Extensive experiments demonstrate that P^2SAM improves the performance by +8.0% and +2.0% mean Dice score within two patient-specific segmentation settings, and exhibits impressive generality across different application domains, e.g., +6.4% mIoU on the PerSeg benchmark.","Code will be released upon acceptance."],"url":"http://arxiv.org/abs/2403.05433v1","category":"cs.CV"}
{"created":"2024-03-08 16:31:47","title":"Dynamic fluctuation-dissipation theory for Generalized Langevin Equations: constructive constraints, stability and realizability","abstract":"Using the initial-value formulation, a dynamic theory for systems evolving according to a Generalized Langevin Equation is developed, providing more restrictive conditions on the existence of equilibrium behavior and its fluctuation-dissipation implications. For systems fulfilling the property of local realizability,   that for all the practical purposes corresponds to the postulate of the existence of a Markovian embedding, physical constraints, expressed in the form of dissipative stability and stochastic realizability are derived. If these two properties are met, Kubo theory is constructively recovered, while if one of these conditions is violated a thermodynamic equilibrium behavior does not exist (and this occurs also for `` well-behaved dissipative systems'' according to the classical Kubo theory), with significant implications in the linear response theory.","sentences":["Using the initial-value formulation, a dynamic theory for systems evolving according to a Generalized Langevin Equation is developed, providing more restrictive conditions on the existence of equilibrium behavior and its fluctuation-dissipation implications.","For systems fulfilling the property of local realizability,   that for all the practical purposes corresponds to the postulate of the existence of a Markovian embedding, physical constraints, expressed in the form of dissipative stability and stochastic realizability are derived.","If these two properties are met, Kubo theory is constructively recovered, while if one of these conditions is violated a thermodynamic equilibrium behavior does not exist (and this occurs also for `` well-behaved dissipative systems'' according to the classical Kubo theory), with significant implications in the linear response theory."],"url":"http://arxiv.org/abs/2403.05431v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-08 16:30:39","title":"Towards Real-World Stickers Use: A New Dataset for Multi-Tag Sticker Recognition","abstract":"In real-world conversations, the diversity and ambiguity of stickers often lead to varied interpretations based on the context, necessitating the requirement for comprehensively understanding stickers and supporting multi-tagging. To address this challenge, we introduce StickerTAG, the first multi-tag sticker dataset comprising a collected tag set with 461 tags and 13,571 sticker-tag pairs, designed to provide a deeper understanding of stickers. Recognizing multiple tags for stickers becomes particularly challenging due to sticker tags usually are fine-grained attribute aware. Hence, we propose an Attentive Attribute-oriented Prompt Learning method, ie, Att$^2$PL, to capture informative features of stickers in a fine-grained manner to better differentiate tags. Specifically, we first apply an Attribute-oriented Description Generation (ADG) module to obtain the description for stickers from four attributes. Then, a Local Re-attention (LoR) module is designed to perceive the importance of local information. Finally, we use prompt learning to guide the recognition process and adopt confidence penalty optimization to penalize the confident output distribution. Extensive experiments show that our method achieves encouraging results for all commonly used metrics.","sentences":["In real-world conversations, the diversity and ambiguity of stickers often lead to varied interpretations based on the context, necessitating the requirement for comprehensively understanding stickers and supporting multi-tagging.","To address this challenge, we introduce StickerTAG, the first multi-tag sticker dataset comprising a collected tag set with 461 tags and 13,571 sticker-tag pairs, designed to provide a deeper understanding of stickers.","Recognizing multiple tags for stickers becomes particularly challenging due to sticker tags usually are fine-grained attribute aware.","Hence, we propose an Attentive Attribute-oriented Prompt Learning method, ie, Att$^2$PL, to capture informative features of stickers in a fine-grained manner to better differentiate tags.","Specifically, we first apply an Attribute-oriented Description Generation (ADG) module to obtain the description for stickers from four attributes.","Then, a Local Re-attention (LoR) module is designed to perceive the importance of local information.","Finally, we use prompt learning to guide the recognition process and adopt confidence penalty optimization to penalize the confident output distribution.","Extensive experiments show that our method achieves encouraging results for all commonly used metrics."],"url":"http://arxiv.org/abs/2403.05428v1","category":"cs.MM"}
{"created":"2024-03-08 16:19:33","title":"Accelerating Airy tensor modes of cosmological gravitational waves","abstract":"From a classical analysis, it is shown that the nondiffractive accelerating gravitational Airy wave packets are solutions of Einstein equations for their linearized tensor modes in a Friedmann-Lema\\^itre-Robertson-Walker cosmological background filled with a perfect fluid, with equations of state $w=1/3$ and $w=-1/3$. These solutions have finite energy, presenting accelerating behavior due to the structured spatial form of the wavepacket. This is manifested by curved trajectories along the wave path. Also, using spectral functions, it is possible, with these packets, to construct more general, arbitrary wave packets. All these new solutions bring insights on new forms for gravitational wave propagation.","sentences":["From a classical analysis, it is shown that the nondiffractive accelerating gravitational Airy wave packets are solutions of Einstein equations for their linearized tensor modes in a Friedmann-Lema\\^itre-Robertson-Walker cosmological background filled with a perfect fluid, with equations of state $w=1/3$ and $w=-1/3$. These solutions have finite energy, presenting accelerating behavior due to the structured spatial form of the wavepacket.","This is manifested by curved trajectories along the wave path.","Also, using spectral functions, it is possible, with these packets, to construct more general, arbitrary wave packets.","All these new solutions bring insights on new forms for gravitational wave propagation."],"url":"http://arxiv.org/abs/2403.05421v1","category":"gr-qc"}
{"created":"2024-03-08 16:18:52","title":"Exponential asymptotics and Stokes surfaces in nonlinear three-dimensional flows","abstract":"In this dissertation, we seek to expand the scope of work done by Lustri and Chapman (2013) in modelling 3D flow past a point source, in order to account for more general flows, where the strength of such a source, $\\delta$, is now $O(1)$. We find that in order to solve for the Stokes surfaces of this nonlinear system, we must develop a numerical scheme to shoot from the analytically continued free surface into real space. The principal contribution of this work is a demonstration of the disparity between the line of intersection found by Lustri and Chapman (2013) and that found by our own nonlinear numerical method, along with proposed extensions for further avenues of research e.g. including surface tension.","sentences":["In this dissertation, we seek to expand the scope of work done by Lustri and Chapman (2013) in modelling 3D flow past a point source, in order to account for more general flows, where the strength of such a source, $\\delta$, is now $O(1)$. We find that in order to solve for the Stokes surfaces of this nonlinear system, we must develop a numerical scheme to shoot from the analytically continued free surface into real space.","The principal contribution of this work is a demonstration of the disparity between the line of intersection found by Lustri and Chapman (2013) and that found by our own nonlinear numerical method, along with proposed extensions for further avenues of research e.g. including surface tension."],"url":"http://arxiv.org/abs/2403.05420v1","category":"physics.flu-dyn"}
{"created":"2024-03-08 16:14:54","title":"SIRST-5K: Exploring Massive Negatives Synthesis with Self-supervised Learning for Robust Infrared Small Target Detection","abstract":"Single-frame infrared small target (SIRST) detection aims to recognize small targets from clutter backgrounds. Recently, convolutional neural networks have achieved significant advantages in general object detection. With the development of Transformer, the scale of SIRST models is constantly increasing. Due to the limited training samples, performance has not been improved accordingly. The quality, quantity, and diversity of the infrared dataset are critical to the detection of small targets. To highlight this issue, we propose a negative sample augmentation method in this paper. Specifically, a negative augmentation approach is proposed to generate massive negatives for self-supervised learning. Firstly, we perform a sequential noise modeling technology to generate realistic infrared data. Secondly, we fuse the extracted noise with the original data to facilitate diversity and fidelity in the generated data. Lastly, we proposed a negative augmentation strategy to enrich diversity as well as maintain semantic invariance. The proposed algorithm produces a synthetic SIRST-5K dataset, which contains massive pseudo-data and corresponding labels. With a rich diversity of infrared small target data, our algorithm significantly improves the model performance and convergence speed. Compared with other state-of-the-art (SOTA) methods, our method achieves outstanding performance in terms of probability of detection (Pd), false-alarm rate (Fa), and intersection over union (IoU).","sentences":["Single-frame infrared small target (SIRST) detection aims to recognize small targets from clutter backgrounds.","Recently, convolutional neural networks have achieved significant advantages in general object detection.","With the development of Transformer, the scale of SIRST models is constantly increasing.","Due to the limited training samples, performance has not been improved accordingly.","The quality, quantity, and diversity of the infrared dataset are critical to the detection of small targets.","To highlight this issue, we propose a negative sample augmentation method in this paper.","Specifically, a negative augmentation approach is proposed to generate massive negatives for self-supervised learning.","Firstly, we perform a sequential noise modeling technology to generate realistic infrared data.","Secondly, we fuse the extracted noise with the original data to facilitate diversity and fidelity in the generated data.","Lastly, we proposed a negative augmentation strategy to enrich diversity as well as maintain semantic invariance.","The proposed algorithm produces a synthetic SIRST-5K dataset, which contains massive pseudo-data and corresponding labels.","With a rich diversity of infrared small target data, our algorithm significantly improves the model performance and convergence speed.","Compared with other state-of-the-art (SOTA) methods, our method achieves outstanding performance in terms of probability of detection (Pd), false-alarm rate (Fa), and intersection over union (IoU)."],"url":"http://arxiv.org/abs/2403.05416v1","category":"cs.CV"}
{"created":"2024-03-08 16:05:47","title":"Algorithmic Identification of Essential Exogenous Nodes for Causal Sufficiency in Brain Networks","abstract":"In the investigation of any causal mechanisms, such as the brain's causal networks, the assumption of causal sufficiency plays a critical role. Notably, neglecting this assumption can result in significant errors, a fact that is often disregarded in the causal analysis of brain networks. In this study, we propose an algorithmic identification approach for determining essential exogenous nodes that satisfy the critical need for causal sufficiency to adhere to it in such inquiries. Our approach consists of three main steps: First, by capturing the essence of the Peter-Clark (PC) algorithm, we conduct independence tests for pairs of regions within a network, as well as for the same pairs conditioned on nodes from other networks. Next, we distinguish candidate confounders by analyzing the differences between the conditional and unconditional results, using the Kolmogorov-Smirnov test. Subsequently, we utilize Non-Factorized identifiable Variational Autoencoders (NF-iVAE) along with the Correlation Coefficient index (CCI) metric to identify the confounding variables within these candidate nodes. Applying our method to the Human Connectome Projects (HCP) movie-watching task data, we demonstrate that while interactions exist between dorsal and ventral regions, only dorsal regions serve as confounders for the visual networks, and vice versa. These findings align consistently with those resulting from the neuroscientific perspective. Finally, we show the reliability of our results by testing 30 independent runs for NF-iVAE initialization.","sentences":["In the investigation of any causal mechanisms, such as the brain's causal networks, the assumption of causal sufficiency plays a critical role.","Notably, neglecting this assumption can result in significant errors, a fact that is often disregarded in the causal analysis of brain networks.","In this study, we propose an algorithmic identification approach for determining essential exogenous nodes that satisfy the critical need for causal sufficiency to adhere to it in such inquiries.","Our approach consists of three main steps:","First, by capturing the essence of the Peter-Clark (PC) algorithm, we conduct independence tests for pairs of regions within a network, as well as for the same pairs conditioned on nodes from other networks.","Next, we distinguish candidate confounders by analyzing the differences between the conditional and unconditional results, using the Kolmogorov-Smirnov test.","Subsequently, we utilize Non-Factorized identifiable Variational Autoencoders (NF-iVAE) along with the Correlation Coefficient index (CCI) metric to identify the confounding variables within these candidate nodes.","Applying our method to the Human Connectome Projects (HCP) movie-watching task data, we demonstrate that while interactions exist between dorsal and ventral regions, only dorsal regions serve as confounders for the visual networks, and vice versa.","These findings align consistently with those resulting from the neuroscientific perspective.","Finally, we show the reliability of our results by testing 30 independent runs for NF-iVAE initialization."],"url":"http://arxiv.org/abs/2403.05407v1","category":"cs.AI"}
{"created":"2024-03-08 16:04:36","title":"Considering Nonstationary within Multivariate Time Series with Variational Hierarchical Transformer for Forecasting","abstract":"The forecasting of Multivariate Time Series (MTS) has long been an important but challenging task. Due to the non-stationary problem across long-distance time steps, previous studies primarily adopt stationarization method to attenuate the non-stationary problem of the original series for better predictability. However, existing methods always adopt the stationarized series, which ignores the inherent non-stationarity, and has difficulty in modeling MTS with complex distributions due to the lack of stochasticity. To tackle these problems, we first develop a powerful hierarchical probabilistic generative module to consider the non-stationarity and stochastic characteristics within MTS, and then combine it with transformer for a well-defined variational generative dynamic model named Hierarchical Time series Variational Transformer (HTV-Trans), which recovers the intrinsic non-stationary information into temporal dependencies. Being a powerful probabilistic model, HTV-Trans is utilized to learn expressive representations of MTS and applied to forecasting tasks. Extensive experiments on diverse datasets show the efficiency of HTV-Trans on MTS forecasting tasks","sentences":["The forecasting of Multivariate Time Series (MTS) has long been an important but challenging task.","Due to the non-stationary problem across long-distance time steps, previous studies primarily adopt stationarization method to attenuate the non-stationary problem of the original series for better predictability.","However, existing methods always adopt the stationarized series, which ignores the inherent non-stationarity, and has difficulty in modeling MTS with complex distributions due to the lack of stochasticity.","To tackle these problems, we first develop a powerful hierarchical probabilistic generative module to consider the non-stationarity and stochastic characteristics within MTS, and then combine it with transformer for a well-defined variational generative dynamic model named Hierarchical Time series Variational Transformer (HTV-Trans), which recovers the intrinsic non-stationary information into temporal dependencies.","Being a powerful probabilistic model, HTV-Trans is utilized to learn expressive representations of MTS and applied to forecasting tasks.","Extensive experiments on diverse datasets show the efficiency of HTV-Trans on MTS forecasting tasks"],"url":"http://arxiv.org/abs/2403.05406v1","category":"cs.LG"}
{"created":"2024-03-08 16:01:46","title":"Inclusive hadronic decay rate of the $\u03c4$ lepton from lattice QCD: the $\\bar u s$ flavour channel and the Cabibbo angle","abstract":"We present a lattice determination of the inclusive decay rate of the process $\\tau\\mapsto X_{us} \\nu_\\tau$ in which the $\\tau$ lepton decays into a generic hadronic state $X_{us}$ with $\\bar u s$ flavour quantum numbers. Our results have been obtained in $n_f=2+1+1$ iso-symmetric QCD with full non-perturbative accuracy, without any OPE approximation and, except for the presently missing long-distance isospin-breaking corrections, include a solid estimate of all sources of theoretical uncertainties. This has been possible by using the Hansen-Lupo-Tantalo method [1] that we have already successfully applied in [2] to compute the inclusive decay rate of the process $\\tau\\mapsto X_{ud} \\nu_\\tau$ in the $\\bar u d$ flavour channel. By combining our first-principles theoretical results with the presently-available experimental data we extract the CKM matrix element $\\vert V_{us}\\vert$, the Cabibbo angle, with a $0.9$\\% accuracy, dominated by the experimental error.","sentences":["We present a lattice determination of the inclusive decay rate of the process $\\tau\\mapsto X_{us} \\nu_\\tau$ in which the $\\tau$ lepton decays into a generic hadronic state $X_{us}$ with $\\bar u s$ flavour quantum numbers.","Our results have been obtained in $n_f=2+1+1$ iso-symmetric QCD with full non-perturbative accuracy, without any OPE approximation and, except for the presently missing long-distance isospin-breaking corrections, include a solid estimate of all sources of theoretical uncertainties.","This has been possible by using the Hansen-Lupo-Tantalo method [1] that we have already successfully applied in [2] to compute the inclusive decay rate of the process $\\tau\\mapsto X_{ud} \\nu_\\tau$ in the $\\bar u d$ flavour channel.","By combining our first-principles theoretical results with the presently-available experimental data we extract the CKM matrix element $\\vert V_{us}\\vert$, the Cabibbo angle, with a $0.9$\\% accuracy, dominated by the experimental error."],"url":"http://arxiv.org/abs/2403.05404v1","category":"hep-lat"}
{"created":"2024-03-08 15:56:23","title":"Numerical simulations of a stochastic dynamics leading to cascades and loss of regularity: applications to fluid turbulence and generation of fractional Gaussian fields","abstract":"Motivated by the modeling of the spatial structure of the velocity field of three-dimensional turbulent flows, and the phenomenology of cascade phenomena, a linear dynamics has been recently proposed able to generate high velocity gradients from a smooth-in-space forcing term. It is based on a linear Partial Differential Equation (PDE) stirred by an additive random forcing term which is delta-correlated in time. The underlying proposed deterministic mechanism corresponds to a transport in Fourier space which aims at transferring energy injected at large scales towards small scales. The key role of the random forcing is to realize these transfers in a statistically homogeneous way. Whereas at finite times and positive viscosity the solutions are smooth, a loss of regularity is observed for the statistically stationary state in the inviscid limit. We here present novel simulations, based on finite volume methods in the Fourier domain and a splitting method in time, which are more accurate than the pseudo-spectral simulations. We show that the novel algorithm is able to reproduce accurately the expected local and statistical structure of the predicted solutions. We conduct numerical simulations in one, two and three spatial dimensions, and we display the solutions both in physical and Fourier spaces. We additionally display key statistical quantities such as second-order structure functions and power spectral densities at various viscosities.","sentences":["Motivated by the modeling of the spatial structure of the velocity field of three-dimensional turbulent flows, and the phenomenology of cascade phenomena, a linear dynamics has been recently proposed able to generate high velocity gradients from a smooth-in-space forcing term.","It is based on a linear Partial Differential Equation (PDE) stirred by an additive random forcing term which is delta-correlated in time.","The underlying proposed deterministic mechanism corresponds to a transport in Fourier space which aims at transferring energy injected at large scales towards small scales.","The key role of the random forcing is to realize these transfers in a statistically homogeneous way.","Whereas at finite times and positive viscosity the solutions are smooth, a loss of regularity is observed for the statistically stationary state in the inviscid limit.","We here present novel simulations, based on finite volume methods in the Fourier domain and a splitting method in time, which are more accurate than the pseudo-spectral simulations.","We show that the novel algorithm is able to reproduce accurately the expected local and statistical structure of the predicted solutions.","We conduct numerical simulations in one, two and three spatial dimensions, and we display the solutions both in physical and Fourier spaces.","We additionally display key statistical quantities such as second-order structure functions and power spectral densities at various viscosities."],"url":"http://arxiv.org/abs/2403.05401v1","category":"physics.flu-dyn"}
{"created":"2024-03-08 15:51:43","title":"HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction","abstract":"Histopathology serves as the gold standard in cancer diagnosis, with clinical reports being vital in interpreting and understanding this process, guiding cancer treatment and patient care. The automation of histopathology report generation with deep learning stands to significantly enhance clinical efficiency and lessen the labor-intensive, time-consuming burden on pathologists in report writing. In pursuit of this advancement, we introduce HistGen, a multiple instance learning-empowered framework for histopathology report generation together with the first benchmark dataset for evaluation. Inspired by diagnostic and report-writing workflows, HistGen features two delicately designed modules, aiming to boost report generation by aligning whole slide images (WSIs) and diagnostic reports from local and global granularity. To achieve this, a local-global hierarchical encoder is developed for efficient visual feature aggregation from a region-to-slide perspective. Meanwhile, a cross-modal context module is proposed to explicitly facilitate alignment and interaction between distinct modalities, effectively bridging the gap between the extensive visual sequences of WSIs and corresponding highly summarized reports. Experimental results on WSI report generation show the proposed model outperforms state-of-the-art (SOTA) models by a large margin. Moreover, the results of fine-tuning our model on cancer subtyping and survival analysis tasks further demonstrate superior performance compared to SOTA methods, showcasing strong transfer learning capability. Dataset, model weights, and source code are available in https://github.com/dddavid4real/HistGen.","sentences":["Histopathology serves as the gold standard in cancer diagnosis, with clinical reports being vital in interpreting and understanding this process, guiding cancer treatment and patient care.","The automation of histopathology report generation with deep learning stands to significantly enhance clinical efficiency and lessen the labor-intensive, time-consuming burden on pathologists in report writing.","In pursuit of this advancement, we introduce HistGen, a multiple instance learning-empowered framework for histopathology report generation together with the first benchmark dataset for evaluation.","Inspired by diagnostic and report-writing workflows, HistGen features two delicately designed modules, aiming to boost report generation by aligning whole slide images (WSIs) and diagnostic reports from local and global granularity.","To achieve this, a local-global hierarchical encoder is developed for efficient visual feature aggregation from a region-to-slide perspective.","Meanwhile, a cross-modal context module is proposed to explicitly facilitate alignment and interaction between distinct modalities, effectively bridging the gap between the extensive visual sequences of WSIs and corresponding highly summarized reports.","Experimental results on WSI report generation show the proposed model outperforms state-of-the-art (SOTA) models by a large margin.","Moreover, the results of fine-tuning our model on cancer subtyping and survival analysis tasks further demonstrate superior performance compared to SOTA methods, showcasing strong transfer learning capability.","Dataset, model weights, and source code are available in https://github.com/dddavid4real/HistGen."],"url":"http://arxiv.org/abs/2403.05396v1","category":"cs.CV"}
{"created":"2024-03-08 15:45:13","title":"Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems trained with Gradient Descent","abstract":"Advanced machine learning methods, and more prominently neural networks, have become standard to solve inverse problems over the last years. However, the theoretical recovery guarantees of such methods are still scarce and difficult to achieve. Only recently did unsupervised methods such as Deep Image Prior (DIP) get equipped with convergence and recovery guarantees for generic loss functions when trained through gradient flow with an appropriate initialization. In this paper, we extend these results by proving that these guarantees hold true when using gradient descent with an appropriately chosen step-size/learning rate. We also show that the discretization only affects the overparametrization bound for a two-layer DIP network by a constant and thus that the different guarantees found for the gradient flow will hold for gradient descent.","sentences":["Advanced machine learning methods, and more prominently neural networks, have become standard to solve inverse problems over the last years.","However, the theoretical recovery guarantees of such methods are still scarce and difficult to achieve.","Only recently did unsupervised methods such as Deep Image Prior (DIP) get equipped with convergence and recovery guarantees for generic loss functions when trained through gradient flow with an appropriate initialization.","In this paper, we extend these results by proving that these guarantees hold true when using gradient descent with an appropriately chosen step-size/learning rate.","We also show that the discretization only affects the overparametrization bound for a two-layer DIP network by a constant and thus that the different guarantees found for the gradient flow will hold for gradient descent."],"url":"http://arxiv.org/abs/2403.05395v1","category":"cs.LG"}
{"created":"2024-03-08 15:45:07","title":"A Deep Learning Method for Classification of Biophilic Artworks","abstract":"Biophilia is an innate love for living things and nature itself that has been associated with a positive impact on mental health and well-being. This study explores the application of deep learning methods for the classification of Biophilic artwork, in order to learn and explain the different Biophilic characteristics present in a visual representation of a painting. Using the concept of Biophilia that postulates the deep connection of human beings with nature, we use an artificially intelligent algorithm to recognise the different patterns underlying the Biophilic features in an artwork. Our proposed method uses a lower-dimensional representation of an image and a decoder model to extract salient features of the image of each Biophilic trait, such as plants, water bodies, seasons, animals, etc., based on learnt factors such as shape, texture, and illumination. The proposed classification model is capable of extracting Biophilic artwork that not only helps artists, collectors, and researchers studying to interpret and exploit the effects of mental well-being on exposure to nature-inspired visual aesthetics but also enables a methodical exploration of the study of Biophilia and Biophilic artwork for aesthetic preferences. Using the proposed algorithms, we have also created a gallery of Biophilic collections comprising famous artworks from different European and American art galleries, which will soon be published on the Vieunite@ online community.","sentences":["Biophilia is an innate love for living things and nature itself that has been associated with a positive impact on mental health and well-being.","This study explores the application of deep learning methods for the classification of Biophilic artwork, in order to learn and explain the different Biophilic characteristics present in a visual representation of a painting.","Using the concept of Biophilia that postulates the deep connection of human beings with nature, we use an artificially intelligent algorithm to recognise the different patterns underlying the Biophilic features in an artwork.","Our proposed method uses a lower-dimensional representation of an image and a decoder model to extract salient features of the image of each Biophilic trait, such as plants, water bodies, seasons, animals, etc., based on learnt factors such as shape, texture, and illumination.","The proposed classification model is capable of extracting Biophilic artwork that not only helps artists, collectors, and researchers studying to interpret and exploit the effects of mental well-being on exposure to nature-inspired visual aesthetics but also enables a methodical exploration of the study of Biophilia and Biophilic artwork for aesthetic preferences.","Using the proposed algorithms, we have also created a gallery of Biophilic collections comprising famous artworks from different European and American art galleries, which will soon be published on the Vieunite@ online community."],"url":"http://arxiv.org/abs/2403.05394v1","category":"cs.CV"}
{"created":"2024-03-08 15:42:13","title":"Binaural Speech Enhancement Using Deep Complex Convolutional Transformer Networks","abstract":"Studies have shown that in noisy acoustic environments, providing binaural signals to the user of an assistive listening device may improve speech intelligibility and spatial awareness. This paper presents a binaural speech enhancement method using a complex convolutional neural network with an encoder-decoder architecture and a complex multi-head attention transformer. The model is trained to estimate individual complex ratio masks in the time-frequency domain for the left and right-ear channels of binaural hearing devices. The model is trained using a novel loss function that incorporates the preservation of spatial information along with speech intelligibility improvement and noise reduction. Simulation results for acoustic scenarios with a single target speaker and isotropic noise of various types show that the proposed method improves the estimated binaural speech intelligibility and preserves the binaural cues better in comparison with several baseline algorithms.","sentences":["Studies have shown that in noisy acoustic environments, providing binaural signals to the user of an assistive listening device may improve speech intelligibility and spatial awareness.","This paper presents a binaural speech enhancement method using a complex convolutional neural network with an encoder-decoder architecture and a complex multi-head attention transformer.","The model is trained to estimate individual complex ratio masks in the time-frequency domain for the left and right-ear channels of binaural hearing devices.","The model is trained using a novel loss function that incorporates the preservation of spatial information along with speech intelligibility improvement and noise reduction.","Simulation results for acoustic scenarios with a single target speaker and isotropic noise of various types show that the proposed method improves the estimated binaural speech intelligibility and preserves the binaural cues better in comparison with several baseline algorithms."],"url":"http://arxiv.org/abs/2403.05393v1","category":"eess.AS"}
{"created":"2024-03-08 15:35:35","title":"The minimal cosmological standard model","abstract":"We propose a novel minimal scenario which simultaneously addresses the following theoretical/cosmological/phenomenological puzzles: (i) the origin of scales, (ii) primordial inflation, (iii) matter-antimatter asymmetry, (iv) tiny neutrino masses, (v) dark matter, and (vi) the strong CP-problem. Exact scale-symmetry was assumed. A global $U(1)_{\\rm PQ}$-symmetry was also assumed but only in the matter sector. The novelty of the scenario is the introduction of an explicit $U(1)_{\\rm PQ}$-breaking term in the gravity sector only. Such a term does not disturb the axion solution whereas naturally realizes an axi-majoron hybrid inflation which allows a natural realization of Affleck-Dine mechanism for generating Peccei-Quinn number asymmetry. The asymmetry is transferred to the visible sector via the right-handed neutrino portal through non-thermal decay and/or thermal processes, even without the presence of a CP-violating phase in the matter sector. Dark matter and dark radiation are obtained by cold and hot components of axi-majorons, respectively.","sentences":["We propose a novel minimal scenario which simultaneously addresses the following theoretical/cosmological/phenomenological puzzles: (i) the origin of scales, (ii) primordial inflation, (iii) matter-antimatter asymmetry, (iv) tiny neutrino masses, (v) dark matter, and (vi) the strong CP-problem.","Exact scale-symmetry was assumed.","A global $U(1)_{\\rm PQ}$-symmetry was also assumed but only in the matter sector.","The novelty of the scenario is the introduction of an explicit $U(1)_{\\rm PQ}$-breaking term in the gravity sector only.","Such a term does not disturb the axion solution whereas naturally realizes an axi-majoron hybrid inflation which allows a natural realization of Affleck-Dine mechanism for generating Peccei-Quinn number asymmetry.","The asymmetry is transferred to the visible sector via the right-handed neutrino portal through non-thermal decay and/or thermal processes, even without the presence of a CP-violating phase in the matter sector.","Dark matter and dark radiation are obtained by cold and hot components of axi-majorons, respectively."],"url":"http://arxiv.org/abs/2403.05390v1","category":"hep-ph"}
{"created":"2024-03-08 15:35:26","title":"Multi-reference coupled cluster theory using the normal ordered exponential ansatz","abstract":"Properly spin-adapted coupled-cluster theory for general open-shell configurations remains an elusive goal in electronic structure theory. In this contribution we examine Lindgren's normal-ordered exponential ansatz using spin-free excitation operators, with the aid of automatic equation generation software. We present a size-extensive reformulation of the unlinked working equations, and analyse the performance of the method with single and double excitations for simple molecular systems in terms of accuracy and size consistency.","sentences":["Properly spin-adapted coupled-cluster theory for general open-shell configurations remains an elusive goal in electronic structure theory.","In this contribution we examine Lindgren's normal-ordered exponential ansatz using spin-free excitation operators, with the aid of automatic equation generation software.","We present a size-extensive reformulation of the unlinked working equations, and analyse the performance of the method with single and double excitations for simple molecular systems in terms of accuracy and size consistency."],"url":"http://arxiv.org/abs/2403.05389v1","category":"physics.chem-ph"}
{"created":"2024-03-08 15:32:18","title":"Generalized Correspondence Matching via Flexible Hierarchical Refinement and Patch Descriptor Distillation","abstract":"Correspondence matching plays a crucial role in numerous robotics applications. In comparison to conventional hand-crafted methods and recent data-driven approaches, there is significant interest in plug-and-play algorithms that make full use of pre-trained backbone networks for multi-scale feature extraction and leverage hierarchical refinement strategies to generate matched correspondences. The primary focus of this paper is to address the limitations of deep feature matching (DFM), a state-of-the-art (SoTA) plug-and-play correspondence matching approach. First, we eliminate the pre-defined threshold employed in the hierarchical refinement process of DFM by leveraging a more flexible nearest neighbor search strategy, thereby preventing the exclusion of repetitive yet valid matches during the early stages. Our second technical contribution is the integration of a patch descriptor, which extends the applicability of DFM to accommodate a wide range of backbone networks pre-trained across diverse computer vision tasks, including image classification, semantic segmentation, and stereo matching. Taking into account the practical applicability of our method in real-world robotics applications, we also propose a novel patch descriptor distillation strategy to further reduce the computational complexity of correspondence matching. Extensive experiments conducted on three public datasets demonstrate the superior performance of our proposed method. Specifically, it achieves an overall performance in terms of mean matching accuracy of 0.68, 0.92, and 0.95 with respect to the tolerances of 1, 3, and 5 pixels, respectively, on the HPatches dataset, outperforming all other SoTA algorithms. Our source code, demo video, and supplement are publicly available at mias.group/GCM.","sentences":["Correspondence matching plays a crucial role in numerous robotics applications.","In comparison to conventional hand-crafted methods and recent data-driven approaches, there is significant interest in plug-and-play algorithms that make full use of pre-trained backbone networks for multi-scale feature extraction and leverage hierarchical refinement strategies to generate matched correspondences.","The primary focus of this paper is to address the limitations of deep feature matching (DFM), a state-of-the-art (SoTA) plug-and-play correspondence matching approach.","First, we eliminate the pre-defined threshold employed in the hierarchical refinement process of DFM by leveraging a more flexible nearest neighbor search strategy, thereby preventing the exclusion of repetitive yet valid matches during the early stages.","Our second technical contribution is the integration of a patch descriptor, which extends the applicability of DFM to accommodate a wide range of backbone networks pre-trained across diverse computer vision tasks, including image classification, semantic segmentation, and stereo matching.","Taking into account the practical applicability of our method in real-world robotics applications, we also propose a novel patch descriptor distillation strategy to further reduce the computational complexity of correspondence matching.","Extensive experiments conducted on three public datasets demonstrate the superior performance of our proposed method.","Specifically, it achieves an overall performance in terms of mean matching accuracy of 0.68, 0.92, and 0.95 with respect to the tolerances of 1, 3, and 5 pixels, respectively, on the HPatches dataset, outperforming all other SoTA algorithms.","Our source code, demo video, and supplement are publicly available at mias.group/GCM."],"url":"http://arxiv.org/abs/2403.05388v1","category":"cs.CV"}
{"created":"2024-03-08 15:30:58","title":"Switching the Loss Reduces the Cost in Batch Reinforcement Learning","abstract":"We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch reinforcement learning (RL). We show that the number of samples needed to learn a near-optimal policy with FQI-LOG scales with the accumulated cost of the optimal policy, which is zero in problems where acting optimally achieves the goal and incurs no cost. In doing so, we provide a general framework for proving $\\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal achievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses fewer samples than FQI trained with squared loss on problems where the optimal policy reliably achieves the goal.","sentences":["We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch reinforcement learning (RL).","We show that the number of samples needed to learn a near-optimal policy with FQI-LOG scales with the accumulated cost of the optimal policy, which is zero in problems where acting optimally achieves the goal and incurs no cost.","In doing so, we provide a general framework for proving $\\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal achievable cost, in batch RL.","Moreover, we empirically verify that FQI-LOG uses fewer samples than FQI trained with squared loss on problems where the optimal policy reliably achieves the goal."],"url":"http://arxiv.org/abs/2403.05385v1","category":"cs.LG"}
{"created":"2024-03-08 15:26:27","title":"A Data Augmentation Pipeline to Generate Synthetic Labeled Datasets of 3D Echocardiography Images using a GAN","abstract":"Due to privacy issues and limited amount of publicly available labeled datasets in the domain of medical imaging, we propose an image generation pipeline to synthesize 3D echocardiographic images with corresponding ground truth labels, to alleviate the need for data collection and for laborious and error-prone human labeling of images for subsequent Deep Learning (DL) tasks. The proposed method utilizes detailed anatomical segmentations of the heart as ground truth label sources. This initial dataset is combined with a second dataset made up of real 3D echocardiographic images to train a Generative Adversarial Network (GAN) to synthesize realistic 3D cardiovascular Ultrasound images paired with ground truth labels. To generate the synthetic 3D dataset, the trained GAN uses high resolution anatomical models from Computed Tomography (CT) as input. A qualitative analysis of the synthesized images showed that the main structures of the heart are well delineated and closely follow the labels obtained from the anatomical models. To assess the usability of these synthetic images for DL tasks, segmentation algorithms were trained to delineate the left ventricle, left atrium, and myocardium. A quantitative analysis of the 3D segmentations given by the models trained with the synthetic images indicated the potential use of this GAN approach to generate 3D synthetic data, use the data to train DL models for different clinical tasks, and therefore tackle the problem of scarcity of 3D labeled echocardiography datasets.","sentences":["Due to privacy issues and limited amount of publicly available labeled datasets in the domain of medical imaging, we propose an image generation pipeline to synthesize 3D echocardiographic images with corresponding ground truth labels, to alleviate the need for data collection and for laborious and error-prone human labeling of images for subsequent Deep Learning (DL) tasks.","The proposed method utilizes detailed anatomical segmentations of the heart as ground truth label sources.","This initial dataset is combined with a second dataset made up of real 3D echocardiographic images to train a Generative Adversarial Network (GAN) to synthesize realistic 3D cardiovascular Ultrasound images paired with ground truth labels.","To generate the synthetic 3D dataset, the trained GAN uses high resolution anatomical models from Computed Tomography (CT) as input.","A qualitative analysis of the synthesized images showed that the main structures of the heart are well delineated and closely follow the labels obtained from the anatomical models.","To assess the usability of these synthetic images for DL tasks, segmentation algorithms were trained to delineate the left ventricle, left atrium, and myocardium.","A quantitative analysis of the 3D segmentations given by the models trained with the synthetic images indicated the potential use of this GAN approach to generate 3D synthetic data, use the data to train DL models for different clinical tasks, and therefore tackle the problem of scarcity of 3D labeled echocardiography datasets."],"url":"http://arxiv.org/abs/2403.05384v1","category":"eess.IV"}
{"created":"2024-03-08 15:16:15","title":"Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia Classification","abstract":"Automated disease diagnosis using medical image analysis relies on deep learning, often requiring large labeled datasets for supervised model training. Diseases like Acute Myeloid Leukemia (AML) pose challenges due to scarce and costly annotations on a single-cell level. Multiple Instance Learning (MIL) addresses weakly labeled scenarios but necessitates powerful encoders typically trained with labeled data. In this study, we explore Self-Supervised Learning (SSL) as a pre-training approach for MIL-based AML subtype classification from blood smears, removing the need for labeled data during encoder training. We investigate the three state-of-the-art SSL methods SimCLR, SwAV, and DINO, and compare their performance against supervised pre-training. Our findings show that SSL-pretrained encoders achieve comparable performance, showcasing the potential of SSL in MIL. This breakthrough offers a cost-effective and data-efficient solution, propelling the field of AI-based disease diagnosis.","sentences":["Automated disease diagnosis using medical image analysis relies on deep learning, often requiring large labeled datasets for supervised model training.","Diseases like Acute Myeloid Leukemia (AML) pose challenges due to scarce and costly annotations on a single-cell level.","Multiple Instance Learning (MIL) addresses weakly labeled scenarios but necessitates powerful encoders typically trained with labeled data.","In this study, we explore Self-Supervised Learning (SSL) as a pre-training approach for MIL-based AML subtype classification from blood smears, removing the need for labeled data during encoder training.","We investigate the three state-of-the-art SSL methods SimCLR, SwAV, and DINO, and compare their performance against supervised pre-training.","Our findings show that SSL-pretrained encoders achieve comparable performance, showcasing the potential of SSL in MIL.","This breakthrough offers a cost-effective and data-efficient solution, propelling the field of AI-based disease diagnosis."],"url":"http://arxiv.org/abs/2403.05379v1","category":"cs.CV"}
{"created":"2024-03-08 15:11:22","title":"Online Contention Resolution Schemes for Network Revenue Management and Combinatorial Auctions","abstract":"In the Network Revenue Management (NRM) problem, products composed of up to L resources are sold to stochastically arriving customers. We take a randomized rounding approach to NRM, motivated by developments in Online Contention Resolution Schemes (OCRS). The goal is to take a fractional solution to NRM that satisfies the resource constraints in expectation, and implement it in an online policy that satisfies the resource constraints in any state, while (approximately) preserving all of the sales that were prescribed by the fractional solution.   OCRS cannot be naively applied to NRM or revenue management problems in general, because customer substitution induces a negative correlation in products being demanded. We start by deriving an OCRS that achieves a guarantee of 1/(1+L) for NRM with customer substitution, matching a common benchmark in the literature. We then show how to beat this benchmark for all integers L>1 assuming no substitution, i.e., in the standard OCRS setting. By contrast, we show that this benchmark is unbeatable using OCRS or any fractional relaxation if there is customer substitution, for all integers L that are the power of a prime number. Finally, we show how to beat 1/(1+L) even with customer substitution, if the products comprise one item from each of up to L groups.   Our results have corresponding implications for Online Combinatorial Auctions, in which buyers bid for bundles of up to L items, and buyers being single-minded is akin to no substitution. Our final result also beats 1/(1+L) for Prophet Inequality on the intersection of L partition matroids. All in all, our paper provides a unifying framework for applying OCRS to these problems, delineating the impact of substitution, and establishing a separation between the guarantees achievable with vs. without substitution under general resource constraints parametrized by L.","sentences":["In the Network Revenue Management (NRM) problem, products composed of up to L resources are sold to stochastically arriving customers.","We take a randomized rounding approach to NRM, motivated by developments in Online Contention Resolution Schemes (OCRS).","The goal is to take a fractional solution to NRM that satisfies the resource constraints in expectation, and implement it in an online policy that satisfies the resource constraints in any state, while (approximately) preserving all of the sales that were prescribed by the fractional solution.   ","OCRS cannot be naively applied to NRM or revenue management problems in general, because customer substitution induces a negative correlation in products being demanded.","We start by deriving an OCRS that achieves a guarantee of 1/(1+L) for NRM with customer substitution, matching a common benchmark in the literature.","We then show how to beat this benchmark for all integers L>1 assuming no substitution, i.e., in the standard OCRS setting.","By contrast, we show that this benchmark is unbeatable using OCRS or any fractional relaxation if there is customer substitution, for all integers L that are the power of a prime number.","Finally, we show how to beat 1/(1+L) even with customer substitution, if the products comprise one item from each of up to L groups.   ","Our results have corresponding implications for Online Combinatorial Auctions, in which buyers bid for bundles of up to L items, and buyers being single-minded is akin to no substitution.","Our final result also beats 1/(1+L) for Prophet Inequality on the intersection of L partition matroids.","All in all, our paper provides a unifying framework for applying OCRS to these problems, delineating the impact of substitution, and establishing a separation between the guarantees achievable with vs. without substitution under general resource constraints parametrized by L."],"url":"http://arxiv.org/abs/2403.05378v1","category":"cs.GT"}
{"created":"2024-03-08 15:08:29","title":"Intrinsic mirror symmetry and Frobenius structure theorem via Gromov-Witten theory of root stacks","abstract":"Using recent results of Battistella, Nabijou, Ranganathan and the author, we compare candidate mirror algebras associated with certain log Calabi-Yau pairs constructed by Gross-Siebert using log Gromov-Witten theory and Tseng-You using orbifold Gromov- Witten theory of root stacks. Although the structure constants used to defined these mirror algebras do not typically agree, we show that any given structure constant involved in the construction the algebra of Gross and Siebert can be computed in terms of structure constants of the algebra of Tseng and You after a sequence of log blowups. Using this relation, we provide another proof of associativity of the log mirror algebra, and a proof of the weak Frobenius Structure Theorem in full generality. Along the way, we introduce a class of twisted punctured Gromov-Witten invariants of generalized root stacks induced by log \\'etale modifications, and use this to study the behavior of log Gromov-Witten invariants under ramified base change.","sentences":["Using recent results of Battistella, Nabijou, Ranganathan and the author, we compare candidate mirror algebras associated with certain log Calabi-Yau pairs constructed by Gross-Siebert using log Gromov-Witten theory and Tseng-You using orbifold Gromov- Witten theory of root stacks.","Although the structure constants used to defined these mirror algebras do not typically agree, we show that any given structure constant involved in the construction the algebra of Gross and Siebert can be computed in terms of structure constants of the algebra of Tseng and You after a sequence of log blowups.","Using this relation, we provide another proof of associativity of the log mirror algebra, and a proof of the weak Frobenius Structure Theorem in full generality.","Along the way, we introduce a class of twisted punctured Gromov-Witten invariants of generalized root stacks induced by log \\'etale modifications, and use this to study the behavior of log Gromov-Witten invariants under ramified base change."],"url":"http://arxiv.org/abs/2403.05376v1","category":"math.AG"}
{"created":"2024-03-08 15:08:24","title":"Multiple correlations of spectra for higher rank Anosov representations","abstract":"We describe multiple correlations of Jordan and Cartan spectra for any finite number of Anosov representations of a finitely generated group. This extends our previous work on correlations of length and displacement spectra for rank one convex cocompact representations. Examples include correlations of the Hilbert length spectra for convex projective structures on a closed surface as well as correlations of eigenvalue gaps and singular value gaps for Hitchin representations. We relate the correlation problem to the counting problem for Jordan and Cartan projections of an Anosov subgroup with respect to a family of carefully chosen truncated {\\it hypertubes}, rather than in tubes as in our previous work. Hypertubes go to infinity in a linear subspace of directions, while tubes go to infinity in a single direction and this feature presents a novel difficulty in this higher rank correlation problem.","sentences":["We describe multiple correlations of Jordan and Cartan spectra for any finite number of Anosov representations of a finitely generated group.","This extends our previous work on correlations of length and displacement spectra for rank one convex cocompact representations.","Examples include correlations of the Hilbert length spectra for convex projective structures on a closed surface as well as correlations of eigenvalue gaps and singular value gaps for Hitchin representations.","We relate the correlation problem to the counting problem for Jordan and Cartan projections of an Anosov subgroup with respect to a family of carefully chosen truncated {\\it hypertubes}, rather than in tubes as in our previous work.","Hypertubes go to infinity in a linear subspace of directions, while tubes go to infinity in a single direction and this feature presents a novel difficulty in this higher rank correlation problem."],"url":"http://arxiv.org/abs/2403.05375v1","category":"math.DS"}
{"created":"2024-03-08 14:59:15","title":"Exploring the Links between the Fundamental Lemma and Kernel Regression","abstract":"Generalizations and variations of the fundamental lemma by Willems et al. are an active topic of recent research. In this note, we explore and formalize the links between kernel regression and known nonlinear extensions of the fundamental lemma. Applying a transformation to the usual linear equation in Hankel matrices, we arrive at an alternative implicit kernel representation of the system trajectories while keeping the requirements on persistency of excitation. We show that this representation is equivalent to the solution of a specific kernel regression problem. We explore the possible structures of the underlying kernel as well as the system classes to which they correspond.","sentences":["Generalizations and variations of the fundamental lemma by Willems et al. are an active topic of recent research.","In this note, we explore and formalize the links between kernel regression and known nonlinear extensions of the fundamental lemma.","Applying a transformation to the usual linear equation in Hankel matrices, we arrive at an alternative implicit kernel representation of the system trajectories while keeping the requirements on persistency of excitation.","We show that this representation is equivalent to the solution of a specific kernel regression problem.","We explore the possible structures of the underlying kernel as well as the system classes to which they correspond."],"url":"http://arxiv.org/abs/2403.05368v1","category":"eess.SY"}
{"created":"2024-03-08 14:47:52","title":"Onset of Spin Entanglement in Doped Carbon Nanotubes Studied by EPR","abstract":"Nanoscale semiconductors with isolated spin impurities have been touted as promising materials for their potential use at the intersection of quantum, spin, and information technologies. Electron paramagnetic resonance (EPR) studies of spins in semiconducting carbon nanotubes have overwhelmingly focused on spins more strongly localized by $\\rm sp^3$-type lattice defects. However, the creation of such impurities is irreversible and requires specific reactions to generate them. Shallow charge impurities, on the other hand, are more readily and widely produced by simple redox chemistry, but have not yet been investigated for their spin properties. Here we use EPR to study p-doped (6,5) semiconducting single-wall carbon nanotubes (s-SWNTs) and elucidate the role of impurity-impurity interactions in conjunction with exchange and correlation effects for the spin behavior of this material. A quantitative comparison of the EPR signals with phenomenological modeling combined with configuration interaction electronic structure calculations of impurity pairs shows that orbital overlap, combined with exchange and correlation effects, causes the EPR signal to disappear due to spin entanglement for doping levels corresponding to impurity spacings of $14\\,\\rm nm$ (at 30 K). This transition is predicted to shift to higher doping levels with increasing temperature and to lower levels with increasing screening, providing an opportunity for improved spin control in doped s-SWNTs.","sentences":["Nanoscale semiconductors with isolated spin impurities have been touted as promising materials for their potential use at the intersection of quantum, spin, and information technologies.","Electron paramagnetic resonance (EPR) studies of spins in semiconducting carbon nanotubes have overwhelmingly focused on spins more strongly localized by $\\rm sp^3$-type lattice defects.","However, the creation of such impurities is irreversible and requires specific reactions to generate them.","Shallow charge impurities, on the other hand, are more readily and widely produced by simple redox chemistry, but have not yet been investigated for their spin properties.","Here we use EPR to study p-doped (6,5) semiconducting single-wall carbon nanotubes (s-SWNTs) and elucidate the role of impurity-impurity interactions in conjunction with exchange and correlation effects for the spin behavior of this material.","A quantitative comparison of the EPR signals with phenomenological modeling combined with configuration interaction electronic structure calculations of impurity pairs shows that orbital overlap, combined with exchange and correlation effects, causes the EPR signal to disappear due to spin entanglement for doping levels corresponding to impurity spacings of $14\\,\\rm nm$ (at 30 K).","This transition is predicted to shift to higher doping levels with increasing temperature and to lower levels with increasing screening, providing an opportunity for improved spin control in doped s-SWNTs."],"url":"http://arxiv.org/abs/2403.05361v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-08 14:45:35","title":"Path eccentricity of $k$-AT-free graphs and application on graphs with the consecutive ones property","abstract":"The central path problem is a variation on the single facility location problem. The aim is to find, in a given connected graph $G$, a path $P$ minimizing its eccentricity, which is the maximal distance from $P$ to any vertex of the graph $G$. The path eccentricity of $G$ is the minimal eccentricity achievable over all paths in $G$. In this article we consider the path eccentricity of the class of the $k$-AT-free graphs. They are graphs in which any set of three vertices contains a pair for which every path between them uses at least one vertex of the closed neighborhood at distance $k$ of the third. We prove that they have path eccentricity bounded by $k$.   Moreover, we answer a question of G\\'omez and Guti\\'errez asking if there is a relation between path eccentricity and the consecutive ones property. The latter is the property for a binary matrix to admit a permutation of the rows placing the 1's consecutively on the columns. It was already known that graphs whose adjacency matrices have the consecutive ones property have path eccentricity at most 1, and that the same remains true when the augmented adjacency matrices (with ones on the diagonal) has the consecutive ones property. We generalize these results as follow. We study graphs whose adjacency matrices can be made to satisfy the consecutive ones property after changing some values on the diagonal, and show that those graphs have path eccentricity at most 2, by showing that they are 2-AT-free.","sentences":["The central path problem is a variation on the single facility location problem.","The aim is to find, in a given connected graph $G$, a path $P$ minimizing its eccentricity, which is the maximal distance from $P$ to any vertex of the graph $G$. The path eccentricity of $G$ is the minimal eccentricity achievable over all paths in $G$.","In this article we consider the path eccentricity of the class of the $k$-AT-free graphs.","They are graphs in which any set of three vertices contains a pair for which every path between them uses at least one vertex of the closed neighborhood at distance $k$ of the third.","We prove that they have path eccentricity bounded by $k$.   ","Moreover, we answer a question of G\\'omez and Guti\\'errez asking if there is a relation between path eccentricity and the consecutive ones property.","The latter is the property for a binary matrix to admit a permutation of the rows placing the 1's consecutively on the columns.","It was already known that graphs whose adjacency matrices have the consecutive ones property have path eccentricity at most 1, and that the same remains true when the augmented adjacency matrices (with ones on the diagonal) has the consecutive ones property.","We generalize these results as follow.","We study graphs whose adjacency matrices can be made to satisfy the consecutive ones property after changing some values on the diagonal, and show that those graphs have path eccentricity at most 2, by showing that they are 2-AT-free."],"url":"http://arxiv.org/abs/2403.05360v1","category":"math.CO"}
{"created":"2024-03-08 14:45:18","title":"Variational Inference of Parameters in Opinion Dynamics Models","abstract":"Despite the frequent use of agent-based models (ABMs) for studying social phenomena, parameter estimation remains a challenge, often relying on costly simulation-based heuristics. This work uses variational inference to estimate the parameters of an opinion dynamics ABM, by transforming the estimation problem into an optimization task that can be solved directly.   Our proposal relies on probabilistic generative ABMs (PGABMs): we start by synthesizing a probabilistic generative model from the ABM rules. Then, we transform the inference process into an optimization problem suitable for automatic differentiation. In particular, we use the Gumbel-Softmax reparameterization for categorical agent attributes and stochastic variational inference for parameter estimation. Furthermore, we explore the trade-offs of using variational distributions with different complexity: normal distributions and normalizing flows.   We validate our method on a bounded confidence model with agent roles (leaders and followers). Our approach estimates both macroscopic (bounded confidence intervals and backfire thresholds) and microscopic ($200$ categorical, agent-level roles) more accurately than simulation-based and MCMC methods. Consequently, our technique enables experts to tune and validate their ABMs against real-world observations, thus providing insights into human behavior in social systems via data-driven analysis.","sentences":["Despite the frequent use of agent-based models (ABMs) for studying social phenomena, parameter estimation remains a challenge, often relying on costly simulation-based heuristics.","This work uses variational inference to estimate the parameters of an opinion dynamics ABM, by transforming the estimation problem into an optimization task that can be solved directly.   ","Our proposal relies on probabilistic generative ABMs (PGABMs): we start by synthesizing a probabilistic generative model from the ABM rules.","Then, we transform the inference process into an optimization problem suitable for automatic differentiation.","In particular, we use the Gumbel-Softmax reparameterization for categorical agent attributes and stochastic variational inference for parameter estimation.","Furthermore, we explore the trade-offs of using variational distributions with different complexity: normal distributions and normalizing flows.   ","We validate our method on a bounded confidence model with agent roles (leaders and followers).","Our approach estimates both macroscopic (bounded confidence intervals and backfire thresholds) and microscopic ($200$ categorical, agent-level roles) more accurately than simulation-based and MCMC methods.","Consequently, our technique enables experts to tune and validate their ABMs against real-world observations, thus providing insights into human behavior in social systems via data-driven analysis."],"url":"http://arxiv.org/abs/2403.05358v1","category":"cs.CY"}
{"created":"2024-03-08 14:32:01","title":"Enhancing Plausibility Evaluation for Generated Designs with Denoising Autoencoder","abstract":"A great interest has arisen in using Deep Generative Models (DGM) for generative design. When assessing the quality of the generated designs, human designers focus more on structural plausibility, e.g., no missing component, rather than visual artifacts, e.g., noises in the images. Meanwhile, commonly used metrics such as Fr\\'echet Inception Distance (FID) may not evaluate accurately as they tend to penalize visual artifacts instead of structural implausibility. As such, FID might not be suitable to assess the performance of DGMs for a generative design task. In this work, we propose to encode the input designs with a simple Denoising Autoencoder (DAE) and measure the distribution distance in the latent space thereof. We experimentally test our DAE-based metrics with FID and other state-of-the-art metrics on three data sets: compared to FID and some more recent works, e.g., FD$_\\text{DINO-V2}$ and topology distance, DAE-based metrics can effectively detect implausible structures and are more consistent with structural inspection by human experts.","sentences":["A great interest has arisen in using Deep Generative Models (DGM) for generative design.","When assessing the quality of the generated designs, human designers focus more on structural plausibility, e.g., no missing component, rather than visual artifacts, e.g., noises in the images.","Meanwhile, commonly used metrics such as Fr\\'echet Inception Distance (FID) may not evaluate accurately as they tend to penalize visual artifacts instead of structural implausibility.","As such, FID might not be suitable to assess the performance of DGMs for a generative design task.","In this work, we propose to encode the input designs with a simple Denoising Autoencoder (DAE) and measure the distribution distance in the latent space thereof.","We experimentally test our DAE-based metrics with FID and other state-of-the-art metrics on three data sets: compared to FID and some more recent works, e.g., FD$_\\text{DINO-V2}$ and topology distance, DAE-based metrics can effectively detect implausible structures and are more consistent with structural inspection by human experts."],"url":"http://arxiv.org/abs/2403.05352v1","category":"cs.CV"}
{"created":"2024-03-08 14:24:54","title":"Attempting to Prove the Riemann Hypothesis through the Reflection Formula","abstract":"The Riemann Hypothesis, originally proposed by the eminent mathematician Bernard Riemann in 1859, remains one of the most profound challenges in number theory. It posits that all non-trivial zeros of the Riemann zeta function {\\zeta}(s) are concentrated precisely along the critical line where the real part equals 1/2. In this paper, our aim is to present an attempt to prove this conjecture. Our approach relies on the use of the reflection formula. By applying this tool with precision and insight, we can conclusively establish that {\\xi}(s)^2 (Riemann's {\\xi}-function) is valid only when Re(s)=1/2. As a direct consequence of this determination, we can assert that every zero of both {\\xi}(s)^2 and {\\xi}(s) has a real part equal to 1/2. This, in turn, leads us to the tentative conclusion that the real part of all non-trivial zeros of the zeta function is consistently 1/2.","sentences":["The Riemann Hypothesis, originally proposed by the eminent mathematician Bernard Riemann in 1859, remains one of the most profound challenges in number theory.","It posits that all non-trivial zeros of the Riemann zeta function {\\zeta}(s) are concentrated precisely along the critical line where the real part equals 1/2.","In this paper, our aim is to present an attempt to prove this conjecture.","Our approach relies on the use of the reflection formula.","By applying this tool with precision and insight, we can conclusively establish that {\\xi}(s)^2 (Riemann's {\\xi}-function) is valid only when Re(s)=1/2.","As a direct consequence of this determination, we can assert that every zero of both {\\xi}(s)^2 and {\\xi}(s) has a real part equal to 1/2.","This, in turn, leads us to the tentative conclusion that the real part of all non-trivial zeros of the zeta function is consistently 1/2."],"url":"http://arxiv.org/abs/2403.05347v1","category":"math.GM"}
{"created":"2024-03-08 14:23:00","title":"VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model","abstract":"In the field of Class Incremental Object Detection (CIOD), creating models that can continuously learn like humans is a major challenge. Pseudo-labeling methods, although initially powerful, struggle with multi-scenario incremental learning due to their tendency to forget past knowledge. To overcome this, we introduce a new approach called Vision-Language Model assisted Pseudo-Labeling (VLM-PL). This technique uses Vision-Language Model (VLM) to verify the correctness of pseudo ground-truths (GTs) without requiring additional model training. VLM-PL starts by deriving pseudo GTs from a pre-trained detector. Then, we generate custom queries for each pseudo GT using carefully designed prompt templates that combine image and text features. This allows the VLM to classify the correctness through its responses. Furthermore, VLM-PL integrates refined pseudo and real GTs from upcoming training, effectively combining new and old knowledge. Extensive experiments conducted on the Pascal VOC and MS COCO datasets not only highlight VLM-PL's exceptional performance in multi-scenario but also illuminate its effectiveness in dual-scenario by achieving state-of-the-art results in both.","sentences":["In the field of Class Incremental Object Detection (CIOD), creating models that can continuously learn like humans is a major challenge.","Pseudo-labeling methods, although initially powerful, struggle with multi-scenario incremental learning due to their tendency to forget past knowledge.","To overcome this, we introduce a new approach called Vision-Language Model assisted Pseudo-Labeling (VLM-PL).","This technique uses Vision-Language Model (VLM) to verify the correctness of pseudo ground-truths (GTs) without requiring additional model training.","VLM-PL starts by deriving pseudo GTs from a pre-trained detector.","Then, we generate custom queries for each pseudo GT using carefully designed prompt templates that combine image and text features.","This allows the VLM to classify the correctness through its responses.","Furthermore, VLM-PL integrates refined pseudo and real GTs from upcoming training, effectively combining new and old knowledge.","Extensive experiments conducted on the Pascal VOC and MS COCO datasets not only highlight VLM-PL's exceptional performance in multi-scenario but also illuminate its effectiveness in dual-scenario by achieving state-of-the-art results in both."],"url":"http://arxiv.org/abs/2403.05346v1","category":"cs.CV"}
{"created":"2024-03-08 14:21:43","title":"Federated Learning Method for Preserving Privacy in Face Recognition System","abstract":"The state-of-the-art face recognition systems are typically trained on a single computer, utilizing extensive image datasets collected from various number of users. However, these datasets often contain sensitive personal information that users may hesitate to disclose. To address potential privacy concerns, we explore the application of federated learning, both with and without secure aggregators, in the context of both supervised and unsupervised face recognition systems. Federated learning facilitates the training of a shared model without necessitating the sharing of individual private data, achieving this by training models on decentralized edge devices housing the data. In our proposed system, each edge device independently trains its own model, which is subsequently transmitted either to a secure aggregator or directly to the central server. To introduce diverse data without the need for data transmission, we employ generative adversarial networks to generate imposter data at the edge. Following this, the secure aggregator or central server combines these individual models to construct a global model, which is then relayed back to the edge devices. Experimental findings based on the CelebA datasets reveal that employing federated learning in both supervised and unsupervised face recognition systems offers dual benefits. Firstly, it safeguards privacy since the original data remains on the edge devices. Secondly, the experimental results demonstrate that the aggregated model yields nearly identical performance compared to the individual models, particularly when the federated model does not utilize a secure aggregator. Hence, our results shed light on the practical challenges associated with privacy-preserving face image training, particularly in terms of the balance between privacy and accuracy.","sentences":["The state-of-the-art face recognition systems are typically trained on a single computer, utilizing extensive image datasets collected from various number of users.","However, these datasets often contain sensitive personal information that users may hesitate to disclose.","To address potential privacy concerns, we explore the application of federated learning, both with and without secure aggregators, in the context of both supervised and unsupervised face recognition systems.","Federated learning facilitates the training of a shared model without necessitating the sharing of individual private data, achieving this by training models on decentralized edge devices housing the data.","In our proposed system, each edge device independently trains its own model, which is subsequently transmitted either to a secure aggregator or directly to the central server.","To introduce diverse data without the need for data transmission, we employ generative adversarial networks to generate imposter data at the edge.","Following this, the secure aggregator or central server combines these individual models to construct a global model, which is then relayed back to the edge devices.","Experimental findings based on the CelebA datasets reveal that employing federated learning in both supervised and unsupervised face recognition systems offers dual benefits.","Firstly, it safeguards privacy since the original data remains on the edge devices.","Secondly, the experimental results demonstrate that the aggregated model yields nearly identical performance compared to the individual models, particularly when the federated model does not utilize a secure aggregator.","Hence, our results shed light on the practical challenges associated with privacy-preserving face image training, particularly in terms of the balance between privacy and accuracy."],"url":"http://arxiv.org/abs/2403.05344v1","category":"cs.CV"}
{"created":"2024-03-08 14:13:07","title":"A scalable method to model large suspensions of colloidal phoretic particles with arbitrary shapes","abstract":"Phoretic colloids self-propel thanks to surface flows generated in response to surface gradients (thermal, electrical, or chemical), that are self-induced and/or generated by other particles. Here we present a scalable and versatile framework to model chemical and hydrodynamic interactions in large suspensions of arbitrarily shaped phoretic particles, accounting for thermal fluctuations at all Damkholer numbers. Our approach, inspired by the Boundary Element Method (BEM), employs second-layer formulations, regularised kernels and a grid optimisation strategy to solve the coupled Laplace-Stokes equations with reasonable accuracy at a fraction of the computational cost associated with BEM. As demonstrated by our large-scale simulations, the capabilities of our method enable the exploration of new physical phenomena that, to our knowledge, have not been previously addressed by numerical simulations.","sentences":["Phoretic colloids self-propel thanks to surface flows generated in response to surface gradients (thermal, electrical, or chemical), that are self-induced and/or generated by other particles.","Here we present a scalable and versatile framework to model chemical and hydrodynamic interactions in large suspensions of arbitrarily shaped phoretic particles, accounting for thermal fluctuations at all Damkholer numbers.","Our approach, inspired by the Boundary Element Method (BEM), employs second-layer formulations, regularised kernels and a grid optimisation strategy to solve the coupled Laplace-Stokes equations with reasonable accuracy at a fraction of the computational cost associated with BEM.","As demonstrated by our large-scale simulations, the capabilities of our method enable the exploration of new physical phenomena that, to our knowledge, have not been previously addressed by numerical simulations."],"url":"http://arxiv.org/abs/2403.05337v1","category":"cond-mat.soft"}
{"created":"2024-03-08 14:12:30","title":"Estimating time-varying exposure effects through continuous-time modelling in Mendelian randomization","abstract":"Mendelian randomization is an instrumental variable method that utilizes genetic information to investigate the causal effect of a modifiable exposure on an outcome. In most cases, the exposure changes over time. Understanding the time-varying causal effect of the exposure can yield detailed insights into mechanistic effects and the potential impact of public health interventions. Recently, a growing number of Mendelian randomization studies have attempted to explore time-varying causal effects. However, the proposed approaches oversimplify temporal information and rely on overly restrictive structural assumptions, limiting their reliability in addressing time-varying causal problems. This paper considers a novel approach to estimate time-varying effects through continuous-time modelling by combining functional principal component analysis and weak-instrument-robust techniques. Our method effectively utilizes available data without making strong structural assumptions and can be applied in general settings where the exposure measurements occur at different timepoints for different individuals. We demonstrate through simulations that our proposed method performs well in estimating time-varying effects and provides reliable inference results when the time-varying effect form is correctly specified. The method could theoretically be used to estimate arbitrarily complex time-varying effects. However, there is a trade-off between model complexity and instrument strength. Estimating complex time-varying effects requires instruments that are unrealistically strong. We illustrate the application of this method in a case study examining the time-varying effects of systolic blood pressure on urea levels.","sentences":["Mendelian randomization is an instrumental variable method that utilizes genetic information to investigate the causal effect of a modifiable exposure on an outcome.","In most cases, the exposure changes over time.","Understanding the time-varying causal effect of the exposure can yield detailed insights into mechanistic effects and the potential impact of public health interventions.","Recently, a growing number of Mendelian randomization studies have attempted to explore time-varying causal effects.","However, the proposed approaches oversimplify temporal information and rely on overly restrictive structural assumptions, limiting their reliability in addressing time-varying causal problems.","This paper considers a novel approach to estimate time-varying effects through continuous-time modelling by combining functional principal component analysis and weak-instrument-robust techniques.","Our method effectively utilizes available data without making strong structural assumptions and can be applied in general settings where the exposure measurements occur at different timepoints for different individuals.","We demonstrate through simulations that our proposed method performs well in estimating time-varying effects and provides reliable inference results when the time-varying effect form is correctly specified.","The method could theoretically be used to estimate arbitrarily complex time-varying effects.","However, there is a trade-off between model complexity and instrument strength.","Estimating complex time-varying effects requires instruments that are unrealistically strong.","We illustrate the application of this method in a case study examining the time-varying effects of systolic blood pressure on urea levels."],"url":"http://arxiv.org/abs/2403.05336v1","category":"stat.ME"}
{"created":"2024-03-08 14:10:25","title":"WatChat: Explaining perplexing programs by debugging mental models","abstract":"Often, a good explanation for a program's unexpected behavior is a bug in the programmer's code. But sometimes, an even better explanation is a bug in the programmer's mental model of the language they are using. Instead of merely debugging our current code (\"giving the programmer a fish\"), what if our tools could directly debug our mental models (\"teaching the programmer to fish\")? In this paper, we apply ideas from computational cognitive science to do exactly that. Given a perplexing program, we use program synthesis techniques to automatically infer potential misconceptions that might cause the user to be surprised by the program's behavior. By analyzing these misconceptions, we provide succinct, useful explanations of the program's behavior. Our methods can even be inverted to synthesize pedagogical example programs for diagnosing and correcting misconceptions in students.","sentences":["Often, a good explanation for a program's unexpected behavior is a bug in the programmer's code.","But sometimes, an even better explanation is a bug in the programmer's mental model of the language they are using.","Instead of merely debugging our current code (\"giving the programmer a fish\"), what if our tools could directly debug our mental models (\"teaching the programmer to fish\")?","In this paper, we apply ideas from computational cognitive science to do exactly that.","Given a perplexing program, we use program synthesis techniques to automatically infer potential misconceptions that might cause the user to be surprised by the program's behavior.","By analyzing these misconceptions, we provide succinct, useful explanations of the program's behavior.","Our methods can even be inverted to synthesize pedagogical example programs for diagnosing and correcting misconceptions in students."],"url":"http://arxiv.org/abs/2403.05334v1","category":"cs.PL"}
{"created":"2024-03-08 14:05:36","title":"ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues","abstract":"Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU. Specifically, this TSA treats the ACR task as an auxiliary task to boost the performance of the primary ASU task, and further integrates trusted learning into reflexion mechanisms to alleviate the LLMs-intrinsic factual hallucination problem in TSA. Furthermore, a high-quality ChatASU dataset is annotated to evaluate TSA, and extensive experiments show that our proposed TSA can significantly outperform several state-of-the-art baselines, justifying the effectiveness of TSA to ChatASU and the importance of considering the coreference and hallucination issues in ChatASU.","sentences":["Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses.","However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance.","Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm.","In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in understanding aspect sentiments in dialogue scenarios.","Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue.","On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU.","Specifically, this TSA treats the ACR task as an auxiliary task to boost the performance of the primary ASU task, and further integrates trusted learning into reflexion mechanisms to alleviate the LLMs-intrinsic factual hallucination problem in TSA.","Furthermore, a high-quality ChatASU dataset is annotated to evaluate TSA, and extensive experiments show that our proposed TSA can significantly outperform several state-of-the-art baselines, justifying the effectiveness of TSA to ChatASU and the importance of considering the coreference and hallucination issues in ChatASU."],"url":"http://arxiv.org/abs/2403.05326v1","category":"cs.CL"}
{"created":"2024-03-08 14:04:30","title":"Fine-tuning a Multiple Instance Learning Feature Extractor with Masked Context Modelling and Knowledge Distillation","abstract":"The first step in Multiple Instance Learning (MIL) algorithms for Whole Slide Image (WSI) classification consists of tiling the input image into smaller patches and computing their feature vectors produced by a pre-trained feature extractor model. Feature extractor models that were pre-trained with supervision on ImageNet have proven to transfer well to this domain, however, this pre-training task does not take into account that visual information in neighboring patches is highly correlated. Based on this observation, we propose to increase downstream MIL classification by fine-tuning the feature extractor model using \\textit{Masked Context Modelling with Knowledge Distillation}. In this task, the feature extractor model is fine-tuned by predicting masked patches in a bigger context window. Since reconstructing the input image would require a powerful image generation model, and our goal is not to generate realistically looking image patches, we predict instead the feature vectors produced by a larger teacher network. A single epoch of the proposed task suffices to increase the downstream performance of the feature-extractor model when used in a MIL scenario, even capable of outperforming the downstream performance of the teacher model, while being considerably smaller and requiring a fraction of its compute.","sentences":["The first step in Multiple Instance Learning (MIL) algorithms for Whole Slide Image (WSI) classification consists of tiling the input image into smaller patches and computing their feature vectors produced by a pre-trained feature extractor model.","Feature extractor models that were pre-trained with supervision on ImageNet have proven to transfer well to this domain, however, this pre-training task does not take into account that visual information in neighboring patches is highly correlated.","Based on this observation, we propose to increase downstream MIL classification by fine-tuning the feature extractor model using \\textit{Masked Context Modelling with Knowledge Distillation}.","In this task, the feature extractor model is fine-tuned by predicting masked patches in a bigger context window.","Since reconstructing the input image would require a powerful image generation model, and our goal is not to generate realistically looking image patches, we predict instead the feature vectors produced by a larger teacher network.","A single epoch of the proposed task suffices to increase the downstream performance of the feature-extractor model when used in a MIL scenario, even capable of outperforming the downstream performance of the teacher model, while being considerably smaller and requiring a fraction of its compute."],"url":"http://arxiv.org/abs/2403.05325v1","category":"cs.CV"}
{"created":"2024-03-08 14:01:00","title":"Modularity in Argyres-Douglas Theories with $a=c$","abstract":"We consider a family of Argyres-Douglas theories, which are 4D $\\mathcal N=2$ strongly coupled superconformal field theories (SCFTs) but share many features with 4D $\\mathcal N=4 $ super-Yang-Mills theories. In particular, the two central charges of these theories are the same, namely $a=c$. We derive a simple and illuminating formula for the Schur index of these theories, which factorizes into the product of a Casimir term and a term referred to as the Schur partition function. While the former is controlled by the anomaly, the latter is identified with the vacuum character of the corresponding chiral algebra and is expected to satisfy the modular linear differential equation. Our simple expression for the Schur partition function, which can be regarded as the generalization of MacMahon's generalized sum-of-divisor function, allows one to numerically compute the series expansions efficiently, and furthermore find the corresponding modular linear differential equation. In a special case where the chiral algebra is known, we are able to derive the corresponding modular linear differential equation using Zhu's recursion relation. We further study the solutions to the modular linear differential equations and discuss their modular transformations. As an application, we study the high temperature limit or the Cardy-like limit of the Schur index using its simple expression and modular properties, thus shedding light on the 1/4-BPS microstates of genuine $\\mathcal N=2$ SCFTs with $a=c$ and their dual quantum gravity via the AdS/CFT correspondence.","sentences":["We consider a family of Argyres-Douglas theories, which are 4D $\\mathcal N=2$ strongly coupled superconformal field theories (SCFTs) but share many features with 4D $\\mathcal N=4 $ super-Yang-Mills theories.","In particular, the two central charges of these theories are the same, namely $a=c$.","We derive a simple and illuminating formula for the Schur index of these theories, which factorizes into the product of a Casimir term and a term referred to as the Schur partition function.","While the former is controlled by the anomaly, the latter is identified with the vacuum character of the corresponding chiral algebra and is expected to satisfy the modular linear differential equation.","Our simple expression for the Schur partition function, which can be regarded as the generalization of MacMahon's generalized sum-of-divisor function, allows one to numerically compute the series expansions efficiently, and furthermore find the corresponding modular linear differential equation.","In a special case where the chiral algebra is known, we are able to derive the corresponding modular linear differential equation using Zhu's recursion relation.","We further study the solutions to the modular linear differential equations and discuss their modular transformations.","As an application, we study the high temperature limit or the Cardy-like limit of the Schur index using its simple expression and modular properties, thus shedding light on the 1/4-BPS microstates of genuine $\\mathcal N=2$ SCFTs with $a=c$ and their dual quantum gravity via the AdS/CFT correspondence."],"url":"http://arxiv.org/abs/2403.05323v1","category":"hep-th"}
{"created":"2024-03-08 13:58:06","title":"GAN-based Massive MIMO Channel Model Trained on Measured Data","abstract":"Wireless channel models are a commonly used tool for the development of wireless telecommunication systems and standards. The currently prevailing geometry-based stochastic channel models (GSCMs) were manually specified for certain environments in a manual process requiring extensive domain knowledge, on the basis of channel measurement campaigns. By taking into account the stochastic distribution of certain channel properties like Rician k-factor, path loss or delay spread, they model the distribution of channel realizations. Instead of this manual process, a generative machine learning model like a generative adversarial network (GAN) may be used to automatically learn the distribution of channel statistics. Subsequently, the GAN's generator may be viewed as a channel model that can replace conventional stochastic or raytracer-based models. We propose a GAN architecture for a massive MIMO channel model, and train it on measurement data produced by a distributed massive MIMO channel sounder.","sentences":["Wireless channel models are a commonly used tool for the development of wireless telecommunication systems and standards.","The currently prevailing geometry-based stochastic channel models (GSCMs) were manually specified for certain environments in a manual process requiring extensive domain knowledge, on the basis of channel measurement campaigns.","By taking into account the stochastic distribution of certain channel properties like Rician k-factor, path loss or delay spread, they model the distribution of channel realizations.","Instead of this manual process, a generative machine learning model like a generative adversarial network (GAN) may be used to automatically learn the distribution of channel statistics.","Subsequently, the GAN's generator may be viewed as a channel model that can replace conventional stochastic or raytracer-based models.","We propose a GAN architecture for a massive MIMO channel model, and train it on measurement data produced by a distributed massive MIMO channel sounder."],"url":"http://arxiv.org/abs/2403.05321v1","category":"cs.IT"}
{"created":"2024-03-08 13:49:21","title":"Looking Ahead to Avoid Being Late: Solving Hard-Constrained Traveling Salesman Problem","abstract":"Many real-world problems can be formulated as a constrained Traveling Salesman Problem (TSP). However, the constraints are always complex and numerous, making the TSPs challenging to solve. When the number of complicated constraints grows, it is time-consuming for traditional heuristic algorithms to avoid illegitimate outcomes. Learning-based methods provide an alternative to solve TSPs in a soft manner, which also supports GPU acceleration to generate solutions quickly. Nevertheless, the soft manner inevitably results in difficulty solving hard-constrained problems with learning algorithms, and the conflicts between legality and optimality may substantially affect the optimality of the solution. To overcome this problem and to have an effective solution against hard constraints, we proposed a novel learning-based method that uses looking-ahead information as the feature to improve the legality of TSP with Time Windows (TSPTW) solutions. Besides, we constructed TSPTW datasets with hard constraints in order to accurately evaluate and benchmark the statistical performance of various approaches, which can serve the community for future research. With comprehensive experiments on diverse datasets, MUSLA outperforms existing baselines and shows generalizability potential.","sentences":["Many real-world problems can be formulated as a constrained Traveling Salesman Problem (TSP).","However, the constraints are always complex and numerous, making the TSPs challenging to solve.","When the number of complicated constraints grows, it is time-consuming for traditional heuristic algorithms to avoid illegitimate outcomes.","Learning-based methods provide an alternative to solve TSPs in a soft manner, which also supports GPU acceleration to generate solutions quickly.","Nevertheless, the soft manner inevitably results in difficulty solving hard-constrained problems with learning algorithms, and the conflicts between legality and optimality may substantially affect the optimality of the solution.","To overcome this problem and to have an effective solution against hard constraints, we proposed a novel learning-based method that uses looking-ahead information as the feature to improve the legality of TSP with Time Windows (TSPTW) solutions.","Besides, we constructed TSPTW datasets with hard constraints in order to accurately evaluate and benchmark the statistical performance of various approaches, which can serve the community for future research.","With comprehensive experiments on diverse datasets, MUSLA outperforms existing baselines and shows generalizability potential."],"url":"http://arxiv.org/abs/2403.05318v1","category":"cs.AI"}
{"created":"2024-03-08 13:48:21","title":"Extreme wave skewing and dispersion spectra of anisotropic elastic plates","abstract":"Guided wave dispersion is commonly assessed by Fourier analysis of the field along a line, resulting in frequency-wavenumber dispersion curves. In anisotropic plates, a point source can generate multiple dispersion branches pertaining to the same modal surface, which arise due to the angle between the power flux and the wave vector. We show that this phenomenon is particularly pronounced near zero-group-velocity points, entailing up to six contributions along a given direction. Stationary phase points accurately describe the measurements conducted on a monocrystalline silicon plate.","sentences":["Guided wave dispersion is commonly assessed by Fourier analysis of the field along a line, resulting in frequency-wavenumber dispersion curves.","In anisotropic plates, a point source can generate multiple dispersion branches pertaining to the same modal surface, which arise due to the angle between the power flux and the wave vector.","We show that this phenomenon is particularly pronounced near zero-group-velocity points, entailing up to six contributions along a given direction.","Stationary phase points accurately describe the measurements conducted on a monocrystalline silicon plate."],"url":"http://arxiv.org/abs/2403.05317v1","category":"physics.class-ph"}
{"created":"2024-03-08 13:47:32","title":"Direction of slip modulates the perception of slip distance and slip speed","abstract":"Purpose: The purpose of this study was to investigate the psychophysical understanding of the slip stimulus. We emphasized that the perception of slip and its characteristics, such as slip distance and slip speed depend on the interaction between slip direction, slip distance as well as slip speed. Methods: We developed a novel slip induction device to simulate the artificial sense of slip. We conducted a psychophysical experiment on eight healthy subjects. The experiment was designed to evaluate the effect of slip direction on slip perception as well as on the perception of slip distance and slip speed. A series of psychophysical questions were asked at the end of the slip stimulation to record the subjective responses of the participants. The average success rate (%) was used to quantify the subject responses. Results: We demonstrated that the perception of slip is independent of slip direction however, perception of slip distance and slip speed are significantly modulated by slip direction. We also observed that a significant interaction exists between slip distance and slip speed in the upward slip direction. It was also observed that the average success rate was significantly different for various combinations of slip distance and slip speed in the upward slip direction. Conclusions: Our study clearly establishes a significant interaction between the slip direction, slip distance, and slip speed for psychophysical understanding of the perception of slip distance and slip speed.","sentences":["Purpose: The purpose of this study was to investigate the psychophysical understanding of the slip stimulus.","We emphasized that the perception of slip and its characteristics, such as slip distance and slip speed depend on the interaction between slip direction, slip distance as well as slip speed.","Methods: We developed a novel slip induction device to simulate the artificial sense of slip.","We conducted a psychophysical experiment on eight healthy subjects.","The experiment was designed to evaluate the effect of slip direction on slip perception as well as on the perception of slip distance and slip speed.","A series of psychophysical questions were asked at the end of the slip stimulation to record the subjective responses of the participants.","The average success rate (%) was used to quantify the subject responses.","Results:","We demonstrated that the perception of slip is independent of slip direction however, perception of slip distance and slip speed are significantly modulated by slip direction.","We also observed that a significant interaction exists between slip distance and slip speed in the upward slip direction.","It was also observed that the average success rate was significantly different for various combinations of slip distance and slip speed in the upward slip direction.","Conclusions: Our study clearly establishes a significant interaction between the slip direction, slip distance, and slip speed for psychophysical understanding of the perception of slip distance and slip speed."],"url":"http://arxiv.org/abs/2403.05316v1","category":"cs.HC"}
{"created":"2024-03-08 13:42:19","title":"RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation","abstract":"We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT","sentences":["We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination.","In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated.","Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning.","The demo page can be found at https://craftjarvis.github.io/RAT"],"url":"http://arxiv.org/abs/2403.05313v1","category":"cs.CL"}
{"created":"2024-03-08 13:34:20","title":"Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents","abstract":"Interactive Data Analysis, the collaboration between humans and LLM agents, enables real-time data exploration for informed decision-making. The challenges and costs of collecting realistic interactive logs for data analysis hinder the quantitative evaluation of Large Language Model (LLM) agents in this task. To mitigate this issue, we introduce Tapilot-Crossing, a new benchmark to evaluate LLM agents on interactive data analysis. Tapilot-Crossing contains 1024 interactions, covering 4 practical scenarios: Normal, Action, Private, and Private Action. Notably, Tapilot-Crossing is constructed by an economical multi-agent environment, Decision Company, with few human efforts. We evaluate popular and advanced LLM agents in Tapilot-Crossing, which underscores the challenges of interactive data analysis. Furthermore, we propose Adaptive Interaction Reflection (AIR), a self-generated reflection strategy that guides LLM agents to learn from successful history. Experiments demonstrate that Air can evolve LLMs into effective interactive data analysis agents, achieving a relative performance improvement of up to 44.5%.","sentences":["Interactive Data Analysis, the collaboration between humans and LLM agents, enables real-time data exploration for informed decision-making.","The challenges and costs of collecting realistic interactive logs for data analysis hinder the quantitative evaluation of Large Language Model (LLM) agents in this task.","To mitigate this issue, we introduce Tapilot-Crossing, a new benchmark to evaluate LLM agents on interactive data analysis.","Tapilot-Crossing contains 1024 interactions, covering 4 practical scenarios: Normal, Action, Private, and Private Action.","Notably, Tapilot-Crossing is constructed by an economical multi-agent environment, Decision Company, with few human efforts.","We evaluate popular and advanced LLM agents in Tapilot-Crossing, which underscores the challenges of interactive data analysis.","Furthermore, we propose Adaptive Interaction Reflection (AIR), a self-generated reflection strategy that guides LLM agents to learn from successful history.","Experiments demonstrate that Air can evolve LLMs into effective interactive data analysis agents, achieving a relative performance improvement of up to 44.5%."],"url":"http://arxiv.org/abs/2403.05307v1","category":"cs.AI"}
{"created":"2024-03-08 13:33:00","title":"Spatiotemporal Predictive Pre-training for Robotic Motor Control","abstract":"Robotic motor control necessitates the ability to predict the dynamics of environments and interaction objects. However, advanced self-supervised pre-trained visual representations (PVRs) in robotic motor control, leveraging large-scale egocentric videos, often focus solely on learning the static content features of sampled image frames. This neglects the crucial temporal motion clues in human video data, which implicitly contain key knowledge about sequential interacting and manipulating with the environments and objects. In this paper, we present a simple yet effective robotic motor control visual pre-training framework that jointly performs spatiotemporal predictive learning utilizing large-scale video data, termed as STP. Our STP samples paired frames from video clips. It adheres to two key designs in a multi-task learning manner. First, we perform spatial prediction on the masked current frame for learning content features. Second, we utilize the future frame with an extremely high masking ratio as a condition, based on the masked current frame, to conduct temporal prediction of future frame for capturing motion features. These efficient designs ensure that our representation focusing on motion information while capturing spatial details. We carry out the largest-scale evaluation of PVRs for robotic motor control to date, which encompasses 21 tasks within a real-world Franka robot arm and 5 simulated environments. Extensive experiments demonstrate the effectiveness of STP as well as unleash its generality and data efficiency by further post-pre-training and hybrid pre-training.","sentences":["Robotic motor control necessitates the ability to predict the dynamics of environments and interaction objects.","However, advanced self-supervised pre-trained visual representations (PVRs) in robotic motor control, leveraging large-scale egocentric videos, often focus solely on learning the static content features of sampled image frames.","This neglects the crucial temporal motion clues in human video data, which implicitly contain key knowledge about sequential interacting and manipulating with the environments and objects.","In this paper, we present a simple yet effective robotic motor control visual pre-training framework that jointly performs spatiotemporal predictive learning utilizing large-scale video data, termed as STP.","Our STP samples paired frames from video clips.","It adheres to two key designs in a multi-task learning manner.","First, we perform spatial prediction on the masked current frame for learning content features.","Second, we utilize the future frame with an extremely high masking ratio as a condition, based on the masked current frame, to conduct temporal prediction of future frame for capturing motion features.","These efficient designs ensure that our representation focusing on motion information while capturing spatial details.","We carry out the largest-scale evaluation of PVRs for robotic motor control to date, which encompasses 21 tasks within a real-world Franka robot arm and 5 simulated environments.","Extensive experiments demonstrate the effectiveness of STP as well as unleash its generality and data efficiency by further post-pre-training and hybrid pre-training."],"url":"http://arxiv.org/abs/2403.05304v1","category":"cs.RO"}
{"created":"2024-03-08 13:32:01","title":"ACLSum: A New Dataset for Aspect-based Summarization of Scientific Publications","abstract":"Extensive efforts in the past have been directed toward the development of summarization datasets. However, a predominant number of these resources have been (semi)-automatically generated, typically through web data crawling, resulting in subpar resources for training and evaluating summarization systems, a quality compromise that is arguably due to the substantial costs associated with generating ground-truth summaries, particularly for diverse languages and specialized domains. To address this issue, we present ACLSum, a novel summarization dataset carefully crafted and evaluated by domain experts. In contrast to previous datasets, ACLSum facilitates multi-aspect summarization of scientific papers, covering challenges, approaches, and outcomes in depth. Through extensive experiments, we evaluate the quality of our resource and the performance of models based on pretrained language models and state-of-the-art large language models (LLMs). Additionally, we explore the effectiveness of extractive versus abstractive summarization within the scholarly domain on the basis of automatically discovered aspects. Our results corroborate previous findings in the general domain and indicate the general superiority of end-to-end aspect-based summarization. Our data is released at https://github.com/sobamchan/aclsum.","sentences":["Extensive efforts in the past have been directed toward the development of summarization datasets.","However, a predominant number of these resources have been (semi)-automatically generated, typically through web data crawling, resulting in subpar resources for training and evaluating summarization systems, a quality compromise that is arguably due to the substantial costs associated with generating ground-truth summaries, particularly for diverse languages and specialized domains.","To address this issue, we present ACLSum, a novel summarization dataset carefully crafted and evaluated by domain experts.","In contrast to previous datasets, ACLSum facilitates multi-aspect summarization of scientific papers, covering challenges, approaches, and outcomes in depth.","Through extensive experiments, we evaluate the quality of our resource and the performance of models based on pretrained language models and state-of-the-art large language models (LLMs).","Additionally, we explore the effectiveness of extractive versus abstractive summarization within the scholarly domain on the basis of automatically discovered aspects.","Our results corroborate previous findings in the general domain and indicate the general superiority of end-to-end aspect-based summarization.","Our data is released at https://github.com/sobamchan/aclsum."],"url":"http://arxiv.org/abs/2403.05303v1","category":"cs.CL"}
{"created":"2024-03-08 13:31:50","title":"Modeling Dynamic (De)Allocations of Local Memory for Translation Validation","abstract":"End-to-End Translation Validation is the problem of verifying the executable code generated by a compiler against the corresponding input source code for a single compilation. This becomes particularly hard in the presence of dynamically-allocated local memory where addresses of local memory may be observed by the program. In the context of validating the translation of a C procedure to executable code, a validator needs to tackle constant-length local arrays, address-taken local variables, address-taken formal parameters, variable-length local arrays, procedure-call arguments (including variadic arguments), and the {\\tt alloca()} operator. We provide an execution model, a definition of refinement, and an algorithm to soundly convert a refinement check into first-order logic queries that an off-the-shelf SMT solver can handle efficiently. In our experiments, we perform blackbox translation validation of C procedures (with up to 100+ SLOC), involving these local memory allocation constructs, against their corresponding assembly implementations (with up to 200+ instructions) generated by an optimizing compiler with complex loop and vectorizing transformations.","sentences":["End-to-End Translation Validation is the problem of verifying the executable code generated by a compiler against the corresponding input source code for a single compilation.","This becomes particularly hard in the presence of dynamically-allocated local memory where addresses of local memory may be observed by the program.","In the context of validating the translation of a C procedure to executable code, a validator needs to tackle constant-length local arrays, address-taken local variables, address-taken formal parameters, variable-length local arrays, procedure-call arguments (including variadic arguments), and the {\\tt alloca()} operator.","We provide an execution model, a definition of refinement, and an algorithm to soundly convert a refinement check into first-order logic queries that an off-the-shelf SMT solver can handle efficiently.","In our experiments, we perform blackbox translation validation of C procedures (with up to 100+ SLOC), involving these local memory allocation constructs, against their corresponding assembly implementations (with up to 200+ instructions) generated by an optimizing compiler with complex loop and vectorizing transformations."],"url":"http://arxiv.org/abs/2403.05302v1","category":"cs.PL"}
{"created":"2024-03-08 13:31:43","title":"Wykorzystanie Rekonfigurowalnych Iinteligentnych Matryc Antenowych w \u0141\u0105czu Dosy\u0142owym Sieci 5G/6G Wykorzystuj\u0105cej Bezza\u0142ogowe Statki Powietrzne","abstract":"Drony, dzi\\k{e}ki mo\\.zliwo\\'sci ich szybkiego rozmieszczenia w trudnym terenie, uwa\\.zane s\\k{a} za jeden z kluczowych element\\'ow system\\'ow bezprzewodowych 6G. Jednak w celu wykorzystania ich jako punkty dost\\k{e}powe sieci konieczne jest zapewnienie {\\l}\\k{a}cza dosy{\\l}owego o odpowiedniej przepustowo\\'sci. Dlatego w niniejszym artykule rozwa\\.zane jest zwi\\k{e}kszenie zasi\\k{e}gu sieci bezprzewodowej przez zapewnienie {\\l}\\k{a}cza dosy{\\l}owego dla ko\\'ncowego punktu dost\\k{e}powego z wykorzystaniem okre\\'slonej liczby dron\\'ow-przeka\\'znik\\'ow oraz rekonfigurowalnych inteligentnych matryc antenowych (RIS). Zaprezentowane wyniki bada\\'n symulacyjnych pokazuj\\k{a}, \\.ze u\\.zycie RIS pozwala na znacz\\k{a}ce zwi\\k{e}kszenie zasi\\k{e}gu sieci bez konieczno\\'sci stosowania dodatkowych przeka\\'znik\\'ow.   --   Unmanned Aerial Vehicles, due to the possibility of their fast deployment, are considered an essential element of the future wireless 6G communication systems. However, an essential enabler for their use as access points is to provide a sufficient throughput wireless backhaul link. Thus, in this paper we consider the aspect of extension of network coverage with the use of drone-based relaying and reconfigurable intelligent surfaces (RIS) for backhauling. Presented results of simulation experiments indicate that the use of RIS allows for significant improvement of network coverage without the need to use additional relays.","sentences":["Drony, dzi\\k{e}ki mo\\.zliwo\\'sci","ich szybkiego rozmieszczenia w trudnym terenie, uwa\\.zane s\\k{a} za jeden z kluczowych element\\'ow system\\'ow","bezprzewodowych","6G. Jednak w celu wykorzystania ich jako punkty dost\\k{e}powe sieci konieczne jest zapewnienie {\\l}\\k{a}cza dosy{\\l}owego o odpowiedniej przepustowo\\'sci.","Dlatego w niniejszym artykule rozwa\\.zane jest zwi\\k{e}kszenie zasi\\k{e}gu sieci bezprzewodowej przez zapewnienie {\\l}\\k{a}cza dosy{\\l}owego dla ko\\'ncowego punktu dost\\k{e}powego z wykorzystaniem okre\\'slonej liczby dron\\'ow-przeka\\'znik\\'ow oraz rekonfigurowalnych inteligentnych matryc antenowych (RIS).","Zaprezentowane wyniki bada\\'n symulacyjnych pokazuj\\k{a}, \\.ze u\\.zycie RIS pozwala na znacz\\k{a}ce zwi\\k{e}kszenie zasi\\k{e}gu sieci bez konieczno\\'sci stosowania dodatkowych przeka\\'znik\\'ow.   --   Unmanned Aerial Vehicles, due to the possibility of their fast deployment, are considered an essential element of the future wireless 6G communication systems.","However, an essential enabler for their use as access points is to provide a sufficient throughput wireless backhaul link.","Thus, in this paper we consider the aspect of extension of network coverage with the use of drone-based relaying and reconfigurable intelligent surfaces (RIS) for backhauling.","Presented results of simulation experiments indicate that the use of RIS allows for significant improvement of network coverage without the need to use additional relays."],"url":"http://arxiv.org/abs/2403.05301v1","category":"cs.NI"}
{"created":"2024-03-08 13:29:46","title":"Unity by Diversity: Improved Representation Learning in Multimodal VAEs","abstract":"Variational Autoencoders for multimodal data hold promise for many tasks in data analysis, such as representation learning, conditional generation, and imputation. Current architectures either share the encoder output, decoder input, or both across modalities to learn a shared representation. Such architectures impose hard constraints on the model. In this work, we show that a better latent representation can be obtained by replacing these hard constraints with a soft constraint. We propose a new mixture-of-experts prior, softly guiding each modality's latent representation towards a shared aggregate posterior. This approach results in a superior latent representation and allows each encoding to preserve information from its uncompressed original features better. In extensive experiments on multiple benchmark datasets and a challenging real-world neuroscience data set, we show improved learned latent representations and imputation of missing data modalities compared to existing methods.","sentences":["Variational Autoencoders for multimodal data hold promise for many tasks in data analysis, such as representation learning, conditional generation, and imputation.","Current architectures either share the encoder output, decoder input, or both across modalities to learn a shared representation.","Such architectures impose hard constraints on the model.","In this work, we show that a better latent representation can be obtained by replacing these hard constraints with a soft constraint.","We propose a new mixture-of-experts prior, softly guiding each modality's latent representation towards a shared aggregate posterior.","This approach results in a superior latent representation and allows each encoding to preserve information from its uncompressed original features better.","In extensive experiments on multiple benchmark datasets and a challenging real-world neuroscience data set, we show improved learned latent representations and imputation of missing data modalities compared to existing methods."],"url":"http://arxiv.org/abs/2403.05300v1","category":"cs.LG"}
{"created":"2024-03-08 13:24:46","title":"PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck","abstract":"CLIP-based classifiers rely on the prompt containing a {class name} that is known to the text encoder. That is, CLIP performs poorly on new classes or the classes whose names rarely appear on the Internet (e.g., scientific names of birds). For fine-grained classification, we propose PEEB - an explainable and editable classifier to (1) express the class name into a set of pre-defined text descriptors that describe the visual parts of that class; and (2) match the embeddings of the detected parts to their textual descriptors in each class to compute a logit score for classification. In a zero-shot setting where the class names are unknown, PEEB outperforms CLIP by a large margin (~10x in accuracy). Compared to part-based classifiers, PEEB is not only the state-of-the-art on the supervised-learning setting (88.80% accuracy) but also the first to enable users to edit the class definitions to form a new classifier without retraining. Compared to concept bottleneck models, PEEB is also the state-of-the-art in both zero-shot and supervised learning settings.","sentences":["CLIP-based classifiers rely on the prompt containing a {class name} that is known to the text encoder.","That is, CLIP performs poorly on new classes or the classes whose names rarely appear on the Internet (e.g., scientific names of birds).","For fine-grained classification, we propose PEEB - an explainable and editable classifier to (1) express the class name into a set of pre-defined text descriptors that describe the visual parts of that class; and (2) match the embeddings of the detected parts to their textual descriptors in each class to compute a logit score for classification.","In a zero-shot setting where the class names are unknown, PEEB outperforms CLIP by a large margin (~10x in accuracy).","Compared to part-based classifiers, PEEB is not only the state-of-the-art on the supervised-learning setting (88.80% accuracy) but also the first to enable users to edit the class definitions to form a new classifier without retraining.","Compared to concept bottleneck models, PEEB is also the state-of-the-art in both zero-shot and supervised learning settings."],"url":"http://arxiv.org/abs/2403.05297v1","category":"cs.CV"}
{"created":"2024-03-08 13:23:09","title":"Bound-extended mode transition in type-II synthetic photonic Weyl heterostructures","abstract":"Photonic structures with Weyl points (WPs), including type-I and type-II, promise nontrivial surface modes and intriguing light manipulations for their three-dimensional topological bands. While previous studies mainly focus on exploring WPs in a uniform Weyl structure, here we establish Weyl heterostructures (i.e., a nonuniform Weyl lattice) with different rotational orientations in the synthetic dimension by nanostructured photonic waveguides. In this work, we unveil a transition between bound and extended modes on the interface of type-II Weyl heterostructures by tuning their rotational phases, despite the reversed topological order across the interface. This mode transition is also manifested from the total transmission to total reflection at the interface. All of these unconventional effects are attributed to the tilted dispersion of type-II Weyl band structure that can lead to mismatched bands and gaps across the interface. As a comparison, the type-I Weyl heterostructures lack the phase transition due to the untilted band structure. This work establishes a flexible scheme of artificial Weyl heterostructures that opens a new avenue towards high-dimensional topological effects and significantly enhances our capabilities in on-chip light manipulations.","sentences":["Photonic structures with Weyl points (WPs), including type-I and type-II, promise nontrivial surface modes and intriguing light manipulations for their three-dimensional topological bands.","While previous studies mainly focus on exploring WPs in a uniform Weyl structure, here we establish Weyl heterostructures (i.e., a nonuniform Weyl lattice) with different rotational orientations in the synthetic dimension by nanostructured photonic waveguides.","In this work, we unveil a transition between bound and extended modes on the interface of type-II Weyl heterostructures by tuning their rotational phases, despite the reversed topological order across the interface.","This mode transition is also manifested from the total transmission to total reflection at the interface.","All of these unconventional effects are attributed to the tilted dispersion of type-II Weyl band structure that can lead to mismatched bands and gaps across the interface.","As a comparison, the type-I Weyl heterostructures lack the phase transition due to the untilted band structure.","This work establishes a flexible scheme of artificial Weyl heterostructures that opens a new avenue towards high-dimensional topological effects and significantly enhances our capabilities in on-chip light manipulations."],"url":"http://arxiv.org/abs/2403.05294v1","category":"physics.optics"}
{"created":"2024-03-08 13:20:40","title":"RIS-aided multi-hop backhauling for 5G/6G UAV-assisted access points","abstract":"Drones are envisaged as an important part of the future 6G systems. With the possibility of their fast deployment they provide additional connectivity options in form of a hotspot. However, typically in such a use case they require provisioning of a wireless backhaul link to facilitate their proper operation, which might be a challenging task in urban environment. One of the possible ways to connect such nodes is to use the integrate access and backhaul (IAB) approach, where part of the spectrum dedicated for user access at the base station is used for wireless backhauling. Thus, in this work we consider the problem of establishing a multi-hop wireless backhaul link following the IAB concept, with the aid of drone relay stations (DRSs) and reconfigurable intelligent surfaces (RISs). We formulate the problem of coverage improvement with fixed number of relays assuming certain throughput requirements on the backhaul. We show with simulations that the use of RISs allows for improvement of coverage in such a scenario or reduction in the number of involved nodes to provide the required backhaul.","sentences":["Drones are envisaged as an important part of the future 6G systems.","With the possibility of their fast deployment they provide additional connectivity options in form of a hotspot.","However, typically in such a use case they require provisioning of a wireless backhaul link to facilitate their proper operation, which might be a challenging task in urban environment.","One of the possible ways to connect such nodes is to use the integrate access and backhaul (IAB) approach, where part of the spectrum dedicated for user access at the base station is used for wireless backhauling.","Thus, in this work we consider the problem of establishing a multi-hop wireless backhaul link following the IAB concept, with the aid of drone relay stations (DRSs) and reconfigurable intelligent surfaces (RISs).","We formulate the problem of coverage improvement with fixed number of relays assuming certain throughput requirements on the backhaul.","We show with simulations that the use of RISs allows for improvement of coverage in such a scenario or reduction in the number of involved nodes to provide the required backhaul."],"url":"http://arxiv.org/abs/2403.05292v1","category":"cs.NI"}
{"created":"2024-03-08 13:06:46","title":"Gravity as a topological gauge theory","abstract":"We describe a topological gauge theory with underlying Cartan geometry $G/H$ and principal $H$-bundle. The topological action is constructed as a linear combination of the Euler and Pontrjagin numbers coming from entries of the curvature $\\bar{\\Omega}$ of a Cartan connection.   As an example we show that for respectively   $G=   \\left\\{   \\begin{array}{ll}   SO(4,1) \\text{ for } \\Lambda_0 > 0   SO(3,2) \\text{ for } \\Lambda_0 < 0   \\end{array}   \\right.$ and $H=SO(3,1)$ a very particular linear combination yields the Holst + Euler and Pontrjagin of the curvature $R$ of the spin connection + Nieh-Yan + bare cosmological constant $\\Lambda_0$ terms. Additionally, in this construction, the coupling constants of these different terms are inherently linked together.   The other treated example is the one of the M\\\"obius group $G= SO(4,2) /\\{\\pm I\\}$ and $H = CO(4,2) \\ltimes \\mathbb{R}^{3,1}$. By considering a constraint on the \"pair of frames\" $(\\alpha,\\beta)$ we retrieve an action comprising all terms compatible with diffeomorphism invariance and local Lorentz invariance described in the previous example + kinetic term for a scalar field + an interaction of that scalar field with the Torsion $T$, the scalar field being related to dilation.   Finally, in both examples, we study the equations of motion associated to the total (Gauge + Matter) action. It is especially shown that one of the solutions in the M\\\"obius case consists of Einstein's equations modified by the Holst term with an additional source term for curvature depending on specific variations of spin density of matter.","sentences":["We describe a topological gauge theory with underlying Cartan geometry $G/H$ and principal $H$-bundle.","The topological action is constructed as a linear combination of the Euler and Pontrjagin numbers coming from entries of the curvature $\\bar{\\Omega}$ of a Cartan connection.   ","As an example we show that for respectively   $G=   \\left\\{   \\begin{array}{ll}   SO(4,1) \\text{ for } \\Lambda_0 > 0   SO(3,2) \\text{ for } \\Lambda_0 < 0   \\end{array}   \\right.$ and $H=SO(3,1)$ a very particular linear combination yields the Holst + Euler and Pontrjagin of the curvature $R$ of the spin connection + Nieh-Yan + bare cosmological constant $\\Lambda_0$ terms.","Additionally, in this construction, the coupling constants of these different terms are inherently linked together.   ","The other treated example is the one of the M\\\"obius group $G= SO(4,2) /\\{\\pm","I\\}$ and $H = CO(4,2) \\ltimes \\mathbb{R}^{3,1}$. By considering a constraint on the \"pair of frames\" $(\\alpha,\\beta)$ we retrieve an action comprising all terms compatible with diffeomorphism invariance and local Lorentz invariance described in the previous example + kinetic term for a scalar field + an interaction of that scalar field with the Torsion $T$, the scalar field being related to dilation.   ","Finally, in both examples, we study the equations of motion associated to the total (Gauge + Matter) action.","It is especially shown that one of the solutions in the M\\\"obius case consists of Einstein's equations modified by the Holst term with an additional source term for curvature depending on specific variations of spin density of matter."],"url":"http://arxiv.org/abs/2403.05284v1","category":"gr-qc"}
{"created":"2024-03-08 13:05:56","title":"Closely piling up of multiple adhesive fronts in adhesive friction due to re-attachment","abstract":"To understand why the adhesive frictional force was in linear proportion to the real contact area in experiments, we investigate the adhesive friction generated by sliding elastic solids adhered to a rigid surface via multiple adhesive springs. Our results indicate that the shear-off force of the interface increases with the energetically guided re-attachment rate of adhesive springs, reaching saturation at high re-attachment rates. Remarkably, this shear-off force can surpass the predictions made by the fracture theory. By plotting the adhesive forces along the interface, we observe substantial high adhesive forces distributed throughout the interface, based on which we identify multiple adhesive fronts closely piling up along the interface. These regions can exhibit similar force profiles, and their number appears to increase with the size of the interface, leading to a linear increase in the calculated shear-off force with the size of the interface. We then suggest that multiple adhesive fronts closely pile up to back up each other in adhesive friction due to re-attachments, which may provide profound insights into understanding the observed phenomena associated with adhesive friction along an interface.","sentences":["To understand why the adhesive frictional force was in linear proportion to the real contact area in experiments, we investigate the adhesive friction generated by sliding elastic solids adhered to a rigid surface via multiple adhesive springs.","Our results indicate that the shear-off force of the interface increases with the energetically guided re-attachment rate of adhesive springs, reaching saturation at high re-attachment rates.","Remarkably, this shear-off force can surpass the predictions made by the fracture theory.","By plotting the adhesive forces along the interface, we observe substantial high adhesive forces distributed throughout the interface, based on which we identify multiple adhesive fronts closely piling up along the interface.","These regions can exhibit similar force profiles, and their number appears to increase with the size of the interface, leading to a linear increase in the calculated shear-off force with the size of the interface.","We then suggest that multiple adhesive fronts closely pile up to back up each other in adhesive friction due to re-attachments, which may provide profound insights into understanding the observed phenomena associated with adhesive friction along an interface."],"url":"http://arxiv.org/abs/2403.05283v1","category":"cond-mat.soft"}
{"created":"2024-03-08 13:01:09","title":"An Efficient Quasi-Random Sampling for Copulas","abstract":"This paper examines an efficient method for quasi-random sampling of copulas in Monte Carlo computations. Traditional methods, like conditional distribution methods (CDM), have limitations when dealing with high-dimensional or implicit copulas, which refer to those that cannot be accurately represented by existing parametric copulas. Instead, this paper proposes the use of generative models, such as Generative Adversarial Networks (GANs), to generate quasi-random samples for any copula. GANs are a type of implicit generative models used to learn the distribution of complex data, thus facilitating easy sampling. In our study, GANs are employed to learn the mapping from a uniform distribution to copulas. Once this mapping is learned, obtaining quasi-random samples from the copula only requires inputting quasi-random samples from the uniform distribution. This approach offers a more flexible method for any copula. Additionally, we provide theoretical analysis of quasi-Monte Carlo estimators based on quasi-random samples of copulas. Through simulated and practical applications, particularly in the field of risk management, we validate the proposed method and demonstrate its superiority over various existing methods.","sentences":["This paper examines an efficient method for quasi-random sampling of copulas in Monte Carlo computations.","Traditional methods, like conditional distribution methods (CDM), have limitations when dealing with high-dimensional or implicit copulas, which refer to those that cannot be accurately represented by existing parametric copulas.","Instead, this paper proposes the use of generative models, such as Generative Adversarial Networks (GANs), to generate quasi-random samples for any copula.","GANs are a type of implicit generative models used to learn the distribution of complex data, thus facilitating easy sampling.","In our study, GANs are employed to learn the mapping from a uniform distribution to copulas.","Once this mapping is learned, obtaining quasi-random samples from the copula only requires inputting quasi-random samples from the uniform distribution.","This approach offers a more flexible method for any copula.","Additionally, we provide theoretical analysis of quasi-Monte Carlo estimators based on quasi-random samples of copulas.","Through simulated and practical applications, particularly in the field of risk management, we validate the proposed method and demonstrate its superiority over various existing methods."],"url":"http://arxiv.org/abs/2403.05281v1","category":"stat.ML"}
{"created":"2024-03-08 12:58:12","title":"Load Balancing For High Performance Computing Using Quantum Annealing","abstract":"With the advent of exascale computing, effective load balancing in massively parallel software applications is critically important for leveraging the full potential of high performance computing systems. Load balancing is the distribution of computational work between available processors. Here, we investigate the application of quantum annealing to load balance two paradigmatic algorithms in high performance computing. Namely, adaptive mesh refinement and smoothed particle hydrodynamics are chosen as representative grid and off-grid target applications. While the methodology for obtaining real simulation data to partition is application specific, the proposed balancing protocol itself remains completely general. In a grid based context, quantum annealing is found to outperform classical methods such as the round robin protocol but lacks a decisive advantage over more advanced methods such as steepest descent or simulated annealing despite remaining competitive. The primary obstacle to scalability is found to be limited coupling on current quantum annealing hardware. However, for the more complex particle formulation, approached as a multi-objective optimization, quantum annealing solutions are demonstrably Pareto dominant to state of the art classical methods across both objectives. This signals a noteworthy advancement in solution quality which can have a large impact on effective CPU usage.","sentences":["With the advent of exascale computing, effective load balancing in massively parallel software applications is critically important for leveraging the full potential of high performance computing systems.","Load balancing is the distribution of computational work between available processors.","Here, we investigate the application of quantum annealing to load balance two paradigmatic algorithms in high performance computing.","Namely, adaptive mesh refinement and smoothed particle hydrodynamics are chosen as representative grid and off-grid target applications.","While the methodology for obtaining real simulation data to partition is application specific, the proposed balancing protocol itself remains completely general.","In a grid based context, quantum annealing is found to outperform classical methods such as the round robin protocol but lacks a decisive advantage over more advanced methods such as steepest descent or simulated annealing despite remaining competitive.","The primary obstacle to scalability is found to be limited coupling on current quantum annealing hardware.","However, for the more complex particle formulation, approached as a multi-objective optimization, quantum annealing solutions are demonstrably Pareto dominant to state of the art classical methods across both objectives.","This signals a noteworthy advancement in solution quality which can have a large impact on effective CPU usage."],"url":"http://arxiv.org/abs/2403.05278v1","category":"quant-ph"}
{"created":"2024-03-08 12:58:00","title":"ADROIT6G DAI-driven Open and Programmable Architecture for 6G Networks","abstract":"In the upcoming 6G era, mobile networks must deal with more challenging applications (e.g., holographic telepresence and immersive communication) and meet far more stringent application requirements stemming along the edge-cloud continuum. These new applications will create an elevated level of expectations on performance, reliability, ubiquity, trustworthiness, security, openness, and sustainability, pushing the boundaries of innovation and driving transformational change across the architecture of future mobile networks. Towards this end, ADROIT6G proposes a set of disruptive innovations with a clear vision on setting a 6G network architecture that can be tailored to the requirements of innovative applications and match the ambitious KPIs set for 6G networks. More specifically, the key transformations that ADROIT6G considers essential to 6G network evolution are: i) AI/ML-powered optimisations across the network, exploring solutions in the \"Distributed Artificial Intelligence (DAI)\" domain for high performance and automation; ii) Transforming to fully cloud-native network software, which can be implemented across various edge-cloud platforms, with security built integrally into the network user plan; and iii) Software driven, zero-touch operations and ultimately automation of every aspect of the network and the services it delivers.","sentences":["In the upcoming 6G era, mobile networks must deal with more challenging applications (e.g., holographic telepresence and immersive communication) and meet far more stringent application requirements stemming along the edge-cloud continuum.","These new applications will create an elevated level of expectations on performance, reliability, ubiquity, trustworthiness, security, openness, and sustainability, pushing the boundaries of innovation and driving transformational change across the architecture of future mobile networks.","Towards this end, ADROIT6G proposes a set of disruptive innovations with a clear vision on setting a 6G network architecture that can be tailored to the requirements of innovative applications and match the ambitious KPIs set for 6G networks.","More specifically, the key transformations that ADROIT6G considers essential to 6G network evolution are: i) AI/ML-powered optimisations across the network, exploring solutions in the \"Distributed Artificial Intelligence (DAI)\" domain for high performance and automation; ii) Transforming to fully cloud-native network software, which can be implemented across various edge-cloud platforms, with security built integrally into the network user plan; and iii) Software driven, zero-touch operations and ultimately automation of every aspect of the network and the services it delivers."],"url":"http://arxiv.org/abs/2403.05277v1","category":"cs.NI"}
{"created":"2024-03-08 12:53:16","title":"A Goldstone Boson Equivalence for Inflation","abstract":"The effective field theory of single-field inflation characterizes the inflationary epoch in terms of a pattern of symmetry breaking. An operator acquires a time-dependent vacuum expectation value, defining a preferred spatial slicing. In the absence of dynamical gravity, the fluctuations around the time-dependent background are described by the Goldstone boson associated with this symmetry breaking process. With gravity, the Goldstone is eaten by the metric, becoming the scalar metric fluctuation. In this paper, we will show that in general single-field inflation, the statistics of scalar metric fluctuations are given by the statistics of this Goldstone boson it decoupled from gravity up to corrections that are controlled as an expansion in slow-roll parameters. This even holds in the presence of additional parameters, like the speed of sound, that naively enhance the impact of the gravitational terms. In the process, we derive expressions for leading and sub-leading gravitational corrections to all-orders in the Goldstone boson.","sentences":["The effective field theory of single-field inflation characterizes the inflationary epoch in terms of a pattern of symmetry breaking.","An operator acquires a time-dependent vacuum expectation value, defining a preferred spatial slicing.","In the absence of dynamical gravity, the fluctuations around the time-dependent background are described by the Goldstone boson associated with this symmetry breaking process.","With gravity, the Goldstone is eaten by the metric, becoming the scalar metric fluctuation.","In this paper, we will show that in general single-field inflation, the statistics of scalar metric fluctuations are given by the statistics of this Goldstone boson it decoupled from gravity up to corrections that are controlled as an expansion in slow-roll parameters.","This even holds in the presence of additional parameters, like the speed of sound, that naively enhance the impact of the gravitational terms.","In the process, we derive expressions for leading and sub-leading gravitational corrections to all-orders in the Goldstone boson."],"url":"http://arxiv.org/abs/2403.05274v1","category":"hep-th"}
{"created":"2024-03-08 12:49:09","title":"A note on lenses in arrangements of pairwise intersecting circles in the plane","abstract":"Let $\\F$ be a family of $n$ pairwise intersecting circles in the plane. We show that the number of lenses, that is convex digons, in the arrangement induced by $\\F$ is at most $2n-2$. This bound is tight. Furthermore, if no two circles in $\\F$ touch, then the geometric graph $G$ on the set of centers of the circles in $\\F$ whose edges correspond to the lenses generated by $\\F$ does not contain pairs of avoiding edges. That is, $G$ does not contain pairs of edges that are opposite edges in a convex quadrilateral. Such graphs are known to have at most $2n-2$ edges.","sentences":["Let $\\F$ be a family of $n$ pairwise intersecting circles in the plane.","We show that the number of lenses, that is convex digons, in the arrangement induced by $\\F$ is at most $2n-2$. This bound is tight.","Furthermore, if no two circles in $\\F$ touch, then the geometric graph $G$ on the set of centers of the circles in $\\F$ whose edges correspond to the lenses generated by $\\F$ does not contain pairs of avoiding edges.","That is, $G$ does not contain pairs of edges that are opposite edges in a convex quadrilateral.","Such graphs are known to have at most $2n-2$ edges."],"url":"http://arxiv.org/abs/2403.05270v1","category":"math.CO"}
{"created":"2024-03-08 12:45:53","title":"Deep Prompt Multi-task Network for Abuse Language Detection","abstract":"The detection of abusive language remains a long-standing challenge with the extensive use of social networks. The detection task of abusive language suffers from limited accuracy. We argue that the existing detection methods utilize the fine-tuning technique of the pre-trained language models (PLMs) to handle downstream tasks. Hence, these methods fail to stimulate the general knowledge of the PLMs. To address the problem, we propose a novel Deep Prompt Multi-task Network (DPMN) for abuse language detection. Specifically, DPMN first attempts to design two forms of deep prompt tuning and light prompt tuning for the PLMs. The effects of different prompt lengths, tuning strategies, and prompt initialization methods on detecting abusive language are studied. In addition, we propose a Task Head based on Bi-LSTM and FFN, which can be used as a short text classifier. Eventually, DPMN utilizes multi-task learning to improve detection metrics further. The multi-task network has the function of transferring effective knowledge. The proposed DPMN is evaluated against eight typical methods on three public datasets: OLID, SOLID, and AbuseAnalyzer. The experimental results show that our DPMN outperforms the state-of-the-art methods.","sentences":["The detection of abusive language remains a long-standing challenge with the extensive use of social networks.","The detection task of abusive language suffers from limited accuracy.","We argue that the existing detection methods utilize the fine-tuning technique of the pre-trained language models (PLMs) to handle downstream tasks.","Hence, these methods fail to stimulate the general knowledge of the PLMs.","To address the problem, we propose a novel Deep Prompt Multi-task Network (DPMN) for abuse language detection.","Specifically, DPMN first attempts to design two forms of deep prompt tuning and light prompt tuning for the PLMs.","The effects of different prompt lengths, tuning strategies, and prompt initialization methods on detecting abusive language are studied.","In addition, we propose a Task Head based on Bi-LSTM and FFN, which can be used as a short text classifier.","Eventually, DPMN utilizes multi-task learning to improve detection metrics further.","The multi-task network has the function of transferring effective knowledge.","The proposed DPMN is evaluated against eight typical methods on three public datasets: OLID, SOLID, and AbuseAnalyzer.","The experimental results show that our DPMN outperforms the state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.05268v1","category":"cs.CL"}
{"created":"2024-03-08 12:44:59","title":"Integrable systems in magnetic fields: the generalized parabolic cylindrical case","abstract":"This article is a contribution to the classification of quadratically integrable systems with vector potentials whose integrals are of the nonstandard, nonseparable type. We focus on generalized parabolic cylindrical case, related to non-subgroup-type coordinates. We find 3 new systems, two with magnetic fields polynomial in Cartesian coordinates and one with unbounded exponential terms. The limit in the parameters of the integrals yields a new parabolic cylindrical system; the limit of vanishing magnetic fields leads to the free motion. This confirms the conjecture that non-subgroup type integrals can be related to separable systems only in a trivial manner.","sentences":["This article is a contribution to the classification of quadratically integrable systems with vector potentials whose integrals are of the nonstandard, nonseparable type.","We focus on generalized parabolic cylindrical case, related to non-subgroup-type coordinates.","We find 3 new systems, two with magnetic fields polynomial in Cartesian coordinates and one with unbounded exponential terms.","The limit in the parameters of the integrals yields a new parabolic cylindrical system; the limit of vanishing magnetic fields leads to the free motion.","This confirms the conjecture that non-subgroup type integrals can be related to separable systems only in a trivial manner."],"url":"http://arxiv.org/abs/2403.05267v1","category":"nlin.SI"}
{"created":"2024-03-08 12:42:36","title":"ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models","abstract":"Large language models (LLMs) have achieved unprecedented performance in various applications, yet their evaluation remains a critical issue. Existing hallucination benchmarks are either static or lack adjustable complexity for thorough analysis. We contend that utilizing existing relational databases is a promising approach for constructing benchmarks due to their accurate knowledge description via functional dependencies. We propose ERBench to automatically convert any relational database into a benchmark based on the entity-relationship (ER) model. Our key idea is to construct questions using the database schema, records, and functional dependencies such that they can be automatically verified. In addition, we use foreign key constraints to join relations and construct multihop questions, which can be arbitrarily complex and used to debug the intermediate answers of LLMs. Finally, ERBench supports continuous evaluation, multimodal questions, and various prompt engineering techniques. In our experiments, we construct an LLM benchmark using databases of multiple domains and make an extensive comparison of contemporary LLMs. We observe that better LLMs like GPT-4 can handle a larger variety of question types, but are by no means perfect. Also, correct answers do not necessarily imply correct rationales, which is an important evaluation that ERBench does better than other benchmarks for various question types. Code is available at https: //github.com/DILAB-KAIST/ERBench.","sentences":["Large language models (LLMs) have achieved unprecedented performance in various applications, yet their evaluation remains a critical issue.","Existing hallucination benchmarks are either static or lack adjustable complexity for thorough analysis.","We contend that utilizing existing relational databases is a promising approach for constructing benchmarks due to their accurate knowledge description via functional dependencies.","We propose ERBench to automatically convert any relational database into a benchmark based on the entity-relationship (ER) model.","Our key idea is to construct questions using the database schema, records, and functional dependencies such that they can be automatically verified.","In addition, we use foreign key constraints to join relations and construct multihop questions, which can be arbitrarily complex and used to debug the intermediate answers of LLMs.","Finally, ERBench supports continuous evaluation, multimodal questions, and various prompt engineering techniques.","In our experiments, we construct an LLM benchmark using databases of multiple domains and make an extensive comparison of contemporary LLMs.","We observe that better LLMs like GPT-4 can handle a larger variety of question types, but are by no means perfect.","Also, correct answers do not necessarily imply correct rationales, which is an important evaluation that ERBench does better than other benchmarks for various question types.","Code is available at https: //github.com/DILAB-KAIST/ERBench."],"url":"http://arxiv.org/abs/2403.05266v1","category":"cs.CL"}
{"created":"2024-03-08 12:42:04","title":"MMoE: Robust Spoiler Detection with Multi-modal Information and Domain-aware Mixture-of-Experts","abstract":"Online movie review websites are valuable for information and discussion about movies. However, the massive spoiler reviews detract from the movie-watching experience, making spoiler detection an important task. Previous methods simply focus on reviews' text content, ignoring the heterogeneity of information in the platform. For instance, the metadata and the corresponding user's information of a review could be helpful. Besides, the spoiler language of movie reviews tends to be genre-specific, thus posing a domain generalization challenge for existing methods. To this end, we propose MMoE, a multi-modal network that utilizes information from multiple modalities to facilitate robust spoiler detection and adopts Mixture-of-Experts to enhance domain generalization. MMoE first extracts graph, text, and meta feature from the user-movie network, the review's textual content, and the review's metadata respectively. To handle genre-specific spoilers, we then adopt Mixture-of-Experts architecture to process information in three modalities to promote robustness. Finally, we use an expert fusion layer to integrate the features from different perspectives and make predictions based on the fused embedding. Experiments demonstrate that MMoE achieves state-of-the-art performance on two widely-used spoiler detection datasets, surpassing previous SOTA methods by 2.56\\% and 8.41\\% in terms of accuracy and F1-score. Further experiments also demonstrate MMoE's superiority in robustness and generalization.","sentences":["Online movie review websites are valuable for information and discussion about movies.","However, the massive spoiler reviews detract from the movie-watching experience, making spoiler detection an important task.","Previous methods simply focus on reviews' text content, ignoring the heterogeneity of information in the platform.","For instance, the metadata and the corresponding user's information of a review could be helpful.","Besides, the spoiler language of movie reviews tends to be genre-specific, thus posing a domain generalization challenge for existing methods.","To this end, we propose MMoE, a multi-modal network that utilizes information from multiple modalities to facilitate robust spoiler detection and adopts Mixture-of-Experts to enhance domain generalization.","MMoE first extracts graph, text, and meta feature from the user-movie network, the review's textual content, and the review's metadata respectively.","To handle genre-specific spoilers, we then adopt Mixture-of-Experts architecture to process information in three modalities to promote robustness.","Finally, we use an expert fusion layer to integrate the features from different perspectives and make predictions based on the fused embedding.","Experiments demonstrate that MMoE achieves state-of-the-art performance on two widely-used spoiler detection datasets, surpassing previous SOTA methods by 2.56\\% and 8.41\\% in terms of accuracy and F1-score.","Further experiments also demonstrate MMoE's superiority in robustness and generalization."],"url":"http://arxiv.org/abs/2403.05265v1","category":"cs.AI"}
{"created":"2024-03-08 12:38:01","title":"Predicting burst events in a forced 2D flow: A wavelet-based analysis","abstract":"Predicting and perhaps mitigating against rare, extreme events in fluid flows is an important challenge. Due to the time-localized nature of these events, Fourier-based methods prove inefficient in capturing them. Instead this paper uses wavelet-based methods to understand the underlying patterns in a forced flow over a 2-torus which has intermittent high-energy burst events interrupting an ambient low energy quiet flow. Three methods are examined to predict burst events: (1) a straightforward energy tracking approach which acts as the benchmark; (2) a wavelet proper orthogonal decomposition (WPOD) which uncovers the key flow patterns seen in the quiet regions and the bursting episodes; and (3) a wavelet resolvent analysis (WRA) that reveals the forcing structures that amplifies the underlying flow patterns. Both the wavelet-based approaches succeed in producing better (earlier) predictions than a simple energy criterion, with the WRA-based technique always bettering WPOD. However, both also produce false positives and the improvement of the WRA approach over WPOD is not as substantial as anticipated. We conjecture that this is because the mechanism for the bursts in the flow studied is found to be largely modal associated with the unstable eigenfunction of the Navier-Stokes operator linearized around the mean flow. The forcing approach should deliver much better improvements over the WPOD approach for generically non-modal bursting mechanisms where there is a lag between the imposed forcing and the final response pattern.","sentences":["Predicting and perhaps mitigating against rare, extreme events in fluid flows is an important challenge.","Due to the time-localized nature of these events, Fourier-based methods prove inefficient in capturing them.","Instead this paper uses wavelet-based methods to understand the underlying patterns in a forced flow over a 2-torus which has intermittent high-energy burst events interrupting an ambient low energy quiet flow.","Three methods are examined to predict burst events: (1) a straightforward energy tracking approach which acts as the benchmark; (2) a wavelet proper orthogonal decomposition (WPOD) which uncovers the key flow patterns seen in the quiet regions and the bursting episodes; and (3) a wavelet resolvent analysis (WRA) that reveals the forcing structures that amplifies the underlying flow patterns.","Both the wavelet-based approaches succeed in producing better (earlier) predictions than a simple energy criterion, with the WRA-based technique always bettering WPOD.","However, both also produce false positives and the improvement of the WRA approach over WPOD is not as substantial as anticipated.","We conjecture that this is because the mechanism for the bursts in the flow studied is found to be largely modal associated with the unstable eigenfunction of the Navier-Stokes operator linearized around the mean flow.","The forcing approach should deliver much better improvements over the WPOD approach for generically non-modal bursting mechanisms where there is a lag between the imposed forcing and the final response pattern."],"url":"http://arxiv.org/abs/2403.05263v1","category":"physics.flu-dyn"}
{"created":"2024-03-08 12:35:07","title":"Debiasing Large Visual Language Models","abstract":"In the realms of computer vision and natural language processing, Large Vision-Language Models (LVLMs) have become indispensable tools, proficient in generating textual descriptions based on visual inputs. Despite their advancements, our investigation reveals a noteworthy bias in the generated content, where the output is primarily influenced by the underlying Large Language Models (LLMs) prior rather than the input image. Our empirical experiments underscore the persistence of this bias, as LVLMs often provide confident answers even in the absence of relevant images or given incongruent visual input. To rectify these biases and redirect the model's focus toward vision information, we introduce two simple, training-free strategies. Firstly, for tasks such as classification or multi-choice question-answering (QA), we propose a ``calibration'' step through affine transformation to adjust the output distribution. This ``Post-Hoc debias'' approach ensures uniform scores for each answer when the image is absent, serving as an effective regularization technique to alleviate the influence of LLM priors. For more intricate open-ended generation tasks, we extend this method to ``Debias sampling'', drawing inspirations from contrastive decoding methods. Furthermore, our investigation sheds light on the instability of LVLMs across various decoding configurations. Through systematic exploration of different settings, we significantly enhance performance, surpassing reported results and raising concerns about the fairness of existing evaluations. Comprehensive experiments substantiate the effectiveness of our proposed strategies in mitigating biases. These strategies not only prove beneficial in minimizing hallucinations but also contribute to the generation of more helpful and precise illustrations.","sentences":["In the realms of computer vision and natural language processing, Large Vision-Language Models (LVLMs) have become indispensable tools, proficient in generating textual descriptions based on visual inputs.","Despite their advancements, our investigation reveals a noteworthy bias in the generated content, where the output is primarily influenced by the underlying Large Language Models (LLMs) prior rather than the input image.","Our empirical experiments underscore the persistence of this bias, as LVLMs often provide confident answers even in the absence of relevant images or given incongruent visual input.","To rectify these biases and redirect the model's focus toward vision information, we introduce two simple, training-free strategies.","Firstly, for tasks such as classification or multi-choice question-answering (QA), we propose a ``calibration'' step through affine transformation to adjust the output distribution.","This ``Post-Hoc debias'' approach ensures uniform scores for each answer when the image is absent, serving as an effective regularization technique to alleviate the influence of LLM priors.","For more intricate open-ended generation tasks, we extend this method to ``Debias sampling'', drawing inspirations from contrastive decoding methods.","Furthermore, our investigation sheds light on the instability of LVLMs across various decoding configurations.","Through systematic exploration of different settings, we significantly enhance performance, surpassing reported results and raising concerns about the fairness of existing evaluations.","Comprehensive experiments substantiate the effectiveness of our proposed strategies in mitigating biases.","These strategies not only prove beneficial in minimizing hallucinations but also contribute to the generation of more helpful and precise illustrations."],"url":"http://arxiv.org/abs/2403.05262v1","category":"cs.CV"}
{"created":"2024-03-08 12:31:03","title":"Predicting Single-cell Drug Sensitivity by Adaptive Weighted Feature for Adversarial Multi-source Domain Adaptation","abstract":"The development of single-cell sequencing technology had promoted the generation of a large amount of single-cell transcriptional profiles, providing valuable opportunities to explore drug-resistant cell subpopulations in a tumor. However, the drug sensitivity data in single-cell level is still scarce to date, pressing an urgent and highly challenging task for computational prediction of the drug sensitivity to individual cells. This paper proposed scAdaDrug, a multi-source adaptive weighting model to predict single-cell drug sensitivity. We used an autoencoder to extract domain-invariant features related to drug sensitivity from multiple source domains by exploiting adversarial domain adaptation. Especially, we introduced an adaptive weight generator to produce importance-aware and mutual independent weights, which could adaptively modulate the embedding of each sample in dimension-level for both source and target domains. Extensive experimental results showed that our model achieved state-of-the-art performance in predicting drug sensitivity on sinle-cell datasets, as well as on cell line and patient datasets.","sentences":["The development of single-cell sequencing technology had promoted the generation of a large amount of single-cell transcriptional profiles, providing valuable opportunities to explore drug-resistant cell subpopulations in a tumor.","However, the drug sensitivity data in single-cell level is still scarce to date, pressing an urgent and highly challenging task for computational prediction of the drug sensitivity to individual cells.","This paper proposed scAdaDrug, a multi-source adaptive weighting model to predict single-cell drug sensitivity.","We used an autoencoder to extract domain-invariant features related to drug sensitivity from multiple source domains by exploiting adversarial domain adaptation.","Especially, we introduced an adaptive weight generator to produce importance-aware and mutual independent weights, which could adaptively modulate the embedding of each sample in dimension-level for both source and target domains.","Extensive experimental results showed that our model achieved state-of-the-art performance in predicting drug sensitivity on sinle-cell datasets, as well as on cell line and patient datasets."],"url":"http://arxiv.org/abs/2403.05260v1","category":"cs.AI"}
{"created":"2024-03-08 12:30:34","title":"Theory of Multimode Squeezed Light Generation in Lossy Media","abstract":"A unified theoretical approach to describe the properties of multimode squeezed light generated in a lossy medium is presented. This approach is valid for Markovian environments and includes both a model of discrete losses based on the beamsplitter approach and a generalized continuous loss model based on the spatial Langevin equation. For an important class of Gaussian states, we derive master equations for the second-order correlation functions and illustrate their solution for both frequency-independent and frequency-dependent losses. Studying the mode structure, we demonstrate that in a lossy environment no broadband basis without quadrature correlations between the different broadband modes exists. Therefore, various techniques and strategies to introduce broadband modes can be considered. We show that the Mercer expansion and the Williamson decomposition do not provide modes in which the maximal squeezing contained in the system can be measured. In turn, we find a new broadband basis that maximizes squeezing in the lossy system and present an algorithm to construct it.","sentences":["A unified theoretical approach to describe the properties of multimode squeezed light generated in a lossy medium is presented.","This approach is valid for Markovian environments and includes both a model of discrete losses based on the beamsplitter approach and a generalized continuous loss model based on the spatial Langevin equation.","For an important class of Gaussian states, we derive master equations for the second-order correlation functions and illustrate their solution for both frequency-independent and frequency-dependent losses.","Studying the mode structure, we demonstrate that in a lossy environment no broadband basis without quadrature correlations between the different broadband modes exists.","Therefore, various techniques and strategies to introduce broadband modes can be considered.","We show that the Mercer expansion and the Williamson decomposition do not provide modes in which the maximal squeezing contained in the system can be measured.","In turn, we find a new broadband basis that maximizes squeezing in the lossy system and present an algorithm to construct it."],"url":"http://arxiv.org/abs/2403.05259v1","category":"quant-ph"}
{"created":"2024-03-08 12:25:50","title":"Tautological characteristic classes II: the Witt class","abstract":"Let $K$ be an arbitrary infinite field. The cohomology group $H^2(SL(2,K), H_2\\,SL(2,K))$ contains the class of the universal central extension. When studying representations of fundamental groups of surfaces in $SL(2,K)$ it is useful to have classes stable under deformations (Fenchel--Nielsen twists) of representations. We identify the maximal quotient of the universal class which is stable under twists as the Witt class of Nekovar. The Milnor--Wood inequality asserts that an $SL(2,{\\bf R})$-bundle over a surface of genus $g$ admits a flat structure if and only if its Euler number is $\\leq (g-1)$. We establish an analog of this inequality, and a saturation result for the Witt class. The result is sharp for the field of rationals, but not sharp in general.","sentences":["Let $K$ be an arbitrary infinite field.","The cohomology group $H^2(SL(2,K), H_2\\,SL(2,K))$ contains the class of the universal central extension.","When studying representations of fundamental groups of surfaces in $SL(2,K)$ it is useful to have classes stable under deformations (Fenchel--Nielsen twists) of representations.","We identify the maximal quotient of the universal class which is stable under twists as the Witt class of Nekovar.","The Milnor--Wood inequality asserts that an $SL(2,{\\bf R})$-bundle over a surface of genus $g$ admits a flat structure if and only if its Euler number is $\\leq (g-1)$. We establish an analog of this inequality, and a saturation result for the Witt class.","The result is sharp for the field of rationals, but not sharp in general."],"url":"http://arxiv.org/abs/2403.05255v1","category":"math.KT"}
{"created":"2024-03-08 12:08:06","title":"Hide in Thicket: Generating Imperceptible and Rational Adversarial Perturbations on 3D Point Clouds","abstract":"Adversarial attack methods based on point manipulation for 3D point cloud classification have revealed the fragility of 3D models, yet the adversarial examples they produce are easily perceived or defended against. The trade-off between the imperceptibility and adversarial strength leads most point attack methods to inevitably introduce easily detectable outlier points upon a successful attack. Another promising strategy, shape-based attack, can effectively eliminate outliers, but existing methods often suffer significant reductions in imperceptibility due to irrational deformations. We find that concealing deformation perturbations in areas insensitive to human eyes can achieve a better trade-off between imperceptibility and adversarial strength, specifically in parts of the object surface that are complex and exhibit drastic curvature changes. Therefore, we propose a novel shape-based adversarial attack method, HiT-ADV, which initially conducts a two-stage search for attack regions based on saliency and imperceptibility scores, and then adds deformation perturbations in each attack region using Gaussian kernel functions. Additionally, HiT-ADV is extendable to physical attack. We propose that by employing benign resampling and benign rigid transformations, we can further enhance physical adversarial strength with little sacrifice to imperceptibility. Extensive experiments have validated the superiority of our method in terms of adversarial and imperceptible properties in both digital and physical spaces. Our code is avaliable at: https://github.com/TRLou/HiT-ADV.","sentences":["Adversarial attack methods based on point manipulation for 3D point cloud classification have revealed the fragility of 3D models, yet the adversarial examples they produce are easily perceived or defended against.","The trade-off between the imperceptibility and adversarial strength leads most point attack methods to inevitably introduce easily detectable outlier points upon a successful attack.","Another promising strategy, shape-based attack, can effectively eliminate outliers, but existing methods often suffer significant reductions in imperceptibility due to irrational deformations.","We find that concealing deformation perturbations in areas insensitive to human eyes can achieve a better trade-off between imperceptibility and adversarial strength, specifically in parts of the object surface that are complex and exhibit drastic curvature changes.","Therefore, we propose a novel shape-based adversarial attack method, HiT-ADV, which initially conducts a two-stage search for attack regions based on saliency and imperceptibility scores, and then adds deformation perturbations in each attack region using Gaussian kernel functions.","Additionally, HiT-ADV is extendable to physical attack.","We propose that by employing benign resampling and benign rigid transformations, we can further enhance physical adversarial strength with little sacrifice to imperceptibility.","Extensive experiments have validated the superiority of our method in terms of adversarial and imperceptible properties in both digital and physical spaces.","Our code is avaliable at: https://github.com/TRLou/HiT-ADV."],"url":"http://arxiv.org/abs/2403.05247v1","category":"cs.CV"}
{"created":"2024-03-08 12:07:18","title":"Noise Level Adaptive Diffusion Model for Robust Reconstruction of Accelerated MRI","abstract":"In general, diffusion model-based MRI reconstruction methods incrementally remove artificially added noise while imposing data consistency to reconstruct the underlying images. However, real-world MRI acquisitions already contain inherent noise due to thermal fluctuations. This phenomenon is particularly notable when using ultra-fast, high-resolution imaging sequences for advanced research, or using low-field systems favored by low- and middle-income countries. These common scenarios can lead to sub-optimal performance or complete failure of existing diffusion model-based reconstruction techniques. Specifically, as the artificially added noise is gradually removed, the inherent MRI noise becomes increasingly pronounced, making the actual noise level inconsistent with the predefined denoising schedule and consequently inaccurate image reconstruction. To tackle this problem, we propose a posterior sampling strategy with a novel NoIse Level Adaptive Data Consistency (Nila-DC) operation. Extensive experiments are conducted on two public datasets and an in-house clinical dataset with field strength ranging from 0.3T to 3T, showing that our method surpasses the state-of-the-art MRI reconstruction methods, and is highly robust against various noise levels. The code will be released after review.","sentences":["In general, diffusion model-based MRI reconstruction methods incrementally remove artificially added noise while imposing data consistency to reconstruct the underlying images.","However, real-world MRI acquisitions already contain inherent noise due to thermal fluctuations.","This phenomenon is particularly notable when using ultra-fast, high-resolution imaging sequences for advanced research, or using low-field systems favored by low- and middle-income countries.","These common scenarios can lead to sub-optimal performance or complete failure of existing diffusion model-based reconstruction techniques.","Specifically, as the artificially added noise is gradually removed, the inherent MRI noise becomes increasingly pronounced, making the actual noise level inconsistent with the predefined denoising schedule and consequently inaccurate image reconstruction.","To tackle this problem, we propose a posterior sampling strategy with a novel NoIse Level Adaptive Data Consistency (Nila-DC) operation.","Extensive experiments are conducted on two public datasets and an in-house clinical dataset with field strength ranging from 0.3T to 3T, showing that our method surpasses the state-of-the-art MRI reconstruction methods, and is highly robust against various noise levels.","The code will be released after review."],"url":"http://arxiv.org/abs/2403.05245v1","category":"eess.IV"}
{"created":"2024-03-08 12:04:58","title":"Inversion and Integral Identities in dCFTs","abstract":"This work derives an application from the identities of arXiv:hep-th/0602028 in order to invert four point functions in defect conformal field theories. For this, a recursion relation is established and the O(N) model with a line defect is considered as a testing ground of this application. Specifically, the CFT data are calculated from inversion of tilt and displacement four point functions. The recursion relation enables efficient computation of hypergeometrics at order $\\epsilon$ in the $\\epsilon$-expansion, leading to the inversion of four point functions and the derivation of CFT data. The inversion method presented offers a faster alternative to traditional approaches using arXiv:hep-ph/0507094v2, arXiv:0708.2443v2. The study also explores a general ansatz approach, assessing the algorithm's restrictiveness, and concludes by examining implications for the integral identity constraint of arXiv:2203.17157v2, predicting corrections to OPE coefficients.","sentences":["This work derives an application from the identities of arXiv:hep-th/0602028 in order to invert four point functions in defect conformal field theories.","For this, a recursion relation is established and the O(N) model with a line defect is considered as a testing ground of this application.","Specifically, the CFT data are calculated from inversion of tilt and displacement four point functions.","The recursion relation enables efficient computation of hypergeometrics at order $\\epsilon$ in the $\\epsilon$-expansion, leading to the inversion of four point functions and the derivation of CFT data.","The inversion method presented offers a faster alternative to traditional approaches using arXiv:hep-ph/0507094v2, arXiv:0708.2443v2.","The study also explores a general ansatz approach, assessing the algorithm's restrictiveness, and concludes by examining implications for the integral identity constraint of arXiv:2203.17157v2, predicting corrections to OPE coefficients."],"url":"http://arxiv.org/abs/2403.05243v1","category":"hep-th"}
{"created":"2024-03-08 12:03:12","title":"Black Holes and Marchenko-Pastur distribution","abstract":"The universal eigenvalue distribution characterizing the Gram matrix of semiclassical ensembles of black hole microstates is recognized as the Marchenko-Pastur distribution, which plays a prominent role as the universal limit distribution in a large class of random matrix and vector models. It is proposed that this distribution also universally determines the energy spectral density of black holes, which allows to construct a Krylov space for the time evolution of typical black hole states and calculate their state complexity. It is checked that the state complexity growth at late times saturates Lloyd's bound. Some implications of the proposed spectral density for the generation of Hawking radiation and black hole evaporation are discussed.","sentences":["The universal eigenvalue distribution characterizing the Gram matrix of semiclassical ensembles of black hole microstates is recognized as the Marchenko-Pastur distribution, which plays a prominent role as the universal limit distribution in a large class of random matrix and vector models.","It is proposed that this distribution also universally determines the energy spectral density of black holes, which allows to construct a Krylov space for the time evolution of typical black hole states and calculate their state complexity.","It is checked that the state complexity growth at late times saturates Lloyd's bound.","Some implications of the proposed spectral density for the generation of Hawking radiation and black hole evaporation are discussed."],"url":"http://arxiv.org/abs/2403.05241v1","category":"hep-th"}
{"created":"2024-03-08 11:59:46","title":"Seiberg-like duality for resolutions of determinantal varieties","abstract":"We study the genus-zero Gromov-Witten theory of two natural resolutions of determinantal varieties, termed the PAX and PAXY models. We realize each resolution as lying in a quiver bundle, and show that the respective quiver bundles are related by a quiver mutation. We prove that generating functions of genus-zero Gromov-Witten invariants for the two resolutions are related by a specific cluster change of variables. Along the way, we obtain a quantum Thom-Porteous formula for determinantal varieties and prove a Seiberg-like duality statement for certain quiver bundles.","sentences":["We study the genus-zero Gromov-Witten theory of two natural resolutions of determinantal varieties, termed the PAX and PAXY models.","We realize each resolution as lying in a quiver bundle, and show that the respective quiver bundles are related by a quiver mutation.","We prove that generating functions of genus-zero Gromov-Witten invariants for the two resolutions are related by a specific cluster change of variables.","Along the way, we obtain a quantum Thom-Porteous formula for determinantal varieties and prove a Seiberg-like duality statement for certain quiver bundles."],"url":"http://arxiv.org/abs/2403.05240v1","category":"math.AG"}
{"created":"2024-03-08 11:59:32","title":"Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation","abstract":"Vanilla text-to-image diffusion models struggle with generating accurate human images, commonly resulting in imperfect anatomies such as unnatural postures or disproportionate limbs.Existing methods address this issue mostly by fine-tuning the model with extra images or adding additional controls -- human-centric priors such as pose or depth maps -- during the image generation phase. This paper explores the integration of these human-centric priors directly into the model fine-tuning stage, essentially eliminating the need for extra conditions at the inference stage. We realize this idea by proposing a human-centric alignment loss to strengthen human-related information from the textual prompts within the cross-attention maps. To ensure semantic detail richness and human structural accuracy during fine-tuning, we introduce scale-aware and step-wise constraints within the diffusion process, according to an in-depth analysis of the cross-attention layer. Extensive experiments show that our method largely improves over state-of-the-art text-to-image models to synthesize high-quality human images based on user-written prompts. Project page: \\url{https://hcplayercvpr2024.github.io}.","sentences":["Vanilla text-to-image diffusion models struggle with generating accurate human images, commonly resulting in imperfect anatomies such as unnatural postures or disproportionate limbs.","Existing methods address this issue mostly by fine-tuning the model with extra images or adding additional controls -- human-centric priors such as pose or depth maps -- during the image generation phase.","This paper explores the integration of these human-centric priors directly into the model fine-tuning stage, essentially eliminating the need for extra conditions at the inference stage.","We realize this idea by proposing a human-centric alignment loss to strengthen human-related information from the textual prompts within the cross-attention maps.","To ensure semantic detail richness and human structural accuracy during fine-tuning, we introduce scale-aware and step-wise constraints within the diffusion process, according to an in-depth analysis of the cross-attention layer.","Extensive experiments show that our method largely improves over state-of-the-art text-to-image models to synthesize high-quality human images based on user-written prompts.","Project page: \\url{https://hcplayercvpr2024.github.io}."],"url":"http://arxiv.org/abs/2403.05239v1","category":"cs.CV"}
{"created":"2024-03-08 11:51:00","title":"Fairness-Aware Interpretable Modeling (FAIM) for Trustworthy Machine Learning in Healthcare","abstract":"The escalating integration of machine learning in high-stakes fields such as healthcare raises substantial concerns about model fairness. We propose an interpretable framework - Fairness-Aware Interpretable Modeling (FAIM), to improve model fairness without compromising performance, featuring an interactive interface to identify a \"fairer\" model from a set of high-performing models and promoting the integration of data-driven evidence and clinical expertise to enhance contextualized fairness. We demonstrated FAIM's value in reducing sex and race biases by predicting hospital admission with two real-world databases, MIMIC-IV-ED and SGH-ED. We show that for both datasets, FAIM models not only exhibited satisfactory discriminatory performance but also significantly mitigated biases as measured by well-established fairness metrics, outperforming commonly used bias-mitigation methods. Our approach demonstrates the feasibility of improving fairness without sacrificing performance and provides an a modeling mode that invites domain experts to engage, fostering a multidisciplinary effort toward tailored AI fairness.","sentences":["The escalating integration of machine learning in high-stakes fields such as healthcare raises substantial concerns about model fairness.","We propose an interpretable framework - Fairness-Aware Interpretable Modeling (FAIM), to improve model fairness without compromising performance, featuring an interactive interface to identify a \"fairer\" model from a set of high-performing models and promoting the integration of data-driven evidence and clinical expertise to enhance contextualized fairness.","We demonstrated FAIM's value in reducing sex and race biases by predicting hospital admission with two real-world databases, MIMIC-IV-ED and SGH-ED.","We show that for both datasets, FAIM models not only exhibited satisfactory discriminatory performance but also significantly mitigated biases as measured by well-established fairness metrics, outperforming commonly used bias-mitigation methods.","Our approach demonstrates the feasibility of improving fairness without sacrificing performance and provides an a modeling mode that invites domain experts to engage, fostering a multidisciplinary effort toward tailored AI fairness."],"url":"http://arxiv.org/abs/2403.05235v1","category":"cs.LG"}
{"created":"2024-03-08 11:46:58","title":"Evolving efficiency of the BRICS markets","abstract":"This paper investigates a time-varying version of weak-form market efficiency in the BRICS countries. A moving window test for sample autocorrelations is applied alongside a Kalman filter approach to recover the hidden dynamics of the market efficiency process through appropriate time-varying autoregressive models with both homoscedastic and heteroscedastic conditional variance. Monthly data covers the period from January 1995 to December 2020, which includes the 2008-2009 global financial crisis and the recent COVID-19 recession. The results reveal that all the BRICS stock markets were affected during both periods, but generally remained weak-form efficient, with the exception of China.","sentences":["This paper investigates a time-varying version of weak-form market efficiency in the BRICS countries.","A moving window test for sample autocorrelations is applied alongside a Kalman filter approach to recover the hidden dynamics of the market efficiency process through appropriate time-varying autoregressive models with both homoscedastic and heteroscedastic conditional variance.","Monthly data covers the period from January 1995 to December 2020, which includes the 2008-2009 global financial crisis and the recent COVID-19 recession.","The results reveal that all the BRICS stock markets were affected during both periods, but generally remained weak-form efficient, with the exception of China."],"url":"http://arxiv.org/abs/2403.05233v1","category":"math.OC"}
{"created":"2024-03-08 11:32:00","title":"Developing Federated Time-to-Event Scores Using Heterogeneous Real-World Survival Data","abstract":"Survival analysis serves as a fundamental component in numerous healthcare applications, where the determination of the time to specific events (such as the onset of a certain disease or death) for patients is crucial for clinical decision-making. Scoring systems are widely used for swift and efficient risk prediction. However, existing methods for constructing survival scores presume that data originates from a single source, posing privacy challenges in collaborations with multiple data owners. We propose a novel framework for building federated scoring systems for multi-site survival outcomes, ensuring both privacy and communication efficiency. We applied our approach to sites with heterogeneous survival data originating from emergency departments in Singapore and the United States. Additionally, we independently developed local scores at each site. In testing datasets from each participant site, our proposed federated scoring system consistently outperformed all local models, evidenced by higher integrated area under the receiver operating characteristic curve (iAUC) values, with a maximum improvement of 11.6%. Additionally, the federated score's time-dependent AUC(t) values showed advantages over local scores, exhibiting narrower confidence intervals (CIs) across most time points. The model developed through our proposed method exhibits effective performance on each local site, signifying noteworthy implications for healthcare research. Sites participating in our proposed federated scoring model training gained benefits by acquiring survival models with enhanced prediction accuracy and efficiency. This study demonstrates the effectiveness of our privacy-preserving federated survival score generation framework and its applicability to real-world heterogeneous survival data.","sentences":["Survival analysis serves as a fundamental component in numerous healthcare applications, where the determination of the time to specific events (such as the onset of a certain disease or death) for patients is crucial for clinical decision-making.","Scoring systems are widely used for swift and efficient risk prediction.","However, existing methods for constructing survival scores presume that data originates from a single source, posing privacy challenges in collaborations with multiple data owners.","We propose a novel framework for building federated scoring systems for multi-site survival outcomes, ensuring both privacy and communication efficiency.","We applied our approach to sites with heterogeneous survival data originating from emergency departments in Singapore and the United States.","Additionally, we independently developed local scores at each site.","In testing datasets from each participant site, our proposed federated scoring system consistently outperformed all local models, evidenced by higher integrated area under the receiver operating characteristic curve (iAUC) values, with a maximum improvement of 11.6%.","Additionally, the federated score's time-dependent AUC(t) values showed advantages over local scores, exhibiting narrower confidence intervals (CIs) across most time points.","The model developed through our proposed method exhibits effective performance on each local site, signifying noteworthy implications for healthcare research.","Sites participating in our proposed federated scoring model training gained benefits by acquiring survival models with enhanced prediction accuracy and efficiency.","This study demonstrates the effectiveness of our privacy-preserving federated survival score generation framework and its applicability to real-world heterogeneous survival data."],"url":"http://arxiv.org/abs/2403.05229v1","category":"cs.AI"}
{"created":"2024-03-08 11:28:51","title":"Trust Recognition in Human-Robot Cooperation Using EEG","abstract":"Collaboration between humans and robots is becoming increasingly crucial in our daily life. In order to accomplish efficient cooperation, trust recognition is vital, empowering robots to predict human behaviors and make trust-aware decisions. Consequently, there is an urgent need for a generalized approach to recognize human-robot trust. This study addresses this need by introducing an EEG-based method for trust recognition during human-robot cooperation. A human-robot cooperation game scenario is used to stimulate various human trust levels when working with robots. To enhance recognition performance, the study proposes an EEG Vision Transformer model coupled with a 3-D spatial representation to capture the spatial information of EEG, taking into account the topological relationship among electrodes. To validate this approach, a public EEG-based human trust dataset called EEGTrust is constructed. Experimental results indicate the effectiveness of the proposed approach, achieving an accuracy of 74.99% in slice-wise cross-validation and 62.00% in trial-wise cross-validation. This outperforms baseline models in both recognition accuracy and generalization. Furthermore, an ablation study demonstrates a significant improvement in trust recognition performance of the spatial representation. The source code and EEGTrust dataset are available at https://github.com/CaiyueXu/EEGTrust.","sentences":["Collaboration between humans and robots is becoming increasingly crucial in our daily life.","In order to accomplish efficient cooperation, trust recognition is vital, empowering robots to predict human behaviors and make trust-aware decisions.","Consequently, there is an urgent need for a generalized approach to recognize human-robot trust.","This study addresses this need by introducing an EEG-based method for trust recognition during human-robot cooperation.","A human-robot cooperation game scenario is used to stimulate various human trust levels when working with robots.","To enhance recognition performance, the study proposes an EEG Vision Transformer model coupled with a 3-D spatial representation to capture the spatial information of EEG, taking into account the topological relationship among electrodes.","To validate this approach, a public EEG-based human trust dataset called EEGTrust is constructed.","Experimental results indicate the effectiveness of the proposed approach, achieving an accuracy of 74.99% in slice-wise cross-validation and 62.00% in trial-wise cross-validation.","This outperforms baseline models in both recognition accuracy and generalization.","Furthermore, an ablation study demonstrates a significant improvement in trust recognition performance of the spatial representation.","The source code and EEGTrust dataset are available at https://github.com/CaiyueXu/EEGTrust."],"url":"http://arxiv.org/abs/2403.05225v1","category":"cs.HC"}
{"created":"2024-03-08 11:19:43","title":"Matching under Imperfectly Transferable Utility","abstract":"In this paper, we examine matching models with imperfectly transferable utility (ITU). We provide motivating examples, discuss the theoretical foundations of ITU matching models and present methods for estimating them. We also explore connected topics and provide an overview of the related literature. This paper has been submitted as a draft chapter for the Handbook of the Economics of Matching, edited by Che, Chiappori and Salani\\'e.","sentences":["In this paper, we examine matching models with imperfectly transferable utility (ITU).","We provide motivating examples, discuss the theoretical foundations of ITU matching models and present methods for estimating them.","We also explore connected topics and provide an overview of the related literature.","This paper has been submitted as a draft chapter for the Handbook of the Economics of Matching, edited by Che, Chiappori and Salani\\'e."],"url":"http://arxiv.org/abs/2403.05222v1","category":"econ.GN"}
{"created":"2024-03-08 11:18:27","title":"Understanding Hybrid Spaces: Designing a Spacetime Model to Represent Dynamic Topologies of Hybrid Spaces","abstract":"This paper develops a spatiotemporal model for the visualization of dynamic topologies of hybrid spaces. The visualization of spatiotemporal data is a well-known problem, for example in digital twins in urban planning. There is also a lack of a basic ontology for understanding hybrid spaces. The developed spatiotemporal model has three levels: a level of places and media types, a level of perception and a level of time and interaction. Existing concepts and types of representation of hybrid spaces are presented. The space-time model is tested on the basis of an art exhibition. Two hypotheses guide the accompanying online survey: (A) there are correlations between media use (modality), the participants' interactions (creativity) and their perception (understanding of art) and (B) individual parameters (demographic data, location and situation, individual knowledge) influence perception (understanding of art). The range, the number of interactions and the response rate were also evaluated.   The online survey generally showed a positive correlation between media use (modality) and individual activity (creativity). However, due to the low participation rate ($P_{TN} = 14$), the survey is unfortunately not very representative. Various dynamic topologies of hybrid spaces were successfully visualized. The joint representation of real and virtual places and media types conveys a new basic understanding of place, range and urban density. Relationships between modality, Mobility and communicative interaction become visible. The current phenomenon of multilocality has been successfully mapped. The space-time model enables more precise class and structure formation, for example in the development of digital twins. Dynamic topologies of hybrid spaces, such as in social media, at events or in urban development, can thus be better represented and compared.","sentences":["This paper develops a spatiotemporal model for the visualization of dynamic topologies of hybrid spaces.","The visualization of spatiotemporal data is a well-known problem, for example in digital twins in urban planning.","There is also a lack of a basic ontology for understanding hybrid spaces.","The developed spatiotemporal model has three levels: a level of places and media types, a level of perception and a level of time and interaction.","Existing concepts and types of representation of hybrid spaces are presented.","The space-time model is tested on the basis of an art exhibition.","Two hypotheses guide the accompanying online survey: (A) there are correlations between media use (modality), the participants' interactions (creativity) and their perception (understanding of art) and (B) individual parameters (demographic data, location and situation, individual knowledge) influence perception (understanding of art).","The range, the number of interactions and the response rate were also evaluated.   ","The online survey generally showed a positive correlation between media use (modality) and individual activity (creativity).","However, due to the low participation rate ($P_{TN} = 14$), the survey is unfortunately not very representative.","Various dynamic topologies of hybrid spaces were successfully visualized.","The joint representation of real and virtual places and media types conveys a new basic understanding of place, range and urban density.","Relationships between modality, Mobility and communicative interaction become visible.","The current phenomenon of multilocality has been successfully mapped.","The space-time model enables more precise class and structure formation, for example in the development of digital twins.","Dynamic topologies of hybrid spaces, such as in social media, at events or in urban development, can thus be better represented and compared."],"url":"http://arxiv.org/abs/2403.05221v1","category":"cs.CY"}
{"created":"2024-03-08 11:18:26","title":"Synthetic Privileged Information Enhances Medical Image Representation Learning","abstract":"Multimodal self-supervised representation learning has consistently proven to be a highly effective method in medical image analysis, offering strong task performance and producing biologically informed insights. However, these methods heavily rely on large, paired datasets, which is prohibitive for their use in scenarios where paired data does not exist, or there is only a small amount available. In contrast, image generation methods can work well on very small datasets, and can find mappings between unpaired datasets, meaning an effectively unlimited amount of paired synthetic data can be generated. In this work, we demonstrate that representation learning can be significantly improved by synthetically generating paired information, both compared to training on either single-modality (up to 4.4x error reduction) or authentic multi-modal paired datasets (up to 5.6x error reduction).","sentences":["Multimodal self-supervised representation learning has consistently proven to be a highly effective method in medical image analysis, offering strong task performance and producing biologically informed insights.","However, these methods heavily rely on large, paired datasets, which is prohibitive for their use in scenarios where paired data does not exist, or there is only a small amount available.","In contrast, image generation methods can work well on very small datasets, and can find mappings between unpaired datasets, meaning an effectively unlimited amount of paired synthetic data can be generated.","In this work, we demonstrate that representation learning can be significantly improved by synthetically generating paired information, both compared to training on either single-modality (up to 4.4x error reduction) or authentic multi-modal paired datasets (up to 5.6x error reduction)."],"url":"http://arxiv.org/abs/2403.05220v1","category":"cs.CV"}
{"created":"2024-03-08 11:09:46","title":"3D Face Reconstruction Using A Spectral-Based Graph Convolution Encoder","abstract":"Monocular 3D face reconstruction plays a crucial role in avatar generation, with significant demand in web-related applications such as generating virtual financial advisors in FinTech. Current reconstruction methods predominantly rely on deep learning techniques and employ 2D self-supervision as a means to guide model learning. However, these methods encounter challenges in capturing the comprehensive 3D structural information of the face due to the utilization of 2D images for model training purposes. To overcome this limitation and enhance the reconstruction of 3D structural features, we propose an innovative approach that integrates existing 2D features with 3D features to guide the model learning process. Specifically, we introduce the 3D-ID Loss, which leverages the high-dimensional structure features extracted from a Spectral-Based Graph Convolution Encoder applied to the facial mesh. This approach surpasses the sole reliance on the 3D information provided by the facial mesh vertices coordinates. Our model is trained using 2D-3D data pairs from a combination of datasets and achieves state-of-the-art performance on the NoW benchmark.","sentences":["Monocular 3D face reconstruction plays a crucial role in avatar generation, with significant demand in web-related applications such as generating virtual financial advisors in FinTech.","Current reconstruction methods predominantly rely on deep learning techniques and employ 2D self-supervision as a means to guide model learning.","However, these methods encounter challenges in capturing the comprehensive 3D structural information of the face due to the utilization of 2D images for model training purposes.","To overcome this limitation and enhance the reconstruction of 3D structural features, we propose an innovative approach that integrates existing 2D features with 3D features to guide the model learning process.","Specifically, we introduce the 3D-ID Loss, which leverages the high-dimensional structure features extracted from a Spectral-Based Graph Convolution Encoder applied to the facial mesh.","This approach surpasses the sole reliance on the 3D information provided by the facial mesh vertices coordinates.","Our model is trained using 2D-3D data pairs from a combination of datasets and achieves state-of-the-art performance on the NoW benchmark."],"url":"http://arxiv.org/abs/2403.05218v1","category":"cs.CV"}
{"created":"2024-03-08 11:09:13","title":"Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering","abstract":"Open-domain question answering (ODQA) has emerged as a pivotal research spotlight in information systems. Existing methods follow two main paradigms to collect evidence: (1) The \\textit{retrieve-then-read} paradigm retrieves pertinent documents from an external corpus; and (2) the \\textit{generate-then-read} paradigm employs large language models (LLMs) to generate relevant documents. However, neither can fully address multifaceted requirements for evidence. To this end, we propose LLMQA, a generalized framework that formulates the ODQA process into three basic steps: query expansion, document selection, and answer generation, combining the superiority of both retrieval-based and generation-based evidence. Since LLMs exhibit their excellent capabilities to accomplish various tasks, we instruct LLMs to play multiple roles as generators, rerankers, and evaluators within our framework, integrating them to collaborate in the ODQA process. Furthermore, we introduce a novel prompt optimization algorithm to refine role-playing prompts and steer LLMs to produce higher-quality evidence and answers. Extensive experimental results on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate that LLMQA achieves the best performance in terms of both answer accuracy and evidence quality, showcasing its potential for advancing ODQA research and applications.","sentences":["Open-domain question answering (ODQA) has emerged as a pivotal research spotlight in information systems.","Existing methods follow two main paradigms to collect evidence: (1) The \\textit{retrieve-then-read} paradigm retrieves pertinent documents from an external corpus; and (2) the \\textit{generate-then-read} paradigm employs large language models (LLMs) to generate relevant documents.","However, neither can fully address multifaceted requirements for evidence.","To this end, we propose LLMQA, a generalized framework that formulates the ODQA process into three basic steps: query expansion, document selection, and answer generation, combining the superiority of both retrieval-based and generation-based evidence.","Since LLMs exhibit their excellent capabilities to accomplish various tasks, we instruct LLMs to play multiple roles as generators, rerankers, and evaluators within our framework, integrating them to collaborate in the ODQA process.","Furthermore, we introduce a novel prompt optimization algorithm to refine role-playing prompts and steer LLMs to produce higher-quality evidence and answers.","Extensive experimental results on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate that LLMQA achieves the best performance in terms of both answer accuracy and evidence quality, showcasing its potential for advancing ODQA research and applications."],"url":"http://arxiv.org/abs/2403.05217v1","category":"cs.CL"}
{"created":"2024-03-08 10:55:43","title":"AQuA: Automated Question-Answering in Software Tutorial Videos with Visual Anchors","abstract":"Tutorial videos are a popular help source for learning feature-rich software. However, getting quick answers to questions about tutorial videos is difficult. We present an automated approach for responding to tutorial questions. By analyzing 633 questions found in 5,944 video comments, we identified different question types and observed that users frequently described parts of the video in questions. We then asked participants (N=24) to watch tutorial videos and ask questions while annotating the video with relevant visual anchors. Most visual anchors referred to UI elements and the application workspace. Based on these insights, we built AQuA, a pipeline that generates useful answers to questions with visual anchors. We demonstrate this for Fusion 360, showing that we can recognize UI elements in visual anchors and generate answers using GPT-4 augmented with that visual information and software documentation. An evaluation study (N=16) demonstrates that our approach provides better answers than baseline methods.","sentences":["Tutorial videos are a popular help source for learning feature-rich software.","However, getting quick answers to questions about tutorial videos is difficult.","We present an automated approach for responding to tutorial questions.","By analyzing 633 questions found in 5,944 video comments, we identified different question types and observed that users frequently described parts of the video in questions.","We then asked participants (N=24) to watch tutorial videos and ask questions while annotating the video with relevant visual anchors.","Most visual anchors referred to UI elements and the application workspace.","Based on these insights, we built AQuA, a pipeline that generates useful answers to questions with visual anchors.","We demonstrate this for Fusion 360, showing that we can recognize UI elements in visual anchors and generate answers using GPT-4 augmented with that visual information and software documentation.","An evaluation study (N=16) demonstrates that our approach provides better answers than baseline methods."],"url":"http://arxiv.org/abs/2403.05213v1","category":"cs.HC"}
{"created":"2024-03-08 10:49:37","title":"Overcoming Data Inequality across Domains with Semi-Supervised Domain Generalization","abstract":"While there have been considerable advancements in machine learning driven by extensive datasets, a significant disparity still persists in the availability of data across various sources and populations. This inequality across domains poses challenges in modeling for those with limited data, which can lead to profound practical and ethical concerns. In this paper, we address a representative case of data inequality problem across domains termed Semi-Supervised Domain Generalization (SSDG), in which only one domain is labeled while the rest are unlabeled. We propose a novel algorithm, ProUD, which can effectively learn domain-invariant features via domain-aware prototypes along with progressive generalization via uncertainty-adaptive mixing of labeled and unlabeled domains. Our experiments on three different benchmark datasets demonstrate the effectiveness of ProUD, outperforming all baseline models including single domain generalization and semi-supervised learning. Source code will be released upon acceptance of the paper.","sentences":["While there have been considerable advancements in machine learning driven by extensive datasets, a significant disparity still persists in the availability of data across various sources and populations.","This inequality across domains poses challenges in modeling for those with limited data, which can lead to profound practical and ethical concerns.","In this paper, we address a representative case of data inequality problem across domains termed Semi-Supervised Domain Generalization (SSDG), in which only one domain is labeled while the rest are unlabeled.","We propose a novel algorithm, ProUD, which can effectively learn domain-invariant features via domain-aware prototypes along with progressive generalization via uncertainty-adaptive mixing of labeled and unlabeled domains.","Our experiments on three different benchmark datasets demonstrate the effectiveness of ProUD, outperforming all baseline models including single domain generalization and semi-supervised learning.","Source code will be released upon acceptance of the paper."],"url":"http://arxiv.org/abs/2403.05209v1","category":"cs.LG"}
{"created":"2024-03-08 10:40:21","title":"Spatial Variations and Breaks in the Optical-NIR spectra of the Pulsar and PWN in SNR 0540-69.3","abstract":"The supernova remnant SNR 0540-69.3, twin of the Crab Nebula, offers an excellent opportunity to study the continuum emission from a young pulsar and pulsar-wind nebula (PWN). We present observations taken with the VLT instruments MUSE and X-shooter in the wavelength range 3000-25,000 \\r{A}, which allow us to study spatial variations of the optical spectra, along with the first near-infrared (NIR) spectrum of the source. We model the optical spectra with a power law (PL) $F_\\nu\\propto\\nu^{-\\alpha}$ and find clear spatial variations (including a torus-jet structure) in the spectral index across the PWN. Generally, we find spectral hardening toward the outer parts, from $\\alpha\\sim1.1$ to $\\sim0.1$, which may indicate particle reacceleration by the PWN shock at the inner edge of the ejecta or alternatively time variability of the pulsar wind. The optical-NIR spectrum of the PWN is best described by a broken PL, confirming that several breaks are needed to model the full spectral energy distribution of the PWN, suggesting the presence of more than one particle population. Finally, subtracting the PWN contribution from the pulsar spectrum we find that the spectrum is best described with a broken-PL model with a flat and a positive spectral index, in contrast to the Crab pulsar that has a negative spectral index and no break in the optical. This might imply that pulsar differences propagate to the PWN spectra.","sentences":["The supernova remnant SNR 0540-69.3, twin of the Crab Nebula, offers an excellent opportunity to study the continuum emission from a young pulsar and pulsar-wind nebula (PWN).","We present observations taken with the VLT instruments MUSE and X-shooter in the wavelength range 3000-25,000 \\r{A}, which allow us to study spatial variations of the optical spectra, along with the first near-infrared (NIR) spectrum of the source.","We model the optical spectra with a power law (PL) $F_\\nu\\propto\\nu^{-\\alpha}$ and find clear spatial variations (including a torus-jet structure) in the spectral index across the PWN.","Generally, we find spectral hardening toward the outer parts, from $\\alpha\\sim1.1$ to $\\sim0.1$, which may indicate particle reacceleration by the PWN shock at the inner edge of the ejecta or alternatively time variability of the pulsar wind.","The optical-NIR spectrum of the PWN is best described by a broken PL, confirming that several breaks are needed to model the full spectral energy distribution of the PWN, suggesting the presence of more than one particle population.","Finally, subtracting the PWN contribution from the pulsar spectrum we find that the spectrum is best described with a broken-PL model with a flat and a positive spectral index, in contrast to the Crab pulsar that has a negative spectral index and no break in the optical.","This might imply that pulsar differences propagate to the PWN spectra."],"url":"http://arxiv.org/abs/2403.05206v1","category":"astro-ph.HE"}
{"created":"2024-03-08 10:24:10","title":"Time changed spherical Brownian motions with longitudinal drifts","abstract":"In this paper, we consider the time change of the diffusion process on the 2-dimensional unit sphere generated by the Laplace-Beltrami operator, perturbed by means of a longitudinal vector field. First, this is done by addressing the problem of finding strong solutions to suitable time-nonlocal Kolmogorov equations, via a spectral decomposition approach. Next, the desired process is constructed as the composition of the aforementioned diffusion process and the inverse of a subordinator, and it is used to provide a stochastic representation of the solution of the involved time-nonlocal Kolmogorov equation, which in turn leads to the spectral decomposition of its probability density function. A family of operators induced by the process is then adopted to provide very weak solutions of the same time-nonlocal Kolmogorov equation with much less regular initial data. From the spectral decomposition results we also get some bounds on the speed of convergence to the stationary state, proving that the process can be considered an anomalous diffusion. These results improve some known ones in terms of both the presence of a perturbation and the lack of regularity of the initial data.","sentences":["In this paper, we consider the time change of the diffusion process on the 2-dimensional unit sphere generated by the Laplace-Beltrami operator, perturbed by means of a longitudinal vector field.","First, this is done by addressing the problem of finding strong solutions to suitable time-nonlocal Kolmogorov equations, via a spectral decomposition approach.","Next, the desired process is constructed as the composition of the aforementioned diffusion process and the inverse of a subordinator, and it is used to provide a stochastic representation of the solution of the involved time-nonlocal Kolmogorov equation, which in turn leads to the spectral decomposition of its probability density function.","A family of operators induced by the process is then adopted to provide very weak solutions of the same time-nonlocal Kolmogorov equation with much less regular initial data.","From the spectral decomposition results we also get some bounds on the speed of convergence to the stationary state, proving that the process can be considered an anomalous diffusion.","These results improve some known ones in terms of both the presence of a perturbation and the lack of regularity of the initial data."],"url":"http://arxiv.org/abs/2403.05202v1","category":"math.PR"}
{"created":"2024-03-08 10:22:28","title":"MarkupLens: An AI-Powered Tool to Support Designers in Video-Based Analysis at Scale","abstract":"Video-Based Design (VBD) is a design methodology that utilizes video as a primary tool for understanding user interactions, prototyping, and conducting research to enhance the design process. Artificial Intelligence (AI) can be instrumental in video-based design by analyzing and interpreting visual data from videos to enhance user interaction, automate design processes, and improve product functionality. In this study, we explore how AI can enhance professional video-based design with a State-of-the-Art (SOTA) deep learning model. We developed a prototype annotation platform (MarkupLens) and conducted a between-subjects eye-tracking study with 36 designers, annotating videos with three levels of AI assistance. Our findings indicate that MarkupLens improved design annotation quality and productivity. Additionally, it reduced the cognitive load that designers exhibited and enhanced their User Experience (UX). We believe that designer-AI collaboration can greatly enhance the process of eliciting insights in video-based design.","sentences":["Video-Based Design (VBD) is a design methodology that utilizes video as a primary tool for understanding user interactions, prototyping, and conducting research to enhance the design process.","Artificial Intelligence (AI) can be instrumental in video-based design by analyzing and interpreting visual data from videos to enhance user interaction, automate design processes, and improve product functionality.","In this study, we explore how AI can enhance professional video-based design with a State-of-the-Art (SOTA) deep learning model.","We developed a prototype annotation platform (MarkupLens) and conducted a between-subjects eye-tracking study with 36 designers, annotating videos with three levels of AI assistance.","Our findings indicate that MarkupLens improved design annotation quality and productivity.","Additionally, it reduced the cognitive load that designers exhibited and enhanced their User Experience (UX).","We believe that designer-AI collaboration can greatly enhance the process of eliciting insights in video-based design."],"url":"http://arxiv.org/abs/2403.05201v1","category":"cs.HC"}
{"created":"2024-03-08 10:19:34","title":"Efficient Algorithms for Personalized PageRank Computation: A Survey","abstract":"Personalized PageRank (PPR) is a traditional measure for node proximity on large graphs. For a pair of nodes $s$ and $t$, the PPR value $\\pi_s(t)$ equals the probability that an $\\alpha$-discounted random walk from $s$ terminates at $t$ and reflects the importance between $s$ and $t$ in a bidirectional way. As a generalization of Google's celebrated PageRank centrality, PPR has been extensively studied and has found multifaceted applications in many fields, such as network analysis, graph mining, and graph machine learning. Despite numerous studies devoted to PPR over the decades, efficient computation of PPR remains a challenging problem, and there is a dearth of systematic summaries and comparisons of existing algorithms. In this paper, we recap several frequently used techniques for PPR computation and conduct a comprehensive survey of various recent PPR algorithms from an algorithmic perspective. We classify these approaches based on the types of queries they address and review their methodologies and contributions. We also discuss some representative algorithms for computing PPR on dynamic graphs and in parallel or distributed environments.","sentences":["Personalized PageRank (PPR) is a traditional measure for node proximity on large graphs.","For a pair of nodes $s$ and $t$, the PPR value $\\pi_s(t)$ equals the probability that an $\\alpha$-discounted random walk from $s$ terminates at $t$ and reflects the importance between $s$ and $t$ in a bidirectional way.","As a generalization of Google's celebrated PageRank centrality, PPR has been extensively studied and has found multifaceted applications in many fields, such as network analysis, graph mining, and graph machine learning.","Despite numerous studies devoted to PPR over the decades, efficient computation of PPR remains a challenging problem, and there is a dearth of systematic summaries and comparisons of existing algorithms.","In this paper, we recap several frequently used techniques for PPR computation and conduct a comprehensive survey of various recent PPR algorithms from an algorithmic perspective.","We classify these approaches based on the types of queries they address and review their methodologies and contributions.","We also discuss some representative algorithms for computing PPR on dynamic graphs and in parallel or distributed environments."],"url":"http://arxiv.org/abs/2403.05198v1","category":"cs.DS"}
{"created":"2024-03-08 10:19:04","title":"Generic ETH: Eigenstate Thermalization beyond the Microcanonical","abstract":"The Eigenstate Thermalization Hypothesis (ETH) has played a key role in recent advances in the high energy and condensed matter communities. It explains how an isolated quantum system in a far-from-equilibrium initial state can evolve to a state that is indistinguishable from thermal equilibrium, with observables relaxing to almost time-independent results that can be described using traditional statistical mechanics ensembles. In this work we probe the limits of ETH, pushing it outside its prototypical applications in several directions. We design a qutrit lattice system with conserved quasilocal charge, in which we verify a form of generalized eigenstate thermalization. We also observe signatures of thermalization in states well outside microcanonical windows of both charge and energy, which we dub `generic ETH.'","sentences":["The Eigenstate Thermalization Hypothesis (ETH) has played a key role in recent advances in the high energy and condensed matter communities.","It explains how an isolated quantum system in a far-from-equilibrium initial state can evolve to a state that is indistinguishable from thermal equilibrium, with observables relaxing to almost time-independent results that can be described using traditional statistical mechanics ensembles.","In this work we probe the limits of ETH, pushing it outside its prototypical applications in several directions.","We design a qutrit lattice system with conserved quasilocal charge, in which we verify a form of generalized eigenstate thermalization.","We also observe signatures of thermalization in states well outside microcanonical windows of both charge and energy, which we dub `generic ETH.'"],"url":"http://arxiv.org/abs/2403.05197v1","category":"quant-ph"}
{"created":"2024-03-08 10:19:00","title":"Denoising Autoregressive Representation Learning","abstract":"In this paper, we explore a new generative approach for learning visual representations. Our method, DARL, employs a decoder-only Transformer to predict image patches autoregressively. We find that training with Mean Squared Error (MSE) alone leads to strong representations. To enhance the image generation ability, we replace the MSE loss with the diffusion objective by using a denoising patch decoder. We show that the learned representation can be improved by using tailored noise schedules and longer training in larger models. Notably, the optimal schedule differs significantly from the typical ones used in standard image diffusion models. Overall, despite its simple architecture, DARL delivers performance remarkably close to state-of-the-art masked prediction models under the fine-tuning protocol. This marks an important step towards a unified model capable of both visual perception and generation, effectively combining the strengths of autoregressive and denoising diffusion models.","sentences":["In this paper, we explore a new generative approach for learning visual representations.","Our method, DARL, employs a decoder-only Transformer to predict image patches autoregressively.","We find that training with Mean Squared Error (MSE) alone leads to strong representations.","To enhance the image generation ability, we replace the MSE loss with the diffusion objective by using a denoising patch decoder.","We show that the learned representation can be improved by using tailored noise schedules and longer training in larger models.","Notably, the optimal schedule differs significantly from the typical ones used in standard image diffusion models.","Overall, despite its simple architecture, DARL delivers performance remarkably close to state-of-the-art masked prediction models under the fine-tuning protocol.","This marks an important step towards a unified model capable of both visual perception and generation, effectively combining the strengths of autoregressive and denoising diffusion models."],"url":"http://arxiv.org/abs/2403.05196v1","category":"cs.LG"}
{"created":"2024-03-08 10:17:50","title":"Celestial $w_{1+\\infty}$ charges and the subleading structure of asymptotically-flat spacetimes","abstract":"We study the subleading structure of asymptotically-flat spacetimes and its relationship to the $w_{1+\\infty}$ loop algebra of higher spin charges. We do so using both the Bondi-Sachs and the Newman-Penrose formalism, via a dictionary built from a preferred choice of tetrad. This enables us to access properties of the so-called higher Bondi aspects, such as their evolution equations, their transformation laws under asymptotic symmetries, and their relationship to the Newman-Penrose and the higher spin charges. By studying the recursive Einstein evolution equations defining these higher spin charges, we derive the general form of their transformation behavior under BMSW symmetries. This leads to an immediate proof that the spin 0,1 and spin $s$ brackets reproduce upon linearization the structure expected from the $w_{1+\\infty}$ algebra. We then define renormalized higher spin charges which are conserved in the radiative vacuum at quadratic order, and show that they satisfy for all spins the $w_{1+\\infty}$ algebra at linear order in the radiative data.","sentences":["We study the subleading structure of asymptotically-flat spacetimes and its relationship to the $w_{1+\\infty}$ loop algebra of higher spin charges.","We do so using both the Bondi-Sachs and the Newman-Penrose formalism, via a dictionary built from a preferred choice of tetrad.","This enables us to access properties of the so-called higher Bondi aspects, such as their evolution equations, their transformation laws under asymptotic symmetries, and their relationship to the Newman-Penrose and the higher spin charges.","By studying the recursive Einstein evolution equations defining these higher spin charges, we derive the general form of their transformation behavior under BMSW symmetries.","This leads to an immediate proof that the spin 0,1 and spin $s$ brackets reproduce upon linearization the structure expected from the $w_{1+\\infty}$ algebra.","We then define renormalized higher spin charges which are conserved in the radiative vacuum at quadratic order, and show that they satisfy for all spins the $w_{1+\\infty}$ algebra at linear order in the radiative data."],"url":"http://arxiv.org/abs/2403.05195v1","category":"hep-th"}
{"created":"2024-03-08 10:14:40","title":"Evaluation of Road User Radio-Frequency Exposure Levels in an Urban Environment from Vehicular Antennas and the Infrastructure in ITS-G5 5.9 GHz Communication","abstract":"This study aims to investigate the variability of exposure levels among road users generated in a realistic urban scenario by Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communication technologies operating at 5.9 GHz. The exposure levels were evaluated in terms of whole-body Specific Absorption Rate (wbSAR) [W/kg] in three different human models, ranging from children to adults. We calculated the electromagnetic field exposure level generated by V2V and V2I using raytracing and we assessed wbSAR resulting in urban exposure scenarios with an increasing number of transmitting antennas. Whole-body SAR was generally very low, on the order of 10^-4 W/kg. The maximum wbSAR, of 4.9x10^-4 W/kg, was obtained in the worst-case exposure condition comprising more than one transmitting vehicle and was found in the adult model for a distance within 10 m from the transmitting cars. We found that the height of the human model highly impacted the exposure level. Namely, the child (which is the shortest human model) was generally much less exposed than adults. All the wbSAR values found by varying the number of transmitting antennas, the distance of the road user from the antennas, and the type of human model (adult vs. child) were very well below the limits set by the ICNIRP and IEEE guidelines of 0.08 W/kg for human exposure in the 100 kHz - 300 GHz range.","sentences":["This study aims to investigate the variability of exposure levels among road users generated in a realistic urban scenario by Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communication technologies operating at 5.9 GHz.","The exposure levels were evaluated in terms of whole-body Specific Absorption Rate (wbSAR)","[W/kg] in three different human models, ranging from children to adults.","We calculated the electromagnetic field exposure level generated by V2V and V2I using raytracing and we assessed wbSAR resulting in urban exposure scenarios with an increasing number of transmitting antennas.","Whole-body SAR was generally very low, on the order of 10^-4 W/kg.","The maximum wbSAR, of 4.9x10^-4 W/kg, was obtained in the worst-case exposure condition comprising more than one transmitting vehicle and was found in the adult model for a distance within 10 m from the transmitting cars.","We found that the height of the human model highly impacted the exposure level.","Namely, the child (which is the shortest human model) was generally much less exposed than adults.","All the wbSAR values found by varying the number of transmitting antennas, the distance of the road user from the antennas, and the type of human model (adult vs. child) were very well below the limits set by the ICNIRP and IEEE guidelines of 0.08 W/kg for human exposure in the 100 kHz - 300 GHz range."],"url":"http://arxiv.org/abs/2403.05193v1","category":"cs.NI"}
{"created":"2024-03-08 10:09:57","title":"Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge","abstract":"Acquiring factual knowledge for language models (LMs) in low-resource languages poses a serious challenge, thus resorting to cross-lingual transfer in multilingual LMs (ML-LMs). In this study, we ask how ML-LMs acquire and represent factual knowledge. Using the multilingual factual knowledge probing dataset, mLAMA, we first conducted a neuron investigation of ML-LMs (specifically, multilingual BERT). We then traced the roots of facts back to the knowledge source (Wikipedia) to identify the ways in which ML-LMs acquire specific facts. We finally identified three patterns of acquiring and representing facts in ML-LMs: language-independent, cross-lingual shared and transferred, and devised methods for differentiating them. Our findings highlight the challenge of maintaining consistent factual knowledge across languages, underscoring the need for better fact representation learning in ML-LMs.","sentences":["Acquiring factual knowledge for language models (LMs) in low-resource languages poses a serious challenge, thus resorting to cross-lingual transfer in multilingual LMs (ML-LMs).","In this study, we ask how ML-LMs acquire and represent factual knowledge.","Using the multilingual factual knowledge probing dataset, mLAMA, we first conducted a neuron investigation of ML-LMs (specifically, multilingual BERT).","We then traced the roots of facts back to the knowledge source (Wikipedia) to identify the ways in which ML-LMs acquire specific facts.","We finally identified three patterns of acquiring and representing facts in ML-LMs: language-independent, cross-lingual shared and transferred, and devised methods for differentiating them.","Our findings highlight the challenge of maintaining consistent factual knowledge across languages, underscoring the need for better fact representation learning in ML-LMs."],"url":"http://arxiv.org/abs/2403.05189v1","category":"cs.CL"}
{"created":"2024-03-08 09:56:45","title":"CommitBench: A Benchmark for Commit Message Generation","abstract":"Writing commit messages is a tedious daily task for many software developers, and often remains neglected. Automating this task has the potential to save time while ensuring that messages are informative. A high-quality dataset and an objective benchmark are vital preconditions for solid research and evaluation towards this goal. We show that existing datasets exhibit various problems, such as the quality of the commit selection, small sample sizes, duplicates, privacy issues, and missing licenses for redistribution. This can lead to unusable models and skewed evaluations, where inferior models achieve higher evaluation scores due to biases in the data. We compile a new large-scale dataset, CommitBench, adopting best practices for dataset creation. We sample commits from diverse projects with licenses that permit redistribution and apply our filtering and dataset enhancements to improve the quality of generated commit messages. We use CommitBench to compare existing models and show that other approaches are outperformed by a Transformer model pretrained on source code. We hope to accelerate future research by publishing the source code( https://github.com/Maxscha/commitbench ).","sentences":["Writing commit messages is a tedious daily task for many software developers, and often remains neglected.","Automating this task has the potential to save time while ensuring that messages are informative.","A high-quality dataset and an objective benchmark are vital preconditions for solid research and evaluation towards this goal.","We show that existing datasets exhibit various problems, such as the quality of the commit selection, small sample sizes, duplicates, privacy issues, and missing licenses for redistribution.","This can lead to unusable models and skewed evaluations, where inferior models achieve higher evaluation scores due to biases in the data.","We compile a new large-scale dataset, CommitBench, adopting best practices for dataset creation.","We sample commits from diverse projects with licenses that permit redistribution and apply our filtering and dataset enhancements to improve the quality of generated commit messages.","We use CommitBench to compare existing models and show that other approaches are outperformed by a Transformer model pretrained on source code.","We hope to accelerate future research by publishing the source code( https://github.com/Maxscha/commitbench )."],"url":"http://arxiv.org/abs/2403.05188v1","category":"cs.CL"}
{"created":"2024-03-08 09:55:07","title":"Robust Semantic Communications for Speech-to-Text Translation","abstract":"In this paper, we propose a robust semantic communication system to achieve the speech-to-text translation task, named Ross-S2T, by delivering the essential semantic information. Particularly, a deep semantic encoder is developed to directly condense and convert the speech in the source language to the textual semantic features associated with the target language, thus encouraging the design of a deep learning-enabled semantic communication system for speech-to-text translation that can be jointly trained in an end-to-end manner. Moreover, to cope with the practical communication scenario when the input speech is corrupted, a novel generative adversarial network (GAN)-enabled deep semantic compensator is proposed to predict the lost semantic information in the source speech and produce the textual semantic features in the target language simultaneously, which establishes a robust semantic transmission mechanism for dynamic speech input. According to the simulation results, the proposed Ross-S2T achieves significant speech-to-text translation performance compared to the conventional approach and exhibits high robustness against the corrupted speech input.","sentences":["In this paper, we propose a robust semantic communication system to achieve the speech-to-text translation task, named Ross-S2T, by delivering the essential semantic information.","Particularly, a deep semantic encoder is developed to directly condense and convert the speech in the source language to the textual semantic features associated with the target language, thus encouraging the design of a deep learning-enabled semantic communication system for speech-to-text translation that can be jointly trained in an end-to-end manner.","Moreover, to cope with the practical communication scenario when the input speech is corrupted, a novel generative adversarial network (GAN)-enabled deep semantic compensator is proposed to predict the lost semantic information in the source speech and produce the textual semantic features in the target language simultaneously, which establishes a robust semantic transmission mechanism for dynamic speech input.","According to the simulation results, the proposed Ross-S2T achieves significant speech-to-text translation performance compared to the conventional approach and exhibits high robustness against the corrupted speech input."],"url":"http://arxiv.org/abs/2403.05187v1","category":"eess.AS"}
{"created":"2024-03-08 09:54:56","title":"ROUGE-K: Do Your Summaries Have Keywords?","abstract":"Keywords, that is, content-relevant words in summaries play an important role in efficient information conveyance, making it critical to assess if system-generated summaries contain such informative words during evaluation. However, existing evaluation metrics for extreme summarization models do not pay explicit attention to keywords in summaries, leaving developers ignorant of their presence. To address this issue, we present a keyword-oriented evaluation metric, dubbed ROUGE-K, which provides a quantitative answer to the question of -- \\textit{How well do summaries include keywords?} Through the lens of this keyword-aware metric, we surprisingly find that a current strong baseline model often misses essential information in their summaries. Our analysis reveals that human annotators indeed find the summaries with more keywords to be more relevant to the source documents. This is an important yet previously overlooked aspect in evaluating summarization systems. Finally, to enhance keyword inclusion, we propose four approaches for incorporating word importance into a transformer-based model and experimentally show that it enables guiding models to include more keywords while keeping the overall quality. Our code is released at https://github.com/sobamchan/rougek.","sentences":["Keywords, that is, content-relevant words in summaries play an important role in efficient information conveyance, making it critical to assess if system-generated summaries contain such informative words during evaluation.","However, existing evaluation metrics for extreme summarization models do not pay explicit attention to keywords in summaries, leaving developers ignorant of their presence.","To address this issue, we present a keyword-oriented evaluation metric, dubbed ROUGE-K, which provides a quantitative answer to the question of -- \\textit{How well do summaries include keywords?}","Through the lens of this keyword-aware metric, we surprisingly find that a current strong baseline model often misses essential information in their summaries.","Our analysis reveals that human annotators indeed find the summaries with more keywords to be more relevant to the source documents.","This is an important yet previously overlooked aspect in evaluating summarization systems.","Finally, to enhance keyword inclusion, we propose four approaches for incorporating word importance into a transformer-based model and experimentally show that it enables guiding models to include more keywords while keeping the overall quality.","Our code is released at https://github.com/sobamchan/rougek."],"url":"http://arxiv.org/abs/2403.05186v1","category":"cs.CL"}
{"created":"2024-03-08 09:51:09","title":"Using Machine Learning to Separate Cherenkov and Scintillation Light in Hybrid Neutrino Detector","abstract":"This research investigates the separation of Cherenkov and Scintillation light signals within a simulated Water-based Liquid Scintillator (WbLS) detector, utilizing the XGBoost machine learning algorithm. The simulation data were gathered using the Rat-Pac software, which was built on the Geant4 architecture. The use of the WbLS medium has the capability to generate both Scintillation and Cherenkov light inside a single detector. To show the separation power of these two physics events, we will use the supervised learning approach. The assessment utilized a confusion matrix, classification report, and ROC curve, with the ROC curve indicating a performance result of $0.96 \\pm 1.2\\times 10^{-4}$. The research also aimed to identify essential parameters for effectively distinguishing these physics events through machine learning. For this, the study also introduced the SHAP methodology, utilizing game theory to assess feature contributions. The findings demonstrated that the number of hits has a significant effect on the trained model, while the mean hit time has a somewhat smaller impact. This research advances the utilization of AI and simulation data for accurate Cherenkov and Scintillation light separation in neutrino detectors","sentences":["This research investigates the separation of Cherenkov and Scintillation light signals within a simulated Water-based Liquid Scintillator (WbLS) detector, utilizing the XGBoost machine learning algorithm.","The simulation data were gathered using the Rat-Pac software, which was built on the Geant4 architecture.","The use of the WbLS medium has the capability to generate both Scintillation and Cherenkov light inside a single detector.","To show the separation power of these two physics events, we will use the supervised learning approach.","The assessment utilized a confusion matrix, classification report, and ROC curve, with the ROC curve indicating a performance result of $0.96 \\pm 1.2\\times 10^{-4}$.","The research also aimed to identify essential parameters for effectively distinguishing these physics events through machine learning.","For this, the study also introduced the SHAP methodology, utilizing game theory to assess feature contributions.","The findings demonstrated that the number of hits has a significant effect on the trained model, while the mean hit time has a somewhat smaller impact.","This research advances the utilization of AI and simulation data for accurate Cherenkov and Scintillation light separation in neutrino detectors"],"url":"http://arxiv.org/abs/2403.05184v1","category":"physics.ins-det"}
{"created":"2024-03-08 09:48:47","title":"Balanced triple product $p$-adic $L$-functions and Stark points","abstract":"Let $E$ be an elliptic curve over $\\mathbb{Q}$ and $\\varrho_1, \\varrho_2 \\colon \\mathrm{Gal}(H/\\mathbb{Q}) \\to \\mathrm{GL}_2(L)$ be two odd Artin representations. We use $p$-adic methods to investigate the part of the Mordell-Weil group $E(H) \\otimes L$ on which the Galois group acts via $\\varrho_1 \\otimes \\varrho_2$. When the rank of the group is two, Darmon-Lauder-Rotger used a dominant triple product $p$-adic $L$-function to study this group, and gave an Elliptic Stark Conjecture which relates its value outside of the interpolation range to two Stark points and one Stark unit. Our paper achieves a similar goal in the rank one setting. We first generalize Hsieh's construction of a 3-variable balanced triple product $p$-adic $L$-function in order to allow Hida families with classical weight one specializations. We then give an Elliptic Stark Conjecture relating its value outside of the interpolation range to a Stark point and two Stark units. As a consequence, we give an explicit $p$-adic formula for a point which should conjecturally lie in $E(H) \\otimes L$. We prove our conjecture for dihedral representations associated with the same imaginary quadratic field. This requires a generalization of the results of Bertolini-Darmon-Prasanna which we prove in the appendix.","sentences":["Let $E$ be an elliptic curve over $\\mathbb{Q}$ and $\\varrho_1, \\varrho_2 \\colon \\mathrm{Gal}(H/\\mathbb{Q})","\\to \\mathrm{GL}_2(L)$ be two odd Artin representations.","We use $p$-adic methods to investigate the part of the Mordell-Weil group $E(H)","\\otimes L$ on which the Galois group acts via $\\varrho_1 \\otimes \\varrho_2$. When the rank of the group is two, Darmon-Lauder-Rotger used a dominant triple product $p$-adic $L$-function to study this group, and gave an Elliptic Stark Conjecture which relates its value outside of the interpolation range to two Stark points and one Stark unit.","Our paper achieves a similar goal in the rank one setting.","We first generalize Hsieh's construction of a 3-variable balanced triple product $p$-adic $L$-function in order to allow Hida families with classical weight one specializations.","We then give an Elliptic Stark Conjecture relating its value outside of the interpolation range to a Stark point and two Stark units.","As a consequence, we give an explicit $p$-adic formula for a point which should conjecturally lie in $E(H)","\\otimes","L$.","We prove our conjecture for dihedral representations associated with the same imaginary quadratic field.","This requires a generalization of the results of Bertolini-Darmon-Prasanna which we prove in the appendix."],"url":"http://arxiv.org/abs/2403.05183v1","category":"math.NT"}
{"created":"2024-03-08 09:40:45","title":"Device Fault Prediction Model based on LSTM and Random Forest","abstract":"The quality of power grid equipment forms the material foundation for the safety of the large power grid. Ensuring the quality of equipment entering the grid is a core task in material management. Currently, the inspection of incoming materials involves the generation of sampling plans, sampling, sealing, sample delivery, and testing. Due to the lack of a comprehensive control system and effective control measures, it is not possible to trace the execution process of business operations afterward. This inability to trace hampers the investigation of testing issues and risk control, as it lacks effective data support. Additionally, a significant amount of original record information for key parameters in the testing process, which is based on sampling operation standards, has not been effectively utilized. To address these issues, we conduct researches on key monitoring technologies in the typical material inspection process based on the Internet of Things (IoT) and analyze the key parameters in inspection results. For purpose of complete the above tasks, this paper investigates the use of Long Short-Term Memory (LSTM) algorithms for quality prediction in material equipment based on key inspection parameters. In summary, this paper aims to provide professional and reliable quality data support for various business processes within the company, including material procurement, engineering construction, and equipment operation.","sentences":["The quality of power grid equipment forms the material foundation for the safety of the large power grid.","Ensuring the quality of equipment entering the grid is a core task in material management.","Currently, the inspection of incoming materials involves the generation of sampling plans, sampling, sealing, sample delivery, and testing.","Due to the lack of a comprehensive control system and effective control measures, it is not possible to trace the execution process of business operations afterward.","This inability to trace hampers the investigation of testing issues and risk control, as it lacks effective data support.","Additionally, a significant amount of original record information for key parameters in the testing process, which is based on sampling operation standards, has not been effectively utilized.","To address these issues, we conduct researches on key monitoring technologies in the typical material inspection process based on the Internet of Things (IoT) and analyze the key parameters in inspection results.","For purpose of complete the above tasks, this paper investigates the use of Long Short-Term Memory (LSTM) algorithms for quality prediction in material equipment based on key inspection parameters.","In summary, this paper aims to provide professional and reliable quality data support for various business processes within the company, including material procurement, engineering construction, and equipment operation."],"url":"http://arxiv.org/abs/2403.05179v1","category":"eess.SY"}
{"created":"2024-03-08 09:32:43","title":"Continual Learning and Catastrophic Forgetting","abstract":"This book chapter delves into the dynamics of continual learning, which is the process of incrementally learning from a non-stationary stream of data. Although continual learning is a natural skill for the human brain, it is very challenging for artificial neural networks. An important reason is that, when learning something new, these networks tend to quickly and drastically forget what they had learned before, a phenomenon known as catastrophic forgetting. Especially in the last decade, continual learning has become an extensively studied topic in deep learning. This book chapter reviews the insights that this field has generated.","sentences":["This book chapter delves into the dynamics of continual learning, which is the process of incrementally learning from a non-stationary stream of data.","Although continual learning is a natural skill for the human brain, it is very challenging for artificial neural networks.","An important reason is that, when learning something new, these networks tend to quickly and drastically forget what they had learned before, a phenomenon known as catastrophic forgetting.","Especially in the last decade, continual learning has become an extensively studied topic in deep learning.","This book chapter reviews the insights that this field has generated."],"url":"http://arxiv.org/abs/2403.05175v1","category":"cs.LG"}
{"created":"2024-03-08 09:28:42","title":"VTruST: Controllable value function based subset selection for Data-Centric Trustworthy AI","abstract":"Trustworthy AI is crucial to the widespread adoption of AI in high-stakes applications with fairness, robustness, and accuracy being some of the key trustworthiness metrics. In this work, we propose a controllable framework for data-centric trustworthy AI (DCTAI)- VTruST, that allows users to control the trade-offs between the different trustworthiness metrics of the constructed training datasets. A key challenge in implementing an efficient DCTAI framework is to design an online value-function-based training data subset selection algorithm. We pose the training data valuation and subset selection problem as an online sparse approximation formulation. We propose a novel online version of the Orthogonal Matching Pursuit (OMP) algorithm for solving this problem. Experimental results show that VTruST outperforms the state-of-the-art baselines on social, image, and scientific datasets. We also show that the data values generated by VTruST can provide effective data-centric explanations for different trustworthiness metrics.","sentences":["Trustworthy AI is crucial to the widespread adoption of AI in high-stakes applications with fairness, robustness, and accuracy being some of the key trustworthiness metrics.","In this work, we propose a controllable framework for data-centric trustworthy AI (DCTAI)- VTruST, that allows users to control the trade-offs between the different trustworthiness metrics of the constructed training datasets.","A key challenge in implementing an efficient DCTAI framework is to design an online value-function-based training data subset selection algorithm.","We pose the training data valuation and subset selection problem as an online sparse approximation formulation.","We propose a novel online version of the Orthogonal Matching Pursuit (OMP) algorithm for solving this problem.","Experimental results show that VTruST outperforms the state-of-the-art baselines on social, image, and scientific datasets.","We also show that the data values generated by VTruST can provide effective data-centric explanations for different trustworthiness metrics."],"url":"http://arxiv.org/abs/2403.05174v1","category":"cs.LG"}
{"created":"2024-03-08 09:25:48","title":"Learning Expressive And Generalizable Motion Features For Face Forgery Detection","abstract":"Previous face forgery detection methods mainly focus on appearance features, which may be easily attacked by sophisticated manipulation. Considering the majority of current face manipulation methods generate fake faces based on a single frame, which do not take frame consistency and coordination into consideration, artifacts on frame sequences are more effective for face forgery detection. However, current sequence-based face forgery detection methods use general video classification networks directly, which discard the special and discriminative motion information for face manipulation detection. To this end, we propose an effective sequence-based forgery detection framework based on an existing video classification method. To make the motion features more expressive for manipulation detection, we propose an alternative motion consistency block instead of the original motion features module. To make the learned features more generalizable, we propose an auxiliary anomaly detection block. With these two specially designed improvements, we make a general video classification network achieve promising results on three popular face forgery datasets.","sentences":["Previous face forgery detection methods mainly focus on appearance features, which may be easily attacked by sophisticated manipulation.","Considering the majority of current face manipulation methods generate fake faces based on a single frame, which do not take frame consistency and coordination into consideration, artifacts on frame sequences are more effective for face forgery detection.","However, current sequence-based face forgery detection methods use general video classification networks directly, which discard the special and discriminative motion information for face manipulation detection.","To this end, we propose an effective sequence-based forgery detection framework based on an existing video classification method.","To make the motion features more expressive for manipulation detection, we propose an alternative motion consistency block instead of the original motion features module.","To make the learned features more generalizable, we propose an auxiliary anomaly detection block.","With these two specially designed improvements, we make a general video classification network achieve promising results on three popular face forgery datasets."],"url":"http://arxiv.org/abs/2403.05172v1","category":"cs.CV"}
{"created":"2024-03-08 09:20:12","title":"Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation","abstract":"We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the pervasive issue of reward over-optimization in Reinforcement Learning from Human Feedback (RLHF) for Large Language Models (LLMs). Over-optimization occurs when a reward model serves as an imperfect proxy for human preference, and RL-driven policy optimization erroneously exploits reward inaccuracies. In this paper, we begin by introducing a lightweight way to quantify uncertainties in rewards, relying solely on the last layer embeddings of the reward model, without the need for computationally expensive reward ensembles. AdvPO then addresses a distributionally robust optimization problem centred around the confidence interval of the reward model's predictions for policy improvement. Through comprehensive experiments on the Anthropic HH and TL;DR summarization datasets, we illustrate the efficacy of AdvPO in mitigating the overoptimization issue, consequently resulting in enhanced performance as evaluated through human-assisted evaluation.","sentences":["We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the pervasive issue of reward over-optimization in Reinforcement Learning from Human Feedback (RLHF) for Large Language Models (LLMs).","Over-optimization occurs when a reward model serves as an imperfect proxy for human preference, and RL-driven policy optimization erroneously exploits reward inaccuracies.","In this paper, we begin by introducing a lightweight way to quantify uncertainties in rewards, relying solely on the last layer embeddings of the reward model, without the need for computationally expensive reward ensembles.","AdvPO then addresses a distributionally robust optimization problem centred around the confidence interval of the reward model's predictions for policy improvement.","Through comprehensive experiments on the Anthropic HH and TL;DR summarization datasets, we illustrate the efficacy of AdvPO in mitigating the overoptimization issue, consequently resulting in enhanced performance as evaluated through human-assisted evaluation."],"url":"http://arxiv.org/abs/2403.05171v1","category":"cs.LG"}
{"created":"2024-03-08 09:19:29","title":"DiffuLT: How to Make Diffusion Model Useful for Long-tail Recognition","abstract":"This paper proposes a new pipeline for long-tail (LT) recognition. Instead of re-weighting or re-sampling, we utilize the long-tailed dataset itself to generate a balanced proxy that can be optimized through cross-entropy (CE). Specifically, a randomly initialized diffusion model, trained exclusively on the long-tailed dataset, is employed to synthesize new samples for underrepresented classes. Then, we utilize the inherent information in the original dataset to filter out harmful samples and keep the useful ones. Our strategy, Diffusion model for Long-Tail recognition (DiffuLT), represents a pioneering utilization of generative models in long-tail recognition. DiffuLT achieves state-of-the-art results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT, surpassing the best competitors with non-trivial margins. Abundant ablations make our pipeline interpretable, too. The whole generation pipeline is done without any external data or pre-trained model weights, making it highly generalizable to real-world long-tailed settings.","sentences":["This paper proposes a new pipeline for long-tail (LT) recognition.","Instead of re-weighting or re-sampling, we utilize the long-tailed dataset itself to generate a balanced proxy that can be optimized through cross-entropy (CE).","Specifically, a randomly initialized diffusion model, trained exclusively on the long-tailed dataset, is employed to synthesize new samples for underrepresented classes.","Then, we utilize the inherent information in the original dataset to filter out harmful samples and keep the useful ones.","Our strategy, Diffusion model for Long-Tail recognition (DiffuLT), represents a pioneering utilization of generative models in long-tail recognition.","DiffuLT achieves state-of-the-art results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT, surpassing the best competitors with non-trivial margins.","Abundant ablations make our pipeline interpretable, too.","The whole generation pipeline is done without any external data or pre-trained model weights, making it highly generalizable to real-world long-tailed settings."],"url":"http://arxiv.org/abs/2403.05170v1","category":"cs.CV"}
{"created":"2024-03-08 09:17:34","title":"Bivariate $Q$-polynomial structures for the nonbinary Johnson scheme and the association scheme obtained from attenuated spaces","abstract":"The study of $P$-polynomial association schemes (distance-regular graphs) and $Q$-polynomial association schemes, and in particular $P$- and $Q$-polynomial association schemes, has been a central theme not only in the theory of association schemes but also in the whole study of algebraic combinatorics in general. Leonard's theorem (1982) says that the spherical functions (or the character tables) of $P$- and $Q$-polynomial association schemes are described by Askey-Wilson orthogonal polynomials or their relatives. These polynomials are one-variable orthogonal polynomials. It seems that the new attempt to define and study higher rank $P$- and $Q$-polynomial association schemes had been hoped for, but had gotten only limited success. The first very successful attempt was initiated recently by Bernard-Cramp\\'{e}-d'Andecy-Vinet-Zaimi [arXiv:2212.10824], and then followed by Bannai-Kurihara-Zhao-Zhu [arXiv:2305.00707]. The general theory and some explicit examples of families of higher rank (multivariate) $P$- and/or $Q$-polynomial association schemes have been obtained there. The main purpose of the present paper is to prove that some important families of association schemes are shown to be bivariate $Q$-polynomial. Namely, we show that all the nonbinary Johnson association schemes and all the attenuated space association schemes are bivariate $Q$-polynomial. It should be noted that the parameter restrictions needed in the previous papers are completely lifted in this paper. Our proofs are done by explicitly calculating the Krein parameters of these association schemes. At the end, we mention some speculations and indications of what we can expect in the future study.","sentences":["The study of $P$-polynomial association schemes (distance-regular graphs) and $Q$-polynomial association schemes, and in particular $P$- and $Q$-polynomial association schemes, has been a central theme not only in the theory of association schemes but also in the whole study of algebraic combinatorics in general.","Leonard's theorem (1982) says that the spherical functions (or the character tables) of $P$- and $Q$-polynomial association schemes are described by Askey-Wilson orthogonal polynomials or their relatives.","These polynomials are one-variable orthogonal polynomials.","It seems that the new attempt to define and study higher rank $P$- and $Q$-polynomial association schemes had been hoped for, but had gotten only limited success.","The first very successful attempt was initiated recently by Bernard-Cramp\\'{e}-d'Andecy-Vinet-Zaimi [arXiv:2212.10824], and then followed by Bannai-Kurihara-Zhao-Zhu [arXiv:2305.00707].","The general theory and some explicit examples of families of higher rank (multivariate) $P$- and/or $Q$-polynomial association schemes have been obtained there.","The main purpose of the present paper is to prove that some important families of association schemes are shown to be bivariate $Q$-polynomial.","Namely, we show that all the nonbinary Johnson association schemes and all the attenuated space association schemes are bivariate $Q$-polynomial.","It should be noted that the parameter restrictions needed in the previous papers are completely lifted in this paper.","Our proofs are done by explicitly calculating the Krein parameters of these association schemes.","At the end, we mention some speculations and indications of what we can expect in the future study."],"url":"http://arxiv.org/abs/2403.05169v1","category":"math.CO"}
{"created":"2024-03-08 09:16:47","title":"Unlocking the Potential of Multimodal Unified Discrete Representation through Training-Free Codebook Optimization and Hierarchical Alignment","abstract":"Recent advances in representation learning have demonstrated the significance of multimodal alignment. The Dual Cross-modal Information Disentanglement (DCID) model, utilizing a unified codebook, shows promising results in achieving fine-grained representation and cross-modal generalization. However, it is still hindered by equal treatment of all channels and neglect of minor event information, resulting in interference from irrelevant channels and limited performance in fine-grained tasks. Thus, in this work, We propose a Training-free Optimization of Codebook (TOC) method to enhance model performance by selecting important channels in the unified space without retraining. Additionally, we introduce the Hierarchical Dual Cross-modal Information Disentanglement (H-DCID) approach to extend information separation and alignment to two levels, capturing more cross-modal details. The experiment results demonstrate significant improvements across various downstream tasks, with TOC contributing to an average improvement of 1.70% for DCID on four tasks, and H-DCID surpassing DCID by an average of 3.64%. The combination of TOC and H-DCID further enhances performance, exceeding DCID by 4.43%. These findings highlight the effectiveness of our methods in facilitating robust and nuanced cross-modal learning, opening avenues for future enhancements. The source code and pre-trained models can be accessed at https://github.com/haihuangcode/TOC_H-DCID.","sentences":["Recent advances in representation learning have demonstrated the significance of multimodal alignment.","The Dual Cross-modal Information Disentanglement (DCID) model, utilizing a unified codebook, shows promising results in achieving fine-grained representation and cross-modal generalization.","However, it is still hindered by equal treatment of all channels and neglect of minor event information, resulting in interference from irrelevant channels and limited performance in fine-grained tasks.","Thus, in this work, We propose a Training-free Optimization of Codebook (TOC) method to enhance model performance by selecting important channels in the unified space without retraining.","Additionally, we introduce the Hierarchical Dual Cross-modal Information Disentanglement (H-DCID) approach to extend information separation and alignment to two levels, capturing more cross-modal details.","The experiment results demonstrate significant improvements across various downstream tasks, with TOC contributing to an average improvement of 1.70% for DCID on four tasks, and H-DCID surpassing DCID by an average of 3.64%.","The combination of TOC and H-DCID further enhances performance, exceeding DCID by 4.43%.","These findings highlight the effectiveness of our methods in facilitating robust and nuanced cross-modal learning, opening avenues for future enhancements.","The source code and pre-trained models can be accessed at https://github.com/haihuangcode/TOC_H-DCID."],"url":"http://arxiv.org/abs/2403.05168v1","category":"cs.CV"}
{"created":"2024-03-08 09:09:15","title":"Synthetic data generation for system identification: leveraging knowledge transfer from similar systems","abstract":"This paper addresses the challenge of overfitting in the learning of dynamical systems by introducing a novel approach for the generation of synthetic data, aimed at enhancing model generalization and robustness in scenarios characterized by data scarcity. Central to the proposed methodology is the concept of knowledge transfer from systems within the same class. Specifically, synthetic data is generated through a pre-trained meta-model that describes a broad class of systems to which the system of interest is assumed to belong. Training data serves a dual purpose: firstly, as input to the pre-trained meta model to discern the system's dynamics, enabling the prediction of its behavior and thereby generating synthetic output sequences for new input sequences; secondly, in conjunction with synthetic data, to define the loss function used for model estimation. A validation dataset is used to tune a scalar hyper-parameter balancing the relative importance of training and synthetic data in the definition of the loss function. The same validation set can be also used for other purposes, such as early stopping during the training, fundamental to avoid overfitting in case of small-size training datasets. The efficacy of the approach is shown through a numerical example that highlights the advantages of integrating synthetic data into the system identification process.","sentences":["This paper addresses the challenge of overfitting in the learning of dynamical systems by introducing a novel approach for the generation of synthetic data, aimed at enhancing model generalization and robustness in scenarios characterized by data scarcity.","Central to the proposed methodology is the concept of knowledge transfer from systems within the same class.","Specifically, synthetic data is generated through a pre-trained meta-model that describes a broad class of systems to which the system of interest is assumed to belong.","Training data serves a dual purpose: firstly, as input to the pre-trained meta model to discern the system's dynamics, enabling the prediction of its behavior and thereby generating synthetic output sequences for new input sequences; secondly, in conjunction with synthetic data, to define the loss function used for model estimation.","A validation dataset is used to tune a scalar hyper-parameter balancing the relative importance of training and synthetic data in the definition of the loss function.","The same validation set can be also used for other purposes, such as early stopping during the training, fundamental to avoid overfitting in case of small-size training datasets.","The efficacy of the approach is shown through a numerical example that highlights the advantages of integrating synthetic data into the system identification process."],"url":"http://arxiv.org/abs/2403.05164v1","category":"cs.LG"}
{"created":"2024-03-08 09:07:25","title":"Properties of the connected components in projections of random bipartite networks: Effects of clique size fluctuations","abstract":"We examined the structure of projections of random bipartite networks characterized by the degree distribution of individual and group nodes through the generating function method. We decomposed a projection into two subgraphs, the giant component, and finite components and analyzed their degree correlation. The projections never exhibit a negative degree correlation. Positive degree correlations in the networks originating from the clique size fluctuation remain after the decomposition at the set of finite components although the values of their clustering coefficient are still finite. The giant component can exhibit either positive or negative degree correlations based on the structure of the projection. However, they are positively correlated in most cases. In addition, we determined the relation between the finite components in a supercritical phase possessing the giant component and the projection in a subcritical phase when the degree distributions for group and individual are Poisson.","sentences":["We examined the structure of projections of random bipartite networks characterized by the degree distribution of individual and group nodes through the generating function method.","We decomposed a projection into two subgraphs, the giant component, and finite components and analyzed their degree correlation.","The projections never exhibit a negative degree correlation.","Positive degree correlations in the networks originating from the clique size fluctuation remain after the decomposition at the set of finite components although the values of their clustering coefficient are still finite.","The giant component can exhibit either positive or negative degree correlations based on the structure of the projection.","However, they are positively correlated in most cases.","In addition, we determined the relation between the finite components in a supercritical phase possessing the giant component and the projection in a subcritical phase when the degree distributions for group and individual are Poisson."],"url":"http://arxiv.org/abs/2403.05162v1","category":"physics.soc-ph"}
{"created":"2024-03-08 09:02:37","title":"Be,La,U-rich spherules as microtektites of terrestrial laterites: What goes up must come down","abstract":"Recently Loeb et al. (2024, \"Recovery and Classification of Spherules from the Pacific Ocean Site of the CNEOS 2014 January 8 (IM1) Bolide\", Res. Notes. Amer. Astron. Soc. 8, 39) reported the magnetic collection of millimeter-sized spherules from the seafloor near Papua New Guinea. About 22% had Mg/Si < 1/3 and were identified as a new \"differentiated\" variety of cosmic spherule (\"D-type\"). In a subset of 26 of these \"D-type\" spherules, 12 \"BeLaU\" spherules were found to be dominated by Fe and Al, marked by low Si and even lower Mg content, depletions of volatile species like Pb and Cs, and remarkable enrichments of Be, La, U, Ba, and other elements. Loeb et al. claimed these have exotic compositions different from other Solar System materials. We show that in fact samples with these compositions are not just found on Earth, they are from Earth; specifically, we identify them as microtektites of terrestrial lateritic sandstone. Based on the location of the sample site, we associate them with the Australasian tektite strewn field, generated 788 kyr ago by an impactor that melted and ejected ~10^8 tons of sandstone, including a lateritic layer, from Indochina. A tektite origin for the spherules is corroborated by their terrestrial Fe isotopic compositions and the compound, non-spherical nature of many of them, which preclude formation as ablation spherules from a bolide. Due to the restriction of laterites to the tropics, iron-rich tektites may be uncommon, but we predict they should comprise ~3% of the Australasian microtektites.","sentences":["Recently Loeb et al. (2024, \"Recovery and Classification of Spherules from the Pacific Ocean Site of the CNEOS 2014 January 8 (IM1) Bolide\", Res. Notes.","Amer.","Astron.","Soc. 8, 39) reported the magnetic collection of millimeter-sized spherules from the seafloor near Papua New Guinea.","About 22% had Mg/Si < 1/3 and were identified as a new \"differentiated\" variety of cosmic spherule (\"D-type\").","In a subset of 26 of these \"D-type\" spherules, 12 \"BeLaU\" spherules were found to be dominated by Fe and Al, marked by low Si and even lower Mg content, depletions of volatile species like Pb and Cs, and remarkable enrichments of Be, La, U, Ba, and other elements.","Loeb et al. claimed these have exotic compositions different from other Solar System materials.","We show that in fact samples with these compositions are not just found on Earth, they are from Earth; specifically, we identify them as microtektites of terrestrial lateritic sandstone.","Based on the location of the sample site, we associate them with the Australasian tektite strewn field, generated 788 kyr ago by an impactor that melted and ejected ~10^8 tons of sandstone, including a lateritic layer, from Indochina.","A tektite origin for the spherules is corroborated by their terrestrial Fe isotopic compositions and the compound, non-spherical nature of many of them, which preclude formation as ablation spherules from a bolide.","Due to the restriction of laterites to the tropics, iron-rich tektites may be uncommon, but we predict they should comprise ~3% of the Australasian microtektites."],"url":"http://arxiv.org/abs/2403.05161v1","category":"astro-ph.EP"}
{"created":"2024-03-08 08:51:37","title":"Adaptive Split Learning over Energy-Constrained Wireless Edge Networks","abstract":"Split learning (SL) is a promising approach for training artificial intelligence (AI) models, in which devices collaborate with a server to train an AI model in a distributed manner, based on a same fixed split point. However, due to the device heterogeneity and variation of channel conditions, this way is not optimal in training delay and energy consumption. In this paper, we design an adaptive split learning (ASL) scheme which can dynamically select split points for devices and allocate computing resource for the server in wireless edge networks. We formulate an optimization problem to minimize the average training latency subject to long-term energy consumption constraint. The difficulties in solving this problem are the lack of future information and mixed integer programming (MIP). To solve it, we propose an online algorithm leveraging the Lyapunov theory, named OPEN, which decomposes it into a new MIP problem only with the current information. Then, a two-layer optimization method is proposed to solve the MIP problem. Extensive simulation results demonstrate that the ASL scheme can reduce the average training delay and energy consumption by 53.7% and 22.1%, respectively, as compared to the existing SL schemes.","sentences":["Split learning (SL) is a promising approach for training artificial intelligence (AI) models, in which devices collaborate with a server to train an AI model in a distributed manner, based on a same fixed split point.","However, due to the device heterogeneity and variation of channel conditions, this way is not optimal in training delay and energy consumption.","In this paper, we design an adaptive split learning (ASL) scheme which can dynamically select split points for devices and allocate computing resource for the server in wireless edge networks.","We formulate an optimization problem to minimize the average training latency subject to long-term energy consumption constraint.","The difficulties in solving this problem are the lack of future information and mixed integer programming (MIP).","To solve it, we propose an online algorithm leveraging the Lyapunov theory, named OPEN, which decomposes it into a new MIP problem only with the current information.","Then, a two-layer optimization method is proposed to solve the MIP problem.","Extensive simulation results demonstrate that the ASL scheme can reduce the average training delay and energy consumption by 53.7% and 22.1%, respectively, as compared to the existing SL schemes."],"url":"http://arxiv.org/abs/2403.05158v1","category":"cs.LG"}
{"created":"2024-03-08 08:47:48","title":"On Protecting the Data Privacy of Large Language Models (LLMs): A Survey","abstract":"Large language models (LLMs) are complex artificial intelligence systems capable of understanding, generating and translating human language. They learn language patterns by analyzing large amounts of text data, allowing them to perform writing, conversation, summarizing and other language tasks. When LLMs process and generate large amounts of data, there is a risk of leaking sensitive information, which may threaten data privacy. This paper concentrates on elucidating the data privacy concerns associated with LLMs to foster a comprehensive understanding. Specifically, a thorough investigation is undertaken to delineate the spectrum of data privacy threats, encompassing both passive privacy leakage and active privacy attacks within LLMs. Subsequently, we conduct an assessment of the privacy protection mechanisms employed by LLMs at various stages, followed by a detailed examination of their efficacy and constraints. Finally, the discourse extends to delineate the challenges encountered and outline prospective directions for advancement in the realm of LLM privacy protection.","sentences":["Large language models (LLMs) are complex artificial intelligence systems capable of understanding, generating and translating human language.","They learn language patterns by analyzing large amounts of text data, allowing them to perform writing, conversation, summarizing and other language tasks.","When LLMs process and generate large amounts of data, there is a risk of leaking sensitive information, which may threaten data privacy.","This paper concentrates on elucidating the data privacy concerns associated with LLMs to foster a comprehensive understanding.","Specifically, a thorough investigation is undertaken to delineate the spectrum of data privacy threats, encompassing both passive privacy leakage and active privacy attacks within LLMs.","Subsequently, we conduct an assessment of the privacy protection mechanisms employed by LLMs at various stages, followed by a detailed examination of their efficacy and constraints.","Finally, the discourse extends to delineate the challenges encountered and outline prospective directions for advancement in the realm of LLM privacy protection."],"url":"http://arxiv.org/abs/2403.05156v1","category":"cs.CR"}
{"created":"2024-03-08 08:45:42","title":"LanePtrNet: Revisiting Lane Detection as Point Voting and Grouping on Curves","abstract":"Lane detection plays a critical role in the field of autonomous driving. Prevailing methods generally adopt basic concepts (anchors, key points, etc.) from object detection and segmentation tasks, while these approaches require manual adjustments for curved objects, involve exhaustive searches on predefined anchors, require complex post-processing steps, and may lack flexibility when applied to real-world scenarios.In this paper, we propose a novel approach, LanePtrNet, which treats lane detection as a process of point voting and grouping on ordered sets: Our method takes backbone features as input and predicts a curve-aware centerness, which represents each lane as a point and assigns the most probable center point to it. A novel point sampling method is proposed to generate a set of candidate points based on the votes received. By leveraging features from local neighborhoods, and cross-instance attention score, we design a grouping module that further performs lane-wise clustering between neighboring and seeding points. Furthermore, our method can accommodate a point-based framework, (PointNet++ series, etc.) as an alternative to the backbone. This flexibility enables effortless extension to 3D lane detection tasks. We conduct comprehensive experiments to validate the effectiveness of our proposed approach, demonstrating its superior performance.","sentences":["Lane detection plays a critical role in the field of autonomous driving.","Prevailing methods generally adopt basic concepts (anchors, key points, etc.) from object detection and segmentation tasks, while these approaches require manual adjustments for curved objects, involve exhaustive searches on predefined anchors, require complex post-processing steps, and may lack flexibility when applied to real-world scenarios.","In this paper, we propose a novel approach, LanePtrNet, which treats lane detection as a process of point voting and grouping on ordered sets: Our method takes backbone features as input and predicts a curve-aware centerness, which represents each lane as a point and assigns the most probable center point to it.","A novel point sampling method is proposed to generate a set of candidate points based on the votes received.","By leveraging features from local neighborhoods, and cross-instance attention score, we design a grouping module that further performs lane-wise clustering between neighboring and seeding points.","Furthermore, our method can accommodate a point-based framework, (PointNet++ series, etc.) as an alternative to the backbone.","This flexibility enables effortless extension to 3D lane detection tasks.","We conduct comprehensive experiments to validate the effectiveness of our proposed approach, demonstrating its superior performance."],"url":"http://arxiv.org/abs/2403.05155v1","category":"cs.CV"}
{"created":"2024-03-08 08:42:23","title":"GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian Splatting","abstract":"We present GSEdit, a pipeline for text-guided 3D object editing based on Gaussian Splatting models. Our method enables the editing of the style and appearance of 3D objects without altering their main details, all in a matter of minutes on consumer hardware. We tackle the problem by leveraging Gaussian splatting to represent 3D scenes, and we optimize the model while progressively varying the image supervision by means of a pretrained image-based diffusion model. The input object may be given as a 3D triangular mesh, or directly provided as Gaussians from a generative model such as DreamGaussian. GSEdit ensures consistency across different viewpoints, maintaining the integrity of the original object's information. Compared to previously proposed methods relying on NeRF-like MLP models, GSEdit stands out for its efficiency, making 3D editing tasks much faster. Our editing process is refined via the application of the SDS loss, ensuring that our edits are both precise and accurate. Our comprehensive evaluation demonstrates that GSEdit effectively alters object shape and appearance following the given textual instructions while preserving their coherence and detail.","sentences":["We present GSEdit, a pipeline for text-guided 3D object editing based on Gaussian Splatting models.","Our method enables the editing of the style and appearance of 3D objects without altering their main details, all in a matter of minutes on consumer hardware.","We tackle the problem by leveraging Gaussian splatting to represent 3D scenes, and we optimize the model while progressively varying the image supervision by means of a pretrained image-based diffusion model.","The input object may be given as a 3D triangular mesh, or directly provided as Gaussians from a generative model such as DreamGaussian.","GSEdit ensures consistency across different viewpoints, maintaining the integrity of the original object's information.","Compared to previously proposed methods relying on NeRF-like MLP models, GSEdit stands out for its efficiency, making 3D editing tasks much faster.","Our editing process is refined via the application of the SDS loss, ensuring that our edits are both precise and accurate.","Our comprehensive evaluation demonstrates that GSEdit effectively alters object shape and appearance following the given textual instructions while preserving their coherence and detail."],"url":"http://arxiv.org/abs/2403.05154v1","category":"cs.CV"}
{"created":"2024-03-08 08:41:14","title":"Towards a Psychology of Machines: Large Language Models Predict Human Memory","abstract":"Large language models (LLMs) are demonstrating remarkable capabilities across various tasks despite lacking a foundation in human cognition. This raises the question: can these models, beyond simply mimicking human language patterns, offer insights into the mechanisms underlying human cognition? This study explores the ability of ChatGPT to predict human performance in a language-based memory task. Building upon theories of text comprehension, we hypothesize that recognizing ambiguous sentences (e.g., \"Because Bill drinks wine is never kept in the house\") is facilitated by preceding them with contextually relevant information. Participants, both human and ChatGPT, were presented with pairs of sentences. The second sentence was always a garden-path sentence designed to be inherently ambiguous, while the first sentence either provided a fitting (e.g., \"Bill has chronic alcoholism\") or an unfitting context (e.g., \"Bill likes to play golf\"). We measured both human's and ChatGPT's ratings of sentence relatedness, ChatGPT's memorability ratings for the garden-path sentences, and humans' spontaneous memory for the garden-path sentences. The results revealed a striking alignment between ChatGPT's assessments and human performance. Sentences deemed more related and assessed as being more memorable by ChatGPT were indeed better remembered by humans, even though ChatGPT's internal mechanisms likely differ significantly from human cognition. This finding, which was confirmed with a robustness check employing synonyms, underscores the potential of generative AI models to predict human performance accurately. We discuss the broader implications of these findings for leveraging LLMs in the development of psychological theories and for gaining a deeper understanding of human cognition.","sentences":["Large language models (LLMs) are demonstrating remarkable capabilities across various tasks despite lacking a foundation in human cognition.","This raises the question: can these models, beyond simply mimicking human language patterns, offer insights into the mechanisms underlying human cognition?","This study explores the ability of ChatGPT to predict human performance in a language-based memory task.","Building upon theories of text comprehension, we hypothesize that recognizing ambiguous sentences (e.g., \"Because Bill drinks wine is never kept in the house\") is facilitated by preceding them with contextually relevant information.","Participants, both human and ChatGPT, were presented with pairs of sentences.","The second sentence was always a garden-path sentence designed to be inherently ambiguous, while the first sentence either provided a fitting (e.g., \"Bill has chronic alcoholism\") or an unfitting context (e.g., \"Bill likes to play golf\").","We measured both human's and ChatGPT's ratings of sentence relatedness, ChatGPT's memorability ratings for the garden-path sentences, and humans' spontaneous memory for the garden-path sentences.","The results revealed a striking alignment between ChatGPT's assessments and human performance.","Sentences deemed more related and assessed as being more memorable by ChatGPT were indeed better remembered by humans, even though ChatGPT's internal mechanisms likely differ significantly from human cognition.","This finding, which was confirmed with a robustness check employing synonyms, underscores the potential of generative AI models to predict human performance accurately.","We discuss the broader implications of these findings for leveraging LLMs in the development of psychological theories and for gaining a deeper understanding of human cognition."],"url":"http://arxiv.org/abs/2403.05152v1","category":"cs.CL"}
{"created":"2024-03-08 08:38:50","title":"Inverse Design of Photonic Crystal Surface Emitting Lasers is a Sequence Modeling Problem","abstract":"Photonic Crystal Surface Emitting Lasers (PCSEL)'s inverse design demands expert knowledge in physics, materials science, and quantum mechanics which is prohibitively labor-intensive. Advanced AI technologies, especially reinforcement learning (RL), have emerged as a powerful tool to augment and accelerate this inverse design process. By modeling the inverse design of PCSEL as a sequential decision-making problem, RL approaches can construct a satisfactory PCSEL structure from scratch. However, the data inefficiency resulting from online interactions with precise and expensive simulation environments impedes the broader applicability of RL approaches. Recently, sequential models, especially the Transformer architecture, have exhibited compelling performance in sequential decision-making problems due to their simplicity and scalability to large language models. In this paper, we introduce a novel framework named PCSEL Inverse Design Transformer (PiT) that abstracts the inverse design of PCSEL as a sequence modeling problem. The central part of our PiT is a Transformer-based structure that leverages the past trajectories and current states to predict the current actions. Compared with the traditional RL approaches, PiT can output the optimal actions and achieve target PCSEL designs by leveraging offline data and conditioning on the desired return. Results demonstrate that PiT achieves superior performance and data efficiency compared to baselines.","sentences":["Photonic Crystal Surface Emitting Lasers (PCSEL)'s inverse design demands expert knowledge in physics, materials science, and quantum mechanics which is prohibitively labor-intensive.","Advanced AI technologies, especially reinforcement learning (RL), have emerged as a powerful tool to augment and accelerate this inverse design process.","By modeling the inverse design of PCSEL as a sequential decision-making problem, RL approaches can construct a satisfactory PCSEL structure from scratch.","However, the data inefficiency resulting from online interactions with precise and expensive simulation environments impedes the broader applicability of RL approaches.","Recently, sequential models, especially the Transformer architecture, have exhibited compelling performance in sequential decision-making problems due to their simplicity and scalability to large language models.","In this paper, we introduce a novel framework named PCSEL Inverse Design Transformer (PiT) that abstracts the inverse design of PCSEL as a sequence modeling problem.","The central part of our PiT is a Transformer-based structure that leverages the past trajectories and current states to predict the current actions.","Compared with the traditional RL approaches, PiT can output the optimal actions and achieve target PCSEL designs by leveraging offline data and conditioning on the desired return.","Results demonstrate that PiT achieves superior performance and data efficiency compared to baselines."],"url":"http://arxiv.org/abs/2403.05149v1","category":"physics.app-ph"}
{"created":"2024-03-08 08:31:48","title":"Simulating adiabatic quantum computation with a variational approach","abstract":"The theoretical analysis of the Adiabatic Quantum Computation protocol presents several challenges resulting from the difficulty of simulating, with classical resources, the unitary dynamics of a large quantum device. We present here a variational approach to substantially alleviate this problem in many situations of interest. Our approach is based on the time-dependent Variational Monte Carlo method, in conjunction with a correlated and time-dependent Jastrow ansatz. We demonstrate that accurate results can be obtained in a variety of problems, ranging from the description of defect generation through a dynamical phase transition in 1D to the complex dynamics of frustrated spin-glass problems both on fully-connected and Chimera graphs.","sentences":["The theoretical analysis of the Adiabatic Quantum Computation protocol presents several challenges resulting from the difficulty of simulating, with classical resources, the unitary dynamics of a large quantum device.","We present here a variational approach to substantially alleviate this problem in many situations of interest.","Our approach is based on the time-dependent Variational Monte Carlo method, in conjunction with a correlated and time-dependent Jastrow ansatz.","We demonstrate that accurate results can be obtained in a variety of problems, ranging from the description of defect generation through a dynamical phase transition in 1D to the complex dynamics of frustrated spin-glass problems both on fully-connected and Chimera graphs."],"url":"http://arxiv.org/abs/2403.05147v1","category":"quant-ph"}
{"created":"2024-03-08 18:57:08","title":"The role of polaron dressing in superradiant emission dynamics","abstract":"Cooperative effects of multiple quantum emitters are characterized by transitions via delocalized collective states with altered emission properties due to the existence of inter-emitter coherences. When realized with excitonic condensed matter nanostructures, these effects are significantly affected by the presence of strong emitter-phonon coupling, which leads to the formation of polarons. We show that, while for single-emitter emission into free space this formation has no impact on its radiative lifetime, the same is not true for superradiant emission. Considering the case of two indistinguishable quantum emitters, we analyse how polaron dressing affects collective photon emission by mixing bright and dark Dicke states. Our numerical simulations show that this mixing crucially depends on the circumstances of the excitation of the system: Depending on the pulse length of an exciting laser, one can choose to either prepare polaronic Dicke states, or bare electronic Dicke states, changing the superradiant decay characteristics of the system. Additionally, we derive analytic expressions for these limiting cases, which match the results of numerically exact calculations.","sentences":["Cooperative effects of multiple quantum emitters are characterized by transitions via delocalized collective states with altered emission properties due to the existence of inter-emitter coherences.","When realized with excitonic condensed matter nanostructures, these effects are significantly affected by the presence of strong emitter-phonon coupling, which leads to the formation of polarons.","We show that, while for single-emitter emission into free space this formation has no impact on its radiative lifetime, the same is not true for superradiant emission.","Considering the case of two indistinguishable quantum emitters, we analyse how polaron dressing affects collective photon emission by mixing bright and dark Dicke states.","Our numerical simulations show that this mixing crucially depends on the circumstances of the excitation of the system: Depending on the pulse length of an exciting laser, one can choose to either prepare polaronic Dicke states, or bare electronic Dicke states, changing the superradiant decay characteristics of the system.","Additionally, we derive analytic expressions for these limiting cases, which match the results of numerically exact calculations."],"url":"http://arxiv.org/abs/2403.05533v1","category":"quant-ph"}
{"created":"2024-03-08 18:22:58","title":"Linear Model Estimators and Consistency under an Infill Asymptotic Domain","abstract":"Functional data present as functions or curves possessing a spatial or temporal component. These components by nature have a fixed observational domain. Consequently, any asymptotic investigation requires modelling the increased correlation among observations as density increases due to this fixed domain constraint. One such appropriate stochastic process is the Ornstein-Uhlenbeck process. Utilizing this spatial autoregressive process, we demonstrate that parameter estimators for a simple linear regression model display inconsistency in an infill asymptotic domain. Such results are contrary to those expected under the customary increasing domain asymptotics. Although none of these estimator variances approach zero, they do display a pattern of diminishing return regarding decreasing estimator variance as sample size increases. This may prove invaluable to a practitioner as this indicates perhaps an optimal sample size to cease data collection. This in turn reduces time and data collection cost because little information is gained in sampling beyond a certain sample size.","sentences":["Functional data present as functions or curves possessing a spatial or temporal component.","These components by nature have a fixed observational domain.","Consequently, any asymptotic investigation requires modelling the increased correlation among observations as density increases due to this fixed domain constraint.","One such appropriate stochastic process is the Ornstein-Uhlenbeck process.","Utilizing this spatial autoregressive process, we demonstrate that parameter estimators for a simple linear regression model display inconsistency in an infill asymptotic domain.","Such results are contrary to those expected under the customary increasing domain asymptotics.","Although none of these estimator variances approach zero, they do display a pattern of diminishing return regarding decreasing estimator variance as sample size increases.","This may prove invaluable to a practitioner as this indicates perhaps an optimal sample size to cease data collection.","This in turn reduces time and data collection cost because little information is gained in sampling beyond a certain sample size."],"url":"http://arxiv.org/abs/2403.05503v1","category":"stat.ME"}
{"created":"2024-03-08 17:53:58","title":"FFSTC: Fongbe to French Speech Translation Corpus","abstract":"In this paper, we introduce the Fongbe to French Speech Translation Corpus (FFSTC) for the first time. This corpus encompasses approximately 31 hours of collected Fongbe language content, featuring both French transcriptions and corresponding Fongbe voice recordings. FFSTC represents a comprehensive dataset compiled through various collection methods and the efforts of dedicated individuals. Furthermore, we conduct baseline experiments using Fairseq's transformer_s and conformer models to evaluate data quality and validity. Our results indicate a score of 8.96 for the transformer_s model and 8.14 for the conformer model, establishing a baseline for the FFSTC corpus.","sentences":["In this paper, we introduce the Fongbe to French Speech Translation Corpus (FFSTC) for the first time.","This corpus encompasses approximately 31 hours of collected Fongbe language content, featuring both French transcriptions and corresponding Fongbe voice recordings.","FFSTC represents a comprehensive dataset compiled through various collection methods and the efforts of dedicated individuals.","Furthermore, we conduct baseline experiments using Fairseq's transformer_s and conformer models to evaluate data quality and validity.","Our results indicate a score of 8.96 for the transformer_s model and 8.14 for the conformer model, establishing a baseline for the FFSTC corpus."],"url":"http://arxiv.org/abs/2403.05488v1","category":"cs.CL"}
{"created":"2024-03-08 17:38:55","title":"Take Your Best Shot: Sampling-Based Next-Best-View Planning for Autonomous Photography & Inspection","abstract":"Autonomous mobile robots (AMRs) equipped with high-quality cameras have revolutionized the field of inspections by providing efficient and cost-effective means of conducting surveys. The use of autonomous inspection is becoming more widespread in a variety of contexts, yet it is still challenging to acquire the best inspection information autonomously. In situations where objects may block a robot's view, it is necessary to use reasoning to determine the optimal points for collecting data. Although researchers have explored cloud-based applications to store inspection data, these applications may not operate optimally under network constraints, and parsing these datasets can be manually intensive. Instead, there is an emerging requirement for AMRs to autonomously capture the most informative views efficiently. To address this challenge, we present an autonomous Next-Best-View (NBV) framework that maximizes the inspection information while reducing the number of pictures needed during operations. The framework consists of a formalized evaluation metric using ray-tracing and Gaussian process interpolation to estimate information reward based on the current understanding of the partially-known environment. A derivative-free optimization (DFO) method is used to sample candidate views in the environment and identify the NBV point. The proposed approach's effectiveness is shown by comparing it with existing methods and further validated through simulations and experiments with various vehicles.","sentences":["Autonomous mobile robots (AMRs) equipped with high-quality cameras have revolutionized the field of inspections by providing efficient and cost-effective means of conducting surveys.","The use of autonomous inspection is becoming more widespread in a variety of contexts, yet it is still challenging to acquire the best inspection information autonomously.","In situations where objects may block a robot's view, it is necessary to use reasoning to determine the optimal points for collecting data.","Although researchers have explored cloud-based applications to store inspection data, these applications may not operate optimally under network constraints, and parsing these datasets can be manually intensive.","Instead, there is an emerging requirement for AMRs to autonomously capture the most informative views efficiently.","To address this challenge, we present an autonomous Next-Best-View (NBV) framework that maximizes the inspection information while reducing the number of pictures needed during operations.","The framework consists of a formalized evaluation metric using ray-tracing and Gaussian process interpolation to estimate information reward based on the current understanding of the partially-known environment.","A derivative-free optimization (DFO) method is used to sample candidate views in the environment and identify the NBV point.","The proposed approach's effectiveness is shown by comparing it with existing methods and further validated through simulations and experiments with various vehicles."],"url":"http://arxiv.org/abs/2403.05477v1","category":"cs.RO"}
{"created":"2024-03-08 17:22:33","title":"On varimax asymptotics in network models and spectral methods for dimensionality reduction","abstract":"Varimax factor rotations, while popular among practitioners in psychology and statistics since being introduced by H. Kaiser, have historically been viewed with skepticism and suspicion by some theoreticians and mathematical statisticians. Now, work by K. Rohe and M. Zeng provides new, fundamental insight: varimax rotations provably perform statistical estimation in certain classes of latent variable models when paired with spectral-based matrix truncations for dimensionality reduction. We build on this newfound understanding of varimax rotations by developing further connections to network analysis and spectral methods rooted in entrywise matrix perturbation analysis. Concretely, this paper establishes the asymptotic multivariate normality of vectors in varimax-transformed Euclidean point clouds that represent low-dimensional node embeddings in certain latent space random graph models. We address related concepts including network sparsity, data denoising, and the role of matrix rank in latent variable parameterizations. Collectively, these findings, at the confluence of classical and contemporary multivariate analysis, reinforce methodology and inference procedures grounded in matrix factorization-based techniques. Numerical examples illustrate our findings and supplement our discussion.","sentences":["Varimax factor rotations, while popular among practitioners in psychology and statistics since being introduced by H. Kaiser, have historically been viewed with skepticism and suspicion by some theoreticians and mathematical statisticians.","Now, work by K. Rohe and M. Zeng provides new, fundamental insight: varimax rotations provably perform statistical estimation in certain classes of latent variable models when paired with spectral-based matrix truncations for dimensionality reduction.","We build on this newfound understanding of varimax rotations by developing further connections to network analysis and spectral methods rooted in entrywise matrix perturbation analysis.","Concretely, this paper establishes the asymptotic multivariate normality of vectors in varimax-transformed Euclidean point clouds that represent low-dimensional node embeddings in certain latent space random graph models.","We address related concepts including network sparsity, data denoising, and the role of matrix rank in latent variable parameterizations.","Collectively, these findings, at the confluence of classical and contemporary multivariate analysis, reinforce methodology and inference procedures grounded in matrix factorization-based techniques.","Numerical examples illustrate our findings and supplement our discussion."],"url":"http://arxiv.org/abs/2403.05461v1","category":"math.ST"}
{"created":"2024-03-08 17:02:24","title":"Control-Oriented Identification for the Linear Quadratic Regulator: Technical Report","abstract":"Data-driven control benefits from rich datasets, but constructing such datasets becomes challenging when gathering data is limited. We consider an offline experiment design approach to gathering data where we design a control input to collect data that will most improve the performance of a feedback controller. We show how such a control-oriented approach can be used in a setting with linear dynamics and quadratic objective and, through design of a gradient estimator, solve the problem via stochastic gradient descent. We contrast our method with a classical A-optimal experiment design approach and numerically demonstrate that our method outperforms A-optimal design in terms of improving control performance.","sentences":["Data-driven control benefits from rich datasets, but constructing such datasets becomes challenging when gathering data is limited.","We consider an offline experiment design approach to gathering data where we design a control input to collect data that will most improve the performance of a feedback controller.","We show how such a control-oriented approach can be used in a setting with linear dynamics and quadratic objective and, through design of a gradient estimator, solve the problem via stochastic gradient descent.","We contrast our method with a classical A-optimal experiment design approach and numerically demonstrate that our method outperforms A-optimal design in terms of improving control performance."],"url":"http://arxiv.org/abs/2403.05455v1","category":"eess.SY"}
{"created":"2024-03-08 15:53:37","title":"Safe Spot: Perceived safety of dominant and submissive appearances of quadruped robots in human-robot interactions","abstract":"Unprecedented possibilities of quadruped robots have driven much research on the technical aspects of these robots. However, the social perception and acceptability of quadruped robots so far remain poorly understood. This work investigates whether the way we design quadruped robots' behaviors can affect people's perception of safety in interactions with these robots. We designed and tested a dominant and submissive personality for the quadruped robot (Boston Dynamics Spot). These were tested in two different walking scenarios (head-on and crossing interactions) in a 2x2 within-subjects study. We collected both behavioral data and subjective reports on participants' perception of the interaction. The results highlight that participants perceived the submissive robot as safer compared to the dominant one. The behavioral dynamics of interactions did not change depending on the robot's appearance. Participants' previous in-person experience with the robot was associated with lower subjective safety ratings but did not correlate with the interaction dynamics. Our findings have implications for the design of quadruped robots and contribute to the body of knowledge on the social perception of non-humanoid robots. We call for a stronger standing of felt experiences in human-robot interaction research.","sentences":["Unprecedented possibilities of quadruped robots have driven much research on the technical aspects of these robots.","However, the social perception and acceptability of quadruped robots so far remain poorly understood.","This work investigates whether the way we design quadruped robots' behaviors can affect people's perception of safety in interactions with these robots.","We designed and tested a dominant and submissive personality for the quadruped robot (Boston Dynamics Spot).","These were tested in two different walking scenarios (head-on and crossing interactions) in a 2x2 within-subjects study.","We collected both behavioral data and subjective reports on participants' perception of the interaction.","The results highlight that participants perceived the submissive robot as safer compared to the dominant one.","The behavioral dynamics of interactions did not change depending on the robot's appearance.","Participants' previous in-person experience with the robot was associated with lower subjective safety ratings but did not correlate with the interaction dynamics.","Our findings have implications for the design of quadruped robots and contribute to the body of knowledge on the social perception of non-humanoid robots.","We call for a stronger standing of felt experiences in human-robot interaction research."],"url":"http://arxiv.org/abs/2403.05400v1","category":"cs.RO"}
{"created":"2024-03-08 13:39:51","title":"Search for soft unclustered energy patterns in proton-proton collisions at 13 TeV","abstract":"The first search for soft unclustered energy patterns (SUEPs) is performed using an integrated luminosity of 138 fb$^{-1}$ of proton-proton collision data at $\\sqrt{s}$ = 13 TeV collected in 2016-2018 by the CMS detector at the LHC. Such SUEPs are predicted by Hidden Valley models with a new, confining force with a large 't Hooft coupling. In events with boosted topologies, selected by high-threshold hadronic triggers, the multiplicity and sphericity of clustered tracks are used to reject the background from standard model quantum chromodynamics. With no observed excess of events over the standard model expectation, limits are set on the cross section for production via gluon fusion of a scalar mediator with SUEP-like decays.","sentences":["The first search for soft unclustered energy patterns (SUEPs) is performed using an integrated luminosity of 138 fb$^{-1}$ of proton-proton collision data at $\\sqrt{s}$ = 13 TeV collected in 2016-2018 by the CMS detector at the LHC.","Such SUEPs are predicted by Hidden Valley models with a new, confining force with a large 't Hooft coupling.","In events with boosted topologies, selected by high-threshold hadronic triggers, the multiplicity and sphericity of clustered tracks are used to reject the background from standard model quantum chromodynamics.","With no observed excess of events over the standard model expectation, limits are set on the cross section for production via gluon fusion of a scalar mediator with SUEP-like decays."],"url":"http://arxiv.org/abs/2403.05311v1","category":"hep-ex"}
{"created":"2024-03-08 12:56:25","title":"Role of tissue fluidization and topological defects in epithelial tubulogenesis","abstract":"Cellular rearrangements, as primary sources of tissue fluidization, facilitate topological transitions during tissue morphogenesis. We study the role of intrinsic cell properties such as cell polarity and cell-cell adhesion in shaping epithelial tissues using a minimal model of interacting polarized cells. The presence of a vortex in the cell polarization poses the topological constraint that induces an inwards migration with the formation of a conical shape. Local rearrangements at the tip of the cone lead to the onset of tube formation. Switching between collective migration and structural rearrangements is key for balancing the contrasting tendencies, such as the tissue rigidity needed to preserve shape and the tissue fluidity allowing for topological transitions during tissue morphogenesis.","sentences":["Cellular rearrangements, as primary sources of tissue fluidization, facilitate topological transitions during tissue morphogenesis.","We study the role of intrinsic cell properties such as cell polarity and cell-cell adhesion in shaping epithelial tissues using a minimal model of interacting polarized cells.","The presence of a vortex in the cell polarization poses the topological constraint that induces an inwards migration with the formation of a conical shape.","Local rearrangements at the tip of the cone lead to the onset of tube formation.","Switching between collective migration and structural rearrangements is key for balancing the contrasting tendencies, such as the tissue rigidity needed to preserve shape and the tissue fluidity allowing for topological transitions during tissue morphogenesis."],"url":"http://arxiv.org/abs/2403.05276v1","category":"physics.bio-ph"}
{"created":"2024-03-08 11:48:44","title":"Benchmarking Micro-action Recognition: Dataset, Methods, and Applications","abstract":"Micro-action is an imperceptible non-verbal behaviour characterised by low-intensity movement. It offers insights into the feelings and intentions of individuals and is important for human-oriented applications such as emotion recognition and psychological assessment. However, the identification, differentiation, and understanding of micro-actions pose challenges due to the imperceptible and inaccessible nature of these subtle human behaviors in everyday life. In this study, we innovatively collect a new micro-action dataset designated as Micro-action-52 (MA-52), and propose a benchmark named micro-action network (MANet) for micro-action recognition (MAR) task. Uniquely, MA-52 provides the whole-body perspective including gestures, upper- and lower-limb movements, attempting to reveal comprehensive micro-action cues. In detail, MA-52 contains 52 micro-action categories along with seven body part labels, and encompasses a full array of realistic and natural micro-actions, accounting for 205 participants and 22,422 video instances collated from the psychological interviews. Based on the proposed dataset, we assess MANet and other nine prevalent action recognition methods. MANet incorporates squeeze-and excitation (SE) and temporal shift module (TSM) into the ResNet architecture for modeling the spatiotemporal characteristics of micro-actions. Then a joint-embedding loss is designed for semantic matching between video and action labels; the loss is used to better distinguish between visually similar yet distinct micro-action categories. The extended application in emotion recognition has demonstrated one of the important values of our proposed dataset and method. In the future, further exploration of human behaviour, emotion, and psychological assessment will be conducted in depth. The dataset and source code are released at https://github.com/VUT-HFUT/Micro-Action.","sentences":["Micro-action is an imperceptible non-verbal behaviour characterised by low-intensity movement.","It offers insights into the feelings and intentions of individuals and is important for human-oriented applications such as emotion recognition and psychological assessment.","However, the identification, differentiation, and understanding of micro-actions pose challenges due to the imperceptible and inaccessible nature of these subtle human behaviors in everyday life.","In this study, we innovatively collect a new micro-action dataset designated as Micro-action-52 (MA-52), and propose a benchmark named micro-action network (MANet) for micro-action recognition (MAR) task.","Uniquely, MA-52 provides the whole-body perspective including gestures, upper- and lower-limb movements, attempting to reveal comprehensive micro-action cues.","In detail, MA-52 contains 52 micro-action categories along with seven body part labels, and encompasses a full array of realistic and natural micro-actions, accounting for 205 participants and 22,422 video instances collated from the psychological interviews.","Based on the proposed dataset, we assess MANet and other nine prevalent action recognition methods.","MANet incorporates squeeze-and excitation (SE) and temporal shift module (TSM) into the ResNet architecture for modeling the spatiotemporal characteristics of micro-actions.","Then a joint-embedding loss is designed for semantic matching between video and action labels; the loss is used to better distinguish between visually similar yet distinct micro-action categories.","The extended application in emotion recognition has demonstrated one of the important values of our proposed dataset and method.","In the future, further exploration of human behaviour, emotion, and psychological assessment will be conducted in depth.","The dataset and source code are released at https://github.com/VUT-HFUT/Micro-Action."],"url":"http://arxiv.org/abs/2403.05234v1","category":"cs.CV"}
{"created":"2024-03-08 11:38:16","title":"Maximal Non-Kochen-Specker Sets and a Lower Bound on the Size of Kochen-Specker Sets","abstract":"A Kochen-Specker (KS) set is a finite collection of vectors on the two-sphere containing no antipodal pairs for which it is impossible to assign 0s and 1s such that no two orthogonal vectors are assigned 1 and exactly one vector in every triplet of mutually orthogonal vectors is assigned 1. The existence of KS sets lies at the heart of Kochen and Specker's argument against non-contextual hidden variable theories and the Conway-Kochen free will theorem. Identifying small KS sets can simplify these arguments and may contribute to the understanding of the role played by contextuality in quantum protocols. In this paper we derive a weak lower bound of 10 vectors for the size of any KS set by studying the opposite notion of large non-KS sets and using a probability argument that is independent of the graph structure of KS sets. We also point out an interesting connection with a generalisation of the moving sofa problem around a right-angled hallway on the two-sphere.","sentences":["A Kochen-Specker (KS) set is a finite collection of vectors on the two-sphere containing no antipodal pairs for which it is impossible to assign 0s and 1s such that no two orthogonal vectors are assigned 1 and exactly one vector in every triplet of mutually orthogonal vectors is assigned 1.","The existence of KS sets lies at the heart of Kochen and Specker's argument against non-contextual hidden variable theories and the Conway-Kochen free will theorem.","Identifying small KS sets can simplify these arguments and may contribute to the understanding of the role played by contextuality in quantum protocols.","In this paper we derive a weak lower bound of 10 vectors for the size of any KS set by studying the opposite notion of large non-KS sets and using a probability argument that is independent of the graph structure of KS sets.","We also point out an interesting connection with a generalisation of the moving sofa problem around a right-angled hallway on the two-sphere."],"url":"http://arxiv.org/abs/2403.05230v1","category":"quant-ph"}
{"created":"2024-03-08 09:41:01","title":"Putting Language into Context Using Smartphone-Based Keyboard Logging","abstract":"While the study of language as typed on smartphones offers valuable insights, existing data collection methods often fall short in providing contextual information and ensuring user privacy. We present a privacy-respectful approach - context-enriched keyboard logging - that allows for the extraction of contextual information on the user's input motive, which is meaningful for linguistics, psychology, and behavioral sciences. In particular, with our approach, we enable distinguishing language contents by their channel (i.e., comments, messaging, search inputs). Filtering by channel allows for better pre-selection of data, which is in the interest of researchers and improves users' privacy. We demonstrate our approach on a large-scale six-month user study (N=624) of language use in smartphone interactions in the wild. Finally, we highlight the implications for research on language use in human-computer interaction and interdisciplinary contexts.","sentences":["While the study of language as typed on smartphones offers valuable insights, existing data collection methods often fall short in providing contextual information and ensuring user privacy.","We present a privacy-respectful approach - context-enriched keyboard logging - that allows for the extraction of contextual information on the user's input motive, which is meaningful for linguistics, psychology, and behavioral sciences.","In particular, with our approach, we enable distinguishing language contents by their channel (i.e., comments, messaging, search inputs).","Filtering by channel allows for better pre-selection of data, which is in the interest of researchers and improves users' privacy.","We demonstrate our approach on a large-scale six-month user study (N=624) of language use in smartphone interactions in the wild.","Finally, we highlight the implications for research on language use in human-computer interaction and interdisciplinary contexts."],"url":"http://arxiv.org/abs/2403.05180v1","category":"cs.HC"}
{"created":"2024-03-08 08:39:04","title":"Multicolor photometry of LEO mega-constellations Starlink and OneWeb","abstract":"The development of low earth orbit (LEO) mega-constellation fundamentally threatens ground-based optical astronomical observations. To study the photometric properties of the LEO mega-constellations, we used the Xinglong 50 cm telescope to conduct a large-sample, high-precision, and multicolor target-tracking photometry of two typical LEO Mega-constellations: Starlink and OneWeb. Over a three-month observation period starting on 2022 January 1st, we collected 1,447 light curves of 404 satellites in four typical versions: Starlink v1.0, DarkSat, VisorSat, Starlink v1.5, and OneWeb. According to data statistics, Starlink v1.0 has the smallest median magnitude at clear and SDSS $gri$ band, and OneWeb is the dimmest bus. The brightness of Starlink v1.5 is slightly brighter than VisorSat. We construct a detailed photometric model with solar phase angle variations by calculating the illumination-visibility geometry based on the orbital parameters. Our data analysis shows that the solar phase angle is the significant characteristic which influencing Starlink satellites' brightness, but it is not sensitive to OneWeb satellites. VisorSat and Starlink v1.5 version, which are equipped with deployable visors, have significantly reduced scattered light compared to the previous Starlink v1.0 version. The multiband LOWESS and color-index are analyzed in characterizing the energy and color features of LEO mega-constellation satellites. This work found that the proportion of scattered sunlight mitigation achieved with VisorSat and Starlink v1.5 was 55.1\\% and 40.4\\%, respectively. The color index of different buses shows an evident clustering feature. Our observation and analysis could provide valuable quantitative data and photometric models, which can contribute to assessing the impact of LEO mega-constellations on astronomical observations.","sentences":["The development of low earth orbit (LEO) mega-constellation fundamentally threatens ground-based optical astronomical observations.","To study the photometric properties of the LEO mega-constellations, we used the Xinglong 50 cm telescope to conduct a large-sample, high-precision, and multicolor target-tracking photometry of two typical LEO Mega-constellations: Starlink and OneWeb.","Over a three-month observation period starting on 2022 January 1st, we collected 1,447 light curves of 404 satellites in four typical versions: Starlink v1.0, DarkSat, VisorSat, Starlink v1.5, and OneWeb.","According to data statistics, Starlink v1.0 has the smallest median magnitude at clear and SDSS $gri$ band, and OneWeb is the dimmest bus.","The brightness of Starlink v1.5 is slightly brighter than VisorSat.","We construct a detailed photometric model with solar phase angle variations by calculating the illumination-visibility geometry based on the orbital parameters.","Our data analysis shows that the solar phase angle is the significant characteristic which influencing Starlink satellites' brightness, but it is not sensitive to OneWeb satellites.","VisorSat and Starlink v1.5 version, which are equipped with deployable visors, have significantly reduced scattered light compared to the previous Starlink v1.0 version.","The multiband LOWESS and color-index are analyzed in characterizing the energy and color features of LEO mega-constellation satellites.","This work found that the proportion of scattered sunlight mitigation achieved with VisorSat and Starlink v1.5 was 55.1\\% and 40.4\\%, respectively.","The color index of different buses shows an evident clustering feature.","Our observation and analysis could provide valuable quantitative data and photometric models, which can contribute to assessing the impact of LEO mega-constellations on astronomical observations."],"url":"http://arxiv.org/abs/2403.05150v1","category":"astro-ph.IM"}
{"created":"2024-03-08 18:57:52","title":"Bayesian Preference Elicitation with Language Models","abstract":"Aligning AI systems to users' interests requires understanding and incorporating humans' complex values and preferences. Recently, language models (LMs) have been used to gather information about the preferences of human users. This preference data can be used to fine-tune or guide other LMs and/or AI systems. However, LMs have been shown to struggle with crucial aspects of preference learning: quantifying uncertainty, modeling human mental states, and asking informative questions. These challenges have been addressed in other areas of machine learning, such as Bayesian Optimal Experimental Design (BOED), which focus on designing informative queries within a well-defined feature space. But these methods, in turn, are difficult to scale and apply to real-world problems where simply identifying the relevant features can be difficult. We introduce OPEN (Optimal Preference Elicitation with Natural language) a framework that uses BOED to guide the choice of informative questions and an LM to extract features and translate abstract BOED queries into natural language questions. By combining the flexibility of LMs with the rigor of BOED, OPEN can optimize the informativity of queries while remaining adaptable to real-world domains. In user studies, we find that OPEN outperforms existing LM- and BOED-based methods for preference elicitation.","sentences":["Aligning AI systems to users' interests requires understanding and incorporating humans' complex values and preferences.","Recently, language models (LMs) have been used to gather information about the preferences of human users.","This preference data can be used to fine-tune or guide other LMs and/or AI systems.","However, LMs have been shown to struggle with crucial aspects of preference learning: quantifying uncertainty, modeling human mental states, and asking informative questions.","These challenges have been addressed in other areas of machine learning, such as Bayesian Optimal Experimental Design (BOED), which focus on designing informative queries within a well-defined feature space.","But these methods, in turn, are difficult to scale and apply to real-world problems where simply identifying the relevant features can be difficult.","We introduce OPEN (Optimal Preference Elicitation with Natural language) a framework that uses BOED to guide the choice of informative questions and an LM to extract features and translate abstract BOED queries into natural language questions.","By combining the flexibility of LMs with the rigor of BOED, OPEN can optimize the informativity of queries while remaining adaptable to real-world domains.","In user studies, we find that OPEN outperforms existing LM- and BOED-based methods for preference elicitation."],"url":"http://arxiv.org/abs/2403.05534v1","category":"cs.CL"}
{"created":"2024-03-08 18:48:19","title":"On Koebe radius and coefficients estimate for univalent harmonic mappings","abstract":"The problem on estimate of the Koebe radius for univalent harmonic mappings of the unit disk $\\mathbb D=\\{z\\in\\mathbb C : |z|<1\\}$ is considered. For a subclass of harmonic mappings with the standard normalization and a certain growth estimate for analytic dilatation, we provide new estimate for the Koebe radius. New estimate for Taylor coefficients of the holomorphic part of a function from the subclass under consideration is obtained as a corollary.","sentences":["The problem on estimate of the Koebe radius for univalent harmonic mappings of the unit disk $\\mathbb D=\\{z\\in\\mathbb C : |z|<1\\}$ is considered.","For a subclass of harmonic mappings with the standard normalization and a certain growth estimate for analytic dilatation, we provide new estimate for the Koebe radius.","New estimate for Taylor coefficients of the holomorphic part of a function from the subclass under consideration is obtained as a corollary."],"url":"http://arxiv.org/abs/2403.05526v1","category":"math.CV"}
{"created":"2024-03-08 18:42:59","title":"Authorship Attribution in Bangla Literature (AABL) via Transfer Learning using ULMFiT","abstract":"Authorship Attribution is the task of creating an appropriate characterization of text that captures the authors' writing style to identify the original author of a given piece of text. With increased anonymity on the internet, this task has become increasingly crucial in various security and plagiarism detection fields. Despite significant advancements in other languages such as English, Spanish, and Chinese, Bangla lacks comprehensive research in this field due to its complex linguistic feature and sentence structure. Moreover, existing systems are not scalable when the number of author increases, and the performance drops for small number of samples per author. In this paper, we propose the use of Average-Stochastic Gradient Descent Weight-Dropped Long Short-Term Memory (AWD-LSTM) architecture and an effective transfer learning approach that addresses the problem of complex linguistic features extraction and scalability for authorship attribution in Bangla Literature (AABL). We analyze the effect of different tokenization, such as word, sub-word, and character level tokenization, and demonstrate the effectiveness of these tokenizations in the proposed model. Moreover, we introduce the publicly available Bangla Authorship Attribution Dataset of 16 authors (BAAD16) containing 17,966 sample texts and 13.4+ million words to solve the standard dataset scarcity problem and release six variations of pre-trained language models for use in any Bangla NLP downstream task. For evaluation, we used our developed BAAD16 dataset as well as other publicly available datasets. Empirically, our proposed model outperformed state-of-the-art models and achieved 99.8% accuracy in the BAAD16 dataset. Furthermore, we showed that the proposed system scales much better even with an increasing number of authors, and performance remains steady despite few training samples.","sentences":["Authorship Attribution is the task of creating an appropriate characterization of text that captures the authors' writing style to identify the original author of a given piece of text.","With increased anonymity on the internet, this task has become increasingly crucial in various security and plagiarism detection fields.","Despite significant advancements in other languages such as English, Spanish, and Chinese, Bangla lacks comprehensive research in this field due to its complex linguistic feature and sentence structure.","Moreover, existing systems are not scalable when the number of author increases, and the performance drops for small number of samples per author.","In this paper, we propose the use of Average-Stochastic Gradient Descent Weight-Dropped Long Short-Term Memory (AWD-LSTM) architecture and an effective transfer learning approach that addresses the problem of complex linguistic features extraction and scalability for authorship attribution in Bangla Literature (AABL).","We analyze the effect of different tokenization, such as word, sub-word, and character level tokenization, and demonstrate the effectiveness of these tokenizations in the proposed model.","Moreover, we introduce the publicly available Bangla Authorship Attribution Dataset of 16 authors (BAAD16) containing 17,966 sample texts and 13.4+ million words to solve the standard dataset scarcity problem and release six variations of pre-trained language models for use in any Bangla NLP downstream task.","For evaluation, we used our developed BAAD16 dataset as well as other publicly available datasets.","Empirically, our proposed model outperformed state-of-the-art models and achieved 99.8% accuracy in the BAAD16 dataset.","Furthermore, we showed that the proposed system scales much better even with an increasing number of authors, and performance remains steady despite few training samples."],"url":"http://arxiv.org/abs/2403.05519v1","category":"cs.CL"}
{"created":"2024-03-08 18:36:30","title":"Volume-entangled exact eigenstates in the PXP and related models in any dimension","abstract":"In this work, we describe an approach for unveiling many-body Einstein-Podolsky-Rosen-like volume-entangled exact scars hosted by diverse PXP-type models for Rydberg-blockaded atom systems and discuss experimentally relevant aspects of these previously unknown states.","sentences":["In this work, we describe an approach for unveiling many-body Einstein-Podolsky-Rosen-like volume-entangled exact scars hosted by diverse PXP-type models for Rydberg-blockaded atom systems and discuss experimentally relevant aspects of these previously unknown states."],"url":"http://arxiv.org/abs/2403.05515v1","category":"quant-ph"}
{"created":"2024-03-08 18:33:53","title":"A Detection and Filtering Framework for Collaborative Localization","abstract":"Increasingly, autonomous vehicles (AVs) are becoming a reality, such as the Advanced Driver Assistance Systems (ADAS) in vehicles that assist drivers in driving and parking functions with vehicles today. The localization problem for AVs relies primarily on multiple sensors, including cameras, LiDARs, and radars. Manufacturing, installing, calibrating, and maintaining these sensors can be very expensive, thereby increasing the overall cost of AVs. This research explores the means to improve localization on vehicles belonging to the ADAS category in a platooning context, where an ADAS vehicle follows a lead \"Smart\" AV equipped with a highly accurate sensor suite. We propose and produce results by using a filtering framework to combine pose information derived from vision and odometry to improve the localization of the ADAS vehicle that follows the smart vehicle.","sentences":["Increasingly, autonomous vehicles (AVs) are becoming a reality, such as the Advanced Driver Assistance Systems (ADAS) in vehicles that assist drivers in driving and parking functions with vehicles today.","The localization problem for AVs relies primarily on multiple sensors, including cameras, LiDARs, and radars.","Manufacturing, installing, calibrating, and maintaining these sensors can be very expensive, thereby increasing the overall cost of AVs.","This research explores the means to improve localization on vehicles belonging to the ADAS category in a platooning context, where an ADAS vehicle follows a lead \"Smart\" AV equipped with a highly accurate sensor suite.","We propose and produce results by using a filtering framework to combine pose information derived from vision and odometry to improve the localization of the ADAS vehicle that follows the smart vehicle."],"url":"http://arxiv.org/abs/2403.05513v1","category":"cs.RO"}
{"created":"2024-03-08 18:33:09","title":"The Wrappingness and Trunkenness of Volume-Preserving Flows","abstract":"Link invariants of long pieces of orbits of a volume-preserving flow can be used to define diffeomorphism invariants of the flow. In this paper, we extend the notions of wrapping number and trunk and define invariants of links with respect to a fibration on a 3-manifold. Extending work of Dehornoy and Rechtman, we apply this to define diffeomorphism invariants wrappingness and trunkenness of volume-preserving flows on 3-manifolds and interpret these invariants as obstructions to the existence of a global surface of section for the flow. Finally, we construct flows and show that wrappingness and trunkenness are not functions of the helicity of a flow.","sentences":["Link invariants of long pieces of orbits of a volume-preserving flow can be used to define diffeomorphism invariants of the flow.","In this paper, we extend the notions of wrapping number and trunk and define invariants of links with respect to a fibration on a 3-manifold.","Extending work of Dehornoy and Rechtman, we apply this to define diffeomorphism invariants wrappingness and trunkenness of volume-preserving flows on 3-manifolds and interpret these invariants as obstructions to the existence of a global surface of section for the flow.","Finally, we construct flows and show that wrappingness and trunkenness are not functions of the helicity of a flow."],"url":"http://arxiv.org/abs/2403.05511v1","category":"math.GT"}
{"created":"2024-03-08 18:29:09","title":"Bond Dipole Theory of Band Offset","abstract":"Understanding the band offset between materials is pivotal for electronic device functionality. While traditional theories attribute this offset solely to intrinsic properties of constituent materials, interfacial chemistry introduces complexities, including charge transfer dipoles. We address this challenge by defining a reference system based on Wigner-Seitz atoms, which is a charge partitioning that tessellates space while retaining bulk crystal symmetry. The interfacial dipole is then expressed entirely by localized dipoles along interfacial bonds. Geometric analysis reveals that for isotropic materials, both bulk and interfacial contributions to the band offset are orientation independent. This finding is confirmed through first-principles calculations and analysis of 28 distinct interfaces, which show very little (< 0.1 eV) dependence on orientation. Furthermore, the direction dependence of anisotropic materials is well captured by the theory. This work provides crucial insights into the geometric underpinnings of band offset, offering a comprehensive understanding essential for advancing materials design in electronic applications.","sentences":["Understanding the band offset between materials is pivotal for electronic device functionality.","While traditional theories attribute this offset solely to intrinsic properties of constituent materials, interfacial chemistry introduces complexities, including charge transfer dipoles.","We address this challenge by defining a reference system based on Wigner-Seitz atoms, which is a charge partitioning that tessellates space while retaining bulk crystal symmetry.","The interfacial dipole is then expressed entirely by localized dipoles along interfacial bonds.","Geometric analysis reveals that for isotropic materials, both bulk and interfacial contributions to the band offset are orientation independent.","This finding is confirmed through first-principles calculations and analysis of 28 distinct interfaces, which show very little (< 0.1 eV) dependence on orientation.","Furthermore, the direction dependence of anisotropic materials is well captured by the theory.","This work provides crucial insights into the geometric underpinnings of band offset, offering a comprehensive understanding essential for advancing materials design in electronic applications."],"url":"http://arxiv.org/abs/2403.05509v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-08 18:27:54","title":"The Michaelis--Menten reaction at low substrate concentrations: Pseudo-first-order kinetics and conditions for timescale separation","abstract":"We demonstrate that the Michaelis-Menten reaction mechanism can be accurately approximated by a linear system when the initial substrate concentration is low. This leads to pseudo-first-order kinetics, simplifying mathematical calculations and experimental analysis. Our proof utilizes a monotonicity property of the system and Kamke's comparison theorem. This linear approximation yields a closed-form solution, enabling accurate modeling and estimation of reaction rate constants even without timescale separation. Building on prior work, we establish that the sufficient condition for the validity of this approximation is $s_0 \\ll K$, where $K=k_2/k_1$ is the Van Slyke-Cullen constant. This condition is independent of the initial enzyme concentration. Further, we investigate timescale separation within the linear system, identifying necessary and sufficient conditions and deriving the corresponding reduced one-dimensional equations.","sentences":["We demonstrate that the Michaelis-Menten reaction mechanism can be accurately approximated by a linear system when the initial substrate concentration is low.","This leads to pseudo-first-order kinetics, simplifying mathematical calculations and experimental analysis.","Our proof utilizes a monotonicity property of the system and Kamke's comparison theorem.","This linear approximation yields a closed-form solution, enabling accurate modeling and estimation of reaction rate constants even without timescale separation.","Building on prior work, we establish that the sufficient condition for the validity of this approximation is $s_0 \\ll K$, where $K=k_2/k_1$ is the Van Slyke-Cullen constant.","This condition is independent of the initial enzyme concentration.","Further, we investigate timescale separation within the linear system, identifying necessary and sufficient conditions and deriving the corresponding reduced one-dimensional equations."],"url":"http://arxiv.org/abs/2403.05507v1","category":"math.DS"}
{"created":"2024-03-08 17:51:31","title":"A Paradigm Shift in Catheter Development: Thermally Drawn Polymeric Fibers for MR-Guided Cardiovascular Interventions","abstract":"Cardiovascular diseases (CVDs) and congenital heart diseases (CHD) pose significant global health challenges. Fluoroscopy-guided endovascular interventions, though effective, are accompanied by ionizing radiation concerns, especially in pediatric cases. Magnetic resonance imaging (MRI) emerges as a radiation-free alternative, offering superior soft tissue visualization and functional insights. However, the lack of compatible instruments remains a hurdle. We present two novel catheter systems, a tendon-driven steerable catheter and an active tracking Tiger-shaped catheter, fabricated using a unique fiber drawing technique. These catheters, showcasing mechanical properties similar to commercial counterparts, have undergone rigorous in-vitro and in-vivo testing, yielding promising outcomes. This innovative approach has the potential to streamline medical device development, thus enhancing patient care in MR-guided interventions.","sentences":["Cardiovascular diseases (CVDs) and congenital heart diseases (CHD) pose significant global health challenges.","Fluoroscopy-guided endovascular interventions, though effective, are accompanied by ionizing radiation concerns, especially in pediatric cases.","Magnetic resonance imaging (MRI) emerges as a radiation-free alternative, offering superior soft tissue visualization and functional insights.","However, the lack of compatible instruments remains a hurdle.","We present two novel catheter systems, a tendon-driven steerable catheter and an active tracking Tiger-shaped catheter, fabricated using a unique fiber drawing technique.","These catheters, showcasing mechanical properties similar to commercial counterparts, have undergone rigorous in-vitro and in-vivo testing, yielding promising outcomes.","This innovative approach has the potential to streamline medical device development, thus enhancing patient care in MR-guided interventions."],"url":"http://arxiv.org/abs/2403.05485v1","category":"physics.med-ph"}
{"created":"2024-03-08 17:44:28","title":"Sampling-Based Risk-Aware Path Planning Around Dynamic Engagement Zones","abstract":"Existing methods for avoiding dynamic engagement zones (EZs) and minimizing risk leverage the calculus of variations to obtain optimal paths. While such methods are deterministic, they scale poorly as the number of engagement zones increases. Furthermore, optimal-control based strategies are sensitive to initial guesses and often converge to local, rather than global, minima. This paper presents a novel sampling-based approach to obtain a feasible flight plan for a Dubins vehicle to reach a desired location in a bounded operating region in the presence of a large number of engagement zones. The dynamic EZs are coupled to the vehicle dynamics through its heading angle. Thus, the dynamic two-dimensional obstacles in the (x,y) plane can be transformed into three-dimensional static obstacles in a lifted (x,y,{\\psi}) space. This insight is leveraged in the formulation of a Rapidly-exploring Random Tree (RRT*) algorithm. The algorithm is evaluated with a Monte Carlo experiment that randomizes EZ locations to characterize the success rate and average path length as a function of the number of EZs and as the computation time made available to the planner is increased.","sentences":["Existing methods for avoiding dynamic engagement zones (EZs) and minimizing risk leverage the calculus of variations to obtain optimal paths.","While such methods are deterministic, they scale poorly as the number of engagement zones increases.","Furthermore, optimal-control based strategies are sensitive to initial guesses and often converge to local, rather than global, minima.","This paper presents a novel sampling-based approach to obtain a feasible flight plan for a Dubins vehicle to reach a desired location in a bounded operating region in the presence of a large number of engagement zones.","The dynamic EZs are coupled to the vehicle dynamics through its heading angle.","Thus, the dynamic two-dimensional obstacles in the (x,y) plane can be transformed into three-dimensional static obstacles in a lifted (x,y,{\\psi}) space.","This insight is leveraged in the formulation of a Rapidly-exploring Random Tree (RRT*) algorithm.","The algorithm is evaluated with a Monte Carlo experiment that randomizes EZ locations to characterize the success rate and average path length as a function of the number of EZs and as the computation time made available to the planner is increased."],"url":"http://arxiv.org/abs/2403.05480v1","category":"math.OC"}
{"created":"2024-03-08 17:41:22","title":"HGIC: A Hand Gesture Based Interactive Control System for Efficient and Scalable Multi-UAV Operations","abstract":"As technological advancements continue to expand the capabilities of multi unmanned-aerial-vehicle systems (mUAV), human operators face challenges in scalability and efficiency due to the complex cognitive load and operations associated with motion adjustments and team coordination. Such cognitive demands limit the feasible size of mUAV teams and necessitate extensive operator training, impeding broader adoption. This paper developed a Hand Gesture Based Interactive Control (HGIC), a novel interface system that utilize computer vision techniques to intuitively translate hand gestures into modular commands for robot teaming. Through learning control models, these commands enable efficient and scalable mUAV motion control and adjustments. HGIC eliminates the need for specialized hardware and offers two key benefits: 1) Minimal training requirements through natural gestures; and 2) Enhanced scalability and efficiency via adaptable commands. By reducing the cognitive burden on operators, HGIC opens the door for more effective large-scale mUAV applications in complex, dynamic, and uncertain scenarios. HGIC will be open-sourced after the paper being published online for the research community, aiming to drive forward innovations in human-mUAV interactions.","sentences":["As technological advancements continue to expand the capabilities of multi unmanned-aerial-vehicle systems (mUAV), human operators face challenges in scalability and efficiency due to the complex cognitive load and operations associated with motion adjustments and team coordination.","Such cognitive demands limit the feasible size of mUAV teams and necessitate extensive operator training, impeding broader adoption.","This paper developed a Hand Gesture Based Interactive Control (HGIC), a novel interface system that utilize computer vision techniques to intuitively translate hand gestures into modular commands for robot teaming.","Through learning control models, these commands enable efficient and scalable mUAV motion control and adjustments.","HGIC eliminates the need for specialized hardware and offers two key benefits: 1) Minimal training requirements through natural gestures; and 2) Enhanced scalability and efficiency via adaptable commands.","By reducing the cognitive burden on operators, HGIC opens the door for more effective large-scale mUAV applications in complex, dynamic, and uncertain scenarios.","HGIC will be open-sourced after the paper being published online for the research community, aiming to drive forward innovations in human-mUAV interactions."],"url":"http://arxiv.org/abs/2403.05478v1","category":"cs.RO"}
{"created":"2024-03-08 17:36:42","title":"Quasiparticle band structure and excitonic optical response in V2O5 bulk and monolayer","abstract":"The electronic band structure of V$_2$O$_5$ is calculated using an all-electron quasiparticle self-consistent (QS) $GW$ method, including electron-hole ladder diagrams in the screening of $W$. The optical dielectric function calculated with the Bethe-Salpeter equation exhibits excitons with large binding energy, consistent with spectroscopic ellipsometry data and other recent calculations. Sharp peaks in the direction perpendicular to the layers at high energy are found to be an artifact of the truncation of the numbers of bands included in the BSE calculation of the macroscopic dielectric function. The $\\varepsilon_1(\\omega=0)$ gives indices of refraction in good agreement with experiment. The excitons are charge transfer excitons with the hole primarily on oxygen and electrons on vanadium, but depending on which exciton, the distribution over different oxygens changes. The exciton wave functions have a spread of about 5-15\\AA, with asymmetric character for the electron distribution around the hole depending on which oxygen the hole is fixed at. The monolayer quasiparticle gap increases inversely proportional to interlayer distance once the initial interlayer covalent couplings are removed which is thanks to the long-range nature of the self-energy and the reduced screening in a 2D system. The optical gap on the other hand is relatively independent of interlayer spacing because of the compensation between the self-energy gap shift and the exciton binding energy, both of which are proportional to the screened Coulomb interaction $\\hat{W}$. Recent experimental results on very thin layer V$_2$O$_5$ obtained by chemical exfoliation provide experimental support for an increase in gap.","sentences":["The electronic band structure of V$_2$O$_5$ is calculated using an all-electron quasiparticle self-consistent (QS) $GW$ method, including electron-hole ladder diagrams in the screening of $W$. The optical dielectric function calculated with the Bethe-Salpeter equation exhibits excitons with large binding energy, consistent with spectroscopic ellipsometry data and other recent calculations.","Sharp peaks in the direction perpendicular to the layers at high energy are found to be an artifact of the truncation of the numbers of bands included in the BSE calculation of the macroscopic dielectric function.","The $\\varepsilon_1(\\omega=0)$ gives indices of refraction in good agreement with experiment.","The excitons are","charge transfer excitons with the hole primarily on oxygen and electrons on vanadium, but depending on which exciton, the distribution over different oxygens changes.","The exciton wave functions have a spread of about 5-15\\AA, with asymmetric character for the electron distribution around the hole depending on which oxygen the hole is fixed at.","The monolayer quasiparticle gap increases inversely proportional to interlayer distance once the initial interlayer covalent couplings are removed which is thanks to the long-range nature of the self-energy and the reduced screening in a 2D system.","The optical gap on the other hand is relatively independent of interlayer spacing because of the compensation between the self-energy gap shift and the exciton binding energy, both of which are proportional to the screened Coulomb interaction $\\hat{W}$. Recent experimental results on very thin layer V$_2$O$_5$ obtained by chemical exfoliation provide experimental support for an increase in gap."],"url":"http://arxiv.org/abs/2403.05473v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-08 17:34:40","title":"Federated Joint Learning of Robot Networks in Stroke Rehabilitation","abstract":"Advanced by rich perception and precise execution, robots possess immense potential to provide professional and customized rehabilitation exercises for patients with mobility impairments caused by strokes. Autonomous robotic rehabilitation significantly reduces human workloads in the long and tedious rehabilitation process. However, training a rehabilitation robot is challenging due to the data scarcity issue. This challenge arises from privacy concerns (e.g., the risk of leaking private disease and identity information of patients) during clinical data access and usage. Data from various patients and hospitals cannot be shared for adequate robot training, further compromising rehabilitation safety and limiting implementation scopes. To address this challenge, this work developed a novel federated joint learning (FJL) method to jointly train robots across hospitals. FJL also adopted a long short-term memory network (LSTM)-Transformer learning mechanism to effectively explore the complex tempo-spatial relations among patient mobility conditions and robotic rehabilitation motions. To validate FJL's effectiveness in training a robot network, a clinic-simulation combined experiment was designed. Real rehabilitation exercise data from 200 patients with stroke diseases (upper limb hemiplegia, Parkinson's syndrome, and back pain syndrome) were adopted. Inversely driven by clinical data, 300,000 robotic rehabilitation guidances were simulated. FJL proved to be effective in joint rehabilitation learning, performing 20% - 30% better than baseline methods.","sentences":["Advanced by rich perception and precise execution, robots possess immense potential to provide professional and customized rehabilitation exercises for patients with mobility impairments caused by strokes.","Autonomous robotic rehabilitation significantly reduces human workloads in the long and tedious rehabilitation process.","However, training a rehabilitation robot is challenging due to the data scarcity issue.","This challenge arises from privacy concerns (e.g., the risk of leaking private disease and identity information of patients) during clinical data access and usage.","Data from various patients and hospitals cannot be shared for adequate robot training, further compromising rehabilitation safety and limiting implementation scopes.","To address this challenge, this work developed a novel federated joint learning (FJL) method to jointly train robots across hospitals.","FJL also adopted a long short-term memory network (LSTM)-Transformer learning mechanism to effectively explore the complex tempo-spatial relations among patient mobility conditions and robotic rehabilitation motions.","To validate FJL's effectiveness in training a robot network, a clinic-simulation combined experiment was designed.","Real rehabilitation exercise data from 200 patients with stroke diseases (upper limb hemiplegia, Parkinson's syndrome, and back pain syndrome) were adopted.","Inversely driven by clinical data, 300,000 robotic rehabilitation guidances were simulated.","FJL proved to be effective in joint rehabilitation learning, performing 20% - 30% better than baseline methods."],"url":"http://arxiv.org/abs/2403.05472v1","category":"cs.RO"}
{"created":"2024-03-08 17:31:47","title":"Observation of Topological Hall Effect and Skyrmions in Pt/Co/Ir/Co/Pt System","abstract":"The interlayer exchange coupling (IEC) between two ferromagnetic (FM) layers separated by a non-magnetic (NM) spacer layer gives rise to different types of coupling with the variation of spacer layer thickness. When the NM is metallic, the IEC is attributed to the well known Ruderman Kittel Kasuya Yosida (RKKY) interaction which shows an oscillatory decaying nature with increasing thickness. Due to this, it is possible to tune the coupling between the two FM to be either ferromagnetic or antiferromagnetic. In this work we have studied a Pt/Co/Ir/Co/Pt system where the Co thickness has been taken in the strong perpendicular magnetic anisotropy regime which is much less than the spin reorientation transition thickness. By tuning the Ir thickness to 2.0 nm, a canted state of magnetization reversal in the system is observed which gives rise to a possibility of nucleating topologically non trivial spin textures like skyrmions. Further, with the combination of transport and magnetic force microscopy (MFM) measurements, we have confirmed the presence of skyrmions in our system. These findings may be useful for potential applications in emerging spintronic and data storage technologies using skyrmions.","sentences":["The interlayer exchange coupling (IEC) between two ferromagnetic (FM) layers separated by a non-magnetic (NM) spacer layer gives rise to different types of coupling with the variation of spacer layer thickness.","When the NM is metallic, the IEC is attributed to the well known Ruderman Kittel Kasuya Yosida (RKKY) interaction which shows an oscillatory decaying nature with increasing thickness.","Due to this, it is possible to tune the coupling between the two FM to be either ferromagnetic or antiferromagnetic.","In this work we have studied a Pt/Co/Ir/Co/Pt system where the Co thickness has been taken in the strong perpendicular magnetic anisotropy regime which is much less than the spin reorientation transition thickness.","By tuning the Ir thickness to 2.0 nm, a canted state of magnetization reversal in the system is observed which gives rise to a possibility of nucleating topologically non trivial spin textures like skyrmions.","Further, with the combination of transport and magnetic force microscopy (MFM) measurements, we have confirmed the presence of skyrmions in our system.","These findings may be useful for potential applications in emerging spintronic and data storage technologies using skyrmions."],"url":"http://arxiv.org/abs/2403.05469v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-08 17:15:34","title":"Rubidium and cesium ion-induced electron and ion signals for scanning ion microscopy applications","abstract":"Scanning ion microscopy applications of novel focused ion beam (FIB) systems based on ultracold rubidium (Rb) and cesium (Cs) atoms were investigated via ion-induced electron and ion yields. Results measured on the Rb$^+$ and Cs$^+$ FIB systems were compared with results from commercially available gallium (Ga$^+$) systems to verify the merits of applying Rb$^+$ and Cs$^+$ for imaging. The comparison shows that Rb$^+$ and Cs$^+$ have higher secondary electron (SE) yields on a variety of pure element targets than Ga$^+$, which implies a higher signal-to-noise ratio can be achieved for the same dose in SE imaging using Rb$^+$/Cs$^+$ than Ga$^+$. In addition, analysis of the ion-induced ion signals reveals that secondary ions dominate Cs$^+$ induced ion signals while the Rb$^+$/Ga$^+$ induced signals contain more backscattered ions.","sentences":["Scanning ion microscopy applications of novel focused ion beam (FIB) systems based on ultracold rubidium (Rb) and cesium (Cs) atoms were investigated via ion-induced electron and ion yields.","Results measured on the Rb$^+$ and Cs$^+$ FIB systems were compared with results from commercially available gallium (Ga$^+$) systems to verify the merits of applying Rb$^+$ and Cs$^+$ for imaging.","The comparison shows that Rb$^+$ and Cs$^+$ have higher secondary electron (SE) yields on a variety of pure element targets than Ga$^+$, which implies a higher signal-to-noise ratio can be achieved for the same dose in SE imaging using Rb$^+$/Cs$^+$ than Ga$^+$.","In addition, analysis of the ion-induced ion signals reveals that secondary ions dominate Cs$^+$ induced ion signals while the Rb$^+$/Ga$^+$ induced signals contain more backscattered ions."],"url":"http://arxiv.org/abs/2403.05460v1","category":"physics.ins-det"}
{"created":"2024-03-08 17:07:37","title":"Sparse dynamic network reconstruction through L1-regularization of a Lyapunov equation","abstract":"An important problem in many areas of science is that of recovering interaction networks from simultaneous time-series of many interacting dynamical processes. A common approach is to use the elements of the correlation matrix or its inverse as proxies of the interaction strengths, but the reconstructed networks are necessarily undirected. Transfer entropy methods have been proposed to reconstruct directed networks but the reconstructed network lacks information about interaction strengths. We propose a network reconstruction method that inherits the best of the two approaches by reconstructing a directed weighted network from noisy data under the assumption that the network is sparse and the dynamics are governed by a linear (or weakly-nonlinear) stochastic dynamical system. The two steps of our method are i) constructing an (infinite) family of candidate networks by solving the covariance matrix Lyapunov equation for the state matrix and ii) using L1-regularization to select a sparse solution. We further show how to use prior information on the (non)existence of a few directed edges to drastically improve the quality of the reconstruction.","sentences":["An important problem in many areas of science is that of recovering interaction networks from simultaneous time-series of many interacting dynamical processes.","A common approach is to use the elements of the correlation matrix or its inverse as proxies of the interaction strengths, but the reconstructed networks are necessarily undirected.","Transfer entropy methods have been proposed to reconstruct directed networks but the reconstructed network lacks information about interaction strengths.","We propose a network reconstruction method that inherits the best of the two approaches by reconstructing a directed weighted network from noisy data under the assumption that the network is sparse and the dynamics are governed by a linear (or weakly-nonlinear) stochastic dynamical system.","The two steps of our method are i) constructing an (infinite) family of candidate networks by solving the covariance matrix Lyapunov equation for the state matrix and ii) using L1-regularization to select a sparse solution.","We further show how to use prior information on the (non)existence of a few directed edges to drastically improve the quality of the reconstruction."],"url":"http://arxiv.org/abs/2403.05457v1","category":"eess.SY"}
{"created":"2024-03-08 17:00:43","title":"Quantitative Propagation of Chaos for Singular Interacting Particle Systems Driven by Fractional Brownian Motion","abstract":"We consider interacting systems particle driven by i.i.d. fractional Brownian motions, subject to irregular, possibly distributional, pairwise interactions. We show propagation of chaos and mean field convergence to the law of the associated McKean--Vlasov equation, as the number of particles $N\\to\\infty$, with quantitative sharp rates of order $N^{-1/2}$. Our results hold for a wide class of possibly time-dependent interactions, which are only assumed to satisfy a Besov-type regularity, related to the Hurst parameter $H\\in (0,+\\infty)\\setminus \\mathbb{N}$ of the driving noises. In particular, as $H$ decreases to $0$, interaction kernels of arbitrary singularity can be considered, a phenomenon frequently observed in regularization by noise results. Our proofs rely on a combinations of Sznitman's direct comparison argument with stochastic sewing techniques.","sentences":["We consider interacting systems particle driven by i.i.d. fractional Brownian motions, subject to irregular, possibly distributional, pairwise interactions.","We show propagation of chaos and mean field convergence to the law of the associated McKean--Vlasov equation, as the number of particles $N\\to\\infty$, with quantitative sharp rates of order $N^{-1/2}$. Our results hold for a wide class of possibly time-dependent interactions, which are only assumed to satisfy a Besov-type regularity, related to the Hurst parameter $H\\in (0,+\\infty)\\setminus \\mathbb{N}$ of the driving noises.","In particular, as $H$ decreases to $0$, interaction kernels of arbitrary singularity can be considered, a phenomenon frequently observed in regularization by noise results.","Our proofs rely on a combinations of Sznitman's direct comparison argument with stochastic sewing techniques."],"url":"http://arxiv.org/abs/2403.05454v1","category":"math.PR"}
{"created":"2024-03-08 16:57:47","title":"Attention-guided Feature Distillation for Semantic Segmentation","abstract":"In contrast to existing complex methodologies commonly employed for distilling knowledge from a teacher to a student, the pro-posed method showcases the efficacy of a simple yet powerful method for utilizing refined feature maps to transfer attention. The proposed method has proven to be effective in distilling rich information, outperforming existing methods in semantic segmentation as a dense prediction task. The proposed Attention-guided Feature Distillation (AttnFD) method, em-ploys the Convolutional Block Attention Module (CBAM), which refines feature maps by taking into account both channel-specific and spatial information content. By only using the Mean Squared Error (MSE) loss function between the refined feature maps of the teacher and the student,AttnFD demonstrates outstanding performance in semantic segmentation, achieving state-of-the-art results in terms of mean Intersection over Union (mIoU) on the PascalVoc 2012 and Cityscapes datasets. The Code is available at https://github.com/AmirMansurian/AttnFD.","sentences":["In contrast to existing complex methodologies commonly employed for distilling knowledge from a teacher to a student, the pro-posed method showcases the efficacy of a simple yet powerful method for utilizing refined feature maps to transfer attention.","The proposed method has proven to be effective in distilling rich information, outperforming existing methods in semantic segmentation as a dense prediction task.","The proposed Attention-guided Feature Distillation (AttnFD) method, em-ploys the Convolutional Block Attention Module (CBAM), which refines feature maps by taking into account both channel-specific and spatial information content.","By only using the Mean Squared Error (MSE) loss function between the refined feature maps of the teacher and the student,AttnFD demonstrates outstanding performance in semantic segmentation, achieving state-of-the-art results in terms of mean Intersection over Union (mIoU) on the PascalVoc 2012 and Cityscapes datasets.","The Code is available at https://github.com/AmirMansurian/AttnFD."],"url":"http://arxiv.org/abs/2403.05451v1","category":"cs.CV"}
{"created":"2024-03-08 16:57:18","title":"Exploiting polar symmetry in designing equivariant observers for vision-based motion estimation","abstract":"Accurately estimating camera motion from image sequences poses a significant challenge in computer vision and robotics. Many computer vision methods first compute the essential matrix associated with a motion and then extract orientation and normalized translation as inputs to pose estimation, reconstructing the scene scale (that is unobservable in the epipolar construction) from separate information. In this paper, we design a continuous-time filter that exploits the same perspective by using the epipolar constraint to define pseudo-measurements. We propose a novel polar symmetry on the pose of the camera that makes these measurements equivariant. This allows us to apply recent results from equivariant systems theory to estimating pose. We provide a novel explicit persistence of excitation condition to characterize observability of the full pose, ensuring reconstruction of the scale parameter that is not directly observable in the epipolar construction.","sentences":["Accurately estimating camera motion from image sequences poses a significant challenge in computer vision and robotics.","Many computer vision methods first compute the essential matrix associated with a motion and then extract orientation and normalized translation as inputs to pose estimation, reconstructing the scene scale (that is unobservable in the epipolar construction) from separate information.","In this paper, we design a continuous-time filter that exploits the same perspective by using the epipolar constraint to define pseudo-measurements.","We propose a novel polar symmetry on the pose of the camera that makes these measurements equivariant.","This allows us to apply recent results from equivariant systems theory to estimating pose.","We provide a novel explicit persistence of excitation condition to characterize observability of the full pose, ensuring reconstruction of the scale parameter that is not directly observable in the epipolar construction."],"url":"http://arxiv.org/abs/2403.05450v1","category":"eess.SY"}
{"created":"2024-03-08 16:55:20","title":"On Practicality of Using ARM TrustZone Trusted Execution Environment for Securing Programmable Logic Controllers","abstract":"Programmable logic controllers (PLCs) are crucial devices for implementing automated control in various industrial control systems (ICS), such as smart power grids, water treatment systems, manufacturing, and transportation systems. Owing to their importance, PLCs are often the target of cyber attackers that are aiming at disrupting the operation of ICS, including the nation's critical infrastructure, by compromising the integrity of control logic execution. While a wide range of cybersecurity solutions for ICS have been proposed, they cannot counter strong adversaries with a foothold on the PLC devices, which could manipulate memory, I/O interface, or PLC logic itself. These days, many ICS devices in the market, including PLCs, run on ARM-based processors, and there is a promising security technology called ARM TrustZone, to offer a Trusted Execution Environment (TEE) on embedded devices. Envisioning that such a hardware-assisted security feature becomes available for ICS devices in the near future, this paper investigates the application of the ARM TrustZone TEE technology for enhancing the security of PLC. Our aim is to evaluate the feasibility and practicality of the TEE-based PLCs through the proof-of-concept design and implementation using open-source software such as OP-TEE and OpenPLC. Our evaluation assesses the performance and resource consumption in real-world ICS configurations, and based on the results, we discuss bottlenecks in the OP-TEE secure OS towards a large-scale ICS and desired changes for its application on ICS devices. Our implementation is made available to public for further study and research.","sentences":["Programmable logic controllers (PLCs) are crucial devices for implementing automated control in various industrial control systems (ICS), such as smart power grids, water treatment systems, manufacturing, and transportation systems.","Owing to their importance, PLCs are often the target of cyber attackers that are aiming at disrupting the operation of ICS, including the nation's critical infrastructure, by compromising the integrity of control logic execution.","While a wide range of cybersecurity solutions for ICS have been proposed, they cannot counter strong adversaries with a foothold on the PLC devices, which could manipulate memory, I/O interface, or PLC logic itself.","These days, many ICS devices in the market, including PLCs, run on ARM-based processors, and there is a promising security technology called ARM TrustZone, to offer a Trusted Execution Environment (TEE) on embedded devices.","Envisioning that such a hardware-assisted security feature becomes available for ICS devices in the near future, this paper investigates the application of the ARM TrustZone TEE technology for enhancing the security of PLC.","Our aim is to evaluate the feasibility and practicality of the TEE-based PLCs through the proof-of-concept design and implementation using open-source software such as OP-TEE and OpenPLC.","Our evaluation assesses the performance and resource consumption in real-world ICS configurations, and based on the results, we discuss bottlenecks in the OP-TEE secure OS towards a large-scale ICS and desired changes for its application on ICS devices.","Our implementation is made available to public for further study and research."],"url":"http://arxiv.org/abs/2403.05448v1","category":"cs.CR"}
{"created":"2024-03-08 16:54:27","title":"An Improved Algorithm for Learning Drifting Discrete Distributions","abstract":"We present a new adaptive algorithm for learning discrete distributions under distribution drift. In this setting, we observe a sequence of independent samples from a discrete distribution that is changing over time, and the goal is to estimate the current distribution. Since we have access to only a single sample for each time step, a good estimation requires a careful choice of the number of past samples to use. To use more samples, we must resort to samples further in the past, and we incur a drift error due to the bias introduced by the change in distribution. On the other hand, if we use a small number of past samples, we incur a large statistical error as the estimation has a high variance. We present a novel adaptive algorithm that can solve this trade-off without any prior knowledge of the drift. Unlike previous adaptive results, our algorithm characterizes the statistical error using data-dependent bounds. This technicality enables us to overcome the limitations of the previous work that require a fixed finite support whose size is known in advance and that cannot change over time. Additionally, we can obtain tighter bounds depending on the complexity of the drifting distribution, and also consider distributions with infinite support.","sentences":["We present a new adaptive algorithm for learning discrete distributions under distribution drift.","In this setting, we observe a sequence of independent samples from a discrete distribution that is changing over time, and the goal is to estimate the current distribution.","Since we have access to only a single sample for each time step, a good estimation requires a careful choice of the number of past samples to use.","To use more samples, we must resort to samples further in the past, and we incur a drift error due to the bias introduced by the change in distribution.","On the other hand, if we use a small number of past samples, we incur a large statistical error as the estimation has a high variance.","We present a novel adaptive algorithm that can solve this trade-off without any prior knowledge of the drift.","Unlike previous adaptive results, our algorithm characterizes the statistical error using data-dependent bounds.","This technicality enables us to overcome the limitations of the previous work that require a fixed finite support whose size is known in advance and that cannot change over time.","Additionally, we can obtain tighter bounds depending on the complexity of the drifting distribution, and also consider distributions with infinite support."],"url":"http://arxiv.org/abs/2403.05446v1","category":"cs.LG"}
{"created":"2024-03-08 16:52:06","title":"Simulating dynamics of ellipsoidal particles using lattice Boltzmann method","abstract":"Anisotropic particles are often encountered in different fields of soft matter and complex fluids. In this work, we present an implementation of the coupled hydrodynamics of solid ellipsoidal particles and the surrounding fluid using the lattice Boltzmann method. A standard link-based mechanism is used to implement the solid-fluid boundary conditions. We develop an implicit method to update the position and orientation of the ellipsoid. This exploits the relations between the quaternion which describes the ellipsoid's orientation and the ellipsoid's angular velocity to obtain a stable and robust dynamic update. The proposed algorithm is validated by looking at four scenarios: (i) the steady translational velocity of a spheroid subject to an external force in different orientations, (ii) the drift of an inclined spheroid subject to an imposed force, (iii) three-dimensional rotational motions in a simple shear flow (Jeffrey's orbits), and (iv) developed fluid flows and self-propulsion exhibited by a spheroidal microswimmer. In all cases the comparison of numerical results showed good agreement with known analytical solutions, irrespective of the choice of the fluid properties, geometrical parameters, and lattice Boltzmann model, thus demonstrating the robustness of the proposed algorithm.","sentences":["Anisotropic particles are often encountered in different fields of soft matter and complex fluids.","In this work, we present an implementation of the coupled hydrodynamics of solid ellipsoidal particles and the surrounding fluid using the lattice Boltzmann method.","A standard link-based mechanism is used to implement the solid-fluid boundary conditions.","We develop an implicit method to update the position and orientation of the ellipsoid.","This exploits the relations between the quaternion which describes the ellipsoid's orientation and the ellipsoid's angular velocity to obtain a stable and robust dynamic update.","The proposed algorithm is validated by looking at four scenarios: (i) the steady translational velocity of a spheroid subject to an external force in different orientations, (ii) the drift of an inclined spheroid subject to an imposed force, (iii) three-dimensional rotational motions in a simple shear flow (Jeffrey's orbits), and (iv) developed fluid flows and self-propulsion exhibited by a spheroidal microswimmer.","In all cases the comparison of numerical results showed good agreement with known analytical solutions, irrespective of the choice of the fluid properties, geometrical parameters, and lattice Boltzmann model, thus demonstrating the robustness of the proposed algorithm."],"url":"http://arxiv.org/abs/2403.05443v1","category":"cond-mat.soft"}
{"created":"2024-03-08 16:38:42","title":"Positive Pluriharmonic Functions on Symmetric Siegel Domains","abstract":"Given a symmetric Siegel domain $\\mathscr D$ and a positive plurihamonic function $f$ on $\\mathscr D$, we study the largest positive Radon measure $\\mu$ on the Silov boundary $\\mathrm b \\mathscr D$ of $\\mathscr D$ whose Poisson integral $\\mathscr P \\mu$ is $\\leq f$. If $\\mathscr D$ has no tubular irreducible factors of rank $\\geq 2$, we show that $\\mathscr P \\mu$ is plurihamonic, and that $f-\\mathscr P \\mu$ is linear. As an application, we describe a possible analogue of the family of Clark measures associated with a holomorphic function from $\\mathscr D$ into the unit disc in $\\mathbb C$.","sentences":["Given a symmetric Siegel domain $\\mathscr D$ and a positive plurihamonic function $f$ on $\\mathscr D$, we study the largest positive Radon measure $\\mu$ on the Silov boundary $\\mathrm b \\mathscr D$ of $\\mathscr D$ whose Poisson integral $\\mathscr P \\mu$ is $\\leq f$.","If $\\mathscr D$ has no tubular irreducible factors of rank $\\geq 2$, we show that $\\mathscr P \\mu$ is plurihamonic, and that $f-\\mathscr P \\mu$ is linear.","As an application, we describe a possible analogue of the family of Clark measures associated with a holomorphic function from $\\mathscr D$ into the unit disc in $\\mathbb C$."],"url":"http://arxiv.org/abs/2403.05436v1","category":"math.CV"}
{"created":"2024-03-08 16:33:49","title":"3d-oxide molecules to tailor large magnetic anisotropy energies on MgO films","abstract":"Designing systems with large magnetic anisotropy energy (MAE) is desirable and critical for nanoscale magnetic devices. A recent breakthrough achieved the theoretical limit of the MAE for 3$d$ transition metal atoms by placing a single Co atom on a MgO(100) surface, a result not replicated by standard first-principles simulations. Our study, incorporating Hubbard-$U$ correction and spin-orbit coupling, successfully reproduces and explains the high MAE of a Co adatom on a MgO (001) surface. We go further by exploring ways to enhance MAE in 3d transition metal adatoms through different structural geometries of 3d--O molecules on MgO. One promising structure, with molecules perpendicular to the surface, enhances MAE while reducing substrate interaction, minimizing spin fluctuations, and boosting magnetic stability. Additionally, we demonstrate significant control over MAE by precisely placing 3d--O molecules on the substrate at the atomic level.","sentences":["Designing systems with large magnetic anisotropy energy (MAE) is desirable and critical for nanoscale magnetic devices.","A recent breakthrough achieved the theoretical limit of the MAE for 3$d$ transition metal atoms by placing a single Co atom on a MgO(100) surface, a result not replicated by standard first-principles simulations.","Our study, incorporating Hubbard-$U$ correction and spin-orbit coupling, successfully reproduces and explains the high MAE of a Co adatom on a MgO (001) surface.","We go further by exploring ways to enhance MAE in 3d transition metal adatoms through different structural geometries of 3d--O molecules on MgO. One promising structure, with molecules perpendicular to the surface, enhances MAE while reducing substrate interaction, minimizing spin fluctuations, and boosting magnetic stability.","Additionally, we demonstrate significant control over MAE by precisely placing 3d--O molecules on the substrate at the atomic level."],"url":"http://arxiv.org/abs/2403.05432v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-08 16:31:27","title":"MambaLithium: Selective state space model for remaining-useful-life, state-of-health, and state-of-charge estimation of lithium-ion batteries","abstract":"Recently, lithium-ion batteries occupy a pivotal position in the realm of electric vehicles and the burgeoning new energy industry. Their performance is heavily dependent on three core states: remaining-useful-life (RUL), state-of-health (SOH), and state-of-charge (SOC). Given the remarkable success of Mamba (Structured state space sequence models with selection mechanism and scan module, S6) in sequence modeling tasks, this paper introduces MambaLithium, a selective state space model tailored for precise estimation of these critical battery states. Leveraging Mamba algorithms, MambaLithium adeptly captures the intricate aging and charging dynamics of lithium-ion batteries. By focusing on pivotal states within the battery's operational envelope, MambaLithium not only enhances estimation accuracy but also maintains computational robustness. Experiments conducted using real-world battery data have validated the model's superiority in predicting battery health and performance metrics, surpassing current methods. The proposed MambaLithium framework is potential for applications in advancing battery management systems and fostering sustainable energy storage solutions. Source code is available at https://github.com/zshicode/MambaLithium.","sentences":["Recently, lithium-ion batteries occupy a pivotal position in the realm of electric vehicles and the burgeoning new energy industry.","Their performance is heavily dependent on three core states: remaining-useful-life (RUL), state-of-health (SOH), and state-of-charge (SOC).","Given the remarkable success of Mamba (Structured state space sequence models with selection mechanism and scan module, S6) in sequence modeling tasks, this paper introduces MambaLithium, a selective state space model tailored for precise estimation of these critical battery states.","Leveraging Mamba algorithms, MambaLithium adeptly captures the intricate aging and charging dynamics of lithium-ion batteries.","By focusing on pivotal states within the battery's operational envelope, MambaLithium not only enhances estimation accuracy but also maintains computational robustness.","Experiments conducted using real-world battery data have validated the model's superiority in predicting battery health and performance metrics, surpassing current methods.","The proposed MambaLithium framework is potential for applications in advancing battery management systems and fostering sustainable energy storage solutions.","Source code is available at https://github.com/zshicode/MambaLithium."],"url":"http://arxiv.org/abs/2403.05430v1","category":"cs.CE"}
{"created":"2024-03-08 16:31:12","title":"Clark Measures on Bounded Symmetric Domains","abstract":"Given a bounded symmetric domain $D$, we study (positive) pluriharmonic functions on $D$ and investigate a possible analogue of the family of Clark measures associated with a holomorphic function from $D$ into the unit disc in $\\mathbb C$.","sentences":["Given a bounded symmetric domain $D$, we study (positive) pluriharmonic functions on $D$ and investigate a possible analogue of the family of Clark measures associated with a holomorphic function from $D$ into the unit disc in $\\mathbb C$."],"url":"http://arxiv.org/abs/2403.05429v1","category":"math.CV"}
{"created":"2024-03-08 16:20:57","title":"Infinite Translation Surfaces in the Wild","abstract":"This book explores infinite-type translation surfaces and is intended as an introductory text for graduate and PhD students, as well as a reference for more advanced researchers.   Chapter 1 introduces the three definitions of translation surfaces and meticulously proves their equivalence. It is enriched with numerous examples that are revisited throughout the book.   Chapter 2 provides a detailed examination of the topological classification of infinite-type surfaces, the construction of infinite coverings of finite-type translation surfaces, and the structure of points within the metric completion.   Chapter 3 investigates the affine symmetries of infinite-type translation surfaces, with special emphasis on infinite coverings of finite-type surfaces, the Hooper-Thurston-Veech construction, and affine homeomorphisms of finite-area infinite-type translation surfaces.   Chapter 4 introduces infinite interval exchange transformations and employs them to demonstrate that the dynamics of translation flows are significantly more complex in the infinite-type context.   The two appendices address hyperbolic geometry and the spectra of infinite graphs, respectively.","sentences":["This book explores infinite-type translation surfaces and is intended as an introductory text for graduate and PhD students, as well as a reference for more advanced researchers.   ","Chapter 1 introduces the three definitions of translation surfaces and meticulously proves their equivalence.","It is enriched with numerous examples that are revisited throughout the book.   ","Chapter 2 provides a detailed examination of the topological classification of infinite-type surfaces, the construction of infinite coverings of finite-type translation surfaces, and the structure of points within the metric completion.   ","Chapter 3 investigates the affine symmetries of infinite-type translation surfaces, with special emphasis on infinite coverings of finite-type surfaces, the Hooper-Thurston-Veech construction, and affine homeomorphisms of finite-area infinite-type translation surfaces.   ","Chapter 4 introduces infinite interval exchange transformations and employs them to demonstrate that the dynamics of translation flows are significantly more complex in the infinite-type context.   ","The two appendices address hyperbolic geometry and the spectra of infinite graphs, respectively."],"url":"http://arxiv.org/abs/2403.05424v1","category":"math.GT"}
{"created":"2024-03-08 16:17:24","title":"On balanceable and simply balanceable regular graphs","abstract":"We continue the study of balanceable graphs, defined by Caro, Hansberg, and Montejano in 2021 as graphs $G$ such that any $2$-coloring of the edges of a sufficiently large complete graph containing sufficiently many edges of each color contains a balanced copy of $G$. While the problem of recognizing balanceable graphs was conjectured to be NP-complete by Dailly, Hansberg, and Ventura in 2021, balanceable graphs admit an elegant combinatorial characterization: a graph is balanceable if and only there exist two vertex subsets, one containing half of all the graph's edges and another one such that the corresponding cut contains half of all the graph's edges. We consider a special case of this property, namely when one of the two sets is a vertex cover, and call the corresponding graphs simply balanceable. We prove a number of results on balanceable and simply balanceable regular graphs. First, we characterize simply balanceable regular graphs via a condition involving the independence number of the graph. Second, we address a question of Dailly, Hansberg, and Ventura from 2021 and show that every cubic graph is balanceable. Third, using Brooks' theorem, we show that every $4$-regular graph with order divisible by $4$ is balanceable. Finally, we show that it is NP-complete to determine if a $9$-regular graph is simply balanceable.","sentences":["We continue the study of balanceable graphs, defined by Caro, Hansberg, and Montejano in 2021 as graphs $G$ such that any $2$-coloring of the edges of a sufficiently large complete graph containing sufficiently many edges of each color contains a balanced copy of $G$. While the problem of recognizing balanceable graphs was conjectured to be NP-complete by Dailly, Hansberg, and Ventura in 2021, balanceable graphs admit an elegant combinatorial characterization: a graph is balanceable if and only there exist two vertex subsets, one containing half of all the graph's edges and another one such that the corresponding cut contains half of all the graph's edges.","We consider a special case of this property, namely when one of the two sets is a vertex cover, and call the corresponding graphs simply balanceable.","We prove a number of results on balanceable and simply balanceable regular graphs.","First, we characterize simply balanceable regular graphs via a condition involving the independence number of the graph.","Second, we address a question of Dailly, Hansberg, and Ventura from 2021 and show that every cubic graph is balanceable.","Third, using Brooks' theorem, we show that every $4$-regular graph with order divisible by $4$ is balanceable.","Finally, we show that it is NP-complete to determine if a $9$-regular graph is simply balanceable."],"url":"http://arxiv.org/abs/2403.05418v1","category":"math.CO"}
{"created":"2024-03-08 16:15:32","title":"We Know I Know You Know; Choreographic Programming With Multicast and Multiply Located Values","abstract":"Concurrent distributed systems are notoriously difficult to construct and reason about. Choreographic programming is a recent paradigm that describes a distributed system in a single global program called a choreography. Choreographies simplify reasoning about distributed systems and can ensure deadlock freedom by static analysis. In previous choreographic programming languages, each value is located at a single party, and the programmer is expected to insert special untyped \"select\" operations to ensure that all parties follow the same communication pattern.   We present He-Lambda-Small, a new choreographic programming language with Multiply Located Values. He-Lambda-Small allows multicasting to a set of parties, and the resulting value will be located at all of them. This approach enables a simple and elegant alternative to \"select\": He-Lambda-Small requires that the guard for a conditional be located at all of the relevant parties. In He-Lambda-Small, checking that a choreography is well-typed suffices to show that it is deadlock-free. We present several case studies that demonstrate the use of multiply-located values to concisely encode tricky communication patterns described in previous work without the use of \"select\" or redundant communication.","sentences":["Concurrent distributed systems are notoriously difficult to construct and reason about.","Choreographic programming is a recent paradigm that describes a distributed system in a single global program called a choreography.","Choreographies simplify reasoning about distributed systems and can ensure deadlock freedom by static analysis.","In previous choreographic programming languages, each value is located at a single party, and the programmer is expected to insert special untyped \"select\" operations to ensure that all parties follow the same communication pattern.   ","We present He-Lambda-Small, a new choreographic programming language with Multiply Located Values.","He-Lambda-Small allows multicasting to a set of parties, and the resulting value will be located at all of them.","This approach enables a simple and elegant alternative to \"select\": He-Lambda-Small requires that the guard for a conditional be located at all of the relevant parties.","In He-Lambda-Small, checking that a choreography is well-typed suffices to show that it is deadlock-free.","We present several case studies that demonstrate the use of multiply-located values to concisely encode tricky communication patterns described in previous work without the use of \"select\" or redundant communication."],"url":"http://arxiv.org/abs/2403.05417v1","category":"cs.PL"}
{"created":"2024-03-08 16:14:49","title":"An Overview of Automated Vehicle Platooning Strategies","abstract":"Automated vehicle (AV) platooning has the potential to improve the safety, operational, and energy efficiency of surface transportation systems by limiting or eliminating human involvement in the driving tasks. The theoretical validity of the AV platooning strategies has been established and practical applications are being tested under real-world conditions. The emergence of sensors, communication, and control strategies has resulted in rapid and constant evolution of AV platooning strategies. In this paper, we review the state-of-the-art knowledge in AV platooning using a five-component platooning framework, which includes vehicle model, information-receiving process, information flow topology, spacing policy, and controller and discuss the advantages and limitations of the components. Based on the discussion about existing strategies and associated limitations, potential future research directions are presented.","sentences":["Automated vehicle (AV) platooning has the potential to improve the safety, operational, and energy efficiency of surface transportation systems by limiting or eliminating human involvement in the driving tasks.","The theoretical validity of the AV platooning strategies has been established and practical applications are being tested under real-world conditions.","The emergence of sensors, communication, and control strategies has resulted in rapid and constant evolution of AV platooning strategies.","In this paper, we review the state-of-the-art knowledge in AV platooning using a five-component platooning framework, which includes vehicle model, information-receiving process, information flow topology, spacing policy, and controller and discuss the advantages and limitations of the components.","Based on the discussion about existing strategies and associated limitations, potential future research directions are presented."],"url":"http://arxiv.org/abs/2403.05415v1","category":"eess.SY"}
{"created":"2024-03-08 16:09:26","title":"Radiation transport methods in star formation simulations","abstract":"Radiation transport plays a crucial role in star formation models, as certain questions within this field cannot be accurately addressed without taking it into account. Given the high complexity of the interstellar medium from which stars form, numerical simulations are frequently employed to model the star formation process. This study reviews recent methods for incorporating radiation transport into star formation simulations, discussing them in terms of the used algorithms, treatment of radiation frequency dependence, the interaction of radiation with the gas, and the parallelization of methods for deployment on supercomputers. Broadly, the algorithms fall into two categories: (i) moment-based methods, encompassing the flux-limited diffusion approximation, M1 closure, and variable Eddington tensor methods, and (ii) methods directly solving the radiation transport equation, including forward and reverse ray tracing, characteristics-based methods, and Monte Carlo techniques. Beyond discussing advantages and disadvantages of these methods, the review also lists recent radiation hydrodynamic codes implemented the described methods.","sentences":["Radiation transport plays a crucial role in star formation models, as certain questions within this field cannot be accurately addressed without taking it into account.","Given the high complexity of the interstellar medium from which stars form, numerical simulations are frequently employed to model the star formation process.","This study reviews recent methods for incorporating radiation transport into star formation simulations, discussing them in terms of the used algorithms, treatment of radiation frequency dependence, the interaction of radiation with the gas, and the parallelization of methods for deployment on supercomputers.","Broadly, the algorithms fall into two categories: (i) moment-based methods, encompassing the flux-limited diffusion approximation, M1 closure, and variable Eddington tensor methods, and (ii) methods directly solving the radiation transport equation, including forward and reverse ray tracing, characteristics-based methods, and Monte Carlo techniques.","Beyond discussing advantages and disadvantages of these methods, the review also lists recent radiation hydrodynamic codes implemented the described methods."],"url":"http://arxiv.org/abs/2403.05410v1","category":"astro-ph.IM"}
{"created":"2024-03-08 16:01:47","title":"Experimental set-up for thermal measurements at the nanoscale using an SThM probe with niobium nitride thermometer","abstract":"Scanning Thermal Microscopy (SThM) has become an important measurement tool for characterizing the thermal properties of materials at the nanometer scale. This technique requires a SThM probe that combines an Atomic Force Microscopy (AFM) probe and a very sensitive resistive thermometry; the thermometer being located at the apex of the probe tip allows the mapping of temperature or thermal properties of nanostructured materials with very high spatial resolution. The high interest of the SThM technique in the field of thermal nanoscience currently suffers from a low temperature sensitivity despite its high spatial resolution. To address this challenge, we developed a high vacuum-based AFM system hosting a highly sensitive niobium nitride (NbN) SThM probe to demonstrate its unique performance. As a proof of concept, we utilized this custom-built system to carry out thermal measurements using the 3$\\omega$ method. By measuring the $V_{3\\omega}$ voltage on the NbN resistive thermometer in vacuum conditions we were able to determine the SThM probe's thermal conductance and thermal time constant. The performance of the probe is demonstrated by doing thermal measurements in-contact with a sapphire sample.","sentences":["Scanning Thermal Microscopy (SThM) has become an important measurement tool for characterizing the thermal properties of materials at the nanometer scale.","This technique requires a SThM probe that combines an Atomic Force Microscopy (AFM) probe and a very sensitive resistive thermometry; the thermometer being located at the apex of the probe tip allows the mapping of temperature or thermal properties of nanostructured materials with very high spatial resolution.","The high interest of the SThM technique in the field of thermal nanoscience currently suffers from a low temperature sensitivity despite its high spatial resolution.","To address this challenge, we developed a high vacuum-based AFM system hosting a highly sensitive niobium nitride (NbN) SThM probe to demonstrate its unique performance.","As a proof of concept, we utilized this custom-built system to carry out thermal measurements using the 3$\\omega$ method.","By measuring the $V_{3\\omega}$ voltage on the NbN resistive thermometer in vacuum conditions we were able to determine the SThM probe's thermal conductance and thermal time constant.","The performance of the probe is demonstrated by doing thermal measurements in-contact with a sapphire sample."],"url":"http://arxiv.org/abs/2403.05405v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-08 15:59:26","title":"Comparison of Spatial Visualization Techniques for Radiation in Augmented Reality","abstract":"Augmented Reality (AR) provides a safe and low-cost option for hazardous safety training that allows for the visualization of aspects that may be invisible, such as radiation. Effectively visually communicating such threats in the environment around the user is not straightforward. This work describes visually encoding radiation using the spatial awareness mesh of an AR Head Mounted Display. We leverage the AR device's GPUs to develop a real time solution that accumulates multiple dynamic sources and uses stencils to prevent an environment being over saturated with a visualization, as well as supporting the encoding of direction explicitly in the visualization. We perform a user study (25 participants) of different visualizations and obtain user feedback. Results show that there are complex interactions and while no visual representation was statistically superior or inferior, user opinions vary widely. We also discuss the evaluation approaches and provide recommendations.","sentences":["Augmented Reality (AR) provides a safe and low-cost option for hazardous safety training that allows for the visualization of aspects that may be invisible, such as radiation.","Effectively visually communicating such threats in the environment around the user is not straightforward.","This work describes visually encoding radiation using the spatial awareness mesh of an AR Head Mounted Display.","We leverage the AR device's GPUs to develop a real time solution that accumulates multiple dynamic sources and uses stencils to prevent an environment being over saturated with a visualization, as well as supporting the encoding of direction explicitly in the visualization.","We perform a user study (25 participants) of different visualizations and obtain user feedback.","Results show that there are complex interactions and while no visual representation was statistically superior or inferior, user opinions vary widely.","We also discuss the evaluation approaches and provide recommendations."],"url":"http://arxiv.org/abs/2403.05403v1","category":"cs.HC"}
{"created":"2024-03-08 15:36:15","title":"Multi-qubit Dynamical Decoupling for Enhanced Crosstalk Suppression","abstract":"Dynamical decoupling (DD) is one of the simplest error suppression methods, aiming to enhance the coherence of qubits in open quantum systems. Moreover, DD has demonstrated effectiveness in reducing coherent crosstalk, one major error source in near-term quantum hardware, which manifests from two types of interactions. Static crosstalk exists in various hardware platforms, including superconductor and semiconductor qubits, by virtue of always-on qubit-qubit coupling. Additionally, driven crosstalk may occur as an unwanted drive term due to leakage from driven gates on other qubits. Here we explore a novel staggered DD protocol tailored for multi-qubit systems that suppresses the decoherence error and both types of coherent crosstalk. We develop two experimental setups - an \"idle-idle\" experiment in which two pairs of qubits undergo free evolution simultaneously and a \"driven-idle\" experiment in which one pair is continuously driven during the free evolution of the other pair. These experiments are performed on an IBM Quantum superconducting processor and demonstrate the significant impact of the staggered DD protocol in suppressing both types of coherent crosstalk. When compared to the standard DD sequences from state-of-the-art methodologies with the application of X2 sequences, our staggered DD protocol enhances circuit fidelity by 16.9% and 8.5%, respectively, in addressing these two crosstalk types.","sentences":["Dynamical decoupling (DD) is one of the simplest error suppression methods, aiming to enhance the coherence of qubits in open quantum systems.","Moreover, DD has demonstrated effectiveness in reducing coherent crosstalk, one major error source in near-term quantum hardware, which manifests from two types of interactions.","Static crosstalk exists in various hardware platforms, including superconductor and semiconductor qubits, by virtue of always-on qubit-qubit coupling.","Additionally, driven crosstalk may occur as an unwanted drive term due to leakage from driven gates on other qubits.","Here we explore a novel staggered DD protocol tailored for multi-qubit systems that suppresses the decoherence error and both types of coherent crosstalk.","We develop two experimental setups - an \"idle-idle\" experiment in which two pairs of qubits undergo free evolution simultaneously and a \"driven-idle\" experiment in which one pair is continuously driven during the free evolution of the other pair.","These experiments are performed on an IBM Quantum superconducting processor and demonstrate the significant impact of the staggered DD protocol in suppressing both types of coherent crosstalk.","When compared to the standard DD sequences from state-of-the-art methodologies with the application of X2 sequences, our staggered DD protocol enhances circuit fidelity by 16.9% and 8.5%, respectively, in addressing these two crosstalk types."],"url":"http://arxiv.org/abs/2403.05391v1","category":"quant-ph"}
{"created":"2024-03-08 15:24:02","title":"Thermal cycling induced evolution and colossal exchange bias in MnPS3/Fe3GeTe2 van der Waals heterostructures","abstract":"The exchange bias phenomenon, inherent in exchange-coupled ferromagnetic and antiferromagnetic systems, has intrigued researchers for decades. Van der Waals materials, with their layered structure, provide an optimal platform for probing such physical phenomena. However, achieving a facile and effective means to manipulate exchange bias in pristine van der Waals heterostructures remains challenging. In this study, we investigate the origin of exchange bias in MnPS3/Fe3GeTe2 van der Waals heterostructures. Our work demonstrates a method to modulate unidirectional exchange anisotropy, achieving an unprecedented nearly 1000% variation through simple thermal cycling. Despite the compensated interfacial spin configuration of MnPS3, magneto-transport measurements reveal a huge 170 mT exchange bias at 5 K, the largest observed in pristine van der Waals antiferromagnet-ferromagnet interfaces. This substantial magnitude of the exchange bias is linked to an anomalous weak ferromagnetic ordering in MnPS3 below 40 K. On the other hand, the tunability of exchange bias during thermal cycling is ascribed to the modified arrangement of interfacial atoms and changes in the vdW gap during field cooling. Our findings highlight a robust and easily adjustable exchange bias in van der Waals antiferromagnetic/ferromagnetic heterostructures, presenting a straightforward approach to enhance other interface related spintronic phenomena for practical applications.","sentences":["The exchange bias phenomenon, inherent in exchange-coupled ferromagnetic and antiferromagnetic systems, has intrigued researchers for decades.","Van der Waals materials, with their layered structure, provide an optimal platform for probing such physical phenomena.","However, achieving a facile and effective means to manipulate exchange bias in pristine van der Waals heterostructures remains challenging.","In this study, we investigate the origin of exchange bias in MnPS3/Fe3GeTe2 van der Waals heterostructures.","Our work demonstrates a method to modulate unidirectional exchange anisotropy, achieving an unprecedented nearly 1000% variation through simple thermal cycling.","Despite the compensated interfacial spin configuration of MnPS3, magneto-transport measurements reveal a huge 170 mT exchange bias at 5 K, the largest observed in pristine van der Waals antiferromagnet-ferromagnet interfaces.","This substantial magnitude of the exchange bias is linked to an anomalous weak ferromagnetic ordering in MnPS3 below 40 K. On the other hand, the tunability of exchange bias during thermal cycling is ascribed to the modified arrangement of interfacial atoms and changes in the vdW gap during field cooling.","Our findings highlight a robust and easily adjustable exchange bias in van der Waals antiferromagnetic/ferromagnetic heterostructures, presenting a straightforward approach to enhance other interface related spintronic phenomena for practical applications."],"url":"http://arxiv.org/abs/2403.05383v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-08 15:21:47","title":"Quasiperiodic and periodic extended Hatano-Nelson model: Anomalous localization transition and non-Hermitian skin effect","abstract":"We study the effect of quasiperiodic and periodic onsite potentials in a Hatano-Nelson model with next-nearest-neighbour hopping. By considering a non-reciprocal next-nearest-neighbour hopping and a quasiperiodic onsite potential under periodic boundary conditions, we show a breakdown of the typical correspondence between the delocalization-localization and complex-real transitions as a function of the potential strength. Moreover, we reveal that in the delocalized regime, when the potential strength increases, the eigenstates under OBC exhibit a bipolar non-Hermitian skin effect, i.e., they tend to localize on both the edges instead of localizing on either of the edges. However, when a periodic onsite potential is considered, the system not only exhibits a bipolar skin effect, but also shows a complete direction reversal of the skin effect as a function of the onsite periodic potential.","sentences":["We study the effect of quasiperiodic and periodic onsite potentials in a Hatano-Nelson model with next-nearest-neighbour hopping.","By considering a non-reciprocal next-nearest-neighbour hopping and a quasiperiodic onsite potential under periodic boundary conditions, we show a breakdown of the typical correspondence between the delocalization-localization and complex-real transitions as a function of the potential strength.","Moreover, we reveal that in the delocalized regime, when the potential strength increases, the eigenstates under OBC exhibit a bipolar non-Hermitian skin effect, i.e., they tend to localize on both the edges instead of localizing on either of the edges.","However, when a periodic onsite potential is considered, the system not only exhibits a bipolar skin effect, but also shows a complete direction reversal of the skin effect as a function of the onsite periodic potential."],"url":"http://arxiv.org/abs/2403.05382v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-08 15:07:30","title":"Anisotropic effects in two-dimensional materials","abstract":"Among a huge variety of known two-dimensional materials, some of them have anisotropic crystal structures; examples include so different systems as a few-layer black phoshphorus (phosphorene), beryllium nitride BeN$_4$, van der Waals magnet CrSBr, rhenium dichalgogenides ReX$_2$. As a consequence, their optical and electronic properties turn out to be highly anisotropic as well. In some cases, the anisotropy results not just in a smooth renormalization of observable properties in comparison with the isotropic case but in the appearance of dramatically new physics. The examples are hyperbolic plasmons and excitons, strongly anisotropic ordering of adatoms at the surface of two-dimensional or van der Waals materials, essential change of transport and superconducting properties. Here, we present a systematic review of electronic structure, transport and optical properties of several representative groups of anisotropic two-dimensional materials including semiconductors, anisotropic Dirac and semi-Dirac materials, as well as superconductors.","sentences":["Among a huge variety of known two-dimensional materials, some of them have anisotropic crystal structures; examples include so different systems as a few-layer black phoshphorus (phosphorene), beryllium nitride BeN$_4$, van der Waals magnet CrSBr, rhenium dichalgogenides ReX$_2$.","As a consequence, their optical and electronic properties turn out to be highly anisotropic as well.","In some cases, the anisotropy results not just in a smooth renormalization of observable properties in comparison with the isotropic case but in the appearance of dramatically new physics.","The examples are hyperbolic plasmons and excitons, strongly anisotropic ordering of adatoms at the surface of two-dimensional or van der Waals materials, essential change of transport and superconducting properties.","Here, we present a systematic review of electronic structure, transport and optical properties of several representative groups of anisotropic two-dimensional materials including semiconductors, anisotropic Dirac and semi-Dirac materials, as well as superconductors."],"url":"http://arxiv.org/abs/2403.05374v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-08 15:03:18","title":"Atomistic investigation of deformation and fracture of individual structural components of metal matrix composites","abstract":"This paper focuses on the development of the atomistic framework for determining the lower scale mechanical parameters of single components of a metal matrix composite for final application to a micromechanical damage model. Here, the deformation and failure behavior of NiAl-Al$_2$O$_3$ interfaces and their components, metal and ceramic, are analyzed in depth using molecular statics calculations. A number of atomistic simulations of strength tests, uniaxial tensile, uniaxial compressive and simple shear, have been performed in order to obtain a set of stiffness tensors and strain-stress characteristics up to failure for 30 different crystalline and amorphous systems. Characteristic points on the strain-stress curves in the vicinity of failure are further analyzed at the atomistic level, using local measures of lattice disorder. Numerical results are discussed in the context of composite damage at upper microscopic scale based on images of the fracture surface of NiAl-Al$_2$O$_3$ composites.","sentences":["This paper focuses on the development of the atomistic framework for determining the lower scale mechanical parameters of single components of a metal matrix composite for final application to a micromechanical damage model.","Here, the deformation and failure behavior of NiAl-Al$_2$O$_3$ interfaces and their components, metal and ceramic, are analyzed in depth using molecular statics calculations.","A number of atomistic simulations of strength tests, uniaxial tensile, uniaxial compressive and simple shear, have been performed in order to obtain a set of stiffness tensors and strain-stress characteristics up to failure for 30 different crystalline and amorphous systems.","Characteristic points on the strain-stress curves in the vicinity of failure are further analyzed at the atomistic level, using local measures of lattice disorder.","Numerical results are discussed in the context of composite damage at upper microscopic scale based on images of the fracture surface of NiAl-Al$_2$O$_3$ composites."],"url":"http://arxiv.org/abs/2403.05371v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-08 14:58:34","title":"Stability-Certified On-Policy Data-Driven LQR via Recursive Learning and Policy Gradient","abstract":"In this paper, we investigate a data-driven framework to solve Linear Quadratic Regulator (LQR) problems when the dynamics is unknown, with the additional challenge of providing stability certificates for the overall learning and control scheme. Specifically, in the proposed on-policy learning framework, the control input is applied to the actual (unknown) linear system while iteratively optimized. We propose a learning and control procedure, termed RELEARN LQR, that combines a recursive least squares method with a direct policy search based on the gradient method. The resulting scheme is analyzed by modeling it as a feedback-interconnected nonlinear dynamical system. A Lyapunov-based approach, exploiting averaging and singular perturbations theory for nonlinear systems, allows us to provide formal stability guarantees for the whole interconnected scheme. The effectiveness of the proposed strategy is corroborated by numerical simulations, where RELEARN LQR is deployed on an aircraft control problem, with both static and drifting parameters.","sentences":["In this paper, we investigate a data-driven framework to solve Linear Quadratic Regulator (LQR) problems when the dynamics is unknown, with the additional challenge of providing stability certificates for the overall learning and control scheme.","Specifically, in the proposed on-policy learning framework, the control input is applied to the actual (unknown) linear system while iteratively optimized.","We propose a learning and control procedure, termed RELEARN LQR, that combines a recursive least squares method with a direct policy search based on the gradient method.","The resulting scheme is analyzed by modeling it as a feedback-interconnected nonlinear dynamical system.","A Lyapunov-based approach, exploiting averaging and singular perturbations theory for nonlinear systems, allows us to provide formal stability guarantees for the whole interconnected scheme.","The effectiveness of the proposed strategy is corroborated by numerical simulations, where RELEARN LQR is deployed on an aircraft control problem, with both static and drifting parameters."],"url":"http://arxiv.org/abs/2403.05367v1","category":"eess.SY"}
{"created":"2024-03-08 14:53:36","title":"A conditional lower bound for the Tur\u00e1n number of spheres","abstract":"We consider the hypergraph Tur\\'an problem of determining $\\mathrm{ex}(n, S^d)$, the maximum number of facets in a $d$-dimensional simplicial complex on $n$ vertices that does not contain a simplicial $d$-sphere (a homeomorph of $S^d$) as a subcomplex. We show that if there is an affirmative answer to a question of Gromov about sphere enumeration in high dimensions, then $\\mathrm{ex}(n, S^d) \\geq \\Omega(n^{d + 1 - (d + 1)/(2^{d + 1} - 2)})$. Furthermore, this lower bound holds unconditionally for 2-LC spheres, which includes all shellable spheres and therefore all polytopes. We also prove an upper bound on $\\mathrm{ex}(n, S^d)$ of $O(n^{d + 1 - 1/2^{d - 1}})$ using a simple induction argument. We conjecture that the upper bound can be improved to match the conditional lower bound.","sentences":["We consider the hypergraph Tur\\'an problem of determining $\\mathrm{ex}(n, S^d)$, the maximum number of facets in a $d$-dimensional simplicial complex on $n$ vertices that does not contain a simplicial $d$-sphere (a homeomorph of $S^d$) as a subcomplex.","We show that if there is an affirmative answer to a question of Gromov about sphere enumeration in high dimensions, then $\\mathrm{ex}(n, S^d) \\geq \\Omega(n^{d + 1 - (d + 1)/(2^{d + 1} - 2)})$.","Furthermore, this lower bound holds unconditionally for 2-LC spheres, which includes all shellable spheres and therefore all polytopes.","We also prove an upper bound on $\\mathrm{ex}(n, S^d)$ of $O(n^{d + 1 - 1/2^{d - 1}})$ using a simple induction argument.","We conjecture that the upper bound can be improved to match the conditional lower bound."],"url":"http://arxiv.org/abs/2403.05364v1","category":"math.CO"}
{"created":"2024-03-08 14:49:43","title":"Physical properties of extreme emission-line galaxies at $z\\sim 4-9$ from the JWST CEERS survey","abstract":"Extreme emission line galaxies (EELGs) are typically characterized by high equivalent widths (EWs) which are driven by elevated specific star formation rates (sSFR) in low-mass galaxies with subsolar metallicities and little dust. Such extreme systems are rare in the local universe, but the number density of EELGs increases with redshift. Such starburst galaxies are currently presumed to be the main drivers of hydrogen reionization over 5.5<z<15, which serves to motivate many of the searches for high-z EELGs. We aim to characterize the physical properties of a sample of ~730 EELGs at 4<z<9 photometrically selected from the CEERS survey using JWST/NIRCam. We validate our method and demonstrate the main physical properties of a subset of EELGs using NIRSpec spectra. We create synthetic NIRCam observations of EELGs using empirical templates based on ~2000 local metal-poor starbursts to select EELGs based on color-color criteria. We study their properties based on SED fitting and flux excess from emission lines in the photometric filters. Our sample has a mean stellar mass of $10^{7.84}$Msun with high sSFRs with a mean value of $10^{-7.03}$ yr$^{-1}$. We consider a delayed-$\\tau$ model for the star formation history and find our sample of EELGs are young with a mean value of the time after the onset of star formation of 45Myr. We find that they have similar line ratios to local metal-poor starbursts with high log([OIII]/H$\\beta$)>0.4-1 which indicates that star formation may be the dominant source of ionization. Based on the photometric fluxes, we find an increase of EW([OIII]+H$\\beta$) with sSFR and $\\Sigma_{SFR}$, and a decrease with age and stellar mass. The sample of EELGs can reach $\\Sigma_{SFR}>$10Msun yr$^{-1}$kpc$^{-2}$ which indicate they are strong candidates of LyC leakers. Another indirect indicator is the high values of O32>5 that can be reached for some galaxies in the sample.","sentences":["Extreme emission line galaxies (EELGs) are typically characterized by high equivalent widths (EWs) which are driven by elevated specific star formation rates (sSFR) in low-mass galaxies with subsolar metallicities and little dust.","Such extreme systems are rare in the local universe, but the number density of EELGs increases with redshift.","Such starburst galaxies are currently presumed to be the main drivers of hydrogen reionization over 5.5<z<15, which serves to motivate many of the searches for high-z EELGs.","We aim to characterize the physical properties of a sample of ~730 EELGs at 4<z<9 photometrically selected from the CEERS survey using JWST/NIRCam.","We validate our method and demonstrate the main physical properties of a subset of EELGs using NIRSpec spectra.","We create synthetic NIRCam observations of EELGs using empirical templates based on ~2000 local metal-poor starbursts to select EELGs based on color-color criteria.","We study their properties based on SED fitting and flux excess from emission lines in the photometric filters.","Our sample has a mean stellar mass of $10^{7.84}$Msun with high sSFRs with a mean value of $10^{-7.03}$ yr$^{-1}$. We consider a delayed-$\\tau$ model for the star formation history and find our sample of EELGs are young with a mean value of the time after the onset of star formation of 45Myr.","We find that they have similar line ratios to local metal-poor starbursts with high log([OIII]/H$\\beta$)>0.4-1 which indicates that star formation may be the dominant source of ionization.","Based on the photometric fluxes, we find an increase of EW([OIII]+H$\\beta$) with sSFR and $\\Sigma_{SFR}$, and a decrease with age and stellar mass.","The sample of EELGs can reach $\\Sigma_{SFR}>$10Msun yr$^{-1}$kpc$^{-2}$ which indicate they are strong candidates of LyC leakers.","Another indirect indicator is the high values of O32>5 that can be reached for some galaxies in the sample."],"url":"http://arxiv.org/abs/2403.05362v1","category":"astro-ph.GA"}
{"created":"2024-03-08 14:34:32","title":"Hybridized Convolutional Neural Networks and Long Short-Term Memory for Improved Alzheimer's Disease Diagnosis from MRI Scans","abstract":"Brain-related diseases are more sensitive than other diseases due to several factors, including the complexity of surgical procedures, high costs, and other challenges. Alzheimer's disease is a common brain disorder that causes memory loss and the shrinking of brain cells. Early detection is critical for providing proper treatment to patients. However, identifying Alzheimer's at an early stage using manual scanning of CT or MRI scans is challenging. Therefore, researchers have delved into the exploration of computer-aided systems, employing Machine Learning and Deep Learning methodologies, which entail the training of datasets to detect Alzheimer's disease. This study aims to present a hybrid model that combines a CNN model's feature extraction capabilities with an LSTM model's detection capabilities. This study has applied the transfer learning called VGG16 in the hybrid model to extract features from MRI images. The LSTM detects features between the convolution layer and the fully connected layer. The output layer of the fully connected layer uses the softmax function. The training of the hybrid model involved utilizing the ADNI dataset. The trial findings revealed that the model achieved a level of accuracy of 98.8%, a sensitivity rate of 100%, and a specificity rate of 76%. The proposed hybrid model outperforms its contemporary CNN counterparts, showcasing a superior performance.","sentences":["Brain-related diseases are more sensitive than other diseases due to several factors, including the complexity of surgical procedures, high costs, and other challenges.","Alzheimer's disease is a common brain disorder that causes memory loss and the shrinking of brain cells.","Early detection is critical for providing proper treatment to patients.","However, identifying Alzheimer's at an early stage using manual scanning of CT or MRI scans is challenging.","Therefore, researchers have delved into the exploration of computer-aided systems, employing Machine Learning and Deep Learning methodologies, which entail the training of datasets to detect Alzheimer's disease.","This study aims to present a hybrid model that combines a CNN model's feature extraction capabilities with an LSTM model's detection capabilities.","This study has applied the transfer learning called VGG16 in the hybrid model to extract features from MRI images.","The LSTM detects features between the convolution layer and the fully connected layer.","The output layer of the fully connected layer uses the softmax function.","The training of the hybrid model involved utilizing the ADNI dataset.","The trial findings revealed that the model achieved a level of accuracy of 98.8%, a sensitivity rate of 100%, and a specificity rate of 76%.","The proposed hybrid model outperforms its contemporary CNN counterparts, showcasing a superior performance."],"url":"http://arxiv.org/abs/2403.05353v1","category":"eess.IV"}
{"created":"2024-03-08 14:31:20","title":"Formal Verification of Unknown Stochastic Systems via Non-parametric Estimation","abstract":"A novel data-driven method for formal verification is proposed to study complex systems operating in safety-critical domains. The proposed approach is able to formally verify discrete-time stochastic dynamical systems against temporal logic specifications only using observation samples and without the knowledge of the model, and provide a probabilistic guarantee on the satisfaction of the specification. We first propose the theoretical results for using non-parametric estimation to estimate an asymptotic upper bound for the \\emph{Lipschitz constant} of the stochastic system, which can determine a finite abstraction of the system. Our results prove that the asymptotic convergence rate of the estimation is $O(n^{-\\frac{1}{3+d}})$, where $d$ is the dimension of the system and $n$ is the data scale. We then construct interval Markov decision processes using two different data-driven methods, namely non-parametric estimation and empirical estimation of transition probabilities, to perform formal verification against a given temporal logic specification. Multiple case studies are presented to validate the effectiveness of the proposed methods.","sentences":["A novel data-driven method for formal verification is proposed to study complex systems operating in safety-critical domains.","The proposed approach is able to formally verify discrete-time stochastic dynamical systems against temporal logic specifications only using observation samples and without the knowledge of the model, and provide a probabilistic guarantee on the satisfaction of the specification.","We first propose the theoretical results for using non-parametric estimation to estimate an asymptotic upper bound for the \\emph{Lipschitz constant} of the stochastic system, which can determine a finite abstraction of the system.","Our results prove that the asymptotic convergence rate of the estimation is $O(n^{-\\frac{1}{3+d}})$, where $d$ is the dimension of the system and $n$ is the data scale.","We then construct interval Markov decision processes using two different data-driven methods, namely non-parametric estimation and empirical estimation of transition probabilities, to perform formal verification against a given temporal logic specification.","Multiple case studies are presented to validate the effectiveness of the proposed methods."],"url":"http://arxiv.org/abs/2403.05350v1","category":"eess.SY"}
{"created":"2024-03-08 14:28:25","title":"Higher Contiguity Distance","abstract":"In this paper, we introduce the higher analogues of contiguity distance and its relations with simplicial Lusternik-Schnirelmann category and discrete topological complexity. Also we study the effects of geometric realisation and barycentric subdivision in the sense that how the geometric realisation of the simplicial maps and the induced simplicial maps on barycentric subdivisions affects higher contiguity distance.","sentences":["In this paper, we introduce the higher analogues of contiguity distance and its relations with simplicial Lusternik-Schnirelmann category and discrete topological complexity.","Also we study the effects of geometric realisation and barycentric subdivision in the sense that how the geometric realisation of the simplicial maps and the induced simplicial maps on barycentric subdivisions affects higher contiguity distance."],"url":"http://arxiv.org/abs/2403.05348v1","category":"math.AT"}
{"created":"2024-03-08 14:21:02","title":"Disentangling the Timescales of a Complex System: A Bayesian Approach to Temporal Network Analysis","abstract":"Changes in the timescales at which complex systems evolve are essential to predicting critical transitions and catastrophic failures. Disentangling the timescales of the dynamics governing complex systems remains a key challenge. With this study, we introduce an integrated Bayesian framework based on temporal network models to address this challenge. We focus on two methodologies: change point detection for identifying shifts in system dynamics, and a spectrum analysis for inferring the distribution of timescales. Applied to synthetic and empirical datasets, these methologies robustly identify critical transitions and comprehensively map the dominant and subsidiaries timescales in complex systems. This dual approach offers a powerful tool for analyzing temporal networks, significantly enhancing our understanding of dynamic behaviors in complex systems.","sentences":["Changes in the timescales at which complex systems evolve are essential to predicting critical transitions and catastrophic failures.","Disentangling the timescales of the dynamics governing complex systems remains a key challenge.","With this study, we introduce an integrated Bayesian framework based on temporal network models to address this challenge.","We focus on two methodologies: change point detection for identifying shifts in system dynamics, and a spectrum analysis for inferring the distribution of timescales.","Applied to synthetic and empirical datasets, these methologies robustly identify critical transitions and comprehensively map the dominant and subsidiaries timescales in complex systems.","This dual approach offers a powerful tool for analyzing temporal networks, significantly enhancing our understanding of dynamic behaviors in complex systems."],"url":"http://arxiv.org/abs/2403.05343v1","category":"stat.ME"}
{"created":"2024-03-08 14:10:12","title":"Entropic van der Corput's Difference Theorem","abstract":"We prove an entropy version of van der Corput's difference theorem: the entropy of a sequence is equal to the entropy of its differences. This reveals a potential correspondence between the theory of uniform distribution mod 1 and entropy. As applications, we establish the corresponding entropy versions for several other results on uniform distribution.","sentences":["We prove an entropy version of van der Corput's difference theorem: the entropy of a sequence is equal to the entropy of its differences.","This reveals a potential correspondence between the theory of uniform distribution mod 1 and entropy.","As applications, we establish the corresponding entropy versions for several other results on uniform distribution."],"url":"http://arxiv.org/abs/2403.05333v1","category":"math.DS"}
{"created":"2024-03-08 14:09:32","title":"Degradation Resilient LiDAR-Radar-Inertial Odometry","abstract":"Enabling autonomous robots to operate robustly in challenging environments is necessary in a future with increased autonomy. For many autonomous systems, estimation and odometry remains a single point of failure, from which it can often be difficult, if not impossible, to recover. As such robust odometry solutions are of key importance. In this work a method for tightly-coupled LiDAR-Radar-Inertial fusion for odometry is proposed, enabling the mitigation of the effects of LiDAR degeneracy by leveraging a complementary perception modality while preserving the accuracy of LiDAR in well-conditioned environments. The proposed approach combines modalities in a factor graph-based windowed smoother with sensor information-specific factor formulations which enable, in the case of degeneracy, partial information to be conveyed to the graph along the non-degenerate axes. The proposed method is evaluated in real-world tests on a flying robot experiencing degraded conditions including geometric self-similarity as well as obscurant occlusion. For the benefit of the community we release the datasets presented: https://github.com/ntnu-arl/lidar_degeneracy_datasets.","sentences":["Enabling autonomous robots to operate robustly in challenging environments is necessary in a future with increased autonomy.","For many autonomous systems, estimation and odometry remains a single point of failure, from which it can often be difficult, if not impossible, to recover.","As such robust odometry solutions are of key importance.","In this work a method for tightly-coupled LiDAR-Radar-Inertial fusion for odometry is proposed, enabling the mitigation of the effects of LiDAR degeneracy by leveraging a complementary perception modality while preserving the accuracy of LiDAR in well-conditioned environments.","The proposed approach combines modalities in a factor graph-based windowed smoother with sensor information-specific factor formulations which enable, in the case of degeneracy, partial information to be conveyed to the graph along the non-degenerate axes.","The proposed method is evaluated in real-world tests on a flying robot experiencing degraded conditions including geometric self-similarity as well as obscurant occlusion.","For the benefit of the community we release the datasets presented: https://github.com/ntnu-arl/lidar_degeneracy_datasets."],"url":"http://arxiv.org/abs/2403.05332v1","category":"cs.RO"}
{"created":"2024-03-08 14:07:37","title":"OccFusion: Depth Estimation Free Multi-sensor Fusion for 3D Occupancy Prediction","abstract":"3D occupancy prediction based on multi-sensor fusion, crucial for a reliable autonomous driving system, enables fine-grained understanding of 3D scenes. Previous fusion-based 3D occupancy predictions relied on depth estimation for processing 2D image features. However, depth estimation is an ill-posed problem, hindering the accuracy and robustness of these methods. Furthermore, fine-grained occupancy prediction demands extensive computational resources. We introduce OccFusion, a multi-modal fusion method free from depth estimation, and a corresponding point cloud sampling algorithm for dense integration of image features. Building on this, we propose an active training method and an active coarse to fine pipeline, enabling the model to adaptively learn more from complex samples and optimize predictions specifically for challenging areas such as small or overlapping objects. The active methods we propose can be naturally extended to any occupancy prediction model. Experiments on the OpenOccupancy benchmark show our method surpasses existing state-of-the-art (SOTA) multi-modal methods in IoU across all categories. Additionally, our model is more efficient during both the training and inference phases, requiring far fewer computational resources. Comprehensive ablation studies demonstrate the effectiveness of our proposed techniques.","sentences":["3D occupancy prediction based on multi-sensor fusion, crucial for a reliable autonomous driving system, enables fine-grained understanding of 3D scenes.","Previous fusion-based 3D occupancy predictions relied on depth estimation for processing 2D image features.","However, depth estimation is an ill-posed problem, hindering the accuracy and robustness of these methods.","Furthermore, fine-grained occupancy prediction demands extensive computational resources.","We introduce OccFusion, a multi-modal fusion method free from depth estimation, and a corresponding point cloud sampling algorithm for dense integration of image features.","Building on this, we propose an active training method and an active coarse to fine pipeline, enabling the model to adaptively learn more from complex samples and optimize predictions specifically for challenging areas such as small or overlapping objects.","The active methods we propose can be naturally extended to any occupancy prediction model.","Experiments on the OpenOccupancy benchmark show our method surpasses existing state-of-the-art (SOTA) multi-modal methods in IoU across all categories.","Additionally, our model is more efficient during both the training and inference phases, requiring far fewer computational resources.","Comprehensive ablation studies demonstrate the effectiveness of our proposed techniques."],"url":"http://arxiv.org/abs/2403.05329v1","category":"cs.CV"}
{"created":"2024-03-08 14:06:15","title":"DiffSF: Diffusion Models for Scene Flow Estimation","abstract":"Scene flow estimation is an essential ingredient for a variety of real-world applications, especially for autonomous agents, such as self-driving cars and robots. While recent scene flow estimation approaches achieve a reasonable accuracy, their applicability to real-world systems additionally benefits from a reliability measure. Aiming at improving accuracy while additionally providing an estimate for uncertainty, we propose DiffSF that combines transformer-based scene flow estimation with denoising diffusion models. In the diffusion process, the ground truth scene flow vector field is gradually perturbed by adding Gaussian noise. In the reverse process, starting from randomly sampled Gaussian noise, the scene flow vector field prediction is recovered by conditioning on a source and a target point cloud. We show that the diffusion process greatly increases the robustness of predictions compared to prior approaches resulting in state-of-the-art performance on standard scene flow estimation benchmarks. Moreover, by sampling multiple times with different initial states, the denoising process predicts multiple hypotheses, which enables measuring the output uncertainty, allowing our approach to detect a majority of the inaccurate predictions.","sentences":["Scene flow estimation is an essential ingredient for a variety of real-world applications, especially for autonomous agents, such as self-driving cars and robots.","While recent scene flow estimation approaches achieve a reasonable accuracy, their applicability to real-world systems additionally benefits from a reliability measure.","Aiming at improving accuracy while additionally providing an estimate for uncertainty, we propose DiffSF that combines transformer-based scene flow estimation with denoising diffusion models.","In the diffusion process, the ground truth scene flow vector field is gradually perturbed by adding Gaussian noise.","In the reverse process, starting from randomly sampled Gaussian noise, the scene flow vector field prediction is recovered by conditioning on a source and a target point cloud.","We show that the diffusion process greatly increases the robustness of predictions compared to prior approaches resulting in state-of-the-art performance on standard scene flow estimation benchmarks.","Moreover, by sampling multiple times with different initial states, the denoising process predicts multiple hypotheses, which enables measuring the output uncertainty, allowing our approach to detect a majority of the inaccurate predictions."],"url":"http://arxiv.org/abs/2403.05327v1","category":"cs.CV"}
{"created":"2024-03-08 13:46:46","title":"System Identification using Energy-Bounded Noise Models: A Full Characterization of Chebyshev Centers and Radii","abstract":"This paper studies the identification of linear time-invariant systems using noisy input-output data in a set-membership framework. It is assumed that the noise sequence belongs to a bounded set, induced by a quadratic matrix inequality, that allows for capturing various energy-bounded noise models. In this framework, the identification problem is to find an estimation for the true system so that the worst-case estimation error with respect to a given norm is minimized. For such a problem, we present closed-form solutions with respect to arbitrary unitarily invariant matrix norms. Examples of these norms include the Frobenius norm, spectral norm, Schatten p-norms, and Ky Fan k-norms.","sentences":["This paper studies the identification of linear time-invariant systems using noisy input-output data in a set-membership framework.","It is assumed that the noise sequence belongs to a bounded set, induced by a quadratic matrix inequality, that allows for capturing various energy-bounded noise models.","In this framework, the identification problem is to find an estimation for the true system so that the worst-case estimation error with respect to a given norm is minimized.","For such a problem, we present closed-form solutions with respect to arbitrary unitarily invariant matrix norms.","Examples of these norms include the Frobenius norm, spectral norm, Schatten p-norms, and Ky Fan k-norms."],"url":"http://arxiv.org/abs/2403.05315v1","category":"math.OC"}
{"created":"2024-03-08 13:45:32","title":"Advances of Deep Learning in Protein Science: A Comprehensive Survey","abstract":"Protein representation learning plays a crucial role in understanding the structure and function of proteins, which are essential biomolecules involved in various biological processes. In recent years, deep learning has emerged as a powerful tool for protein modeling due to its ability to learn complex patterns and representations from large-scale protein data. This comprehensive survey aims to provide an overview of the recent advances in deep learning techniques applied to protein science. The survey begins by introducing the developments of deep learning based protein models and emphasizes the importance of protein representation learning in drug discovery, protein engineering, and function annotation. It then delves into the fundamentals of deep learning, including convolutional neural networks, recurrent neural networks, attention models, and graph neural networks in modeling protein sequences, structures, and functions, and explores how these techniques can be used to extract meaningful features and capture intricate relationships within protein data. Next, the survey presents various applications of deep learning in the field of proteins, including protein structure prediction, protein-protein interaction prediction, protein function prediction, etc. Furthermore, it highlights the challenges and limitations of these deep learning techniques and also discusses potential solutions and future directions for overcoming these challenges. This comprehensive survey provides a valuable resource for researchers and practitioners in the field of proteins who are interested in harnessing the power of deep learning techniques. By consolidating the latest advancements and discussing potential avenues for improvement, this review contributes to the ongoing progress in protein research and paves the way for future breakthroughs in the field.","sentences":["Protein representation learning plays a crucial role in understanding the structure and function of proteins, which are essential biomolecules involved in various biological processes.","In recent years, deep learning has emerged as a powerful tool for protein modeling due to its ability to learn complex patterns and representations from large-scale protein data.","This comprehensive survey aims to provide an overview of the recent advances in deep learning techniques applied to protein science.","The survey begins by introducing the developments of deep learning based protein models and emphasizes the importance of protein representation learning in drug discovery, protein engineering, and function annotation.","It then delves into the fundamentals of deep learning, including convolutional neural networks, recurrent neural networks, attention models, and graph neural networks in modeling protein sequences, structures, and functions, and explores how these techniques can be used to extract meaningful features and capture intricate relationships within protein data.","Next, the survey presents various applications of deep learning in the field of proteins, including protein structure prediction, protein-protein interaction prediction, protein function prediction, etc.","Furthermore, it highlights the challenges and limitations of these deep learning techniques and also discusses potential solutions and future directions for overcoming these challenges.","This comprehensive survey provides a valuable resource for researchers and practitioners in the field of proteins who are interested in harnessing the power of deep learning techniques.","By consolidating the latest advancements and discussing potential avenues for improvement, this review contributes to the ongoing progress in protein research and paves the way for future breakthroughs in the field."],"url":"http://arxiv.org/abs/2403.05314v1","category":"q-bio.BM"}
{"created":"2024-03-08 13:38:17","title":"An in-Contact Robotic System for the Process of Desoldering PCB Components","abstract":"The disposal and recycling of electronic waste (e-waste) is a global challenge. The disassembly of components is a crucial step towards an efficient recycling process, avoiding the destructive methods. Although most disassembly work is still done manually due to the diversity and complexity of components, there is a growing interest in developing automated methods to improve efficiency and reduce labor costs. This study aims to robotize the desoldering process and extracting components from printed circuit boards (PCBs), with the goal of automating the process as much as possible. The proposed strategy consists of several phases, including the controlled contact of the robotic tool with the PCB components. A specific tool was developed to apply a controlled force against the PCB component, removing it from the board. The results demonstrate that it is feasible to remove the PCB components with a high success rate (approximately 100% for the bigger PCB components).","sentences":["The disposal and recycling of electronic waste (e-waste) is a global challenge.","The disassembly of components is a crucial step towards an efficient recycling process, avoiding the destructive methods.","Although most disassembly work is still done manually due to the diversity and complexity of components, there is a growing interest in developing automated methods to improve efficiency and reduce labor costs.","This study aims to robotize the desoldering process and extracting components from printed circuit boards (PCBs), with the goal of automating the process as much as possible.","The proposed strategy consists of several phases, including the controlled contact of the robotic tool with the PCB components.","A specific tool was developed to apply a controlled force against the PCB component, removing it from the board.","The results demonstrate that it is feasible to remove the PCB components with a high success rate (approximately 100% for the bigger PCB components)."],"url":"http://arxiv.org/abs/2403.05309v1","category":"cs.RO"}
{"created":"2024-03-08 13:38:07","title":"Sparse Wearable Sonomyography Sensor-based Proprioceptive Proportional Control Across Multiple Gestures","abstract":"Sonomyography (SMG) is a non-invasive technique that uses ultrasound imaging to detect the dynamic activity of muscles. Wearable SMG systems have recently gained popularity due to their potential as human-computer interfaces for their superior performance compared to conventional methods. This paper demonstrates real-time positional proportional control of multiple gestures using a multiplexed 8-channel wearable SMG system. The amplitude-mode ultrasound signals from the SMG system were utilized to detect muscle activity from the forearm of 8 healthy individuals. The derived signals were used to control the on-screen movement of the cursor. A target achievement task was performed to analyze the performance of our SMG-based human-machine interface. Our wearable SMG system provided accurate, stable, and intuitive control in real-time by achieving an average success rate greater than 80% with all gestures. Furthermore, the wearable SMG system's abilities to detect volitional movement and decode movement kinematic information from SMG trajectories using standard performance metrics were evaluated. Our results provide insights to validate SMG as an intuitive human-machine interface.","sentences":["Sonomyography (SMG) is a non-invasive technique that uses ultrasound imaging to detect the dynamic activity of muscles.","Wearable SMG systems have recently gained popularity due to their potential as human-computer interfaces for their superior performance compared to conventional methods.","This paper demonstrates real-time positional proportional control of multiple gestures using a multiplexed 8-channel wearable SMG system.","The amplitude-mode ultrasound signals from the SMG system were utilized to detect muscle activity from the forearm of 8 healthy individuals.","The derived signals were used to control the on-screen movement of the cursor.","A target achievement task was performed to analyze the performance of our SMG-based human-machine interface.","Our wearable SMG system provided accurate, stable, and intuitive control in real-time by achieving an average success rate greater than 80% with all gestures.","Furthermore, the wearable SMG system's abilities to detect volitional movement and decode movement kinematic information from SMG trajectories using standard performance metrics were evaluated.","Our results provide insights to validate SMG as an intuitive human-machine interface."],"url":"http://arxiv.org/abs/2403.05308v1","category":"cs.HC"}
{"created":"2024-03-08 13:33:57","title":"On discrete Routh reduction and structures on the reduced space","abstract":"In this paper we work, first, with forced discrete mechanical systems on the configuration space $Q$ and construct closed $2$-forms $\\omega^+$ and $\\omega^-$ on $Q \\times Q$, that are symplectic if and only if the system is regular. For a special type of discrete force, we prove that $\\omega^+$ and $\\omega^-$ are invariant by the flow of the system. We also consider the Lagrangian reduction of a discrete mechanical system by a symmetry group (using an affine discrete connection derived from the discrete momentum) and prove that, under some conditions on the action, the trajectories of the reduced system (with discrete momentum $\\mu$) can be seen as trajectories of a forced discrete mechanical system, where the discrete force is of the type analyzed before. Therefore, we prove that there is a symplectic structure that is invariant by the flow of the forced reduced system; the symplectic structure can be seen as a pullback of a canonical cotangent structure plus a magnetic term. This discrete reduction process is the (discrete) Routh reduction and the behavior obtained runs parallel to the well known case for (continuous) Routh reduction.","sentences":["In this paper we work, first, with forced discrete mechanical systems on the configuration space $Q$ and construct closed $2$-forms $\\omega^+$ and $\\omega^-$ on $Q \\times Q$, that are symplectic if and only if the system is regular.","For a special type of discrete force, we prove that $\\omega^+$ and $\\omega^-$ are invariant by the flow of the system.","We also consider the Lagrangian reduction of a discrete mechanical system by a symmetry group (using an affine discrete connection derived from the discrete momentum) and prove that, under some conditions on the action, the trajectories of the reduced system (with discrete momentum $\\mu$) can be seen as trajectories of a forced discrete mechanical system, where the discrete force is of the type analyzed before.","Therefore, we prove that there is a symplectic structure that is invariant by the flow of the forced reduced system; the symplectic structure can be seen as a pullback of a canonical cotangent structure plus a magnetic term.","This discrete reduction process is the (discrete) Routh reduction and the behavior obtained runs parallel to the well known case for (continuous) Routh reduction."],"url":"http://arxiv.org/abs/2403.05305v1","category":"math.DG"}
{"created":"2024-03-08 13:26:06","title":"Disorder-induced instability of a Weyl nodal loop semimetal towards a diffusive topological metal with protected multifractal surface states","abstract":"Weyl nodal loop semimetals are gapless topological phases that, unlike their insulator counterparts, may be unstable to small perturbations that respect their topology-protecting symmetries. Here, we analyze a clean system perturbed by chiral off-diagonal disorder using numerically exact methods. We establish that the ballistic semimetallic phase is unstable towards the formation of an unconventional topological diffusive metal hosting topological multifractal surface states. Although, as in the clean case, surface states are exponentially localized along the direction perpendicular to the nodal loop, disorder induces a multifractal structure in the remaining directions. Surprisingly, the number of these states also increases with a small amount of disorder. Eventually, as disorder is further increased, the number of surface states starts decreasing. In the strong disordered regime we predict that some types of disorder induce an Anderson transition into an electrically-polarized insulator whose signature may be detected experimentally.","sentences":["Weyl nodal loop semimetals are gapless topological phases that, unlike their insulator counterparts, may be unstable to small perturbations that respect their topology-protecting symmetries.","Here, we analyze a clean system perturbed by chiral off-diagonal disorder using numerically exact methods.","We establish that the ballistic semimetallic phase is unstable towards the formation of an unconventional topological diffusive metal hosting topological multifractal surface states.","Although, as in the clean case, surface states are exponentially localized along the direction perpendicular to the nodal loop, disorder induces a multifractal structure in the remaining directions.","Surprisingly, the number of these states also increases with a small amount of disorder.","Eventually, as disorder is further increased, the number of surface states starts decreasing.","In the strong disordered regime we predict that some types of disorder induce an Anderson transition into an electrically-polarized insulator whose signature may be detected experimentally."],"url":"http://arxiv.org/abs/2403.05298v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-08 13:10:59","title":"LLM4Decompile: Decompiling Binary Code with Large Language Models","abstract":"Decompilation aims to restore compiled code to human-readable source code, but struggles with details like names and structure. Large language models (LLMs) show promise for programming tasks, motivating their application to decompilation. However, there does not exist any open-source LLM for decompilation. Moreover, existing decompilation evaluation systems mainly consider token-level accuracy and largely ignore code executability, which is the most important feature of any program. Therefore, we release the first open-access decompilation LLMs ranging from 1B to 33B pre-trained on 4 billion tokens of C source code and the corresponding assembly code. The open-source LLMs can serve as baselines for further development in the field. To ensure practical program evaluation, we introduce Decompile-Eval, the first dataset that considers re-compilability and re-executability for decompilation. The benchmark emphasizes the importance of evaluating the decompilation model from the perspective of program semantics. Experiments indicate that our LLM4Decompile has demonstrated the capability to accurately decompile 21% of the assembly code, which achieves a 50% improvement over GPT-4. Our code, dataset, and models are released at https://github.com/albertan017/LLM4Decompile","sentences":["Decompilation aims to restore compiled code to human-readable source code, but struggles with details like names and structure.","Large language models (LLMs) show promise for programming tasks, motivating their application to decompilation.","However, there does not exist any open-source LLM for decompilation.","Moreover, existing decompilation evaluation systems mainly consider token-level accuracy and largely ignore code executability, which is the most important feature of any program.","Therefore, we release the first open-access decompilation LLMs ranging from 1B to 33B pre-trained on 4 billion tokens of C source code and the corresponding assembly code.","The open-source LLMs can serve as baselines for further development in the field.","To ensure practical program evaluation, we introduce Decompile-Eval, the first dataset that considers re-compilability and re-executability for decompilation.","The benchmark emphasizes the importance of evaluating the decompilation model from the perspective of program semantics.","Experiments indicate that our LLM4Decompile has demonstrated the capability to accurately decompile 21% of the assembly code, which achieves a 50% improvement over GPT-4.","Our code, dataset, and models are released at https://github.com/albertan017/LLM4Decompile"],"url":"http://arxiv.org/abs/2403.05286v1","category":"cs.PL"}
{"created":"2024-03-08 13:07:33","title":"Provably Time-Optimal Cooling of Markovian Quantum Systems","abstract":"We address the problem of cooling a Markovian quantum system to a pure state in the shortest amount of time possible. Here the system drift takes the form of a Lindblad master equation and we assume fast unitary control. This setting allows for a natural reduction of the control system to the eigenvalues of the state density matrix. We give a simple necessary and sufficient characterization of systems which are (asymptotically) coolable and present a powerful result which allows to considerably simplify the search for optimal cooling solutions. With these tools at our disposal we derive explicit provably time-optimal cooling protocols for rank one qubit systems, inverted $\\Lambda$-systems on a qutrit, and a certain system consisting of two coupled qubits.","sentences":["We address the problem of cooling a Markovian quantum system to a pure state in the shortest amount of time possible.","Here the system drift takes the form of a Lindblad master equation and we assume fast unitary control.","This setting allows for a natural reduction of the control system to the eigenvalues of the state density matrix.","We give a simple necessary and sufficient characterization of systems which are (asymptotically) coolable and present a powerful result which allows to considerably simplify the search for optimal cooling solutions.","With these tools at our disposal we derive explicit provably time-optimal cooling protocols for rank one qubit systems, inverted $\\Lambda$-systems on a qutrit, and a certain system consisting of two coupled qubits."],"url":"http://arxiv.org/abs/2403.05285v1","category":"quant-ph"}
{"created":"2024-03-08 12:59:47","title":"Approaching the double-Heisenberg-scaling sensitivity in the Tavis-Cummings model","abstract":"The pursuit of quantum-enhanced parameter estimations without the need for nonclassical initial states has long been driven by the goal of achieving experimentally accessible quantum metrology. In this paper, employing a coherent averaging mechanism, we prove that the prototypical cavity-quantum electrodynamics (QED) system, such as the Tavis-Cummings (TC) model, enables us to achieve not only the Heisenberg scaling (HS) precision in terms of the average photon number but also the double-HS sensitivity concerning both the average photon and atom numbers. Such a double sensibility can be experimentally realized by introducing either photon- or atom-number fluctuations through quantum squeezing. Furthermore, we discuss the methodology to achieve this double-HS precision in a realistic experimental circumstance where the squeezing is not perfect. Our results provide insights into understanding the coherent averaging mechanism for evaluating quantum-enhanced precision measurements and also present a usable metrological application of the cavity QED systems.","sentences":["The pursuit of quantum-enhanced parameter estimations without the need for nonclassical initial states has long been driven by the goal of achieving experimentally accessible quantum metrology.","In this paper, employing a coherent averaging mechanism, we prove that the prototypical cavity-quantum electrodynamics (QED) system, such as the Tavis-Cummings (TC) model, enables us to achieve not only the Heisenberg scaling (HS) precision in terms of the average photon number but also the double-HS sensitivity concerning both the average photon and atom numbers.","Such a double sensibility can be experimentally realized by introducing either photon- or atom-number fluctuations through quantum squeezing.","Furthermore, we discuss the methodology to achieve this double-HS precision in a realistic experimental circumstance where the squeezing is not perfect.","Our results provide insights into understanding the coherent averaging mechanism for evaluating quantum-enhanced precision measurements and also present a usable metrological application of the cavity QED systems."],"url":"http://arxiv.org/abs/2403.05279v1","category":"quant-ph"}
{"created":"2024-03-08 12:56:10","title":"vSPACE: Voting in a Scalable, Privacy-Aware and Confidential Election","abstract":"The vSPACE experimental proof-of-concept (PoC) on the TrueElect[Anon][Creds] protocol presents a novel approach to secure, private, and scalable elections, extending the TrueElect and ElectAnon protocols with the integration of AnonCreds SSI (Self-Sovereign Identity). Such a protocol PoC is situated within a Zero-Trust Architecture (ZTA) and leverages confidential computing, continuous authentication, multi-party computation (MPC), and well-architected framework (WAF) principles to address the challenges of cybersecurity, privacy, and trust over IP (ToIP) protection. Employing a Kubernetes confidential cluster within an Enterprise-Scale Landing Zone (ESLZ), vSPACE integrates Distributed Ledger Technology (DLT) for immutable and certifiable audit trails. The Infrastructure as Code (IaC) model ensures rapid deployment, consistent management, and adherence to security standards, making vSPACE a future-proof solution for digital voting systems.","sentences":["The vSPACE experimental proof-of-concept (PoC) on the TrueElect[Anon][Creds] protocol presents a novel approach to secure, private, and scalable elections, extending the TrueElect and ElectAnon protocols with the integration of AnonCreds SSI (Self-Sovereign Identity).","Such a protocol PoC is situated within a Zero-Trust Architecture (ZTA) and leverages confidential computing, continuous authentication, multi-party computation (MPC), and well-architected framework (WAF) principles to address the challenges of cybersecurity, privacy, and trust over IP (ToIP) protection.","Employing a Kubernetes confidential cluster within an Enterprise-Scale Landing Zone (ESLZ), vSPACE integrates Distributed Ledger Technology (DLT) for immutable and certifiable audit trails.","The Infrastructure as Code (IaC) model ensures rapid deployment, consistent management, and adherence to security standards, making vSPACE a future-proof solution for digital voting systems."],"url":"http://arxiv.org/abs/2403.05275v1","category":"cs.CR"}
{"created":"2024-03-08 12:52:11","title":"Elections in the Post-Quantum Era: Is the Complexity Shield Strong Enough?","abstract":"The election, a cornerstone of democracy, is one of the best-recognizable symbols of democratic governance. Voters' confidence in elections is essential, and these days, we can watch practically in live broadcast what consequences distrust in the fairness of elections may have. From the times of the celebrated Gibbard-Satterthwaite theorem, it is well-known in the social-choice community that most voting systems are vulnerable to the efforts of various players to influence elections. Luckily for us, computing such influence to affect election outcomes is a hard problem from the computational complexity perspective. This intractability is regarded as a ``complexity shield'' that secures voting rules against this malicious behavior.   In this work, we consider quantum computers to be a new threat to the complexity shield described above, as they break out of standard computing paradigms and unlock additional computational resources. To this end, we provide an overview of possible attacks on election, discuss the abilities of quantum computing, and chart possible directions for future research in this area.","sentences":["The election, a cornerstone of democracy, is one of the best-recognizable symbols of democratic governance.","Voters' confidence in elections is essential, and these days, we can watch practically in live broadcast what consequences distrust in the fairness of elections may have.","From the times of the celebrated Gibbard-Satterthwaite theorem, it is well-known in the social-choice community that most voting systems are vulnerable to the efforts of various players to influence elections.","Luckily for us, computing such influence to affect election outcomes is a hard problem from the computational complexity perspective.","This intractability is regarded as a ``complexity shield'' that secures voting rules against this malicious behavior.   ","In this work, we consider quantum computers to be a new threat to the complexity shield described above, as they break out of standard computing paradigms and unlock additional computational resources.","To this end, we provide an overview of possible attacks on election, discuss the abilities of quantum computing, and chart possible directions for future research in this area."],"url":"http://arxiv.org/abs/2403.05273v1","category":"cs.CR"}
{"created":"2024-03-08 12:51:10","title":"Engineering consensus in static networks with unknown disruptors","abstract":"Distributed control increases system scalability, flexibility, and redundancy. Foundational to such decentralisation is consensus formation, by which decision-making and coordination are achieved. However, decentralised multi-agent systems are inherently vulnerable to disruption. To develop a resilient consensus approach, inspiration is taken from the study of social systems and their dynamics; specifically, the Deffuant Model. A dynamic algorithm is presented enabling efficient consensus to be reached with an unknown number of disruptors present within a multi-agent system. By inverting typical social tolerance, agents filter out extremist non-standard opinions that would drive them away from consensus. This approach allows distributed systems to deal with unknown disruptions, without knowledge of the network topology or the numbers and behaviours of the disruptors. A disruptor-agnostic algorithm is particularly suitable to real-world applications where this information is typically unknown. Faster and tighter convergence can be achieved across a range of scenarios with the social dynamics inspired algorithm, compared with standard Mean-Subsequence-Reduced-type methods.","sentences":["Distributed control increases system scalability, flexibility, and redundancy.","Foundational to such decentralisation is consensus formation, by which decision-making and coordination are achieved.","However, decentralised multi-agent systems are inherently vulnerable to disruption.","To develop a resilient consensus approach, inspiration is taken from the study of social systems and their dynamics; specifically, the Deffuant Model.","A dynamic algorithm is presented enabling efficient consensus to be reached with an unknown number of disruptors present within a multi-agent system.","By inverting typical social tolerance, agents filter out extremist non-standard opinions that would drive them away from consensus.","This approach allows distributed systems to deal with unknown disruptions, without knowledge of the network topology or the numbers and behaviours of the disruptors.","A disruptor-agnostic algorithm is particularly suitable to real-world applications where this information is typically unknown.","Faster and tighter convergence can be achieved across a range of scenarios with the social dynamics inspired algorithm, compared with standard Mean-Subsequence-Reduced-type methods."],"url":"http://arxiv.org/abs/2403.05272v1","category":"cs.MA"}
{"created":"2024-03-08 12:41:47","title":"To Reach the Unreachable: Exploring the Potential of VR Hand Redirection for Upper Limb Rehabilitation","abstract":"Rehabilitation therapies are widely employed to assist people with motor impairments in regaining control over their affected body parts. Nevertheless, factors such as fatigue and low self-efficacy can hinder patient compliance during extensive rehabilitation processes. Utilizing hand redirection in virtual reality (VR) enables patients to accomplish seemingly more challenging tasks, thereby bolstering their motivation and confidence. While previous research has investigated user experience and hand redirection among able-bodied people, its effects on motor-impaired people remain unexplored. In this paper, we present a VR rehabilitation application that harnesses hand redirection. Through a user study and semi-structured interviews, we examine the impact of hand redirection on the rehabilitation experiences of people with motor impairments and its potential to enhance their motivation for upper limb rehabilitation. Our findings suggest that patients are not sensitive to hand movement inconsistency, and the majority express interest in incorporating hand redirection into future long-term VR rehabilitation programs.","sentences":["Rehabilitation therapies are widely employed to assist people with motor impairments in regaining control over their affected body parts.","Nevertheless, factors such as fatigue and low self-efficacy can hinder patient compliance during extensive rehabilitation processes.","Utilizing hand redirection in virtual reality (VR) enables patients to accomplish seemingly more challenging tasks, thereby bolstering their motivation and confidence.","While previous research has investigated user experience and hand redirection among able-bodied people, its effects on motor-impaired people remain unexplored.","In this paper, we present a VR rehabilitation application that harnesses hand redirection.","Through a user study and semi-structured interviews, we examine the impact of hand redirection on the rehabilitation experiences of people with motor impairments and its potential to enhance their motivation for upper limb rehabilitation.","Our findings suggest that patients are not sensitive to hand movement inconsistency, and the majority express interest in incorporating hand redirection into future long-term VR rehabilitation programs."],"url":"http://arxiv.org/abs/2403.05264v1","category":"cs.HC"}
{"created":"2024-03-08 12:29:52","title":"Some homological properties of category $\\mathcal{O}$, VII","abstract":"We describe Calabi-Yau objects in the regular block of the (parabolic) BGG category $\\mathcal{O}$ associated to a semi-simple finite dimensional complex Lie algebra. Each such object comes with a natural transformation from the Serre functor to a shifted identity whose evaluation at that object is an isomorphism.","sentences":["We describe Calabi-Yau objects in the regular block of the (parabolic) BGG category $\\mathcal{O}$ associated to a semi-simple finite dimensional complex Lie algebra.","Each such object comes with a natural transformation from the Serre functor to a shifted identity whose evaluation at that object is an isomorphism."],"url":"http://arxiv.org/abs/2403.05258v1","category":"math.RT"}
{"created":"2024-03-08 12:20:14","title":"Non-additivity in many-body interactions between membrane-deforming spheres increases disorder","abstract":"Membrane-induced interactions have been predicted to be important for the organization of membrane proteins. Measurements of the interactions between two and three membrane deforming objects have revealed their non-additive nature. They are thought to lead to complex many-body effects, however, experimental evidence is lacking to date. We here present an experimental method to measure many-body effects in membrane-mediated interactions using colloidal spheres placed between a deflated giant unilamellar vesicles and a planar substrate. The thus confined colloidal particles cause a large deformation of the membrane while not being physochemically attached to it and interact through it. Two particles are found to attract with a maximum force of 0.2~pN. For three particles, we observe a preference for forming compact equilateral triangles over a linear arrangement. We use numerical energy minimization to establish that the attraction stems from a reduction in the membrane-deformation energy caused by the particles. Confining up to 36 particles, we find a preference for hexagonally close packed clusters. However, with increasing number of particles the order of the confined particles decreases, while at the same time, diffusivity of the particles increases. Our experiments for the first time show that the non-additive nature of membrane-mediated interactions affects the interactions and arrangements and ultimately leads to spherical aggregates with liquid-like order of potential importance for cellular processes.","sentences":["Membrane-induced interactions have been predicted to be important for the organization of membrane proteins.","Measurements of the interactions between two and three membrane deforming objects have revealed their non-additive nature.","They are thought to lead to complex many-body effects, however, experimental evidence is lacking to date.","We here present an experimental method to measure many-body effects in membrane-mediated interactions using colloidal spheres placed between a deflated giant unilamellar vesicles and a planar substrate.","The thus confined colloidal particles cause a large deformation of the membrane while not being physochemically attached to it and interact through it.","Two particles are found to attract with a maximum force of 0.2~pN. For three particles, we observe a preference for forming compact equilateral triangles over a linear arrangement.","We use numerical energy minimization to establish that the attraction stems from a reduction in the membrane-deformation energy caused by the particles.","Confining up to 36 particles, we find a preference for hexagonally close packed clusters.","However, with increasing number of particles the order of the confined particles decreases, while at the same time, diffusivity of the particles increases.","Our experiments for the first time show that the non-additive nature of membrane-mediated interactions affects the interactions and arrangements and ultimately leads to spherical aggregates with liquid-like order of potential importance for cellular processes."],"url":"http://arxiv.org/abs/2403.05253v1","category":"cond-mat.soft"}
{"created":"2024-03-08 12:16:43","title":"Quantum error cancellation in photonic systems -- undoing photon losses","abstract":"Real photonic devices are subject to photon losses that can decohere quantum information encoded in the system. In the absence of full fault tolerance, quantum error mitigation techniques have been introduced to help manage errors in noisy quantum devices. In this work, we introduce an error mitigation protocol inspired by probabilistic error cancellation (a popular error mitigation technique in discrete variable systems) for continuous variable systems. We show that our quantum error cancellation protocol can undo photon losses in expectation value estimation tasks. To do this, we analytically derive the (non-physical) inverse photon loss channel and decompose it into a sum over physically realisable channels with potentially negative coefficients. The bias of our ideal expectation value estimator can be made arbitrarily small at the cost of increasing the sampling overhead. The protocol requires a noiseless amplification followed by a series of photon-subtractions. While these operations can be implemented probabilistically, for certain classes of initial state one can avoid the burden of carrying out the amplification and photon-subtractions by leveraging Monte-Carlo methods to give an unbiased estimate of the ideal expectation value. We validate our proposed mitigation protocol by simulating the scheme on squeezed vacuum states, cat states and entangled coherent states.","sentences":["Real photonic devices are subject to photon losses that can decohere quantum information encoded in the system.","In the absence of full fault tolerance, quantum error mitigation techniques have been introduced to help manage errors in noisy quantum devices.","In this work, we introduce an error mitigation protocol inspired by probabilistic error cancellation (a popular error mitigation technique in discrete variable systems) for continuous variable systems.","We show that our quantum error cancellation protocol can undo photon losses in expectation value estimation tasks.","To do this, we analytically derive the (non-physical) inverse photon loss channel and decompose it into a sum over physically realisable channels with potentially negative coefficients.","The bias of our ideal expectation value estimator can be made arbitrarily small at the cost of increasing the sampling overhead.","The protocol requires a noiseless amplification followed by a series of photon-subtractions.","While these operations can be implemented probabilistically, for certain classes of initial state one can avoid the burden of carrying out the amplification and photon-subtractions by leveraging Monte-Carlo methods to give an unbiased estimate of the ideal expectation value.","We validate our proposed mitigation protocol by simulating the scheme on squeezed vacuum states, cat states and entangled coherent states."],"url":"http://arxiv.org/abs/2403.05252v1","category":"quant-ph"}
{"created":"2024-03-08 12:16:16","title":"Errors due to departure from independence in multivariate Weibull distributions","abstract":"We do the error analysis in reliability measures due to the assumption of independence amongst the component lifetimes. In reliability theory, we come across different n-component structures like series, parallel, and k-out-of-n systems. A n component series system works only if all the n components work. While studying the reliability measures of a n-component series system, we mostly assume that all the components have independent lifetimes. Such an assumption eases mathematical complexity while analyzing the data and hence is very common. But in reality, the lifetimes of the components are very much interdependent. Such an assumption of independence hence leads to inaccurate analysis of data. In multiple situations like studying a complex system with many components, we turn to assuming independence keeping some room for error. However, if we have some knowledge of the behaviour of errors or some estimate on the error bound, we could decide if we assume independence and prefer mathematical simplicity (if we know the error is within our allowed limit), or keep the mathematical complexity and get accurate results without assuming independence. We aim to find the relative errors in the reliability measures for a n-component series system.","sentences":["We do the error analysis in reliability measures due to the assumption of independence amongst the component lifetimes.","In reliability theory, we come across different n-component structures like series, parallel, and k-out-of-n systems.","A n component series system works only if all the n components work.","While studying the reliability measures of a n-component series system, we mostly assume that all the components have independent lifetimes.","Such an assumption eases mathematical complexity while analyzing the data and hence is very common.","But in reality, the lifetimes of the components are very much interdependent.","Such an assumption of independence hence leads to inaccurate analysis of data.","In multiple situations like studying a complex system with many components, we turn to assuming independence keeping some room for error.","However, if we have some knowledge of the behaviour of errors or some estimate on the error bound, we could decide if we assume independence and prefer mathematical simplicity (if we know the error is within our allowed limit), or keep the mathematical complexity and get accurate results without assuming independence.","We aim to find the relative errors in the reliability measures for a n-component series system."],"url":"http://arxiv.org/abs/2403.05251v1","category":"math.ST"}
{"created":"2024-03-08 12:15:04","title":"Klt degenerations of projective spaces","abstract":"We study degenerations of complex projective spaces $\\mathbb P^n$ into normal projective klt varieties $X$. If the tangent sheaf of $X$ is semi-stable, we show that $X$ itself is a projective space. If $X$ is a threefold with canonical singularities, we show that there are only three varieties which satisfy all the conditions.","sentences":["We study degenerations of complex projective spaces $\\mathbb P^n$ into normal projective klt varieties $X$. If the tangent sheaf of $X$ is semi-stable, we show that $X$ itself is a projective space.","If $X$ is a threefold with canonical singularities, we show that there are only three varieties which satisfy all the conditions."],"url":"http://arxiv.org/abs/2403.05250v1","category":"math.AG"}
{"created":"2024-03-08 12:07:42","title":"LightM-UNet: Mamba Assists in Lightweight UNet for Medical Image Segmentation","abstract":"UNet and its variants have been widely used in medical image segmentation. However, these models, especially those based on Transformer architectures, pose challenges due to their large number of parameters and computational loads, making them unsuitable for mobile health applications. Recently, State Space Models (SSMs), exemplified by Mamba, have emerged as competitive alternatives to CNN and Transformer architectures. Building upon this, we employ Mamba as a lightweight substitute for CNN and Transformer within UNet, aiming at tackling challenges stemming from computational resource limitations in real medical settings. To this end, we introduce the Lightweight Mamba UNet (LightM-UNet) that integrates Mamba and UNet in a lightweight framework. Specifically, LightM-UNet leverages the Residual Vision Mamba Layer in a pure Mamba fashion to extract deep semantic features and model long-range spatial dependencies, with linear computational complexity. Extensive experiments conducted on two real-world 2D/3D datasets demonstrate that LightM-UNet surpasses existing state-of-the-art literature. Notably, when compared to the renowned nnU-Net, LightM-UNet achieves superior segmentation performance while drastically reducing parameter and computation costs by 116x and 21x, respectively. This highlights the potential of Mamba in facilitating model lightweighting. Our code implementation is publicly available at https://github.com/MrBlankness/LightM-UNet.","sentences":["UNet and its variants have been widely used in medical image segmentation.","However, these models, especially those based on Transformer architectures, pose challenges due to their large number of parameters and computational loads, making them unsuitable for mobile health applications.","Recently, State Space Models (SSMs), exemplified by Mamba, have emerged as competitive alternatives to CNN and Transformer architectures.","Building upon this, we employ Mamba as a lightweight substitute for CNN and Transformer within UNet, aiming at tackling challenges stemming from computational resource limitations in real medical settings.","To this end, we introduce the Lightweight Mamba UNet (LightM-UNet) that integrates Mamba and UNet in a lightweight framework.","Specifically, LightM-UNet leverages the Residual Vision Mamba Layer in a pure Mamba fashion to extract deep semantic features and model long-range spatial dependencies, with linear computational complexity.","Extensive experiments conducted on two real-world 2D/3D datasets demonstrate that LightM-UNet surpasses existing state-of-the-art literature.","Notably, when compared to the renowned nnU-Net, LightM-UNet achieves superior segmentation performance while drastically reducing parameter and computation costs by 116x and 21x, respectively.","This highlights the potential of Mamba in facilitating model lightweighting.","Our code implementation is publicly available at https://github.com/MrBlankness/LightM-UNet."],"url":"http://arxiv.org/abs/2403.05246v1","category":"eess.IV"}
{"created":"2024-03-08 12:06:16","title":"Multipartite entanglement in the diagonal symmetric subspace","abstract":"We investigate the entanglement properties in the symmetric subspace of $N$-partite $d$-dimensional systems (qudits). For diagonal symmetric states, we show that there is no bound entanglement for $d = 3,4 $ and $N = 3$. Further, we present a constructive algorithm to map multipartite diagonal symmetric states of qudits onto bipartite symmetric states of larger local dimension. This technique greatly simplifies the analysis of multipartite states and allows to infer entanglement properties for any even $N \\geq 4 $ due to the fact that the PPT conditions that arise from the bipartite symmetric state correspond to the same PPT conditions that appear in the multipartite diagonal symmetric state.","sentences":["We investigate the entanglement properties in the symmetric subspace of $N$-partite $d$-dimensional systems (qudits).","For diagonal symmetric states, we show that there is no bound entanglement for $d = 3,4 $ and $N = 3$. Further, we present a constructive algorithm to map multipartite diagonal symmetric states of qudits onto bipartite symmetric states of larger local dimension.","This technique greatly simplifies the analysis of multipartite states and allows to infer entanglement properties for any even $N \\geq 4 $ due to the fact that the PPT conditions that arise from the bipartite symmetric state correspond to the same PPT conditions that appear in the multipartite diagonal symmetric state."],"url":"http://arxiv.org/abs/2403.05244v1","category":"quant-ph"}
{"created":"2024-03-08 11:59:01","title":"Spin-1/2 string correlations and singlet-triplet gaps of frustrated ladders with ferromagnetic (F) legs and alternate F and AF rungs","abstract":"The frustrated ladder with alternate ferromagnetic(F) exchange $-J_F$ and AF exchange $J_A$ to first neighbors and F exchange $-J_L$ to second neighbors is studied by exact diagonalization (ED) and density matrix renormalization group (DMRG) calculations in systems of $2N$ spins-1/2 with periodic boundary conditions. The ground state is a singlet $(S = 0)$ and the singlet-triplet gap $\\varepsilon_T$ is finite for the exchanges considered. Spin-1/2 string correlation functions $g_1(N)$ and $g_2(N)$ are defined for an even number $N$ of consecutive spins in systems with two spins per unit cell; the ladder has string order $g_2(\\infty)> 0$ and $g_1(\\infty) = 0$. The minimum $N^*$ of $g_2(N)$ is related to the range of ground-state spin correlations. Convergence to $g_2(\\infty)$ is from below, and $g_1(N)$ decreases exponentially for $N \\geq N^*$. Singlet valence bond (VB) diagrams account for the size dependencies. The frustrated ladder at special values of $J_F$, $J_L$ and $J_A$ reduces to well-known models such as the spin-1 Heisenberg antiferromagnet and the $J_1-J_2$ model, among others. Numerical analysis of ladders matches previous results for spin-1 gaps or string correlation functions and extends them to spin-1/2 systems. The nondegenerate singlet ground state of ladder is a bond-order wave, a Kekul\\'e VB diagrams at $J_L = J_F/2 \\leq J_A$, that is reversed on interchanging $-J_F$ and $J_A$. Inversion symmetry is spontaneously broken in the dimer phase of the $J_1-J_2$ model where the Kekul\\'e diagrams are the doubly degenerate ground states at $J_2/J_1 = 1/2$.","sentences":["The frustrated ladder with alternate ferromagnetic(F) exchange $-J_F$ and AF exchange $J_A$ to first neighbors and F exchange $-J_L$ to second neighbors is studied by exact diagonalization (ED) and density matrix renormalization group (DMRG) calculations in systems of $2N$ spins-1/2 with periodic boundary conditions.","The ground state is a singlet $(S = 0)$ and the singlet-triplet gap $\\varepsilon_T$ is finite for the exchanges considered.","Spin-1/2 string correlation functions $g_1(N)$ and $g_2(N)$ are defined for an even number $N$ of consecutive spins in systems with two spins per unit cell; the ladder has string order $g_2(\\infty)> 0$ and $g_1(\\infty) = 0$.","The minimum $N^*$ of $g_2(N)$ is related to the range of ground-state spin correlations.","Convergence to $g_2(\\infty)$ is from below, and $g_1(N)$ decreases exponentially for $N \\geq N^*$. Singlet valence bond (VB) diagrams account for the size dependencies.","The frustrated ladder at special values of $J_F$, $J_L$ and $J_A$ reduces to well-known models such as the spin-1 Heisenberg antiferromagnet and the $J_1-J_2$ model, among others.","Numerical analysis of ladders matches previous results for spin-1 gaps or string correlation functions and extends them to spin-1/2 systems.","The nondegenerate singlet ground state of ladder is a bond-order wave, a Kekul\\'e VB diagrams at $J_L = J_F/2 \\leq J_A$, that is reversed on interchanging $-J_F$ and $J_A$. Inversion symmetry is spontaneously broken in the dimer phase of the $J_1-J_2$ model where the Kekul\\'e diagrams are the doubly degenerate ground states at $J_2/J_1 = 1/2$."],"url":"http://arxiv.org/abs/2403.05238v1","category":"cond-mat.str-el"}
{"created":"2024-03-08 11:57:49","title":"Complex organic molecules uncover deeply embedded precursors of hot cores","abstract":"During the process of star formation, the dense gas undergoes significant chemical evolution leading to the emergence of a rich variety of molecules associated with hot cores and hot corinos. However, the physical and chemical conditions involved in this evolution are poorly constrained. We provide here a full inventory of the emission from complex organic molecules (COMs) to investigate the physical structure and chemical composition of six high-mass protostellar envelopes. We aim to investigate the conditions for the emergence of COMs in hot cores. We performed an unbiased spectral survey towards six infrared-quiet massive clumps between 159 GHz and 374 GHz with the APEX 12 m telescope. We detect up to 11 COMs, of which at least five COMs are detected towards all sources. Towards all the objects, most of the COM emission is found to be cold, with respect to the typical temperatures at which COMs are found, with a temperature of 30 K and extended with a size of ~0.3 pc. Although for our sample of young massive clumps the bulk of the gas has a cold temperature, we also detect emission from COMs originating from the immediate vicinity of the protostar revealing a compact and hot component of the envelope. Only three out of the six sources exhibit a hot gas component. We find a gradual emergence of the warm component in terms of size and temperature, together with an increasing molecular complexity, allowing us to establish an evolutionary sequence for our sample based on COMs. Our findings confirm that our sample of infrared-quiet massive clumps are in an early evolutionary stage during which the bulk of the gas is cold. The presence of COMs is found to be characteristic of these early evolutionary stages. We suggest that the emergence of hot cores is preceded by a phase in which mostly O-bearing COMs appear first with similar abundances to hot corinos albeit with larger source sizes.","sentences":["During the process of star formation, the dense gas undergoes significant chemical evolution leading to the emergence of a rich variety of molecules associated with hot cores and hot corinos.","However, the physical and chemical conditions involved in this evolution are poorly constrained.","We provide here a full inventory of the emission from complex organic molecules (COMs) to investigate the physical structure and chemical composition of six high-mass protostellar envelopes.","We aim to investigate the conditions for the emergence of COMs in hot cores.","We performed an unbiased spectral survey towards six infrared-quiet massive clumps between 159 GHz and 374 GHz with the APEX 12 m telescope.","We detect up to 11 COMs, of which at least five COMs are detected towards all sources.","Towards all the objects, most of the COM emission is found to be cold, with respect to the typical temperatures at which COMs are found, with a temperature of 30 K and extended with a size of ~0.3 pc.","Although for our sample of young massive clumps the bulk of the gas has a cold temperature, we also detect emission from COMs originating from the immediate vicinity of the protostar revealing a compact and hot component of the envelope.","Only three out of the six sources exhibit a hot gas component.","We find a gradual emergence of the warm component in terms of size and temperature, together with an increasing molecular complexity, allowing us to establish an evolutionary sequence for our sample based on COMs.","Our findings confirm that our sample of infrared-quiet massive clumps are in an early evolutionary stage during which the bulk of the gas is cold.","The presence of COMs is found to be characteristic of these early evolutionary stages.","We suggest that the emergence of hot cores is preceded by a phase in which mostly O-bearing COMs appear first with similar abundances to hot corinos albeit with larger source sizes."],"url":"http://arxiv.org/abs/2403.05237v1","category":"astro-ph.GA"}
{"created":"2024-03-08 11:54:40","title":"Fault Recovery and Transient Stability of Grid-Forming Converters Equipped with Current Saturation","abstract":"When grid-forming (GFM) inverter-based resources (IBRs) experience large grid disturbances (e.g., short-circuit faults), the current limiter may be triggered and GFM IBRs enter the current saturation mode, inducing nonlinear dynamical behaviors and imposing great challenges to the post-disturbance transient angle stability. This paper presents a systematic study to reveal the fault recovery behaviors of a GFM IBR and identify the risk of instability. The impact of the angle of the magnitude-saturated current on the post-fault recovery and transient stability is also investigated. The selection of the angle of magnitude-saturated current significantly influences the post-fault behaviors while a few additional dynamical conditions that have a substantial impact are also identified. It is found that the system may follow multiple post-fault recovery trajectories depending on those conditions: 1) Convergence to the normal stable equilibrium point (SEP), 2) convergence to the saturated stable equilibrium point (SSEP), and 3) divergence (instability). To examine the models' accuracy, several cases are simulated.","sentences":["When grid-forming (GFM) inverter-based resources (IBRs) experience large grid disturbances (e.g., short-circuit faults), the current limiter may be triggered and GFM IBRs enter the current saturation mode, inducing nonlinear dynamical behaviors and imposing great challenges to the post-disturbance transient angle stability.","This paper presents a systematic study to reveal the fault recovery behaviors of a GFM IBR and identify the risk of instability.","The impact of the angle of the magnitude-saturated current on the post-fault recovery and transient stability is also investigated.","The selection of the angle of magnitude-saturated current significantly influences the post-fault behaviors while a few additional dynamical conditions that have a substantial impact are also identified.","It is found that the system may follow multiple post-fault recovery trajectories depending on those conditions: 1) Convergence to the normal stable equilibrium point (SEP), 2) convergence to the saturated stable equilibrium point (SSEP), and 3) divergence (instability).","To examine the models' accuracy, several cases are simulated."],"url":"http://arxiv.org/abs/2403.05236v1","category":"eess.SY"}
{"created":"2024-03-08 11:31:49","title":"Chemical Reactions regulated by Phase-Separated Condensates","abstract":"Phase-separated liquid condensates can spatially organize and thereby regulate chemical processes. However, the physicochemical mechanisms underlying such regulation remain elusive as the intramolecular interactions responsible for phase separation give rise to a coupling between diffusion and chemical reactions at non-dilute conditions. Here, we derive a theoretical framework that decouples the phase separation of scaffold molecules from the reaction kinetics of diluted clients. As a result, phase volume and client partitioning coefficients become control parameters, which enables us to dissect the impact of phase-separated condensates on chemical reactions. We apply this framework to two chemical processes and show how condensates affect the yield of reversible chemical reactions and the initial rate of a simple assembly process. In both cases, we find an optimal condensate volume at which the respective chemical reaction property is maximal. Our work can be applied to experimentally quantify how condensed phases alter chemical processes in systems biology and unravel the mechanisms of how biomolecular condensates regulate biochemistry in living cells.","sentences":["Phase-separated liquid condensates can spatially organize and thereby regulate chemical processes.","However, the physicochemical mechanisms underlying such regulation remain elusive as the intramolecular interactions responsible for phase separation give rise to a coupling between diffusion and chemical reactions at non-dilute conditions.","Here, we derive a theoretical framework that decouples the phase separation of scaffold molecules from the reaction kinetics of diluted clients.","As a result, phase volume and client partitioning coefficients become control parameters, which enables us to dissect the impact of phase-separated condensates on chemical reactions.","We apply this framework to two chemical processes and show how condensates affect the yield of reversible chemical reactions and the initial rate of a simple assembly process.","In both cases, we find an optimal condensate volume at which the respective chemical reaction property is maximal.","Our work can be applied to experimentally quantify how condensed phases alter chemical processes in systems biology and unravel the mechanisms of how biomolecular condensates regulate biochemistry in living cells."],"url":"http://arxiv.org/abs/2403.05228v1","category":"cond-mat.soft"}
{"created":"2024-03-08 10:55:07","title":"Improving the Successful Robotic Grasp Detection Using Convolutional Neural Networks","abstract":"Robotic grasp should be carried out in a real-time manner by proper accuracy. Perception is the first and significant step in this procedure. This paper proposes an improved pipeline model trying to detect grasp as a rectangle representation for different seen or unseen objects. It helps the robot to start control procedures from nearer to the proper part of the object. The main idea consists in pre-processing, output normalization, and data augmentation to improve accuracy by 4.3 percent without making the system slow. Also, a comparison has been conducted over different pre-trained models like AlexNet, ResNet, Vgg19, which are the most famous feature extractors for image processing in object detection. Although AlexNet has less complexity than other ones, it outperformed them, which helps the real-time property.","sentences":["Robotic grasp should be carried out in a real-time manner by proper accuracy.","Perception is the first and significant step in this procedure.","This paper proposes an improved pipeline model trying to detect grasp as a rectangle representation for different seen or unseen objects.","It helps the robot to start control procedures from nearer to the proper part of the object.","The main idea consists in pre-processing, output normalization, and data augmentation to improve accuracy by 4.3 percent without making the system slow.","Also, a comparison has been conducted over different pre-trained models like AlexNet, ResNet, Vgg19, which are the most famous feature extractors for image processing in object detection.","Although AlexNet has less complexity than other ones, it outperformed them, which helps the real-time property."],"url":"http://arxiv.org/abs/2403.05211v1","category":"cs.RO"}
{"created":"2024-03-08 10:44:04","title":"Bass numbers and endomorphism rings of Gorenstein injective modules","abstract":"Let $R$ be a commutative noetherian ring admitting a dualizing complex and let $\\mathfrak p$ be a prime ideal of $R$. In this paper we investigate when $G(R/\\frak p)$ is an $R_{\\frak p}$-module. We give some necessary and sufficient conditions under which $G(R/\\frak p)$ is an $R_{\\frak p}$-module. We also study the Bass numbers of $G(R/\\frak p)$ and we show that if ${\\rm Gid}_RR/\\frak p$ is finite, then $\\mu^i(\\frak q,G(R/\\frak p))$ is finite for all $i\\geq 0$ and all $\\frak q\\in{\\rm Spec} R$. If ${\\rm Gpd}_RR/\\frak p$ is finite, then $\\mu^i(\\frak p,G(R/\\frak p))$ is finite for all $i\\geq 0$. We define a subring $S(\\frak p)_{\\frak p}$ of ${\\rm End}_{R_{\\frak p}}(G(R_{\\frak p}/\\frak pR_{\\frak p}))$ and we show that it is noetherian and contains a subring which is a quotient of $\\widehat{R_{\\frak p}}$.","sentences":["Let $R$ be a commutative noetherian ring admitting a dualizing complex and let $\\mathfrak p$ be a prime ideal of $R$. In this paper we investigate when $G(R/\\frak p)$ is an $R_{\\frak p}$-module.","We give some necessary and sufficient conditions under which $G(R/\\frak p)$ is an $R_{\\frak p}$-module.","We also study the Bass numbers of $G(R/\\frak p)$","and we show that if ${\\rm Gid}_RR/\\frak p$ is finite, then $\\mu^i(\\frak q,G(R/\\frak p))$ is finite for all $i\\geq 0$ and all $\\frak q\\in{\\rm Spec} R$.","If ${\\rm Gpd}_RR/\\frak p$ is finite, then $\\mu^i(\\frak p,G(R/\\frak p))$ is finite for all $i\\geq 0$.","We define a subring $S(\\frak p)_{\\frak p}$ of ${\\rm End}_{R_{\\frak p}}(G(R_{\\frak p}/\\frak pR_{\\frak p}))$ and we show that it is noetherian and contains a subring which is a quotient of $\\widehat{R_{\\frak p}}$."],"url":"http://arxiv.org/abs/2403.05207v1","category":"math.AC"}
{"created":"2024-03-08 10:38:01","title":"Interoperability of the Metaverse: A Digital Ecosystem Perspective Review","abstract":"The Metaverse is at the vanguard of the impending digital revolution, with the potential to significantly transform industries and lifestyles. However, in 2023, skepticism surfaced within industrial and academic spheres, raising concerns that excitement may outpace actual technological progress. Interoperability, recognized as a major barrier to the Metaverse's full potential, is central to this debate. CoinMarketCap's report in February 2023 indicated that of over 240 metaverse initiatives, most existed in isolation, underscoring the interoperability challenge. Despite consensus on its critical role, there is a research gap in exploring the impact on the Metaverse, significance, and developmental extent. Our study bridges this gap via a systematic literature review and content analysis of the Web of Science (WoS) and Scopus databases, yielding 74 publications after a rigorous selection process. Interoperability, difficult to define due to varied contexts and lack of standardization, is central to the Metaverse, often seen as a digital ecosystem. Urs Gasser's framework from Harvard Law School, outlining technological, data, human, and institutional dimensions, systematically addresses interoperability complexities. Incorporating this framework, we dissect literature for a comprehensive Metaverse interoperability overview. Our study seeks to establish benchmarks for future inquiries, navigating the complex field of Metaverse interoperability studies and contributing to academic advancement.","sentences":["The Metaverse is at the vanguard of the impending digital revolution, with the potential to significantly transform industries and lifestyles.","However, in 2023, skepticism surfaced within industrial and academic spheres, raising concerns that excitement may outpace actual technological progress.","Interoperability, recognized as a major barrier to the Metaverse's full potential, is central to this debate.","CoinMarketCap's report in February 2023 indicated that of over 240 metaverse initiatives, most existed in isolation, underscoring the interoperability challenge.","Despite consensus on its critical role, there is a research gap in exploring the impact on the Metaverse, significance, and developmental extent.","Our study bridges this gap via a systematic literature review and content analysis of the Web of Science (WoS) and Scopus databases, yielding 74 publications after a rigorous selection process.","Interoperability, difficult to define due to varied contexts and lack of standardization, is central to the Metaverse, often seen as a digital ecosystem.","Urs Gasser's framework from Harvard Law School, outlining technological, data, human, and institutional dimensions, systematically addresses interoperability complexities.","Incorporating this framework, we dissect literature for a comprehensive Metaverse interoperability overview.","Our study seeks to establish benchmarks for future inquiries, navigating the complex field of Metaverse interoperability studies and contributing to academic advancement."],"url":"http://arxiv.org/abs/2403.05205v1","category":"cs.CY"}
{"created":"2024-03-08 10:30:32","title":"Global Solutions and Asymptotic Behavior for the Three-dimensional Viscous Non-resistive MHD System with Some Large Perturbations","abstract":"We revisit the global existence of solutions with some large perturbations to the incompressible, viscous, and non-resistive MHD system in a three-dimensional periodic domain, where the impressed magnetic field satisfies the Diophantine condition, and the intensity of the impressed magnetic field, denoted by $m$, is large compared to the perturbations. It was proved by Jiang--Jiang that the highest-order derivatives of the velocity increase with $m$, and the convergence rate of the nonlinear system towards a linearized problem is of $m^{-1/2}$ in [F. Jiang and S. Jiang, Arch. Ration. Mech. Anal., 247 (2023), 96]. In this paper, we adopt a different approach by leveraging vorticity estimates to establish the highest-order energy estimate. This strategy prevents the appearance of terms that grow with $m$, and thus the increasing behavior of the highest-order derivatives of the velocity with respect to $m$ does not appear. Additionally, we use the vorticity estimate to demonstrate the convergence rate of the nonlinear system towards a linearized problem as time or $m$ approaches infinity. Notably, our analysis reveals that the convergence rate in $m$ is faster compared to the finding of Jiang--Jiang. Finally, a key contribution of our work is the identification of an integrable time-decay of the lower dissipation, which can replace the time-decay of lower energy in closing the highest-order energy estimate. This finding significantly relaxes the regularity requirements for the initial perturbations.","sentences":["We revisit the global existence of solutions with some large perturbations to the incompressible, viscous, and non-resistive MHD system in a three-dimensional periodic domain, where the impressed magnetic field satisfies the Diophantine condition, and the intensity of the impressed magnetic field, denoted by $m$, is large compared to the perturbations.","It was proved by Jiang--Jiang that the highest-order derivatives of the velocity increase with $m$, and the convergence rate of the nonlinear system towards a linearized problem is of $m^{-1/2}$ in [F. Jiang and S. Jiang, Arch.","Ration.","Mech.","Anal., 247 (2023), 96].","In this paper, we adopt a different approach by leveraging vorticity estimates to establish the highest-order energy estimate.","This strategy prevents the appearance of terms that grow with $m$, and thus the increasing behavior of the highest-order derivatives of the velocity with respect to $m$ does not appear.","Additionally, we use the vorticity estimate to demonstrate the convergence rate of the nonlinear system towards a linearized problem as time or $m$ approaches infinity.","Notably, our analysis reveals that the convergence rate in $m$ is faster compared to the finding of Jiang--Jiang.","Finally, a key contribution of our work is the identification of an integrable time-decay of the lower dissipation, which can replace the time-decay of lower energy in closing the highest-order energy estimate.","This finding significantly relaxes the regularity requirements for the initial perturbations."],"url":"http://arxiv.org/abs/2403.05203v1","category":"math.AP"}
{"created":"2024-03-08 10:22:01","title":"A fully discretization, unconditionally energy stable finite element method solving the thermodynamically consistent diffuse interface model for incompressible two-phase MHD flows with large density ratios","abstract":"A diffusion interface two-phase magnetohydrodynamic model has been used for matched densities in our previous work [1,2], which may limit the applications of the model. In this work, we derive a thermodynamically consistent diffuse interface model for diffusion interface two-phase magnetohydrodynamic fluids with large density ratios by Onsager's variational principle and conservation law for the first time. The finite element method for spatial discretization and the first order semi-implicit scheme linked with convect splitting method for temporal discretization, is proposed to solve this new model. The mass conservation, unconditionally energy stability and convergence of the scheme can be proved. Then we derive the existence of weak solutions of governing system employing the above properties of the scheme and compactness method. Finally, we show some numerical results to test the effectiveness and well behavior of proposed scheme.","sentences":["A diffusion interface two-phase magnetohydrodynamic model has been used for matched densities in our previous work [1,2], which may limit the applications of the model.","In this work, we derive a thermodynamically consistent diffuse interface model for diffusion interface two-phase magnetohydrodynamic fluids with large density ratios by Onsager's variational principle and conservation law for the first time.","The finite element method for spatial discretization and the first order semi-implicit scheme linked with convect splitting method for temporal discretization, is proposed to solve this new model.","The mass conservation, unconditionally energy stability and convergence of the scheme can be proved.","Then we derive the existence of weak solutions of governing system employing the above properties of the scheme and compactness method.","Finally, we show some numerical results to test the effectiveness and well behavior of proposed scheme."],"url":"http://arxiv.org/abs/2403.05200v1","category":"math.NA"}
{"created":"2024-03-08 10:15:07","title":"On the crossing number of arithmetic curve systems","abstract":"We show that the family of systoles of hyperbolic surfaces associated with congruence lattices in $\\mathrm{SL}_2(\\mathbb{Z})$ have asymptotically minimal crossing number.","sentences":["We show that the family of systoles of hyperbolic surfaces associated with congruence lattices in $\\mathrm{SL}_2(\\mathbb{Z})$ have asymptotically minimal crossing number."],"url":"http://arxiv.org/abs/2403.05194v1","category":"math.GT"}
{"created":"2024-03-08 10:13:45","title":"Congruences of regular variants of finite full transformation semigroups","abstract":"Let $T_X$ be the full transformation monoid over a finite set $X$, and fix some $a\\in T_X$ of rank $r$. The variant $T_X^a$ has underlying set $T_X$, and operation $f\\star g=fag$. We study the congruences of the subsemigroup $P=Reg(T_X^a)$ consisting of all regular elements of $T_X^a$, and the lattice $Cong(P)$ of all such congruences. Our main structure theorem ultimately decomposes $Cong(P)$ as a specific subdirect product of $Cong(T_r)$ and the full equivalence relation lattices of certain combinatorial systems of subsets and partitions. We use this to give an explicit classification of the congruences themselves, and we also give a formula for the height of the lattice.","sentences":["Let $T_X$ be the full transformation monoid over a finite set $X$, and fix some $a\\in T_X$ of rank $r$. The variant $T_X^a$ has underlying set $T_X$, and operation $f\\star g=fag$. We study the congruences of the subsemigroup $P=Reg(T_X^a)$ consisting of all regular elements of $T_X^a$, and the lattice $Cong(P)$ of all such congruences.","Our main structure theorem ultimately decomposes $Cong(P)$ as a specific subdirect product of $Cong(T_r)$ and the full equivalence relation lattices of certain combinatorial systems of subsets and partitions.","We use this to give an explicit classification of the congruences themselves, and we also give a formula for the height of the lattice."],"url":"http://arxiv.org/abs/2403.05191v1","category":"math.RA"}
{"created":"2024-03-08 10:13:34","title":"Stable tree expressions with Omega-classes and Double Ramification cycles","abstract":"We propose a new system of conjectural relations in the tautological ring of the moduli space of curves involving stable rooted trees with level structure decorated by Hodge and {\\Omega}-classes and prove these conjectures in different cases.","sentences":["We propose a new system of conjectural relations in the tautological ring of the moduli space of curves involving stable rooted trees with level structure decorated by Hodge and {\\Omega}-classes and prove these conjectures in different cases."],"url":"http://arxiv.org/abs/2403.05190v1","category":"math.AG"}
{"created":"2024-03-08 09:53:07","title":"Personalized Audiobook Recommendations at Spotify Through Graph Neural Networks","abstract":"In the ever-evolving digital audio landscape, Spotify, well-known for its music and talk content, has recently introduced audiobooks to its vast user base. While promising, this move presents significant challenges for personalized recommendations. Unlike music and podcasts, audiobooks, initially available for a fee, cannot be easily skimmed before purchase, posing higher stakes for the relevance of recommendations. Furthermore, introducing a new content type into an existing platform confronts extreme data sparsity, as most users are unfamiliar with this new content type. Lastly, recommending content to millions of users requires the model to react fast and be scalable. To address these challenges, we leverage podcast and music user preferences and introduce 2T-HGNN, a scalable recommendation system comprising Heterogeneous Graph Neural Networks (HGNNs) and a Two Tower (2T) model. This novel approach uncovers nuanced item relationships while ensuring low latency and complexity. We decouple users from the HGNN graph and propose an innovative multi-link neighbor sampler. These choices, together with the 2T component, significantly reduce the complexity of the HGNN model. Empirical evaluations involving millions of users show significant improvement in the quality of personalized recommendations, resulting in a +46% increase in new audiobooks start rate and a +23% boost in streaming rates. Intriguingly, our model's impact extends beyond audiobooks, benefiting established products like podcasts.","sentences":["In the ever-evolving digital audio landscape, Spotify, well-known for its music and talk content, has recently introduced audiobooks to its vast user base.","While promising, this move presents significant challenges for personalized recommendations.","Unlike music and podcasts, audiobooks, initially available for a fee, cannot be easily skimmed before purchase, posing higher stakes for the relevance of recommendations.","Furthermore, introducing a new content type into an existing platform confronts extreme data sparsity, as most users are unfamiliar with this new content type.","Lastly, recommending content to millions of users requires the model to react fast and be scalable.","To address these challenges, we leverage podcast and music user preferences and introduce 2T-HGNN, a scalable recommendation system comprising Heterogeneous Graph Neural Networks (HGNNs) and a Two Tower (2T) model.","This novel approach uncovers nuanced item relationships while ensuring low latency and complexity.","We decouple users from the HGNN graph and propose an innovative multi-link neighbor sampler.","These choices, together with the 2T component, significantly reduce the complexity of the HGNN model.","Empirical evaluations involving millions of users show significant improvement in the quality of personalized recommendations, resulting in a +46% increase in new audiobooks start rate and a +23% boost in streaming rates.","Intriguingly, our model's impact extends beyond audiobooks, benefiting established products like podcasts."],"url":"http://arxiv.org/abs/2403.05185v1","category":"cs.IR"}
{"created":"2024-03-08 09:33:13","title":"A design methodology for nonlinear oscillator chains enabling energy localization tuning and soliton stability enhancement with optimal damping","abstract":"In this paper, the vibration energy localization in coupled nonlinear oscillators is investigated, based on the creation of standing solitons. The main objective is to establish a design methodology for mechanical lattices using the Nonlinear Schr\\\"odinger Equation (NLSE) as a guide strategy, even in the presence of damping. A three-dimensional diagram is used to illustrate stable parameter regions for damped stationary solitons. Moreover, an analysis of the influence of the number of oscillators in the system, and a numerical investigation regarding the stability of solitonic behavior is done. Through numerical analyses, it is observed that the developed algorithm not only has the capability to locate the highest amplitudes in the chain of oscillators, but also to control the intensity at which these amplitudes are located according to design requirements. The outcomes of the proposed methodology elucidate the impact that the coupling stiffness has on the stabilization of the NLSE, as well as the influence of the number of oscillators on the continuity hypothesis. The developed algorithm holds potential for practical applications in mechanical engineering since the NLSE is used as a design line rather than as a consequence of the phenomenon description.","sentences":["In this paper, the vibration energy localization in coupled nonlinear oscillators is investigated, based on the creation of standing solitons.","The main objective is to establish a design methodology for mechanical lattices using the Nonlinear Schr\\\"odinger Equation (NLSE) as a guide strategy, even in the presence of damping.","A three-dimensional diagram is used to illustrate stable parameter regions for damped stationary solitons.","Moreover, an analysis of the influence of the number of oscillators in the system, and a numerical investigation regarding the stability of solitonic behavior is done.","Through numerical analyses, it is observed that the developed algorithm not only has the capability to locate the highest amplitudes in the chain of oscillators, but also to control the intensity at which these amplitudes are located according to design requirements.","The outcomes of the proposed methodology elucidate the impact that the coupling stiffness has on the stabilization of the NLSE, as well as the influence of the number of oscillators on the continuity hypothesis.","The developed algorithm holds potential for practical applications in mechanical engineering since the NLSE is used as a design line rather than as a consequence of the phenomenon description."],"url":"http://arxiv.org/abs/2403.05176v1","category":"nlin.PS"}
{"created":"2024-03-08 09:26:44","title":"Bending-Rotation coupling in the viscoelasticity of semi-flexible polymers -- Rigorous perturbation analysis from the rod limit","abstract":"Brownian motion and viscoelasticity of semi-flexible polymers is a subject that has been studied for many years. Still, rigorous analysis has been hindered due to the difficulty in handling the constraint that polymer chains cannot be stretched along the contour. Here, we show a straightforward method to solve the problem. We consider a stiff polymer that has a persistent length $L_p$ much larger than the contour length $L$. We express the polymer configuration using three types of variables: the position vector of the center of mass $R_c$, the unit vector $n$ along the main axis, and the normal coordinates $u_p$ for bending. Solving the Smoluchowski equation for the distribution function of these variables, we calculate the equilibrium time correlation function $ \\langle P(t)\\cdot P(0) \\rangle$ of the end-to-end vector $P$ and the complex modulus $G^*(\\omega)$ of dilute solution. They include the bending effect to the first order in $\\theta \\equiv L/L_p$ and reduce to the exact results for the rigid rod in the limit of $\\theta \\to 0$. The rotational diffusion coefficient increases slightly by the semi-flexibility because the equilibrium length of the semi-flexible polymer is smaller than that of the rigid rod with the same contour length. The storage modulus shows the same asymptotic dependence $G'(\\omega) \\sim \\omega^{3/4}$ predicted by Shankar, Pasquali, and Morse [J. Rheol. 2002, 46, 1111--1154]. The high-frequency viscosity is predicted to be dependent on the thickness of the semi-flexible polymers.","sentences":["Brownian motion and viscoelasticity of semi-flexible polymers is a subject that has been studied for many years.","Still, rigorous analysis has been hindered due to the difficulty in handling the constraint that polymer chains cannot be stretched along the contour.","Here, we show a straightforward method to solve the problem.","We consider a stiff polymer that has a persistent length $L_p$ much larger than the contour length $","L$. We express the polymer configuration using three types of variables: the position vector of the center of mass $R_c$, the unit vector $n$ along the main axis, and the normal coordinates $u_p$ for bending.","Solving the Smoluchowski equation for the distribution function of these variables, we calculate the equilibrium time correlation function $ \\langle P(t)\\cdot P(0) \\rangle$ of the end-to-end vector $P$ and the complex modulus $G^*(\\omega)$ of dilute solution.","They include the bending effect to the first order in $\\theta \\equiv L/L_p$ and reduce to the exact results for the rigid rod in the limit of $\\theta \\to 0$.","The rotational diffusion coefficient increases slightly by the semi-flexibility because the equilibrium length of the semi-flexible polymer is smaller than that of the rigid rod with the same contour length.","The storage modulus shows the same asymptotic dependence $G'(\\omega) \\sim \\omega^{3/4}$ predicted by Shankar, Pasquali, and Morse [J. Rheol. 2002, 46, 1111--1154].","The high-frequency viscosity is predicted to be dependent on the thickness of the semi-flexible polymers."],"url":"http://arxiv.org/abs/2403.05173v1","category":"cond-mat.soft"}
{"created":"2024-03-08 09:13:27","title":"Quantum duality principal and quantum symmetric pairs","abstract":"The quantum duality principal (QDP) by Drinfeld predicts a connection between the quantized universial enveloping algebras and the quantized coordinate algebras, where the underlying classical objects are related by the duality in Poisson geometry. The current paper gives an explicit formulization of the QDP for quantum symmetric pairs.   Let $\\mathfrak{g}$ be a complex semi-simple Lie algebra, equipped with the standard Lie bialgebra structure. Let $\\theta$ be a Lie algebra involution on $\\mathfrak{g}$ and denote by $\\mathfrak{k}=\\mathfrak{g}^\\theta$ the fixed point subalgebra. The quantum symmetric pair $(\\mathrm{U},\\mathrm{U}^\\imath)$ is originally defined to be a quantization of the symmetric pair of the universial enveloping algebras $(U(\\mathfrak{g}),U(\\mathfrak{k}))$. In this paper, we show that an explicit specialisation of $(\\mathrm{U},\\mathrm{U}^\\imath)$ gives rise to the pair of the coordinate algebras $(\\mathcal{O}(G^*),\\mathcal{O}(K^\\perp\\backslash G^*))$, where $G^*$ is the dual Poisson-Lie group with the Lie algebra $\\mathfrak{g}^*$, and $K^\\perp\\backslash G^*$ is a $G^*$-Poisson homogeneous space. Here $K^\\perp$ is the closed subgroup of $G^*$associated to the complementary dual of $\\mathfrak{k}$. Therefore $(\\mathrm{U},\\mathrm{U}^\\imath)$ can be viewed as a pair of quantized coordinate algebras. This generalises the well-known fact that the quantum group $\\mathrm{U}$ provides a quantization of the coordinate algebra $\\mathcal{O}(G^*)$.","sentences":["The quantum duality principal (QDP) by Drinfeld predicts a connection between the quantized universial enveloping algebras and the quantized coordinate algebras, where the underlying classical objects are related by the duality in Poisson geometry.","The current paper gives an explicit formulization of the QDP for quantum symmetric pairs.   ","Let $\\mathfrak{g}$ be a complex semi-simple Lie algebra, equipped with the standard Lie bialgebra structure.","Let $\\theta$ be a Lie algebra involution on $\\mathfrak{g}$ and denote by $\\mathfrak{k}=\\mathfrak{g}^\\theta$ the fixed point subalgebra.","The quantum symmetric pair $(\\mathrm{U},\\mathrm{U}^\\imath)$ is originally defined to be a quantization of the symmetric pair of the universial enveloping algebras $(U(\\mathfrak{g}),U(\\mathfrak{k}))$.","In this paper, we show that an explicit specialisation of $(\\mathrm{U},\\mathrm{U}^\\imath)$ gives rise to the pair of the coordinate algebras $(\\mathcal{O}(G^*),\\mathcal{O}(K^\\perp\\backslash G^*))$, where $G^*$ is the dual Poisson-Lie group with the Lie algebra $\\mathfrak{g}^*$, and $K^\\perp\\backslash G^*$ is a $G^*$-Poisson homogeneous space.","Here $K^\\perp$ is the closed subgroup of $G^*$associated to the complementary dual of $\\mathfrak{k}$. Therefore $(\\mathrm{U},\\mathrm{U}^\\imath)$ can be viewed as a pair of quantized coordinate algebras.","This generalises the well-known fact that the quantum group $\\mathrm{U}$ provides a quantization of the coordinate algebra $\\mathcal{O}(G^*)$."],"url":"http://arxiv.org/abs/2403.05167v1","category":"math.QA"}
{"created":"2024-03-08 09:11:07","title":"Hiding images in quantum correlations","abstract":"Photon-pair correlations in spontaneous parametric down conversion are ubiquitous in quantum photonics. The ability to engineer their properties for optimising a specific task is essential, but often challenging in practice. We demonstrate the shaping of spatial correlations between entangled photons in the form of arbitrary amplitude and phase objects. By doing this, we encode image information within the pair correlations, making it undetectable by conventional intensity measurements. It enables the transmission of complex, high-dimensional information using quantum correlations of photons, which can be useful for developing quantum communication and imaging protocols.","sentences":["Photon-pair correlations in spontaneous parametric down conversion are ubiquitous in quantum photonics.","The ability to engineer their properties for optimising a specific task is essential, but often challenging in practice.","We demonstrate the shaping of spatial correlations between entangled photons in the form of arbitrary amplitude and phase objects.","By doing this, we encode image information within the pair correlations, making it undetectable by conventional intensity measurements.","It enables the transmission of complex, high-dimensional information using quantum correlations of photons, which can be useful for developing quantum communication and imaging protocols."],"url":"http://arxiv.org/abs/2403.05166v1","category":"quant-ph"}
{"created":"2024-03-08 09:02:13","title":"MamMIL: Multiple Instance Learning for Whole Slide Images with State Space Models","abstract":"Recently, pathological diagnosis, the gold standard for cancer diagnosis, has achieved superior performance by combining the Transformer with the multiple instance learning (MIL) framework using whole slide images (WSIs). However, the giga-pixel nature of WSIs poses a great challenge for the quadratic-complexity self-attention mechanism in Transformer to be applied in MIL. Existing studies usually use linear attention to improve computing efficiency but inevitably bring performance bottlenecks. To tackle this challenge, we propose a MamMIL framework for WSI classification by cooperating the selective structured state space model (i.e., Mamba) with MIL for the first time, enabling the modeling of instance dependencies while maintaining linear complexity. Specifically, to solve the problem that Mamba can only conduct unidirectional one-dimensional (1D) sequence modeling, we innovatively introduce a bidirectional state space model and a 2D context-aware block to enable MamMIL to learn the bidirectional instance dependencies with 2D spatial relationships. Experiments on two datasets show that MamMIL can achieve advanced classification performance with smaller memory footprints than the state-of-the-art MIL frameworks based on the Transformer. The code will be open-sourced if accepted.","sentences":["Recently, pathological diagnosis, the gold standard for cancer diagnosis, has achieved superior performance by combining the Transformer with the multiple instance learning (MIL) framework using whole slide images (WSIs).","However, the giga-pixel nature of WSIs poses a great challenge for the quadratic-complexity self-attention mechanism in Transformer to be applied in MIL.","Existing studies usually use linear attention to improve computing efficiency but inevitably bring performance bottlenecks.","To tackle this challenge, we propose a MamMIL framework for WSI classification by cooperating the selective structured state space model (i.e., Mamba) with MIL for the first time, enabling the modeling of instance dependencies while maintaining linear complexity.","Specifically, to solve the problem that Mamba can only conduct unidirectional one-dimensional (1D) sequence modeling, we innovatively introduce a bidirectional state space model and a 2D context-aware block to enable MamMIL to learn the bidirectional instance dependencies with 2D spatial relationships.","Experiments on two datasets show that MamMIL can achieve advanced classification performance with smaller memory footprints than the state-of-the-art MIL frameworks based on the Transformer.","The code will be open-sourced if accepted."],"url":"http://arxiv.org/abs/2403.05160v1","category":"cs.CV"}
{"created":"2024-03-08 08:48:01","title":"Quantum embedding for molecules using auxiliary particles -- The ghost Gutzwiller Ansatz","abstract":"Strong/static electronic correlation mediates the emergence of remarkable phases of matter, and underlies the exceptional reactivity properties in transition metal-based catalysts. Modeling strongly correlated molecules and solids calls for multi-reference Ans\\\"atze, which explicitly capture the competition of energy scales characteristic of such systems. With the efficient computational screening of correlated solids in mind, the ghost Gutzwiller (gGut) Ansatz has been recently developed. This is a variational Ansatz which can be formulated as a self-consistent embedding approach, describing the system within a non-interacting, quasiparticle model, yet providing with accurate spectra in both low and high energy regimes. Crucially, small fragments of the system are identified as responsible for the strong correlation, and are therefore enhanced by adding a set of auxiliary orbitals, the ghosts. These capture many-body correlations through one-body fluctuations and subsequent out-projection when computing physical observables. gGut has been shown to accurately describe multi-orbital lattice models at modest computational cost. In this work, we extend the gGut framework to strongly correlated molecules. To adapt the gGut Ansatz for molecular calculations, we address the fact that, unlike in the lattice model previously considered, electronic interactions in molecules are not local. Hence, we explore a hierarchy of approximations of increasing accuracy capturing interactions between fragments and environment, and within the environment, and discuss how these affect the embedding description of correlations in the whole molecule. We will compare the accuracy of the gGut model with established methods to capture strong correlation within active space formulations, and assess the realistic use of this novel approximation to the theoretical description of correlated molecular clusters.","sentences":["Strong/static electronic correlation mediates the emergence of remarkable phases of matter, and underlies the exceptional reactivity properties in transition metal-based catalysts.","Modeling strongly correlated molecules and solids calls for multi-reference Ans\\\"atze, which explicitly capture the competition of energy scales characteristic of such systems.","With the efficient computational screening of correlated solids in mind, the ghost Gutzwiller (gGut) Ansatz has been recently developed.","This is a variational Ansatz which can be formulated as a self-consistent embedding approach, describing the system within a non-interacting, quasiparticle model, yet providing with accurate spectra in both low and high energy regimes.","Crucially, small fragments of the system are identified as responsible for the strong correlation, and are therefore enhanced by adding a set of auxiliary orbitals, the ghosts.","These capture many-body correlations through one-body fluctuations and subsequent out-projection when computing physical observables.","gGut has been shown to accurately describe multi-orbital lattice models at modest computational cost.","In this work, we extend the gGut framework to strongly correlated molecules.","To adapt the gGut Ansatz for molecular calculations, we address the fact that, unlike in the lattice model previously considered, electronic interactions in molecules are not local.","Hence, we explore a hierarchy of approximations of increasing accuracy capturing interactions between fragments and environment, and within the environment, and discuss how these affect the embedding description of correlations in the whole molecule.","We will compare the accuracy of the gGut model with established methods to capture strong correlation within active space formulations, and assess the realistic use of this novel approximation to the theoretical description of correlated molecular clusters."],"url":"http://arxiv.org/abs/2403.05157v1","category":"physics.chem-ph"}
{"created":"2024-03-08 08:32:28","title":"Crossing number of curves on surfaces","abstract":"We consider systems of simple closed curves on surfaces and their total number of intersection points, their so-called crossing number. For a fixed number of curves, we aim to minimise the crossing number. We determine the minimal crossing number of up to 12 curves on a surface of genus 2 and prove that minimising systems are unique up to homeomorphisms of the surface and isotopies of curves.","sentences":["We consider systems of simple closed curves on surfaces and their total number of intersection points, their so-called crossing number.","For a fixed number of curves, we aim to minimise the crossing number.","We determine the minimal crossing number of up to 12 curves on a surface of genus 2 and prove that minimising systems are unique up to homeomorphisms of the surface and isotopies of curves."],"url":"http://arxiv.org/abs/2403.05148v1","category":"math.GT"}
{"created":"2024-03-08 08:27:43","title":"Multirate Time-Integration based on Dynamic ODE Partitioning through Adaptively Refined Meshes for Compressible Fluid Dynamics","abstract":"In this paper, we apply the Paired-Explicit Runge-Kutta (P-ERK) schemes by Vermeire et. al. (2019, 2022) to dynamically partitioned systems arising from adaptive mesh refinement. The P-ERK schemes enable multirate time-integration with no changes in the spatial discretization methodology, making them readily implementable in existing codes that employ a method-of-lines approach.   We show that speedup compared to a range of state of the art Runge-Kutta methods can be realized, despite additional overhead due to the dynamic re-assignment of flagging variables and restricting nonlinear stability properties. The effectiveness of the approach is demonstrated for a range of simulation setups for viscous and inviscid convection-dominated compressible flows for which we provide a reproducibility repository.   In addition, we perform a thorough investigation of the nonlinear stability properties of the Paired-Explicit Runge-Kutta schemes regarding limitations due to the violation of monotonicity properties of the underlying spatial discretization. Furthermore, we present a novel approach for estimating the relevant eigenvalues of large Jacobians required for the optimization of stability polynomials.","sentences":["In this paper, we apply the Paired-Explicit Runge-Kutta (P-ERK) schemes by Vermeire et. al. (2019, 2022) to dynamically partitioned systems arising from adaptive mesh refinement.","The P-ERK schemes enable multirate time-integration with no changes in the spatial discretization methodology, making them readily implementable in existing codes that employ a method-of-lines approach.   ","We show that speedup compared to a range of state of the art Runge-Kutta methods can be realized, despite additional overhead due to the dynamic re-assignment of flagging variables and restricting nonlinear stability properties.","The effectiveness of the approach is demonstrated for a range of simulation setups for viscous and inviscid convection-dominated compressible flows for which we provide a reproducibility repository.   ","In addition, we perform a thorough investigation of the nonlinear stability properties of the Paired-Explicit Runge-Kutta schemes regarding limitations due to the violation of monotonicity properties of the underlying spatial discretization.","Furthermore, we present a novel approach for estimating the relevant eigenvalues of large Jacobians required for the optimization of stability polynomials."],"url":"http://arxiv.org/abs/2403.05144v1","category":"math.NA"}
{"created":"2024-03-08 18:18:00","title":"Nodal finite element approximation of peridynamics","abstract":"This work considers the nodal finite element approximation of peridynamics, in which the nodal displacements satisfy the peridynamics equation at each mesh node. For the nonlinear bond-based peridynamics model, it is shown that, under the suitable assumptions on an exact solution, the discretized solution associated with the central-in-time and nodal finite element discretization converges to the exact solution in $L^2$ norm at the rate $C_1 \\Delta t + C_2 h^2/\\epsilon^2$. Here, $\\Delta t$, $h$, and $\\epsilon$ are time step size, mesh size, and the size of the horizon or nonlocal length scale, respectively. Constants $C_1$ and $C_2$ are independent of $h$ and $\\Delta t$ and depend on the norms of the exact solution. Several numerical examples involving pre-crack, void, and notch are considered, and the efficacy of the proposed nodal finite element discretization is analyzed.","sentences":["This work considers the nodal finite element approximation of peridynamics, in which the nodal displacements satisfy the peridynamics equation at each mesh node.","For the nonlinear bond-based peridynamics model, it is shown that, under the suitable assumptions on an exact solution, the discretized solution associated with the central-in-time and nodal finite element discretization converges to the exact solution in $L^2$ norm at the rate $C_1 \\Delta t + C_2 h^2/\\epsilon^2$. Here, $\\Delta t$, $h$, and $\\epsilon$ are time step size, mesh size, and the size of the horizon or nonlocal length scale, respectively.","Constants $C_1$ and $C_2$ are independent of $h$ and $\\Delta t$ and depend on the norms of the exact solution.","Several numerical examples involving pre-crack, void, and notch are considered, and the efficacy of the proposed nodal finite element discretization is analyzed."],"url":"http://arxiv.org/abs/2403.05501v1","category":"math.NA"}
{"created":"2024-03-08 17:51:30","title":"Bayesian mass mapping with weak lensing data using KARMMA -- validation with simulations and application to Dark Energy Survey Year 3 data","abstract":"We update the field-level inference code KARMMA to enable tomographic forward-modelling of shear maps. Our code assumes a lognormal prior on the convergence field, and properly accounts for the cross-covariance in the lensing signal across tomographic source bins. We use mock weak lensing data from N-body simulations to validate our mass-mapping forward model by comparing our posterior maps to the input convergence fields. We find that KARMMA produces more accurate reconstructions than traditional mass-mapping algorithms. More-over, the KARMMA posteriors reproduce all statistical properties of the input density field we tested -- one- and two-point functions, and the peak and void number counts -- with $\\lesssim~10\\%$ accuracy. Our posteriors exhibit a small bias that increases with decreasing source redshift, but these biases are small compared to the statistical uncertainties of current (DES) cosmic shear surveys. Finally, we apply KARMMA to Dark Energy Survey Year 3 (DES-Y3) weak lensing data, and verify that the two point shear correlation function $\\xi_+$ is well fit by the correlation function of the reconstructed convergence field. This is a non-trivial test that traditional mass mapping algorithms fail. The code is publicly available at https://github.com/Supranta/KaRMMa.git. KARMMA DES-Y3 mass maps are publicly available at https://zenodo.org/records/10672062.","sentences":["We update the field-level inference code KARMMA to enable tomographic forward-modelling of shear maps.","Our code assumes a lognormal prior on the convergence field, and properly accounts for the cross-covariance in the lensing signal across tomographic source bins.","We use mock weak lensing data from N-body simulations to validate our mass-mapping forward model by comparing our posterior maps to the input convergence fields.","We find that KARMMA produces more accurate reconstructions than traditional mass-mapping algorithms.","More-over, the KARMMA posteriors reproduce all statistical properties of the input density field we tested -- one- and two-point functions, and the peak and void number counts -- with $\\lesssim~10\\%$ accuracy.","Our posteriors exhibit a small bias that increases with decreasing source redshift, but these biases are small compared to the statistical uncertainties of current (DES) cosmic shear surveys.","Finally, we apply KARMMA to Dark Energy Survey Year 3 (DES-Y3) weak lensing data, and verify that the two point shear correlation function $\\xi_+$ is well fit by the correlation function of the reconstructed convergence field.","This is a non-trivial test that traditional mass mapping algorithms fail.","The code is publicly available at https://github.com/Supranta/KaRMMa.git.","KARMMA DES-Y3 mass maps are publicly available at https://zenodo.org/records/10672062."],"url":"http://arxiv.org/abs/2403.05484v1","category":"astro-ph.CO"}
{"created":"2024-03-08 16:55:57","title":"On the structure of Completely Reducible States","abstract":"The complete reducibility property for bipartite states reduced the separability problem to a proper subset of positive under partial transpose states and was used to prove several theorems inside and outside entanglement theory. So far only three types of bipartite states were proved to possess this property. In this work, we provide some procedures to create states with this property, we call these states by the name of completely reducible states. The convex combination of such states is the first procedure, showing that the set of completely reducible states is a convex cone. We also provide a complete description of the extreme rays of this set. Then we show that powers, roots and partial traces of completely reducible states result in states of the same type. Finally, we consider a shuffle of states that preserves this property. This shuffle allow us to construct states with the complete reducibility property avoiding the only three conditions known to date that imply this property.","sentences":["The complete reducibility property for bipartite states reduced the separability problem to a proper subset of positive under partial transpose states and was used to prove several theorems inside and outside entanglement theory.","So far only three types of bipartite states were proved to possess this property.","In this work, we provide some procedures to create states with this property, we call these states by the name of completely reducible states.","The convex combination of such states is the first procedure, showing that the set of completely reducible states is a convex cone.","We also provide a complete description of the extreme rays of this set.","Then we show that powers, roots and partial traces of completely reducible states result in states of the same type.","Finally, we consider a shuffle of states that preserves this property.","This shuffle allow us to construct states with the complete reducibility property avoiding the only three conditions known to date that imply this property."],"url":"http://arxiv.org/abs/2403.05449v1","category":"quant-ph"}
{"created":"2024-03-08 16:51:27","title":"Bayesian Hierarchical Probabilistic Forecasting of Intraday Electricity Prices","abstract":"We present a first study of Bayesian forecasting of electricity prices traded on the German continuous intraday market which fully incorporates parameter uncertainty. Our target variable is the IDFull price index, forecasts are given in terms of posterior predictive distributions. For validation we use the exceedingly volatile electricity prices of 2022, which have hardly been the subject of forecasting studies before. As a benchmark model, we use all available intraday transactions at the time of forecast creation to compute a current value for the IDFull. According to the weak-form efficiency hypothesis, it would not be possible to significantly improve this benchmark built from last price information. We do, however, observe statistically significant improvement in terms of both point measures and probability scores. Finally, we challenge the declared gold standard of using LASSO for feature selection in electricity price forecasting by presenting strong statistical evidence that Orthogonal Matching Pursuit (OMP) leads to better forecasting performance.","sentences":["We present a first study of Bayesian forecasting of electricity prices traded on the German continuous intraday market which fully incorporates parameter uncertainty.","Our target variable is the IDFull price index, forecasts are given in terms of posterior predictive distributions.","For validation we use the exceedingly volatile electricity prices of 2022, which have hardly been the subject of forecasting studies before.","As a benchmark model, we use all available intraday transactions at the time of forecast creation to compute a current value for the IDFull.","According to the weak-form efficiency hypothesis, it would not be possible to significantly improve this benchmark built from last price information.","We do, however, observe statistically significant improvement in terms of both point measures and probability scores.","Finally, we challenge the declared gold standard of using LASSO for feature selection in electricity price forecasting by presenting strong statistical evidence that Orthogonal Matching Pursuit (OMP) leads to better forecasting performance."],"url":"http://arxiv.org/abs/2403.05441v1","category":"stat.AP"}
{"created":"2024-03-08 16:18:04","title":"Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery","abstract":"Recent advances in unsupervised learning have demonstrated the ability of large vision models to achieve promising results on downstream tasks by pre-training on large amount of unlabelled data. Such pre-training techniques have also been explored recently in the remote sensing domain due to the availability of large amount of unlabelled data. Different from standard natural image datasets, remote sensing data is acquired from various sensor technologies and exhibit diverse range of scale variations as well as modalities. Existing satellite image pre-training methods either ignore the scale information present in the remote sensing imagery or restrict themselves to use only a single type of data modality. In this paper, we re-visit transformers pre-training and leverage multi-scale information that is effectively utilized with multiple modalities. Our proposed approach, named SatMAE++, performs multi-scale pre-training and utilizes convolution based upsampling blocks to reconstruct the image at higher scales making it extensible to include more scales. Compared to existing works, the proposed SatMAE++ with multi-scale pre-training is equally effective for both optical as well as multi-spectral imagery. Extensive experiments on six datasets reveal the merits of proposed contributions, leading to state-of-the-art performance on all datasets. SatMAE++ achieves mean average precision (mAP) gain of 2.5\\% for multi-label classification task on BigEarthNet dataset. Our code and pre-trained models are available at \\url{https://github.com/techmn/satmae_pp}.","sentences":["Recent advances in unsupervised learning have demonstrated the ability of large vision models to achieve promising results on downstream tasks by pre-training on large amount of unlabelled data.","Such pre-training techniques have also been explored recently in the remote sensing domain due to the availability of large amount of unlabelled data.","Different from standard natural image datasets, remote sensing data is acquired from various sensor technologies and exhibit diverse range of scale variations as well as modalities.","Existing satellite image pre-training methods either ignore the scale information present in the remote sensing imagery or restrict themselves to use only a single type of data modality.","In this paper, we re-visit transformers pre-training and leverage multi-scale information that is effectively utilized with multiple modalities.","Our proposed approach, named SatMAE++, performs multi-scale pre-training and utilizes convolution based upsampling blocks to reconstruct the image at higher scales making it extensible to include more scales.","Compared to existing works, the proposed SatMAE++ with multi-scale pre-training is equally effective for both optical as well as multi-spectral imagery.","Extensive experiments on six datasets reveal the merits of proposed contributions, leading to state-of-the-art performance on all datasets.","SatMAE++ achieves mean average precision (mAP) gain of 2.5\\% for multi-label classification task on BigEarthNet dataset.","Our code and pre-trained models are available at \\url{https://github.com/techmn/satmae_pp}."],"url":"http://arxiv.org/abs/2403.05419v1","category":"cs.CV"}
{"created":"2024-03-08 16:10:22","title":"Revised BDS Test","abstract":"In this paper, we focus on the BDS test, which is a nonparametric test of independence. Specifically, the null hypothesis $H_{0}$ of it is that $\\{u_{t}\\}$ is i.i.d. (independent and identically distributed), where $\\{u_{t}\\}$ is a random sequence. The BDS test is widely used in economics and finance, but it has a weakness that cannot be ignored: over-rejecting $H_{0}$ even if the length $T$ of $\\{u_{t}\\}$ is as large as $(100,2000)$. To improve the over-rejection problem of BDS test, considering that the correlation integral is the foundation of BDS test, we not only accurately describe the expectation of the correlation integral under $H_{0}$, but also calculate all terms of the asymptotic variance of the correlation integral whose order is $O(T^{-1})$ and $O(T^{-2})$, which is essential to improve the finite sample performance of BDS test. Based on this, we propose a revised BDS (RBDS) test and prove its asymptotic normality under $H_{0}$. The RBDS test not only inherits all the advantages of the BDS test, but also effectively corrects the over-rejection problem of the BDS test, which can be fully confirmed by the simulation results we presented. Moreover, based on the simulation results, we find that similar to BDS test, RBDS test would also be affected by the parameter estimations of the ARCH-type model, resulting in size distortion, but this phenomenon can be alleviated by the logarithmic transformation preprocessing of the estimate residuals of the model. Besides, through some actual datasets that have been demonstrated to fit well with ARCH-type models, we also compared the performance of BDS test and RBDS test in evaluating the goodness-of-fit of the model in empirical problem, and the results reflect that, under the same condition, the performance of the RBDS test is more encouraging.","sentences":["In this paper, we focus on the BDS test, which is a nonparametric test of independence.","Specifically, the null hypothesis $H_{0}$ of it is that $\\{u_{t}\\}$ is i.i.d.","(independent and identically distributed), where $\\{u_{t}\\}$ is a random sequence.","The BDS test is widely used in economics and finance, but it has a weakness that cannot be ignored: over-rejecting $H_{0}$ even if the length $T$ of $\\{u_{t}\\}$ is as large as $(100,2000)$. To improve the over-rejection problem of BDS test, considering that the correlation integral is the foundation of BDS test, we not only accurately describe the expectation of the correlation integral under $H_{0}$, but also calculate all terms of the asymptotic variance of the correlation integral whose order is $O(T^{-1})$ and $O(T^{-2})$, which is essential to improve the finite sample performance of BDS test.","Based on this, we propose a revised BDS (RBDS) test and prove its asymptotic normality under $H_{0}$. The RBDS test not only inherits all the advantages of the BDS test, but also effectively corrects the over-rejection problem of the BDS test, which can be fully confirmed by the simulation results we presented.","Moreover, based on the simulation results, we find that similar to BDS test, RBDS test would also be affected by the parameter estimations of the ARCH-type model, resulting in size distortion, but this phenomenon can be alleviated by the logarithmic transformation preprocessing of the estimate residuals of the model.","Besides, through some actual datasets that have been demonstrated to fit well with ARCH-type models, we also compared the performance of BDS test and RBDS test in evaluating the goodness-of-fit of the model in empirical problem, and the results reflect that, under the same condition, the performance of the RBDS test is more encouraging."],"url":"http://arxiv.org/abs/2403.05411v1","category":"math.ST"}
{"created":"2024-03-08 16:06:54","title":"FedFMS: Exploring Federated Foundation Models for Medical Image Segmentation","abstract":"Medical image segmentation is crucial for clinical diagnosis. The Segmentation Anything Model (SAM) serves as a powerful foundation model for visual segmentation and can be adapted for medical image segmentation. However, medical imaging data typically contain privacy-sensitive information, making it challenging to train foundation models with centralized storage and sharing. To date, there are few foundation models tailored for medical image deployment within the federated learning framework, and the segmentation performance, as well as the efficiency of communication and training, remain unexplored. In response to these issues, we developed Federated Foundation models for Medical image Segmentation (FedFMS), which includes the Federated SAM (FedSAM) and a communication and training-efficient Federated SAM with Medical SAM Adapter (FedMSA). Comprehensive experiments on diverse datasets are conducted to investigate the performance disparities between centralized training and federated learning across various configurations of FedFMS. The experiments revealed that FedFMS could achieve performance comparable to models trained via centralized training methods while maintaining privacy. Furthermore, FedMSA demonstrated the potential to enhance communication and training efficiency. Our model implementation codes are available at https://github.com/LIU-YUXI/FedFMS.","sentences":["Medical image segmentation is crucial for clinical diagnosis.","The Segmentation Anything Model (SAM) serves as a powerful foundation model for visual segmentation and can be adapted for medical image segmentation.","However, medical imaging data typically contain privacy-sensitive information, making it challenging to train foundation models with centralized storage and sharing.","To date, there are few foundation models tailored for medical image deployment within the federated learning framework, and the segmentation performance, as well as the efficiency of communication and training, remain unexplored.","In response to these issues, we developed Federated Foundation models for Medical image Segmentation (FedFMS), which includes the Federated SAM (FedSAM) and a communication and training-efficient Federated SAM with Medical SAM Adapter (FedMSA).","Comprehensive experiments on diverse datasets are conducted to investigate the performance disparities between centralized training and federated learning across various configurations of FedFMS.","The experiments revealed that FedFMS could achieve performance comparable to models trained via centralized training methods while maintaining privacy.","Furthermore, FedMSA demonstrated the potential to enhance communication and training efficiency.","Our model implementation codes are available at https://github.com/LIU-YUXI/FedFMS."],"url":"http://arxiv.org/abs/2403.05408v1","category":"eess.IV"}
{"created":"2024-03-08 15:53:03","title":"Planning and Inverse Kinematics of Hyper-Redundant Manipulators with VO-FABRIK","abstract":"Hyper-redundant Robotic Manipulators (HRMs) offer great dexterity and flexibility of operation, but solving Inverse Kinematics (IK) is challenging. In this work, we introduce VO-FABRIK, an algorithm combining Forward and Backward Reaching Inverse Kinematics (FABRIK) for repeatable deterministic IK computation, and an approach inspired from velocity obstacles to perform path planning under collision and joint limits constraints. We show preliminary results on an industrial HRM with 19 actuated joints. Our algorithm achieves good performance where a state-of-the-art IK solver fails.","sentences":["Hyper-redundant Robotic Manipulators (HRMs) offer great dexterity and flexibility of operation, but solving Inverse Kinematics (IK) is challenging.","In this work, we introduce VO-FABRIK, an algorithm combining Forward and Backward Reaching Inverse Kinematics (FABRIK) for repeatable deterministic IK computation, and an approach inspired from velocity obstacles to perform path planning under collision and joint limits constraints.","We show preliminary results on an industrial HRM with 19 actuated joints.","Our algorithm achieves good performance where a state-of-the-art IK solver fails."],"url":"http://arxiv.org/abs/2403.05399v1","category":"cs.RO"}
{"created":"2024-03-08 15:52:12","title":"Survival probability of structures under fatigue: a data-based approach","abstract":"This article addresses the probabilistic nature of fatigue life in structures subjected to cyclic loading with variable amplitude. Drawing on the formalisation of Miner's cumulative damage rule that we introduced in the recent article [Cartiaux, Ehrlacher, Legoll, Libal and Reygner, Prob. Eng. Mech. 2023], we apply our methodology to estimate the survival probability of an industrial structure using experimental data. The study considers both the randomness in the initial state of the structure and in the amplitude of loading cycles. The results indicate that the variability of loading cycles can be captured through the concept of deterministic equivalent damage, providing a computationally efficient method for assessing the fatigue life of the structure. Furthermore, the article highlights that the usual combination of Miner's rule and of the weakest link principle systematically overestimates the structure's fatigue life. On the case study that we consider, this overestimation reaches a multiplicative factor of more than two. We then describe how the probabilistic framework that we have introduced offers a remedy to this overestimation.","sentences":["This article addresses the probabilistic nature of fatigue life in structures subjected to cyclic loading with variable amplitude.","Drawing on the formalisation of Miner's cumulative damage rule that we introduced in the recent article [Cartiaux, Ehrlacher, Legoll, Libal and Reygner, Prob.","Eng.","Mech.","2023], we apply our methodology to estimate the survival probability of an industrial structure using experimental data.","The study considers both the randomness in the initial state of the structure and in the amplitude of loading cycles.","The results indicate that the variability of loading cycles can be captured through the concept of deterministic equivalent damage, providing a computationally efficient method for assessing the fatigue life of the structure.","Furthermore, the article highlights that the usual combination of Miner's rule and of the weakest link principle systematically overestimates the structure's fatigue life.","On the case study that we consider, this overestimation reaches a multiplicative factor of more than two.","We then describe how the probabilistic framework that we have introduced offers a remedy to this overestimation."],"url":"http://arxiv.org/abs/2403.05397v1","category":"cs.CE"}
{"created":"2024-03-08 15:32:12","title":"Partition of Sparse Graphs into Two Forests with Bounded Degree","abstract":"Borodin and Kostochka proved that for $d_2 \\geq 2d_1+2$ and a graph $G$ where every subgraph $H$ satisfies $$ e(H) < \\left(2 - \\frac{d_2+2}{(d_1+2)(d_2+1)}\\right)n(H) + \\frac{1}{d_2+1} $$ has a vertex partition $V(G) = V_1 \\cup V_2$ such that $G[V_i]$ has maximum degree at most $d_i$ for each $i$. We show that under the same conditions we can additionally conclude that each $G[V_i]$ is a forest.","sentences":["Borodin and Kostochka proved that for $d_2 \\geq 2d_1+2$ and a graph $G$ where every subgraph $H$ satisfies $$ e(H) < \\left(2 - \\frac{d_2+2}{(d_1+2)(d_2+1)}\\right)n(H) + \\frac{1}{d_2+1} $$ has a vertex partition $V(G) = V_1 \\cup V_2$ such that $G[V_i]$ has maximum degree at most $d_i$ for each $i$. We show that under the same conditions we can additionally conclude that each $G[V_i]$ is a forest."],"url":"http://arxiv.org/abs/2403.05387v1","category":"math.CO"}
{"created":"2024-03-08 15:06:45","title":"Regularized Principal Spline Functions to Mitigate Spatial Confounding","abstract":"This paper proposes a new approach to address the problem of unmeasured confounding in spatial designs. Spatial confounding occurs when some confounding variables are unobserved and not included in the model, leading to distorted inferential results about the effect of an exposure on an outcome. We show the relationship existing between the confounding bias of a non-spatial model and that of a semi-parametric model that includes a basis matrix to represent the unmeasured confounder conditional on the exposure. This relationship holds for any basis expansion, however it is shown that using the semi-parametric approach guarantees a reduction in the confounding bias only under certain circumstances, which are related to the spatial structures of the exposure and the unmeasured confounder, the type of basis expansion utilized, and the regularization mechanism. To adjust for spatial confounding, and therefore try to recover the effect of interest, we propose a Bayesian semi-parametric regression model, where an expansion matrix of principal spline basis functions is used to approximate the unobserved factor, and spike-and-slab priors are imposed on the respective expansion coefficients in order to select the most important bases. From the results of an extensive simulation study, we conclude that our proposal is able to reduce the confounding bias with respect to the non-spatial model, and it also seems more robust to bias amplification than competing approaches.","sentences":["This paper proposes a new approach to address the problem of unmeasured confounding in spatial designs.","Spatial confounding occurs when some confounding variables are unobserved and not included in the model, leading to distorted inferential results about the effect of an exposure on an outcome.","We show the relationship existing between the confounding bias of a non-spatial model and that of a semi-parametric model that includes a basis matrix to represent the unmeasured confounder conditional on the exposure.","This relationship holds for any basis expansion, however it is shown that using the semi-parametric approach guarantees a reduction in the confounding bias only under certain circumstances, which are related to the spatial structures of the exposure and the unmeasured confounder, the type of basis expansion utilized, and the regularization mechanism.","To adjust for spatial confounding, and therefore try to recover the effect of interest, we propose a Bayesian semi-parametric regression model, where an expansion matrix of principal spline basis functions is used to approximate the unobserved factor, and spike-and-slab priors are imposed on the respective expansion coefficients in order to select the most important bases.","From the results of an extensive simulation study, we conclude that our proposal is able to reduce the confounding bias with respect to the non-spatial model, and it also seems more robust to bias amplification than competing approaches."],"url":"http://arxiv.org/abs/2403.05373v1","category":"stat.ME"}
{"created":"2024-03-08 14:39:46","title":"Modeling of progressive high-cycle fatigue in composite laminates accounting for local stress ratios","abstract":"A numerical framework for simulating progressive failure under high-cycle fatigue loading is validated against experiments of composite quasi-isotropic open-hole laminates. Transverse matrix cracking and delamination are modeled with a mixed-mode fatigue cohesive zone model, covering crack initiation and propagation. Furthermore, XFEM is used for simulating transverse matrix cracks and splits at arbitrary locations. An adaptive cycle jump approach is employed for efficiently simulating high-cycle fatigue while accounting for local stress ratio variations in the presence of thermal residual stresses. The cycle jump scheme is integrated in the XFEM framework, where the local stress ratio is used to determine the insertion of cracks and to propagate fatigue damage. The fatigue cohesive zone model is based on S-N curves and requires static material properties and only a few fatigue parameters, calibrated on simple fracture testing specimens. The simulations demonstrate a good correspondence with experiments in terms of fatigue life and damage evolution.","sentences":["A numerical framework for simulating progressive failure under high-cycle fatigue loading is validated against experiments of composite quasi-isotropic open-hole laminates.","Transverse matrix cracking and delamination are modeled with a mixed-mode fatigue cohesive zone model, covering crack initiation and propagation.","Furthermore, XFEM is used for simulating transverse matrix cracks and splits at arbitrary locations.","An adaptive cycle jump approach is employed for efficiently simulating high-cycle fatigue while accounting for local stress ratio variations in the presence of thermal residual stresses.","The cycle jump scheme is integrated in the XFEM framework, where the local stress ratio is used to determine the insertion of cracks and to propagate fatigue damage.","The fatigue cohesive zone model is based on S-N curves and requires static material properties and only a few fatigue parameters, calibrated on simple fracture testing specimens.","The simulations demonstrate a good correspondence with experiments in terms of fatigue life and damage evolution."],"url":"http://arxiv.org/abs/2403.05356v1","category":"cs.CE"}
{"created":"2024-03-08 14:18:16","title":"Theoretical estimation of stimulated and spontaneous Raman signals in Raman microscopy","abstract":"As a highly sensitive vibrational imaging method, stimulated Raman scattering (SRS) microscopy is finding various applications, while its theoretical treatment seems still under development. Here we present a theoretical estimation of spontaneous Raman signal and SRS signal from Raman scattering cross section, irrespective of the numerical aperture of the objective lens. We confirm a reasonable agreement between our theory with a recently proposed treatment based on the stimulated Raman scattering cross section. Furthermore, we point out that the acceleration factor of SRS can be interpreted as the number of Stokes photons in the vibrational coherence time.","sentences":["As a highly sensitive vibrational imaging method, stimulated Raman scattering (SRS) microscopy is finding various applications, while its theoretical treatment seems still under development.","Here we present a theoretical estimation of spontaneous Raman signal and SRS signal from Raman scattering cross section, irrespective of the numerical aperture of the objective lens.","We confirm a reasonable agreement between our theory with a recently proposed treatment based on the stimulated Raman scattering cross section.","Furthermore, we point out that the acceleration factor of SRS can be interpreted as the number of Stokes photons in the vibrational coherence time."],"url":"http://arxiv.org/abs/2403.05341v1","category":"physics.optics"}
{"created":"2024-03-08 14:07:44","title":"Consecutive Model Editing with Batch alongside HooK Layers","abstract":"As the typical retraining paradigm is unacceptably time- and resource-consuming, researchers are turning to model editing in order to seek an effective, consecutive, and batch-supportive way to edit the model behavior directly. Despite all these practical expectations, existing model editing methods fail to realize all of them. Furthermore, the memory demands for such succession-supportive model editing approaches tend to be prohibitive, frequently necessitating an external memory that grows incrementally over time. To cope with these challenges, we propose COMEBA-HK, a model editing method that is both consecutive and batch-supportive. COMEBA-HK is memory-friendly as it only needs a small amount of it to store several hook layers with updated weights. Experimental results demonstrate the superiority of our method over other batch-supportive model editing methods under both single-round and consecutive batch editing scenarios. Extensive analyses of COMEBA-HK have been conducted to verify the stability of our method over 1) the number of consecutive steps and 2) the number of editing instance.","sentences":["As the typical retraining paradigm is unacceptably time- and resource-consuming, researchers are turning to model editing in order to seek an effective, consecutive, and batch-supportive way to edit the model behavior directly.","Despite all these practical expectations, existing model editing methods fail to realize all of them.","Furthermore, the memory demands for such succession-supportive model editing approaches tend to be prohibitive, frequently necessitating an external memory that grows incrementally over time.","To cope with these challenges, we propose COMEBA-HK, a model editing method that is both consecutive and batch-supportive.","COMEBA-HK is memory-friendly as it only needs a small amount of it to store several hook layers with updated weights.","Experimental results demonstrate the superiority of our method over other batch-supportive model editing methods under both single-round and consecutive batch editing scenarios.","Extensive analyses of COMEBA-HK have been conducted to verify the stability of our method over 1) the number of consecutive steps and 2) the number of editing instance."],"url":"http://arxiv.org/abs/2403.05330v1","category":"cs.CL"}
{"created":"2024-03-08 13:54:39","title":"Quantum tomography of molecules using ultrafast electron diffraction","abstract":"We propose a quantum tomography (QT) approach to retrieve the temporally evolving reduced density matrix in elecotronic state basis, where the populations and coherence between ground state and excited state are reconstructed from the ultrafast electron diffraction signal. In order to showcase the capability of the proposed QT approach, we simulate the nuclear wavepacket dynamics and ultrafast electron diffraction of photoexcited pyrrole molecules using ab initio quantum chemical CASSCF method. From simulated time-resolved diffraction data, we retrieve the evolving density matrix in a crude diabatic representation basis and reveal the symmetry of the excited pyrrole wavepacket. Our QT approach opens the route to make quantum version of \"molecular movie\" that covers the electronic degree of freedom, and equips ultrafast electron diffraction with the power to reveal the coherence between electronic states, relaxation and dynamics of population transfer.","sentences":["We propose a quantum tomography (QT) approach to retrieve the temporally evolving reduced density matrix in elecotronic state basis, where the populations and coherence between ground state and excited state are reconstructed from the ultrafast electron diffraction signal.","In order to showcase the capability of the proposed QT approach, we simulate the nuclear wavepacket dynamics and ultrafast electron diffraction of photoexcited pyrrole molecules using ab initio quantum chemical CASSCF method.","From simulated time-resolved diffraction data, we retrieve the evolving density matrix in a crude diabatic representation basis and reveal the symmetry of the excited pyrrole wavepacket.","Our QT approach opens the route to make quantum version of \"molecular movie\" that covers the electronic degree of freedom, and equips ultrafast electron diffraction with the power to reveal the coherence between electronic states, relaxation and dynamics of population transfer."],"url":"http://arxiv.org/abs/2403.05320v1","category":"physics.chem-ph"}
{"created":"2024-03-08 13:16:17","title":"Foundational propositions of hesitant fuzzy soft $\u03b2$-covering approximation spaces","abstract":"Soft set theory serves as a mathematical framework for handling uncertain information, and hesitant fuzzy sets find extensive application in scenarios involving uncertainty and hesitation. Hesitant fuzzy sets exhibit diverse membership degrees, giving rise to various forms of inclusion relationships among them. This article introduces the notions of hesitant fuzzy soft $\\beta$-coverings and hesitant fuzzy soft $\\beta$-neighborhoods, which are formulated based on distinct forms of inclusion relationships among hesitancy fuzzy sets. Subsequently, several associated properties are investigated. Additionally, specific variations of hesitant fuzzy soft $\\beta$-coverings are introduced by incorporating hesitant fuzzy rough sets, followed by an exploration of properties pertaining to hesitant fuzzy soft $\\beta$-covering approximation spaces.","sentences":["Soft set theory serves as a mathematical framework for handling uncertain information, and hesitant fuzzy sets find extensive application in scenarios involving uncertainty and hesitation.","Hesitant fuzzy sets exhibit diverse membership degrees, giving rise to various forms of inclusion relationships among them.","This article introduces the notions of hesitant fuzzy soft $\\beta$-coverings and hesitant fuzzy soft $\\beta$-neighborhoods, which are formulated based on distinct forms of inclusion relationships among hesitancy fuzzy sets.","Subsequently, several associated properties are investigated.","Additionally, specific variations of hesitant fuzzy soft $\\beta$-coverings are introduced by incorporating hesitant fuzzy rough sets, followed by an exploration of properties pertaining to hesitant fuzzy soft $\\beta$-covering approximation spaces."],"url":"http://arxiv.org/abs/2403.05290v1","category":"cs.LG"}
{"created":"2024-03-08 13:00:52","title":"ContrastDiagnosis: Enhancing Interpretability in Lung Nodule Diagnosis Using Contrastive Learning","abstract":"With the ongoing development of deep learning, an increasing number of AI models have surpassed the performance levels of human clinical practitioners. However, the prevalence of AI diagnostic products in actual clinical practice remains significantly lower than desired. One crucial reason for this gap is the so-called `black box' nature of AI models. Clinicians' distrust of black box models has directly hindered the clinical deployment of AI products. To address this challenge, we propose ContrastDiagnosis, a straightforward yet effective interpretable diagnosis framework. This framework is designed to introduce inherent transparency and provide extensive post-hoc explainability for deep learning model, making them more suitable for clinical medical diagnosis. ContrastDiagnosis incorporates a contrastive learning mechanism to provide a case-based reasoning diagnostic rationale, enhancing the model's transparency and also offers post-hoc interpretability by highlighting similar areas. High diagnostic accuracy was achieved with AUC of 0.977 while maintain a high transparency and explainability.","sentences":["With the ongoing development of deep learning, an increasing number of AI models have surpassed the performance levels of human clinical practitioners.","However, the prevalence of AI diagnostic products in actual clinical practice remains significantly lower than desired.","One crucial reason for this gap is the so-called `black box' nature of AI models.","Clinicians' distrust of black box models has directly hindered the clinical deployment of AI products.","To address this challenge, we propose ContrastDiagnosis, a straightforward yet effective interpretable diagnosis framework.","This framework is designed to introduce inherent transparency and provide extensive post-hoc explainability for deep learning model, making them more suitable for clinical medical diagnosis.","ContrastDiagnosis incorporates a contrastive learning mechanism to provide a case-based reasoning diagnostic rationale, enhancing the model's transparency and also offers post-hoc interpretability by highlighting similar areas.","High diagnostic accuracy was achieved with AUC of 0.977 while maintain a high transparency and explainability."],"url":"http://arxiv.org/abs/2403.05280v1","category":"cs.CV"}
{"created":"2024-03-08 12:04:40","title":"The Simons Observatory: impact of bandpass, polarization angle and calibration uncertainties on small-scale power spectrum analysis","abstract":"We study the effects due to mismatches in passbands, polarization angles, and temperature and polarization calibrations in the context of the upcoming cosmic microwave background experiment Simons Observatory (SO). Using the SO multi-frequency likelihood, we estimate the bias and the degradation of constraining power in cosmological and astrophysical foreground parameters assuming different levels of knowledge of the instrumental effects. We find that incorrect but reasonable assumptions on the values of all the systematics examined here can have important effects in cosmological analyses, hence requiring marginalization approaches at likelihood level. When doing so, we find that the most relevant effect is due to bandpass shifts. When marginalizing over them, the posteriors of parameters describing astrophysical microwave foregrounds (such as radio point sources or dust) get degraded, while cosmological parameters constraints are not significantly affected. Marginalization over polarization angles with up to 0.25$^\\circ$ uncertainty causes an irrelevant bias $\\lesssim 0.05 \\sigma$ in all parameters. Marginalization over calibration factors in polarization broadens the constraints on the effective number of relativistic degrees of freedom $N_\\mathrm{eff}$ by a factor 1.2, interpreted here as a proxy parameter for non standard model physics targeted by high-resolution CMB measurements.","sentences":["We study the effects due to mismatches in passbands, polarization angles, and temperature and polarization calibrations in the context of the upcoming cosmic microwave background experiment Simons Observatory (SO).","Using the SO multi-frequency likelihood, we estimate the bias and the degradation of constraining power in cosmological and astrophysical foreground parameters assuming different levels of knowledge of the instrumental effects.","We find that incorrect but reasonable assumptions on the values of all the systematics examined here can have important effects in cosmological analyses, hence requiring marginalization approaches at likelihood level.","When doing so, we find that the most relevant effect is due to bandpass shifts.","When marginalizing over them, the posteriors of parameters describing astrophysical microwave foregrounds (such as radio point sources or dust) get degraded, while cosmological parameters constraints are not significantly affected.","Marginalization over polarization angles with up to 0.25$^\\circ$ uncertainty causes an irrelevant bias $\\lesssim 0.05 \\sigma$ in all parameters.","Marginalization over calibration factors in polarization broadens the constraints on the effective number of relativistic degrees of freedom $N_\\mathrm{eff}$ by a factor 1.2, interpreted here as a proxy parameter for non standard model physics targeted by high-resolution CMB measurements."],"url":"http://arxiv.org/abs/2403.05242v1","category":"astro-ph.CO"}
{"created":"2024-03-08 11:41:48","title":"Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance","abstract":"Motivated by the Parameter-Efficient Fine-Tuning (PEFT) in large language models, we propose LoRAT, a method that unveils the power of larger Vision Transformers (ViT) for tracking within laboratory-level resources. The essence of our work lies in adapting LoRA, a technique that fine-tunes a small subset of model parameters without adding inference latency, to the domain of visual tracking. However, unique challenges and potential domain gaps make this transfer not as easy as the first intuition. Firstly, a transformer-based tracker constructs unshared position embedding for template and search image. This poses a challenge for the transfer of LoRA, usually requiring consistency in the design when applied to the pre-trained backbone, to downstream tasks. Secondly, the inductive bias inherent in convolutional heads diminishes the effectiveness of parameter-efficient fine-tuning in tracking models. To overcome these limitations, we first decouple the position embeddings in transformer-based trackers into shared spatial ones and independent type ones. The shared embeddings, which describe the absolute coordinates of multi-resolution images (namely, the template and search images), are inherited from the pre-trained backbones. In contrast, the independent embeddings indicate the sources of each token and are learned from scratch. Furthermore, we design an anchor-free head solely based on a multilayer perceptron (MLP) to adapt PETR, enabling better performance with less computational overhead. With our design, 1) it becomes practical to train trackers with the ViT-g backbone on GPUs with only memory of 25.8GB (batch size of 16); 2) we reduce the training time of the L-224 variant from 35.0 to 10.8 GPU hours; 3) we improve the LaSOT SUC score from 0.703 to 0.743 with the L-224 variant; 4) we fast the inference speed of the L-224 variant from 52 to 119 FPS. Code and models will be released.","sentences":["Motivated by the Parameter-Efficient Fine-Tuning (PEFT) in large language models, we propose LoRAT, a method that unveils the power of larger Vision Transformers (ViT) for tracking within laboratory-level resources.","The essence of our work lies in adapting LoRA, a technique that fine-tunes a small subset of model parameters without adding inference latency, to the domain of visual tracking.","However, unique challenges and potential domain gaps make this transfer not as easy as the first intuition.","Firstly, a transformer-based tracker constructs unshared position embedding for template and search image.","This poses a challenge for the transfer of LoRA, usually requiring consistency in the design when applied to the pre-trained backbone, to downstream tasks.","Secondly, the inductive bias inherent in convolutional heads diminishes the effectiveness of parameter-efficient fine-tuning in tracking models.","To overcome these limitations, we first decouple the position embeddings in transformer-based trackers into shared spatial ones and independent type ones.","The shared embeddings, which describe the absolute coordinates of multi-resolution images (namely, the template and search images), are inherited from the pre-trained backbones.","In contrast, the independent embeddings indicate the sources of each token and are learned from scratch.","Furthermore, we design an anchor-free head solely based on a multilayer perceptron (MLP) to adapt PETR, enabling better performance with less computational overhead.","With our design, 1) it becomes practical to train trackers with the ViT-g backbone on GPUs with only memory of 25.8GB (batch size of 16); 2) we reduce the training time of the L-224 variant from 35.0 to 10.8 GPU hours; 3) we improve the LaSOT SUC score from 0.703 to 0.743 with the L-224 variant; 4) we fast the inference speed of the L-224 variant from 52 to 119 FPS.","Code and models will be released."],"url":"http://arxiv.org/abs/2403.05231v1","category":"cs.CV"}
{"created":"2024-03-08 11:20:48","title":"Investigating the shortcomings of the Flow Convergence Method for quantification of mitral regurgitation in a pulsatile in-vitro environment and with Computational Fluid Dynamics","abstract":"The flow convergence method includes calculation of the proximal isovelocity surface area (PISA) and is widely used to classify mitral regurgitation (MR) with echocardiography. It constitutes a primary decision factor for determination of treatment and should therefore be a robust quantification method. However, it is known for its tendency to underestimate MR and its dependence on user expertise. The present work systematically compares different pulsatile flow profiles arising from different regurgitation orifices using transesophageal chocardiographic probe and particle image velocimetry (PIV) as a reference in an in-vitro environment. It has been found that the inter-observer variability using echocardiography is small compared to the systematic underestimation of the regurgitation volume for large orifice areas (up to 52%) where a violation of the flow convergence method assumptions occurs. From a flow perspective, a starting vortex was found as a dominant flow pattern in the regurgant jet for all orifice shapes and sizes. A series of simplified computational fluid dynamics (CFD) simulations indicate that selecting a suboptimal aliasing velocity during echocardiography measurements might be a primary source of potential underestimation in MR characterization via the PISA-based method, reaching up to 40%. It has been noted in clinical observations that physicians often select an aliasing velocity higher than necessary for optimal estimation in diagnostic procedures.","sentences":["The flow convergence method includes calculation of the proximal isovelocity surface area (PISA) and is widely used to classify mitral regurgitation (MR) with echocardiography.","It constitutes a primary decision factor for determination of treatment and should therefore be a robust quantification method.","However, it is known for its tendency to underestimate MR and its dependence on user expertise.","The present work systematically compares different pulsatile flow profiles arising from different regurgitation orifices using transesophageal chocardiographic probe and particle image velocimetry (PIV) as a reference in an in-vitro environment.","It has been found that the inter-observer variability using echocardiography is small compared to the systematic underestimation of the regurgitation volume for large orifice areas (up to 52%) where a violation of the flow convergence method assumptions occurs.","From a flow perspective, a starting vortex was found as a dominant flow pattern in the regurgant jet for all orifice shapes and sizes.","A series of simplified computational fluid dynamics (CFD) simulations indicate that selecting a suboptimal aliasing velocity during echocardiography measurements might be a primary source of potential underestimation in MR characterization via the PISA-based method, reaching up to 40%.","It has been noted in clinical observations that physicians often select an aliasing velocity higher than necessary for optimal estimation in diagnostic procedures."],"url":"http://arxiv.org/abs/2403.05224v1","category":"physics.flu-dyn"}
{"created":"2024-03-08 11:17:44","title":"Matchings in multipartite hypergraphs","abstract":"A folklore result on matchings in graphs states that if $G$ is a bipartite graph whose vertex classes $A$ and $B$ each have size $n$, with $\\mathrm{deg}(u) \\geq a$ for every $u \\in A$ and $\\mathrm{deg}(v) \\geq b$ for every $v \\in B$, then $G$ admits a matching of size $\\min\\{n, a+b\\}$. In this paper we establish the analogous result for large $k$-partite $k$-uniform hypergraphs, answering a question of Han, Zang and Zhao, who previously demonstrated that this result holds under the additional condition that the minimum degrees into at least two of the vertex classes are large. A key part of our proof is a study of rainbow matchings under a combination of degree and multiplicity conditions, which may be of independent interest.","sentences":["A folklore result on matchings in graphs states that if $G$ is a bipartite graph whose vertex classes $A$ and $B$ each have size $n$, with $\\mathrm{deg}(u) \\geq","a$ for every $u \\in A$ and $\\mathrm{deg}(v)","\\geq b$ for every $v \\in B$, then $G$ admits a matching of size $\\min\\{n, a+b\\}$.","In this paper we establish the analogous result for large $k$-partite $k$-uniform hypergraphs, answering a question of Han, Zang and Zhao, who previously demonstrated that this result holds under the additional condition that the minimum degrees into at least two of the vertex classes are large.","A key part of our proof is a study of rainbow matchings under a combination of degree and multiplicity conditions, which may be of independent interest."],"url":"http://arxiv.org/abs/2403.05219v1","category":"math.CO"}
{"created":"2024-03-08 11:00:09","title":"SocialPET: Socially Informed Pattern Exploiting Training for Few-Shot Stance Detection in Social Media","abstract":"Stance detection, as the task of determining the viewpoint of a social media post towards a target as 'favor' or 'against', has been understudied in the challenging yet realistic scenario where there is limited labeled data for a certain target. Our work advances research in few-shot stance detection by introducing SocialPET, a socially informed approach to leveraging language models for the task. Our proposed approach builds on the Pattern Exploiting Training (PET) technique, which addresses classification tasks as cloze questions through the use of language models. To enhance the approach with social awareness, we exploit the social network structure surrounding social media posts. We prove the effectiveness of SocialPET on two stance datasets, Multi-target and P-Stance, outperforming competitive stance detection models as well as the base model, PET, where the labeled instances for the target under study is as few as 100. When we delve into the results, we observe that SocialPET is comparatively strong in identifying instances of the `against' class, where baseline models underperform.","sentences":["Stance detection, as the task of determining the viewpoint of a social media post towards a target as 'favor' or 'against', has been understudied in the challenging yet realistic scenario where there is limited labeled data for a certain target.","Our work advances research in few-shot stance detection by introducing SocialPET, a socially informed approach to leveraging language models for the task.","Our proposed approach builds on the Pattern Exploiting Training (PET) technique, which addresses classification tasks as cloze questions through the use of language models.","To enhance the approach with social awareness, we exploit the social network structure surrounding social media posts.","We prove the effectiveness of SocialPET on two stance datasets, Multi-target and P-Stance, outperforming competitive stance detection models as well as the base model, PET, where the labeled instances for the target under study is as few as 100.","When we delve into the results, we observe that SocialPET is comparatively strong in identifying instances of the `against' class, where baseline models underperform."],"url":"http://arxiv.org/abs/2403.05216v1","category":"cs.CL"}
{"created":"2024-03-08 10:59:56","title":"Proper motion of Cygnus loop shock filaments","abstract":"We determined shock speed in the Galactic supernova remnant Cygnus Loop, using proper motion of its optical filaments and the latest estimate for its distance. The proper motion was measured by comparing H$\\alpha$ images of the remnant observed in two epochs: 1993 (Kitt Peak National Observatory) and 2018/2019 (National Astronomical Observatory Rozhen and Astronomical station Vidojevica). We derived shock speed for 35 locations along different filaments, which is twice as much as in earlier studies of north-eastern part of Cygnus Loop. For the first time, we have measured shock speed of radiative filaments in this region. Three of the analyzed locations where we measured proper motion of filaments are radiative, based on their presence in [SII] images from the second epoch. The other filaments are non-radiative. The speed we obtained for the non-radiative filaments is in the range of 240-650 $\\mathrm{km\\ s^{-1}}$, with an estimate for the uncertainty of 70 $\\mathrm{km\\ s^{-1}}$. These values are mostly in agreement with previous studies. The radiative filaments have lower speed of 100-160$\\pm${70} $\\mathrm{km\\ s^{-1}}$, which is in agreement with the assumption that they are older in evolutionary terms. This clear distinction between the speed of the two types of filaments proves that the [SII] emission can be used for identifying radiative filaments in supernova remnants.","sentences":["We determined shock speed in the Galactic supernova remnant Cygnus Loop, using proper motion of its optical filaments and the latest estimate for its distance.","The proper motion was measured by comparing H$\\alpha$ images of the remnant observed in two epochs: 1993 (Kitt Peak National Observatory) and 2018/2019 (National Astronomical Observatory Rozhen and Astronomical station Vidojevica).","We derived shock speed for 35 locations along different filaments, which is twice as much as in earlier studies of north-eastern part of Cygnus Loop.","For the first time, we have measured shock speed of radiative filaments in this region.","Three of the analyzed locations where we measured proper motion of filaments are radiative, based on their presence in [SII] images from the second epoch.","The other filaments are non-radiative.","The speed we obtained for the non-radiative filaments is in the range of 240-650 $\\mathrm{km\\ s^{-1}}$, with an estimate for the uncertainty of 70 $\\mathrm{km\\ s^{-1}}$.","These values are mostly in agreement with previous studies.","The radiative filaments have lower speed of 100-160$\\pm${70} $\\mathrm{km\\ s^{-1}}$, which is in agreement with the assumption that they are older in evolutionary terms.","This clear distinction between the speed of the two types of filaments proves that the [SII] emission can be used for identifying radiative filaments in supernova remnants."],"url":"http://arxiv.org/abs/2403.05215v1","category":"astro-ph.HE"}
{"created":"2024-03-08 10:55:27","title":"Oppie Op-ed: Reflections on Christopher Nolan's Oppenheimer","abstract":"Right before Nolan's movie was released, I gave a talk on Oppenheimer, trying to anticipate what elements of Bird and Sherwin's biography on which the movie is based would make it into the movie. In this article, written for the October 2023 edition of the newsletter of the Forum on Science and Society of the APS, I elaborate on some of the elements I missed. I used this article as the basis for a post-movie version of my talk. I corrected some errors and made some additions to this article before submitting it right before Nolan's movie is expected to clean up at the Oscars (links to recordings of the pre- and post-movie versions of my talk are included).","sentences":["Right before Nolan's movie was released, I gave a talk on Oppenheimer, trying to anticipate what elements of Bird and Sherwin's biography on which the movie is based would make it into the movie.","In this article, written for the October 2023 edition of the newsletter of the Forum on Science and Society of the APS, I elaborate on some of the elements I missed.","I used this article as the basis for a post-movie version of my talk.","I corrected some errors and made some additions to this article before submitting it right before Nolan's movie is expected to clean up at the Oscars (links to recordings of the pre- and post-movie versions of my talk are included)."],"url":"http://arxiv.org/abs/2403.05212v1","category":"physics.soc-ph"}
{"created":"2024-03-08 10:14:32","title":"An End-to-End Pipeline Perspective on Video Streaming in Best-Effort Networks: A Survey and Tutorial","abstract":"Video streaming continues to captivate attention of users and service providers, dominate in Internet traffic, and form a vibrant research field. Taking a pragmatic approach to reviewing recent research in the field, this paper considers the most dominant streaming paradigm, the main aspects of which include transmission of two-dimensional videos over the best-effort Internet, support from content delivery networks, and client-side bitrate adaptation. To make the survey more accessible, we incorporate extensive tutorial materials. In contrast with the siloed approaches of existing surveys, our paper holistically covers the end-to-end streaming pipeline from video capture and upload for server processing to distribution for playback on diverse user devices. Reflecting the practical interests of respective stakeholders, our survey presents a novel perspective on end-to-end streaming and sheds light on the relationships and interactions between its ingestion, processing, and distribution stages. At each stage, we classify streaming designs in regard to their methodology depending on whether intuition, theory, or machine learning serves as a methodological basis for their core contribution. In addition to tasks confined to a single stage, the survey also examines transversal topics such as coding, super resolution, and quality of experience. After surveying more than 200 papers, we synthesize current trends and project future directions in video streaming research.","sentences":["Video streaming continues to captivate attention of users and service providers, dominate in Internet traffic, and form a vibrant research field.","Taking a pragmatic approach to reviewing recent research in the field, this paper considers the most dominant streaming paradigm, the main aspects of which include transmission of two-dimensional videos over the best-effort Internet, support from content delivery networks, and client-side bitrate adaptation.","To make the survey more accessible, we incorporate extensive tutorial materials.","In contrast with the siloed approaches of existing surveys, our paper holistically covers the end-to-end streaming pipeline from video capture and upload for server processing to distribution for playback on diverse user devices.","Reflecting the practical interests of respective stakeholders, our survey presents a novel perspective on end-to-end streaming and sheds light on the relationships and interactions between its ingestion, processing, and distribution stages.","At each stage, we classify streaming designs in regard to their methodology depending on whether intuition, theory, or machine learning serves as a methodological basis for their core contribution.","In addition to tasks confined to a single stage, the survey also examines transversal topics such as coding, super resolution, and quality of experience.","After surveying more than 200 papers, we synthesize current trends and project future directions in video streaming research."],"url":"http://arxiv.org/abs/2403.05192v1","category":"cs.NI"}
{"created":"2024-03-08 09:36:38","title":"The Strong Nine Dragon Tree Conjecture is True for $d \\leq 2(k+1)$","abstract":"The arboricity $\\Gamma(G)$ of an undirected graph $G =(V,E)$ is the minimal number $k$ such that $E$ can be partitioned into $k$ forests on $V$. Nash-Williams' formula states that $k = \\lceil \\gamma(G) \\rceil$, where $\\gamma(G)$ is the maximum of $\\frac{|E_{H}|}{|V_{H}|-1}$ over all subgraphs $(V_H , E_H )$ of $G$ with $|V_H | \\geq 2$. The Strong Nine Dragon Tree Conjecture states that if $\\gamma(G) \\leq k + \\frac{d}{d+k+1}$ for $k, d \\in \\mathbb{N}$, then there is a partition of the edge set of $G$ into $k + 1$ forests on $V$ such that one forest has at most $d$ edges in each connected component. Here we prove the Strong Nine Dragon Tree Conjecture when $d \\leq 2(k +1)$, which is a new result for all $(k, d)$ such that $d > k + 1$. In fact, we prove a stronger theorem. We prove that a weaker sparsity notion, called $(k, d)$-sparseness, suffices to give the decomposition, under the assumption that the graph decomposes into $k+1$ forests. This is a new result for all $(k, d)$ where $d > 1$, and improves upon the recent resolution of the Overfull Nine Dragon Tree Theorem for all $(k, d)$ when $d \\leq 2(k +1)$. As a corollary, we obtain that planar graphs of girth five decompose into a forest and a forest where every component has at most four edges, and by duality, we obtain that $5$-edge-connected planar graphs have a $\\frac{4}{5}$-thin tree, improving a result of the authors that $5$-edge-connected planar graphs have a $\\frac{5}{6}$-thin tree","sentences":["The arboricity $\\Gamma(G)$ of an undirected graph $G =(V,E)$ is the minimal number $k$ such that $E$ can be partitioned into $k$ forests on $V$. Nash-Williams' formula states that $k = \\lceil \\gamma(G) \\rceil$, where $\\gamma(G)$ is the maximum of $\\frac{|E_{H}|}{|V_{H}|-1}$ over all subgraphs $(V_H , E_H )$ of $G$ with $|V_H | \\geq 2$.","The Strong Nine Dragon Tree Conjecture states that if $\\gamma(G)","\\leq k + \\frac{d}{d+k+1}$ for $k, d \\in \\mathbb{N}$, then there is a partition of the edge set of $G$ into $k + 1$ forests on $V$ such that one forest has at most $d$ edges in each connected component.","Here we prove the Strong Nine Dragon Tree Conjecture when","$d \\leq 2(k +1)$, which is a new result for all $(k, d)$ such that $d > k + 1$.","In fact, we prove a stronger theorem.","We prove that a weaker sparsity notion, called $(k, d)$-sparseness, suffices to give the decomposition, under the assumption that the graph decomposes into $k+1$ forests.","This is a new result for all $(k, d)$ where $d > 1$, and improves upon the recent resolution of the Overfull Nine Dragon Tree Theorem for all $(k, d)$ when $d \\leq","2(k +1)$.","As a corollary, we obtain that planar graphs of girth five decompose into a forest and a forest where every component has at most four edges, and by duality, we obtain that $5$-edge-connected planar graphs have a $\\frac{4}{5}$-thin tree, improving a result of the authors that $5$-edge-connected planar graphs have a $\\frac{5}{6}$-thin tree"],"url":"http://arxiv.org/abs/2403.05178v1","category":"math.CO"}
{"created":"2024-03-08 09:34:06","title":"Interactive Perception for Deformable Object Manipulation","abstract":"Interactive perception enables robots to manipulate the environment and objects to bring them into states that benefit the perception process. Deformable objects pose challenges to this due to significant manipulation difficulty and occlusion in vision-based perception. In this work, we address such a problem with a setup involving both an active camera and an object manipulator. Our approach is based on a sequential decision-making framework and explicitly considers the motion regularity and structure in coupling the camera and manipulator. We contribute a method for constructing and computing a subspace, called Dynamic Active Vision Space (DAVS), for effectively utilizing the regularity in motion exploration. The effectiveness of the framework and approach are validated in both a simulation and a real dual-arm robot setup. Our results confirm the necessity of an active camera and coordinative motion in interactive perception for deformable objects.","sentences":["Interactive perception enables robots to manipulate the environment and objects to bring them into states that benefit the perception process.","Deformable objects pose challenges to this due to significant manipulation difficulty and occlusion in vision-based perception.","In this work, we address such a problem with a setup involving both an active camera and an object manipulator.","Our approach is based on a sequential decision-making framework and explicitly considers the motion regularity and structure in coupling the camera and manipulator.","We contribute a method for constructing and computing a subspace, called Dynamic Active Vision Space (DAVS), for effectively utilizing the regularity in motion exploration.","The effectiveness of the framework and approach are validated in both a simulation and a real dual-arm robot setup.","Our results confirm the necessity of an active camera and coordinative motion in interactive perception for deformable objects."],"url":"http://arxiv.org/abs/2403.05177v1","category":"cs.RO"}
{"created":"2024-03-08 08:52:55","title":"LVIC: Multi-modality segmentation by Lifting Visual Info as Cue","abstract":"Multi-modality fusion is proven an effective method for 3d perception for autonomous driving. However, most current multi-modality fusion pipelines for LiDAR semantic segmentation have complicated fusion mechanisms. Point painting is a quite straight forward method which directly bind LiDAR points with visual information. Unfortunately, previous point painting like methods suffer from projection error between camera and LiDAR. In our experiments, we find that this projection error is the devil in point painting. As a result of that, we propose a depth aware point painting mechanism, which significantly boosts the multi-modality fusion. Apart from that, we take a deeper look at the desired visual feature for LiDAR to operate semantic segmentation. By Lifting Visual Information as Cue, LVIC ranks 1st on nuScenes LiDAR semantic segmentation benchmark. Our experiments show the robustness and effectiveness. Codes would be make publicly available soon.","sentences":["Multi-modality fusion is proven an effective method for 3d perception for autonomous driving.","However, most current multi-modality fusion pipelines for LiDAR semantic segmentation have complicated fusion mechanisms.","Point painting is a quite straight forward method which directly bind LiDAR points with visual information.","Unfortunately, previous point painting like methods suffer from projection error between camera and LiDAR.","In our experiments, we find that this projection error is the devil in point painting.","As a result of that, we propose a depth aware point painting mechanism, which significantly boosts the multi-modality fusion.","Apart from that, we take a deeper look at the desired visual feature for LiDAR to operate semantic segmentation.","By Lifting Visual Information as Cue, LVIC ranks 1st on nuScenes LiDAR semantic segmentation benchmark.","Our experiments show the robustness and effectiveness.","Codes would be make publicly available soon."],"url":"http://arxiv.org/abs/2403.05159v1","category":"cs.CV"}
{"created":"2024-03-08 08:41:55","title":"Noise Robustness of Quantum Relaxation for Combinatorial Optimization","abstract":"QRAO (Quantum Random Access Optimization) is a relaxation algorithm that reduces the number of qubits required to solve a problem by encoding multiple variables per qubit using QRAC (Quantum Random Access Code). Reducing the number of qubits is a common way of dealing with the impact of noise on a quantum algorithm. Our interest lies in the impact of noise on the quality of the binary solution of QRAO, which is unknown. We demonstrate that the mean approximation ratio of the (3, 1)-QRAC Hamiltonian, i.e., the Hamiltonian utilizing the encoding of 3 bits into 1 qubit by QRAC, is less affected by noise compared to the Ising Hamiltonian used in quantum annealer and QAOA (Quantum Approximate Optimization Algorithm). Based on this observation, we discuss a plausible mechanism behind the robustness of QRAO under depolarizing noise. Finally, we assess the number of shots required to estimate the values of binary variables correctly under depolarizing noise and show that the (3, 1)-QRAC Hamiltonian requires less shots to achieve the same accuracy compared to the Ising Hamiltonian.","sentences":["QRAO (Quantum Random Access Optimization) is a relaxation algorithm that reduces the number of qubits required to solve a problem by encoding multiple variables per qubit using QRAC (Quantum Random Access Code).","Reducing the number of qubits is a common way of dealing with the impact of noise on a quantum algorithm.","Our interest lies in the impact of noise on the quality of the binary solution of QRAO, which is unknown.","We demonstrate that the mean approximation ratio of the (3, 1)-QRAC Hamiltonian, i.e., the Hamiltonian utilizing the encoding of 3 bits into 1 qubit by QRAC, is less affected by noise compared to the Ising Hamiltonian used in quantum annealer and QAOA (Quantum Approximate Optimization Algorithm).","Based on this observation, we discuss a plausible mechanism behind the robustness of QRAO under depolarizing noise.","Finally, we assess the number of shots required to estimate the values of binary variables correctly under depolarizing noise and show that the (3, 1)-QRAC Hamiltonian requires less shots to achieve the same accuracy compared to the Ising Hamiltonian."],"url":"http://arxiv.org/abs/2403.05153v1","category":"quant-ph"}
{"created":"2024-03-08 08:27:23","title":"Improving the open cluster census. III. Using cluster masses, radii, and dynamics to create a cleaned open cluster catalogue","abstract":"The census of open clusters has exploded in size thanks to data from the Gaia satellite. However, it is likely that many of these reported clusters are not gravitationally bound, making the open cluster census impractical for many scientific applications. We test different physically motivated methods for distinguishing between bound and unbound clusters, using them to create a cleaned cluster catalogue. We derived completeness-corrected photometric masses for 6956 clusters from our earlier work. Then, we used these masses to compute the size of the Roche surface of these clusters (their Jacobi radius) and distinguish between bound and unbound clusters. We find that only 5647 (79%) of the clusters from our previous catalogue are compatible with bound open clusters, dropping to just 11% of clusters within 250 pc. 3530 open clusters are in a strongly cut high quality sample. The moving groups in our sample show different trends in their size as a function of age and mass, suggesting that they are unbound and undergoing different dynamical processes. Our cluster mass measurements constitute the largest catalogue of Milky Way cluster masses to date, which we also use for further science. Firstly, we inferred the mass-dependent completeness limit of the open cluster census, showing that the census is complete within 1.8 kpc only for objects heavier than 230 M$_\\odot$. Next, we derived a completeness-corrected age and mass function for our open cluster catalogue, including estimating that the Milky Way contains a total of $1.3 \\times 10^5$ open clusters, only ~4% of which are currently known. Finally, we show that most open clusters have mass functions compatible with the Kroupa initial mass function. We demonstrate Jacobi radii for distinguishing between bound and unbound star clusters, and publish an updated star cluster catalogue with masses and improved cluster classifications. (abridged)","sentences":["The census of open clusters has exploded in size thanks to data from the Gaia satellite.","However, it is likely that many of these reported clusters are not gravitationally bound, making the open cluster census impractical for many scientific applications.","We test different physically motivated methods for distinguishing between bound and unbound clusters, using them to create a cleaned cluster catalogue.","We derived completeness-corrected photometric masses for 6956 clusters from our earlier work.","Then, we used these masses to compute the size of the Roche surface of these clusters (their Jacobi radius) and distinguish between bound and unbound clusters.","We find that only 5647 (79%) of the clusters from our previous catalogue are compatible with bound open clusters, dropping to just 11% of clusters within 250 pc.","3530 open clusters are in a strongly cut high quality sample.","The moving groups in our sample show different trends in their size as a function of age and mass, suggesting that they are unbound and undergoing different dynamical processes.","Our cluster mass measurements constitute the largest catalogue of Milky Way cluster masses to date, which we also use for further science.","Firstly, we inferred the mass-dependent completeness limit of the open cluster census, showing that the census is complete within 1.8 kpc only for objects heavier than 230 M$_\\odot$.","Next, we derived a completeness-corrected age and mass function for our open cluster catalogue, including estimating that the Milky Way contains a total of $1.3 \\times 10^5$ open clusters, only ~4% of which are currently known.","Finally, we show that most open clusters have mass functions compatible with the Kroupa initial mass function.","We demonstrate Jacobi radii for distinguishing between bound and unbound star clusters, and publish an updated star cluster catalogue with masses and improved cluster classifications.","(abridged)"],"url":"http://arxiv.org/abs/2403.05143v1","category":"astro-ph.GA"}
{"created":"2024-03-08 18:43:28","title":"Probabilistic Image-Driven Traffic Modeling via Remote Sensing","abstract":"This work addresses the task of modeling spatiotemporal traffic patterns directly from overhead imagery, which we refer to as image-driven traffic modeling. We extend this line of work and introduce a multi-modal, multi-task transformer-based segmentation architecture that can be used to create dense city-scale traffic models. Our approach includes a geo-temporal positional encoding module for integrating geo-temporal context and a probabilistic objective function for estimating traffic speeds that naturally models temporal variations. We evaluate our method extensively using the Dynamic Traffic Speeds (DTS) benchmark dataset and significantly improve the state-of-the-art. Finally, we introduce the DTS++ dataset to support mobility-related location adaptation experiments.","sentences":["This work addresses the task of modeling spatiotemporal traffic patterns directly from overhead imagery, which we refer to as image-driven traffic modeling.","We extend this line of work and introduce a multi-modal, multi-task transformer-based segmentation architecture that can be used to create dense city-scale traffic models.","Our approach includes a geo-temporal positional encoding module for integrating geo-temporal context and a probabilistic objective function for estimating traffic speeds that naturally models temporal variations.","We evaluate our method extensively using the Dynamic Traffic Speeds (DTS) benchmark dataset and significantly improve the state-of-the-art.","Finally, we introduce the DTS++ dataset to support mobility-related location adaptation experiments."],"url":"http://arxiv.org/abs/2403.05521v1","category":"cs.CV"}
{"created":"2024-03-08 17:54:38","title":"JointMotion: Joint Self-supervision for Joint Motion Prediction","abstract":"We present JointMotion, a self-supervised learning method for joint motion prediction in autonomous driving. Our method includes a scene-level objective connecting motion and environments, and an instance-level objective to refine learned representations. Our evaluations show that these objectives are complementary and outperform recent contrastive and autoencoding methods as pre-training for joint motion prediction. Furthermore, JointMotion adapts to all common types of environment representations used for motion prediction (i.e., agent-centric, scene-centric, and pairwise relative), and enables effective transfer learning between the Waymo Open Motion and the Argoverse 2 Forecasting datasets. Notably, our method improves the joint final displacement error of Wayformer, Scene Transformer, and HPTR by 3%, 7%, and 11%, respectively.","sentences":["We present JointMotion, a self-supervised learning method for joint motion prediction in autonomous driving.","Our method includes a scene-level objective connecting motion and environments, and an instance-level objective to refine learned representations.","Our evaluations show that these objectives are complementary and outperform recent contrastive and autoencoding methods as pre-training for joint motion prediction.","Furthermore, JointMotion adapts to all common types of environment representations used for motion prediction (i.e., agent-centric, scene-centric, and pairwise relative), and enables effective transfer learning between the Waymo Open Motion and the Argoverse 2 Forecasting datasets.","Notably, our method improves the joint final displacement error of Wayformer, Scene Transformer, and HPTR by 3%, 7%, and 11%, respectively."],"url":"http://arxiv.org/abs/2403.05489v1","category":"cs.CV"}
{"created":"2024-03-08 16:21:08","title":"An Adaptive Dimension Reduction Estimation Method for High-dimensional Bayesian Optimization","abstract":"Bayesian optimization (BO) has shown impressive results in a variety of applications within low-to-moderate dimensional Euclidean spaces. However, extending BO to high-dimensional settings remains a significant challenge. We address this challenge by proposing a two-step optimization framework. Initially, we identify the effective dimension reduction (EDR) subspace for the objective function using the minimum average variance estimation (MAVE) method. Subsequently, we construct a Gaussian process model within this EDR subspace and optimize it using the expected improvement criterion. Our algorithm offers the flexibility to operate these steps either concurrently or in sequence. In the sequential approach, we meticulously balance the exploration-exploitation trade-off by distributing the sampling budget between subspace estimation and function optimization, and the convergence rate of our algorithm in high-dimensional contexts has been established. Numerical experiments validate the efficacy of our method in challenging scenarios.","sentences":["Bayesian optimization (BO) has shown impressive results in a variety of applications within low-to-moderate dimensional Euclidean spaces.","However, extending BO to high-dimensional settings remains a significant challenge.","We address this challenge by proposing a two-step optimization framework.","Initially, we identify the effective dimension reduction (EDR) subspace for the objective function using the minimum average variance estimation (MAVE) method.","Subsequently, we construct a Gaussian process model within this EDR subspace and optimize it using the expected improvement criterion.","Our algorithm offers the flexibility to operate these steps either concurrently or in sequence.","In the sequential approach, we meticulously balance the exploration-exploitation trade-off by distributing the sampling budget between subspace estimation and function optimization, and the convergence rate of our algorithm in high-dimensional contexts has been established.","Numerical experiments validate the efficacy of our method in challenging scenarios."],"url":"http://arxiv.org/abs/2403.05425v1","category":"stat.ML"}
{"created":"2024-03-08 15:00:44","title":"Frequency-Adaptive Dilated Convolution for Semantic Segmentation","abstract":"Dilated convolution, which expands the receptive field by inserting gaps between its consecutive elements, is widely employed in computer vision. In this study, we propose three strategies to improve individual phases of dilated convolution from the view of spectrum analysis. Departing from the conventional practice of fixing a global dilation rate as a hyperparameter, we introduce Frequency-Adaptive Dilated Convolution (FADC), which dynamically adjusts dilation rates spatially based on local frequency components. Subsequently, we design two plug-in modules to directly enhance effective bandwidth and receptive field size. The Adaptive Kernel (AdaKern) module decomposes convolution weights into low-frequency and high-frequency components, dynamically adjusting the ratio between these components on a per-channel basis. By increasing the high-frequency part of convolution weights, AdaKern captures more high-frequency components, thereby improving effective bandwidth. The Frequency Selection (FreqSelect) module optimally balances high- and low-frequency components in feature representations through spatially variant reweighting. It suppresses high frequencies in the background to encourage FADC to learn a larger dilation, thereby increasing the receptive field for an expanded scope. Extensive experiments on segmentation and object detection consistently validate the efficacy of our approach. The code is publicly available at \\url{https://github.com/Linwei-Chen/FADC}.","sentences":["Dilated convolution, which expands the receptive field by inserting gaps between its consecutive elements, is widely employed in computer vision.","In this study, we propose three strategies to improve individual phases of dilated convolution from the view of spectrum analysis.","Departing from the conventional practice of fixing a global dilation rate as a hyperparameter, we introduce Frequency-Adaptive Dilated Convolution (FADC), which dynamically adjusts dilation rates spatially based on local frequency components.","Subsequently, we design two plug-in modules to directly enhance effective bandwidth and receptive field size.","The Adaptive Kernel (AdaKern) module decomposes convolution weights into low-frequency and high-frequency components, dynamically adjusting the ratio between these components on a per-channel basis.","By increasing the high-frequency part of convolution weights, AdaKern captures more high-frequency components, thereby improving effective bandwidth.","The Frequency Selection (FreqSelect) module optimally balances high- and low-frequency components in feature representations through spatially variant reweighting.","It suppresses high frequencies in the background to encourage FADC to learn a larger dilation, thereby increasing the receptive field for an expanded scope.","Extensive experiments on segmentation and object detection consistently validate the efficacy of our approach.","The code is publicly available at \\url{https://github.com/Linwei-Chen/FADC}."],"url":"http://arxiv.org/abs/2403.05369v1","category":"cs.CV"}
{"created":"2024-03-08 14:14:37","title":"Explaining Pre-Trained Language Models with Attribution Scores: An Analysis in Low-Resource Settings","abstract":"Attribution scores indicate the importance of different input parts and can, thus, explain model behaviour. Currently, prompt-based models are gaining popularity, i.a., due to their easier adaptability in low-resource settings. However, the quality of attribution scores extracted from prompt-based models has not been investigated yet. In this work, we address this topic by analyzing attribution scores extracted from prompt-based models w.r.t. plausibility and faithfulness and comparing them with attribution scores extracted from fine-tuned models and large language models. In contrast to previous work, we introduce training size as another dimension into the analysis. We find that using the prompting paradigm (with either encoder-based or decoder-based models) yields more plausible explanations than fine-tuning the models in low-resource settings and Shapley Value Sampling consistently outperforms attention and Integrated Gradients in terms of leading to more plausible and faithful explanations.","sentences":["Attribution scores indicate the importance of different input parts and can, thus, explain model behaviour.","Currently, prompt-based models are gaining popularity, i.a., due to their easier adaptability in low-resource settings.","However, the quality of attribution scores extracted from prompt-based models has not been investigated yet.","In this work, we address this topic by analyzing attribution scores extracted from prompt-based models w.r.t. plausibility and faithfulness and comparing them with attribution scores extracted from fine-tuned models and large language models.","In contrast to previous work, we introduce training size as another dimension into the analysis.","We find that using the prompting paradigm (with either encoder-based or decoder-based models) yields more plausible explanations than fine-tuning the models in low-resource settings and Shapley Value Sampling consistently outperforms attention and Integrated Gradients in terms of leading to more plausible and faithful explanations."],"url":"http://arxiv.org/abs/2403.05338v1","category":"cs.CL"}
{"created":"2024-03-08 12:26:48","title":"DuDoUniNeXt: Dual-domain unified hybrid model for single and multi-contrast undersampled MRI reconstruction","abstract":"Multi-contrast (MC) Magnetic Resonance Imaging (MRI) reconstruction aims to incorporate a reference image of auxiliary modality to guide the reconstruction process of the target modality. Known MC reconstruction methods perform well with a fully sampled reference image, but usually exhibit inferior performance, compared to single-contrast (SC) methods, when the reference image is missing or of low quality. To address this issue, we propose DuDoUniNeXt, a unified dual-domain MRI reconstruction network that can accommodate to scenarios involving absent, low-quality, and high-quality reference images. DuDoUniNeXt adopts a hybrid backbone that combines CNN and ViT, enabling specific adjustment of image domain and k-space reconstruction. Specifically, an adaptive coarse-to-fine feature fusion module (AdaC2F) is devised to dynamically process the information from reference images of varying qualities. Besides, a partially shared shallow feature extractor (PaSS) is proposed, which uses shared and distinct parameters to handle consistent and discrepancy information among contrasts. Experimental results demonstrate that the proposed model surpasses state-of-the-art SC and MC models significantly. Ablation studies show the effectiveness of the proposed hybrid backbone, AdaC2F, PaSS, and the dual-domain unified learning scheme.","sentences":["Multi-contrast (MC) Magnetic Resonance Imaging (MRI) reconstruction aims to incorporate a reference image of auxiliary modality to guide the reconstruction process of the target modality.","Known MC reconstruction methods perform well with a fully sampled reference image, but usually exhibit inferior performance, compared to single-contrast (SC) methods, when the reference image is missing or of low quality.","To address this issue, we propose DuDoUniNeXt, a unified dual-domain MRI reconstruction network that can accommodate to scenarios involving absent, low-quality, and high-quality reference images.","DuDoUniNeXt adopts a hybrid backbone that combines CNN and ViT, enabling specific adjustment of image domain and k-space reconstruction.","Specifically, an adaptive coarse-to-fine feature fusion module (AdaC2F) is devised to dynamically process the information from reference images of varying qualities.","Besides, a partially shared shallow feature extractor (PaSS) is proposed, which uses shared and distinct parameters to handle consistent and discrepancy information among contrasts.","Experimental results demonstrate that the proposed model surpasses state-of-the-art SC and MC models significantly.","Ablation studies show the effectiveness of the proposed hybrid backbone, AdaC2F, PaSS, and the dual-domain unified learning scheme."],"url":"http://arxiv.org/abs/2403.05256v1","category":"eess.IV"}
{"created":"2024-03-08 18:25:56","title":"Large deviations for slow-fast processes on connected complete Riemannian manifolds","abstract":"We consider a class of slow-fast processes on a connected complete Riemannian manifold $M$.The limiting dynamics as the scale separation goes to $\\infty$ is governed by the averaging principle. Around this limit, we prove large deviation principles with an action-integral rate function for the slow process by nonlinear semigroup methods together with the Hamilton-Jacobi-Bellman equation techniques. The innovation is solving a comparison principle for viscosity solutions on $M$ and the existence of a viscosity solution via a control problem for a non-smooth Hamiltonian.","sentences":["We consider a class of slow-fast processes on a connected complete Riemannian manifold $M$.The limiting dynamics as the scale separation goes to $\\infty$ is governed by the averaging principle.","Around this limit, we prove large deviation principles with an action-integral rate function for the slow process by nonlinear semigroup methods together with the Hamilton-Jacobi-Bellman equation techniques.","The innovation is solving a comparison principle for viscosity solutions on $M$ and the existence of a viscosity solution via a control problem for a non-smooth Hamiltonian."],"url":"http://arxiv.org/abs/2403.05505v1","category":"math.PR"}
{"created":"2024-03-08 17:53:38","title":"Nonlinear Hall effect from long-lived valley-polarizing relaxons","abstract":"The nonlinear Hall effect has attracted much attention due to the famous, widely adopted interpretation in terms of the Berry curvature dipole in momentum space. Using \\textit{ab initio} Boltzmann transport equations, we find a 60\\% enhancement in the nonlinear Hall effect of $n$-doped GeTe and its noticeable frequency dependence, qualitatively different from the predictions based on the Berry curvature dipole. The origin of these differences is long-lived valley polarization in the electron distribution arising from electron-phonon scattering. Our findings await immediate experimental confirmation.","sentences":["The nonlinear Hall effect has attracted much attention due to the famous, widely adopted interpretation in terms of the Berry curvature dipole in momentum space.","Using \\textit{ab initio} Boltzmann transport equations, we find a 60\\% enhancement in the nonlinear Hall effect of $n$-doped GeTe and its noticeable frequency dependence, qualitatively different from the predictions based on the Berry curvature dipole.","The origin of these differences is long-lived valley polarization in the electron distribution arising from electron-phonon scattering.","Our findings await immediate experimental confirmation."],"url":"http://arxiv.org/abs/2403.05487v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-08 17:52:58","title":"Confronting the Lippmann-Schwinger equation and the $N/D$ method for coupled-wave separable potentials","abstract":"We study a family of separable potentials with and without added contact interactions by solving the associated Lippmann-Schwinger equation with two coupled partial waves. The matching of the resulting amplitude matrix with the effective-range expansion is studied in detail. When a counterterm is included in the potential we also carefully discuss its renormalization. Next, we use the matrix $N/D$ method and study whether the amplitude matrices from the potentials considered admit an $N/D$ representation in matrix form. As a novel result we show that it is typically not possible to find such matrix representation for the coupled partial-wave case. However, a separate $N/D$ representation for each coupled partial wave, a valid option known in the literature, is explicitly implemented and numerically solved in cases where the matrix $N/D$ method is unavailable.","sentences":["We study a family of separable potentials with and without added contact interactions by solving the associated Lippmann-Schwinger equation with two coupled partial waves.","The matching of the resulting amplitude matrix with the effective-range expansion is studied in detail.","When a counterterm is included in the potential we also carefully discuss its renormalization.","Next, we use the matrix $N/D$ method and study whether the amplitude matrices from the potentials considered admit an $N/D$ representation in matrix form.","As a novel result we show that it is typically not possible to find such matrix representation for the coupled partial-wave case.","However, a separate $N/D$ representation for each coupled partial wave, a valid option known in the literature, is explicitly implemented and numerically solved in cases where the matrix $N/D$ method is unavailable."],"url":"http://arxiv.org/abs/2403.05486v1","category":"nucl-th"}
{"created":"2024-03-08 17:38:45","title":"Geometric inverse problems on gas giants","abstract":"On gas giant planets the speed of sound is isotropic and goes to zero at the surface. Geometrically, this corresponds to a Riemannian manifold whose metric tensor has a conformal blow-up near the boundary. The blow-up is tamer than in asymptotically hyperbolic geometry: the boundary is at a finite distance.   We study the differential geometry of such manifolds, especially the asymptotic behavior of geodesics near the boundary. We relate the geometry to the propagation of singularities of a hydrodynamic PDE and we give the basic properties of the Laplace--Beltrami operator. We solve two inverse problems, showing that the interior structure of a gas giant is uniquely determined by different types of boundary data.","sentences":["On gas giant planets the speed of sound is isotropic and goes to zero at the surface.","Geometrically, this corresponds to a Riemannian manifold whose metric tensor has a conformal blow-up near the boundary.","The blow-up is tamer than in asymptotically hyperbolic geometry: the boundary is at a finite distance.   ","We study the differential geometry of such manifolds, especially the asymptotic behavior of geodesics near the boundary.","We relate the geometry to the propagation of singularities of a hydrodynamic PDE and we give the basic properties of the Laplace--Beltrami operator.","We solve two inverse problems, showing that the interior structure of a gas giant is uniquely determined by different types of boundary data."],"url":"http://arxiv.org/abs/2403.05475v1","category":"math.DG"}
{"created":"2024-03-08 17:24:26","title":"Accelerating high order discontinuous Galerkin solvers through a clustering-based viscous/turbulent-inviscid domain decomposition","abstract":"We explore the unsupervised clustering technique introduced in [25] to identify viscous/turbulent from inviscid regions in incompressible flows. The separation of regions allows solving the Navier-Stokes equations including Large Eddy Simulation closure models only in the viscous/turbulent ones, while solving the Euler equations in the remaining of the computational domain. By solving different sets of equations, the computational cost is significantly reduced. This coupling strategy is implemented within a discontinuous Galerkin numerical framework, which allows discontinuous solutions (i.e., different set of equations) in neighbouring elements that interact through numerical fluxes. The proposed strategy maintains the same accuracy at lower cost, when compared to solving the full Navier-Stokes equations throughout the entire domain. Validation of this approach is conducted across diverse flow regimes, spanning from unsteady laminar flows to unsteady turbulent flows, including an airfoil section at Reynolds numbers Re = 103 and 104 and large angles of attack, and the flow past a wind turbine, modelled using actuator lines. The computational cost is reduced by 25% and 29% for the unsteady turbulent flow around an airfoil section and the flow past the wind turbine, respectively. In addition, to further accelerate the simulations, we combine the proposed decoupling with local P -adaptation, as proposed in [ 30]. When doing so, we reduce the computational cost by 41% and 45% for the flow around the airfoil section and the flow past the wind turbine, respectively","sentences":["We explore the unsupervised clustering technique introduced in [25] to identify viscous/turbulent from inviscid regions in incompressible flows.","The separation of regions allows solving the Navier-Stokes equations including Large Eddy Simulation closure models only in the viscous/turbulent ones, while solving the Euler equations in the remaining of the computational domain.","By solving different sets of equations, the computational cost is significantly reduced.","This coupling strategy is implemented within a discontinuous Galerkin numerical framework, which allows discontinuous solutions (i.e., different set of equations) in neighbouring elements that interact through numerical fluxes.","The proposed strategy maintains the same accuracy at lower cost, when compared to solving the full Navier-Stokes equations throughout the entire domain.","Validation of this approach is conducted across diverse flow regimes, spanning from unsteady laminar flows to unsteady turbulent flows, including an airfoil section at Reynolds numbers Re = 103 and 104 and large angles of attack, and the flow past a wind turbine, modelled using actuator lines.","The computational cost is reduced by 25% and 29% for the unsteady turbulent flow around an airfoil section and the flow past the wind turbine, respectively.","In addition, to further accelerate the simulations, we combine the proposed decoupling with local P -adaptation, as proposed in [ 30].","When doing so, we reduce the computational cost by 41% and 45% for the flow around the airfoil section and the flow past the wind turbine, respectively"],"url":"http://arxiv.org/abs/2403.05463v1","category":"physics.flu-dyn"}
{"created":"2024-03-08 16:57:54","title":"The R2D2 deep neural network series paradigm for fast precision imaging in radio astronomy","abstract":"Radio-interferometric (RI) imaging entails solving high-resolution high-dynamic range inverse problems from large data volumes. Recent image reconstruction techniques grounded in optimization theory have demonstrated remarkable capability for imaging precision, well beyond CLEAN's capability. These range from advanced proximal algorithms propelled by handcrafted regularization operators, such as the SARA family, to hybrid plug-and-play (PnP) algorithms propelled by learned regularization denoisers, such as AIRI. Optimization and PnP structures are however highly iterative, which hinders their ability to handle the extreme data sizes expected from future instruments. To address this scalability challenge, we introduce a novel deep learning approach, dubbed ``Residual-to-Residual DNN series for high-Dynamic range imaging'. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of Deep Neural Networks (DNNs) taking the previous iteration's image estimate and associated data residual as inputs. It thus takes a hybrid structure between a PnP algorithm and a learned version of the matching pursuit algorithm that underpins CLEAN. We present a comprehensive study of our approach, featuring its multiple incarnations distinguished by their DNN architectures. We provide a detailed description of its training process, targeting a telescope-specific approach. R2D2's capability to deliver high precision is demonstrated in simulation, across a variety of image and observation settings using the Very Large Array (VLA). Its reconstruction speed is also demonstrated: with only few iterations required to clean data residuals at dynamic ranges up to 105, R2D2 opens the door to fast precision imaging. R2D2 codes are available in the BASPLib library on GitHub.","sentences":["Radio-interferometric (RI) imaging entails solving high-resolution high-dynamic range inverse problems from large data volumes.","Recent image reconstruction techniques grounded in optimization theory have demonstrated remarkable capability for imaging precision, well beyond CLEAN's capability.","These range from advanced proximal algorithms propelled by handcrafted regularization operators, such as the SARA family, to hybrid plug-and-play (PnP) algorithms propelled by learned regularization denoisers, such as AIRI.","Optimization and PnP structures are however highly iterative, which hinders their ability to handle the extreme data sizes expected from future instruments.","To address this scalability challenge, we introduce a novel deep learning approach, dubbed ``Residual-to-Residual DNN series for high-Dynamic range imaging'.","R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of Deep Neural Networks (DNNs) taking the previous iteration's image estimate and associated data residual as inputs.","It thus takes a hybrid structure between a PnP algorithm and a learned version of the matching pursuit algorithm that underpins CLEAN.","We present a comprehensive study of our approach, featuring its multiple incarnations distinguished by their DNN architectures.","We provide a detailed description of its training process, targeting a telescope-specific approach.","R2D2's capability to deliver high precision is demonstrated in simulation, across a variety of image and observation settings using the Very Large Array (VLA).","Its reconstruction speed is also demonstrated: with only few iterations required to clean data residuals at dynamic ranges up to 105, R2D2 opens the door to fast precision imaging.","R2D2 codes are available in the BASPLib library on GitHub."],"url":"http://arxiv.org/abs/2403.05452v1","category":"astro-ph.IM"}
{"created":"2024-03-08 16:22:04","title":"Hidden monotonicity and canonical transformations for mean field games and master equations","abstract":"In this paper we unveil novel monotonicity conditions applicable for Mean Field Games through the exploration of finite dimensional $canonical\\ transformations$. Our findings contribute to establishing new global well-posedness results for the associated master equations, also in the case of potentially degenerate idiosyncratic noise. Additionally, we show that recent advancements in global well-posedness results, specifically those related to displacement semi-monotone and anti-monotone data, can be easily obtained as a consequence of our main results.","sentences":["In this paper we unveil novel monotonicity conditions applicable for Mean Field Games through the exploration of finite dimensional $canonical\\ transformations$.","Our findings contribute to establishing new global well-posedness results for the associated master equations, also in the case of potentially degenerate idiosyncratic noise.","Additionally, we show that recent advancements in global well-posedness results, specifically those related to displacement semi-monotone and anti-monotone data, can be easily obtained as a consequence of our main results."],"url":"http://arxiv.org/abs/2403.05426v1","category":"math.AP"}
{"created":"2024-03-08 16:19:39","title":"EVD4UAV: An Altitude-Sensitive Benchmark to Evade Vehicle Detection in UAV","abstract":"Vehicle detection in Unmanned Aerial Vehicle (UAV) captured images has wide applications in aerial photography and remote sensing. There are many public benchmark datasets proposed for the vehicle detection and tracking in UAV images. Recent studies show that adding an adversarial patch on objects can fool the well-trained deep neural networks based object detectors, posing security concerns to the downstream tasks. However, the current public UAV datasets might ignore the diverse altitudes, vehicle attributes, fine-grained instance-level annotation in mostly side view with blurred vehicle roof, so none of them is good to study the adversarial patch based vehicle detection attack problem. In this paper, we propose a new dataset named EVD4UAV as an altitude-sensitive benchmark to evade vehicle detection in UAV with 6,284 images and 90,886 fine-grained annotated vehicles. The EVD4UAV dataset has diverse altitudes (50m, 70m, 90m), vehicle attributes (color, type), fine-grained annotation (horizontal and rotated bounding boxes, instance-level mask) in top view with clear vehicle roof. One white-box and two black-box patch based attack methods are implemented to attack three classic deep neural networks based object detectors on EVD4UAV. The experimental results show that these representative attack methods could not achieve the robust altitude-insensitive attack performance.","sentences":["Vehicle detection in Unmanned Aerial Vehicle (UAV) captured images has wide applications in aerial photography and remote sensing.","There are many public benchmark datasets proposed for the vehicle detection and tracking in UAV images.","Recent studies show that adding an adversarial patch on objects can fool the well-trained deep neural networks based object detectors, posing security concerns to the downstream tasks.","However, the current public UAV datasets might ignore the diverse altitudes, vehicle attributes, fine-grained instance-level annotation in mostly side view with blurred vehicle roof, so none of them is good to study the adversarial patch based vehicle detection attack problem.","In this paper, we propose a new dataset named EVD4UAV as an altitude-sensitive benchmark to evade vehicle detection in UAV with 6,284 images and 90,886 fine-grained annotated vehicles.","The EVD4UAV dataset has diverse altitudes (50m, 70m, 90m), vehicle attributes (color, type), fine-grained annotation (horizontal and rotated bounding boxes, instance-level mask) in top view with clear vehicle roof.","One white-box and two black-box patch based attack methods are implemented to attack three classic deep neural networks based object detectors on EVD4UAV.","The experimental results show that these representative attack methods could not achieve the robust altitude-insensitive attack performance."],"url":"http://arxiv.org/abs/2403.05422v1","category":"cs.CV"}
{"created":"2024-03-08 16:11:03","title":"On classical solutions and canonical transformations for Hamilton--Jacobi--Bellman equations","abstract":"In this note we show how canonical transformations reveal hidden convexity properties for deterministic optimal control problems, which in turn result in global existence of $C^{1,1}_{loc}$ solutions to first order Hamilton--Jacobi--Bellman equations.","sentences":["In this note we show how canonical transformations reveal hidden convexity properties for deterministic optimal control problems, which in turn result in global existence of $C^{1,1}_{loc}$ solutions to first order Hamilton--Jacobi--Bellman equations."],"url":"http://arxiv.org/abs/2403.05412v1","category":"math.OC"}
{"created":"2024-03-08 15:40:20","title":"All paths admit trajectoids","abstract":"In a recent paper published in Nature, Y.I. Sobolev et al. introduced the concept of trajectoids: convex, rigid objects, which roll without slip or spin on a flat plane along a prescribed periodic, unbounded planar path. A geometric construction method applicable to many paths was introduced, and the theory was experimentally verified using objects rolling downwards on slightly inclined planes. The construction method was applicable to many but not all curves. A possible extension of the method (referred to as period-n trajectoids) was also proposed, but the limits of applicability were not clarified. Here, a geometric proof is given for the existence of period-n trajectoids for any sufficiently smooth prescribed curve. A somewhat different proof was recently proposed by O. Muller independently from this work. We also highlight some related geometry problems.","sentences":["In a recent paper published in Nature, Y.I. Sobolev et al. introduced the concept of trajectoids: convex, rigid objects, which roll without slip or spin on a flat plane along a prescribed periodic, unbounded planar path.","A geometric construction method applicable to many paths was introduced, and the theory was experimentally verified using objects rolling downwards on slightly inclined planes.","The construction method was applicable to many but not all curves.","A possible extension of the method (referred to as period-n trajectoids) was also proposed, but the limits of applicability were not clarified.","Here, a geometric proof is given for the existence of period-n trajectoids for any sufficiently smooth prescribed curve.","A somewhat different proof was recently proposed by O. Muller independently from this work.","We also highlight some related geometry problems."],"url":"http://arxiv.org/abs/2403.05392v1","category":"math.DG"}
{"created":"2024-03-08 14:52:22","title":"Super-adiabatic Temperature Gradient at Jupiter's Equatorial Zone and Implications for the Water Abundance","abstract":"The temperature structure of a giant planet was traditionally thought to be an adiabat assuming convective mixing homogenizes entropy. The only in-situ measurement made by the Galileo Probe detected a near-adiabatic temperature structure within one of Jupiter's 5$\\mu$m hot spots with small but definite local departures from adiabaticity. We analyze Juno's microwave observations near Jupiter's equator (0 ~ 5$^o$N) and find that the equatorial temperature structure is best characterized by a stable super-adiabatic temperature profile rather than an adiabatic one. Water is the only substance with sufficient abundance to alter the atmosphere's mean molecular weight and prevent dynamic instability if a super-adiabatic temperature gradient exists. Thus, from the super-adiabaticity, our results indicate a water concentration (or the oxygen to hydrogen ratio) of about 4.9 times solar with a possible range of 1.5 ~ 8.3 times solar in Jupiter's equatorial region.","sentences":["The temperature structure of a giant planet was traditionally thought to be an adiabat assuming convective mixing homogenizes entropy.","The only in-situ measurement made by the Galileo Probe detected a near-adiabatic temperature structure within one of Jupiter's 5$\\mu$m hot spots with small but definite local departures from adiabaticity.","We analyze Juno's microwave observations near Jupiter's equator (0 ~ 5$^o$N) and find that the equatorial temperature structure is best characterized by a stable super-adiabatic temperature profile rather than an adiabatic one.","Water is the only substance with sufficient abundance to alter the atmosphere's mean molecular weight and prevent dynamic instability if a super-adiabatic temperature gradient exists.","Thus, from the super-adiabaticity, our results indicate a water concentration (or the oxygen to hydrogen ratio) of about 4.9 times solar with a possible range of 1.5 ~ 8.3 times solar in Jupiter's equatorial region."],"url":"http://arxiv.org/abs/2403.05363v1","category":"astro-ph.EP"}
{"created":"2024-03-08 14:22:50","title":"An implicit algorithm for simulating the dynamics of small dust grains with smoothed particle hydrodynamics","abstract":"We present an implicit method for solving the diffusion equation for the evolution of the dust fraction in the terminal velocity approximation using dust-as-mixture smoothed particle hydrodynamics (SPH). The numerical scheme involves casting the dust diffusion equation into implicit form, rearranging into its resolvent cubic equation and solving analytically. This method is relevant for small grains that are tightly coupled to the gas, such as sub-micron dust grains in the interstellar medium or millimetre-sized dust grains in protoplanetary discs. The method avoids problems with the variable used to evolve the dust fraction becoming negative when evolved explicitly and is fast and accurate, avoiding the need for dust stopping time limiters and significantly reducing computational expense. Whilst this method is an improvement over using the explicit terminal velocity approximation method, as with any dust-as-mixture method it still fails to give accurate solutions in the limit of large (weakly coupled) grains.","sentences":["We present an implicit method for solving the diffusion equation for the evolution of the dust fraction in the terminal velocity approximation using dust-as-mixture smoothed particle hydrodynamics (SPH).","The numerical scheme involves casting the dust diffusion equation into implicit form, rearranging into its resolvent cubic equation and solving analytically.","This method is relevant for small grains that are tightly coupled to the gas, such as sub-micron dust grains in the interstellar medium or millimetre-sized dust grains in protoplanetary discs.","The method avoids problems with the variable used to evolve the dust fraction becoming negative when evolved explicitly and is fast and accurate, avoiding the need for dust stopping time limiters and significantly reducing computational expense.","Whilst this method is an improvement over using the explicit terminal velocity approximation method, as with any dust-as-mixture method it still fails to give accurate solutions in the limit of large (weakly coupled) grains."],"url":"http://arxiv.org/abs/2403.05345v1","category":"astro-ph.EP"}
{"created":"2024-03-08 14:18:57","title":"A study of the Kuramoto model for synchronization phenomena based on degenerate Kolmogorov-Fokker-Planck equations","abstract":"We study a nonlinear partial differential equation that arises when introducing inertial effects in the Kuramoto model. Based on the known theory of degenerate Kolmogorov operators, we prove existence, uniqueness and a priori estimates of the solution to the relevant Cauchy problem. Moreover, a stable numerical operator, which is consistent with the degenerate Kolmogorov operator, is introduced in order to produce numerical solutions. Finally, numerical experiments show how the synchronization phenomena depend on the parameters of the Kuramoto model with inertia.","sentences":["We study a nonlinear partial differential equation that arises when introducing inertial effects in the Kuramoto model.","Based on the known theory of degenerate Kolmogorov operators, we prove existence, uniqueness and a priori estimates of the solution to the relevant Cauchy problem.","Moreover, a stable numerical operator, which is consistent with the degenerate Kolmogorov operator, is introduced in order to produce numerical solutions.","Finally, numerical experiments show how the synchronization phenomena depend on the parameters of the Kuramoto model with inertia."],"url":"http://arxiv.org/abs/2403.05342v1","category":"math.AP"}
{"created":"2024-03-08 14:17:07","title":"Embedded Deployment of Semantic Segmentation in Medicine through Low-Resolution Inputs","abstract":"When deploying neural networks in real-life situations, the size and computational effort are often the limiting factors. This is especially true in environments where big, expensive hardware is not affordable, like in embedded medical devices, where budgets are often tight. State-of-the-art proposed multiple different lightweight solutions for such use cases, mostly by changing the base model architecture, not taking the input and output resolution into consideration. In this paper, we propose our architecture that takes advantage of the fact that in hardware-limited environments, we often refrain from using the highest available input resolutions to guarantee a higher throughput. Although using lower-resolution input leads to a significant reduction in computing and memory requirements, it may also incur reduced prediction quality. Our architecture addresses this problem by exploiting the fact that we can still utilize high-resolution ground-truths in training. The proposed model inputs lower-resolution images and high-resolution ground truths, which can improve the prediction quality by 5.5% while adding less than 200 parameters to the model. %reducing the frames per second only from 25 to 20. We conduct an extensive analysis to illustrate that our architecture enhances existing state-of-the-art frameworks for lightweight semantic segmentation of cancer in MRI images. We also tested the deployment speed of state-of-the-art lightweight networks and our architecture on Nvidia's Jetson Nano to emulate deployment in resource-constrained embedded scenarios.","sentences":["When deploying neural networks in real-life situations, the size and computational effort are often the limiting factors.","This is especially true in environments where big, expensive hardware is not affordable, like in embedded medical devices, where budgets are often tight.","State-of-the-art proposed multiple different lightweight solutions for such use cases, mostly by changing the base model architecture, not taking the input and output resolution into consideration.","In this paper, we propose our architecture that takes advantage of the fact that in hardware-limited environments, we often refrain from using the highest available input resolutions to guarantee a higher throughput.","Although using lower-resolution input leads to a significant reduction in computing and memory requirements, it may also incur reduced prediction quality.","Our architecture addresses this problem by exploiting the fact that we can still utilize high-resolution ground-truths in training.","The proposed model inputs lower-resolution images and high-resolution ground truths, which can improve the prediction quality by 5.5% while adding less than 200 parameters to the model.","%reducing the frames per second only from 25 to 20.","We conduct an extensive analysis to illustrate that our architecture enhances existing state-of-the-art frameworks for lightweight semantic segmentation of cancer in MRI images.","We also tested the deployment speed of state-of-the-art lightweight networks and our architecture on Nvidia's Jetson Nano to emulate deployment in resource-constrained embedded scenarios."],"url":"http://arxiv.org/abs/2403.05340v1","category":"eess.IV"}
{"created":"2024-03-08 12:13:11","title":"On Representing Electronic Wave Functions with Sign Equivariant Neural Networks","abstract":"Recent neural networks demonstrated impressively accurate approximations of electronic ground-state wave functions. Such neural networks typically consist of a permutation-equivariant neural network followed by a permutation-antisymmetric operation to enforce the electronic exchange symmetry. While accurate, such neural networks are computationally expensive. In this work, we explore the flipped approach, where we first compute antisymmetric quantities based on the electronic coordinates and then apply sign equivariant neural networks to preserve the antisymmetry. While this approach promises acceleration thanks to the lower-dimensional representation, we demonstrate that it reduces to a Jastrow factor, a commonly used permutation-invariant multiplicative factor in the wave function. Our empirical results support this further, finding little to no improvements over baselines. We conclude with neither theoretical nor empirical advantages of sign equivariant functions for representing electronic wave functions within the evaluation of this work.","sentences":["Recent neural networks demonstrated impressively accurate approximations of electronic ground-state wave functions.","Such neural networks typically consist of a permutation-equivariant neural network followed by a permutation-antisymmetric operation to enforce the electronic exchange symmetry.","While accurate, such neural networks are computationally expensive.","In this work, we explore the flipped approach, where we first compute antisymmetric quantities based on the electronic coordinates and then apply sign equivariant neural networks to preserve the antisymmetry.","While this approach promises acceleration thanks to the lower-dimensional representation, we demonstrate that it reduces to a Jastrow factor, a commonly used permutation-invariant multiplicative factor in the wave function.","Our empirical results support this further, finding little to no improvements over baselines.","We conclude with neither theoretical nor empirical advantages of sign equivariant functions for representing electronic wave functions within the evaluation of this work."],"url":"http://arxiv.org/abs/2403.05249v1","category":"quant-ph"}
{"created":"2024-03-08 10:49:11","title":"One-loop Double Copy Relation from Twisted (Co)homology","abstract":"We propose a geometric relation between closed and open string amplitudes at one-loop. After imposing a homological splitting on the world-sheet torus twisted intersection theory is used to establish a one-loop double copy relation. The latter expresses a closed string amplitude by a pair of open string amplitudes and twisted intersection numbers. These inner products on the vector space of allowed differential forms are related to the twisted homology and cohomology groups associated with the Riemann-Wirtinger integral.","sentences":["We propose a geometric relation between closed and open string amplitudes at one-loop.","After imposing a homological splitting on the world-sheet torus twisted intersection theory is used to establish a one-loop double copy relation.","The latter expresses a closed string amplitude by a pair of open string amplitudes and twisted intersection numbers.","These inner products on the vector space of allowed differential forms are related to the twisted homology and cohomology groups associated with the Riemann-Wirtinger integral."],"url":"http://arxiv.org/abs/2403.05208v1","category":"hep-th"}
{"created":"2024-03-08 09:09:23","title":"Electrical conductivity and shear viscosity of a pion gas in a thermo-magnetic medium","abstract":"We evaluate the electrical conductivity and shear viscosity of a interacting pion gas in a thermo-magnetic medium using the kinetic theory. The collision term of the relativistic Boltzmann transport equation in presence of background magnetic field is solved using the relaxation time approximation. The medium modified relaxation time is obtained from the corresponding in-medium $\\pi\\pi\\rightarrow \\pi\\pi$ scattering cross-section calculated using the thermo-magnetic $\\rho$ propagator. It is observed that the average relaxation time shows a $1/T^4$ variation with temperature for a fixed value of magnetic field. The relaxation time shows a mild oscillatory variation with respect to the magnetic field. It is also observed that the medium dependent scattering cross-section causes a considerable amount of influence on the electrical conductivity and shear viscosity compared to its vacuum counterpart.","sentences":["We evaluate the electrical conductivity and shear viscosity of a interacting pion gas in a thermo-magnetic medium using the kinetic theory.","The collision term of the relativistic Boltzmann transport equation in presence of background magnetic field is solved using the relaxation time approximation.","The medium modified relaxation time is obtained from the corresponding in-medium $\\pi\\pi\\rightarrow \\pi\\pi$ scattering cross-section calculated using the thermo-magnetic $\\rho$ propagator.","It is observed that the average relaxation time shows a $1/T^4$ variation with temperature for a fixed value of magnetic field.","The relaxation time shows a mild oscillatory variation with respect to the magnetic field.","It is also observed that the medium dependent scattering cross-section causes a considerable amount of influence on the electrical conductivity and shear viscosity compared to its vacuum counterpart."],"url":"http://arxiv.org/abs/2403.05165v1","category":"nucl-th"}
{"created":"2024-03-08 08:31:46","title":"Motion-Guided Dual-Camera Tracker for Low-Cost Skill Evaluation of Gastric Endoscopy","abstract":"Gastric simulators with objective educational feedback have been proven useful for endoscopy training. Existing electronic simulators with feedback are however not commonly adopted due to their high cost. In this work, a motion-guided dual-camera tracker is proposed to provide reliable endoscope tip position feedback at a low cost inside a mechanical simulator for endoscopy skill evaluation, tackling several unique challenges. To address the issue of significant appearance variation of the endoscope tip while keeping dual-camera tracking consistency, the cross-camera mutual template strategy (CMT) is proposed to introduce dynamic transient mutual templates to dual-camera tracking. To alleviate disturbance from large occlusion and distortion by the light source from the endoscope tip, the Mamba-based motion-guided prediction head (MMH) is presented to aggregate visual tracking with historical motion information modeled by the state space model. The proposed tracker was evaluated on datasets captured by low-cost camera pairs during endoscopy procedures performed inside the mechanical simulator. The tracker achieves SOTA performance with robust and consistent tracking on dual cameras. Further downstream evaluation proves that the 3D tip position determined by the proposed tracker enables reliable skill differentiation. The code and dataset will be released upon acceptance.","sentences":["Gastric simulators with objective educational feedback have been proven useful for endoscopy training.","Existing electronic simulators with feedback are however not commonly adopted due to their high cost.","In this work, a motion-guided dual-camera tracker is proposed to provide reliable endoscope tip position feedback at a low cost inside a mechanical simulator for endoscopy skill evaluation, tackling several unique challenges.","To address the issue of significant appearance variation of the endoscope tip while keeping dual-camera tracking consistency, the cross-camera mutual template strategy (CMT) is proposed to introduce dynamic transient mutual templates to dual-camera tracking.","To alleviate disturbance from large occlusion and distortion by the light source from the endoscope tip, the Mamba-based motion-guided prediction head (MMH) is presented to aggregate visual tracking with historical motion information modeled by the state space model.","The proposed tracker was evaluated on datasets captured by low-cost camera pairs during endoscopy procedures performed inside the mechanical simulator.","The tracker achieves SOTA performance with robust and consistent tracking on dual cameras.","Further downstream evaluation proves that the 3D tip position determined by the proposed tracker enables reliable skill differentiation.","The code and dataset will be released upon acceptance."],"url":"http://arxiv.org/abs/2403.05146v1","category":"cs.CV"}
{"created":"2024-03-08 18:37:11","title":"The Vatican and the Fallibility of Science: Augustine, Copernicus, Darwin and Race","abstract":"This paper provides an overview of work, published since the opening of the archives of the Vatican Congregation for the Doctrine of the Faith at the end of the twentieth century, regarding the Vatican confronting evolution in the nineteenth century. It argues that this work, considered in light of recent studies of scientific writings by Jesuit astronomers who in the seventeenth century were opposed to the ideas of Copernicus, points to interesting things yet to be learned regarding the Vatican's actions on heliocentrism. Concern for Scripture and for the fallible and consequential nature of science, together with the processes used by the Vatican in these confrontations, inevitably led to messy results in these well-known \"religion and science\" confrontations. Nevertheless, these confrontations suggest that what the Vatican was attempting to do in confronting evolution or heliocentrism is something that is needed in science, and something that will be done in the future, probably not by the Vatican, and probably in a fashion not less messy.","sentences":["This paper provides an overview of work, published since the opening of the archives of the Vatican Congregation for the Doctrine of the Faith at the end of the twentieth century, regarding the Vatican confronting evolution in the nineteenth century.","It argues that this work, considered in light of recent studies of scientific writings by Jesuit astronomers who in the seventeenth century were opposed to the ideas of Copernicus, points to interesting things yet to be learned regarding the Vatican's actions on heliocentrism.","Concern for Scripture and for the fallible and consequential nature of science, together with the processes used by the Vatican in these confrontations, inevitably led to messy results in these well-known \"religion and science\" confrontations.","Nevertheless, these confrontations suggest that what the Vatican was attempting to do in confronting evolution or heliocentrism is something that is needed in science, and something that will be done in the future, probably not by the Vatican, and probably in a fashion not less messy."],"url":"http://arxiv.org/abs/2403.05516v1","category":"physics.hist-ph"}
{"created":"2024-03-08 16:48:20","title":"Is Cosine-Similarity of Embeddings Really About Similarity?","abstract":"Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless `similarities.' For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models; these have implicit and unintended effects when taking cosine-similarities of the resulting embeddings, rendering results opaque and possibly arbitrary. Based on these insights, we caution against blindly using cosine-similarity and outline alternatives.","sentences":["Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations.","A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding.","This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice.","To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights.","We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless `similarities.'","For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization.","We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models; these have implicit and unintended effects when taking cosine-similarities of the resulting embeddings, rendering results opaque and possibly arbitrary.","Based on these insights, we caution against blindly using cosine-similarity and outline alternatives."],"url":"http://arxiv.org/abs/2403.05440v1","category":"cs.IR"}
{"created":"2024-03-08 16:24:42","title":"Reply with Sticker: New Dataset and Model for Sticker Retrieval","abstract":"Using stickers in online chatting is very prevalent on social media platforms, where the stickers used in the conversation can express someone's intention/emotion/attitude in a vivid, tactful, and intuitive way. Existing sticker retrieval research typically retrieves stickers based on context and the current utterance delivered by the user. That is, the stickers serve as a supplement to the current utterance. However, in the real-world scenario, using stickers to express what we want to say rather than as a supplement to our words only is also important. Therefore, in this paper, we create a new dataset for sticker retrieval in conversation, called StickerInt, where stickers are used to reply to previous conversations or supplement our words. Based on the created dataset, we present a simple yet effective framework for sticker retrieval in conversation based on the learning of intention and the cross-modal relationships between conversation context and stickers, coined as \\textbf{Int-RA}. Specifically, we first devise a knowledge-enhanced intention predictor to introduce the intention information into the conversation representations. Subsequently, a relation-aware sticker selector is devised to retrieve the response sticker via cross-modal relationships. Extensive experiments on the created dataset show that the proposed model achieves state-of-the-art performance in sticker retrieval.","sentences":["Using stickers in online chatting is very prevalent on social media platforms, where the stickers used in the conversation can express someone's intention/emotion/attitude in a vivid, tactful, and intuitive way.","Existing sticker retrieval research typically retrieves stickers based on context and the current utterance delivered by the user.","That is, the stickers serve as a supplement to the current utterance.","However, in the real-world scenario, using stickers to express what we want to say rather than as a supplement to our words only is also important.","Therefore, in this paper, we create a new dataset for sticker retrieval in conversation, called StickerInt, where stickers are used to reply to previous conversations or supplement our words.","Based on the created dataset, we present a simple yet effective framework for sticker retrieval in conversation based on the learning of intention and the cross-modal relationships between conversation context and stickers, coined as \\textbf{Int-RA}.","Specifically, we first devise a knowledge-enhanced intention predictor to introduce the intention information into the conversation representations.","Subsequently, a relation-aware sticker selector is devised to retrieve the response sticker via cross-modal relationships.","Extensive experiments on the created dataset show that the proposed model achieves state-of-the-art performance in sticker retrieval."],"url":"http://arxiv.org/abs/2403.05427v1","category":"cs.MM"}
{"created":"2024-03-08 15:20:27","title":"Exploring Robust Features for Few-Shot Object Detection in Satellite Imagery","abstract":"The goal of this paper is to perform object detection in satellite imagery with only a few examples, thus enabling users to specify any object class with minimal annotation. To this end, we explore recent methods and ideas from open-vocabulary detection for the remote sensing domain. We develop a few-shot object detector based on a traditional two-stage architecture, where the classification block is replaced by a prototype-based classifier. A large-scale pre-trained model is used to build class-reference embeddings or prototypes, which are compared to region proposal contents for label prediction. In addition, we propose to fine-tune prototypes on available training images to boost performance and learn differences between similar classes, such as aircraft types. We perform extensive evaluations on two remote sensing datasets containing challenging and rare objects. Moreover, we study the performance of both visual and image-text features, namely DINOv2 and CLIP, including two CLIP models specifically tailored for remote sensing applications. Results indicate that visual features are largely superior to vision-language models, as the latter lack the necessary domain-specific vocabulary. Lastly, the developed detector outperforms fully supervised and few-shot methods evaluated on the SIMD and DIOR datasets, despite minimal training parameters.","sentences":["The goal of this paper is to perform object detection in satellite imagery with only a few examples, thus enabling users to specify any object class with minimal annotation.","To this end, we explore recent methods and ideas from open-vocabulary detection for the remote sensing domain.","We develop a few-shot object detector based on a traditional two-stage architecture, where the classification block is replaced by a prototype-based classifier.","A large-scale pre-trained model is used to build class-reference embeddings or prototypes, which are compared to region proposal contents for label prediction.","In addition, we propose to fine-tune prototypes on available training images to boost performance and learn differences between similar classes, such as aircraft types.","We perform extensive evaluations on two remote sensing datasets containing challenging and rare objects.","Moreover, we study the performance of both visual and image-text features, namely DINOv2 and CLIP, including two CLIP models specifically tailored for remote sensing applications.","Results indicate that visual features are largely superior to vision-language models, as the latter lack the necessary domain-specific vocabulary.","Lastly, the developed detector outperforms fully supervised and few-shot methods evaluated on the SIMD and DIOR datasets, despite minimal training parameters."],"url":"http://arxiv.org/abs/2403.05381v1","category":"cs.CV"}
{"created":"2024-03-08 14:55:05","title":"The Impact of Quantization on the Robustness of Transformer-based Text Classifiers","abstract":"Transformer-based models have made remarkable advancements in various NLP areas. Nevertheless, these models often exhibit vulnerabilities when confronted with adversarial attacks. In this paper, we explore the effect of quantization on the robustness of Transformer-based models. Quantization usually involves mapping a high-precision real number to a lower-precision value, aiming at reducing the size of the model at hand. To the best of our knowledge, this work is the first application of quantization on the robustness of NLP models. In our experiments, we evaluate the impact of quantization on BERT and DistilBERT models in text classification using SST-2, Emotion, and MR datasets. We also evaluate the performance of these models against TextFooler, PWWS, and PSO adversarial attacks. Our findings show that quantization significantly improves (by an average of 18.68%) the adversarial accuracy of the models. Furthermore, we compare the effect of quantization versus that of the adversarial training approach on robustness. Our experiments indicate that quantization increases the robustness of the model by 18.80% on average compared to adversarial training without imposing any extra computational overhead during training. Therefore, our results highlight the effectiveness of quantization in improving the robustness of NLP models.","sentences":["Transformer-based models have made remarkable advancements in various NLP areas.","Nevertheless, these models often exhibit vulnerabilities when confronted with adversarial attacks.","In this paper, we explore the effect of quantization on the robustness of Transformer-based models.","Quantization usually involves mapping a high-precision real number to a lower-precision value, aiming at reducing the size of the model at hand.","To the best of our knowledge, this work is the first application of quantization on the robustness of NLP models.","In our experiments, we evaluate the impact of quantization on BERT and DistilBERT models in text classification using SST-2, Emotion, and MR datasets.","We also evaluate the performance of these models against TextFooler, PWWS, and PSO adversarial attacks.","Our findings show that quantization significantly improves (by an average of 18.68%) the adversarial accuracy of the models.","Furthermore, we compare the effect of quantization versus that of the adversarial training approach on robustness.","Our experiments indicate that quantization increases the robustness of the model by 18.80% on average compared to adversarial training without imposing any extra computational overhead during training.","Therefore, our results highlight the effectiveness of quantization in improving the robustness of NLP models."],"url":"http://arxiv.org/abs/2403.05365v1","category":"cs.CL"}
{"created":"2024-03-08 14:31:40","title":"Multiple Instance Learning with random sampling for Whole Slide Image Classification","abstract":"In computational pathology, random sampling of patches during training of Multiple Instance Learning (MIL) methods is computationally efficient and serves as a regularization strategy. Despite its promising benefits, questions concerning performance trends for varying sample sizes and its influence on model interpretability remain. Addressing these, we reach an optimal performance enhancement of 1.7% using thirty percent of patches on the CAMELYON16 dataset, and 3.7% with only eight samples on the TUPAC16 dataset. We also find interpretability effects are strongly dataset-dependent, with interpretability impacted on CAMELYON16, while remaining unaffected on TUPAC16. This reinforces that both the performance and interpretability relationships with sampling are closely task-specific. End-to-end training with 1024 samples reveals improvements across both datasets compared to pre-extracted features, further highlighting the potential of this efficient approach.","sentences":["In computational pathology, random sampling of patches during training of Multiple Instance Learning (MIL) methods is computationally efficient and serves as a regularization strategy.","Despite its promising benefits, questions concerning performance trends for varying sample sizes and its influence on model interpretability remain.","Addressing these, we reach an optimal performance enhancement of 1.7% using thirty percent of patches on the CAMELYON16 dataset, and 3.7% with only eight samples on the TUPAC16 dataset.","We also find interpretability effects are strongly dataset-dependent, with interpretability impacted on CAMELYON16, while remaining unaffected on TUPAC16.","This reinforces that both the performance and interpretability relationships with sampling are closely task-specific.","End-to-end training with 1024 samples reveals improvements across both datasets compared to pre-extracted features, further highlighting the potential of this efficient approach."],"url":"http://arxiv.org/abs/2403.05351v1","category":"cs.CV"}
{"created":"2024-03-08 14:08:00","title":"Causality and extremes","abstract":"In this work, we summarize the state-of-the-art methods in causal inference for extremes. In a non-exhaustive way, we start by describing an extremal approach to quantile treatment effect where the treatment has an impact on the tail of the outcome. Then, we delve into two primary causal structures for extremes, offering in-depth insights into their identifiability. Additionally, we discuss causal structure learning in relation to these two models as well as in a model-agnostic framework. To illustrate the practicality of the approaches, we apply and compare these different methods using a Seine network dataset. This work concludes with a summary and outlines potential directions for future research.","sentences":["In this work, we summarize the state-of-the-art methods in causal inference for extremes.","In a non-exhaustive way, we start by describing an extremal approach to quantile treatment effect where the treatment has an impact on the tail of the outcome.","Then, we delve into two primary causal structures for extremes, offering in-depth insights into their identifiability.","Additionally, we discuss causal structure learning in relation to these two models as well as in a model-agnostic framework.","To illustrate the practicality of the approaches, we apply and compare these different methods using a Seine network dataset.","This work concludes with a summary and outlines potential directions for future research."],"url":"http://arxiv.org/abs/2403.05331v1","category":"stat.ME"}
{"created":"2024-03-08 13:21:07","title":"Leveraging Continuous Time to Understand Momentum When Training Diagonal Linear Networks","abstract":"In this work, we investigate the effect of momentum on the optimisation trajectory of gradient descent. We leverage a continuous-time approach in the analysis of momentum gradient descent with step size $\\gamma$ and momentum parameter $\\beta$ that allows us to identify an intrinsic quantity $\\lambda = \\frac{ \\gamma }{ (1 - \\beta)^2 }$ which uniquely defines the optimisation path and provides a simple acceleration rule. When training a $2$-layer diagonal linear network in an overparametrised regression setting, we characterise the recovered solution through an implicit regularisation problem. We then prove that small values of $\\lambda$ help to recover sparse solutions. Finally, we give similar but weaker results for stochastic momentum gradient descent. We provide numerical experiments which support our claims.","sentences":["In this work, we investigate the effect of momentum on the optimisation trajectory of gradient descent.","We leverage a continuous-time approach in the analysis of momentum gradient descent with step size $\\gamma$ and momentum parameter $\\beta$ that allows us to identify an intrinsic quantity $\\lambda = \\frac{ \\gamma }{ (1 - \\beta)^2 }$ which uniquely defines the optimisation path and provides a simple acceleration rule.","When training a $2$-layer diagonal linear network in an overparametrised regression setting, we characterise the recovered solution through an implicit regularisation problem.","We then prove that small values of $\\lambda$ help to recover sparse solutions.","Finally, we give similar but weaker results for stochastic momentum gradient descent.","We provide numerical experiments which support our claims."],"url":"http://arxiv.org/abs/2403.05293v1","category":"cs.LG"}
{"created":"2024-03-08 12:28:15","title":"Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity","abstract":"Learning better sentence embeddings leads to improved performance for natural language understanding tasks including semantic textual similarity (STS) and natural language inference (NLI). As prior studies leverage large-scale labeled NLI datasets for fine-tuning masked language models to yield sentence embeddings, task performance for languages other than English is often left behind. In this study, we directly compared two data augmentation techniques as potential solutions for monolingual STS: (a) cross-lingual transfer that exploits English resources alone as training data to yield non-English sentence embeddings as zero-shot inference, and (b) machine translation that coverts English data into pseudo non-English training data in advance. In our experiments on monolingual STS in Japanese and Korean, we find that the two data techniques yield performance on par. Rather, we find a superiority of the Wikipedia domain over the NLI domain for these languages, in contrast to prior studies that focused on NLI as training data. Combining our findings, we demonstrate that the cross-lingual transfer of Wikipedia data exhibits improved performance, and that native Wikipedia data can further improve performance for monolingual STS.","sentences":["Learning better sentence embeddings leads to improved performance for natural language understanding tasks including semantic textual similarity (STS) and natural language inference (NLI).","As prior studies leverage large-scale labeled NLI datasets for fine-tuning masked language models to yield sentence embeddings, task performance for languages other than English is often left behind.","In this study, we directly compared two data augmentation techniques as potential solutions for monolingual STS: (a) cross-lingual transfer that exploits English resources alone as training data to yield non-English sentence embeddings as zero-shot inference, and (b) machine translation that coverts English data into pseudo non-English training data in advance.","In our experiments on monolingual STS in Japanese and Korean, we find that the two data techniques yield performance on par.","Rather, we find a superiority of the Wikipedia domain over the NLI domain for these languages, in contrast to prior studies that focused on NLI as training data.","Combining our findings, we demonstrate that the cross-lingual transfer of Wikipedia data exhibits improved performance, and that native Wikipedia data can further improve performance for monolingual STS."],"url":"http://arxiv.org/abs/2403.05257v1","category":"cs.CL"}
{"created":"2024-03-08 09:43:27","title":"Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples","abstract":"Knowledge Distillation (KD) facilitates the transfer of discriminative capabilities from an advanced teacher model to a simpler student model, ensuring performance enhancement without compromising accuracy. It is also exploited for model stealing attacks, where adversaries use KD to mimic the functionality of a teacher model. Recent developments in this domain have been influenced by the Stingy Teacher model, which provided empirical analysis showing that sparse outputs can significantly degrade the performance of student models. Addressing the risk of intellectual property leakage, our work introduces an approach to train a teacher model that inherently protects its logits, influenced by the Nasty Teacher concept. Differing from existing methods, we incorporate sparse outputs of adversarial examples with standard training data to strengthen the teacher's defense against student distillation. Our approach carefully reduces the relative entropy between the original and adversarially perturbed outputs, allowing the model to produce adversarial logits with minimal impact on overall performance. The source codes will be made publicly available soon.","sentences":["Knowledge Distillation (KD) facilitates the transfer of discriminative capabilities from an advanced teacher model to a simpler student model, ensuring performance enhancement without compromising accuracy.","It is also exploited for model stealing attacks, where adversaries use KD to mimic the functionality of a teacher model.","Recent developments in this domain have been influenced by the Stingy Teacher model, which provided empirical analysis showing that sparse outputs can significantly degrade the performance of student models.","Addressing the risk of intellectual property leakage, our work introduces an approach to train a teacher model that inherently protects its logits, influenced by the Nasty Teacher concept.","Differing from existing methods, we incorporate sparse outputs of adversarial examples with standard training data to strengthen the teacher's defense against student distillation.","Our approach carefully reduces the relative entropy between the original and adversarially perturbed outputs, allowing the model to produce adversarial logits with minimal impact on overall performance.","The source codes will be made publicly available soon."],"url":"http://arxiv.org/abs/2403.05181v1","category":"cs.LG"}
{"created":"2024-03-08 18:55:52","title":"Non-parametric Bayesian reconstruction of Galactic magnetic fields using Information Field Theory: The inclusion of line-of-sight information in ultra-high energy cosmic ray backtracking","abstract":"(abridged) Ultra-high energy cosmic rays (UHECRs) are extremely energetic charged particles with energies surpassing $10^{18}$ eV. Their sources remain elusive, obscured by deflections caused by the Galactic magnetic field (GMF). This challenge is further complicated by our limited understanding of the three-dimensional structure of the GMF, as current GMF observations consist primarily of quantities integrated along the line-of-sight (LOS). Nevertheless, data from upcoming stellar polarisation surveys along with Gaia's stellar parallax data are expected to yield local GMF measurements.. In this work, we employ methods of Bayesian statistical inference in order to sample the posterior distribution of the GMF within part of the Galaxy. By assuming a known rigidity and arrival direction of an UHECR, we backtrack its trajectory through various GMF configurations drawn from the posterior distribution. Our objective is to rigorously evaluate our algorithm's performance in scenarios that closely mirror the setting of expected future applications. In pursuit of this, we condition the posterior to synthetic integrated LOS measurements of the GMF, in addition to synthetic local POS-component measurements. In this proof of concept work, we assume the ground truth to be a magnetic field produced by a dynamo simulation of the Galactic ISM. Our results demonstrate that for all locations of the observed arrival direction on the POS, our algorithm is able to substantially update our knowledge on the original arrival direction of UHECRs with rigidity $E/Z = 5 \\times 10^{19}$ eV, even in the case of complete absence of LOS information. If integrated data is included in the inference, then the regions of the celestial sphere where the maximum error occurs diminishes greatly. Even in those regions the maximum error is diminished by a factor of about $3$ in the specific setting studied.","sentences":["(abridged) Ultra-high energy cosmic rays (UHECRs) are extremely energetic charged particles with energies surpassing $10^{18}$ eV. Their sources remain elusive, obscured by deflections caused by the Galactic magnetic field (GMF).","This challenge is further complicated by our limited understanding of the three-dimensional structure of the GMF, as current GMF observations consist primarily of quantities integrated along the line-of-sight (LOS).","Nevertheless, data from upcoming stellar polarisation surveys along with Gaia's stellar parallax data are expected to yield local GMF measurements..","In this work, we employ methods of Bayesian statistical inference in order to sample the posterior distribution of the GMF within part of the Galaxy.","By assuming a known rigidity and arrival direction of an UHECR, we backtrack its trajectory through various GMF configurations drawn from the posterior distribution.","Our objective is to rigorously evaluate our algorithm's performance in scenarios that closely mirror the setting of expected future applications.","In pursuit of this, we condition the posterior to synthetic integrated LOS measurements of the GMF, in addition to synthetic local POS-component measurements.","In this proof of concept work, we assume the ground truth to be a magnetic field produced by a dynamo simulation of the Galactic ISM.","Our results demonstrate that for all locations of the observed arrival direction on the POS, our algorithm is able to substantially update our knowledge on the original arrival direction of UHECRs with rigidity $E/Z = 5 \\times 10^{19}$ eV, even in the case of complete absence of LOS information.","If integrated data is included in the inference, then the regions of the celestial sphere where the maximum error occurs diminishes greatly.","Even in those regions the maximum error is diminished by a factor of about $3$ in the specific setting studied."],"url":"http://arxiv.org/abs/2403.05531v1","category":"astro-ph.HE"}
{"created":"2024-03-08 18:26:38","title":"The NIRSpec Wide GTO Survey","abstract":"The Near-infrared Spectrograph (NIRSpec) on the James Webb Space Telescope is uniquely suited to studying galaxies in the distant Universe with its combination of multi-object capabilities and sensitivity over a large range in wavelength (0.6-5.3 microns). Here we present the NIRSpec Wide survey, part of the NIRSpec Instrument Science Team's Guaranteed Time Observations, using NIRSpec's microshutter array to obtain spectra of more than 3200 galaxies at $z>1$ at both low- and high-resolution ($R\\approx100$ and 2700) for a total of 105 hours. With 31 pointings covering $\\approx$320 arcmin$^2$ across the five CANDELS fields with exquisite ancillary photometry from the Hubble Space Telescope, the NIRSpec Wide survey represents a fast and efficient way of using JWST to probe galaxies in the early Universe. Pointing centers are determined to maximize the observability of the rarest, high-value sources. Subsequently, the microshutter configurations are optimized to observe the maximum number of \"census\" galaxies with a selection function based primarily on HST/F160W magnitude, photometric/slitless grism redshift, and predicted \\ha\\ flux tracing the bulk of the galaxy population at cosmic noon ($z_{\\rm med}=2.0$). We present details on the survey strategy, the target selection, an outline of the motivating science cases, and discuss upcoming public data releases to the community.","sentences":["The Near-infrared Spectrograph (NIRSpec) on the James Webb Space Telescope is uniquely suited to studying galaxies in the distant Universe with its combination of multi-object capabilities and sensitivity over a large range in wavelength (0.6-5.3 microns).","Here we present the NIRSpec Wide survey, part of the NIRSpec Instrument Science Team's Guaranteed Time Observations, using NIRSpec's microshutter array to obtain spectra of more than 3200 galaxies at $z>1$ at both low- and high-resolution ($R\\approx100$ and 2700) for a total of 105 hours.","With 31 pointings covering $\\approx$320 arcmin$^2$ across the five CANDELS fields with exquisite ancillary photometry from the Hubble Space Telescope, the NIRSpec Wide survey represents a fast and efficient way of using JWST to probe galaxies in the early Universe.","Pointing centers are determined to maximize the observability of the rarest, high-value sources.","Subsequently, the microshutter configurations are optimized to observe the maximum number of \"census\" galaxies with a selection function based primarily on HST/F160W magnitude, photometric/slitless grism redshift, and predicted \\ha\\ flux tracing the bulk of the galaxy population at cosmic noon ($z_{\\rm med}=2.0$).","We present details on the survey strategy, the target selection, an outline of the motivating science cases, and discuss upcoming public data releases to the community."],"url":"http://arxiv.org/abs/2403.05506v1","category":"astro-ph.GA"}
{"created":"2024-03-08 18:20:21","title":"Quantum bounds for compiled XOR games and $d$-outcome CHSH games","abstract":"Nonlocal games play a crucial role in quantum information theory and have numerous applications in certification and cryptographic protocols. Kalai et al. (STOC 2023) introduced a procedure to compile a nonlocal game into a single-prover interactive proof, using a quantum homomorphic encryption scheme, and showed that their compilation method preserves the classical bound of the game. Natarajan and Zhang (FOCS 2023) then showed that the quantum bound is preserved for the specific case of the CHSH game. Extending the proof techniques of Natarajan and Zhang, we show that the compilation procedure of Kalai et al. preserves the quantum bound for two classes of games: XOR games and d-outcome CHSH games. We also establish that, for any pair of qubit measurements, there exists an XOR game such that its optimal winning probability serves as a self-test for that particular pair of measurements.","sentences":["Nonlocal games play a crucial role in quantum information theory and have numerous applications in certification and cryptographic protocols.","Kalai et al. (STOC 2023) introduced a procedure to compile a nonlocal game into a single-prover interactive proof, using a quantum homomorphic encryption scheme, and showed that their compilation method preserves the classical bound of the game.","Natarajan and Zhang (FOCS 2023) then showed that the quantum bound is preserved for the specific case of the CHSH game.","Extending the proof techniques of Natarajan and Zhang, we show that the compilation procedure of Kalai et al. preserves the quantum bound for two classes of games: XOR games and d-outcome CHSH games.","We also establish that, for any pair of qubit measurements, there exists an XOR game such that its optimal winning probability serves as a self-test for that particular pair of measurements."],"url":"http://arxiv.org/abs/2403.05502v1","category":"quant-ph"}
{"created":"2024-03-08 17:30:03","title":"On the Set of Possible Minimizers of a Sum of Convex Functions","abstract":"Consider a sum of convex functions, where the only information known about each individual summand is the location of a minimizer. In this work, we give an exact characterization of the set of possible minimizers of the sum. Our results cover several types of assumptions on the summands, such as smoothness or strong convexity. Our main tool is the use of necessary and sufficient conditions for interpolating the considered function classes, which leads to shorter and more direct proofs in comparison with previous work. We also address the setting where each summand minimizer is assumed to lie in a unit ball, and prove a tight bound on the norm of any minimizer of the sum.","sentences":["Consider a sum of convex functions, where the only information known about each individual summand is the location of a minimizer.","In this work, we give an exact characterization of the set of possible minimizers of the sum.","Our results cover several types of assumptions on the summands, such as smoothness or strong convexity.","Our main tool is the use of necessary and sufficient conditions for interpolating the considered function classes, which leads to shorter and more direct proofs in comparison with previous work.","We also address the setting where each summand minimizer is assumed to lie in a unit ball, and prove a tight bound on the norm of any minimizer of the sum."],"url":"http://arxiv.org/abs/2403.05467v1","category":"math.OC"}
{"created":"2024-03-08 14:45:22","title":"Applying Non-negative Matrix Factorization with Covariates to the Longitudinal Data as Growth Curve Model","abstract":"Using Non-negative Matrix Factorization (NMF), the observed matrix can be approximated by the product of the basis and coefficient matrices. Moreover, if the coefficient vectors are explained by the covariates for each individual, the coefficient matrix can be written as the product of the parameter matrix and the covariate matrix, and additionally described in the framework of Non-negative Matrix tri-Factorization (tri-NMF) with covariates. Consequently, this is equal to the mean structure of the Growth Curve Model (GCM). The difference is that the basis matrix for GCM is given by the analyst, whereas that for NMF with covariates is unknown and optimized. In this study, we applied NMF with covariance to longitudinal data and compared it with GCM. We have also published an R package that implements this method, and we show how to use it through examples of data analyses including longitudinal measurement, spatiotemporal data and text data. In particular, we demonstrate the usefulness of Gaussian kernel functions as covariates.","sentences":["Using Non-negative Matrix Factorization (NMF), the observed matrix can be approximated by the product of the basis and coefficient matrices.","Moreover, if the coefficient vectors are explained by the covariates for each individual, the coefficient matrix can be written as the product of the parameter matrix and the covariate matrix, and additionally described in the framework of Non-negative Matrix tri-Factorization (tri-NMF) with covariates.","Consequently, this is equal to the mean structure of the Growth Curve Model (GCM).","The difference is that the basis matrix for GCM is given by the analyst, whereas that for NMF with covariates is unknown and optimized.","In this study, we applied NMF with covariance to longitudinal data and compared it with GCM.","We have also published an R package that implements this method, and we show how to use it through examples of data analyses including longitudinal measurement, spatiotemporal data and text data.","In particular, we demonstrate the usefulness of Gaussian kernel functions as covariates."],"url":"http://arxiv.org/abs/2403.05359v1","category":"stat.ME"}
{"created":"2024-03-08 14:45:17","title":"Optimized detection modality for double resonance alignment based optical magnetometer","abstract":"In this work, we present a comprehensive and comparative analysis of two detection modalities, i.e., polarization rotation and absorption measurement of light, for a double resonance alignment based optical magnetometer (DRAM). We derive algebraic expressions for magnetometry signals based on multipole moments description. Experiments are carried out using a room-temperature paraffin-coated Caesium vapour cell and measuring either the polarization rotation or absorption of the transmitted laser light. A detailed experimental analysis of the resonance spectra is performed to validate the theoretical findings for various input parameters. The results signify the use of a single isotropic relaxation rate thus simplifying the data analysis for optimization of the DRAM. The sensitivity measurements are performed and reveal that the polarization rotation detection mode yields larger signals and better sensitivity than absorption measurement of light.","sentences":["In this work, we present a comprehensive and comparative analysis of two detection modalities, i.e., polarization rotation and absorption measurement of light, for a double resonance alignment based optical magnetometer (DRAM).","We derive algebraic expressions for magnetometry signals based on multipole moments description.","Experiments are carried out using a room-temperature paraffin-coated Caesium vapour cell and measuring either the polarization rotation or absorption of the transmitted laser light.","A detailed experimental analysis of the resonance spectra is performed to validate the theoretical findings for various input parameters.","The results signify the use of a single isotropic relaxation rate thus simplifying the data analysis for optimization of the DRAM.","The sensitivity measurements are performed and reveal that the polarization rotation detection mode yields larger signals and better sensitivity than absorption measurement of light."],"url":"http://arxiv.org/abs/2403.05357v1","category":"physics.optics"}
{"created":"2024-03-08 14:11:43","title":"Ultimate Performance of Biomedical Ablation","abstract":"Microwave ablation is a therapeutic procedure to eliminate abnormal tissue within a body selectively. There are two types of ablations; the thermal one aims to raise the temperature at the target, while the non-thermal one induces a temporarily high electric field at the target to disrupt cellular membrane integrity. This work identifies the fundamental bounds of the efficiency for each type of ablation and the sources to achieve them. For both types, the bounds exceed the performance of existing solutions by tenfold, showing a large room for improvement. Finally, the optimal source for thermal ablation is physically realized with an 11 x 11 dipole array, the performance of which closely approaches the bound.","sentences":["Microwave ablation is a therapeutic procedure to eliminate abnormal tissue within a body selectively.","There are two types of ablations; the thermal one aims to raise the temperature at the target, while the non-thermal one induces a temporarily high electric field at the target to disrupt cellular membrane integrity.","This work identifies the fundamental bounds of the efficiency for each type of ablation and the sources to achieve them.","For both types, the bounds exceed the performance of existing solutions by tenfold, showing a large room for improvement.","Finally, the optimal source for thermal ablation is physically realized with an 11 x 11 dipole array, the performance of which closely approaches the bound."],"url":"http://arxiv.org/abs/2403.05335v1","category":"physics.med-ph"}
{"created":"2024-03-08 13:58:59","title":"Revisiting Theoretical Guarantees of Direct-Search Methods","abstract":"Optimizing a function without using derivatives is a challenging paradigm, that precludes from using classical algorithms from nonlinear optimization and may thus seem intractable other than by using heuristics. However, the field of derivative-free optimization has succeeded in producing algorithms that do not rely on derivatives and yet are endowed with convergence guarantees. One class of such methods, called direct search, is particularly popular thanks to its simplicity of implementation, even though its theoretical underpinnings are not always easy to grasp. In this work, we survey contemporary direct-search algorithms from a theoretical viewpoint, with the aim of highlighting the key theoretical features of these methods. Our study goes beyond the classical, textbook cases and tackles the presence of nonsmoothness, noise, and constraints in the problem at hand. In addition to reviewing classical results in the field, we provide new perspectives on existing results, as well as novel proofs that illustrate the versatility of direct-search schemes.","sentences":["Optimizing a function without using derivatives is a challenging paradigm, that precludes from using classical algorithms from nonlinear optimization and may thus seem intractable other than by using heuristics.","However, the field of derivative-free optimization has succeeded in producing algorithms that do not rely on derivatives and yet are endowed with convergence guarantees.","One class of such methods, called direct search, is particularly popular thanks to its simplicity of implementation, even though its theoretical underpinnings are not always easy to grasp.","In this work, we survey contemporary direct-search algorithms from a theoretical viewpoint, with the aim of highlighting the key theoretical features of these methods.","Our study goes beyond the classical, textbook cases and tackles the presence of nonsmoothness, noise, and constraints in the problem at hand.","In addition to reviewing classical results in the field, we provide new perspectives on existing results, as well as novel proofs that illustrate the versatility of direct-search schemes."],"url":"http://arxiv.org/abs/2403.05322v1","category":"math.OC"}
{"created":"2024-03-08 10:33:21","title":"A Decoupled Approach for Composite Sparse-plus-Smooth Penalized Optimization","abstract":"We consider a linear inverse problem whose solution is expressed as a sum of two components, one of them being smooth while the other presents sparse properties. This problem is solved by minimizing an objective function with a least square data-fidelity term and a different regularization term applied to each of the components. Sparsity is promoted with a $\\ell_1$ norm, while the other component is penalized by means of a $\\ell_2$ norm. We characterize the solution set of this composite optimization problem by stating a Representer Theorem. Consequently, we identify that solving the optimization problem can be decoupled, first identifying the sparse solution as a solution of a modified single-variable problem, then deducing the smooth component. We illustrate that this decoupled solving method can lead to significant computational speedups in applications, considering the problem of Dirac recovery over a smooth background with two-dimensional partial Fourier measurements.","sentences":["We consider a linear inverse problem whose solution is expressed as a sum of two components, one of them being smooth while the other presents sparse properties.","This problem is solved by minimizing an objective function with a least square data-fidelity term and a different regularization term applied to each of the components.","Sparsity is promoted with a $\\ell_1$ norm, while the other component is penalized by means of a $\\ell_2$ norm.","We characterize the solution set of this composite optimization problem by stating a Representer Theorem.","Consequently, we identify that solving the optimization problem can be decoupled, first identifying the sparse solution as a solution of a modified single-variable problem, then deducing the smooth component.","We illustrate that this decoupled solving method can lead to significant computational speedups in applications, considering the problem of Dirac recovery over a smooth background with two-dimensional partial Fourier measurements."],"url":"http://arxiv.org/abs/2403.05204v1","category":"eess.SP"}
{"created":"2024-03-08 18:59:58","title":"Can the Dirac deltas in dipole fields be ignored in classical interactions?","abstract":"When studying (or teaching) classical electromagnetism, one is bound to deal with the electric field of an ideal electric dipole, as well as its magnetic counterpart. A careful analysis then reveals that each of those fields must include, for consistency, a term proportional to a Dirac delta function localized at the position of the dipole. However, one is usually told not to worry about those terms since, as classical interactions always involve sources which are spatially separated, the Dirac-delta terms are only relevant for quantum mechanics, where they are directly related to important phenomena. In this work, we pose and solve a purely classical problem in electrostatics in which the Dirac-delta terms in the dipole fields are indispensable. It involves the computation of the interaction energy between a conductor with a spherical cavity and an (ideal) electric dipole located at the center of that cavity. We also solve its magnetic counterpart, replacing the conductor with a superconductor and the electric dipole with a magnetic one.","sentences":["When studying (or teaching) classical electromagnetism, one is bound to deal with the electric field of an ideal electric dipole, as well as its magnetic counterpart.","A careful analysis then reveals that each of those fields must include, for consistency, a term proportional to a Dirac delta function localized at the position of the dipole.","However, one is usually told not to worry about those terms since, as classical interactions always involve sources which are spatially separated, the Dirac-delta terms are only relevant for quantum mechanics, where they are directly related to important phenomena.","In this work, we pose and solve a purely classical problem in electrostatics in which the Dirac-delta terms in the dipole fields are indispensable.","It involves the computation of the interaction energy between a conductor with a spherical cavity and an (ideal) electric dipole located at the center of that cavity.","We also solve its magnetic counterpart, replacing the conductor with a superconductor and the electric dipole with a magnetic one."],"url":"http://arxiv.org/abs/2403.05537v1","category":"physics.class-ph"}
{"created":"2024-03-08 18:49:09","title":"Systematic analysis of relative phase extraction in one-dimensional Bose gases interferometry","abstract":"Spatially resolved relative phase measurement of two adjacent 1D Bose gases is enabled by matter-wave interference upon free expansion. However, longitudinal dynamics is typically ignored in the analysis of experimental data. We provide an analytical formula showing a correction to the readout of the relative phase due to longitudinal expansion and mixing with the common phase. We numerically assess the error propagation to the estimation of the gases' physical quantities such as correlation functions and temperature. Our work characterizes the reliability and robustness of interferometric measurements, directing us to the improvement of existing phase extraction methods necessary to observe new physical phenomena in cold-atomic quantum simulators.","sentences":["Spatially resolved relative phase measurement of two adjacent 1D Bose gases is enabled by matter-wave interference upon free expansion.","However, longitudinal dynamics is typically ignored in the analysis of experimental data.","We provide an analytical formula showing a correction to the readout of the relative phase due to longitudinal expansion and mixing with the common phase.","We numerically assess the error propagation to the estimation of the gases' physical quantities such as correlation functions and temperature.","Our work characterizes the reliability and robustness of interferometric measurements, directing us to the improvement of existing phase extraction methods necessary to observe new physical phenomena in cold-atomic quantum simulators."],"url":"http://arxiv.org/abs/2403.05528v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-08 18:37:38","title":"Orthorhombic metal carbide-borides MeC$_2$B$_{12}$ (Me=Mg, Ca, Sr) from first principles: structure, stability and mechanical properties","abstract":"First principle DFT simulations are employed to study structural and mechanical properties of orthorhombic B12-based metal carbide-borides. The simulations predict the existence of Ca- and Sr- based phases with the structure similar to that of experimentally observed earlier compound MeC$_2$B$_{12}$. Dynamical stability of both phases is demonstrated, and the phase MeC$_2$B$_{12}$ is found to be thermodynamically stable. According to simulations, Ca- and Sr- based phases have significantly enhanced mechanical characteristics, which suggest their potential application as superhard materials. Calculated shear and Young moduli of these phases are nearly 250 and 540 GPa, respectively, and estimated Vickers hardness is 45-55 GPa.","sentences":["First principle DFT simulations are employed to study structural and mechanical properties of orthorhombic B12-based metal carbide-borides.","The simulations predict the existence of Ca- and Sr- based phases with the structure similar to that of experimentally observed earlier compound MeC$_2$B$_{12}$. Dynamical stability of both phases is demonstrated, and the phase MeC$_2$B$_{12}$ is found to be thermodynamically stable.","According to simulations, Ca- and Sr- based phases have significantly enhanced mechanical characteristics, which suggest their potential application as superhard materials.","Calculated shear and Young moduli of these phases are nearly 250 and 540 GPa, respectively, and estimated Vickers hardness is 45-55 GPa."],"url":"http://arxiv.org/abs/2403.05517v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-08 18:15:48","title":"Design of Magnetic Polar Double-Double Perovskite Oxides through Cation Ordering","abstract":"Commencing from the centrosymmetric MnRMnSbO$_6$ compound, we explore the realm of magnetic polar double-double perovskite oxides characterized by significant ferroelectric polarization. Employing symmetry operations, first-principles methodologies, and Monte Carlo simulations, our investigation delves into the structural, magnetic, ferroelectric, and electronic attributes of the polar LaFeMnNiO$_6$ and LaTiMnNiO$_6$ compounds. The structural analysis uncovers that the paraelectric-ferroelectric phase transition is intricately linked to the Fe/Ti-displacement of square planar Fe/TiO$_4$. Notably, the magnetic LaFeMnNiO$_6$ and LaTiMnNiO$_6$ compounds demonstrate robust ferroelectric polarizations, measuring 20.0 $\\mu$C/cm$^2$ and 21.8 $\\mu$C/cm$^2$, respectively, accompanied by minimalist forbidden energy gaps of 1.40 eV and 1.18 eV using the GGA+U method. Furthermore, we pinpoint elevated magnetic transition temperatures for these compounds. Additionally, our study scrutinizes the energies associated with diverse spin configurations and identifies potential minimum decomposition pathways into stable oxides. This comprehensive analysis ensures the meticulous formation of the LaFeMnNiO$_6$ and LaTiMnNiO$_6$ compounds.","sentences":["Commencing from the centrosymmetric MnRMnSbO$_6$ compound, we explore the realm of magnetic polar double-double perovskite oxides characterized by significant ferroelectric polarization.","Employing symmetry operations, first-principles methodologies, and Monte Carlo simulations, our investigation delves into the structural, magnetic, ferroelectric, and electronic attributes of the polar LaFeMnNiO$_6$ and LaTiMnNiO$_6$ compounds.","The structural analysis uncovers that the paraelectric-ferroelectric phase transition is intricately linked to the Fe/Ti-displacement of square planar Fe/TiO$_4$. Notably, the magnetic LaFeMnNiO$_6$ and LaTiMnNiO$_6$ compounds demonstrate robust ferroelectric polarizations, measuring 20.0 $\\mu$C/cm$^2$ and 21.8 $\\mu$C/cm$^2$, respectively, accompanied by minimalist forbidden energy gaps of 1.40 eV and 1.18 eV using the GGA+U method.","Furthermore, we pinpoint elevated magnetic transition temperatures for these compounds.","Additionally, our study scrutinizes the energies associated with diverse spin configurations and identifies potential minimum decomposition pathways into stable oxides.","This comprehensive analysis ensures the meticulous formation of the LaFeMnNiO$_6$ and LaTiMnNiO$_6$ compounds."],"url":"http://arxiv.org/abs/2403.05498v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-08 18:08:55","title":"Absence of concordance in a simple self-interacting neutrino cosmology","abstract":"Some cosmic microwave background (CMB) data allow a cosmological scenario in which the free streaming of neutrinos is delayed until close to matter-radiation equality. Interestingly, recent analyses have revealed that large-scale structure (LSS) data also align with this scenario, discarding the possibility of an accidental feature in the CMB sky and calling for further investigation into the free-streaming nature of neutrinos. By assuming a simple representation of self-interacting neutrinos, we investigate whether this nonstandard scenario can accommodate a consistent cosmology for both the CMB power spectra and the large-scale distribution of galaxies simultaneously. Employing three different approaches - a profile likelihood exploration, a nested sampling method, and a heuristic Metropolis-Hasting approximation - we exhaustively explore the parameter space and demonstrate that galaxy data exacerbates the challenge already posed by the Planck polarization data for this nonstandard scenario. We find that the most conservative value of the Bayes factor disfavors the interactions among neutrinos over a $\\Lambda$CDM + $N_\\mathrm{eff}$ + $\\sum m_\\nu$ model with odds of $23:1000$ and that the difficulty of simultaneously fitting the galaxy and CMB data relates to the so-called $S_8$ discrepancy. Our analysis not only emphasizes the need to consider a broader range of phenomenologies in the early Universe but also highlights significant numerical and theoretical challenges ahead in uncovering the exact nature of the feature observed in the data or, ultimately, confirming the standard chronological evolution of the Universe.","sentences":["Some cosmic microwave background (CMB) data allow a cosmological scenario in which the free streaming of neutrinos is delayed until close to matter-radiation equality.","Interestingly, recent analyses have revealed that large-scale structure (LSS) data also align with this scenario, discarding the possibility of an accidental feature in the CMB sky and calling for further investigation into the free-streaming nature of neutrinos.","By assuming a simple representation of self-interacting neutrinos, we investigate whether this nonstandard scenario can accommodate a consistent cosmology for both the CMB power spectra and the large-scale distribution of galaxies simultaneously.","Employing three different approaches - a profile likelihood exploration, a nested sampling method, and a heuristic Metropolis-Hasting approximation - we exhaustively explore the parameter space and demonstrate that galaxy data exacerbates the challenge already posed by the Planck polarization data for this nonstandard scenario.","We find that the most conservative value of the Bayes factor disfavors the interactions among neutrinos over a $\\Lambda$CDM + $N_\\mathrm{eff}$ + $\\sum m_\\nu$ model with odds of $23:1000$ and that the difficulty of simultaneously fitting the galaxy and CMB data relates to the so-called $S_8$ discrepancy.","Our analysis not only emphasizes the need to consider a broader range of phenomenologies in the early Universe but also highlights significant numerical and theoretical challenges ahead in uncovering the exact nature of the feature observed in the data or, ultimately, confirming the standard chronological evolution of the Universe."],"url":"http://arxiv.org/abs/2403.05496v1","category":"astro-ph.CO"}
{"created":"2024-03-08 17:38:01","title":"Standard energy function for grain boundary orientation fundamental zone","abstract":"The success of grain boundary (GB) orientation fundamental zone (FZ) has connected GB structures across multiple crystallographic characters with their properties in a unique insight, but quantitative understandings of the structure-property relationship in the FZ are still lacking yet. Here, a theoretical derivation is proposed to transfer the well-known Read-Shockley relationship to the 3D FZ to form a standard energy function, which is proven to capture the simulated energy trends in a simple and accurate manner. The theorization therefore provides a basis for the establishment of modern GB energy functions.","sentences":["The success of grain boundary (GB) orientation fundamental zone (FZ) has connected GB structures across multiple crystallographic characters with their properties in a unique insight, but quantitative understandings of the structure-property relationship in the FZ are still lacking yet.","Here, a theoretical derivation is proposed to transfer the well-known Read-Shockley relationship to the 3D FZ to form a standard energy function, which is proven to capture the simulated energy trends in a simple and accurate manner.","The theorization therefore provides a basis for the establishment of modern GB energy functions."],"url":"http://arxiv.org/abs/2403.05474v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-08 17:23:44","title":"Incompleteness of Sinclair-type continuum flexible boundary conditions for atomistic fracture simulations","abstract":"The elastic field around a crack opening is known to be described by continuum linearised elasticity in leading order. In this work, we explicitly develop the next term in the atomistic asymptotic expansion in the case of a Mode III crack in anti-plane geometry. The aim of such an expansion is twofold. First, we show that the well-known flexible boundary condition ansatz due to Sinclair is incomplete, meaning that, in principle, employing it in atomistic fracture simulations is no better than using boundary conditions from continuum linearised elasticity. And secondly, the higher order far-field expansion can be employed as a boundary condition for high-accuracy atomistic simulations. To obtain our results, we develop an asymptotic expansion of the associated lattice Green's function. In an interesting departure from the recently developed theory for spatially homogeneous cases, this includes a novel notion of a discrete geometry predictor, which accounts for the peculiar discrete geometry near the crack tip.","sentences":["The elastic field around a crack opening is known to be described by continuum linearised elasticity in leading order.","In this work, we explicitly develop the next term in the atomistic asymptotic expansion in the case of a Mode III crack in anti-plane geometry.","The aim of such an expansion is twofold.","First, we show that the well-known flexible boundary condition ansatz due to Sinclair is incomplete, meaning that, in principle, employing it in atomistic fracture simulations is no better than using boundary conditions from continuum linearised elasticity.","And secondly, the higher order far-field expansion can be employed as a boundary condition for high-accuracy atomistic simulations.","To obtain our results, we develop an asymptotic expansion of the associated lattice Green's function.","In an interesting departure from the recently developed theory for spatially homogeneous cases, this includes a novel notion of a discrete geometry predictor, which accounts for the peculiar discrete geometry near the crack tip."],"url":"http://arxiv.org/abs/2403.05462v1","category":"math.AP"}
{"created":"2024-03-08 17:12:25","title":"Soft Theorems for Boostless Amplitudes","abstract":"We consider effective field theories (EFTs) of scalar fields with broken Lorentz boosts, which arise by taking the decoupling and flat-space limits of the EFT of inflation, and derive constraints that must be satisfied by the corresponding scattering amplitudes if there is an underlying non-linearly realised symmetry. We primarily concentrate on extended shift symmetries which depend on the space-time coordinates, and find that combinations of scattering amplitudes obey enhanced Adler zeros. That is, such combinations vanish as one external momentum is taken soft, with the rate at which they vanish dictated by the corresponding symmetry. In our soft theorem derivation, we pay particular care to the energy and momentum-conserving delta functions that arise due to space-time translations, and show that when acted upon by derivatives with respect to spatial momenta, they yield a tower of soft theorems which are ultimately required for closure of the underlying symmetry algebra. All of our soft theorems correspond to constraints that must be satisfied by on-shell amplitudes and, even for symmetries that depend on the time coordinate, our soft theorems only require derivatives to be taken with respect to spatial momenta. We perform a soft bootstrap procedure to find solutions to our soft theorems, and compare these solutions to what we find from an off-shell analysis using the coset construction.","sentences":["We consider effective field theories (EFTs) of scalar fields with broken Lorentz boosts, which arise by taking the decoupling and flat-space limits of the EFT of inflation, and derive constraints that must be satisfied by the corresponding scattering amplitudes if there is an underlying non-linearly realised symmetry.","We primarily concentrate on extended shift symmetries which depend on the space-time coordinates, and find that combinations of scattering amplitudes obey enhanced Adler zeros.","That is, such combinations vanish as one external momentum is taken soft, with the rate at which they vanish dictated by the corresponding symmetry.","In our soft theorem derivation, we pay particular care to the energy and momentum-conserving delta functions that arise due to space-time translations, and show that when acted upon by derivatives with respect to spatial momenta, they yield a tower of soft theorems which are ultimately required for closure of the underlying symmetry algebra.","All of our soft theorems correspond to constraints that must be satisfied by on-shell amplitudes and, even for symmetries that depend on the time coordinate, our soft theorems only require derivatives to be taken with respect to spatial momenta.","We perform a soft bootstrap procedure to find solutions to our soft theorems, and compare these solutions to what we find from an off-shell analysis using the coset construction."],"url":"http://arxiv.org/abs/2403.05459v1","category":"hep-th"}
{"created":"2024-03-08 16:19:52","title":"Fourier-transform infrared spectroscopy with undetected photons from high-gain spontaneous parametric down-conversion","abstract":"Fourier-transform infrared spectroscopy (FTIR) is an indispensable analytical method that allows label-free identification of substances via fundamental molecular vibrations. However, the sensitivity of FTIR is often limited by the low efficiency of mid-infrared (MIR) photodetectors. SU(1,1) interferometry has previously enabled FTIR with undetected MIR photons via spontaneous parametric down-conversion in the low-parametric-gain regime, where the number of photons per mode is much less than one and sensitive photodetectors are needed. In this work, we develop a high-parametric-gain SU(1,1) interferometer for MIR-range FTIR with undetected photons. Using our new method, we demonstrate three major advantages: a high photon number at the interferometer output, a considerably lower photon number at the sample, and improved interference contrast. In addition, we analyze different methods to broaden the spectral range of the interferometer by aperiodic poling and temperature gradient in the gain medium. Exploiting the broadband SU(1,1) interferometer, we measure and evaluate the MIR absorption spectra of polymers in the 3-{\\mu}m region.","sentences":["Fourier-transform infrared spectroscopy (FTIR) is an indispensable analytical method that allows label-free identification of substances via fundamental molecular vibrations.","However, the sensitivity of FTIR is often limited by the low efficiency of mid-infrared (MIR) photodetectors.","SU(1,1) interferometry has previously enabled FTIR with undetected MIR photons via spontaneous parametric down-conversion in the low-parametric-gain regime, where the number of photons per mode is much less than one and sensitive photodetectors are needed.","In this work, we develop a high-parametric-gain SU(1,1) interferometer for MIR-range FTIR with undetected photons.","Using our new method, we demonstrate three major advantages: a high photon number at the interferometer output, a considerably lower photon number at the sample, and improved interference contrast.","In addition, we analyze different methods to broaden the spectral range of the interferometer by aperiodic poling and temperature gradient in the gain medium.","Exploiting the broadband SU(1,1) interferometer, we measure and evaluate the MIR absorption spectra of polymers in the 3-{\\mu}m region."],"url":"http://arxiv.org/abs/2403.05423v1","category":"physics.optics"}
{"created":"2024-03-08 16:13:33","title":"Large deviation principle for the largest eigenvalue of random matrices with a variance profile","abstract":"We establish large deviation principles for the largest eigenvalue of large random matrices with variance profiles.","sentences":["We establish large deviation principles for the largest eigenvalue of large random matrices with variance profiles."],"url":"http://arxiv.org/abs/2403.05413v1","category":"math.PR"}
{"created":"2024-03-08 14:35:20","title":"Using Gaussian Processes to detect AGN flares","abstract":"A key feature of active galactic nuclei (AGN) is their variability across all wavelengths. Typically, AGN vary by a few tenths of a magnitude or more over periods lasting from hours to years. By contrast, extreme variability of AGN -- large luminosity changes that are a significant departure from the baseline variability -- are known as AGN flares. These events are rare and their timescales poorly constrained, with most of the literature focusing on individual events. It has been suggested that extreme AGN variability including flares can provide insights into the accretion processes in the disk. With surveys such as the Legacy Survey of Space and Time (LSST) promising millions of transient detections per night in the coming decade, there is a need for fast and efficient classification of AGN flares. The problem with the systematic detection of AGN flares is the requirement to detect them against a stochastically variable baseline; the ability to define a signal as a significant departure from the ever-present variability is a statistical challenge. Recently, Gaussian Processes (GPs) have revolutionised the analysis of time-series data in many areas of astronomical research. They have, however, seen limited uptake within the field of transient detection and classification. Here we investigate the efficacy of Gaussian Processes to detect AGN flares in both simulated and real optical light curves. We show that GP analysis can successfully detect AGN flares with a false-positive rate of less than seven per cent, and we present examples of AGN light curves that show extreme variability.","sentences":["A key feature of active galactic nuclei (AGN) is their variability across all wavelengths.","Typically, AGN vary by a few tenths of a magnitude or more over periods lasting from hours to years.","By contrast, extreme variability of AGN -- large luminosity changes that are a significant departure from the baseline variability -- are known as AGN flares.","These events are rare and their timescales poorly constrained, with most of the literature focusing on individual events.","It has been suggested that extreme AGN variability including flares can provide insights into the accretion processes in the disk.","With surveys such as the Legacy Survey of Space and Time (LSST) promising millions of transient detections per night in the coming decade, there is a need for fast and efficient classification of AGN flares.","The problem with the systematic detection of AGN flares is the requirement to detect them against a stochastically variable baseline; the ability to define a signal as a significant departure from the ever-present variability is a statistical challenge.","Recently, Gaussian Processes (GPs) have revolutionised the analysis of time-series data in many areas of astronomical research.","They have, however, seen limited uptake within the field of transient detection and classification.","Here we investigate the efficacy of Gaussian Processes to detect AGN flares in both simulated and real optical light curves.","We show that GP analysis can successfully detect AGN flares with a false-positive rate of less than seven per cent, and we present examples of AGN light curves that show extreme variability."],"url":"http://arxiv.org/abs/2403.05354v1","category":"astro-ph.HE"}
{"created":"2024-03-08 14:06:23","title":"Swift monitoring of GK Persei during the 2018 dwarf nova outburst","abstract":"The old nova and intermediate polar (IP) GK Persei underwent one of its recurrent dwarf nova (DN) outbursts in 2018. We proposed monitoring it in UV and X-rays with the Neil Gehrels Swift Observatory, starting less than six days after the eruption, until 16 days after the eruption ended. For the first time we could follow the decay to minimum light UV and X-rays. We present the timing and spectral analysis, comparing the results with the previous outbursts and with the quiescent status. We confirm the spin modulation in X-rays with a period 351.325(9) s, only in the 2-10 keV range. The period was not detected in the 0.3-2 keV range and in the UV band, suggesting that the soft portion of the X-ray spectrum in GK Per does not originate near the poles, but in a wind or circumstellar material. The amplitude of the modulation was less prominent than in 2015, a fact that seems correlated with a lower average mass accretion rate. The spectral fits are consistent with a mass accretion rate increasing by a factor of 2 from rise to maximum and decreasing during the return to minimum, following the trend of the modulation amplitude. The maximum plasma temperature is higher than the Swift XRT energy range of 0.3-10 keV, thus it is not well constrained, but our spectral fits indicate that it may have varied irregularly during the outburst.","sentences":["The old nova and intermediate polar (IP) GK Persei underwent one of its recurrent dwarf nova (DN) outbursts in 2018.","We proposed monitoring it in UV and X-rays with the Neil Gehrels Swift Observatory, starting less than six days after the eruption, until 16 days after the eruption ended.","For the first time we could follow the decay to minimum light UV and X-rays.","We present the timing and spectral analysis, comparing the results with the previous outbursts and with the quiescent status.","We confirm the spin modulation in X-rays with a period 351.325(9) s, only in the 2-10 keV range.","The period was not detected in the 0.3-2 keV range and in the UV band, suggesting that the soft portion of the X-ray spectrum in GK Per does not originate near the poles, but in a wind or circumstellar material.","The amplitude of the modulation was less prominent than in 2015, a fact that seems correlated with a lower average mass accretion rate.","The spectral fits are consistent with a mass accretion rate increasing by a factor of 2 from rise to maximum and decreasing during the return to minimum, following the trend of the modulation amplitude.","The maximum plasma temperature is higher than the Swift XRT energy range of 0.3-10 keV, thus it is not well constrained, but our spectral fits indicate that it may have varied irregularly during the outburst."],"url":"http://arxiv.org/abs/2403.05328v1","category":"astro-ph.HE"}
{"created":"2024-03-08 13:24:15","title":"Inverse semigroups of separated graphs and associated algebras","abstract":"In this paper we introduce an inverse semigroup $\\mathcal{S}(E,C)$ associated to a separated graph $(E,C)$ and describe its internal structure. In particular we show that it is strongly $E^*$-unitary and can be realized as a partial semidirect product of the form $\\mathcal{Y}\\rtimes\\mathbb{F}$ for a certain partial action of the free group $\\mathbb{F}=\\mathbb{F}(E^1)$ on the edges of $E$ on a semilattice $\\mathcal{Y}$ realizing the idempotents of $\\mathcal{S}(E,C)$. In addition we also describe the spectrum as well as the tight spectrum of $\\mathcal{Y}$.   We then use the inverse semigroup $\\mathcal{S}(E,C)$ to describe several ''tame'' algebras associated to $(E,C)$, including its Cohn algebra, its Leavitt-path algebra, and analogues in the realm of $C^*$-algebras, like the tame $C^*$-algebra $\\mathcal{O}(E,C)$ and its Toeplitz extension $\\mathcal{T}(E,C)$, proving that these algebras are canonically isomorphic to certain algebras attached to $\\mathcal{S}(E,C)$. Our structural results on $\\mathcal{S}(E,C)$ will then imply certain natural structure results for these algebras, like their description as partial crossed products.","sentences":["In this paper we introduce an inverse semigroup $\\mathcal{S}(E,C)$ associated to a separated graph $(E,C)$ and describe its internal structure.","In particular we show that it is strongly $E^*$-unitary and can be realized as a partial semidirect product of the form $\\mathcal{Y}\\rtimes\\mathbb{F}$ for a certain partial action of the free group $\\mathbb{F}=\\mathbb{F}(E^1)$ on the edges of $E$ on a semilattice $\\mathcal{Y}$ realizing the idempotents of $\\mathcal{S}(E,C)$.","In addition we also describe the spectrum as well as the tight spectrum of $\\mathcal{Y}$.   We then use the inverse semigroup $\\mathcal{S}(E,C)$ to describe several ''tame'' algebras associated to $(E,C)$, including its Cohn algebra, its Leavitt-path algebra, and analogues in the realm of $C^*$-algebras, like the tame $C^*$-algebra $\\mathcal{O}(E,C)$ and its Toeplitz extension $\\mathcal{T}(E,C)$, proving that these algebras are canonically isomorphic to certain algebras attached to $\\mathcal{S}(E,C)$. Our structural results on $\\mathcal{S}(E,C)$ will then imply certain natural structure results for these algebras, like their description as partial crossed products."],"url":"http://arxiv.org/abs/2403.05295v1","category":"math.OA"}
{"created":"2024-03-08 13:16:46","title":"A discrete formulation for three-dimensional winding number","abstract":"For a smooth map $g\\colon X \\to U(N)$, where $X$ is a three-dimensional, oriented, and closed manifold, the winding number or the map's degree is defined by $W_3 = \\frac{1}{24\\pi^2} \\int_{X} \\mathrm{Tr}\\left[(g^{-1}dg)^3\\right]$. We introduce a method to compute $W_3$ using a discrete approximation of $X$. in which the result is manifestly This approach provides a formulation in which quantization is manifest.","sentences":["For a smooth map $g\\colon X \\to U(N)$, where $X$ is a three-dimensional, oriented, and closed manifold, the winding number or the map's degree is defined by $W_3 = \\frac{1}{24\\pi^2} \\int_{X} \\mathrm{Tr}\\left[(g^{-1}dg)^3\\right]$. We introduce a method to compute $W_3$ using a discrete approximation of $X$. in which the result is manifestly This approach provides a formulation in which quantization is manifest."],"url":"http://arxiv.org/abs/2403.05291v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-08 13:14:51","title":"Neutrino fluxes from different classes of galactic sources","abstract":"We estimate the neutrino flux from different kinds of galactic sources and compare it with the recently diffuse neutrino flux detected by IceCube. We find that the flux from these sources may contribute to ~ 20% of the IceCube neutrino flux. Most of the sources selected in this work populate the southern hemisphere, therefore a detector like KM3NeT could help in resolving the sources out of the observed diffused galactic neutrino flux.","sentences":["We estimate the neutrino flux from different kinds of galactic sources and compare it with the recently diffuse neutrino flux detected by IceCube.","We find that the flux from these sources may contribute to ~ 20% of the IceCube neutrino flux.","Most of the sources selected in this work populate the southern hemisphere, therefore a detector like KM3NeT could help in resolving the sources out of the observed diffused galactic neutrino flux."],"url":"http://arxiv.org/abs/2403.05288v1","category":"astro-ph.HE"}
{"created":"2024-03-08 13:05:03","title":"Proton Helicity GPDs from Lattice QCD","abstract":"First lattice QCD calculations of $x$-dependent GPD have been performed in the (symmetric) Breit frame, where the momentum transfer is evenly divided between the initial and final hadron states. However, employing the asymmetric frame, we are able to obtain proton GPDs for multiple momentum transfers in a computationally efficient setup. In these proceedings, we focus on the helicity twist-2 GPD at zero skewness that gives access to the $\\widetilde{H}$ GPD. We will cover the implementation of the asymmetric frame, its comparison to the Breit frame, and the dependence of the GPD on the squared four-momentum transfer, $-t$. The calculation is performed on an $N_f = 2+1+1$ ensemble of twisted mass fermions with a clover improvement. The mass of the pion for this ensemble is roughly 260 MeV.","sentences":["First lattice QCD calculations of $x$-dependent GPD have been performed in the (symmetric) Breit frame, where the momentum transfer is evenly divided between the initial and final hadron states.","However, employing the asymmetric frame, we are able to obtain proton GPDs for multiple momentum transfers in a computationally efficient setup.","In these proceedings, we focus on the helicity twist-2 GPD at zero skewness that gives access to the $\\widetilde{H}$ GPD.","We will cover the implementation of the asymmetric frame, its comparison to the Breit frame, and the dependence of the GPD on the squared four-momentum transfer, $-t$.","The calculation is performed on an $N_f = 2+1+1$ ensemble of twisted mass fermions with a clover improvement.","The mass of the pion for this ensemble is roughly 260 MeV."],"url":"http://arxiv.org/abs/2403.05282v1","category":"hep-lat"}
{"created":"2024-03-08 10:21:19","title":"Effects of gravitational particle production on Higgs portal dark matter","abstract":"The gravitational interaction is ubiquitous and the effect of gravitational particle production necessarily contributes to the dark matter abundance. A simple candidate of dark matter is a scalar particle, whose only renormalizable interaction is the Higgs portal coupling. We show that the abundance of Higgs portal dark matter is significantly affected by the gravitational production effect. In particular, the gravitational production from the coherently oscillating inflaton field during the reheating often gives dominant contribution.","sentences":["The gravitational interaction is ubiquitous and the effect of gravitational particle production necessarily contributes to the dark matter abundance.","A simple candidate of dark matter is a scalar particle, whose only renormalizable interaction is the Higgs portal coupling.","We show that the abundance of Higgs portal dark matter is significantly affected by the gravitational production effect.","In particular, the gravitational production from the coherently oscillating inflaton field during the reheating often gives dominant contribution."],"url":"http://arxiv.org/abs/2403.05199v1","category":"hep-ph"}
{"created":"2024-03-08 09:08:16","title":"Experimental Evidence of Direct Exchange Interaction Mediating Intramolecular Singlet Fission in Weakly-Coupled Dimers","abstract":"The electronic interaction between an optically active singlet state ($S_1S_0$) and a dark state of singlet multiplicity, known as correlated triplet pair ($^1[TT]$), plays a crucial role in the effective transformation from $S_1S_0$ to $^1[TT]$ during intramolecular singlet fission (iSF). This process is understood through mechanisms such as direct exchange coupling and incoherent processes that involve super-exchange coupling through charge-transfer states. However, most insights into these mechanisms are derived from theoretical studies due to the difficulties in obtaining experimental evidence. In this study, we investigate the excited-state interactions between $S_1S_0$ and $^1[TT]$ in spiro-conjugated iSF sensitizers by employing transient two-dimensional electronic spectroscopy. This approach allows us to focus on the early stages of the conversion from $S_1S_0$ to $^1[TT]$. Upon optical excitation, a superposition of $S_1S_0$ and $^1[TT]$ is created, which gradually transitions to favor $^1[TT]$ within the characteristic time frames of iSF. The observed high-order signals indicate circular repopulation dynamic that effectively reinitiates the iSF process from higher energy electronic states. Our findings, supported by semi-quantum-mechanical simulations of the experimental data, suggest the presence of a direct iSF mechanism in the dimers, facilitated by weak non-adiabatic coupling between $S_1S_0$ and $^1[TT]$. This experiment provides new insights into the equilibrium between the two electronic states, a phenomenon previously understood primarily through theoretical models.","sentences":["The electronic interaction between an optically active singlet state ($S_1S_0$) and a dark state of singlet multiplicity, known as correlated triplet pair ($^1[TT]$), plays a crucial role in the effective transformation from $S_1S_0$ to $^1[TT]$ during intramolecular singlet fission (iSF).","This process is understood through mechanisms such as direct exchange coupling and incoherent processes that involve super-exchange coupling through charge-transfer states.","However, most insights into these mechanisms are derived from theoretical studies due to the difficulties in obtaining experimental evidence.","In this study, we investigate the excited-state interactions between $S_1S_0$ and $^1[TT]$ in spiro-conjugated iSF sensitizers by employing transient two-dimensional electronic spectroscopy.","This approach allows us to focus on the early stages of the conversion from $S_1S_0$ to $^1[TT]$. Upon optical excitation, a superposition of $S_1S_0$ and $^1[TT]$ is created, which gradually transitions to favor $^1[TT]$ within the characteristic time frames of iSF.","The observed high-order signals indicate circular repopulation dynamic that effectively reinitiates the iSF process from higher energy electronic states.","Our findings, supported by semi-quantum-mechanical simulations of the experimental data, suggest the presence of a direct iSF mechanism in the dimers, facilitated by weak non-adiabatic coupling between $S_1S_0$ and $^1[TT]$. This experiment provides new insights into the equilibrium between the two electronic states, a phenomenon previously understood primarily through theoretical models."],"url":"http://arxiv.org/abs/2403.05163v1","category":"physics.chem-ph"}
