{"created":"2024-02-19 18:58:26","title":"LTL learning on GPUs","abstract":"Linear temporal logic (LTL) is widely used in industrial verification. LTL formulae can be learned from traces. Scaling LTL formula learning is an open problem. We implement the first GPU-based LTL learner using a novel form of enumerative program synthesis. The learner is sound and complete. Our benchmarks indicate that it handles traces at least 2048 times more numerous, and on average at least 46 times faster than existing state-of-the-art learners. This is achieved with, among others, novel branch-free LTL semantics that has $O(\\log n)$ time complexity, where $n$ is trace length, while previous implementations are $O(n^2)$ or worse (assuming bitwise boolean operations and shifts by powers of 2 have unit costs -- a realistic assumption on modern processors).","sentences":["Linear temporal logic (LTL) is widely used in industrial verification.","LTL formulae can be learned from traces.","Scaling LTL formula learning is an open problem.","We implement the first GPU-based LTL learner using a novel form of enumerative program synthesis.","The learner is sound and complete.","Our benchmarks indicate that it handles traces at least 2048 times more numerous, and on average at least 46 times faster than existing state-of-the-art learners.","This is achieved with, among others, novel branch-free LTL semantics that has $O(\\log n)$ time complexity, where $n$ is trace length, while previous implementations are $O(n^2)$ or worse (assuming bitwise boolean operations and shifts by powers of 2 have unit costs -- a realistic assumption on modern processors)."],"url":"http://arxiv.org/abs/2402.12373v1","category":"cs.PL"}
{"created":"2024-02-19 18:56:44","title":"AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies","abstract":"Humans regularly engage in analogical thinking, relating personal experiences to current situations ($X$ is analogous to $Y$ because of $Z$). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose ANALOBENCH, a benchmark to determine analogical reasoning ability in LMs. Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios. We test a broad collection of proprietary models (e.g., GPT family, Claude V2) and open source models such as LLaMA2. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack. We hope these observations encourage further research in this field.","sentences":["Humans regularly engage in analogical thinking, relating personal experiences to current situations ($X$ is analogous to $Y$ because of $Z$).","Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively.","Can language models (LMs) do the same?","To answer this question, we propose ANALOBENCH, a benchmark to determine analogical reasoning ability in LMs.","Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios.","We test a broad collection of proprietary models (e.g., GPT family, Claude V2) and open source models such as LLaMA2.","As in prior results, scaling up LMs results in some performance boosts.","Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack.","We hope these observations encourage further research in this field."],"url":"http://arxiv.org/abs/2402.12370v1","category":"cs.CL"}
{"created":"2024-02-19 18:56:35","title":"Short-Period Variables in TESS Full-Frame Image Light Curves Identified via Convolutional Neural Networks","abstract":"The Transiting Exoplanet Survey Satellite (TESS) mission measured light from stars in ~85% of the sky throughout its two-year primary mission, resulting in millions of TESS 30-minute cadence light curves to analyze in the search for transiting exoplanets. To search this vast dataset, we aim to provide an approach that is both computationally efficient, produces highly performant predictions, and minimizes the required human search effort. We present a convolutional neural network that we train to identify short period variables. To make a prediction for a given light curve, our network requires no prior target parameters identified using other methods. Our network performs inference on a TESS 30-minute cadence light curve in ~5ms on a single GPU, enabling large scale archival searches. We present a collection of 14156 short-period variables identified by our network. The majority of our identified variables fall into two prominent populations, one of short-period main sequence binaries and another of Delta Scuti stars. Our neural network model and related code is additionally provided as open-source code for public use and extension.","sentences":["The Transiting Exoplanet Survey Satellite (TESS) mission measured light from stars in ~85% of the sky throughout its two-year primary mission, resulting in millions of TESS 30-minute cadence light curves to analyze in the search for transiting exoplanets.","To search this vast dataset, we aim to provide an approach that is both computationally efficient, produces highly performant predictions, and minimizes the required human search effort.","We present a convolutional neural network that we train to identify short period variables.","To make a prediction for a given light curve, our network requires no prior target parameters identified using other methods.","Our network performs inference on a TESS 30-minute cadence light curve in ~5ms on a single GPU, enabling large scale archival searches.","We present a collection of 14156 short-period variables identified by our network.","The majority of our identified variables fall into two prominent populations, one of short-period main sequence binaries and another of Delta Scuti stars.","Our neural network model and related code is additionally provided as open-source code for public use and extension."],"url":"http://arxiv.org/abs/2402.12369v1","category":"astro-ph.SR"}
{"created":"2024-02-19 18:55:16","title":"A synthetic data approach for domain generalization of NLI models","abstract":"Natural Language Inference (NLI) remains an important benchmark task for LLMs. NLI datasets are a springboard for transfer learning to other semantic tasks, and NLI models are standard tools for identifying the faithfulness of model-generated text. There are several large scale NLI datasets today, and models have improved greatly by hill-climbing on these collections. Yet their realistic performance on out-of-distribution/domain data is less well-understood. We present an in-depth exploration of the problem of domain generalization of NLI models. We demonstrate a new approach for generating synthetic NLI data in diverse domains and lengths, so far not covered by existing training sets. The resulting examples have meaningful premises, the hypotheses are formed in creative ways rather than simple edits to a few premise tokens, and the labels have high accuracy. We show that models trained on this data ($685$K synthetic examples) have the best generalization to completely new downstream test settings. On the TRUE benchmark, a T5-small model trained with our data improves around $7\\%$ on average compared to training on the best alternative dataset. The improvements are more pronounced for smaller models, while still meaningful on a T5 XXL model. We also demonstrate gains on test sets when in-domain training data is augmented with our domain-general synthetic data.","sentences":["Natural Language Inference (NLI) remains an important benchmark task for LLMs.","NLI datasets are a springboard for transfer learning to other semantic tasks, and NLI models are standard tools for identifying the faithfulness of model-generated text.","There are several large scale NLI datasets today, and models have improved greatly by hill-climbing on these collections.","Yet their realistic performance on out-of-distribution/domain data is less well-understood.","We present an in-depth exploration of the problem of domain generalization of NLI models.","We demonstrate a new approach for generating synthetic NLI data in diverse domains and lengths, so far not covered by existing training sets.","The resulting examples have meaningful premises, the hypotheses are formed in creative ways rather than simple edits to a few premise tokens, and the labels have high accuracy.","We show that models trained on this data ($685$K synthetic examples) have the best generalization to completely new downstream test settings.","On the TRUE benchmark, a T5-small model trained with our data improves around $7\\%$ on average compared to training on the best alternative dataset.","The improvements are more pronounced for smaller models, while still meaningful on a T5 XXL model.","We also demonstrate gains on test sets when in-domain training data is augmented with our domain-general synthetic data."],"url":"http://arxiv.org/abs/2402.12368v1","category":"cs.CL"}
{"created":"2024-02-19 18:53:54","title":"A Critical Evaluation of AI Feedback for Aligning Large Language Models","abstract":"Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models. RLAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation. Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF pipelines. More generally, we find that the gains from RLAIF vary substantially across base model families, test-time evaluation protocols, and critic models. Finally, we provide a mechanistic explanation for when SFT may outperform the full two-step RLAIF pipeline as well as suggestions for making RLAIF maximally useful in practice.","sentences":["Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models.","RLAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL), using feedback from a critic model.","While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback.","We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation.","Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF pipelines.","More generally, we find that the gains from RLAIF vary substantially across base model families, test-time evaluation protocols, and critic models.","Finally, we provide a mechanistic explanation for when SFT may outperform the full two-step RLAIF pipeline as well as suggestions for making RLAIF maximally useful in practice."],"url":"http://arxiv.org/abs/2402.12366v1","category":"cs.LG"}
{"created":"2024-02-19 18:52:13","title":"Universal Physics Transformers","abstract":"Deep neural network based surrogates for partial differential equations have recently gained increased interest. However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space representation at any point in space-time. We demonstrate the efficacy of UPTs in mesh-based fluid simulations, steady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-based dynamics. Project page: https://ml-jku.github.io/UPT","sentences":["Deep neural network based surrogates for partial differential equations have recently gained increased interest.","However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar.","A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics.","We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes.","UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles.","UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques.","Finally, UPTs allow for queries of the latent space representation at any point in space-time.","We demonstrate the efficacy of UPTs in mesh-based fluid simulations, steady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-based dynamics.","Project page: https://ml-jku.github.io/UPT"],"url":"http://arxiv.org/abs/2402.12365v1","category":"cs.LG"}
{"created":"2024-02-19 18:47:56","title":"Nonlinear Discrete-Time Observers with Physics-Informed Neural Networks","abstract":"We use Physics-Informed Neural Networks (PINNs) to solve the discrete-time nonlinear observer state estimation problem. Integrated within a single-step exact observer linearization framework, the proposed PINN approach aims at learning a nonlinear state transformation map by solving a system of inhomogeneous functional equations. The performance of the proposed PINN approach is assessed via two illustrative case studies for which the observer linearizing transformation map can be derived analytically. We also perform an uncertainty quantification analysis for the proposed PINN scheme and we compare it with conventional power-series numerical implementations, which rely on the computation of a power series solution.","sentences":["We use Physics-Informed Neural Networks (PINNs) to solve the discrete-time nonlinear observer state estimation problem.","Integrated within a single-step exact observer linearization framework, the proposed PINN approach aims at learning a nonlinear state transformation map by solving a system of inhomogeneous functional equations.","The performance of the proposed PINN approach is assessed via two illustrative case studies for which the observer linearizing transformation map can be derived analytically.","We also perform an uncertainty quantification analysis for the proposed PINN scheme and we compare it with conventional power-series numerical implementations, which rely on the computation of a power series solution."],"url":"http://arxiv.org/abs/2402.12360v1","category":"math.NA"}
{"created":"2024-02-19 18:33:49","title":"LoRA+: Efficient Low Rank Adaptation of Large Models","abstract":"In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$ 2X SpeedUp), at the same computational cost as LoRA.","sentences":["In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al.","(2021) leads to suboptimal finetuning of models with large width (embedding dimension).","This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate.","Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning.","We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA","adapter matrices A and B with a well-chosen ratio.","We call this proposed algorithm LoRA$+$.","In our extensive experiments, LoRA$+$ improves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$ 2X SpeedUp), at the same computational cost as LoRA."],"url":"http://arxiv.org/abs/2402.12354v1","category":"cs.LG"}
{"created":"2024-02-19 18:23:36","title":"GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations","abstract":"As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we investigate two key problems: (1) Characterizing game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios; (2) Open-source LLMs, e.g., CodeLlama-34b-Instruct, are less competitive than commercial LLMs, e.g., GPT-4, in complex games. In addition, code-pretraining greatly benefits strategic reasoning, while advanced reasoning methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always help. Detailed error profiles are also provided for a better understanding of LLMs' behavior.","sentences":["As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial.","This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents.","We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios.","Then, we investigate two key problems: (1) Characterizing game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning evaluation.","We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios; (2) Open-source LLMs, e.g., CodeLlama-34b-Instruct, are less competitive than commercial LLMs, e.g., GPT-4, in complex games.","In addition, code-pretraining greatly benefits strategic reasoning, while advanced reasoning methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always help.","Detailed error profiles are also provided for a better understanding of LLMs' behavior."],"url":"http://arxiv.org/abs/2402.12348v1","category":"cs.CL"}
{"created":"2024-02-19 18:16:51","title":"Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!","abstract":"Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.","sentences":["Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans.","However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation.","This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training.","Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin.","Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment."],"url":"http://arxiv.org/abs/2402.12343v1","category":"cs.CL"}
{"created":"2024-02-19 18:11:37","title":"An Adversarial Approach to Evaluating the Robustness of Event Identification Models","abstract":"Intelligent machine learning approaches are finding active use for event detection and identification that allow real-time situational awareness. Yet, such machine learning algorithms have been shown to be susceptible to adversarial attacks on the incoming telemetry data. This paper considers a physics-based modal decomposition method to extract features for event classification and focuses on interpretable classifiers including logistic regression and gradient boosting to distinguish two types of events: load loss and generation loss. The resulting classifiers are then tested against an adversarial algorithm to evaluate their robustness. The adversarial attack is tested in two settings: the white box setting, wherein the attacker knows exactly the classification model; and the gray box setting, wherein the attacker has access to historical data from the same network as was used to train the classifier, but does not know the classification model. Thorough experiments on the synthetic South Carolina 500-bus system highlight that a relatively simpler model such as logistic regression is more susceptible to adversarial attacks than gradient boosting.","sentences":["Intelligent machine learning approaches are finding active use for event detection and identification that allow real-time situational awareness.","Yet, such machine learning algorithms have been shown to be susceptible to adversarial attacks on the incoming telemetry data.","This paper considers a physics-based modal decomposition method to extract features for event classification and focuses on interpretable classifiers including logistic regression and gradient boosting to distinguish two types of events: load loss and generation loss.","The resulting classifiers are then tested against an adversarial algorithm to evaluate their robustness.","The adversarial attack is tested in two settings: the white box setting, wherein the attacker knows exactly the classification model; and the gray box setting, wherein the attacker has access to historical data from the same network as was used to train the classifier, but does not know the classification model.","Thorough experiments on the synthetic South Carolina 500-bus system highlight that a relatively simpler model such as logistic regression is more susceptible to adversarial attacks than gradient boosting."],"url":"http://arxiv.org/abs/2402.12338v1","category":"eess.SY"}
{"created":"2024-02-19 18:09:48","title":"Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models","abstract":"Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the VLM is required. The code and robust models are available at https://github.com/chs20/RobustVLM","sentences":["Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks.","Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality.","These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem.","The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e.g. LLaVA and OpenFlamingo.","We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP.","In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one.","No retraining or fine-tuning of the VLM is required.","The code and robust models are available at https://github.com/chs20/RobustVLM"],"url":"http://arxiv.org/abs/2402.12336v1","category":"cs.LG"}
{"created":"2024-02-19 18:02:10","title":"Generating Survival Interpretable Trajectories and Data","abstract":"A new model for generating survival trajectories and data based on applying an autoencoder of a specific structure is proposed. It solves three tasks. First, it provides predictions in the form of the expected event time and the survival function for a new generated feature vector on the basis of the Beran estimator. Second, the model generates additional data based on a given training set that would supplement the original dataset. Third, the most important, it generates a prototype time-dependent trajectory for an object, which characterizes how features of the object could be changed to achieve a different time to an event. The trajectory can be viewed as a type of the counterfactual explanation. The proposed model is robust during training and inference due to a specific weighting scheme incorporating into the variational autoencoder. The model also determines the censored indicators of new generated data by solving a classification task. The paper demonstrates the efficiency and properties of the proposed model using numerical experiments on synthetic and real datasets. The code of the algorithm implementing the proposed model is publicly available.","sentences":["A new model for generating survival trajectories and data based on applying an autoencoder of a specific structure is proposed.","It solves three tasks.","First, it provides predictions in the form of the expected event time and the survival function for a new generated feature vector on the basis of the Beran estimator.","Second, the model generates additional data based on a given training set that would supplement the original dataset.","Third, the most important, it generates a prototype time-dependent trajectory for an object, which characterizes how features of the object could be changed to achieve a different time to an event.","The trajectory can be viewed as a type of the counterfactual explanation.","The proposed model is robust during training and inference due to a specific weighting scheme incorporating into the variational autoencoder.","The model also determines the censored indicators of new generated data by solving a classification task.","The paper demonstrates the efficiency and properties of the proposed model using numerical experiments on synthetic and real datasets.","The code of the algorithm implementing the proposed model is publicly available."],"url":"http://arxiv.org/abs/2402.12331v1","category":"cs.LG"}
{"created":"2024-02-19 18:01:36","title":"Query-Based Adversarial Prompt Generation","abstract":"Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.","sentences":["Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior.","Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models.","We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks.","We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability."],"url":"http://arxiv.org/abs/2402.12329v1","category":"cs.CL"}
{"created":"2024-02-19 18:00:53","title":"Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents","abstract":"Recent advancements have shown that agents powered by large language models (LLMs) possess capabilities to simulate human behaviors and societal dynamics. However, the potential for LLM agents to spontaneously establish collaborative relationships in the absence of explicit instructions has not been studied. To address this gap, we conduct three case studies, revealing that LLM agents are capable of spontaneously forming collaborations even within competitive settings. This finding not only demonstrates the capacity of LLM agents to mimic competition and cooperation in human societies but also validates a promising vision of computational social science. Specifically, it suggests that LLM agents could be utilized to model human social interactions, including those with spontaneous collaborations, thus offering insights into social phenomena. The source codes for this study are available at https://github.com/wuzengqing001225/SABM_ShallWeTalk .","sentences":["Recent advancements have shown that agents powered by large language models (LLMs) possess capabilities to simulate human behaviors and societal dynamics.","However, the potential for LLM agents to spontaneously establish collaborative relationships in the absence of explicit instructions has not been studied.","To address this gap, we conduct three case studies, revealing that LLM agents are capable of spontaneously forming collaborations even within competitive settings.","This finding not only demonstrates the capacity of LLM agents to mimic competition and cooperation in human societies but also validates a promising vision of computational social science.","Specifically, it suggests that LLM agents could be utilized to model human social interactions, including those with spontaneous collaborations, thus offering insights into social phenomena.","The source codes for this study are available at https://github.com/wuzengqing001225/SABM_ShallWeTalk ."],"url":"http://arxiv.org/abs/2402.12327v1","category":"cs.AI"}
{"created":"2024-02-19 17:44:35","title":"Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness","abstract":"The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning. In this scenario, the learner's objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender, when it comes to the newly introduced tasks. A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework. Nevertheless, it's crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions. In this paper, to tackle the fairness-aware online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by incorporating long-term fairness constraints into a strongly adapted loss regret framework. Moreover, to determine an optimal model parameter at each time step, we introduce an innovative adaptive fairness-aware online meta-learning algorithm, referred to as FairSAOML. This algorithm possesses the ability to adjust to dynamic environments by effectively managing bias control and model accuracy. The problem is framed as a bi-level convex-concave optimization, considering both the model's primal and dual parameters, which pertain to its accuracy and fairness attributes, respectively. Theoretical analysis yields sub-linear upper bounds for both loss regret and the cumulative violation of fairness constraints. Our experimental evaluation on various real-world datasets in dynamic environments demonstrates that our proposed FairSAOML algorithm consistently outperforms alternative approaches rooted in the most advanced prior online learning methods.","sentences":["The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning.","In this scenario, the learner's objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender, when it comes to the newly introduced tasks.","A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework.","Nevertheless, it's crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions.","In this paper, to tackle the fairness-aware online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by incorporating long-term fairness constraints into a strongly adapted loss regret framework.","Moreover, to determine an optimal model parameter at each time step, we introduce an innovative adaptive fairness-aware online meta-learning algorithm, referred to as FairSAOML.","This algorithm possesses the ability to adjust to dynamic environments by effectively managing bias control and model accuracy.","The problem is framed as a bi-level convex-concave optimization, considering both the model's primal and dual parameters, which pertain to its accuracy and fairness attributes, respectively.","Theoretical analysis yields sub-linear upper bounds for both loss regret and the cumulative violation of fairness constraints.","Our experimental evaluation on various real-world datasets in dynamic environments demonstrates that our proposed FairSAOML algorithm consistently outperforms alternative approaches rooted in the most advanced prior online learning methods."],"url":"http://arxiv.org/abs/2402.12319v1","category":"cs.LG"}
{"created":"2024-02-19 17:37:28","title":"ARKS: Active Retrieval in Knowledge Soup for Code Generation","abstract":"Recently the retrieval-augmented generation (RAG) paradigm has raised much attention for its potential in incorporating external knowledge into large language models (LLMs) without further training. While widely explored in natural language applications, its utilization in code generation remains under-explored. In this paper, we introduce Active Retrieval in Knowledge Soup (ARKS), an advanced strategy for generalizing large language models for code. In contrast to relying on a single source, we construct a knowledge soup integrating web search, documentation, execution feedback, and evolved code snippets. We employ an active retrieval strategy that iteratively refines the query and updates the knowledge soup. To assess the performance of ARKS, we compile a new benchmark comprising realistic coding problems associated with frequently updated libraries and long-tail programming languages. Experimental results on ChatGPT and CodeLlama demonstrate a substantial improvement in the average execution accuracy of ARKS on LLMs. The analysis confirms the effectiveness of our proposed knowledge soup and active retrieval strategies, offering rich insights into the construction of effective retrieval-augmented code generation (RACG) pipelines. Our model, code, and data are available at https://arks-codegen.github.io.","sentences":["Recently the retrieval-augmented generation (RAG) paradigm has raised much attention for its potential in incorporating external knowledge into large language models (LLMs) without further training.","While widely explored in natural language applications, its utilization in code generation remains under-explored.","In this paper, we introduce Active Retrieval in Knowledge Soup (ARKS), an advanced strategy for generalizing large language models for code.","In contrast to relying on a single source, we construct a knowledge soup integrating web search, documentation, execution feedback, and evolved code snippets.","We employ an active retrieval strategy that iteratively refines the query and updates the knowledge soup.","To assess the performance of ARKS, we compile a new benchmark comprising realistic coding problems associated with frequently updated libraries and long-tail programming languages.","Experimental results on ChatGPT and CodeLlama demonstrate a substantial improvement in the average execution accuracy of ARKS on LLMs.","The analysis confirms the effectiveness of our proposed knowledge soup and active retrieval strategies, offering rich insights into the construction of effective retrieval-augmented code generation (RACG) pipelines.","Our model, code, and data are available at https://arks-codegen.github.io."],"url":"http://arxiv.org/abs/2402.12317v1","category":"cs.CL"}
{"created":"2024-02-19 17:30:09","title":"Multi-View Conformal Learning for Heterogeneous Sensor Fusion","abstract":"Being able to assess the confidence of individual predictions in machine learning models is crucial for decision making scenarios. Specially, in critical applications such as medical diagnosis, security, and unmanned vehicles, to name a few. In the last years, complex predictive models have had great success in solving hard tasks and new methods are being proposed every day. While the majority of new developments in machine learning models focus on improving the overall performance, less effort is put on assessing the trustworthiness of individual predictions, and even to a lesser extent, in the context of sensor fusion. To this end, we build and test multi-view and single-view conformal models for heterogeneous sensor fusion. Our models provide theoretical marginal confidence guarantees since they are based on the conformal prediction framework. We also propose a multi-view semi-conformal model based on sets intersection. Through comprehensive experimentation, we show that multi-view models perform better than single-view models not only in terms of accuracy-based performance metrics (as it has already been shown in several previous works) but also in conformal measures that provide uncertainty estimation. Our results also showed that multi-view models generate prediction sets with less uncertainty compared to single-view models.","sentences":["Being able to assess the confidence of individual predictions in machine learning models is crucial for decision making scenarios.","Specially, in critical applications such as medical diagnosis, security, and unmanned vehicles, to name a few.","In the last years, complex predictive models have had great success in solving hard tasks and new methods are being proposed every day.","While the majority of new developments in machine learning models focus on improving the overall performance, less effort is put on assessing the trustworthiness of individual predictions, and even to a lesser extent, in the context of sensor fusion.","To this end, we build and test multi-view and single-view conformal models for heterogeneous sensor fusion.","Our models provide theoretical marginal confidence guarantees since they are based on the conformal prediction framework.","We also propose a multi-view semi-conformal model based on sets intersection.","Through comprehensive experimentation, we show that multi-view models perform better than single-view models not only in terms of accuracy-based performance metrics (as it has already been shown in several previous works) but also in conformal measures that provide uncertainty estimation.","Our results also showed that multi-view models generate prediction sets with less uncertainty compared to single-view models."],"url":"http://arxiv.org/abs/2402.12307v1","category":"cs.LG"}
{"created":"2024-02-19 17:27:04","title":"UncertaintyTrack: Exploiting Detection and Localization Uncertainty in Multi-Object Tracking","abstract":"Multi-object tracking (MOT) methods have seen a significant boost in performance recently, due to strong interest from the research community and steadily improving object detection methods. The majority of tracking methods follow the tracking-by-detection (TBD) paradigm, blindly trust the incoming detections with no sense of their associated localization uncertainty. This lack of uncertainty awareness poses a problem in safety-critical tasks such as autonomous driving where passengers could be put at risk due to erroneous detections that have propagated to downstream tasks, including MOT. While there are existing works in probabilistic object detection that predict the localization uncertainty around the boxes, no work in 2D MOT for autonomous driving has studied whether these estimates are meaningful enough to be leveraged effectively in object tracking. We introduce UncertaintyTrack, a collection of extensions that can be applied to multiple TBD trackers to account for localization uncertainty estimates from probabilistic object detectors. Experiments on the Berkeley Deep Drive MOT dataset show that the combination of our method and informative uncertainty estimates reduces the number of ID switches by around 19\\% and improves mMOTA by 2-3%. The source code is available at https://github.com/TRAILab/UncertaintyTrack","sentences":["Multi-object tracking (MOT) methods have seen a significant boost in performance recently, due to strong interest from the research community and steadily improving object detection methods.","The majority of tracking methods follow the tracking-by-detection (TBD) paradigm, blindly trust the incoming detections with no sense of their associated localization uncertainty.","This lack of uncertainty awareness poses a problem in safety-critical tasks such as autonomous driving where passengers could be put at risk due to erroneous detections that have propagated to downstream tasks, including MOT.","While there are existing works in probabilistic object detection that predict the localization uncertainty around the boxes, no work in 2D MOT for autonomous driving has studied whether these estimates are meaningful enough to be leveraged effectively in object tracking.","We introduce UncertaintyTrack, a collection of extensions that can be applied to multiple TBD trackers to account for localization uncertainty estimates from probabilistic object detectors.","Experiments on the Berkeley Deep Drive MOT dataset show that the combination of our method and informative uncertainty estimates reduces the number of ID switches by around 19\\% and improves mMOTA by 2-3%.","The source code is available at https://github.com/TRAILab/UncertaintyTrack"],"url":"http://arxiv.org/abs/2402.12303v1","category":"cs.CV"}
{"created":"2024-02-19 17:23:10","title":"Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports","abstract":"Introduction: With the rapid advances in large language models (LLMs), there have been numerous new open source as well as commercial models. While recent publications have explored GPT-4 in its application to extracting information of interest from radiology reports, there has not been a real-world comparison of GPT-4 to different leading open-source models.   Materials and Methods: Two different and independent datasets were used. The first dataset consists of 540 chest x-ray reports that were created at the Massachusetts General Hospital between July 2019 and July 2021. The second dataset consists of 500 chest x-ray reports from the ImaGenome dataset. We then compared the commercial models GPT-3.5 Turbo and GPT-4 from OpenAI to the open-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B, QWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accurately label the presence of multiple findings in x-ray text reports using different prompting techniques.   Results: On the ImaGenome dataset, the best performing open-source model was Llama2-70B with micro F1-scores of 0.972 and 0.970 for zero- and few-shot prompts, respectively. GPT-4 achieved micro F1-scores of 0.975 and 0.984, respectively. On the institutional dataset, the best performing open-source model was QWEN1.5-72B with micro F1-scores of 0.952 and 0.965 for zero- and few-shot prompting, respectively. GPT-4 achieved micro F1-scores of 0.975 and 0.973, respectively.   Conclusion: In this paper, we show that while GPT-4 is superior to open-source models in zero-shot report labeling, the implementation of few-shot prompting can bring open-source models on par with GPT-4. This shows that open-source models could be a performant and privacy preserving alternative to GPT-4 for the task of radiology report classification.","sentences":["Introduction: With the rapid advances in large language models (LLMs), there have been numerous new open source as well as commercial models.","While recent publications have explored GPT-4 in its application to extracting information of interest from radiology reports, there has not been a real-world comparison of GPT-4 to different leading open-source models.   ","Materials and Methods: Two different and independent datasets were used.","The first dataset consists of 540 chest x-ray reports that were created at the Massachusetts General Hospital between July 2019 and July 2021.","The second dataset consists of 500 chest x-ray reports from the ImaGenome dataset.","We then compared the commercial models GPT-3.5","Turbo and GPT-4 from OpenAI to the open-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B, QWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accurately label the presence of multiple findings in x-ray text reports using different prompting techniques.   ","Results: On the ImaGenome dataset, the best performing open-source model was Llama2-70B with micro F1-scores of 0.972 and 0.970 for zero- and few-shot prompts, respectively.","GPT-4 achieved micro F1-scores of 0.975 and 0.984, respectively.","On the institutional dataset, the best performing open-source model was QWEN1.5-72B with micro F1-scores of 0.952 and 0.965 for zero- and few-shot prompting, respectively.","GPT-4 achieved micro F1-scores of 0.975 and 0.973, respectively.   ","Conclusion: In this paper, we show that while GPT-4 is superior to open-source models in zero-shot report labeling, the implementation of few-shot prompting can bring open-source models on par with GPT-4.","This shows that open-source models could be a performant and privacy preserving alternative to GPT-4 for the task of radiology report classification."],"url":"http://arxiv.org/abs/2402.12298v1","category":"cs.CL"}
{"created":"2024-02-19 17:05:29","title":"KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students","abstract":"Flashcard schedulers are tools that rely on 1) student models to predict the flashcards a student knows; and 2) teaching policies to schedule cards based on these predictions. Existing student models, however, only use flashcard-level features, like the student's past responses, ignoring the semantic ties of flashcards. Deep Knowledge Tracing (DKT) models can capture semantic relations with language models, but are inefficient, lack content-rich datasets for evaluation, and require robust teaching policies. To address these issues, we design KARL, a DKT-inspired student model that uses retrieval and BERT embeddings for efficient and accurate student recall predictions. To test KARL, we collect a new dataset of diverse study history on trivia questions. KARL bests existing student models in AUC and calibration error. Finally, we propose a novel teaching policy that exploits the predictive power of DKT models to deploy KARL online. Based on 27 learners and 32 6-day study trajectories, KARL shows the ability to enhance medium-term educational learning, proving its efficacy for scheduling.","sentences":["Flashcard schedulers are tools that rely on 1) student models to predict the flashcards a student knows; and 2) teaching policies to schedule cards based on these predictions.","Existing student models, however, only use flashcard-level features, like the student's past responses, ignoring the semantic ties of flashcards.","Deep Knowledge Tracing (DKT) models can capture semantic relations with language models, but are inefficient, lack content-rich datasets for evaluation, and require robust teaching policies.","To address these issues, we design KARL, a DKT-inspired student model that uses retrieval and BERT embeddings for efficient and accurate student recall predictions.","To test KARL, we collect a new dataset of diverse study history on trivia questions.","KARL bests existing student models in AUC and calibration error.","Finally, we propose a novel teaching policy that exploits the predictive power of DKT models to deploy KARL online.","Based on 27 learners and 32 6-day study trajectories, KARL shows the ability to enhance medium-term educational learning, proving its efficacy for scheduling."],"url":"http://arxiv.org/abs/2402.12291v1","category":"cs.CL"}
{"created":"2024-02-19 16:51:29","title":"Refining Minimax Regret for Unsupervised Environment Design","abstract":"In unsupervised environment design, reinforcement learning agents are trained on environment configurations (levels) generated by an adversary that maximises some objective. Regret is a commonly used objective that theoretically results in a minimax regret (MMR) policy with desirable robustness guarantees; in particular, the agent's maximum regret is bounded. However, once the agent reaches this regret bound on all levels, the adversary will only sample levels where regret cannot be further reduced. Although there are possible performance improvements to be made outside of these regret-maximising levels, learning stagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a refinement of the minimax regret objective that overcomes this limitation. We formally show that solving for this objective results in a subset of MMR policies, and that BLP policies act consistently with a Perfect Bayesian policy over all levels. We further introduce an algorithm, ReMiDi, that results in a BLP policy at convergence. We empirically demonstrate that training on levels from a minimax regret adversary causes learning to prematurely stagnate, but that ReMiDi continues learning.","sentences":["In unsupervised environment design, reinforcement learning agents are trained on environment configurations (levels) generated by an adversary that maximises some objective.","Regret is a commonly used objective that theoretically results in a minimax regret (MMR) policy with desirable robustness guarantees; in particular, the agent's maximum regret is bounded.","However, once the agent reaches this regret bound on all levels, the adversary will only sample levels where regret cannot be further reduced.","Although there are possible performance improvements to be made outside of these regret-maximising levels, learning stagnates.","In this work, we introduce Bayesian level-perfect MMR (BLP), a refinement of the minimax regret objective that overcomes this limitation.","We formally show that solving for this objective results in a subset of MMR policies, and that BLP policies act consistently with a Perfect Bayesian policy over all levels.","We further introduce an algorithm, ReMiDi, that results in a BLP policy at convergence.","We empirically demonstrate that training on levels from a minimax regret adversary causes learning to prematurely stagnate, but that ReMiDi continues learning."],"url":"http://arxiv.org/abs/2402.12284v1","category":"cs.LG"}
{"created":"2024-02-19 16:47:04","title":"Adaptive Skeleton Graph Decoding","abstract":"Large language models (LLMs) have seen significant adoption for natural language tasks, owing their success to massive numbers of model parameters (e.g., 70B+); however, LLM inference incurs significant computation and memory costs. Recent approaches propose parallel decoding strategies, such as Skeleton-of-Thought (SoT), to improve performance by breaking prompts down into sub-problems that can be decoded in parallel; however, they often suffer from reduced response quality. Our key insight is that we can request additional information, specifically dependencies and difficulty, when generating the sub-problems to improve both response quality and performance. In this paper, we propose Skeleton Graph Decoding (SGD), which uses dependencies exposed between sub-problems to support information forwarding between dependent sub-problems for improved quality while exposing parallelization opportunities for decoding independent sub-problems. Additionally, we leverage difficulty estimates for each sub-problem to select an appropriately-sized model, improving performance without significantly reducing quality. Compared to standard autoregressive generation and SoT, SGD achieves a 1.69x speedup while improving quality by up to 51%.","sentences":["Large language models (LLMs) have seen significant adoption for natural language tasks, owing their success to massive numbers of model parameters (e.g., 70B+); however, LLM inference incurs significant computation and memory costs.","Recent approaches propose parallel decoding strategies, such as Skeleton-of-Thought (SoT), to improve performance by breaking prompts down into sub-problems that can be decoded in parallel; however, they often suffer from reduced response quality.","Our key insight is that we can request additional information, specifically dependencies and difficulty, when generating the sub-problems to improve both response quality and performance.","In this paper, we propose Skeleton Graph Decoding (SGD), which uses dependencies exposed between sub-problems to support information forwarding between dependent sub-problems for improved quality while exposing parallelization opportunities for decoding independent sub-problems.","Additionally, we leverage difficulty estimates for each sub-problem to select an appropriately-sized model, improving performance without significantly reducing quality.","Compared to standard autoregressive generation and SoT, SGD achieves a 1.69x speedup while improving quality by up to 51%."],"url":"http://arxiv.org/abs/2402.12280v1","category":"cs.CL"}
{"created":"2024-02-19 16:43:57","title":"Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks","abstract":"Zero-shot cross-lingual generation implies finetuning of the multilingual pretrained language model on a generation task in one language and then using it to make predictions for this task in other languages. Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model. In this work we compare various approaches proposed from the literature in unified settings, also including alternative backbone models, namely mBART and NLLB-200. We first underline the importance of tuning learning rate used for finetuning, which helps to substantially alleviate the problem of generation in the wrong language. Then, we show that with careful learning rate tuning, the simple full finetuning of the model acts as a very strong baseline and alternative approaches bring only marginal improvements. Finally, we find that mBART performs similarly to mT5 of the same size, and NLLB-200 can be competitive in some cases. Our final models reach the performance of the approach based on data translation which is usually considered as an upper baseline for zero-shot cross-lingual generation.","sentences":["Zero-shot cross-lingual generation implies finetuning of the multilingual pretrained language model on a generation task in one language and then using it to make predictions for this task in other languages.","Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model.","In this work we compare various approaches proposed from the literature in unified settings, also including alternative backbone models, namely mBART and NLLB-200.","We first underline the importance of tuning learning rate used for finetuning, which helps to substantially alleviate the problem of generation in the wrong language.","Then, we show that with careful learning rate tuning, the simple full finetuning of the model acts as a very strong baseline and alternative approaches bring only marginal improvements.","Finally, we find that mBART performs similarly to mT5 of the same size, and NLLB-200 can be competitive in some cases.","Our final models reach the performance of the approach based on data translation which is usually considered as an upper baseline for zero-shot cross-lingual generation."],"url":"http://arxiv.org/abs/2402.12279v1","category":"cs.CL"}
{"created":"2024-02-19 16:39:18","title":"WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment","abstract":"We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We do this by extending work on program synthesis via LLMs. We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents.","sentences":["We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment.","The world model tries to explain its interactions, while also being optimistic about what reward it can achieve.","We do this by extending work on program synthesis via LLMs.","We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents."],"url":"http://arxiv.org/abs/2402.12275v1","category":"cs.AI"}
{"created":"2024-02-19 16:26:40","title":"On the Byzantine-Resilience of Distillation-Based Federated Learning","abstract":"Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost. These methods depart from transmitting model parameters and, instead, communicate information about a learning task by sharing predictions on a public dataset. In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process. We show that KD-based FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process compared to Federated Averaging. Based on these insights, we introduce two new byzantine attacks and demonstrate that they are effective against prior byzantine-resilient methods. Additionally, we propose FilterExp, a novel method designed to enhance the byzantine resilience of KD-based FL algorithms and demonstrate its efficacy. Finally, we provide a general method to make attacks harder to detect, improving their effectiveness.","sentences":["Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost.","These methods depart from transmitting model parameters and, instead, communicate information about a learning task by sharing predictions on a public dataset.","In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process.","We show that KD-based FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process compared to Federated Averaging.","Based on these insights, we introduce two new byzantine attacks and demonstrate that they are effective against prior byzantine-resilient methods.","Additionally, we propose FilterExp, a novel method designed to enhance the byzantine resilience of KD-based FL algorithms and demonstrate its efficacy.","Finally, we provide a general method to make attacks harder to detect, improving their effectiveness."],"url":"http://arxiv.org/abs/2402.12265v1","category":"cs.LG"}
{"created":"2024-02-19 16:26:00","title":"Uncertainty quantification in fine-tuned LLMs using LoRA ensembles","abstract":"Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing. We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles. We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning. In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn.","sentences":["Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing.","We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles.","We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning.","In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn."],"url":"http://arxiv.org/abs/2402.12264v1","category":"cs.LG"}
{"created":"2024-02-19 16:19:15","title":"NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms","abstract":"The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks. LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address. We will release our benchmark and code for reproducing our experiments.","sentences":["The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference.","One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time.","We create a diverse resource of recent English neologisms by using several popular collection methods.","We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words.","Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence.","Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity.","Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks.","LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address.","We will release our benchmark and code for reproducing our experiments."],"url":"http://arxiv.org/abs/2402.12261v1","category":"cs.CL"}
{"created":"2024-02-19 15:54:36","title":"BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts","abstract":"Neuro-Symbolic (NeSy) predictors that conform to symbolic knowledge - encoding, e.g., safety constraints - can be affected by Reasoning Shortcuts (RSs): They learn concepts consistent with the symbolic knowledge by exploiting unintended semantics. RSs compromise reliability and generalization and, as we show in this paper, they are linked to NeSy models being overconfident about the predicted concepts. Unfortunately, the only trustworthy mitigation strategy requires collecting costly dense supervision over the concepts. Rather than attempting to avoid RSs altogether, we propose to ensure NeSy models are aware of the semantic ambiguity of the concepts they learn, thus enabling their users to identify and distrust low-quality concepts. Starting from three simple desiderata, we derive bears (BE Aware of Reasoning Shortcuts), an ensembling technique that calibrates the model's concept-level confidence without compromising prediction accuracy, thus encouraging NeSy architectures to be uncertain about concepts affected by RSs. We show empirically that bears improves RS-awareness of several state-of-the-art NeSy models, and also facilitates acquiring informative dense annotations for mitigation purposes.","sentences":["Neuro-Symbolic (NeSy) predictors that conform to symbolic knowledge - encoding, e.g., safety constraints - can be affected by Reasoning Shortcuts (RSs): They learn concepts consistent with the symbolic knowledge by exploiting unintended semantics.","RSs compromise reliability and generalization and, as we show in this paper, they are linked to NeSy models being overconfident about the predicted concepts.","Unfortunately, the only trustworthy mitigation strategy requires collecting costly dense supervision over the concepts.","Rather than attempting to avoid RSs altogether, we propose to ensure NeSy models are aware of the semantic ambiguity of the concepts they learn, thus enabling their users to identify and distrust low-quality concepts.","Starting from three simple desiderata, we derive bears (BE Aware of Reasoning Shortcuts), an ensembling technique that calibrates the model's concept-level confidence without compromising prediction accuracy, thus encouraging NeSy architectures to be uncertain about concepts affected by RSs.","We show empirically that bears improves RS-awareness of several state-of-the-art NeSy models, and also facilitates acquiring informative dense annotations for mitigation purposes."],"url":"http://arxiv.org/abs/2402.12240v1","category":"cs.LG"}
{"created":"2024-02-19 15:47:47","title":"Learning to Defer in Content Moderation: The Human-AI Interplay","abstract":"Successful content moderation in online platforms relies on a human-AI collaboration approach. A typical heuristic estimates the expected harmfulness of a post and uses fixed thresholds to decide whether to remove it and whether to send it for human review. This disregards the prediction uncertainty, the time-varying element of human review capacity and post arrivals, and the selective sampling in the dataset (humans only review posts filtered by the admission algorithm).   In this paper, we introduce a model to capture the human-AI interplay in content moderation. The algorithm observes contextual information for incoming posts, makes classification and admission decisions, and schedules posts for human review. Only admitted posts receive human reviews on their harmfulness. These reviews help educate the machine-learning algorithms but are delayed due to congestion in the human review system. The classical learning-theoretic way to capture this human-AI interplay is via the framework of learning to defer, where the algorithm has the option to defer a classification task to humans for a fixed cost and immediately receive feedback. Our model contributes to this literature by introducing congestion in the human review system. Moreover, unlike work on online learning with delayed feedback where the delay in the feedback is exogenous to the algorithm's decisions, the delay in our model is endogenous to both the admission and the scheduling decisions.   We propose a near-optimal learning algorithm that carefully balances the classification loss from a selectively sampled dataset, the idiosyncratic loss of non-reviewed posts, and the delay loss of having congestion in the human review system. To the best of our knowledge, this is the first result for online learning in contextual queueing systems and hence our analytical framework may be of independent interest.","sentences":["Successful content moderation in online platforms relies on a human-AI collaboration approach.","A typical heuristic estimates the expected harmfulness of a post and uses fixed thresholds to decide whether to remove it and whether to send it for human review.","This disregards the prediction uncertainty, the time-varying element of human review capacity and post arrivals, and the selective sampling in the dataset (humans only review posts filtered by the admission algorithm).   ","In this paper, we introduce a model to capture the human-AI interplay in content moderation.","The algorithm observes contextual information for incoming posts, makes classification and admission decisions, and schedules posts for human review.","Only admitted posts receive human reviews on their harmfulness.","These reviews help educate the machine-learning algorithms but are delayed due to congestion in the human review system.","The classical learning-theoretic way to capture this human-AI interplay is via the framework of learning to defer, where the algorithm has the option to defer a classification task to humans for a fixed cost and immediately receive feedback.","Our model contributes to this literature by introducing congestion in the human review system.","Moreover, unlike work on online learning with delayed feedback where the delay in the feedback is exogenous to the algorithm's decisions, the delay in our model is endogenous to both the admission and the scheduling decisions.   ","We propose a near-optimal learning algorithm that carefully balances the classification loss from a selectively sampled dataset, the idiosyncratic loss of non-reviewed posts, and the delay loss of having congestion in the human review system.","To the best of our knowledge, this is the first result for online learning in contextual queueing systems and hence our analytical framework may be of independent interest."],"url":"http://arxiv.org/abs/2402.12237v1","category":"cs.LG"}
{"created":"2024-02-19 15:39:39","title":"Kernel KMeans clustering splits for end-to-end unsupervised decision trees","abstract":"Trees are convenient models for obtaining explainable predictions on relatively small datasets. Although there are many proposals for the end-to-end construction of such trees in supervised learning, learning a tree end-to-end for clustering without labels remains an open challenge. As most works focus on interpreting with trees the result of another clustering algorithm, we present here a novel end-to-end trained unsupervised binary tree for clustering: Kauri. This method performs a greedy maximisation of the kernel KMeans objective without requiring the definition of centroids. We compare this model on multiple datasets with recent unsupervised trees and show that Kauri performs identically when using a linear kernel. For other kernels, Kauri often outperforms the concatenation of kernel KMeans and a CART decision tree.","sentences":["Trees are convenient models for obtaining explainable predictions on relatively small datasets.","Although there are many proposals for the end-to-end construction of such trees in supervised learning, learning a tree end-to-end for clustering without labels remains an open challenge.","As most works focus on interpreting with trees the result of another clustering algorithm, we present here a novel end-to-end trained unsupervised binary tree for clustering: Kauri.","This method performs a greedy maximisation of the kernel KMeans objective without requiring the definition of centroids.","We compare this model on multiple datasets with recent unsupervised trees and show that Kauri performs identically when using a linear kernel.","For other kernels, Kauri often outperforms the concatenation of kernel KMeans and a CART decision tree."],"url":"http://arxiv.org/abs/2402.12232v1","category":"stat.ML"}
{"created":"2024-02-19 15:33:10","title":"AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling","abstract":"We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/","sentences":["We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music.","AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms.","Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages.","We build a multimodal text-centric dataset for multimodal alignment pre-training.","Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset.","It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs.","Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model.","Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/"],"url":"http://arxiv.org/abs/2402.12226v1","category":"cs.CL"}
{"created":"2024-02-19 15:33:09","title":"Pushing Auto-regressive Models for 3D Shape Generation at Capacity and Scalability","abstract":"Auto-regressive models have achieved impressive results in 2D image generation by modeling joint distributions in grid space. In this paper, we extend auto-regressive models to 3D domains, and seek a stronger ability of 3D shape generation by improving auto-regressive models at capacity and scalability simultaneously. Firstly, we leverage an ensemble of publicly available 3D datasets to facilitate the training of large-scale models. It consists of a comprehensive collection of approximately 900,000 objects, with multiple properties of meshes, points, voxels, rendered images, and text captions. This diverse labeled dataset, termed Objaverse-Mix, empowers our model to learn from a wide range of object variations. However, directly applying 3D auto-regression encounters critical challenges of high computational demands on volumetric grids and ambiguous auto-regressive order along grid dimensions, resulting in inferior quality of 3D shapes. To this end, we then present a novel framework Argus3D in terms of capacity. Concretely, our approach introduces discrete representation learning based on a latent vector instead of volumetric grids, which not only reduces computational costs but also preserves essential geometric details by learning the joint distributions in a more tractable order. The capacity of conditional generation can thus be realized by simply concatenating various conditioning inputs to the latent vector, such as point clouds, categories, images, and texts. In addition, thanks to the simplicity of our model architecture, we naturally scale up our approach to a larger model with an impressive 3.6 billion parameters, further enhancing the quality of versatile 3D generation. Extensive experiments on four generation tasks demonstrate that Argus3D can synthesize diverse and faithful shapes across multiple categories, achieving remarkable performance.","sentences":["Auto-regressive models have achieved impressive results in 2D image generation by modeling joint distributions in grid space.","In this paper, we extend auto-regressive models to 3D domains, and seek a stronger ability of 3D shape generation by improving auto-regressive models at capacity and scalability simultaneously.","Firstly, we leverage an ensemble of publicly available 3D datasets to facilitate the training of large-scale models.","It consists of a comprehensive collection of approximately 900,000 objects, with multiple properties of meshes, points, voxels, rendered images, and text captions.","This diverse labeled dataset, termed Objaverse-Mix, empowers our model to learn from a wide range of object variations.","However, directly applying 3D auto-regression encounters critical challenges of high computational demands on volumetric grids and ambiguous auto-regressive order along grid dimensions, resulting in inferior quality of 3D shapes.","To this end, we then present a novel framework Argus3D in terms of capacity.","Concretely, our approach introduces discrete representation learning based on a latent vector instead of volumetric grids, which not only reduces computational costs but also preserves essential geometric details by learning the joint distributions in a more tractable order.","The capacity of conditional generation can thus be realized by simply concatenating various conditioning inputs to the latent vector, such as point clouds, categories, images, and texts.","In addition, thanks to the simplicity of our model architecture, we naturally scale up our approach to a larger model with an impressive 3.6 billion parameters, further enhancing the quality of versatile 3D generation.","Extensive experiments on four generation tasks demonstrate that Argus3D can synthesize diverse and faithful shapes across multiple categories, achieving remarkable performance."],"url":"http://arxiv.org/abs/2402.12225v1","category":"cs.CV"}
{"created":"2024-02-19 15:21:58","title":"Reformatted Alignment","abstract":"The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.   Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13B's mathematical reasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy. Additionally, a mere 5% of ReAlign data yields a 67% boost in general alignment ability measured by the Alpaca dataset. This work highlights the need for further research into the science and mechanistic interpretability of LLMs. We have made the associated code and data publicly accessible to support future studies at https://github.com/GAIR-NLP/ReAlign.","sentences":["The quality of finetuning data is crucial for aligning large language models (LLMs) with human values.","Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations.","This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence.","This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques.","Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.   ","Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13B's mathematical reasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy.","Additionally, a mere 5% of ReAlign data yields a 67% boost in general alignment ability measured by the Alpaca dataset.","This work highlights the need for further research into the science and mechanistic interpretability of LLMs.","We have made the associated code and data publicly accessible to support future studies at https://github.com/GAIR-NLP/ReAlign."],"url":"http://arxiv.org/abs/2402.12219v1","category":"cs.CL"}
{"created":"2024-02-19 15:20:35","title":"Copyleft for Alleviating AIGC Copyright Dilemma: What-if Analysis, Public Perception and Implications","abstract":"As AIGC has impacted our society profoundly in the past years, ethical issues have received tremendous attention. The most urgent one is the AIGC copyright dilemma, which can immensely stifle the development of AIGC and greatly cost the entire society. Given the complexity of AIGC copyright governance and the fact that no perfect solution currently exists, previous work advocated copyleft on AI governance but without substantive analysis. In this paper, we take a step further to explore the feasibility of copyleft to alleviate the AIGC copyright dilemma. We conduct a mixed-methods study from two aspects: qualitatively, we use a formal what-if analysis to clarify the dilemma and provide case studies to show the feasibility of copyleft; quantitatively, we perform a carefully designed survey to find out how the public feels about copylefting AIGC. The key findings include: a) people generally perceive the dilemma, b) they prefer to use authorized AIGC under loose restriction, and c) they are positive to copyleft in AIGC and willing to use it in the future.","sentences":["As AIGC has impacted our society profoundly in the past years, ethical issues have received tremendous attention.","The most urgent one is the AIGC copyright dilemma, which can immensely stifle the development of AIGC and greatly cost the entire society.","Given the complexity of AIGC copyright governance and the fact that no perfect solution currently exists, previous work advocated copyleft on AI governance but without substantive analysis.","In this paper, we take a step further to explore the feasibility of copyleft to alleviate the AIGC copyright dilemma.","We conduct a mixed-methods study from two aspects: qualitatively, we use a formal what-if analysis to clarify the dilemma and provide case studies to show the feasibility of copyleft; quantitatively, we perform a carefully designed survey to find out how the public feels about copylefting AIGC.","The key findings include: a) people generally perceive the dilemma, b) they prefer to use authorized AIGC under loose restriction, and c) they are positive to copyleft in AIGC and willing to use it in the future."],"url":"http://arxiv.org/abs/2402.12216v1","category":"cs.CY"}
{"created":"2024-02-19 15:17:16","title":"SlopeSeeker: A Search Tool for Exploring a Dataset of Quantifiable Trends","abstract":"Natural language and search interfaces intuitively facilitate data exploration and provide visualization responses to diverse analytical queries based on the underlying datasets. However, these interfaces often fail to interpret more complex analytical intents, such as discerning subtleties and quantifiable differences between terms like \"bump\" and \"spike\" in the context of COVID cases, for example. We address this gap by extending the capabilities of a data exploration search interface for interpreting semantic concepts in time series trends. We first create a comprehensive dataset of semantic concepts by mapping quantifiable univariate data trends such as slope and angle to crowdsourced, semantically meaningful trend labels. The dataset contains quantifiable properties that capture the slope-scalar effect of semantic modifiers like \"sharply\" and \"gradually,\" as well as multi-line trends (e.g., \"peak,\" \"valley\"). We demonstrate the utility of this dataset in SlopeSeeker, a tool that supports natural language querying of quantifiable trends, such as \"show me stocks that tanked in 2010.\" The tool incorporates novel scoring and ranking techniques based on semantic relevance and visual prominence to present relevant trend chart responses containing these semantic trend concepts. In addition, SlopeSeeker provides a faceted search interface for users to navigate a semantic hierarchy of concepts from general trends (e.g., \"increase\") to more specific ones (e.g., \"sharp increase\"). A preliminary user evaluation of the tool demonstrates that the search interface supports greater expressivity of queries containing concepts that describe data trends. We identify potential future directions for leveraging our publicly available quantitative semantics dataset in other data domains and for novel visual analytics interfaces.","sentences":["Natural language and search interfaces intuitively facilitate data exploration and provide visualization responses to diverse analytical queries based on the underlying datasets.","However, these interfaces often fail to interpret more complex analytical intents, such as discerning subtleties and quantifiable differences between terms like \"bump\" and \"spike\" in the context of COVID cases, for example.","We address this gap by extending the capabilities of a data exploration search interface for interpreting semantic concepts in time series trends.","We first create a comprehensive dataset of semantic concepts by mapping quantifiable univariate data trends such as slope and angle to crowdsourced, semantically meaningful trend labels.","The dataset contains quantifiable properties that capture the slope-scalar effect of semantic modifiers like \"sharply\" and \"gradually,\" as well as multi-line trends (e.g., \"peak,\" \"valley\").","We demonstrate the utility of this dataset in SlopeSeeker, a tool that supports natural language querying of quantifiable trends, such as \"show me stocks that tanked in 2010.\"","The tool incorporates novel scoring and ranking techniques based on semantic relevance and visual prominence to present relevant trend chart responses containing these semantic trend concepts.","In addition, SlopeSeeker provides a faceted search interface for users to navigate a semantic hierarchy of concepts from general trends (e.g., \"increase\") to more specific ones (e.g., \"sharp increase\").","A preliminary user evaluation of the tool demonstrates that the search interface supports greater expressivity of queries containing concepts that describe data trends.","We identify potential future directions for leveraging our publicly available quantitative semantics dataset in other data domains and for novel visual analytics interfaces."],"url":"http://arxiv.org/abs/2402.12214v1","category":"cs.HC"}
{"created":"2024-02-19 15:12:10","title":"Dislike of general opinion makes for tight elections","abstract":"In modern democracies, the outcome of elections and referendums is often remarkably tight. The repetition of these divisive events are the hallmark of a split society; to the physicist, however, it is an astonishing feat for such large collections of diverse individuals. Many sociophysics models reproduce the emergence of collective human behavior with interacting agents, which respond to their environment according to simple rules, modulated by random fluctuations. A paragon of this class is the Ising model which, when interactions are strong, predicts that order can emerge from a chaotic initial state. In contrast with many elections, however, this model favors a strong majority. Here, we introduce a new element to this classical theory, which accounts for the influence of opinion polls on the electorate. This brings about a new phase in which two groups divide the opinion equally. These political camps are spatially segregated, and the sharp boundary that separates them makes the system size-dependent, even in the limit of a large electorate. Election data show that, over the last 30 years, countries with more than about a million voters often found themselves in this state, whereas elections in smaller countries yielded more consensual results. We suggest that this transition hinges on the electorate's awareness of the general opinion.","sentences":["In modern democracies, the outcome of elections and referendums is often remarkably tight.","The repetition of these divisive events are the hallmark of a split society; to the physicist, however, it is an astonishing feat for such large collections of diverse individuals.","Many sociophysics models reproduce the emergence of collective human behavior with interacting agents, which respond to their environment according to simple rules, modulated by random fluctuations.","A paragon of this class is the Ising model which, when interactions are strong, predicts that order can emerge from a chaotic initial state.","In contrast with many elections, however, this model favors a strong majority.","Here, we introduce a new element to this classical theory, which accounts for the influence of opinion polls on the electorate.","This brings about a new phase in which two groups divide the opinion equally.","These political camps are spatially segregated, and the sharp boundary that separates them makes the system size-dependent, even in the limit of a large electorate.","Election data show that, over the last 30 years, countries with more than about a million voters often found themselves in this state, whereas elections in smaller countries yielded more consensual results.","We suggest that this transition hinges on the electorate's awareness of the general opinion."],"url":"http://arxiv.org/abs/2402.12207v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-19 15:06:04","title":"Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid Federated Approach","abstract":"In the era of modern education, addressing cross-school learner diversity is crucial, especially in personalized recommender systems for elective course selection. However, privacy concerns often limit cross-school data sharing, which hinders existing methods' ability to model sparse data and address heterogeneity effectively, ultimately leading to suboptimal recommendations. In response, we propose HFRec, a heterogeneity-aware hybrid federated recommender system designed for cross-school elective course recommendations. The proposed model constructs heterogeneous graphs for each school, incorporating various interactions and historical behaviors between students to integrate context and content information. We design an attention mechanism to capture heterogeneity-aware representations. Moreover, under a federated scheme, we train individual school-based models with adaptive learning settings to recommend tailored electives. Our HFRec model demonstrates its effectiveness in providing personalized elective recommendations while maintaining privacy, as it outperforms state-of-the-art models on both open-source and real-world datasets.","sentences":["In the era of modern education, addressing cross-school learner diversity is crucial, especially in personalized recommender systems for elective course selection.","However, privacy concerns often limit cross-school data sharing, which hinders existing methods' ability to model sparse data and address heterogeneity effectively, ultimately leading to suboptimal recommendations.","In response, we propose HFRec, a heterogeneity-aware hybrid federated recommender system designed for cross-school elective course recommendations.","The proposed model constructs heterogeneous graphs for each school, incorporating various interactions and historical behaviors between students to integrate context and content information.","We design an attention mechanism to capture heterogeneity-aware representations.","Moreover, under a federated scheme, we train individual school-based models with adaptive learning settings to recommend tailored electives.","Our HFRec model demonstrates its effectiveness in providing personalized elective recommendations while maintaining privacy, as it outperforms state-of-the-art models on both open-source and real-world datasets."],"url":"http://arxiv.org/abs/2402.12202v1","category":"cs.IR"}
{"created":"2024-02-19 14:52:50","title":"Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships","abstract":"Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their membership probabilities. Our empirical findings indicate a remarkable outcome: LMs with over 1B parameters exhibit a four to eight-fold increase in training data exposure. We discuss potential mitigations and suggest future research directions.","sentences":["Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization.","This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data.","This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset.","To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data.","However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging.","To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM.","We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their membership probabilities.","Our empirical findings indicate a remarkable outcome: LMs with over 1B parameters exhibit a four to eight-fold increase in training data exposure.","We discuss potential mitigations and suggest future research directions."],"url":"http://arxiv.org/abs/2402.12189v1","category":"cs.CL"}
{"created":"2024-02-19 14:45:46","title":"MultiFIX: An XAI-friendly feature inducing approach to building models from multimodal data","abstract":"In the health domain, decisions are often based on different data modalities. Thus, when creating prediction models, multimodal fusion approaches that can extract and combine relevant features from different data modalities, can be highly beneficial. Furthermore, it is important to understand how each modality impacts the final prediction, especially in high-stake domains, so that these models can be used in a trustworthy and responsible manner. We propose MultiFIX: a new interpretability-focused multimodal data fusion pipeline that explicitly induces separate features from different data types that can subsequently be combined to make a final prediction. An end-to-end deep learning architecture is used to train a predictive model and extract representative features of each modality. Each part of the model is then explained using explainable artificial intelligence techniques. Attention maps are used to highlight important regions in image inputs. Inherently interpretable symbolic expressions, learned with GP-GOMEA, are used to describe the contribution of tabular inputs. The fusion of the extracted features to predict the target label is also replaced by a symbolic expression, learned with GP-GOMEA. Results on synthetic problems demonstrate the strengths and limitations of MultiFIX. Lastly, we apply MultiFIX to a publicly available dataset for the detection of malignant skin lesions.","sentences":["In the health domain, decisions are often based on different data modalities.","Thus, when creating prediction models, multimodal fusion approaches that can extract and combine relevant features from different data modalities, can be highly beneficial.","Furthermore, it is important to understand how each modality impacts the final prediction, especially in high-stake domains, so that these models can be used in a trustworthy and responsible manner.","We propose MultiFIX: a new interpretability-focused multimodal data fusion pipeline that explicitly induces separate features from different data types that can subsequently be combined to make a final prediction.","An end-to-end deep learning architecture is used to train a predictive model and extract representative features of each modality.","Each part of the model is then explained using explainable artificial intelligence techniques.","Attention maps are used to highlight important regions in image inputs.","Inherently interpretable symbolic expressions, learned with GP-GOMEA, are used to describe the contribution of tabular inputs.","The fusion of the extracted features to predict the target label is also replaced by a symbolic expression, learned with GP-GOMEA.","Results on synthetic problems demonstrate the strengths and limitations of MultiFIX.","Lastly, we apply MultiFIX to a publicly available dataset for the detection of malignant skin lesions."],"url":"http://arxiv.org/abs/2402.12183v1","category":"cs.AI"}
{"created":"2024-02-19 14:42:10","title":"Revisiting Data Augmentation in Deep Reinforcement Learning","abstract":"Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL). Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear. To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected. Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them. We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values. This analysis suggests recommendations on how to exploit data augmentation in a more principled way. In addition, we include a regularization term called tangent prop, previously proposed in computer vision, but whose adaptation to DRL is novel to the best of our knowledge. We evaluate our proposition and validate our analysis in several domains. Compared to different relevant baselines, we demonstrate that it achieves state-of-the-art performance in most environments and shows higher sample efficiency and better generalization ability in some complex environments.","sentences":["Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL).","Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear.","To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected.","Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them.","We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values.","This analysis suggests recommendations on how to exploit data augmentation in a more principled way.","In addition, we include a regularization term called tangent prop, previously proposed in computer vision, but whose adaptation to DRL is novel to the best of our knowledge.","We evaluate our proposition and validate our analysis in several domains.","Compared to different relevant baselines, we demonstrate that it achieves state-of-the-art performance in most environments and shows higher sample efficiency and better generalization ability in some complex environments."],"url":"http://arxiv.org/abs/2402.12181v1","category":"cs.LG"}
{"created":"2024-02-19 14:37:17","title":"Examining Monitoring System: Detecting Abnormal Behavior In Online Examinations","abstract":"Cheating in online exams has become a prevalent issue over the past decade, especially during the COVID-19 pandemic. To address this issue of academic dishonesty, our \"Exam Monitoring System: Detecting Abnormal Behavior in Online Examinations\" is designed to assist proctors in identifying unusual student behavior. Our system demonstrates high accuracy and speed in detecting cheating in real-time scenarios, providing valuable information, and aiding proctors in decision-making. This article outlines our methodology and the effectiveness of our system in mitigating the widespread problem of cheating in online exams.","sentences":["Cheating in online exams has become a prevalent issue over the past decade, especially during the COVID-19 pandemic.","To address this issue of academic dishonesty, our \"Exam Monitoring System: Detecting Abnormal Behavior in Online Examinations\" is designed to assist proctors in identifying unusual student behavior.","Our system demonstrates high accuracy and speed in detecting cheating in real-time scenarios, providing valuable information, and aiding proctors in decision-making.","This article outlines our methodology and the effectiveness of our system in mitigating the widespread problem of cheating in online exams."],"url":"http://arxiv.org/abs/2402.12179v1","category":"cs.CV"}
{"created":"2024-02-19 14:33:24","title":"Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning","abstract":"Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, illustrating its broad applicability and efficiency.","sentences":["Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs).","The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics.","However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning.","This paper addresses scenarios where the embeddings are only available from a black-box model.","We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model.","Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model.","We validate the effectiveness of our method on both labeled and unlabeled datasets, illustrating its broad applicability and efficiency."],"url":"http://arxiv.org/abs/2402.12177v1","category":"cs.LG"}
{"created":"2024-02-19 14:22:54","title":"Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning","abstract":"Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference process, extreme confidence serves as an indicator for poisoned samples, while others are clean. We conduct experiments on text classification tasks, five fine-tuning strategies, and three weight-poisoning backdoor attack methods. Experiments show near 100% success rates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore, our defensive approach exhibits overall competitive performance in mitigating weight-poisoning backdoor attacks.","sentences":["Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented.","However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks.","In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning.","Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks.","Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels.","During the inference process, extreme confidence serves as an indicator for poisoned samples, while others are clean.","We conduct experiments on text classification tasks, five fine-tuning strategies, and three weight-poisoning backdoor attack methods.","Experiments show near 100% success rates for weight-poisoning backdoor attacks when utilizing PEFT.","Furthermore, our defensive approach exhibits overall competitive performance in mitigating weight-poisoning backdoor attacks."],"url":"http://arxiv.org/abs/2402.12168v1","category":"cs.CR"}
{"created":"2024-02-19 14:16:08","title":"Endowing Pre-trained Graph Models with Provable Fairness","abstract":"Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks. Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications. The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs. However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient. Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario. To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained graph models with provable fairness (called GraphPAR). GraphPAR freezes the parameters of PGMs and trains a parameter-efficient adapter to flexibly improve the fairness of PGMs in downstream tasks. Specifically, we design a sensitive semantic augmenter on node representations, to extend the node representations with different sensitive attribute semantics for each node. The extended representations will be used to further train an adapter, to prevent the propagation of sensitive attribute semantics from PGMs to task predictions. Furthermore, with GraphPAR, we quantify whether the fairness of each node is provable, i.e., predictions are always fair within a certain range of sensitive attribute semantics. Experimental evaluations on real-world datasets demonstrate that GraphPAR achieves state-of-the-art prediction performance and fairness on node classification task. Furthermore, based on our GraphPAR, around 90\\% nodes have provable fairness.","sentences":["Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks.","Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications.","The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs.","However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient.","Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario.","To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained graph models with provable fairness (called GraphPAR).","GraphPAR freezes the parameters of PGMs and trains a parameter-efficient adapter to flexibly improve the fairness of PGMs in downstream tasks.","Specifically, we design a sensitive semantic augmenter on node representations, to extend the node representations with different sensitive attribute semantics for each node.","The extended representations will be used to further train an adapter, to prevent the propagation of sensitive attribute semantics from PGMs to task predictions.","Furthermore, with GraphPAR, we quantify whether the fairness of each node is provable, i.e., predictions are always fair within a certain range of sensitive attribute semantics.","Experimental evaluations on real-world datasets demonstrate that GraphPAR achieves state-of-the-art prediction performance and fairness on node classification task.","Furthermore, based on our GraphPAR, around 90\\% nodes have provable fairness."],"url":"http://arxiv.org/abs/2402.12161v2","category":"cs.LG"}
{"created":"2024-02-19 18:50:53","title":"Almost-linear time parameterized algorithm for rankwidth via dynamic rankwidth","abstract":"We give an algorithm that given a graph $G$ with $n$ vertices and $m$ edges and an integer $k$, in time $O_k(n^{1+o(1)}) + O(m)$ either outputs a rank decomposition of $G$ of width at most $k$ or determines that the rankwidth of $G$ is larger than $k$; the $O_k(\\cdot)$-notation hides factors depending on $k$. Our algorithm returns also a $(2^{k+1}-1)$-expression for cliquewidth, yielding a $(2^{k+1}-1)$-approximation algorithm for cliquewidth with the same running time. This improves upon the $O_k(n^2)$ time algorithm of Fomin and Korhonen [STOC 2022].   The main ingredient of our algorithm is a fully dynamic algorithm for maintaining rank decompositions of bounded width: We give a data structure that for a dynamic $n$-vertex graph $G$ that is updated by edge insertions and deletions maintains a rank decomposition of $G$ of width at most $4k$ under the promise that the rankwidth of $G$ never grows above $k$. The amortized running time of each update is $O_k(2^{\\sqrt{\\log n} \\log \\log n})$. The data structure furthermore can maintain whether $G$ satisfies some fixed ${\\sf CMSO}_1$ property within the same running time. We also give a framework for performing ``dense'' edge updates inside a given set of vertices $X$, where the new edges inside $X$ are described by a given ${\\sf CMSO}_1$ sentence and vertex labels, in amortized $O_k(|X| \\cdot 2^{\\sqrt{\\log n} \\log \\log n})$ time. Our dynamic algorithm generalizes the dynamic treewidth algorithm of Korhonen, Majewski, Nadara, Pilipczuk, and Soko{\\l}owski [FOCS 2023].","sentences":["We give an algorithm that given a graph $G$ with $n$ vertices and $m$ edges and an integer $k$, in time $O_k(n^{1+o(1)})","+ O(m)$ either outputs a rank decomposition of $G$ of width at most $k$ or determines that the rankwidth of $G$ is larger than $k$; the $O_k(\\cdot)$-notation hides factors depending on $k$. Our algorithm returns also a $(2^{k+1}-1)$-expression for cliquewidth, yielding a $(2^{k+1}-1)$-approximation algorithm for cliquewidth with the same running time.","This improves upon the $O_k(n^2)$ time algorithm of Fomin and Korhonen","[STOC 2022].   ","The main ingredient of our algorithm is a fully dynamic algorithm for maintaining rank decompositions of bounded width: We give a data structure that for a dynamic $n$-vertex graph $G$ that is updated by edge insertions and deletions maintains a rank decomposition of $G$ of width at most $4k$ under the promise that the rankwidth of $G$ never grows above $k$.","The amortized running time of each update is $O_k(2^{\\sqrt{\\log n} \\log \\log","n})$.","The data structure furthermore can maintain whether $G$ satisfies some fixed ${\\sf CMSO}_1$ property within the same running time.","We also give a framework for performing ``dense'' edge updates inside a given set of vertices $X$, where the new edges inside $X$ are described by a given ${\\sf CMSO}_1$ sentence and vertex labels, in amortized $O_k(|X| \\cdot 2^{\\sqrt{\\log n} \\log \\log n})$ time.","Our dynamic algorithm generalizes the dynamic treewidth algorithm of Korhonen, Majewski, Nadara, Pilipczuk, and Soko{\\l}owski","[FOCS 2023]."],"url":"http://arxiv.org/abs/2402.12364v1","category":"cs.DS"}
{"created":"2024-02-19 18:24:01","title":"An optimal replacement policy under variable shocks and self-healing patterns","abstract":"We study a system that experiences damaging external shocks at stochastic intervals, continuous degradation, and self-healing. The motivation for such a system comes from real-life applications based on micro-electro-mechanical systems (MEMS). The system fails if the cumulative damage exceeds a time-dependent threshold. We develop a preventive maintenance policy to replace the system such that its lifetime is prudently utilized. Further, three variations on the healing pattern have been considered: (i) shocks heal for a fixed duration $\\tau$; (ii) a fixed proportion of shocks are non-healable (that is, $\\tau=0$); (iii) there are two types of shocks -- self healable shocks heal for a finite duration, and nonhealable shocks inflict a random system degradation. We implement a proposed preventive maintenance policy and compare the optimal replacement times in these new cases to that of the original case where all shocks heal indefinitely and thereby enable the system manager to take necessary decisions in generalized system set-ups.","sentences":["We study a system that experiences damaging external shocks at stochastic intervals, continuous degradation, and self-healing.","The motivation for such a system comes from real-life applications based on micro-electro-mechanical systems (MEMS).","The system fails if the cumulative damage exceeds a time-dependent threshold.","We develop a preventive maintenance policy to replace the system such that its lifetime is prudently utilized.","Further, three variations on the healing pattern have been considered: (i) shocks heal for a fixed duration $\\tau$; (ii) a fixed proportion of shocks are non-healable (that is, $\\tau=0$); (iii) there are two types of shocks -- self healable shocks heal for a finite duration, and nonhealable shocks inflict a random system degradation.","We implement a proposed preventive maintenance policy and compare the optimal replacement times in these new cases to that of the original case where all shocks heal indefinitely and thereby enable the system manager to take necessary decisions in generalized system set-ups."],"url":"http://arxiv.org/abs/2402.12349v1","category":"stat.ME"}
{"created":"2024-02-19 18:59:50","title":"Observation of a phase transition from a continuous to a discrete time crystal","abstract":"Discrete (DTCs) and continuous time crystals (CTCs) are novel dynamical many-body states, that are characterized by robust self-sustained oscillations, emerging via spontaneous breaking of discrete or continuous time translation symmetry. DTCs are periodically driven systems that oscillate with a subharmonic of the drive, while CTCs are driven continuously and oscillate with a system inherent frequency. Here, we explore a phase transition from a continuous time crystal to a discrete time crystal. A CTC with a characteristic oscillation frequency $\\omega_\\mathrm{CTC}$ is prepared in a continuously pumped atom-cavity system. Modulating the pump intensity of the CTC with a frequency $\\omega_{\\mathrm{dr}}$ close to $2\\,\\omega_\\mathrm{CTC}$ leads to robust locking of $\\omega_\\mathrm{CTC}$ to $\\omega_{\\mathrm{dr}}/2$, and hence a DTC arises. This phase transition in a quantum many-body system is related to subharmonic injection locking of non-linear mechanical and electronic oscillators or lasers.","sentences":["Discrete (DTCs) and continuous time crystals (CTCs) are novel dynamical many-body states, that are characterized by robust self-sustained oscillations, emerging via spontaneous breaking of discrete or continuous time translation symmetry.","DTCs are periodically driven systems that oscillate with a subharmonic of the drive, while CTCs are driven continuously and oscillate with a system inherent frequency.","Here, we explore a phase transition from a continuous time crystal to a discrete time crystal.","A CTC with a characteristic oscillation frequency $\\omega_\\mathrm{CTC}$ is prepared in a continuously pumped atom-cavity system.","Modulating the pump intensity of the CTC with a frequency $\\omega_{\\mathrm{dr}}$ close to $2\\,\\omega_\\mathrm{CTC}$ leads to robust locking of $\\omega_\\mathrm{CTC}$ to $\\omega_{\\mathrm{dr}}/2$, and hence a DTC arises.","This phase transition in a quantum many-body system is related to subharmonic injection locking of non-linear mechanical and electronic oscillators or lasers."],"url":"http://arxiv.org/abs/2402.12378v1","category":"quant-ph"}
{"created":"2024-02-19 18:59:07","title":"FiT: Flexible Vision Transformer for Diffusion Model","abstract":"Nature is infinitely resolution-free. In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To overcome this limitation, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios. Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens. This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping. Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation. Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions, showcasing its effectiveness both within and beyond its training resolution distribution. Repository available at https://github.com/whlzy/FiT.","sentences":["Nature is infinitely resolution-free.","In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain.","To overcome this limitation, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios.","Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens.","This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping.","Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation.","Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions, showcasing its effectiveness both within and beyond its training resolution distribution.","Repository available at https://github.com/whlzy/FiT."],"url":"http://arxiv.org/abs/2402.12376v1","category":"cs.CV"}
{"created":"2024-02-19 18:58:32","title":"Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding","abstract":"As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding. To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a given hardware platform. Evaluation shows that Sequoia improves the decoding speed of Llama2-7B, Llama2-13B, and Vicuna-33B on an A100 by up to $4.04\\times$, $3.84\\times$, and $2.37\\times$, and Llama2-70B offloading by up to $10.33\\times$ on L40.","sentences":["As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important.","While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware.","This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding.","To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens.","To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures.","Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a given hardware platform.","Evaluation shows that Sequoia improves the decoding speed of Llama2-7B, Llama2-13B, and Vicuna-33B on an A100 by up to $4.04\\times$, $3.84\\times$, and $2.37\\times$, and Llama2-70B offloading by up to $10.33\\times$ on L40."],"url":"http://arxiv.org/abs/2402.12374v1","category":"cs.CL"}
{"created":"2024-02-19 18:15:09","title":"Post-Minkowskian Theory Meets the Spinning Effective-One-Body Approach for Two-Body Scattering","abstract":"Effective-one-body (EOB) waveforms employed by the LIGO-Virgo-KAGRA Collaboration have primarily been developed by resumming the post-Newtonian expansion of the relativistic two-body problem. Given the recent significant advancements in post-Minkowskian (PM) theory and gravitational self-force formalism, there is considerable interest in creating waveform models that integrate information from various perturbative methods in innovative ways. This becomes particularly crucial when tackling the accuracy challenge posed by upcoming ground-based detectors (such as the Einstein Telescope and Cosmic Explorer) and space-based detectors (such as LISA, TianQin or Taiji) expected to operate in the next decade. In this context, we present the derivation of the first spinning EOB Hamiltonian that incorporates PM results up to three-loop order: the SEOB-PM model. The model accounts for the complete hyperbolic motion, encompassing nonlocal-in-time tails. To evaluate its accuracy, we compare its predictions for the conservative scattering angle, augmented with dissipative contributions, against numerical-relativity data of non-spinning and spinning equal-mass black holes. We observe very good agreement, comparable, and in some cases slightly better to the recently proposed $w_{\\rm EOB}$-potential model, of which the SEOB-PM model is a resummation around the probe limit. Indeed, in the probe limit, the SEOB-PM Hamiltonian and scattering angles reduce to the one of a test mass in Kerr spacetime. Once complemented with nonlocal-in-time contributions for bound orbits, the SEOB-PM Hamiltonian can be utilized to generate waveform models for spinning black holes on quasi-circular orbits.","sentences":["Effective-one-body (EOB) waveforms employed by the LIGO-Virgo-KAGRA Collaboration have primarily been developed by resumming the post-Newtonian expansion of the relativistic two-body problem.","Given the recent significant advancements in post-Minkowskian (PM) theory and gravitational self-force formalism, there is considerable interest in creating waveform models that integrate information from various perturbative methods in innovative ways.","This becomes particularly crucial when tackling the accuracy challenge posed by upcoming ground-based detectors (such as the Einstein Telescope and Cosmic Explorer) and space-based detectors (such as LISA, TianQin or Taiji) expected to operate in the next decade.","In this context, we present the derivation of the first spinning EOB Hamiltonian that incorporates PM results up to three-loop order: the SEOB-PM model.","The model accounts for the complete hyperbolic motion, encompassing nonlocal-in-time tails.","To evaluate its accuracy, we compare its predictions for the conservative scattering angle, augmented with dissipative contributions, against numerical-relativity data of non-spinning and spinning equal-mass black holes.","We observe very good agreement, comparable, and in some cases slightly better to the recently proposed $w_{\\rm EOB}$-potential model, of which the SEOB-PM model is a resummation around the probe limit.","Indeed, in the probe limit, the SEOB-PM Hamiltonian and scattering angles reduce to the one of a test mass in Kerr spacetime.","Once complemented with nonlocal-in-time contributions for bound orbits, the SEOB-PM Hamiltonian can be utilized to generate waveform models for spinning black holes on quasi-circular orbits."],"url":"http://arxiv.org/abs/2402.12342v1","category":"gr-qc"}
{"created":"2024-02-19 18:13:37","title":"Designed spin-texture-lattice to control anisotropic magnon transport in antiferromagnets","abstract":"Spin waves in magnetic materials are promising information carriers for future computing technologies due to their ultra-low energy dissipation and long coherence length. Antiferromagnets are strong candidate materials due, in part, to their stability to external fields and larger group velocities. Multiferroic aniferromagnets, such as BiFeO$_3$ (BFO), have an additional degree of freedom stemming from magnetoelectric coupling, allowing for control of the magnetic structure, and thus spin waves, with electric field. Unfortunately, spin-wave propagation in BFO is not well understood due to the complexity of the magnetic structure. In this work, we explore long-range spin transport within an epitaxially engineered, electrically tunable, one-dimensional (1D) magnonic crystal. We discover a striking anisotropy in the spin transport parallel and perpendicular to the 1D crystal axis. Multiscale theory and simulation suggests that this preferential magnon conduction emerges from a combination of a population imbalance in its dispersion, as well as anisotropic structural scattering. This work provides a pathway to electrically-reconfigurable magnonic crystals in antiferromagnets.","sentences":["Spin waves in magnetic materials are promising information carriers for future computing technologies due to their ultra-low energy dissipation and long coherence length.","Antiferromagnets are strong candidate materials due, in part, to their stability to external fields and larger group velocities.","Multiferroic aniferromagnets, such as BiFeO$_3$ (BFO), have an additional degree of freedom stemming from magnetoelectric coupling, allowing for control of the magnetic structure, and thus spin waves, with electric field.","Unfortunately, spin-wave propagation in BFO is not well understood due to the complexity of the magnetic structure.","In this work, we explore long-range spin transport within an epitaxially engineered, electrically tunable, one-dimensional (1D) magnonic crystal.","We discover a striking anisotropy in the spin transport parallel and perpendicular to the 1D crystal axis.","Multiscale theory and simulation suggests that this preferential magnon conduction emerges from a combination of a population imbalance in its dispersion, as well as anisotropic structural scattering.","This work provides a pathway to electrically-reconfigurable magnonic crystals in antiferromagnets."],"url":"http://arxiv.org/abs/2402.12341v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-19 17:37:11","title":"Cosserat Rod Modeling and Validation for a Soft Continuum Robot with Self-Controllable Variable Curvature","abstract":"This paper introduces a Cosserat rod based mathematical model for modeling a self-controllable variable curvature soft continuum robot. This soft continuum robot has a hollow inner channel and was developed with the ability to perform variable curvature utilizing a growing spine. The growing spine is able to grow and retract while modifies its stiffness through milli-size particle (glass bubble) granular jamming. This soft continuum robot can then perform continuous curvature variation, unlike previous approaches whose curvature variation is discrete and depends on the number of locking mechanisms or manual configurations. The robot poses an emergent modeling problem due to the variable stiffness growing spine which is addressed in this paper. We investigate the property of growing spine stiffness and incorporate it into the Cosserat rod model by implementing a combined stiffness approach. We conduct experiments with the soft continuum robot in various configurations and compared the results with our developed mathematical model. The results show that the mathematical model based on the adapted Cosserat rod matches the experimental results with only a 3.3\\% error with respect to the length of the soft continuum robot.","sentences":["This paper introduces a Cosserat rod based mathematical model for modeling a self-controllable variable curvature soft continuum robot.","This soft continuum robot has a hollow inner channel and was developed with the ability to perform variable curvature utilizing a growing spine.","The growing spine is able to grow and retract while modifies its stiffness through milli-size particle (glass bubble) granular jamming.","This soft continuum robot can then perform continuous curvature variation, unlike previous approaches whose curvature variation is discrete and depends on the number of locking mechanisms or manual configurations.","The robot poses an emergent modeling problem due to the variable stiffness growing spine which is addressed in this paper.","We investigate the property of growing spine stiffness and incorporate it into the Cosserat rod model by implementing a combined stiffness approach.","We conduct experiments with the soft continuum robot in various configurations and compared the results with our developed mathematical model.","The results show that the mathematical model based on the adapted Cosserat rod matches the experimental results with only a 3.3\\% error with respect to the length of the soft continuum robot."],"url":"http://arxiv.org/abs/2402.12315v1","category":"cs.RO"}
{"created":"2024-02-19 16:30:35","title":"End-to-end Supervised Prediction of Arbitrary-size Graphs with Partially-Masked Fused Gromov-Wasserstein Matching","abstract":"We present a novel end-to-end deep learning-based approach for Supervised Graph Prediction (SGP). We introduce an original Optimal Transport (OT)-based loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows to directly leverage graph representations such as adjacency and feature matrices. PM-FGW exhibits all the desirable properties for SGP: it is node permutation invariant, sub-differentiable and handles graphs of different sizes by comparing their padded representations as well as their masking vectors. Moreover, we present a flexible transformer-based architecture that easily adapts to different types of input data. In the experimental section, three different tasks, a novel and challenging synthetic dataset (image2graph) and two real-world tasks, image2map and fingerprint2molecule - showcase the efficiency and versatility of the approach compared to competitors.","sentences":["We present a novel end-to-end deep learning-based approach for Supervised Graph Prediction (SGP).","We introduce an original Optimal Transport (OT)-based loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows to directly leverage graph representations such as adjacency and feature matrices.","PM-FGW exhibits all the desirable properties for SGP: it is node permutation invariant, sub-differentiable and handles graphs of different sizes by comparing their padded representations as well as their masking vectors.","Moreover, we present a flexible transformer-based architecture that easily adapts to different types of input data.","In the experimental section, three different tasks, a novel and challenging synthetic dataset (image2graph) and two real-world tasks, image2map and fingerprint2molecule - showcase the efficiency and versatility of the approach compared to competitors."],"url":"http://arxiv.org/abs/2402.12269v1","category":"cs.LG"}
{"created":"2024-02-19 15:26:19","title":"Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting","abstract":"Although motivated by the adaptation of text-to-speech synthesis models, we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. However, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance, and using the Kronecker factored approximations produces a better preservation of the pre-training knowledge than the diagonal ones.","sentences":["Although motivated by the adaptation of text-to-speech synthesis models, we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation.","However, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities.","We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably.","In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation.","Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance, and using the Kronecker factored approximations produces a better preservation of the pre-training knowledge than the diagonal ones."],"url":"http://arxiv.org/abs/2402.12220v1","category":"eess.AS"}
{"created":"2024-02-19 15:00:51","title":"Nanomechanical crystalline AlN resonators with high quality factors for quantum optoelectromechanics","abstract":"High-$Q_m$ mechanical resonators are crucial for applications where low noise and long coherence time are required, as mirror suspensions, quantum cavity optomechanical devices, or nanomechanical sensors. Tensile strain in the material enables the use of dissipation dilution and strain engineering techniques, which increase the mechanical quality factor. These techniques have been employed for high-$Q_m$ mechanical resonators made from amorphous materials and, recently, from crystalline materials such as InGaP, SiC, and Si. A strained crystalline film exhibiting substantial piezoelectricity expands the capability of high-$Q_m$ nanomechanical resonators to directly utilize electronic degrees of freedom. In this work we realize nanomechanical resonators with $Q_m$ up to $2.9\\times 10^{7}$ made from tensile-strained 290 nm-thick AlN, which is an epitaxially-grown crystalline material offering strong piezoelectricity. We demonstrate nanomechanical resonators that exploit dissipation dilution and strain engineering to reach a $Q_m \\times f_m$-product approaching $10^{13}$ Hz at room temperature. We realize a novel resonator geometry, triangline, whose shape follows the Al-N bonds and offers a central pad that we pattern with a photonic crystal. This allows us to reach an optical reflectivity above 80% for efficient coupling to out-of-plane light. The presented results pave the way for quantum optoelectromechanical devices at room temperature based on tensile-strained AlN.","sentences":["High-$Q_m$ mechanical resonators are crucial for applications where low noise and long coherence time are required, as mirror suspensions, quantum cavity optomechanical devices, or nanomechanical sensors.","Tensile strain in the material enables the use of dissipation dilution and strain engineering techniques, which increase the mechanical quality factor.","These techniques have been employed for high-$Q_m$ mechanical resonators made from amorphous materials and, recently, from crystalline materials such as InGaP, SiC, and Si.","A strained crystalline film exhibiting substantial piezoelectricity expands the capability of high-$Q_m$ nanomechanical resonators to directly utilize electronic degrees of freedom.","In this work we realize nanomechanical resonators with $Q_m$ up to $2.9\\times 10^{7}$ made from tensile-strained 290 nm-thick AlN, which is an epitaxially-grown crystalline material offering strong piezoelectricity.","We demonstrate nanomechanical resonators that exploit dissipation dilution and strain engineering to reach a $Q_m \\times f_m$-product approaching $10^{13}$ Hz at room temperature.","We realize a novel resonator geometry, triangline, whose shape follows the Al-N bonds and offers a central pad that we pattern with a photonic crystal.","This allows us to reach an optical reflectivity above 80% for efficient coupling to out-of-plane light.","The presented results pave the way for quantum optoelectromechanical devices at room temperature based on tensile-strained AlN."],"url":"http://arxiv.org/abs/2402.12196v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-19 14:42:29","title":"A Riemannian rank-adaptive method for higher-order tensor completion in the tensor-train format","abstract":"In this paper a new Riemannian rank adaptive method (RRAM) is proposed for the low-rank tensor completion problem (LRTCP) formulated as a least-squares optimization problem on the algebraic variety of tensors of bounded tensor-train (TT) rank. The method iteratively optimizes over fixed-rank smooth manifolds using a Riemannian conjugate gradient algorithm from Steinlechner (2016) and gradually increases the rank by computing a descent direction in the tangent cone to the variety. Additionally, a numerical method to estimate the amount of rank increase is proposed based on a theoretical result for the stationary points of the low-rank tensor approximation problem and a definition of an estimated TT-rank. Furthermore, when the iterate comes close to a lower-rank set, the RRAM decreases the rank based on the TT-rounding algorithm from Oseledets (2011) and a definition of a numerical rank. We prove that the TT-rounding algorithm can be considered as an approximate projection onto the lower-rank set which satisfies a certain angle condition to ensure that the image is sufficiently close to that of an exact projection. Several numerical experiments are given to illustrate the use of the RRAM and its subroutines in {\\Matlab}. Furthermore, in all experiments the proposed RRAM outperforms the state-of-the-art RRAM for tensor completion in the TT format from Steinlechner (2016) in terms of computation time.","sentences":["In this paper a new Riemannian rank adaptive method (RRAM) is proposed for the low-rank tensor completion problem (LRTCP) formulated as a least-squares optimization problem on the algebraic variety of tensors of bounded tensor-train (TT) rank.","The method iteratively optimizes over fixed-rank smooth manifolds using a Riemannian conjugate gradient algorithm from Steinlechner (2016) and gradually increases the rank by computing a descent direction in the tangent cone to the variety.","Additionally, a numerical method to estimate the amount of rank increase is proposed based on a theoretical result for the stationary points of the low-rank tensor approximation problem and a definition of an estimated TT-rank.","Furthermore, when the iterate comes close to a lower-rank set, the RRAM decreases the rank based on the TT-rounding algorithm from Oseledets (2011) and a definition of a numerical rank.","We prove that the TT-rounding algorithm can be considered as an approximate projection onto the lower-rank set which satisfies a certain angle condition to ensure that the image is sufficiently close to that of an exact projection.","Several numerical experiments are given to illustrate the use of the RRAM and its subroutines in {\\Matlab}.","Furthermore, in all experiments the proposed RRAM outperforms the state-of-the-art RRAM for tensor completion in the TT format from Steinlechner (2016) in terms of computation time."],"url":"http://arxiv.org/abs/2402.12182v1","category":"math.OC"}
{"created":"2024-02-19 14:26:58","title":"Second-order flows for approaching stationary points of a class of non-convex energies via convex-splitting schemes","abstract":"The use of accelerated gradient flows is an emerging field in optimization, scientific computing and beyond. This paper contributes to the theoretical underpinnings of a recently-introduced computational paradigm known as second-order flows, which demonstrate significant performance particularly for the minimization of non-convex energy functionals defined on Sobolev spaces, and are characterized by novel dissipative hyperbolic partial differential equations. Our approach hinges upon convex-splitting schemes, a tool which is not only pivotal for clarifying the well-posedness of second-order flows, but also yields a versatile array of robust numerical schemes through temporal and spatial discretization. We prove the convergence to stationary points of such schemes in the semi-discrete setting. Further, we establish their convergence to time-continuous solutions as the time-step tends to zero, and perform a comprehensive error analysis in the fully discrete case. Finally, these algorithms undergo thorough testing and validation in approaching stationary points of non-convex variational models in applied sciences, such as the Ginzburg-Landau energy in phase-field modeling and a specific case of the Landau-de Gennes energy of the Q-tensor model for liquid crystals.","sentences":["The use of accelerated gradient flows is an emerging field in optimization, scientific computing and beyond.","This paper contributes to the theoretical underpinnings of a recently-introduced computational paradigm known as second-order flows, which demonstrate significant performance particularly for the minimization of non-convex energy functionals defined on Sobolev spaces, and are characterized by novel dissipative hyperbolic partial differential equations.","Our approach hinges upon convex-splitting schemes, a tool which is not only pivotal for clarifying the well-posedness of second-order flows, but also yields a versatile array of robust numerical schemes through temporal and spatial discretization.","We prove the convergence to stationary points of such schemes in the semi-discrete setting.","Further, we establish their convergence to time-continuous solutions as the time-step tends to zero, and perform a comprehensive error analysis in the fully discrete case.","Finally, these algorithms undergo thorough testing and validation in approaching stationary points of non-convex variational models in applied sciences, such as the Ginzburg-Landau energy in phase-field modeling and a specific case of the Landau-de Gennes energy of the Q-tensor model for liquid crystals."],"url":"http://arxiv.org/abs/2402.12173v1","category":"math.NA"}
{"created":"2024-02-19 14:26:41","title":"Large $N$ Schur index of $\\mathcal N=4$ SYM from semiclassical D3 brane","abstract":"We consider the refined Schur superconformal index of 4d $\\mathcal N=4$ $U(N)$ SYM and the first term of its giant-graviton expansion, first predicted in arXiv:2001.11667 using indirect superconformal algebra considerations and analytic continuation of fugacities. This correction is the leading non-perturbative correction to the index at large $N$ and we reproduce it from the semiclassical partition function of quantum D3 brane wrapped on $S^{1}\\times S^{3}$ in a twisted modification of the $AdS_{5}\\times S^{5}$ string background, depending on the index R-symmetry fugacity. Our calculation does not exploit directly supersymmetry. It is based on the determination of the partition function of the various bosonic and fermionic fluctuations on the wrapped brane whose action is conformal with specific constant holonomies along thermal cycle. We show how those partition functions may be obtained by adapting the operator counting method of Cardy to the twisted background.","sentences":["We consider the refined Schur superconformal index of 4d $\\mathcal N=4$ $U(N)$ SYM and the first term of its giant-graviton expansion, first predicted in arXiv:2001.11667 using indirect superconformal algebra considerations and analytic continuation of fugacities.","This correction is the leading non-perturbative correction to the index at large $N$ and we reproduce it from the semiclassical partition function of quantum D3 brane wrapped on $S^{1}\\times S^{3}$ in a twisted modification of the $AdS_{5}\\times S^{5}$ string background, depending on the index R-symmetry fugacity.","Our calculation does not exploit directly supersymmetry.","It is based on the determination of the partition function of the various bosonic and fermionic fluctuations on the wrapped brane whose action is conformal with specific constant holonomies along thermal cycle.","We show how those partition functions may be obtained by adapting the operator counting method of Cardy to the twisted background."],"url":"http://arxiv.org/abs/2402.12172v1","category":"hep-th"}
{"created":"2024-02-19 13:16:10","title":"Almost sure convergence rates of adaptive increasingly rare Markov chain Monte Carlo","abstract":"We consider adaptive increasingly rare Markov chain Monte Carlo (AIR MCMC), which is an adaptive MCMC method, where the adaptation concerning the past happens less and less frequently over time. Under a contraction assumption for a Wasserstein-like function we deduce upper bounds of the convergence rate of Monte Carlo sums taking a renormalisation factor into account that is close to the one that appears in a law of the iterated logarithm. We demonstrate the applicability of our results by considering different settings, among which are those of simultaneous geometric and uniform ergodicity. All proofs are carried out on an augmented state space, including the classical non-augmented setting as a special case. In contrast to other adaptive MCMC limit theory, some technical assumptions, like diminishing adaptation, are not needed.","sentences":["We consider adaptive increasingly rare Markov chain Monte Carlo (AIR MCMC), which is an adaptive MCMC method, where the adaptation concerning the past happens less and less frequently over time.","Under a contraction assumption for a Wasserstein-like function we deduce upper bounds of the convergence rate of Monte Carlo sums taking a renormalisation factor into account that is close to the one that appears in a law of the iterated logarithm.","We demonstrate the applicability of our results by considering different settings, among which are those of simultaneous geometric and uniform ergodicity.","All proofs are carried out on an augmented state space, including the classical non-augmented setting as a special case.","In contrast to other adaptive MCMC limit theory, some technical assumptions, like diminishing adaptation, are not needed."],"url":"http://arxiv.org/abs/2402.12122v1","category":"math.NA"}
{"created":"2024-02-19 13:16:09","title":"TASTE V. A new ground-based investigation of orbital decay in the ultra-hot Jupiter WASP-12b","abstract":"The discovery of the first transiting hot Jupiters (HJs; giant planets on orbital periods shorter than $P\\sim10$ days) was announced more than twenty years ago. As both ground- and space-based follow-up observations are piling up, we are approaching the temporal baseline required to detect secular variations in their orbital parameters. In particular, several recent studies focused on constraining the efficiency of the tidal decay mechanism to better understand the evolutionary time scales of HJ migration and engulfment. This can be achieved by measuring a monotonic decrease of orbital period $\\mathrm{d}P/\\mathrm{d}t<0$ due to mechanical energy being dissipated by tidal friction. WASP-12b was the first HJ for which a tidal decay scenario appeared convincing, even though alternative explanations have been hypothesized. Here we present a new analysis based on 28 unpublished high-precision transit light curves gathered over a twelve-year baseline and combined with all the available archival data, and an updated set of stellar parameters from HARPS-N high-resolution spectra, which are consistent with a main sequence scenario, close to the hydrogen exhaustion in the core. Our values of $\\mathrm{d}P/\\mathrm{d}t$ = $-30.72 \\pm 2.67$ and $Q_{\\ast}^{'}$ = $(2.13 \\pm 0.18) \\times 10^{5}$ are statistically consistent with previous studies, and indicate that WASP-12 is undergoing fast tidal dissipation. We additionally report the presence of an excess scatter in the timing data and discuss its possible origin.","sentences":["The discovery of the first transiting hot Jupiters (HJs; giant planets on orbital periods shorter than $P\\sim10$ days) was announced more than twenty years ago.","As both ground- and space-based follow-up observations are piling up, we are approaching the temporal baseline required to detect secular variations in their orbital parameters.","In particular, several recent studies focused on constraining the efficiency of the tidal decay mechanism to better understand the evolutionary time scales of HJ migration and engulfment.","This can be achieved by measuring a monotonic decrease of orbital period $\\mathrm{d}P/\\mathrm{d}t<0$ due to mechanical energy being dissipated by tidal friction.","WASP-12b was the first HJ for which a tidal decay scenario appeared convincing, even though alternative explanations have been hypothesized.","Here we present a new analysis based on 28 unpublished high-precision transit light curves gathered over a twelve-year baseline and combined with all the available archival data, and an updated set of stellar parameters from HARPS-N high-resolution spectra, which are consistent with a main sequence scenario, close to the hydrogen exhaustion in the core.","Our values of $\\mathrm{d}P/\\mathrm{d}t$ = $-30.72 \\pm 2.67$ and $Q_{\\ast}^{'}$ = $(2.13 \\pm 0.18)","\\times 10^{5}$ are statistically consistent with previous studies, and indicate that WASP-12 is undergoing fast tidal dissipation.","We additionally report the presence of an excess scatter in the timing data and discuss its possible origin."],"url":"http://arxiv.org/abs/2402.12120v2","category":"astro-ph.EP"}
{"created":"2024-02-19 12:11:20","title":"Strengths and Weaknesses of the ETSI Adaptive DCC Algorithm: A Proposal for Improvement","abstract":"This letter studies the adaptive Decentralized Congestion Control (DCC) algorithm defined in the ETSI TS 102 687 V1.2.1 specification. We provide insights on the parameters used in the algorithm and explore the impact of those parameters on its performance. We show how the algorithm achieves good average medium utilization while protecting against congestion, but we also show how the chosen parameters can result in slow speed of convergence and long periods of unfairness in transitory situations. Finally, we propose a modification to the algorithm which results in significant improvements in speed of convergence and fairness.","sentences":["This letter studies the adaptive Decentralized Congestion Control (DCC)","algorithm defined in the ETSI TS 102 687","V1.2.1 specification.","We provide insights on the parameters used in the algorithm and explore the impact of those parameters on its performance.","We show how the algorithm achieves good average medium utilization while protecting against congestion, but we also show how the chosen parameters can result in slow speed of convergence and long periods of unfairness in transitory situations.","Finally, we propose a modification to the algorithm which results in significant improvements in speed of convergence and fairness."],"url":"http://arxiv.org/abs/2402.12089v1","category":"cs.NI"}
{"created":"2024-02-19 11:55:17","title":"Single and Multi-Objective Real-Time Optimisation of an Industrial Injection Moulding Process via a Bayesian Adaptive Design of Experiment Approach","abstract":"Minimising cycle time without inducing quality defects is a major challenge in the injection moulding (IM). Design of Experiment methods (DoE) have been widely studied for optimisation of the IM, however existing methods have limitations, including the need for a large number of experiments and a pre-determined search space. Bayesian adaptive design of experiment (ADoE) is an iterative process where the results of the previous experiments are used to make an informed selection for the next design. In this study, for the first time, an experimental ADoE approach, based on Bayesian optimisation, was developed in injection moulding using process and sensor data to optimise the quality and cycle time in real-time. A novel approach for the real-time characterisation of post-production shrinkage was introduced, utilising in-mould sensor data on temperature differential during part cooling. This characterisation approach was verified by post-production metrology results.   A single and multi-objective optimisation of the cycle time and temperature differential in an injection moulded component is proposed. The multi-objective optimisation techniques, composite desirability function and Nondominated Sorting Genetic Algorithm (NSGA-II) using Response Surface Methodology (RSM) model, are compared with the real-time novel ADoE approach. ADoE achieved almost a 50% reduction in the number of experiments required for the single optimisation of temperature differential, and an almost 30% decrease for the optimisation of temperature differential and cycle time together compared to composite desirability function and NSGA-II. Also, the optimal settings identified by ADoE for multiobjective optimisation were similar to the selected Pareto optimal solution found by the NSGA-II.","sentences":["Minimising cycle time without inducing quality defects is a major challenge in the injection moulding (IM).","Design of Experiment methods (DoE) have been widely studied for optimisation of the IM, however existing methods have limitations, including the need for a large number of experiments and a pre-determined search space.","Bayesian adaptive design of experiment (ADoE) is an iterative process where the results of the previous experiments are used to make an informed selection for the next design.","In this study, for the first time, an experimental ADoE approach, based on Bayesian optimisation, was developed in injection moulding using process and sensor data to optimise the quality and cycle time in real-time.","A novel approach for the real-time characterisation of post-production shrinkage was introduced, utilising in-mould sensor data on temperature differential during part cooling.","This characterisation approach was verified by post-production metrology results.   ","A single and multi-objective optimisation of the cycle time and temperature differential in an injection moulded component is proposed.","The multi-objective optimisation techniques, composite desirability function and Nondominated Sorting Genetic Algorithm (NSGA-II) using Response Surface Methodology (RSM) model, are compared with the real-time novel ADoE approach.","ADoE achieved almost a 50% reduction in the number of experiments required for the single optimisation of temperature differential, and an almost 30% decrease for the optimisation of temperature differential and cycle time together compared to composite desirability function and NSGA-II.","Also, the optimal settings identified by ADoE for multiobjective optimisation were similar to the selected Pareto optimal solution found by the NSGA-II."],"url":"http://arxiv.org/abs/2402.12077v1","category":"eess.SY"}
{"created":"2024-02-19 11:02:05","title":"Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models","abstract":"Catastrophic forgetting emerges as a critical challenge when fine-tuning multi-modal large language models (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks. This paper presents a comprehensive analysis of catastrophic forgetting in MLLMs and introduces a post-training adjustment method called Model Tailor. Our method primarily preserves the pre-trained parameters while replacing a small number ($\\leq$ 10\\%) of fine-tuned parameters, maintaining $\\sim$ 99\\% effectiveness on original tasks versus pre-training, and achieving $\\sim$ 97\\% on new tasks compared to standard fine-tuning. Specifically, we derive a sparse mask to identify the \"model patch\", based on a fusion strategy that integrates salience and sensitivity analysis. Subsequently, a compensation mechanism is introduced to \"decorate the patch\", enhancing the model's performance on both target and original tasks. Additionally, our method is adaptable to multi-task scenarios. Through extensive experiments on InstructBLIP and LLaVA-1.5 in both image captioning and visual question answering tasks, our approach demonstrates significant task adaptability while preserving inherent pre-trained capabilities.","sentences":["Catastrophic forgetting emerges as a critical challenge when fine-tuning multi-modal large language models (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks.","This paper presents a comprehensive analysis of catastrophic forgetting in MLLMs and introduces a post-training adjustment method called Model Tailor.","Our method primarily preserves the pre-trained parameters while replacing a small number ($\\leq$ 10\\%) of fine-tuned parameters, maintaining $\\sim$ 99\\% effectiveness on original tasks versus pre-training, and achieving $\\sim$ 97\\% on new tasks compared to standard fine-tuning.","Specifically, we derive a sparse mask to identify the \"model patch\", based on a fusion strategy that integrates salience and sensitivity analysis.","Subsequently, a compensation mechanism is introduced to \"decorate the patch\", enhancing the model's performance on both target and original tasks.","Additionally, our method is adaptable to multi-task scenarios.","Through extensive experiments on InstructBLIP and LLaVA-1.5 in both image captioning and visual question answering tasks, our approach demonstrates significant task adaptability while preserving inherent pre-trained capabilities."],"url":"http://arxiv.org/abs/2402.12048v1","category":"cs.CL"}
{"created":"2024-02-19 10:43:27","title":"Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics","abstract":"Recent advances in pre-trained language modeling have facilitated significant progress across various natural language processing (NLP) tasks. Word masking during model training constitutes a pivotal component of language modeling in architectures like BERT. However, the prevalent method of word masking relies on random selection, potentially disregarding domain-specific linguistic attributes. In this article, we introduce an innovative masking approach leveraging genre and topicality information to tailor language models to specialized domains. Our method incorporates a ranking process that prioritizes words based on their significance, subsequently guiding the masking procedure. Experiments conducted using continual pre-training within the legal domain have underscored the efficacy of our approach on the LegalGLUE benchmark in the English language. Pre-trained language models and code are freely available for use.","sentences":["Recent advances in pre-trained language modeling have facilitated significant progress across various natural language processing (NLP) tasks.","Word masking during model training constitutes a pivotal component of language modeling in architectures like BERT.","However, the prevalent method of word masking relies on random selection, potentially disregarding domain-specific linguistic attributes.","In this article, we introduce an innovative masking approach leveraging genre and topicality information to tailor language models to specialized domains.","Our method incorporates a ranking process that prioritizes words based on their significance, subsequently guiding the masking procedure.","Experiments conducted using continual pre-training within the legal domain have underscored the efficacy of our approach on the LegalGLUE benchmark in the English language.","Pre-trained language models and code are freely available for use."],"url":"http://arxiv.org/abs/2402.12036v1","category":"cs.CL"}
{"created":"2024-02-19 10:34:48","title":"Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space","abstract":"Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning. Experimental results demonstrate that MuScleLoRA outperforms baselines significantly. Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15\\% across multiple datasets and generalizes to various backbone LMs, including BERT, RoBERTa, and Llama2. The codes are available at https://github.com/ZrW00/MuScleLoRA.","sentences":["Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks.","Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios.","In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis.","Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping.","To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters.","Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning.","Experimental results demonstrate that MuScleLoRA outperforms baselines significantly.","Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15\\% across multiple datasets and generalizes to various backbone LMs, including BERT, RoBERTa, and Llama2.","The codes are available at https://github.com/ZrW00/MuScleLoRA."],"url":"http://arxiv.org/abs/2402.12026v1","category":"cs.CL"}
{"created":"2024-02-19 10:13:25","title":"An Index Policy Based on Sarsa and Q-learning for Heterogeneous Smart Target Tracking","abstract":"In solving the non-myopic radar scheduling for multiple smart target tracking within an active and passive radar network, we need to consider both short-term enhanced tracking performance and a higher probability of target maneuvering in the future with active tracking. Acquiring the long-term tracking performance while scheduling the beam resources of active and passive radars poses a challenge. To address this challenge, we model this problem as a Markov decision process consisting of parallel restless bandit processes. Each bandit process is associated with a smart target, of which the estimation state evolves according to different discrete dynamic models for different actions - whether or not the target is being tracked. The discrete state is defined by the dynamic mode. The problem exhibits the curse of dimensionality, where optimal solutions are in general intractable. We resort to heuristics through the famous restless multi-armed bandit techniques. It follows with efficient scheduling policies based on the indices that are real numbers representing the marginal rewards of taking different actions. For the inevitable practical case with unknown transition matrices, we propose a new method that utilizes the forward Sarsa and backward Q-learning to approximate the indices through adapting the state-action value functions, or equivalently the Q-functions, and propose a new policy, namely ISQ, aiming to maximize the long-term tracking rewards. Numerical results demonstrate that the proposed ISQ policy outperforms conventional Q-learning-based methods and rapidly converges to the well-known Whittle index policy with revealed state transition models, which is considered the benchmark.","sentences":["In solving the non-myopic radar scheduling for multiple smart target tracking within an active and passive radar network, we need to consider both short-term enhanced tracking performance and a higher probability of target maneuvering in the future with active tracking.","Acquiring the long-term tracking performance while scheduling the beam resources of active and passive radars poses a challenge.","To address this challenge, we model this problem as a Markov decision process consisting of parallel restless bandit processes.","Each bandit process is associated with a smart target, of which the estimation state evolves according to different discrete dynamic models for different actions - whether or not the target is being tracked.","The discrete state is defined by the dynamic mode.","The problem exhibits the curse of dimensionality, where optimal solutions are in general intractable.","We resort to heuristics through the famous restless multi-armed bandit techniques.","It follows with efficient scheduling policies based on the indices that are real numbers representing the marginal rewards of taking different actions.","For the inevitable practical case with unknown transition matrices, we propose a new method that utilizes the forward Sarsa and backward Q-learning to approximate the indices through adapting the state-action value functions, or equivalently the Q-functions, and propose a new policy, namely ISQ, aiming to maximize the long-term tracking rewards.","Numerical results demonstrate that the proposed ISQ policy outperforms conventional Q-learning-based methods and rapidly converges to the well-known Whittle index policy with revealed state transition models, which is considered the benchmark."],"url":"http://arxiv.org/abs/2402.12015v1","category":"eess.SY"}
{"created":"2024-02-19 10:02:10","title":"Moduli of Continuity in Metric Models and Extension of Liveability Indices","abstract":"Index spaces serve as valuable metric models for studying properties relevant to various applications, such as social science or economics. These properties are represented by real Lipschitz functions that describe the degree of association with each element within the underlying metric space. After determining the index value within a given sample subset, the classic McShane and Whitney formulas allow a Lipschitz regression procedure to be performed to extend the index values over the entire metric space. To improve the adaptability of the metric model to specific scenarios, this paper introduces the concept of a composition metric, which involves composing a metric with an increasing, positive and subadditive function $\\phi$. The results presented here extend well-established results for Lipschitz indices on metric spaces to composition metrics. In addition, we establish the corresponding approximation properties that facilitate the use of this functional structure. To illustrate the power and simplicity of this mathematical framework, we provide a concrete application involving the modelling of livability indices in North American cities.","sentences":["Index spaces serve as valuable metric models for studying properties relevant to various applications, such as social science or economics.","These properties are represented by real Lipschitz functions that describe the degree of association with each element within the underlying metric space.","After determining the index value within a given sample subset, the classic McShane and Whitney formulas allow a Lipschitz regression procedure to be performed to extend the index values over the entire metric space.","To improve the adaptability of the metric model to specific scenarios, this paper introduces the concept of a composition metric, which involves composing a metric with an increasing, positive and subadditive function $\\phi$. The results presented here extend well-established results for Lipschitz indices on metric spaces to composition metrics.","In addition, we establish the corresponding approximation properties that facilitate the use of this functional structure.","To illustrate the power and simplicity of this mathematical framework, we provide a concrete application involving the modelling of livability indices in North American cities."],"url":"http://arxiv.org/abs/2402.12009v1","category":"stat.ME"}
{"created":"2024-02-19 09:32:48","title":"Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models","abstract":"Low-rank adaptation (LoRA) is an efficient strategy for adapting latent diffusion models (LDMs) on a training dataset to generate specific objects by minimizing the adaptation loss. However, adapted LDMs via LoRA are vulnerable to membership inference (MI) attacks that can judge whether a particular data point belongs to private training datasets, thus facing severe risks of privacy leakage. To defend against MI attacks, we make the first effort to propose a straightforward solution: privacy-preserving LoRA (PrivateLoRA). PrivateLoRA is formulated as a min-max optimization problem where a proxy attack model is trained by maximizing its MI gain while the LDM is adapted by minimizing the sum of the adaptation loss and the proxy attack model's MI gain. However, we empirically disclose that PrivateLoRA has the issue of unstable optimization due to the large fluctuation of the gradient scale which impedes adaptation. To mitigate this issue, we propose Stable PrivateLoRA that adapts the LDM by minimizing the ratio of the adaptation loss to the MI gain, which implicitly rescales the gradient and thus stabilizes the optimization. Our comprehensive empirical results corroborate that adapted LDMs via Stable PrivateLoRA can effectively defend against MI attacks while generating high-quality images. Our code is available at https://github.com/WilliamLUO0/StablePrivateLoRA.","sentences":["Low-rank adaptation (LoRA) is an efficient strategy for adapting latent diffusion models (LDMs) on a training dataset to generate specific objects by minimizing the adaptation loss.","However, adapted LDMs via LoRA are vulnerable to membership inference (MI) attacks that can judge whether a particular data point belongs to private training datasets, thus facing severe risks of privacy leakage.","To defend against MI attacks, we make the first effort to propose a straightforward solution: privacy-preserving LoRA (PrivateLoRA).","PrivateLoRA is formulated as a min-max optimization problem where a proxy attack model is trained by maximizing its MI gain while the LDM is adapted by minimizing the sum of the adaptation loss and the proxy attack model's MI gain.","However, we empirically disclose that PrivateLoRA has the issue of unstable optimization due to the large fluctuation of the gradient scale which impedes adaptation.","To mitigate this issue, we propose Stable PrivateLoRA that adapts the LDM by minimizing the ratio of the adaptation loss to the MI gain, which implicitly rescales the gradient and thus stabilizes the optimization.","Our comprehensive empirical results corroborate that adapted LDMs via Stable PrivateLoRA can effectively defend against MI attacks while generating high-quality images.","Our code is available at https://github.com/WilliamLUO0/StablePrivateLoRA."],"url":"http://arxiv.org/abs/2402.11989v1","category":"cs.LG"}
{"created":"2024-02-19 09:26:22","title":"Buffered Streaming Edge Partitioning","abstract":"Addressing the challenges of processing massive graphs, which are prevalent in diverse fields such as social, biological, and technical networks, we introduce HeiStreamE and FreightE, two innovative (buffered) streaming algorithms designed for efficient edge partitioning of large-scale graphs. HeiStreamE utilizes an adapted Split-and-Connect graph model and a Fennel-based multilevel partitioning scheme, while FreightE partitions a hypergraph representation of the input graph. Besides ensuring superior solution quality, these approaches also overcome the limitations of existing algorithms by maintaining linear dependency on the graph size in both time and memory complexity with no dependence on the number of blocks of partition. Our comprehensive experimental analysis demonstrates that HeiStreamE outperforms current streaming algorithms and the re-streaming algorithm 2PS in partitioning quality (replication factor), and is more memory-efficient for real-world networks where the number of edges is far greater than the number of vertices. Further, FreightE is shown to produce fast and efficient partitions, particularly for higher numbers of partition blocks.","sentences":["Addressing the challenges of processing massive graphs, which are prevalent in diverse fields such as social, biological, and technical networks, we introduce HeiStreamE and FreightE, two innovative (buffered) streaming algorithms designed for efficient edge partitioning of large-scale graphs.","HeiStreamE utilizes an adapted Split-and-Connect graph model and a Fennel-based multilevel partitioning scheme, while FreightE partitions a hypergraph representation of the input graph.","Besides ensuring superior solution quality, these approaches also overcome the limitations of existing algorithms by maintaining linear dependency on the graph size in both time and memory complexity with no dependence on the number of blocks of partition.","Our comprehensive experimental analysis demonstrates that HeiStreamE outperforms current streaming algorithms and the re-streaming algorithm 2PS in partitioning quality (replication factor), and is more memory-efficient for real-world networks where the number of edges is far greater than the number of vertices.","Further, FreightE is shown to produce fast and efficient partitions, particularly for higher numbers of partition blocks."],"url":"http://arxiv.org/abs/2402.11980v1","category":"cs.DS"}
{"created":"2024-02-19 09:18:43","title":"Going Beyond Perfect Absorption: Reconfigurable Super-directive Absorbers","abstract":"In the context of electromagnetic absorption, it is obvious that for an infinite planar periodic structure illuminated by a plane wave, the maximum attainable absorptance, i.e., perfect absorption, is theoretically limited to 100% of the incident power. Here we show that an intriguing possibility of overcoming this limit arises in finite-size resonant absorbing arrays. We present a comprehensive analysis of a simple two-dimensional strip array over an infinite perfectly conducting plane, where the strips are loaded by reconfigurable impedance loads. The absorptance is defined as the ratio of the dissipated power per unit length of the strips to the incident power on the unit length of the array width. The results show that even regular arrays of impedance strips can slightly overcome the limit of 100% absorptance, while using aperiodic arrays with optimized loads, absorptance can be significantly increased as compared with the scenario where the strips are identical. In principle, by tuning the reconfigurable loads, high super-unity absorptance can be realized for all angles of illumination.","sentences":["In the context of electromagnetic absorption, it is obvious that for an infinite planar periodic structure illuminated by a plane wave, the maximum attainable absorptance, i.e., perfect absorption, is theoretically limited to 100% of the incident power.","Here we show that an intriguing possibility of overcoming this limit arises in finite-size resonant absorbing arrays.","We present a comprehensive analysis of a simple two-dimensional strip array over an infinite perfectly conducting plane, where the strips are loaded by reconfigurable impedance loads.","The absorptance is defined as the ratio of the dissipated power per unit length of the strips to the incident power on the unit length of the array width.","The results show that even regular arrays of impedance strips can slightly overcome the limit of 100% absorptance, while using aperiodic arrays with optimized loads, absorptance can be significantly increased as compared with the scenario where the strips are identical.","In principle, by tuning the reconfigurable loads, high super-unity absorptance can be realized for all angles of illumination."],"url":"http://arxiv.org/abs/2402.11971v1","category":"physics.app-ph"}
{"created":"2024-02-19 08:27:14","title":"CRAP Part II: Clutter Removal with Continuous Acquisitions Under Phase Noise","abstract":"The mitigation of clutter is an important research branch in Integrated Sensing and Communication (ISAC), one of the emerging technologies of future cellular networks. In this work, we extend our previously introduced method Clutter Removal with Acquisitions Under Phase Noise (CRAP) by means to track clutter over time. This is necessary in scenarios that require high reliability but can change dynamically, like safety applications in factory floors. To that end, exponential smoothing is leveraged to process new measurements and previous clutter information in a unique matrix using the singular value decomposition, allowing adaptation to changing environments in an efficient way.We further propose a singular value threshold based on the Marchenko-Pastur distribution to select the meaningful clutter components. Results from both simulations and measurements show that continuously updating the clutter components with new acquisitions according to our proposed algorithm Smoothed CRAP (SCRAP) enables coping with dynamic clutter environments and facilitates the detection of sensing targets.","sentences":["The mitigation of clutter is an important research branch in Integrated Sensing and Communication (ISAC), one of the emerging technologies of future cellular networks.","In this work, we extend our previously introduced method Clutter Removal with Acquisitions Under Phase Noise (CRAP) by means to track clutter over time.","This is necessary in scenarios that require high reliability but can change dynamically, like safety applications in factory floors.","To that end, exponential smoothing is leveraged to process new measurements and previous clutter information in a unique matrix using the singular value decomposition, allowing adaptation to changing environments in an efficient way.","We further propose a singular value threshold based on the Marchenko-Pastur distribution to select the meaningful clutter components.","Results from both simulations and measurements show that continuously updating the clutter components with new acquisitions according to our proposed algorithm Smoothed CRAP (SCRAP) enables coping with dynamic clutter environments and facilitates the detection of sensing targets."],"url":"http://arxiv.org/abs/2402.11939v1","category":"eess.SP"}
{"created":"2024-02-19 08:22:51","title":"Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting AI-generated Text","abstract":"This paper presents the participation of team QUST in Task 8 SemEval 2024. We first performed data augmentation and cleaning on the dataset to enhance model training efficiency and accuracy. In the monolingual task, we evaluated traditional deep-learning methods, multiscale positive-unlabeled framework (MPU), fine-tuning, adapters and ensemble methods. Then, we selected the top-performing models based on their accuracy from the monolingual models and evaluated them in subtasks A and B. The final model construction employed a stacking ensemble that combined fine-tuning with MPU. Our system achieved 8th (scored 8th in terms of accuracy, officially ranked 13th) place in the official test set in multilingual settings of subtask A. We release our system code at:https://github.com/warmth27/SemEval2024_QUST","sentences":["This paper presents the participation of team QUST in Task 8 SemEval 2024.","We first performed data augmentation and cleaning on the dataset to enhance model training efficiency and accuracy.","In the monolingual task, we evaluated traditional deep-learning methods, multiscale positive-unlabeled framework (MPU), fine-tuning, adapters and ensemble methods.","Then, we selected the top-performing models based on their accuracy from the monolingual models and evaluated them in subtasks A and B.","The final model construction employed a stacking ensemble that combined fine-tuning with MPU.","Our system achieved 8th (scored 8th in terms of accuracy, officially ranked 13th) place in the official test set in multilingual settings of subtask A.","We release our system code at:https://github.com/warmth27/SemEval2024_QUST"],"url":"http://arxiv.org/abs/2402.11934v1","category":"cs.CL"}
{"created":"2024-02-19 08:19:26","title":"SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised Learning","abstract":"To detect anomalies in real-world graphs, such as social, email, and financial networks, various approaches have been developed. While they typically assume static input graphs, most real-world graphs grow over time, naturally represented as edge streams. In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic anomaly labels. In this paper, we propose SLADE (Self-supervised Learning for Anomaly Detection in Edge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels. SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time. To this end, it trains a deep neural network to perform two self-supervised tasks: (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term ones. Failure in these tasks for a node signals its deviation from the norm. Notably, the neural network and tasks are carefully designed so that all required operations can be performed in constant time (w.r.t. the graph size) in response to each new edge in the input stream. In dynamic anomaly detection across four real-world datasets, SLADE outperforms nine competing methods, even those leveraging label supervision.","sentences":["To detect anomalies in real-world graphs, such as social, email, and financial networks, various approaches have been developed.","While they typically assume static input graphs, most real-world graphs grow over time, naturally represented as edge streams.","In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic anomaly labels.","In this paper, we propose SLADE (Self-supervised Learning for Anomaly Detection in Edge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels.","SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time.","To this end, it trains a deep neural network to perform two self-supervised tasks: (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term ones.","Failure in these tasks for a node signals its deviation from the norm.","Notably, the neural network and tasks are carefully designed so that all required operations can be performed in constant time (w.r.t.","the graph size) in response to each new edge in the input stream.","In dynamic anomaly detection across four real-world datasets, SLADE outperforms nine competing methods, even those leveraging label supervision."],"url":"http://arxiv.org/abs/2402.11933v1","category":"cs.LG"}
{"created":"2024-02-19 08:17:13","title":"Separating common from salient patterns with Contrastive Representation Learning","abstract":"Contrastive Analysis is a sub-field of Representation Learning that aims at separating common factors of variation between two datasets, a background (i.e., healthy subjects) and a target (i.e., diseased subjects), from the salient factors of variation, only present in the target dataset. Despite their relevance, current models based on Variational Auto-Encoders have shown poor performance in learning semantically-expressive representations. On the other hand, Contrastive Representation Learning has shown tremendous performance leaps in various applications (classification, clustering, etc.). In this work, we propose to leverage the ability of Contrastive Learning to learn semantically expressive representations well adapted for Contrastive Analysis. We reformulate it under the lens of the InfoMax Principle and identify two Mutual Information terms to maximize and one to minimize. We decompose the first two terms into an Alignment and a Uniformity term, as commonly done in Contrastive Learning. Then, we motivate a novel Mutual Information minimization strategy to prevent information leakage between common and salient distributions. We validate our method, called SepCLR, on three visual datasets and three medical datasets, specifically conceived to assess the pattern separation capability in Contrastive Analysis. Code available at https://github.com/neurospin-projects/2024_rlouiset_sep_clr.","sentences":["Contrastive Analysis is a sub-field of Representation Learning that aims at separating common factors of variation between two datasets, a background (i.e., healthy subjects) and a target (i.e., diseased subjects), from the salient factors of variation, only present in the target dataset.","Despite their relevance, current models based on Variational Auto-Encoders have shown poor performance in learning semantically-expressive representations.","On the other hand, Contrastive Representation Learning has shown tremendous performance leaps in various applications (classification, clustering, etc.).","In this work, we propose to leverage the ability of Contrastive Learning to learn semantically expressive representations well adapted for Contrastive Analysis.","We reformulate it under the lens of the InfoMax Principle and identify two Mutual Information terms to maximize and one to minimize.","We decompose the first two terms into an Alignment and a Uniformity term, as commonly done in Contrastive Learning.","Then, we motivate a novel Mutual Information minimization strategy to prevent information leakage between common and salient distributions.","We validate our method, called SepCLR, on three visual datasets and three medical datasets, specifically conceived to assess the pattern separation capability in Contrastive Analysis.","Code available at https://github.com/neurospin-projects/2024_rlouiset_sep_clr."],"url":"http://arxiv.org/abs/2402.11928v1","category":"cs.CV"}
{"created":"2024-02-19 08:15:19","title":"Lax-Wendroff Flux Reconstruction on adaptive curvilinear meshes with error based time stepping for hyperbolic conservation laws","abstract":"Lax-Wendroff Flux Reconstruction (LWFR) is a single-stage, high order, quadrature free method for solving hyperbolic conservation laws. This work extends the LWFR scheme to solve conservation laws on curvilinear meshes with adaptive mesh refinement (AMR). The scheme uses a subcell based blending limiter to perform shock capturing and exploits the same subcell structure to obtain admissibility preservation on curvilinear meshes. It is proven that the proposed extension of LWFR scheme to curvilinear grids preserves constant solution (free stream preservation) under the standard metric identities. For curvilinear meshes, linear Fourier stability analysis cannot be used to obtain an optimal CFL number. Thus, an embedded-error based time step computation method is proposed for LWFR method which reduces fine-tuning process required to select a stable CFL number using the wave speed based time step computation. The developments are tested on compressible Euler's equations, validating the blending limiter, admissibility preservation, AMR algorithm, curvilinear meshes and error based time stepping.","sentences":["Lax-Wendroff Flux Reconstruction (LWFR) is a single-stage, high order, quadrature free method for solving hyperbolic conservation laws.","This work extends the LWFR scheme to solve conservation laws on curvilinear meshes with adaptive mesh refinement (AMR).","The scheme uses a subcell based blending limiter to perform shock capturing and exploits the same subcell structure to obtain admissibility preservation on curvilinear meshes.","It is proven that the proposed extension of LWFR scheme to curvilinear grids preserves constant solution (free stream preservation) under the standard metric identities.","For curvilinear meshes, linear Fourier stability analysis cannot be used to obtain an optimal CFL number.","Thus, an embedded-error based time step computation method is proposed for LWFR method which reduces fine-tuning process required to select a stable CFL number using the wave speed based time step computation.","The developments are tested on compressible Euler's equations, validating the blending limiter, admissibility preservation, AMR algorithm, curvilinear meshes and error based time stepping."],"url":"http://arxiv.org/abs/2402.11926v2","category":"math.NA"}
{"created":"2024-02-19 08:11:26","title":"A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning","abstract":"Spatio-temporal graph (STG) learning is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPDiff, for STG transfer learning. Unlike conventional approaches that heavily rely on common feature extraction or intricate transfer learning designs, our solution takes a novel approach by performing generative pre-training on a collection of model parameters optimized with data from source cities. We recast STG transfer learning as pre-training a generative hypernetwork, which generates tailored model parameters guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPDiff employs a diffusion model with a transformer-based denoising network, which is model-agnostic to integrate with powerful STG models. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The implementation of our approach is available: https://github.com/PLUTO-SCY/GPDiff.","sentences":["Spatio-temporal graph (STG) learning is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions.","To bridge this gap, we propose a novel generative pre-training framework, GPDiff, for STG transfer learning.","Unlike conventional approaches that heavily rely on common feature extraction or intricate transfer learning designs, our solution takes a novel approach by performing generative pre-training on a collection of model parameters optimized with data from source cities.","We recast STG transfer learning as pre-training a generative hypernetwork, which generates tailored model parameters guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics.","GPDiff employs a diffusion model with a transformer-based denoising network, which is model-agnostic to integrate with powerful STG models.","By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction.","The implementation of our approach is available: https://github.com/PLUTO-SCY/GPDiff."],"url":"http://arxiv.org/abs/2402.11922v2","category":"cs.LG"}
{"created":"2024-02-19 07:48:29","title":"One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation","abstract":"Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability. This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user. We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from few-shot images. Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on few-shot inputs. We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better few-shot adaptation. Our method demonstrates compelling results and outperforms existing state-of-the-art methods for few-shot avatar adaptation, paving the way for more efficient and personalized avatar creation.","sentences":["Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability.","This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user.","We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from few-shot images.","Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on few-shot inputs.","We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better few-shot adaptation.","Our method demonstrates compelling results and outperforms existing state-of-the-art methods for few-shot avatar adaptation, paving the way for more efficient and personalized avatar creation."],"url":"http://arxiv.org/abs/2402.11909v1","category":"cs.CV"}
{"created":"2024-02-19 07:38:57","title":"SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning","abstract":"Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning. While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions. In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability. In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability. The backdoor theory ensures that SoLA can obtain accurate solutions within polynomial loops. We evaluate the performance of SoLA on various datasets and empirically demonstrate its consistent outperformance against existing symbolic solvers (including Z3 and Kissat) and tool-learning methods in terms of efficiency in large-scale problem-solving.","sentences":["Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning.","While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions.","In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability.","In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution.","Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability.","The backdoor theory ensures that SoLA can obtain accurate solutions within polynomial loops.","We evaluate the performance of SoLA on various datasets and empirically demonstrate its consistent outperformance against existing symbolic solvers (including Z3 and Kissat) and tool-learning methods in terms of efficiency in large-scale problem-solving."],"url":"http://arxiv.org/abs/2402.11903v1","category":"cs.CL"}
{"created":"2024-02-19 07:35:49","title":"Real-World Planning with PDDL+ and Beyond","abstract":"Real-world applications of AI Planning often require a highly expressive modeling language to accurately capture important intricacies of target systems. Hybrid systems are ubiquitous in the real-world, and PDDL+ is the standardized modeling language for capturing such systems as planning domains. PDDL+ enables accurate encoding of mixed discrete-continuous system dynamics, exogenous activity, and many other interesting features exhibited in realistic scenarios. However, the uptake in usage of PDDL+ has been slow and apprehensive, largely due to a general shortage of PDDL+ planning software, and rigid limitations of the few existing planners. To overcome this chasm, we present Nyx, a novel PDDL+ planner built to emphasize lightness, simplicity, and, most importantly, adaptability. The planner is designed to be effortlessly customizable to expand its capabilities well beyond the scope of PDDL+. As a result, Nyx can be tailored to virtually any potential real-world application requiring some form of AI Planning, paving the way for wider adoption of planning methods for solving real-world problems.","sentences":["Real-world applications of AI Planning often require a highly expressive modeling language to accurately capture important intricacies of target systems.","Hybrid systems are ubiquitous in the real-world, and PDDL+ is the standardized modeling language for capturing such systems as planning domains.","PDDL+ enables accurate encoding of mixed discrete-continuous system dynamics, exogenous activity, and many other interesting features exhibited in realistic scenarios.","However, the uptake in usage of PDDL+ has been slow and apprehensive, largely due to a general shortage of PDDL+ planning software, and rigid limitations of the few existing planners.","To overcome this chasm, we present Nyx, a novel PDDL+ planner built to emphasize lightness, simplicity, and, most importantly, adaptability.","The planner is designed to be effortlessly customizable to expand its capabilities well beyond the scope of PDDL+.","As a result, Nyx can be tailored to virtually any potential real-world application requiring some form of AI Planning, paving the way for wider adoption of planning methods for solving real-world problems."],"url":"http://arxiv.org/abs/2402.11901v1","category":"cs.AI"}
{"created":"2024-02-19 07:30:36","title":"Automatic Radio Map Adaptation for Robust Localization with Dynamic Adversarial Learning","abstract":"Wireless fingerprint-based localization has become one of the most promising technologies for ubiquitous location-aware computing and intelligent location-based services. However, due to RF vulnerability to environmental dynamics over time, continuous radio map updates are time-consuming and infeasible, resulting in severe accuracy degradation. To address this issue, we propose a novel approach of robust localization with dynamic adversarial learning, known as DadLoc which realizes automatic radio map adaptation by incorporating multiple robust factors underlying RF fingerprints to learn the evolving feature representation with the complicated environmental dynamics. DadLoc performs a finer-grained distribution adaptation with the developed dynamic adversarial adaptation network and quantifies the contributions of both global and local distribution adaptation in a dynamics-adaptive manner. Furthermore, we adopt the strategy of prediction uncertainty suppression to conduct source-supervised training, target-unsupervised training, and source-target dynamic adversarial adaptation which can trade off the environment adaptability and the location discriminability of the learned deep representation for safe and effective feature transfer across different environments. With extensive experimental results, the satisfactory accuracy over other comparative schemes demonstrates that the proposed DanLoc can facilitate fingerprint-based localization for wide deployments.","sentences":["Wireless fingerprint-based localization has become one of the most promising technologies for ubiquitous location-aware computing and intelligent location-based services.","However, due to RF vulnerability to environmental dynamics over time, continuous radio map updates are time-consuming and infeasible, resulting in severe accuracy degradation.","To address this issue, we propose a novel approach of robust localization with dynamic adversarial learning, known as DadLoc which realizes automatic radio map adaptation by incorporating multiple robust factors underlying RF fingerprints to learn the evolving feature representation with the complicated environmental dynamics.","DadLoc performs a finer-grained distribution adaptation with the developed dynamic adversarial adaptation network and quantifies the contributions of both global and local distribution adaptation in a dynamics-adaptive manner.","Furthermore, we adopt the strategy of prediction uncertainty suppression to conduct source-supervised training, target-unsupervised training, and source-target dynamic adversarial adaptation which can trade off the environment adaptability and the location discriminability of the learned deep representation for safe and effective feature transfer across different environments.","With extensive experimental results, the satisfactory accuracy over other comparative schemes demonstrates that the proposed DanLoc can facilitate fingerprint-based localization for wide deployments."],"url":"http://arxiv.org/abs/2402.11898v1","category":"eess.SP"}
{"created":"2024-02-19 07:22:29","title":"SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning","abstract":"Fine-tuning all parameters of large language models (LLMs) necessitates substantial computational power and extended time. Latest advancements in parameter-efficient fine-tuning (PEFT) techniques, such as Adapter tuning and LoRA, allow for adjustments to only a minor fraction of the parameters of these LLMs. Concurrently, it has been noted that the issue of over-smoothing diminishes the effectiveness of these Transformer-based LLMs, resulting in suboptimal performances in downstream tasks. In this paper, we present SIBO, which is a SImple BOoster to enhance PEFT, by injecting an initial residual. SIBO is straight-forward and readily extensible to a range of state-of-the-art PEFT techniques to alleviate over-smoothing and enhance performance. Extensive experiments on 22 benchmark datasets demonstrate that SIBO significantly enhances the performance of various strong baselines, achieving up to 15.7% and 23.5% improvement over existing PEFT methods on the arithmetic and commonsense reasoning tasks, respectively.","sentences":["Fine-tuning all parameters of large language models (LLMs) necessitates substantial computational power and extended time.","Latest advancements in parameter-efficient fine-tuning (PEFT) techniques, such as Adapter tuning and LoRA, allow for adjustments to only a minor fraction of the parameters of these LLMs.","Concurrently, it has been noted that the issue of over-smoothing diminishes the effectiveness of these Transformer-based LLMs, resulting in suboptimal performances in downstream tasks.","In this paper, we present SIBO, which is a SImple BOoster to enhance PEFT, by injecting an initial residual.","SIBO is straight-forward and readily extensible to a range of state-of-the-art PEFT techniques to alleviate over-smoothing and enhance performance.","Extensive experiments on 22 benchmark datasets demonstrate that SIBO significantly enhances the performance of various strong baselines, achieving up to 15.7% and 23.5% improvement over existing PEFT methods on the arithmetic and commonsense reasoning tasks, respectively."],"url":"http://arxiv.org/abs/2402.11896v1","category":"cs.CL"}
{"created":"2024-02-19 07:10:30","title":"Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint","abstract":"Large language models internalize enormous parametric knowledge during pre-training. Concurrently, realistic applications necessitate external contextual knowledge to aid models on the underlying tasks. This raises a crucial dilemma known as knowledge conflicts, where the contextual knowledge clashes with the However, existing decoding works are specialized in resolving knowledge conflicts and could inadvertently deteriorate performance in absence of conflicts. In this paper, we propose an adaptive decoding method, termed as contextual information-entropy constraint decoding (COIECD), to discern whether the knowledge conflicts occur and resolve them. It can improve the model's faithfulness to conflicting context, and simultaneously maintain high performance among non- Our experiments show that COIECD exhibits strong performance and robustness over knowledge conflicts in realistic datasets. Code is available.","sentences":["Large language models internalize enormous parametric knowledge during pre-training.","Concurrently, realistic applications necessitate external contextual knowledge to aid models on the underlying tasks.","This raises a crucial dilemma known as knowledge conflicts, where the contextual knowledge clashes with the However, existing decoding works are specialized in resolving knowledge conflicts and could inadvertently deteriorate performance in absence of conflicts.","In this paper, we propose an adaptive decoding method, termed as contextual information-entropy constraint decoding (COIECD), to discern whether the knowledge conflicts occur and resolve them.","It can improve the model's faithfulness to conflicting context, and simultaneously maintain high performance among non- Our experiments show that COIECD exhibits strong performance and robustness over knowledge conflicts in realistic datasets.","Code is available."],"url":"http://arxiv.org/abs/2402.11893v1","category":"cs.AI"}
{"created":"2024-02-19 07:01:10","title":"Revisiting Knowledge Distillation for Autoregressive Language Models","abstract":"Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student. In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation. Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the KD. The core of ATKD is to reduce rote learning and make teaching more diverse and flexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline KD methods can achieve consistent and significant performance gains (up to +3.04% average score) across all model types and sizes. More encouragingly, ATKD can improve the student model generalization effectively.","sentences":["Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model.","However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student.","In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation.","Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the KD.","The core of ATKD is to reduce rote learning and make teaching more diverse and flexible.","Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline KD methods can achieve consistent and significant performance gains (up to +3.04% average score) across all model types and sizes.","More encouragingly, ATKD can improve the student model generalization effectively."],"url":"http://arxiv.org/abs/2402.11890v1","category":"cs.CL"}
{"created":"2024-02-19 06:55:50","title":"Generative Semi-supervised Graph Anomaly Detection","abstract":"This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the unsupervised setting in most GAD studies with a fully unlabeled graph. As expected, we find that having access to these normal nodes helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate outlier nodes that assimilate anomaly nodes in both local structure and node representations for providing effective negative node samples in training a discriminative one-class classifier. There have been many generative anomaly detection approaches, but they are designed for non-graph data, and as a result, they fail to take account of the graph structure information. Our approach tackles this problem by generating graph structure-aware outlier nodes that have asymmetric affinity separability from normal nodes while being enforced to achieve egocentric closeness to normal nodes in the node representation space. Comprehensive experiments on four real-world datasets are performed to establish a benchmark for semi-supervised GAD and show that GGAD substantially outperforms state-of-the-art unsupervised and semi-supervised GAD methods with varying numbers of training normal nodes. Code will be made available at https://github.com/mala-lab/GGAD.","sentences":["This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the unsupervised setting in most GAD studies with a fully unlabeled graph.","As expected, we find that having access to these normal nodes helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting.","However, their utilization of these normal nodes is limited.","In this paper, we propose a novel Generative GAD approach (GGAD) for the semi-supervised scenario to better exploit the normal nodes.","The key idea is to generate outlier nodes that assimilate anomaly nodes in both local structure and node representations for providing effective negative node samples in training a discriminative one-class classifier.","There have been many generative anomaly detection approaches, but they are designed for non-graph data, and as a result, they fail to take account of the graph structure information.","Our approach tackles this problem by generating graph structure-aware outlier nodes that have asymmetric affinity separability from normal nodes while being enforced to achieve egocentric closeness to normal nodes in the node representation space.","Comprehensive experiments on four real-world datasets are performed to establish a benchmark for semi-supervised GAD and show that GGAD substantially outperforms state-of-the-art unsupervised and semi-supervised GAD methods with varying numbers of training normal nodes.","Code will be made available at https://github.com/mala-lab/GGAD."],"url":"http://arxiv.org/abs/2402.11887v1","category":"cs.LG"}
{"created":"2024-02-19 06:46:16","title":"InMD-X: Large Language Models for Internal Medicine Doctors","abstract":"In this paper, we introduce InMD-X, a collection of multiple large language models specifically designed to cater to the unique characteristics and demands of Internal Medicine Doctors (IMD). InMD-X represents a groundbreaking development in natural language processing, offering a suite of language models fine-tuned for various aspects of the internal medicine field. These models encompass a wide range of medical sub-specialties, enabling IMDs to perform more efficient and accurate research, diagnosis, and documentation. InMD-X's versatility and adaptability make it a valuable tool for improving the healthcare industry, enhancing communication between healthcare professionals, and advancing medical research. Each model within InMD-X is meticulously tailored to address specific challenges faced by IMDs, ensuring the highest level of precision and comprehensiveness in clinical text analysis and decision support. This paper provides an overview of the design, development, and evaluation of InMD-X, showcasing its potential to revolutionize the way internal medicine practitioners interact with medical data and information. We present results from extensive testing, demonstrating the effectiveness and practical utility of InMD-X in real-world medical scenarios.","sentences":["In this paper, we introduce InMD-X, a collection of multiple large language models specifically designed to cater to the unique characteristics and demands of Internal Medicine Doctors (IMD).","InMD-X represents a groundbreaking development in natural language processing, offering a suite of language models fine-tuned for various aspects of the internal medicine field.","These models encompass a wide range of medical sub-specialties, enabling IMDs to perform more efficient and accurate research, diagnosis, and documentation.","InMD-X's versatility and adaptability make it a valuable tool for improving the healthcare industry, enhancing communication between healthcare professionals, and advancing medical research.","Each model within InMD-X is meticulously tailored to address specific challenges faced by IMDs, ensuring the highest level of precision and comprehensiveness in clinical text analysis and decision support.","This paper provides an overview of the design, development, and evaluation of InMD-X, showcasing its potential to revolutionize the way internal medicine practitioners interact with medical data and information.","We present results from extensive testing, demonstrating the effectiveness and practical utility of InMD-X in real-world medical scenarios."],"url":"http://arxiv.org/abs/2402.11883v2","category":"cs.CV"}
{"created":"2024-02-19 06:32:39","title":"M2K-VDG: Model-Adaptive Multimodal Knowledge Anchor Enhanced Video-grounded Dialogue Generation","abstract":"Video-grounded dialogue generation (VDG) requires the system to generate a fluent and accurate answer based on multimodal knowledge. However, the difficulty in multimodal knowledge utilization brings serious hallucinations to VDG models in practice. Although previous works mitigate the hallucination in a variety of ways, they hardly take notice of the importance of the multimodal knowledge anchor answer tokens. In this paper, we reveal via perplexity that different VDG models experience varying hallucinations and exhibit diverse anchor tokens. Based on this observation, we propose M2K-VDG, a model-adaptive multimodal knowledge anchor enhancement framework for hallucination reduction. Furthermore, we introduce the counterfactual effect for more accurate anchor token detection. The experimental results on three popular benchmarks exhibit the superiority of our approach over state-of-the-art methods, demonstrating its effectiveness in reducing hallucinations.","sentences":["Video-grounded dialogue generation (VDG) requires the system to generate a fluent and accurate answer based on multimodal knowledge.","However, the difficulty in multimodal knowledge utilization brings serious hallucinations to VDG models in practice.","Although previous works mitigate the hallucination in a variety of ways, they hardly take notice of the importance of the multimodal knowledge anchor answer tokens.","In this paper, we reveal via perplexity that different VDG models experience varying hallucinations and exhibit diverse anchor tokens.","Based on this observation, we propose M2K-VDG, a model-adaptive multimodal knowledge anchor enhancement framework for hallucination reduction.","Furthermore, we introduce the counterfactual effect for more accurate anchor token detection.","The experimental results on three popular benchmarks exhibit the superiority of our approach over state-of-the-art methods, demonstrating its effectiveness in reducing hallucinations."],"url":"http://arxiv.org/abs/2402.11875v1","category":"cs.CL"}
{"created":"2024-02-19 06:22:09","title":"LoRA Training in the NTK Regime has No Spurious Local Minima","abstract":"Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\\lesssim \\sqrt{N}$; (ii) using LoRA with rank $r\\gtrsim \\sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well.","sentences":["Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited.","In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\\lesssim \\sqrt{N}$; (ii) using LoRA with rank $r\\gtrsim \\sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well."],"url":"http://arxiv.org/abs/2402.11867v1","category":"cs.LG"}
{"created":"2024-02-19 05:15:13","title":"Modularized Networks for Few-shot Hateful Meme Detection","abstract":"In this paper, we address the challenge of detecting hateful memes in the low-resource setting where only a few labeled examples are available. Our approach leverages the compositionality of Low-rank adaptation (LoRA), a widely used parameter-efficient tuning technique. We commence by fine-tuning large language models (LLMs) with LoRA on selected tasks pertinent to hateful meme detection, thereby generating a suite of LoRA modules. These modules are capable of essential reasoning skills for hateful meme detection. We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance. The model's learnable parameters are directly proportional to the number of LoRA modules. This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection. Our evaluation spans three datasets designed for hateful meme detection in a few-shot learning context. The proposed method demonstrates superior performance to traditional in-context learning, which is also more computationally intensive during inference.We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance. The model's learnable parameters are directly proportional to the number of LoRA modules. This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection. Our evaluation spans three datasets designed for hateful meme detection in a few-shot learning context. The proposed method demonstrates superior performance to traditional in-context learning, which is also more computationally intensive during inference.","sentences":["In this paper, we address the challenge of detecting hateful memes in the low-resource setting where only a few labeled examples are available.","Our approach leverages the compositionality of Low-rank adaptation (LoRA), a widely used parameter-efficient tuning technique.","We commence by fine-tuning large language models (LLMs) with LoRA on selected tasks pertinent to hateful meme detection, thereby generating a suite of LoRA modules.","These modules are capable of essential reasoning skills for hateful meme detection.","We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance.","The model's learnable parameters are directly proportional to the number of LoRA modules.","This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection.","Our evaluation spans three datasets designed for hateful meme detection in a few-shot learning context.","The proposed method demonstrates superior performance to traditional in-context learning, which is also more computationally intensive during inference.","We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance.","The model's learnable parameters are directly proportional to the number of LoRA modules.","This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection.","Our evaluation spans three datasets designed for hateful meme detection in a few-shot learning context.","The proposed method demonstrates superior performance to traditional in-context learning, which is also more computationally intensive during inference."],"url":"http://arxiv.org/abs/2402.11845v1","category":"cs.CL"}
{"created":"2024-02-19 05:08:44","title":"ASGNet: Adaptive Semantic Gate Networks for Log-Based Anomaly Diagnosis","abstract":"Logs are widely used in the development and maintenance of software systems. Logs can help engineers understand the runtime behavior of systems and diagnose system failures. For anomaly diagnosis, existing methods generally use log event data extracted from historical logs to build diagnostic models. However, we find that existing methods do not make full use of two types of features, (1) statistical features: some inherent statistical features in log data, such as word frequency and abnormal label distribution, are not well exploited. Compared with log raw data, statistical features are deterministic and naturally compatible with corresponding tasks. (2) semantic features: Logs contain the execution logic behind software systems, thus log statements share deep semantic relationships. How to effectively combine statistical features and semantic features in log data to improve the performance of log anomaly diagnosis is the key point of this paper. In this paper, we propose an adaptive semantic gate networks (ASGNet) that combines statistical features and semantic features to selectively use statistical features to consolidate log text semantic representation. Specifically, ASGNet encodes statistical features via a variational encoding module and fuses useful information through a well-designed adaptive semantic threshold mechanism. The threshold mechanism introduces the information flow into the classifier based on the confidence of the semantic features in the decision, which is conducive to training a robust classifier and can solve the overfitting problem caused by the use of statistical features. The experimental results on the real data set show that our method proposed is superior to all baseline methods in terms of various performance indicators.","sentences":["Logs are widely used in the development and maintenance of software systems.","Logs can help engineers understand the runtime behavior of systems and diagnose system failures.","For anomaly diagnosis, existing methods generally use log event data extracted from historical logs to build diagnostic models.","However, we find that existing methods do not make full use of two types of features, (1) statistical features: some inherent statistical features in log data, such as word frequency and abnormal label distribution, are not well exploited.","Compared with log raw data, statistical features are deterministic and naturally compatible with corresponding tasks.","(2) semantic features:","Logs contain the execution logic behind software systems, thus log statements share deep semantic relationships.","How to effectively combine statistical features and semantic features in log data to improve the performance of log anomaly diagnosis is the key point of this paper.","In this paper, we propose an adaptive semantic gate networks (ASGNet) that combines statistical features and semantic features to selectively use statistical features to consolidate log text semantic representation.","Specifically, ASGNet encodes statistical features via a variational encoding module and fuses useful information through a well-designed adaptive semantic threshold mechanism.","The threshold mechanism introduces the information flow into the classifier based on the confidence of the semantic features in the decision, which is conducive to training a robust classifier and can solve the overfitting problem caused by the use of statistical features.","The experimental results on the real data set show that our method proposed is superior to all baseline methods in terms of various performance indicators."],"url":"http://arxiv.org/abs/2402.11841v1","category":"cs.SE"}
{"created":"2024-02-19 04:58:39","title":"Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret Minimization","abstract":"We propose ABCs (Adaptive Branching through Child stationarity), a best-of-both-worlds algorithm combining Boltzmann Q-learning (BQL), a classic reinforcement learning algorithm for single-agent domains, and counterfactual regret minimization (CFR), a central algorithm for learning in multi-agent domains. ABCs adaptively chooses what fraction of the environment to explore each iteration by measuring the stationarity of the environment's reward and transition dynamics. In Markov decision processes, ABCs converges to the optimal policy with at most an O(A) factor slowdown compared to BQL, where A is the number of actions in the environment. In two-player zero-sum games, ABCs is guaranteed to converge to a Nash equilibrium (assuming access to a perfect oracle for detecting stationarity), while BQL has no such guarantees. Empirically, ABCs demonstrates strong performance when benchmarked across environments drawn from the OpenSpiel game library and OpenAI Gym and exceeds all prior methods in environments which are neither fully stationary nor fully nonstationary.","sentences":["We propose ABCs (Adaptive Branching through Child stationarity), a best-of-both-worlds algorithm combining Boltzmann Q-learning (BQL), a classic reinforcement learning algorithm for single-agent domains, and counterfactual regret minimization (CFR), a central algorithm for learning in multi-agent domains.","ABCs adaptively chooses what fraction of the environment to explore each iteration by measuring the stationarity of the environment's reward and transition dynamics.","In Markov decision processes, ABCs converges to the optimal policy with at most an O(A) factor slowdown compared to BQL, where A is the number of actions in the environment.","In two-player zero-sum games, ABCs is guaranteed to converge to a Nash equilibrium (assuming access to a perfect oracle for detecting stationarity), while BQL has no such guarantees.","Empirically, ABCs demonstrates strong performance when benchmarked across environments drawn from the OpenSpiel game library and OpenAI Gym and exceeds all prior methods in environments which are neither fully stationary nor fully nonstationary."],"url":"http://arxiv.org/abs/2402.11835v1","category":"cs.LG"}
{"created":"2024-02-19 04:13:33","title":"Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before","abstract":"Self-Supervised contrastive learning has emerged as a powerful method for obtaining high-quality representations from unlabeled data. However, feature suppression has recently been identified in standard contrastive learning ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive model captures only parts of the shared information across contrasting views, while ignore the other potentially useful information. With feature suppression, contrastive models often fail to learn sufficient representations capable for various downstream tasks. To mitigate the feature suppression problem and ensure the contrastive model to learn comprehensive representations, we develop a novel Multistage Contrastive Learning (MCL) framework. Unlike standard contrastive learning that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-learned features. Extensive experiments conducted on various publicly available benchmarks validate the effectiveness of our proposed framework. In addition, we demonstrate that the proposed MCL can be adapted to a variety of popular contrastive learning backbones and boost their performance by learning features that could not be gained from standard contrastive learning procedures.","sentences":["Self-Supervised contrastive learning has emerged as a powerful method for obtaining high-quality representations from unlabeled data.","However, feature suppression has recently been identified in standard contrastive learning ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive model captures only parts of the shared information across contrasting views, while ignore the other potentially useful information.","With feature suppression, contrastive models often fail to learn sufficient representations capable for various downstream tasks.","To mitigate the feature suppression problem and ensure the contrastive model to learn comprehensive representations, we develop a novel Multistage Contrastive Learning (MCL) framework.","Unlike standard contrastive learning that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-learned features.","Extensive experiments conducted on various publicly available benchmarks validate the effectiveness of our proposed framework.","In addition, we demonstrate that the proposed MCL can be adapted to a variety of popular contrastive learning backbones and boost their performance by learning features that could not be gained from standard contrastive learning procedures."],"url":"http://arxiv.org/abs/2402.11816v1","category":"cs.CV"}
{"created":"2024-02-19 04:02:40","title":"A novel framework for adaptive stress testing of autonomous vehicles in highways","abstract":"Guaranteeing the safe operations of autonomous vehicles (AVs) is crucial for their widespread adoption and public acceptance. It is thus of a great significance to not only assess the AV against the standard safety tests, but also discover potential corner cases of the AV under test that could lead to unsafe behaviour or scenario. In this paper, we propose a novel framework to systematically explore corner cases that can result in safety concerns in a highway traffic scenario. The framework is based on an adaptive stress testing (AST) approach, an emerging validation method that leverages a Markov decision process to formulate the scenarios and deep reinforcement learning (DRL) to discover the desirable patterns representing corner cases. To this end, we develop a new reward function for DRL to guide the AST in identifying crash scenarios based on the collision probability estimate between the AV under test (i.e., the ego vehicle) and the trajectory of other vehicles on the highway. The proposed framework is further integrated with a new driving model enabling us to create more realistic traffic scenarios capturing both the longitudinal and lateral movements of vehicles on the highway. In our experiment, we calibrate our model using real-world crash statistics involving automated vehicles in California, and then we analyze the characteristics of the AV and the framework. Quantitative and qualitative analyses of our experimental results demonstrate that our framework outperforms other existing AST schemes. The study can help discover crash scenarios of AV that are unknown or absent in human driving, thereby enhancing the safety and trustworthiness of AV technology.","sentences":["Guaranteeing the safe operations of autonomous vehicles (AVs) is crucial for their widespread adoption and public acceptance.","It is thus of a great significance to not only assess the AV against the standard safety tests, but also discover potential corner cases of the AV under test that could lead to unsafe behaviour or scenario.","In this paper, we propose a novel framework to systematically explore corner cases that can result in safety concerns in a highway traffic scenario.","The framework is based on an adaptive stress testing (AST) approach, an emerging validation method that leverages a Markov decision process to formulate the scenarios and deep reinforcement learning (DRL) to discover the desirable patterns representing corner cases.","To this end, we develop a new reward function for DRL to guide the AST in identifying crash scenarios based on the collision probability estimate between the AV under test (i.e., the ego vehicle) and the trajectory of other vehicles on the highway.","The proposed framework is further integrated with a new driving model enabling us to create more realistic traffic scenarios capturing both the longitudinal and lateral movements of vehicles on the highway.","In our experiment, we calibrate our model using real-world crash statistics involving automated vehicles in California, and then we analyze the characteristics of the AV and the framework.","Quantitative and qualitative analyses of our experimental results demonstrate that our framework outperforms other existing AST schemes.","The study can help discover crash scenarios of AV that are unknown or absent in human driving, thereby enhancing the safety and trustworthiness of AV technology."],"url":"http://arxiv.org/abs/2402.11813v1","category":"cs.RO"}
{"created":"2024-02-19 03:56:44","title":"FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema","abstract":"In the quest to facilitate the deep intelligence of Large Language Models (LLMs) accessible in final-end user-bot interactions, the art of prompt crafting emerges as a critical yet complex task for the average user. Contrast to previous model-oriented yet instruction-agnostic Automatic Prompt Optimization methodologies, yielding polished results for predefined target models while suffering rapid degradation with out-of-box models, we present Free-form Instruction-oriented Prompt Optimization (FIPO). This approach is supported by our large-scale prompt preference dataset and employs a modular fine-tuning schema. The FIPO schema reimagines the optimization process into manageable modules, anchored by a meta prompt that dynamically adapts content. This allows for the flexible integration of the raw task instruction, the optional instruction response, and the optional ground truth to produce finely optimized task prompts. The FIPO preference dataset is meticulously constructed using the optimal and suboptimal LLMs, undergoing rigorous cross-verification by human experts and analytical models. Applying the insights from the data with Tulu2 models and fine-tuning strategies, we validate the efficacy of FIPO schema across five public benchmarks. Codes, data and scripts are here: https://github.com/LuJunru/FIPO_Project.","sentences":["In the quest to facilitate the deep intelligence of Large Language Models (LLMs) accessible in final-end user-bot interactions, the art of prompt crafting emerges as a critical yet complex task for the average user.","Contrast to previous model-oriented yet instruction-agnostic Automatic Prompt Optimization methodologies, yielding polished results for predefined target models while suffering rapid degradation with out-of-box models, we present Free-form Instruction-oriented Prompt Optimization (FIPO).","This approach is supported by our large-scale prompt preference dataset and employs a modular fine-tuning schema.","The FIPO schema reimagines the optimization process into manageable modules, anchored by a meta prompt that dynamically adapts content.","This allows for the flexible integration of the raw task instruction, the optional instruction response, and the optional ground truth to produce finely optimized task prompts.","The FIPO preference dataset is meticulously constructed using the optimal and suboptimal LLMs, undergoing rigorous cross-verification by human experts and analytical models.","Applying the insights from the data with Tulu2 models and fine-tuning strategies, we validate the efficacy of FIPO schema across five public benchmarks.","Codes, data and scripts are here: https://github.com/LuJunru/FIPO_Project."],"url":"http://arxiv.org/abs/2402.11811v1","category":"cs.CL"}
{"created":"2024-02-19 03:08:02","title":"Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling","abstract":"Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling. While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood. In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \\emph{last iterate} to a ball around the SA operator's fixed point. Notably, our bound is \\emph{tight} in its dependence on both the maximum delay $\\tau_{max}$, and the mixing time $\\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing uniform boundedness of the iterates. As such, our proof may be of independent interest. Next, to mitigate the impact of the maximum delay on the convergence rate, we provide the first finite-time analysis of a delay-adaptive SA scheme under Markovian sampling. In particular, we show that the exponent of convergence of this scheme gets scaled down by $\\tau_{avg}$, as opposed to $\\tau_{max}$ for the vanilla delayed SA rule; here, $\\tau_{avg}$ denotes the average delay across all iterations. Moreover, the adaptive scheme requires no prior knowledge of the delay sequence for step-size tuning. Our theoretical findings shed light on the finite-time effects of delays for a broad class of algorithms, including TD learning, Q-learning, and stochastic gradient descent under Markovian sampling.","sentences":["Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling.","While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood.","In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \\emph{last iterate} to a ball around the SA operator's fixed point.","Notably, our bound is \\emph{tight} in its dependence on both the maximum delay $\\tau_{max}$, and the mixing time $\\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing uniform boundedness of the iterates.","As such, our proof may be of independent interest.","Next, to mitigate the impact of the maximum delay on the convergence rate, we provide the first finite-time analysis of a delay-adaptive SA scheme under Markovian sampling.","In particular, we show that the exponent of convergence of this scheme gets scaled down by $\\tau_{avg}$, as opposed to $\\tau_{max}$ for the vanilla delayed SA rule; here, $\\tau_{avg}$ denotes the average delay across all iterations.","Moreover, the adaptive scheme requires no prior knowledge of the delay sequence for step-size tuning.","Our theoretical findings shed light on the finite-time effects of delays for a broad class of algorithms, including TD learning, Q-learning, and stochastic gradient descent under Markovian sampling."],"url":"http://arxiv.org/abs/2402.11800v1","category":"cs.LG"}
{"created":"2024-02-19 03:06:43","title":"Decentralized Multi-Robot Navigation for Autonomous Surface Vehicles with Distributional Reinforcement Learning","abstract":"Collision avoidance algorithms for Autonomous Surface Vehicles (ASV) that follow the Convention on the International Regulations for Preventing Collisions at Sea (COLREGs) have been proposed in recent years. However, it may be difficult and unsafe to follow COLREGs in congested waters, where multiple ASVs are navigating in the presence of static obstacles and strong currents, due to the complex interactions. To address this problem, we propose a decentralized multi-ASV collision avoidance policy based on Distributional Reinforcement Learning, which considers the interactions among ASVs as well as with static obstacles and current flows. We evaluate the performance of the proposed Distributional RL based policy against a traditional RL-based policy and two classical methods, Artificial Potential Fields (APF) and Reciprocal Velocity Obstacles (RVO), in simulation experiments, which show that the proposed policy achieves superior performance in navigation safety, while requiring minimal travel time and energy. A variant of our framework that automatically adapts its risk sensitivity is also demonstrated to improve ASV safety in highly congested environments.","sentences":["Collision avoidance algorithms for Autonomous Surface Vehicles (ASV) that follow the Convention on the International Regulations for Preventing Collisions at Sea (COLREGs) have been proposed in recent years.","However, it may be difficult and unsafe to follow COLREGs in congested waters, where multiple ASVs are navigating in the presence of static obstacles and strong currents, due to the complex interactions.","To address this problem, we propose a decentralized multi-ASV collision avoidance policy based on Distributional Reinforcement Learning, which considers the interactions among ASVs as well as with static obstacles and current flows.","We evaluate the performance of the proposed Distributional RL based policy against a traditional RL-based policy and two classical methods, Artificial Potential Fields (APF) and Reciprocal Velocity Obstacles (RVO), in simulation experiments, which show that the proposed policy achieves superior performance in navigation safety, while requiring minimal travel time and energy.","A variant of our framework that automatically adapts its risk sensitivity is also demonstrated to improve ASV safety in highly congested environments."],"url":"http://arxiv.org/abs/2402.11799v1","category":"cs.RO"}
{"created":"2024-02-19 01:28:48","title":"ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs","abstract":"Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debiasing performance while also preserving internal knowledge of a pre-trained LLM; and (3) synthetic data exhibits generalizability across categories, effectively mitigating various biases, including intersectional ones. These findings underscore the potential of synthetic data in advancing the fairness of LLMs with minimal retraining cost.","sentences":["Large Language models (LLMs), while powerful, exhibit harmful social biases.","Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities.","This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs.","We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories.","We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets.","Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debiasing performance while also preserving internal knowledge of a pre-trained LLM; and (3) synthetic data exhibits generalizability across categories, effectively mitigating various biases, including intersectional ones.","These findings underscore the potential of synthetic data in advancing the fairness of LLMs with minimal retraining cost."],"url":"http://arxiv.org/abs/2402.11764v1","category":"cs.CL"}
{"created":"2024-02-19 01:26:22","title":"Autonomous Hyperspectral Characterisation Station: Robotically Assisted Characterisation of Polymer Degradation","abstract":"This paper addresses the gap between the capabilities and utilisation of robotics and automation in laboratory settings and builds upon the concept of Self Driving Labs (SDL). %to significantly impact laboratory operations. We introduce an innovative approach to the temporal characterisation of materials. The article discusses the challenges posed by manual methods involving established laboratory equipment and presents an automated hyperspectral characterisation station. This station integrates robot-aided hyperspectral imaging, complex material characterisation modeling, and automated data analysis, offering a non-destructive and comprehensive approach. This work explains how the proposed assembly can automatically measure the half-life of biodegradable polymers with higher throughput and accuracy than manual methods. The investigation explores the effect of pH, number of average molecular weight (Mn), end groups, and blends on the degradation rate of polylactic acid (PLA). The contributions of the paper lie in introducing an adaptable classification station for novel characterisation methods and presenting an innovative methodology for polymer degradation rate measurement. The proposed system has the potential to accelerate the development of high-throughput screening and characterisation methods in material and chemistry laboratories.","sentences":["This paper addresses the gap between the capabilities and utilisation of robotics and automation in laboratory settings and builds upon the concept of Self Driving Labs (SDL).","%to significantly impact laboratory operations.","We introduce an innovative approach to the temporal characterisation of materials.","The article discusses the challenges posed by manual methods involving established laboratory equipment and presents an automated hyperspectral characterisation station.","This station integrates robot-aided hyperspectral imaging, complex material characterisation modeling, and automated data analysis, offering a non-destructive and comprehensive approach.","This work explains how the proposed assembly can automatically measure the half-life of biodegradable polymers with higher throughput and accuracy than manual methods.","The investigation explores the effect of pH, number of average molecular weight (Mn), end groups, and blends on the degradation rate of polylactic acid (PLA).","The contributions of the paper lie in introducing an adaptable classification station for novel characterisation methods and presenting an innovative methodology for polymer degradation rate measurement.","The proposed system has the potential to accelerate the development of high-throughput screening and characterisation methods in material and chemistry laboratories."],"url":"http://arxiv.org/abs/2402.11763v1","category":"eess.SY"}
{"created":"2024-02-19 01:17:52","title":"Reinforcement Learning as a Parsimonious Alternative to Prediction Cascades: A Case Study on Image Segmentation","abstract":"Deep learning architectures have achieved state-of-the-art (SOTA) performance on computer vision tasks such as object detection and image segmentation. This may be attributed to the use of over-parameterized, monolithic deep learning architectures executed on large datasets. Although such architectures lead to increased accuracy, this is usually accompanied by a large increase in computation and memory requirements during inference. While this is a non-issue in traditional machine learning pipelines, the recent confluence of machine learning and fields like the Internet of Things has rendered such large architectures infeasible for execution in low-resource settings. In such settings, previous efforts have proposed decision cascades where inputs are passed through models of increasing complexity until desired performance is achieved. However, we argue that cascaded prediction leads to increased computational cost due to wasteful intermediate computations. To address this, we propose PaSeR (Parsimonious Segmentation with Reinforcement Learning) a non-cascading, cost-aware learning pipeline as an alternative to cascaded architectures. Through experimental evaluation on real-world and standard datasets, we demonstrate that PaSeR achieves better accuracy while minimizing computational cost relative to cascaded models. Further, we introduce a new metric IoU/GigaFlop to evaluate the balance between cost and performance. On the real-world task of battery material phase segmentation, PaSeR yields a minimum performance improvement of 174% on the IoU/GigaFlop metric with respect to baselines. We also demonstrate PaSeR's adaptability to complementary models trained on a noisy MNIST dataset, where it achieved a minimum performance improvement on IoU/GigaFlop of 13.4% over SOTA models. Code and data are available at https://github.com/scailab/paser .","sentences":["Deep learning architectures have achieved state-of-the-art (SOTA) performance on computer vision tasks such as object detection and image segmentation.","This may be attributed to the use of over-parameterized, monolithic deep learning architectures executed on large datasets.","Although such architectures lead to increased accuracy, this is usually accompanied by a large increase in computation and memory requirements during inference.","While this is a non-issue in traditional machine learning pipelines, the recent confluence of machine learning and fields like the Internet of Things has rendered such large architectures infeasible for execution in low-resource settings.","In such settings, previous efforts have proposed decision cascades where inputs are passed through models of increasing complexity until desired performance is achieved.","However, we argue that cascaded prediction leads to increased computational cost due to wasteful intermediate computations.","To address this, we propose PaSeR (Parsimonious Segmentation with Reinforcement Learning) a non-cascading, cost-aware learning pipeline as an alternative to cascaded architectures.","Through experimental evaluation on real-world and standard datasets, we demonstrate that PaSeR achieves better accuracy while minimizing computational cost relative to cascaded models.","Further, we introduce a new metric IoU/GigaFlop to evaluate the balance between cost and performance.","On the real-world task of battery material phase segmentation, PaSeR yields a minimum performance improvement of 174% on the IoU/GigaFlop metric with respect to baselines.","We also demonstrate PaSeR's adaptability to complementary models trained on a noisy MNIST dataset, where it achieved a minimum performance improvement on IoU/GigaFlop of 13.4% over SOTA models.","Code and data are available at https://github.com/scailab/paser ."],"url":"http://arxiv.org/abs/2402.11760v1","category":"cs.LG"}
{"created":"2024-02-19 00:21:07","title":"Parameter Efficient Finetuning for Speech Emotion Recognition and Domain Adaptation","abstract":"Foundation models have shown superior performance for speech emotion recognition (SER). However, given the limited data in emotion corpora, finetuning all parameters of large pre-trained models for SER can be both resource-intensive and susceptible to overfitting. This paper investigates parameter-efficient finetuning (PEFT) for SER. Various PEFT adaptors are systematically studied for both classification of discrete emotion categories and prediction of dimensional emotional attributes. The results demonstrate that the combination of PEFT methods surpasses full finetuning with a significant reduction in the number of trainable parameters. Furthermore, a two-stage adaptation strategy is proposed to adapt models trained on acted emotion data, which is more readily available, to make the model more adept at capturing natural emotional expressions. Both intra- and cross-corpus experiments validate the efficacy of the proposed approach in enhancing the performance on both the source and target domains.","sentences":["Foundation models have shown superior performance for speech emotion recognition (SER).","However, given the limited data in emotion corpora, finetuning all parameters of large pre-trained models for SER can be both resource-intensive and susceptible to overfitting.","This paper investigates parameter-efficient finetuning (PEFT) for SER.","Various PEFT adaptors are systematically studied for both classification of discrete emotion categories and prediction of dimensional emotional attributes.","The results demonstrate that the combination of PEFT methods surpasses full finetuning with a significant reduction in the number of trainable parameters.","Furthermore, a two-stage adaptation strategy is proposed to adapt models trained on acted emotion data, which is more readily available, to make the model more adept at capturing natural emotional expressions.","Both intra- and cross-corpus experiments validate the efficacy of the proposed approach in enhancing the performance on both the source and target domains."],"url":"http://arxiv.org/abs/2402.11747v1","category":"eess.AS"}
{"created":"2024-02-18 23:29:28","title":"LiRaFusion: Deep Adaptive LiDAR-Radar Fusion for 3D Object Detection","abstract":"We propose LiRaFusion to tackle LiDAR-radar fusion for 3D object detection to fill the performance gap of existing LiDAR-radar detectors. To improve the feature extraction capabilities from these two modalities, we design an early fusion module for joint voxel feature encoding, and a middle fusion module to adaptively fuse feature maps via a gated network. We perform extensive evaluation on nuScenes to demonstrate that LiRaFusion leverages the complementary information of LiDAR and radar effectively and achieves notable improvement over existing methods.","sentences":["We propose LiRaFusion to tackle LiDAR-radar fusion for 3D object detection to fill the performance gap of existing LiDAR-radar detectors.","To improve the feature extraction capabilities from these two modalities, we design an early fusion module for joint voxel feature encoding, and a middle fusion module to adaptively fuse feature maps via a gated network.","We perform extensive evaluation on nuScenes to demonstrate that LiRaFusion leverages the complementary information of LiDAR and radar effectively and achieves notable improvement over existing methods."],"url":"http://arxiv.org/abs/2402.11735v1","category":"cs.RO"}
{"created":"2024-02-18 21:25:09","title":"MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization","abstract":"RL-based techniques can be used to search for prompts that when fed into a target language model maximize a set of user-specified reward functions. However, in many target applications, the natural reward functions are in tension with one another -- for example, content preservation vs. style matching in style transfer tasks. Current techniques focus on maximizing the average of reward functions, which does not necessarily lead to prompts that achieve balance across rewards -- an issue that has been well-studied in the multi-objective and robust optimization literature. In this paper, we adapt several techniques for multi-objective optimization to RL-based discrete prompt optimization -- two that consider volume of the Pareto reward surface, and another that chooses an update direction that benefits all rewards simultaneously. We conduct an empirical analysis of these methods on two NLP tasks: style transfer and machine translation, each using three competing reward functions. Our experiments demonstrate that multi-objective methods that directly optimize volume perform better and achieve a better balance of all rewards than those that attempt to find monotonic update directions.","sentences":["RL-based techniques can be used to search for prompts that when fed into a target language model maximize a set of user-specified reward functions.","However, in many target applications, the natural reward functions are in tension with one another -- for example, content preservation vs. style matching in style transfer tasks.","Current techniques focus on maximizing the average of reward functions, which does not necessarily lead to prompts that achieve balance across rewards -- an issue that has been well-studied in the multi-objective and robust optimization literature.","In this paper, we adapt several techniques for multi-objective optimization to RL-based discrete prompt optimization -- two that consider volume of the Pareto reward surface, and another that chooses an update direction that benefits all rewards simultaneously.","We conduct an empirical analysis of these methods on two NLP tasks: style transfer and machine translation, each using three competing reward functions.","Our experiments demonstrate that multi-objective methods that directly optimize volume perform better and achieve a better balance of all rewards than those that attempt to find monotonic update directions."],"url":"http://arxiv.org/abs/2402.11711v1","category":"cs.CL"}
{"created":"2024-02-18 21:13:05","title":"GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network","abstract":"Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are applied to them. However, fine-tuning still remains crucial to further enhance their adaptability. Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN. Our experiments on text classification tasks with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-based fine-tuning methods in few-shot settings by updating just 0.2% to 0.5% of parameters. We compare GNNavi with prevalent PEFT approaches, such as prefix tuning, LoRA and Adapter in terms of performance and efficiency. Our analysis reveals that GNNavi enhances information flow and ensures a clear aggregation process.","sentences":["Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are applied to them.","However, fine-tuning still remains crucial to further enhance their adaptability.","Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality.","We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach.","GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation.","GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN.","Our experiments on text classification tasks with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-based fine-tuning methods in few-shot settings by updating just 0.2% to 0.5% of parameters.","We compare GNNavi with prevalent PEFT approaches, such as prefix tuning, LoRA and Adapter in terms of performance and efficiency.","Our analysis reveals that GNNavi enhances information flow and ensures a clear aggregation process."],"url":"http://arxiv.org/abs/2402.11709v1","category":"cs.CL"}
{"created":"2024-02-18 19:12:18","title":"Learning Conditional Invariances through Non-Commutativity","abstract":"Invariance learning algorithms that conditionally filter out domain-specific random variables as distractors, do so based only on the data semantics, and not the target domain under evaluation. We show that a provably optimal and sample-efficient way of learning conditional invariances is by relaxing the invariance criterion to be non-commutatively directed towards the target domain. Under domain asymmetry, i.e., when the target domain contains semantically relevant information absent in the source, the risk of the encoder $\\varphi^*$ that is optimal on average across domains is strictly lower-bounded by the risk of the target-specific optimal encoder $\\Phi^*_\\tau$. We prove that non-commutativity steers the optimization towards $\\Phi^*_\\tau$ instead of $\\varphi^*$, bringing the $\\mathcal{H}$-divergence between domains down to zero, leading to a stricter bound on the target risk. Both our theory and experiments demonstrate that non-commutative invariance (NCI) can leverage source domain samples to meet the sample complexity needs of learning $\\Phi^*_\\tau$, surpassing SOTA invariance learning algorithms for domain adaptation, at times by over $2\\%$, approaching the performance of an oracle. Implementation is available at https://github.com/abhrac/nci.","sentences":["Invariance learning algorithms that conditionally filter out domain-specific random variables as distractors, do so based only on the data semantics, and not the target domain under evaluation.","We show that a provably optimal and sample-efficient way of learning conditional invariances is by relaxing the invariance criterion to be non-commutatively directed towards the target domain.","Under domain asymmetry, i.e., when the target domain contains semantically relevant information absent in the source, the risk of the encoder $\\varphi^*$ that is optimal on average across domains is strictly lower-bounded by the risk of the target-specific optimal encoder $\\Phi^*_\\tau$. We prove that non-commutativity steers the optimization towards $\\Phi^*_\\tau$ instead of $\\varphi^*$, bringing the $\\mathcal{H}$-divergence between domains down to zero, leading to a stricter bound on the target risk.","Both our theory and experiments demonstrate that non-commutative invariance (NCI) can leverage source domain samples to meet the sample complexity needs of learning $\\Phi^*_\\tau$, surpassing SOTA invariance learning algorithms for domain adaptation, at times by over $2\\%$, approaching the performance of an oracle.","Implementation is available at https://github.com/abhrac/nci."],"url":"http://arxiv.org/abs/2402.11682v1","category":"cs.LG"}
{"created":"2024-02-18 19:01:02","title":"Adaptively Learning Memory Incorporating PSO","abstract":"Selection of perefect parameters for low-pass filters can sometimes be an expensive problem with no analytical solution or differentiability of cost function. In this paper, we introduce a new PSO-inspired algorithm, that incorporates the positive experiences of the swarm to learn the geometry of the search space,thus obtaining the ability to consistently reach global optimum and is especially suitable for nonsmooth semiconvex functions optimization. We compare it to a set of other algorithms on test functions of choice to prove it's suitability to a certain range of problems, and then apply it to the problem of finding perfect parameters for exponential smoothing algorithm.","sentences":["Selection of perefect parameters for low-pass filters can sometimes be an expensive problem with no analytical solution or differentiability of cost function.","In this paper, we introduce a new PSO-inspired algorithm, that incorporates the positive experiences of the swarm to learn the geometry of the search space,thus obtaining the ability to consistently reach global optimum and is especially suitable for nonsmooth semiconvex functions optimization.","We compare it to a set of other algorithms on test functions of choice to prove it's suitability to a certain range of problems, and then apply it to the problem of finding perfect parameters for exponential smoothing algorithm."],"url":"http://arxiv.org/abs/2402.11679v1","category":"math.OC"}
{"created":"2024-02-18 18:33:48","title":"A Fast Algorithm to Simulate Nonlinear Resistive Networks","abstract":"In the quest for energy-efficient artificial intelligence systems, resistor networks are attracting interest as an alternative to conventional GPU-based neural networks. These networks leverage the physics of electrical circuits for inference and can be optimized with local training techniques such as equilibrium propagation. Despite their potential advantage in terms of power consumption, the challenge of efficiently simulating these resistor networks has been a significant bottleneck to assess their scalability, with current methods either being limited to linear networks or relying on realistic, yet slow circuit simulators like SPICE. Assuming ideal circuit elements, we introduce a novel approach for the simulation of nonlinear resistive networks, which we frame as a quadratic programming problem with linear inequality constraints, and which we solve using a fast, exact coordinate descent algorithm. Our simulation methodology significantly outperforms existing SPICE-based simulations, enabling the training of networks up to 325 times larger at speeds 150 times faster, resulting in a 50,000-fold improvement in the ratio of network size to epoch duration. Our approach, adaptable to other electrical components, can foster more rapid progress in the simulations of nonlinear electrical networks.","sentences":["In the quest for energy-efficient artificial intelligence systems, resistor networks are attracting interest as an alternative to conventional GPU-based neural networks.","These networks leverage the physics of electrical circuits for inference and can be optimized with local training techniques such as equilibrium propagation.","Despite their potential advantage in terms of power consumption, the challenge of efficiently simulating these resistor networks has been a significant bottleneck to assess their scalability, with current methods either being limited to linear networks or relying on realistic, yet slow circuit simulators like SPICE.","Assuming ideal circuit elements, we introduce a novel approach for the simulation of nonlinear resistive networks, which we frame as a quadratic programming problem with linear inequality constraints, and which we solve using a fast, exact coordinate descent algorithm.","Our simulation methodology significantly outperforms existing SPICE-based simulations, enabling the training of networks up to 325 times larger at speeds 150 times faster, resulting in a 50,000-fold improvement in the ratio of network size to epoch duration.","Our approach, adaptable to other electrical components, can foster more rapid progress in the simulations of nonlinear electrical networks."],"url":"http://arxiv.org/abs/2402.11674v1","category":"cs.ET"}
{"created":"2024-02-18 18:10:44","title":"Hybrid Kerr-electro-optic frequency combs on thin-film lithium niobate","abstract":"Optical frequency combs are indispensable links between the optical and microwave domains, enabling a wide range of applications including precision spectroscopy, ultrastable frequency generation, and timekeeping. Chip-scale integration miniaturizes bulk implementations onto photonic chips, offering highly compact, stable, and power-efficient frequency comb sources. State of the art integrated frequency comb sources are based on resonantly-enhanced Kerr effect and, more recently, on electro-optic effect. While the former can routinely reach octave-spanning bandwidths and the latter feature microwave-rate spacings, achieving both in the same material platform has been challenging. Here, we leverage both strong Kerr nonlinearity and efficient electro-optic phase modulation available in the ultralow-loss thin-film lithium niobate photonic platform, to demonstrate a hybrid Kerr-electro-optic frequency comb with stabilized spacing. In our approach, a dissipative Kerr soliton is first generated, and then electro-optic division is used to realize a frequency comb with 2,589 comb lines spaced by 29.308 GHz and spanning 75.9 THz (588 nm) end-to-end. Further, we demonstrate electronic stabilization and control of the soliton spacing, naturally facilitated by our approach. The broadband, microwave-rate comb in this work overcomes the spacing-span tradeoff that exists in all integrated frequency comb sources, and paves the way towards chip-scale solutions for complex tasks such as laser spectroscopy covering multiple bands, micro- and millimeter-wave generation, and massively parallel optical communications.","sentences":["Optical frequency combs are indispensable links between the optical and microwave domains, enabling a wide range of applications including precision spectroscopy, ultrastable frequency generation, and timekeeping.","Chip-scale integration miniaturizes bulk implementations onto photonic chips, offering highly compact, stable, and power-efficient frequency comb sources.","State of the art integrated frequency comb sources are based on resonantly-enhanced Kerr effect and, more recently, on electro-optic effect.","While the former can routinely reach octave-spanning bandwidths and the latter feature microwave-rate spacings, achieving both in the same material platform has been challenging.","Here, we leverage both strong Kerr nonlinearity and efficient electro-optic phase modulation available in the ultralow-loss thin-film lithium niobate photonic platform, to demonstrate a hybrid Kerr-electro-optic frequency comb with stabilized spacing.","In our approach, a dissipative Kerr soliton is first generated, and then electro-optic division is used to realize a frequency comb with 2,589 comb lines spaced by 29.308 GHz and spanning 75.9 THz (588 nm) end-to-end.","Further, we demonstrate electronic stabilization and control of the soliton spacing, naturally facilitated by our approach.","The broadband, microwave-rate comb in this work overcomes the spacing-span tradeoff that exists in all integrated frequency comb sources, and paves the way towards chip-scale solutions for complex tasks such as laser spectroscopy covering multiple bands, micro- and millimeter-wave generation, and massively parallel optical communications."],"url":"http://arxiv.org/abs/2402.11669v1","category":"physics.optics"}
{"created":"2024-02-18 18:05:36","title":"Fast-forwarding molecular ground state preparation with optimal control on analog quantum simulators","abstract":"We show that optimal control of the electron dynamics is able to prepare molecular ground states, within chemical accuracy, with evolution times approaching the bounds imposed by quantum mechanics. We propose a specific parameterization of the molecular evolution only in terms of interaction already present in the molecular Hamiltonian. Thus, the proposed method solely utilizes quantum simulation routines, retaining their favourable scalings. Due to the intimate relationships between variational quantum algorithms and optimal control we compare, when possible, our results with state-of-the-art methods in literature. We found that the number of parameters needed to reach chemical accuracy and algorithmic scaling are in line with compact adaptive strategies to build variational ansatze. The algorithm, which is also suitable for quantum simulators, is implemented emulating a digital quantum processor (up to 16 qubits) and tested on different molecules and geometries spanning different degrees of electron correlation.","sentences":["We show that optimal control of the electron dynamics is able to prepare molecular ground states, within chemical accuracy, with evolution times approaching the bounds imposed by quantum mechanics.","We propose a specific parameterization of the molecular evolution only in terms of interaction already present in the molecular Hamiltonian.","Thus, the proposed method solely utilizes quantum simulation routines, retaining their favourable scalings.","Due to the intimate relationships between variational quantum algorithms and optimal control we compare, when possible, our results with state-of-the-art methods in literature.","We found that the number of parameters needed to reach chemical accuracy and algorithmic scaling are in line with compact adaptive strategies to build variational ansatze.","The algorithm, which is also suitable for quantum simulators, is implemented emulating a digital quantum processor (up to 16 qubits) and tested on different molecules and geometries spanning different degrees of electron correlation."],"url":"http://arxiv.org/abs/2402.11667v1","category":"quant-ph"}
{"created":"2024-02-18 17:32:53","title":"Dynamic planning in hierarchical active inference","abstract":"By dynamic planning, we refer to the ability of the human brain to infer and impose motor trajectories related to cognitive decisions. A recent paradigm, active inference, brings fundamental insights into the adaptation of biological organisms, constantly striving to minimize prediction errors to restrict themselves to life-compatible states. Over the past years, many studies have shown how human and animal behavior could be explained in terms of an active inferential process -- either as discrete decision-making or continuous motor control -- inspiring innovative solutions in robotics and artificial intelligence. Still, the literature lacks a comprehensive outlook on how to effectively plan actions in changing environments. Setting ourselves the goal of modeling tool use, we delve into the topic of dynamic planning in active inference, keeping in mind two crucial aspects of biological goal-directed behavior: the capacity to understand and exploit affordances for object manipulation, and to learn the hierarchical interactions between the self and the environment, including other agents. We start from a simple unit and gradually describe more advanced structures, comparing recently proposed design choices and providing basic examples for each section. This study distances itself from traditional views centered on neural networks and reinforcement learning, and points toward a yet unexplored direction in active inference: hybrid representations in hierarchical models.","sentences":["By dynamic planning, we refer to the ability of the human brain to infer and impose motor trajectories related to cognitive decisions.","A recent paradigm, active inference, brings fundamental insights into the adaptation of biological organisms, constantly striving to minimize prediction errors to restrict themselves to life-compatible states.","Over the past years, many studies have shown how human and animal behavior could be explained in terms of an active inferential process -- either as discrete decision-making or continuous motor control -- inspiring innovative solutions in robotics and artificial intelligence.","Still, the literature lacks a comprehensive outlook on how to effectively plan actions in changing environments.","Setting ourselves the goal of modeling tool use, we delve into the topic of dynamic planning in active inference, keeping in mind two crucial aspects of biological goal-directed behavior: the capacity to understand and exploit affordances for object manipulation, and to learn the hierarchical interactions between the self and the environment, including other agents.","We start from a simple unit and gradually describe more advanced structures, comparing recently proposed design choices and providing basic examples for each section.","This study distances itself from traditional views centered on neural networks and reinforcement learning, and points toward a yet unexplored direction in active inference: hybrid representations in hierarchical models."],"url":"http://arxiv.org/abs/2402.11658v1","category":"cs.AI"}
{"created":"2024-02-18 17:17:15","title":"Combinatorial Client-Master Multiagent Deep Reinforcement Learning for Task Offloading in Mobile Edge Computing","abstract":"Recently, there has been an explosion of mobile applications that perform computationally intensive tasks such as video streaming, data mining, virtual reality, augmented reality, image processing, video processing, face recognition, and online gaming. However, user devices (UDs), such as tablets and smartphones, have a limited ability to perform the computation needs of the tasks. Mobile edge computing (MEC) has emerged as a promising technology to meet the increasing computing demands of UDs. Task offloading in MEC is a strategy that meets the demands of UDs by distributing tasks between UDs and MEC servers. Deep reinforcement learning (DRL) is gaining attention in task-offloading problems because it can adapt to dynamic changes and minimize online computational complexity. However, the various types of continuous and discrete resource constraints on UDs and MEC servers pose challenges to the design of an efficient DRL-based task-offloading strategy. Existing DRL-based task-offloading algorithms focus on the constraints of the UDs, assuming the availability of enough storage resources on the server. Moreover, existing multiagent DRL (MADRL)--based task-offloading algorithms are homogeneous agents and consider homogeneous constraints as a penalty in their reward function. We proposed a novel combinatorial client-master MADRL (CCM\\_MADRL) algorithm for task offloading in MEC (CCM\\_MADRL\\_MEC) that enables UDs to decide their resource requirements and the server to make a combinatorial decision based on the requirements of the UDs. CCM\\_MADRL\\_MEC is the first MADRL in task offloading to consider server storage capacity in addition to the constraints in the UDs. By taking advantage of the combinatorial action selection, CCM\\_MADRL\\_MEC has shown superior convergence over existing MADDPG and heuristic algorithms.","sentences":["Recently, there has been an explosion of mobile applications that perform computationally intensive tasks such as video streaming, data mining, virtual reality, augmented reality, image processing, video processing, face recognition, and online gaming.","However, user devices (UDs), such as tablets and smartphones, have a limited ability to perform the computation needs of the tasks.","Mobile edge computing (MEC) has emerged as a promising technology to meet the increasing computing demands of UDs.","Task offloading in MEC is a strategy that meets the demands of UDs by distributing tasks between UDs and MEC servers.","Deep reinforcement learning (DRL) is gaining attention in task-offloading problems because it can adapt to dynamic changes and minimize online computational complexity.","However, the various types of continuous and discrete resource constraints on UDs and MEC servers pose challenges to the design of an efficient DRL-based task-offloading strategy.","Existing DRL-based task-offloading algorithms focus on the constraints of the UDs, assuming the availability of enough storage resources on the server.","Moreover, existing multiagent DRL (MADRL)--based task-offloading algorithms are homogeneous agents and consider homogeneous constraints as a penalty in their reward function.","We proposed a novel combinatorial client-master MADRL (CCM\\_MADRL) algorithm for task offloading in MEC (CCM\\_MADRL\\_MEC) that enables UDs to decide their resource requirements and the server to make a combinatorial decision based on the requirements of the UDs.","CCM\\_MADRL\\_MEC is the first MADRL in task offloading to consider server storage capacity in addition to the constraints in the UDs.","By taking advantage of the combinatorial action selection, CCM\\_MADRL\\_MEC has shown superior convergence over existing MADDPG and heuristic algorithms."],"url":"http://arxiv.org/abs/2402.11653v1","category":"cs.AI"}
{"created":"2024-02-18 16:37:32","title":"In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness","abstract":"A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context. As such that learner must adapt to the context without additional training. We explore the role of softmax attention in an ICL setting where each context encodes a regression task. We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks. Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks. We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference. Further, we show that this adaptivity relies crucially on the softmax activation and thus cannot be replicated by the linear activation often studied in prior theoretical analyses.","sentences":["A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context.","As such that learner must adapt to the context without additional training.","We explore the role of softmax attention in an ICL setting where each context encodes a regression task.","We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks.","Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks.","We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference.","Further, we show that this adaptivity relies crucially on the softmax activation and thus cannot be replicated by the linear activation often studied in prior theoretical analyses."],"url":"http://arxiv.org/abs/2402.11639v1","category":"cs.LG"}
{"created":"2024-02-18 16:20:43","title":"Self-seeding and Multi-intent Self-instructing LLMs for Generating Intent-aware Information-Seeking dialogs","abstract":"Identifying user intents in information-seeking dialogs is crucial for a system to meet user's information needs. Intent prediction (IP) is challenging and demands sufficient dialogs with human-labeled intents for training. However, manually annotating intents is resource-intensive. While large language models (LLMs) have been shown to be effective in generating synthetic data, there is no study on using LLMs to generate intent-aware information-seeking dialogs. In this paper, we focus on leveraging LLMs for zero-shot generation of large-scale, open-domain, and intent-aware information-seeking dialogs. We propose SOLID, which has novel self-seeding and multi-intent self-instructing schemes. The former improves the generation quality by using the LLM's own knowledge scope to initiate dialog generation; the latter prompts the LLM to generate utterances sequentially, and mitigates the need for manual prompt design by asking the LLM to autonomously adapt its prompt instruction when generating complex multi-intent utterances. Furthermore, we propose SOLID-RL, which is further trained to generate a dialog in one step on the data generated by SOLID. We propose a length-based quality estimation mechanism to assign varying weights to SOLID-generated dialogs based on their quality during the training process of SOLID-RL. We use SOLID and SOLID-RL to generate more than 300k intent-aware dialogs, surpassing the size of existing datasets. Experiments show that IP methods trained on dialogs generated by SOLID and SOLID-RL achieve better IP quality than ones trained on human-generated dialogs.","sentences":["Identifying user intents in information-seeking dialogs is crucial for a system to meet user's information needs.","Intent prediction (IP) is challenging and demands sufficient dialogs with human-labeled intents for training.","However, manually annotating intents is resource-intensive.","While large language models (LLMs) have been shown to be effective in generating synthetic data, there is no study on using LLMs to generate intent-aware information-seeking dialogs.","In this paper, we focus on leveraging LLMs for zero-shot generation of large-scale, open-domain, and intent-aware information-seeking dialogs.","We propose SOLID, which has novel self-seeding and multi-intent self-instructing schemes.","The former improves the generation quality by using the LLM's own knowledge scope to initiate dialog generation; the latter prompts the LLM to generate utterances sequentially, and mitigates the need for manual prompt design by asking the LLM to autonomously adapt its prompt instruction when generating complex multi-intent utterances.","Furthermore, we propose SOLID-RL, which is further trained to generate a dialog in one step on the data generated by SOLID.","We propose a length-based quality estimation mechanism to assign varying weights to SOLID-generated dialogs based on their quality during the training process of SOLID-RL.","We use SOLID and SOLID-RL to generate more than 300k intent-aware dialogs, surpassing the size of existing datasets.","Experiments show that IP methods trained on dialogs generated by SOLID and SOLID-RL achieve better IP quality than ones trained on human-generated dialogs."],"url":"http://arxiv.org/abs/2402.11633v1","category":"cs.CL"}
{"created":"2024-02-19 18:01:16","title":"Observations of compact stars and fermion-boson stars with a quartic self-interaction","abstract":"We investigated the possibility that compact stars could be described by a fermion-boson star with a quartic self-interaction in the boson sector. Specifically, by varying the polytropic constant $K$ and adiabatic index $\\Gamma$ in the polytropic equation of state, the boson mass $\\mu$, and the self-interaction parameter $\\Lambda$, we construct equilibrium configurations of these mixed-stars with total mass compatible with the mass constraints obtained from observational data of the collaborations NICE, NICER/XMN-Newton, and LIGO. Our work confirms that the addition of a boson sector eases the comparison of neutron star models with gravitational events related to compact objects and that in such a case observations may have preference for a positive self-interaction in the boson sector.","sentences":["We investigated the possibility that compact stars could be described by a fermion-boson star with a quartic self-interaction in the boson sector.","Specifically, by varying the polytropic constant $K$ and adiabatic index $\\Gamma$ in the polytropic equation of state, the boson mass $\\mu$, and the self-interaction parameter $\\Lambda$, we construct equilibrium configurations of these mixed-stars with total mass compatible with the mass constraints obtained from observational data of the collaborations NICE, NICER/XMN-Newton, and LIGO.","Our work confirms that the addition of a boson sector eases the comparison of neutron star models with gravitational events related to compact objects and that in such a case observations may have preference for a positive self-interaction in the boson sector."],"url":"http://arxiv.org/abs/2402.12328v1","category":"gr-qc"}
{"created":"2024-02-19 17:57:32","title":"Investigating $\u03c9(z)$ Parametrizations in Horava-Lifshitz Gravity: Observational Constraints with Covariance Matrix Simulation","abstract":"This study investigates accelerated cosmic expansion using various cosmological models within Horava-Lifshitz gravity, including BBCCFHKO, Seljak, ASSS, PADE-I, PADE-II, and BAZS, utilizing Equation of State Parametrization. To constrain the cosmological parameters of each model, we incorporate 24 Baryon Acoustic Oscillation points, 30 Cosmic Chronometer points, 40 Type Ia Supernovae points, 24 quasar Hubble diagram points, and 162 Gamma Ray Bursts points, along with the latest Hubble constant measurement (R22). We treat $r_{d}$ as a free parameter, aiming to extract $H_{0}$ and $r_{d}$ using late-time datasets and obtain optimal fitting values for each parameter in every model. The benefits of treating $r_{d}$ as a free parameter include reduced bias, improved precision, and enhanced dataset compatibility. The resulting values of $H_{0}$ and $r_{d}$ are relative to the $\\Lambda$CDM model, showcasing alignment with early Planck and SDSS estimations. We additionally minimize errors by simulating random correlations in the covariance matrix. Furthermore, the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) support all models, with the $\\Lambda$CDM model exhibiting the lowest AIC. Evaluation via reduced chi-square statistic confirms reasonable fits for all models. While $\\Lambda$CDM remains favored, extensions warrant further investigation. This study underscores the importance of exploring alternative cosmological models to deepen our understanding of the universe's fundamental properties and evolution. Continued refinement of models is essential for advancing cosmological research.","sentences":["This study investigates accelerated cosmic expansion using various cosmological models within Horava-Lifshitz gravity, including BBCCFHKO, Seljak, ASSS, PADE-I, PADE-II, and BAZS, utilizing Equation of State Parametrization.","To constrain the cosmological parameters of each model, we incorporate 24 Baryon Acoustic Oscillation points, 30 Cosmic Chronometer points, 40 Type Ia Supernovae points, 24 quasar Hubble diagram points, and 162 Gamma Ray Bursts points, along with the latest Hubble constant measurement (R22).","We treat $r_{d}$ as a free parameter, aiming to extract $H_{0}$ and $r_{d}$ using late-time datasets and obtain optimal fitting values for each parameter in every model.","The benefits of treating $r_{d}$ as a free parameter include reduced bias, improved precision, and enhanced dataset compatibility.","The resulting values of $H_{0}$ and $r_{d}$ are relative to the $\\Lambda$CDM model, showcasing alignment with early Planck and SDSS estimations.","We additionally minimize errors by simulating random correlations in the covariance matrix.","Furthermore, the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) support all models, with the $\\Lambda$CDM model exhibiting the lowest AIC.","Evaluation via reduced chi-square statistic confirms reasonable fits for all models.","While $\\Lambda$CDM remains favored, extensions warrant further investigation.","This study underscores the importance of exploring alternative cosmological models to deepen our understanding of the universe's fundamental properties and evolution.","Continued refinement of models is essential for advancing cosmological research."],"url":"http://arxiv.org/abs/2402.12324v1","category":"astro-ph.CO"}
{"created":"2024-02-19 17:36:39","title":"Prescribed $L_p$ quotient curvature problem and related eigenvalue problem","abstract":"In this paper, we investigate the existence of admissible (and strictly convex) smooth solutions to the prescribed $L_p$ quotient type curvature problem with $p>1$. For cases where $p=k-l+1$ and $p> k-l+1$, we obtain an admissible solution without any additional conditions, which is strictly spherically convex under a convexity condition. Under the same convexity condition, we establish the existence of a strictly spherically convex solution for the case $p<k-l+1$, provided that the prescribed function is even, a condition known to be necessary.","sentences":["In this paper, we investigate the existence of admissible (and strictly convex) smooth solutions to the prescribed $L_p$ quotient type curvature problem with $p>1$. For cases where $p=k-l+1$ and $p> k-l+1$, we obtain an admissible solution without any additional conditions, which is strictly spherically convex under a convexity condition.","Under the same convexity condition, we establish the existence of a strictly spherically convex solution for the case $p<k-l+1$, provided that the prescribed function is even, a condition known to be necessary."],"url":"http://arxiv.org/abs/2402.12314v1","category":"math.AP"}
{"created":"2024-02-19 17:32:04","title":"Free probability, path developments and signature kernels as universal scaling limits","abstract":"Random developments of a path into a matrix Lie group $G_N$ have recently been used to construct signature-based kernels on path space. Two examples include developments into GL$(N;\\mathbb{R})$ and $U(N;\\mathbb{C})$, the general linear and unitary groups of dimension $N$. For the former, [MLS23] showed that the signature kernel is obtained via a scaling limit of developments with Gaussian vector fields. The second instance was used in [LLN23] to construct a metric between probability measures on path space. We present a unified treatment to obtaining large $N$ limits by leveraging the tools of free probability theory. An important conclusion is that the limiting kernels, while dependent on the choice of Lie group, are nonetheless universal limits with respect to how the development map is randomised. For unitary developments, the limiting kernel is given by the contraction of a signature against the monomials of freely independent semicircular random variables. Using the Schwinger-Dyson equations, we show that this kernel can be obtained by solving a novel quadratic functional equation. We provide a convergent numerical scheme for this equation, together with rates, which does not require computation of signatures themselves.","sentences":["Random developments of a path into a matrix Lie group $G_N$ have recently been used to construct signature-based kernels on path space.","Two examples include developments into GL$(N;\\mathbb{R})$ and $U(N;\\mathbb{C})$, the general linear and unitary groups of dimension $N$. For the former, [MLS23] showed that the signature kernel is obtained via a scaling limit of developments with Gaussian vector fields.","The second instance was used in [LLN23] to construct a metric between probability measures on path space.","We present a unified treatment to obtaining large $N$ limits by leveraging the tools of free probability theory.","An important conclusion is that the limiting kernels, while dependent on the choice of Lie group, are nonetheless universal limits with respect to how the development map is randomised.","For unitary developments, the limiting kernel is given by the contraction of a signature against the monomials of freely independent semicircular random variables.","Using the Schwinger-Dyson equations, we show that this kernel can be obtained by solving a novel quadratic functional equation.","We provide a convergent numerical scheme for this equation, together with rates, which does not require computation of signatures themselves."],"url":"http://arxiv.org/abs/2402.12311v1","category":"math.PR"}
{"created":"2024-02-19 17:30:44","title":"TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs","abstract":"Compared with static knowledge graphs, temporal knowledge graphs (tKG), which can capture the evolution and change of information over time, are more realistic and general. However, due to the complexity that the notion of time introduces to the learning of the rules, an accurate graph reasoning, e.g., predicting new links between entities, is still a difficult problem. In this paper, we propose TILP, a differentiable framework for temporal logical rules learning. By designing a constrained random walk mechanism and the introduction of temporal operators, we ensure the efficiency of our model. We present temporal features modeling in tKG, e.g., recurrence, temporal order, interval between pair of relations, and duration, and incorporate it into our learning process. We compare TILP with state-of-the-art methods on two benchmark datasets. We show that our proposed framework can improve upon the performance of baseline methods while providing interpretable results. In particular, we consider various scenarios in which training samples are limited, data is biased, and the time range between training and inference are different. In all these cases, TILP works much better than the state-of-the-art methods.","sentences":["Compared with static knowledge graphs, temporal knowledge graphs (tKG), which can capture the evolution and change of information over time, are more realistic and general.","However, due to the complexity that the notion of time introduces to the learning of the rules, an accurate graph reasoning, e.g., predicting new links between entities, is still a difficult problem.","In this paper, we propose TILP, a differentiable framework for temporal logical rules learning.","By designing a constrained random walk mechanism and the introduction of temporal operators, we ensure the efficiency of our model.","We present temporal features modeling in tKG, e.g., recurrence, temporal order, interval between pair of relations, and duration, and incorporate it into our learning process.","We compare TILP with state-of-the-art methods on two benchmark datasets.","We show that our proposed framework can improve upon the performance of baseline methods while providing interpretable results.","In particular, we consider various scenarios in which training samples are limited, data is biased, and the time range between training and inference are different.","In all these cases, TILP works much better than the state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.12309v1","category":"cs.CL"}
{"created":"2024-02-19 17:29:41","title":"Sommerfeld Radiation Condition for Helmholtz Equations with long-range Potentials","abstract":"We study the electric Helmholtz equation $\\Delta u + Vu + \\lambda u =f$ and show that, for certain potentials, the solution $u$ given by the limited absorption principle obeys a Sommerfeld radiation condition. We use a non-spherical approach based on the solution $K$ of the eikonal equation $|\\nabla K|^2=1 + \\frac{p}{\\lambda}$ to improve previous results in that area and extend them to long-range potentials which decay like $|x|^{-2-\\alpha}$ at infinity, with $\\alpha > 0$.","sentences":["We study the electric Helmholtz equation $\\Delta u + Vu + \\lambda u =f$ and show that, for certain potentials, the solution $u$ given by the limited absorption principle obeys a Sommerfeld radiation condition.","We use a non-spherical approach based on the solution $K$ of the eikonal equation $|\\nabla K|^2=1 + \\frac{p}{\\lambda}$ to improve previous results in that area and extend them to long-range potentials which decay like $|x|^{-2-\\alpha}$ at infinity, with $\\alpha > 0$."],"url":"http://arxiv.org/abs/2402.12306v1","category":"math.AP"}
{"created":"2024-02-19 17:28:15","title":"Analysis of the Picard-Newton iteration for the Navier-Stokes equations: global stability and quadratic convergence","abstract":"We analyze and test a simple-to-implement two-step iteration for the incompressible Navier-Stokes equations that consists of first applying the Picard iteration and then applying the Newton iteration to the Picard output. We prove that this composition of Picard and Newton converges quadratically, and our analysis (which covers both the unique solution and non-unique solution cases) also suggests that this solver has a larger convergence basin than usual Newton because of the improved stability properties of Picard-Newton over Newton. Numerical tests show that Picard-Newton dramatically outperforms both the Picard and Newton iterations, especially as the Reynolds number increases. We also consider enhancing the Picard step with Anderson acceleration (AA), and find that the AAPicard-Newton iteration has even better convergence properties on several benchmark test problems.","sentences":["We analyze and test a simple-to-implement two-step iteration for the incompressible Navier-Stokes equations that consists of first applying the Picard iteration and then applying the Newton iteration to the Picard output.","We prove that this composition of Picard and Newton converges quadratically, and our analysis (which covers both the unique solution and non-unique solution cases) also suggests that this solver has a larger convergence basin than usual Newton because of the improved stability properties of Picard-Newton over Newton.","Numerical tests show that Picard-Newton dramatically outperforms both the Picard and Newton iterations, especially as the Reynolds number increases.","We also consider enhancing the Picard step with Anderson acceleration (AA), and find that the AAPicard-Newton iteration has even better convergence properties on several benchmark test problems."],"url":"http://arxiv.org/abs/2402.12304v1","category":"math.NA"}
{"created":"2024-02-19 17:24:35","title":"Simple model for the kinetics of stimuli-responsive gels with porous structures","abstract":"Stimuli-responsive gels are the gels that vary the volume depending on environmental conditions. It has been reported that the stimuli-responsive gel with an inhomogeneous structure exhibits faster volume change than the gel without it. It is understood as a difference in the transfer dynamics of the solvent, though, there are few models for discussing the effect of inhomogeneity explicitly. In this paper, we propose a simple model for the kinetics of volume change by introducing inhomogeneity as the probability distribution for a random variable that characterizes the structure. Under this framework, we demonstrate that inhomogeneity actually increases the rate of volume change. Furthermore, through analysis of a simplified example, we show that some features that are often seen in experiments can be reproduced by the model. This model provides a simple equation for the time series of volume change that can be used to characterize the experimental data based on a simple physical picture of the phenomenon.","sentences":["Stimuli-responsive gels are the gels that vary the volume depending on environmental conditions.","It has been reported that the stimuli-responsive gel with an inhomogeneous structure exhibits faster volume change than the gel without it.","It is understood as a difference in the transfer dynamics of the solvent, though, there are few models for discussing the effect of inhomogeneity explicitly.","In this paper, we propose a simple model for the kinetics of volume change by introducing inhomogeneity as the probability distribution for a random variable that characterizes the structure.","Under this framework, we demonstrate that inhomogeneity actually increases the rate of volume change.","Furthermore, through analysis of a simplified example, we show that some features that are often seen in experiments can be reproduced by the model.","This model provides a simple equation for the time series of volume change that can be used to characterize the experimental data based on a simple physical picture of the phenomenon."],"url":"http://arxiv.org/abs/2402.12301v1","category":"cond-mat.soft"}
{"created":"2024-02-19 17:16:00","title":"The multigraded BGG correspondence in Macaulay2","abstract":"We give an overview of a Macaulay2 package for computing with the multigraded BGG correspondence. This software builds on the package BGG due to Abo-Decker-Eisenbud-Schreyer-Smith-Stillman, which concerns the standard graded BGG correspondence. In addition to implementing the multigraded BGG functors, this package includes an implementation of differential modules and their minimal free resolutions, and it contains a method for computing strongly linear strands of multigraded free resolutions.","sentences":["We give an overview of a Macaulay2 package for computing with the multigraded BGG correspondence.","This software builds on the package BGG due to Abo-Decker-Eisenbud-Schreyer-Smith-Stillman, which concerns the standard graded BGG correspondence.","In addition to implementing the multigraded BGG functors, this package includes an implementation of differential modules and their minimal free resolutions, and it contains a method for computing strongly linear strands of multigraded free resolutions."],"url":"http://arxiv.org/abs/2402.12293v1","category":"math.AC"}
{"created":"2024-02-19 16:50:58","title":"Ontology Enhanced Claim Detection","abstract":"We propose an ontology enhanced model for sentence based claim detection. We fused ontology embeddings from a knowledge base with BERT sentence embeddings to perform claim detection for the ClaimBuster and the NewsClaims datasets. Our ontology enhanced approach showed the best results with these small-sized unbalanced datasets, compared to other statistical and neural machine learning models. The experiments demonstrate that adding domain specific features (either trained word embeddings or knowledge graph metadata) can improve traditional ML methods. In addition, adding domain knowledge in the form of ontology embeddings helps avoid the bias encountered in neural network based models, for example the pure BERT model bias towards larger classes in our small corpus.","sentences":["We propose an ontology enhanced model for sentence based claim detection.","We fused ontology embeddings from a knowledge base with BERT sentence embeddings to perform claim detection for the ClaimBuster and the NewsClaims datasets.","Our ontology enhanced approach showed the best results with these small-sized unbalanced datasets, compared to other statistical and neural machine learning models.","The experiments demonstrate that adding domain specific features (either trained word embeddings or knowledge graph metadata) can improve traditional ML methods.","In addition, adding domain knowledge in the form of ontology embeddings helps avoid the bias encountered in neural network based models, for example the pure BERT model bias towards larger classes in our small corpus."],"url":"http://arxiv.org/abs/2402.12282v1","category":"cs.CL"}
{"created":"2024-02-19 16:40:38","title":"Explain then Rank: Scale Calibration of Neural Rankers Using Natural Language Explanations from Large Language Models","abstract":"The process of scale calibration in ranking systems involves adjusting the outputs of rankers to correspond with significant qualities like click-through rates or relevance, crucial for mirroring real-world value and thereby boosting the system's effectiveness and reliability. Although there has been research on calibrated ranking losses within learning-to-rank models, the particular issue of adjusting the scale for neural rankers, which excel in handling textual information, has not been thoroughly examined. Neural ranking models are adept at processing text data, yet the application of existing scale calibration techniques to these models poses significant challenges due to their complexity and the intensive training they require, often resulting in suboptimal outcomes.   This study delves into the potential of large language models (LLMs) to provide uncertainty measurements for a query and document pair that correlate with the scale-calibrated scores. By employing Monte Carlo sampling to gauge relevance probabilities from LLMs and incorporating natural language explanations (NLEs) to articulate this uncertainty, we carry out comprehensive tests on two major document ranking datasets. Our findings reveal that the approach leveraging NLEs outperforms existing calibration methods under various training scenarios, leading to better calibrated neural rankers.","sentences":["The process of scale calibration in ranking systems involves adjusting the outputs of rankers to correspond with significant qualities like click-through rates or relevance, crucial for mirroring real-world value and thereby boosting the system's effectiveness and reliability.","Although there has been research on calibrated ranking losses within learning-to-rank models, the particular issue of adjusting the scale for neural rankers, which excel in handling textual information, has not been thoroughly examined.","Neural ranking models are adept at processing text data, yet the application of existing scale calibration techniques to these models poses significant challenges due to their complexity and the intensive training they require, often resulting in suboptimal outcomes.   ","This study delves into the potential of large language models (LLMs) to provide uncertainty measurements for a query and document pair that correlate with the scale-calibrated scores.","By employing Monte Carlo sampling to gauge relevance probabilities from LLMs and incorporating natural language explanations (NLEs) to articulate this uncertainty, we carry out comprehensive tests on two major document ranking datasets.","Our findings reveal that the approach leveraging NLEs outperforms existing calibration methods under various training scenarios, leading to better calibrated neural rankers."],"url":"http://arxiv.org/abs/2402.12276v1","category":"cs.IR"}
{"created":"2024-02-19 16:38:57","title":"Exact Ansatz of Fermion-Boson Systems for a Quantum Device","abstract":"We present an exact ansatz for the eigenstate problem of mixed fermion-boson systems that can be implemented on quantum devices. Based on a generalization of the electronic contracted Schr\\\"odinger equation (CSE), our approach guides a trial wave function to the ground state of any arbitrary mixed Hamiltonian by directly measuring residuals of the mixed CSE on a quantum device. Unlike density-functional and coupled-cluster theories applied to electron-phonon or electron-photon systems, the accuracy of our approach is not limited by the unknown exchange-correlation functional or the uncontrolled form of the exponential ansatz. To test the performance of the method, we study the Tavis-Cummings model, commonly used in polaritonic quantum chemistry. Our results demonstrate that the CSE is a powerful tool in the development of quantum algorithms for solving general fermion-boson many-body problems.","sentences":["We present an exact ansatz for the eigenstate problem of mixed fermion-boson systems that can be implemented on quantum devices.","Based on a generalization of the electronic contracted Schr\\\"odinger equation (CSE), our approach guides a trial wave function to the ground state of any arbitrary mixed Hamiltonian by directly measuring residuals of the mixed CSE on a quantum device.","Unlike density-functional and coupled-cluster theories applied to electron-phonon or electron-photon systems, the accuracy of our approach is not limited by the unknown exchange-correlation functional or the uncontrolled form of the exponential ansatz.","To test the performance of the method, we study the Tavis-Cummings model, commonly used in polaritonic quantum chemistry.","Our results demonstrate that the CSE is a powerful tool in the development of quantum algorithms for solving general fermion-boson many-body problems."],"url":"http://arxiv.org/abs/2402.12273v1","category":"quant-ph"}
{"created":"2024-02-19 16:28:16","title":"L-QLES: Sparse Laplacian generator for evaluating Quantum Linear Equation Solvers","abstract":"L-QLES is an open source python code for generating 1D, 2D and 3D Laplacian operators and associated Poisson equations and their classical solutions. Its goal is to provide quantum algorithm developers with a flexible test case framework where features of industrial applications can be incorporated without the need for end-user domain knowledge or reliance on inflexible one-off industry supplied matrix sets. A sample set of 1, 2, and 3 dimensional Laplacians are suggested and used to compare the performance of the Prepare-Select and FABLE block encoding techniques. Results show that large matrices are not needed to investigate industrial characteristics. A matrix with a condition number of 17,000 can be encoded using 13 qubits. L-QLES has also been produced to enable algorithm developers to investigate and optimise both the classical and quantum aspects of the inevitable hybrid nature of quantum linear equation solvers. Prepare-Select encoding that takes over an hour of classical preprocessing time to decompose a 4,096x4,096 matrix into Pauli strings can be can investigated using L-QLES matrices. Similarly, row-column query oracles that have success probabilities $\\le 10^{-7}$ for the same matrix can be investigated.","sentences":["L-QLES is an open source python code for generating 1D, 2D and 3D Laplacian operators and associated Poisson equations and their classical solutions.","Its goal is to provide quantum algorithm developers with a flexible test case framework where features of industrial applications can be incorporated without the need for end-user domain knowledge or reliance on inflexible one-off industry supplied matrix sets.","A sample set of 1, 2, and 3 dimensional Laplacians are suggested and used to compare the performance of the Prepare-Select and FABLE block encoding techniques.","Results show that large matrices are not needed to investigate industrial characteristics.","A matrix with a condition number of 17,000 can be encoded using 13 qubits.","L-QLES has also been produced to enable algorithm developers to investigate and optimise both the classical and quantum aspects of the inevitable hybrid nature of quantum linear equation solvers.","Prepare-Select encoding that takes over an hour of classical preprocessing time to decompose a 4,096x4,096 matrix into Pauli strings can be can investigated using L-QLES matrices.","Similarly, row-column query oracles that have success probabilities $\\le 10^{-7}$ for the same matrix can be investigated."],"url":"http://arxiv.org/abs/2402.12266v1","category":"quant-ph"}
{"created":"2024-02-19 16:24:20","title":"Towards a tailored mixed-precision sub-8bit quantization scheme for Gated Recurrent Units using Genetic Algorithms","abstract":"Despite the recent advances in model compression techniques for deep neural networks, deploying such models on ultra-low-power embedded devices still proves challenging. In particular, quantization schemes for Gated Recurrent Units (GRU) are difficult to tune due to their dependence on an internal state, preventing them from fully benefiting from sub-8bit quantization. In this work, we propose a modular integer quantization scheme for GRUs where the bit width of each operator can be selected independently. We then employ Genetic Algorithms (GA) to explore the vast search space of possible bit widths, simultaneously optimising for model size and accuracy. We evaluate our methods on four different sequential tasks and demonstrate that mixed-precision solutions exceed homogeneous-precision ones in terms of Pareto efficiency. In our results, we achieve a model size reduction between 25% and 55% while maintaining an accuracy comparable with the 8-bit homogeneous equivalent.","sentences":["Despite the recent advances in model compression techniques for deep neural networks, deploying such models on ultra-low-power embedded devices still proves challenging.","In particular, quantization schemes for Gated Recurrent Units (GRU) are difficult to tune due to their dependence on an internal state, preventing them from fully benefiting from sub-8bit quantization.","In this work, we propose a modular integer quantization scheme for GRUs where the bit width of each operator can be selected independently.","We then employ Genetic Algorithms (GA) to explore the vast search space of possible bit widths, simultaneously optimising for model size and accuracy.","We evaluate our methods on four different sequential tasks and demonstrate that mixed-precision solutions exceed homogeneous-precision ones in terms of Pareto efficiency.","In our results, we achieve a model size reduction between 25% and 55% while maintaining an accuracy comparable with the 8-bit homogeneous equivalent."],"url":"http://arxiv.org/abs/2402.12263v1","category":"cs.LG"}
{"created":"2024-02-19 16:09:43","title":"Water Vapour Transit Ambiguities for Habitable M-Earths","abstract":"We have shown in a recent study, using 3D climate simulations, that dayside land cover has a substantial impact on the climate of a synchronously rotating temperate rocky planet such as Proxima Centauri b. Building on that result, we generate synthetic transit spectra from our simulations to assess the impact of these land-induced climate uncertainties on water vapour transit signals. We find that distinct climate regimes will likely be very difficult to differentiate in transit spectra, even under the more favourable conditions of smaller planets orbiting ultracool dwarfs. Further, we show that additional climate ambiguities arise when both land cover and atmosphere mass are unknown, as is likely to be the case for transiting planets. While water vapour may be detectable under favourable conditions, it may be nearly impossible to infer a rocky planet's surface conditions or climate state from its transit spectrum due to the interdependent effects of land cover and atmosphere mass on surface temperature, humidity, and terminator cloud cover.","sentences":["We have shown in a recent study, using 3D climate simulations, that dayside land cover has a substantial impact on the climate of a synchronously rotating temperate rocky planet such as Proxima Centauri b. Building on that result, we generate synthetic transit spectra from our simulations to assess the impact of these land-induced climate uncertainties on water vapour transit signals.","We find that distinct climate regimes will likely be very difficult to differentiate in transit spectra, even under the more favourable conditions of smaller planets orbiting ultracool dwarfs.","Further, we show that additional climate ambiguities arise when both land cover and atmosphere mass are unknown, as is likely to be the case for transiting planets.","While water vapour may be detectable under favourable conditions, it may be nearly impossible to infer a rocky planet's surface conditions or climate state from its transit spectrum due to the interdependent effects of land cover and atmosphere mass on surface temperature, humidity, and terminator cloud cover."],"url":"http://arxiv.org/abs/2402.12253v1","category":"astro-ph.EP"}
{"created":"2024-02-19 15:56:43","title":"Convergence of Gradient Descent for Recurrent Neural Networks: A Nonasymptotic Analysis","abstract":"We analyze recurrent neural networks trained with gradient descent in the supervised learning setting for dynamical systems, and prove that gradient descent can achieve optimality \\emph{without} massive overparameterization. Our in-depth nonasymptotic analysis (i) provides sharp bounds on the network size $m$ and iteration complexity $\\tau$ in terms of the sequence length $T$, sample size $n$ and ambient dimension $d$, and (ii) identifies the significant impact of long-term dependencies in the dynamical system on the convergence and network width bounds characterized by a cutoff point that depends on the Lipschitz continuity of the activation function. Remarkably, this analysis reveals that an appropriately-initialized recurrent neural network trained with $n$ samples can achieve optimality with a network size $m$ that scales only logarithmically with $n$. This sharply contrasts with the prior works that require high-order polynomial dependency of $m$ on $n$ to establish strong regularity conditions. Our results are based on an explicit characterization of the class of dynamical systems that can be approximated and learned by recurrent neural networks via norm-constrained transportation mappings, and establishing local smoothness properties of the hidden state with respect to the learnable parameters.","sentences":["We analyze recurrent neural networks trained with gradient descent in the supervised learning setting for dynamical systems, and prove that gradient descent can achieve optimality \\emph{without} massive overparameterization.","Our in-depth nonasymptotic analysis (i) provides sharp bounds on the network size $m$ and iteration complexity $\\tau$ in terms of the sequence length $T$, sample size $n$ and ambient dimension $d$, and (ii) identifies the significant impact of long-term dependencies in the dynamical system on the convergence and network width bounds characterized by a cutoff point that depends on the Lipschitz continuity of the activation function.","Remarkably, this analysis reveals that an appropriately-initialized recurrent neural network trained with $n$ samples can achieve optimality with a network size $m$ that scales only logarithmically with $n$. This sharply contrasts with the prior works that require high-order polynomial dependency of $m$ on $n$ to establish strong regularity conditions.","Our results are based on an explicit characterization of the class of dynamical systems that can be approximated and learned by recurrent neural networks via norm-constrained transportation mappings, and establishing local smoothness properties of the hidden state with respect to the learnable parameters."],"url":"http://arxiv.org/abs/2402.12241v1","category":"cs.LG"}
{"created":"2024-02-19 15:42:54","title":"Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers","abstract":"The feed-forward networks (FFNs) in transformers are recognized as a group of key-value neural memories to restore abstract high-level knowledge. In this work, we conduct an empirical ablation study on updating keys (the 1st layer in the FFNs layer) or values (the 2nd layer in the FFNs layer). We compare those two methods in various knowledge editing and fine-tuning tasks of large language models to draw insights to understand FFNs further. Code is available at $\\href{https://github.com/qiuzh20/Tuning-keys-v.s.-values}{this\\,repo}$.","sentences":["The feed-forward networks (FFNs) in transformers are recognized as a group of key-value neural memories to restore abstract high-level knowledge.","In this work, we conduct an empirical ablation study on updating keys (the 1st layer in the FFNs layer) or values (the 2nd layer in the FFNs layer).","We compare those two methods in various knowledge editing and fine-tuning tasks of large language models to draw insights to understand FFNs further.","Code is available at $\\href{https://github.com/qiuzh20/Tuning-keys-v.s.-values}{this\\,repo}$."],"url":"http://arxiv.org/abs/2402.12233v1","category":"cs.CL"}
{"created":"2024-02-19 15:36:36","title":"Diffusion Tempering Improves Parameter Estimation with Probabilistic Integrators for Ordinary Differential Equations","abstract":"Ordinary differential equations (ODEs) are widely used to describe dynamical systems in science, but identifying parameters that explain experimental measurements is challenging. In particular, although ODEs are differentiable and would allow for gradient-based parameter optimization, the nonlinear dynamics of ODEs often lead to many local minima and extreme sensitivity to initial conditions. We therefore propose diffusion tempering, a novel regularization technique for probabilistic numerical methods which improves convergence of gradient-based parameter optimization in ODEs. By iteratively reducing a noise parameter of the probabilistic integrator, the proposed method converges more reliably to the true parameters. We demonstrate that our method is effective for dynamical systems of different complexity and show that it obtains reliable parameter estimates for a Hodgkin-Huxley model with a practically relevant number of parameters.","sentences":["Ordinary differential equations (ODEs) are widely used to describe dynamical systems in science, but identifying parameters that explain experimental measurements is challenging.","In particular, although ODEs are differentiable and would allow for gradient-based parameter optimization, the nonlinear dynamics of ODEs often lead to many local minima and extreme sensitivity to initial conditions.","We therefore propose diffusion tempering, a novel regularization technique for probabilistic numerical methods which improves convergence of gradient-based parameter optimization in ODEs.","By iteratively reducing a noise parameter of the probabilistic integrator, the proposed method converges more reliably to the true parameters.","We demonstrate that our method is effective for dynamical systems of different complexity and show that it obtains reliable parameter estimates for a Hodgkin-Huxley model with a practically relevant number of parameters."],"url":"http://arxiv.org/abs/2402.12231v1","category":"cs.LG"}
{"created":"2024-02-19 15:36:04","title":"Half Space Property in RCD(0,N) spaces","abstract":"The goal of this note is to prove the Half Space Property for $RCD(0,N)$ spaces, namely that if $(X,d,m)$ is a parabolic $RCD(0,N)$ space and $ C \\subset X \\times \\mathbb{R}$ is locally the boundary of a locally perimeter minimizing set and it is contained in a half space, then $C$ is a locally finite union of horizontal slices.   As a consequence, we obtain oscillation estimates and a Half Space Theorem for minimal hypersurfaces in products $M \\times \\mathbb{R}$, where $M$ is a parabolic smooth manifold (possibly weighted and with boundary) with non-negative Ricci curvature.   On the way of proving the main results, we also obtain some properties of Green's functions on $RCD(K,N)$ spaces that are of independent interest.","sentences":["The goal of this note is to prove the Half Space Property for $RCD(0,N)$ spaces, namely that if $(X,d,m)$ is a parabolic $RCD(0,N)$ space and $ C \\subset X \\times \\mathbb{R}$ is locally the boundary of a locally perimeter minimizing set and it is contained in a half space, then $C$ is a locally finite union of horizontal slices.   ","As a consequence, we obtain oscillation estimates and a Half Space Theorem for minimal hypersurfaces in products $M \\times \\mathbb{R}$, where $M$ is a parabolic smooth manifold (possibly weighted and with boundary) with non-negative Ricci curvature.   ","On the way of proving the main results, we also obtain some properties of Green's functions on $RCD(K,N)$ spaces that are of independent interest."],"url":"http://arxiv.org/abs/2402.12230v1","category":"math.DG"}
{"created":"2024-02-19 15:32:33","title":"Holographic transport in anisotropic plasmas","abstract":"We study energy-momentum and charge transport in strongly interacting holographic quantum field theories in an anisotropic thermal state by contrasting three different holographic methods to compute transport coefficients: standard holographic calculation of retarded Greens functions, a method based on the null-focusing equation near horizon and the novel method based on background variations. Employing these methods we compute anisotropic shear and bulk viscosities and conductivities with anisotropy induced externally, for example by an external magnetic field. We show that all three methods yield consistent results. The novel method allows us to read off the transport coefficients from the horizon data and express them in analytic form from which we derive universal relations among them. Furthermore we extend the method based on the null-focusing equation to Gauss-Bonnet theory to compute higher derivative corrections to the aforementioned transport coefficients.","sentences":["We study energy-momentum and charge transport in strongly interacting holographic quantum field theories in an anisotropic thermal state by contrasting three different holographic methods to compute transport coefficients: standard holographic calculation of retarded Greens functions, a method based on the null-focusing equation near horizon and the novel method based on background variations.","Employing these methods we compute anisotropic shear and bulk viscosities and conductivities with anisotropy induced externally, for example by an external magnetic field.","We show that all three methods yield consistent results.","The novel method allows us to read off the transport coefficients from the horizon data and express them in analytic form from which we derive universal relations among them.","Furthermore we extend the method based on the null-focusing equation to Gauss-Bonnet theory to compute higher derivative corrections to the aforementioned transport coefficients."],"url":"http://arxiv.org/abs/2402.12224v1","category":"hep-th"}
{"created":"2024-02-19 15:32:25","title":"Second Order Meanfield Approximation for calculating Dynamics in Au-Nanoparticle Networks","abstract":"Exploiting physical processes for fast and energy-efficient computation bears great potential in the advancement of modern hardware components. This paper explores non-linear charge tunneling in nanoparticle networks, controlled by external voltages. The dynamics are described by a master equation, which describes the development of a distribution function over the set of charge occupation numbers. The driving force behind this evolution are charge tunneling events among nanoparticles and their associated rates. In this paper, we introduce two meanfield approximations to this master equation. By parametrization of the distribution function using its first- and second-order statistical moments, and a subsequent projection of the dynamics onto the resulting moment manifold, one can deterministically calculate expected charges and currents. Unlike a kinetic Monte Carlo approach, which extracts samples from the distribution function, this meanfield approach avoids any random elements. A comparison of results between the meanfield approximation and an already available kinetic Monte Carlo simulation demonstrates great accuracy. Our analysis also reveals that transitioning from a first-order to a second-order approximation significantly enhances the accuracy. Furthermore, we demonstrate the applicability of our approach to time-dependent simulations, using eulerian time-integration schemes.","sentences":["Exploiting physical processes for fast and energy-efficient computation bears great potential in the advancement of modern hardware components.","This paper explores non-linear charge tunneling in nanoparticle networks, controlled by external voltages.","The dynamics are described by a master equation, which describes the development of a distribution function over the set of charge occupation numbers.","The driving force behind this evolution are charge tunneling events among nanoparticles and their associated rates.","In this paper, we introduce two meanfield approximations to this master equation.","By parametrization of the distribution function using its first- and second-order statistical moments, and a subsequent projection of the dynamics onto the resulting moment manifold, one can deterministically calculate expected charges and currents.","Unlike a kinetic Monte Carlo approach, which extracts samples from the distribution function, this meanfield approach avoids any random elements.","A comparison of results between the meanfield approximation and an already available kinetic Monte Carlo simulation demonstrates great accuracy.","Our analysis also reveals that transitioning from a first-order to a second-order approximation significantly enhances the accuracy.","Furthermore, we demonstrate the applicability of our approach to time-dependent simulations, using eulerian time-integration schemes."],"url":"http://arxiv.org/abs/2402.12223v1","category":"physics.comp-ph"}
{"created":"2024-02-19 15:15:48","title":"Representation formulas and far-field behavior of time-periodic incompressible viscous flow around a translating rigid body","abstract":"This paper is concerned with integral representations and asymptotic expansions of solutions to the time-periodic incompressible Navier-Stokes equations for fluid flow in the exterior of a rigid body that moves with constant velocity. Using the time-periodic Oseen fundamental solution, we derive representation formulas for solutions with suitable regularity. From these formulas, the decomposition of the velocity component of the fundamental solution into steady-state and purely periodic parts and their detailed decay rate in space, we deduce complete information on the asymptotic structure of the velocity and pressure fields.","sentences":["This paper is concerned with integral representations and asymptotic expansions of solutions to the time-periodic incompressible Navier-Stokes equations for fluid flow in the exterior of a rigid body that moves with constant velocity.","Using the time-periodic Oseen fundamental solution, we derive representation formulas for solutions with suitable regularity.","From these formulas, the decomposition of the velocity component of the fundamental solution into steady-state and purely periodic parts and their detailed decay rate in space, we deduce complete information on the asymptotic structure of the velocity and pressure fields."],"url":"http://arxiv.org/abs/2402.12213v1","category":"math.AP"}
{"created":"2024-02-19 15:12:12","title":"Some Riemannian properties of $\\mathbf{SU_n}$ endowed with a bi-invariant metric","abstract":"We study some properties of $SU_n$ endowed with the Frobenius metric $\\phi$, which is, up to a positive constant multiple, the unique bi-invariant Riemannian metric on $SU_n$. In particular we express the distance between $P, Q \\in SU_n$ in terms of eigenvalues of $P^*Q$; we compute the diameter of $(SU_n, \\phi)$ and we determine its diametral pairs; we prove that the set of all minimizing geodesic segments with endpoints $P$, $Q$ can be parametrized by means of a compact connected submanifold of $\\mathfrak{su}_n$, diffeomorphic to a suitable complex Grassmannian depending on $P$ and $Q$.","sentences":["We study some properties of $SU_n$ endowed with the Frobenius metric $\\phi$, which is, up to a positive constant multiple, the unique bi-invariant Riemannian metric on $SU_n$. In particular we express the distance between $P, Q \\in SU_n$ in terms of eigenvalues of $P^*Q$; we compute the diameter of $(SU_n, \\phi)$ and we determine its diametral pairs; we prove that the set of all minimizing geodesic segments with endpoints $P$, $Q$ can be parametrized by means of a compact connected submanifold of $\\mathfrak{su}_n$, diffeomorphic to a suitable complex Grassmannian depending on $P$ and $Q$."],"url":"http://arxiv.org/abs/2402.12209v1","category":"math.DG"}
{"created":"2024-02-19 15:10:21","title":"On Li-Lin's open problem","abstract":"In this paper, we give a first partial negative answer to a question proposed by Li and Lin (Arch Ration Mech Anal 203(3):943-968, 2012). Meanwhile we also give a second partial positive answer to the Li-Lin's open problem. The first partial positive answer was given by G. Cerami, X. Zhong and W. Zou (Calc. Var. Partial Differential Equations, 54(2):1793-1829, 2015).","sentences":["In this paper, we give a first partial negative answer to a question proposed by Li and Lin (Arch Ration Mech Anal 203(3):943-968, 2012).","Meanwhile we also give a second partial positive answer to the Li-Lin's open problem.","The first partial positive answer was given by G. Cerami, X. Zhong and W. Zou (Calc.","Var.","Partial Differential Equations, 54(2):1793-1829, 2015)."],"url":"http://arxiv.org/abs/2402.12206v1","category":"math.AP"}
{"created":"2024-02-19 15:07:56","title":"Self-organized clustering, prediction, and superposition of long-term cognitive decline from short-term individual cognitive test scores in Alzheimer's disease","abstract":"Progressive cognitive decline spanning across decades is characteristic of Alzheimer's disease (AD). Various predictive models have been designed to realize its early onset and study the long-term trajectories of cognitive test scores across populations of interest. Research efforts have been geared towards superimposing patients' cognitive test scores with the long-term trajectory denoting gradual cognitive decline, while considering the heterogeneity of AD. Multiple trajectories representing cognitive assessment for the long-term have been developed based on various parameters, highlighting the importance of classifying several groups based on disease progression patterns. In this study, a novel method capable of self-organized prediction, classification, and the overlay of long-term cognitive trajectories based on short-term individual data was developed, based on statistical and differential equation modeling. We validated the predictive accuracy of the proposed method for the long-term trajectory of cognitive test score results on two cohorts: the Alzheimer's Disease Neuroimaging Initiative (ADNI) study and the Japanese ADNI study. We also presented two practical illustrations of the simultaneous evaluation of risk factor associated with both the onset and the longitudinal progression of AD, and an innovative randomized controlled trial design for AD that standardizes the heterogeneity of patients enrolled in a clinical trial. These resources would improve the power of statistical hypothesis testing and help evaluate the therapeutic effect. The application of predicting the trajectory of longitudinal disease progression goes beyond AD, and is especially relevant for progressive and neurodegenerative disorders.","sentences":["Progressive cognitive decline spanning across decades is characteristic of Alzheimer's disease (AD).","Various predictive models have been designed to realize its early onset and study the long-term trajectories of cognitive test scores across populations of interest.","Research efforts have been geared towards superimposing patients' cognitive test scores with the long-term trajectory denoting gradual cognitive decline, while considering the heterogeneity of AD.","Multiple trajectories representing cognitive assessment for the long-term have been developed based on various parameters, highlighting the importance of classifying several groups based on disease progression patterns.","In this study, a novel method capable of self-organized prediction, classification, and the overlay of long-term cognitive trajectories based on short-term individual data was developed, based on statistical and differential equation modeling.","We validated the predictive accuracy of the proposed method for the long-term trajectory of cognitive test score results on two cohorts: the Alzheimer's Disease Neuroimaging Initiative (ADNI) study and the Japanese ADNI study.","We also presented two practical illustrations of the simultaneous evaluation of risk factor associated with both the onset and the longitudinal progression of AD, and an innovative randomized controlled trial design for AD that standardizes the heterogeneity of patients enrolled in a clinical trial.","These resources would improve the power of statistical hypothesis testing and help evaluate the therapeutic effect.","The application of predicting the trajectory of longitudinal disease progression goes beyond AD, and is especially relevant for progressive and neurodegenerative disorders."],"url":"http://arxiv.org/abs/2402.12205v1","category":"q-bio.QM"}
{"created":"2024-02-19 15:03:13","title":"Molecular Dynamics Simulations of Anisotropic Particles Accelerated by Neural-Net Predicted Interactions","abstract":"Rigid bodies, made of smaller composite beads, are commonly used to simulate anisotropic particles with molecular dynamics or Monte Carlo methods. To accurately represent the particle shape and to obtain smooth and realistic effective pair interactions between two rigid bodies, each body may need to contain hundreds of spherical beads. Given an interacting pair of particles, traditional MD methods calculate the inter-body distances between the beads of the rigid bodies within a certain distance. For a system containing many anisotropic particles, distance calculations are computationally costly and limit the attainable system size and simulation time. However, the effective interaction between two rigid particles only depends on the distance between their center of masses and their relative orientation. Therefore, a function directly mapping the center of mass distance and orientation to the interaction energy between the two rigid bodies, would completely bypass inter-bead distance calculations. It is challenging to derive such a general function analytically for most non-spherical rigid bodies. We have trained neural nets, powerful tools to fit nonlinear functions to complex datasets, to achieve this task. The pair configuration is taken as input and the energy, forces and torques between two rigid particles are predicted directly. We show that MD simulations of cubes and cylinders performed with forces and torques obtained from the gradients of the energy neural-nets quantitatively match traditional simulations that uses composite rigid bodies. Both structural quantities and dynamic measures are in agreement, while achieving up to 23 times speed up over traditional molecular dynamics, depending on hardware and system size. The method can, in principle, be applied to any irregular shape with any pair interaction, provided that sufficient training data can be obtained.","sentences":["Rigid bodies, made of smaller composite beads, are commonly used to simulate anisotropic particles with molecular dynamics or Monte Carlo methods.","To accurately represent the particle shape and to obtain smooth and realistic effective pair interactions between two rigid bodies, each body may need to contain hundreds of spherical beads.","Given an interacting pair of particles, traditional MD methods calculate the inter-body distances between the beads of the rigid bodies within a certain distance.","For a system containing many anisotropic particles, distance calculations are computationally costly and limit the attainable system size and simulation time.","However, the effective interaction between two rigid particles only depends on the distance between their center of masses and their relative orientation.","Therefore, a function directly mapping the center of mass distance and orientation to the interaction energy between the two rigid bodies, would completely bypass inter-bead distance calculations.","It is challenging to derive such a general function analytically for most non-spherical rigid bodies.","We have trained neural nets, powerful tools to fit nonlinear functions to complex datasets, to achieve this task.","The pair configuration is taken as input and the energy, forces and torques between two rigid particles are predicted directly.","We show that MD simulations of cubes and cylinders performed with forces and torques obtained from the gradients of the energy neural-nets quantitatively match traditional simulations that uses composite rigid bodies.","Both structural quantities and dynamic measures are in agreement, while achieving up to 23 times speed up over traditional molecular dynamics, depending on hardware and system size.","The method can, in principle, be applied to any irregular shape with any pair interaction, provided that sufficient training data can be obtained."],"url":"http://arxiv.org/abs/2402.12199v1","category":"cond-mat.soft"}
{"created":"2024-02-19 14:51:55","title":"Structure of activity in multiregion recurrent neural networks","abstract":"Neural circuits are composed of multiple regions, each with rich dynamics and engaging in communication with other regions. The combination of local, within-region dynamics and global, network-level dynamics is thought to provide computational flexibility. However, the nature of such multiregion dynamics and the underlying synaptic connectivity patterns remain poorly understood. Here, we study the dynamics of recurrent neural networks with multiple interconnected regions. Within each region, neurons have a combination of random and structured recurrent connections. Motivated by experimental evidence of communication subspaces between cortical areas, these networks have low-rank connectivity between regions, enabling selective routing of activity. These networks exhibit two interacting forms of dynamics: high-dimensional fluctuations within regions and low-dimensional signal transmission between regions. To characterize this interaction, we develop a dynamical mean-field theory to analyze such networks in the limit where each region contains infinitely many neurons, with cross-region currents as key order parameters. Regions can act as both generators and transmitters of activity, roles that we show are in conflict. Specifically, taming the complexity of activity within a region is necessary for it to route signals to and from other regions. Unlike previous models of routing in neural circuits, which suppressed the activities of neuronal groups to control signal flow, routing in our model is achieved by exciting different high-dimensional activity patterns through a combination of connectivity structure and nonlinear recurrent dynamics. This theory provides insight into the interpretation of both multiregion neural data and trained neural networks.","sentences":["Neural circuits are composed of multiple regions, each with rich dynamics and engaging in communication with other regions.","The combination of local, within-region dynamics and global, network-level dynamics is thought to provide computational flexibility.","However, the nature of such multiregion dynamics and the underlying synaptic connectivity patterns remain poorly understood.","Here, we study the dynamics of recurrent neural networks with multiple interconnected regions.","Within each region, neurons have a combination of random and structured recurrent connections.","Motivated by experimental evidence of communication subspaces between cortical areas, these networks have low-rank connectivity between regions, enabling selective routing of activity.","These networks exhibit two interacting forms of dynamics: high-dimensional fluctuations within regions and low-dimensional signal transmission between regions.","To characterize this interaction, we develop a dynamical mean-field theory to analyze such networks in the limit where each region contains infinitely many neurons, with cross-region currents as key order parameters.","Regions can act as both generators and transmitters of activity, roles that we show are in conflict.","Specifically, taming the complexity of activity within a region is necessary for it to route signals to and from other regions.","Unlike previous models of routing in neural circuits, which suppressed the activities of neuronal groups to control signal flow, routing in our model is achieved by exciting different high-dimensional activity patterns through a combination of connectivity structure and nonlinear recurrent dynamics.","This theory provides insight into the interpretation of both multiregion neural data and trained neural networks."],"url":"http://arxiv.org/abs/2402.12188v2","category":"q-bio.NC"}
{"created":"2024-02-19 14:47:23","title":"Colorizing Monochromatic Radiance Fields","abstract":"Though Neural Radiance Fields (NeRF) can produce colorful 3D representations of the world by using a set of 2D images, such ability becomes non-existent when only monochromatic images are provided. Since color is necessary in representing the world, reproducing color from monochromatic radiance fields becomes crucial. To achieve this goal, instead of manipulating the monochromatic radiance fields directly, we consider it as a representation-prediction task in the Lab color space. By first constructing the luminance and density representation using monochromatic images, our prediction stage can recreate color representation on the basis of an image colorization module. We then reproduce a colorful implicit model through the representation of luminance, density, and color. Extensive experiments have been conducted to validate the effectiveness of our approaches. Our project page: https://liquidammonia.github.io/color-nerf.","sentences":["Though Neural Radiance Fields (NeRF) can produce colorful 3D representations of the world by using a set of 2D images, such ability becomes non-existent when only monochromatic images are provided.","Since color is necessary in representing the world, reproducing color from monochromatic radiance fields becomes crucial.","To achieve this goal, instead of manipulating the monochromatic radiance fields directly, we consider it as a representation-prediction task in the Lab color space.","By first constructing the luminance and density representation using monochromatic images, our prediction stage can recreate color representation on the basis of an image colorization module.","We then reproduce a colorful implicit model through the representation of luminance, density, and color.","Extensive experiments have been conducted to validate the effectiveness of our approaches.","Our project page: https://liquidammonia.github.io/color-nerf."],"url":"http://arxiv.org/abs/2402.12184v1","category":"cs.CV"}
{"created":"2024-02-19 14:29:35","title":"Learning Discretized Bayesian Networks with GOMEA","abstract":"Bayesian networks model relationships between random variables under uncertainty and can be used to predict the likelihood of events and outcomes while incorporating observed evidence. From an eXplainable AI (XAI) perspective, such models are interesting as they tend to be compact. Moreover, captured relations can be directly inspected by domain experts. In practice, data is often real-valued. Unless assumptions of normality can be made, discretization is often required. The optimal discretization, however, depends on the relations modelled between the variables. This complicates learning Bayesian networks from data. For this reason, most literature focuses on learning conditional dependencies between sets of variables, called structure learning. In this work, we extend an existing state-of-the-art structure learning approach based on the Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA) to jointly learn variable discretizations. The proposed Discretized Bayesian Network GOMEA (DBN-GOMEA) obtains similar or better results than the current state-of-the-art when tasked to retrieve randomly generated ground-truth networks. Moreover, leveraging a key strength of evolutionary algorithms, we can straightforwardly perform DBN learning multi-objectively. We show how this enables incorporating expert knowledge in a uniquely insightful fashion, finding multiple DBNs that trade-off complexity, accuracy, and the difference with a pre-determined expert network.","sentences":["Bayesian networks model relationships between random variables under uncertainty and can be used to predict the likelihood of events and outcomes while incorporating observed evidence.","From an eXplainable AI (XAI) perspective, such models are interesting as they tend to be compact.","Moreover, captured relations can be directly inspected by domain experts.","In practice, data is often real-valued.","Unless assumptions of normality can be made, discretization is often required.","The optimal discretization, however, depends on the relations modelled between the variables.","This complicates learning Bayesian networks from data.","For this reason, most literature focuses on learning conditional dependencies between sets of variables, called structure learning.","In this work, we extend an existing state-of-the-art structure learning approach based on the Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA) to jointly learn variable discretizations.","The proposed Discretized Bayesian Network GOMEA (DBN-GOMEA) obtains similar or better results than the current state-of-the-art when tasked to retrieve randomly generated ground-truth networks.","Moreover, leveraging a key strength of evolutionary algorithms, we can straightforwardly perform DBN learning multi-objectively.","We show how this enables incorporating expert knowledge in a uniquely insightful fashion, finding multiple DBNs that trade-off complexity, accuracy, and the difference with a pre-determined expert network."],"url":"http://arxiv.org/abs/2402.12175v1","category":"cs.LG"}
{"created":"2024-02-19 14:23:18","title":"Automating Boundary Filling in Cubical Agda","abstract":"When working in a proof assistant, automation is key to discharging routine proof goals such as equations between algebraic expressions. Homotopy Type Theory allows the user to reason about higher structures, such as topological spaces, using higher inductive types (HITs) and univalence. Cubical Agda is an extension of Agda with computational support for HITs and univalence. A difficulty when working in Cubical Agda is dealing with the complex combinatorics of higher structures, an infinite-dimensional generalisation of equational reasoning. To solve these higher-dimensional equations consists in constructing cubes with specified boundaries.   We develop a simplified cubical language in which we isolate and study two automation problems: contortion solving, where we attempt to \"contort\" a cube to fit a given boundary, and the more general Kan solving, where we search for solutions that involve pasting multiple cubes together. Both problems are difficult in the general case - Kan solving is even undecidable - so we focus on heuristics that perform well on practical examples. We provide a solver for the contortion problem using a reformulation of contortions in terms of poset maps, while we solve Kan problems using constraint satisfaction programming. We have implemented our algorithms in an experimental Haskell solver that can be used to automatically solve goals presented by Cubical Agda. We illustrate this with a case study establishing the Eckmann-Hilton theorem using our solver, as well as various benchmarks - providing the ground for further study of proof automation in cubical type theories.","sentences":["When working in a proof assistant, automation is key to discharging routine proof goals such as equations between algebraic expressions.","Homotopy Type Theory allows the user to reason about higher structures, such as topological spaces, using higher inductive types (HITs) and univalence.","Cubical Agda is an extension of Agda with computational support for HITs and univalence.","A difficulty when working in Cubical Agda is dealing with the complex combinatorics of higher structures, an infinite-dimensional generalisation of equational reasoning.","To solve these higher-dimensional equations consists in constructing cubes with specified boundaries.   ","We develop a simplified cubical language in which we isolate and study two automation problems: contortion solving, where we attempt to \"contort\" a cube to fit a given boundary, and the more general Kan solving, where we search for solutions that involve pasting multiple cubes together.","Both problems are difficult in the general case - Kan solving is even undecidable - so we focus on heuristics that perform well on practical examples.","We provide a solver for the contortion problem using a reformulation of contortions in terms of poset maps, while we solve Kan problems using constraint satisfaction programming.","We have implemented our algorithms in an experimental Haskell solver that can be used to automatically solve goals presented by Cubical Agda.","We illustrate this with a case study establishing the Eckmann-Hilton theorem using our solver, as well as various benchmarks - providing the ground for further study of proof automation in cubical type theories."],"url":"http://arxiv.org/abs/2402.12169v1","category":"cs.LO"}
{"created":"2024-02-19 14:22:48","title":"Electroweak three-body decays in the presence of two- and three-body bound states","abstract":"Recently, formalism has been derived for studying electroweak transition amplitudes for three-body systems both in infinite and finite volumes. The formalism provides exact relations that the infinite-volume amplitudes must satisfy, as well as a relationship between physical amplitudes and finite-volume matrix elements, which can be constrained from lattice QCD calculations. This formalism poses additional challenges when compared with the analogous well-studied two-body equivalent one, including the necessary step of solving integral equations of singular functions. In this work, we provide some non-trivial analytical and numerical tests on the aforementioned formalism. In particular, we consider a case where the three-particle system can have three-body bound states as well as bound states in the two-body subsystem. For kinematics below the three-body threshold, we demonstrate that the scattering amplitudes satisfy unitarity. We also check that for these kinematics the finite-volume matrix elements are accurately described by the formalism for two-body systems up to exponentially suppressed corrections. Finally, we verify that in the case of the three-body bound state, the finite-volume matrix element is equal to the infinite-volume coupling of the bound state, up to exponentially suppressed errors.","sentences":["Recently, formalism has been derived for studying electroweak transition amplitudes for three-body systems both in infinite and finite volumes.","The formalism provides exact relations that the infinite-volume amplitudes must satisfy, as well as a relationship between physical amplitudes and finite-volume matrix elements, which can be constrained from lattice QCD calculations.","This formalism poses additional challenges when compared with the analogous well-studied two-body equivalent one, including the necessary step of solving integral equations of singular functions.","In this work, we provide some non-trivial analytical and numerical tests on the aforementioned formalism.","In particular, we consider a case where the three-particle system can have three-body bound states as well as bound states in the two-body subsystem.","For kinematics below the three-body threshold, we demonstrate that the scattering amplitudes satisfy unitarity.","We also check that for these kinematics the finite-volume matrix elements are accurately described by the formalism for two-body systems up to exponentially suppressed corrections.","Finally, we verify that in the case of the three-body bound state, the finite-volume matrix element is equal to the infinite-volume coupling of the bound state, up to exponentially suppressed errors."],"url":"http://arxiv.org/abs/2402.12167v1","category":"hep-lat"}
{"created":"2024-02-19 14:22:18","title":"Classifications of cusps appearing on plane curves","abstract":"In this paper, we deal with plane curves with cusps. It is well known that there are various types of cusps. Among them, we investigate criteria for $(n, n+1)$ cusps with respect to several differential conditions and relations between these singularities and evolutes of fronts. We give complete classifications with respect to $(4, 5)$-cusps.","sentences":["In this paper, we deal with plane curves with cusps.","It is well known that there are various types of cusps.","Among them, we investigate criteria for $(n, n+1)$ cusps with respect to several differential conditions and relations between these singularities and evolutes of fronts.","We give complete classifications with respect to $(4, 5)$-cusps."],"url":"http://arxiv.org/abs/2402.12166v1","category":"math.DG"}
{"created":"2024-02-19 13:56:20","title":"Nonlocal to local convergence of phase field systems with inertial term","abstract":"This paper deals with a nonlocal model for a hyperbolic phase field system coupling the standard energy balance equation for temperature with a dynamic for the phase variable: the latter includes an inertial term and a nonlocal convolution-type operator where the family of kernels depends on a small parameter. We rigorously study the asymptotic convergence of the system as the approximating parameter tends to zero and we obtain at the limit the local system with the elliptic laplacian operator acting on the phase variable. Our analysis is based on some asymptotic properties on nonlocal-to-local convergence that have been recently and successfully applied to families of Cahn--Hilliard models.","sentences":["This paper deals with a nonlocal model for a hyperbolic phase field system coupling the standard energy balance equation for temperature with a dynamic for the phase variable: the latter includes an inertial term and a nonlocal convolution-type operator where the family of kernels depends on a small parameter.","We rigorously study the asymptotic convergence of the system as the approximating parameter tends to zero and we obtain at the limit the local system with the elliptic laplacian operator acting on the phase variable.","Our analysis is based on some asymptotic properties on nonlocal-to-local convergence that have been recently and successfully applied to families of Cahn--Hilliard models."],"url":"http://arxiv.org/abs/2402.12145v1","category":"math.AP"}
{"created":"2024-02-19 18:57:02","title":"Computing Enclosing Depth","abstract":"Enclosing depth is a recently introduced depth measure which gives a lower bound to many depth measures studied in the literature. So far, enclosing depth has only been studied from a combinatorial perspective. In this work, we give the first algorithms to compute the enclosing depth of a query point with respect to a data point set in any dimension. In the plane we are able to optimize the algorithm to get a runtime of O(n log n). In constant dimension, our algorithms still run in polynomial time.","sentences":["Enclosing depth is a recently introduced depth measure which gives a lower bound to many depth measures studied in the literature.","So far, enclosing depth has only been studied from a combinatorial perspective.","In this work, we give the first algorithms to compute the enclosing depth of a query point with respect to a data point set in any dimension.","In the plane we are able to optimize the algorithm to get a runtime of O(n log n).","In constant dimension, our algorithms still run in polynomial time."],"url":"http://arxiv.org/abs/2402.12371v1","category":"cs.CG"}
{"created":"2024-02-19 18:48:19","title":"SPAM-Robust Multi-axis Quantum Noise Spectroscopy in Temporally Correlated Environments","abstract":"Characterizing temporally correlated (``non-Markovian'') noise is a key prerequisite for achieving noise-tailored error mitigation and optimal device performance. Quantum noise spectroscopy can afford quantitative estimation of the noise spectral features; however, in its current form it is highly vulnerable to implementation non-idealities, notably, state-preparation and measurement (SPAM) errors. Further to that, existing protocols have been mostly developed for dephasing-dominated noise processes, with competing dephasing and relaxation effects being largely unaccounted for. We introduce quantum noise spectroscopy protocols inspired by spin-locking techniques that enable the characterization of arbitrary temporally correlated multi-axis noise on a qubit with fixed energy splitting, while remaining resilient to realistic static SPAM errors. By validating our protocol's performance in both numerical simulation and cloud-based IBM quantum processors, we demonstrate the successful separation and estimation of native noise spectrum components as well as SPAM error rates. We find that SPAM errors can significantly alter or mask important noise features, with spectra overestimated by up to 26.4% in a classical noise regime. Clear signatures of non-classical noise are manifest in the reconstructed IBM-qubit dephasing spectra, once SPAM-error effects are compensated for. Our work provides a timely tool for benchmarking realistic sources of noise in qubit devices.","sentences":["Characterizing temporally correlated (``non-Markovian'') noise is a key prerequisite for achieving noise-tailored error mitigation and optimal device performance.","Quantum noise spectroscopy can afford quantitative estimation of the noise spectral features; however, in its current form it is highly vulnerable to implementation non-idealities, notably, state-preparation and measurement (SPAM) errors.","Further to that, existing protocols have been mostly developed for dephasing-dominated noise processes, with competing dephasing and relaxation effects being largely unaccounted for.","We introduce quantum noise spectroscopy protocols inspired by spin-locking techniques that enable the characterization of arbitrary temporally correlated multi-axis noise on a qubit with fixed energy splitting, while remaining resilient to realistic static SPAM errors.","By validating our protocol's performance in both numerical simulation and cloud-based IBM quantum processors, we demonstrate the successful separation and estimation of native noise spectrum components as well as SPAM error rates.","We find that SPAM errors can significantly alter or mask important noise features, with spectra overestimated by up to 26.4% in a classical noise regime.","Clear signatures of non-classical noise are manifest in the reconstructed IBM-qubit dephasing spectra, once SPAM-error effects are compensated for.","Our work provides a timely tool for benchmarking realistic sources of noise in qubit devices."],"url":"http://arxiv.org/abs/2402.12361v1","category":"quant-ph"}
{"created":"2024-02-19 18:13:09","title":"Simple Mechanisms for Utility Maximization: Approximating Welfare in the I.I.D. Unit-Demand Setting","abstract":"We investigate the objective of utility maximization from the perspective of Bayesian mechanism design, initiating this direction, and focus on the unit-demand setting where values are i.i.d. across both items and buyers. We take the approach of developing simple, approximately optimal mechanisms, targeting the simplest benchmark of optimal welfare. We give a $(1-1/e)$-approximation when there are more items than buyers, and an $O(\\log(n/m))$-approximation when there are more buyers than items, which is tight up to constant factors. We also characterize complexities in this setting that defy our intuition from the welfare and revenue literature, and motivate why coming up with a better benchmark than welfare is a hard problem itself.","sentences":["We investigate the objective of utility maximization from the perspective of Bayesian mechanism design, initiating this direction, and focus on the unit-demand setting where values are i.i.d. across both items and buyers.","We take the approach of developing simple, approximately optimal mechanisms, targeting the simplest benchmark of optimal welfare.","We give a $(1-1/e)$-approximation when there are more items than buyers, and an $O(\\log(n/m))$-approximation when there are more buyers than items, which is tight up to constant factors.","We also characterize complexities in this setting that defy our intuition from the welfare and revenue literature, and motivate why coming up with a better benchmark than welfare is a hard problem itself."],"url":"http://arxiv.org/abs/2402.12340v1","category":"cs.GT"}
{"created":"2024-02-19 18:10:48","title":"Hot carrier distribution engineering by alloying: picking elements for the desired purposes","abstract":"Metal alloys hold the promise of providing hot carrier generation distributions superior to pure metals in applications such as sensing, catalysis and solar energy harvesting. Guidelines for finding the optimal alloy configuration for a target application require understanding the connection between alloy composition and hot carrier distribution. Here we present a DFT-based computational approach to investigate the photo-generated hot carrier distribution of metal alloys based on the joint density of states and the electronic structure. We classified the metals by their electronic structure into closed d-shell, open d-shell, p-block and s-block elements. It is shown that combining closed d-shell elements enables modulating the distribution of highly energetic holes typical of pure metals but also leads to hot carrier production by IR light excitation and the appearance of highly energetic electrons due to band folding and splitting. This feature arises as an emergent property of alloying and is only unveiled when the hot carrier distribution computation takes momentum conservation into account. The combination of closed d-shell with open d-shell elements allows an abundant production of hot carriers in a broad energy range, while alloying a closed d-shell elements with an s-block element opens the door to hot electron distribution skewed toward high energy electrons. The combination of d-shell with p-block elements results in moderate hot carrier distribution whose asymmetry can be tuned by composition. Overall, the obtained insights that connect alloy composition, band structure and resulting carrier distribution provide a toolkit to match elements in an alloy for the deliberate engineering of hot carrier distribution.","sentences":["Metal alloys hold the promise of providing hot carrier generation distributions superior to pure metals in applications such as sensing, catalysis and solar energy harvesting.","Guidelines for finding the optimal alloy configuration for a target application require understanding the connection between alloy composition and hot carrier distribution.","Here we present a DFT-based computational approach to investigate the photo-generated hot carrier distribution of metal alloys based on the joint density of states and the electronic structure.","We classified the metals by their electronic structure into closed d-shell, open d-shell, p-block and s-block elements.","It is shown that combining closed d-shell elements enables modulating the distribution of highly energetic holes typical of pure metals but also leads to hot carrier production by IR light excitation and the appearance of highly energetic electrons due to band folding and splitting.","This feature arises as an emergent property of alloying and is only unveiled when the hot carrier distribution computation takes momentum conservation into account.","The combination of closed d-shell with open d-shell elements allows an abundant production of hot carriers in a broad energy range, while alloying a closed d-shell elements with an s-block element opens the door to hot electron distribution skewed toward high energy electrons.","The combination of d-shell with p-block elements results in moderate hot carrier distribution whose asymmetry can be tuned by composition.","Overall, the obtained insights that connect alloy composition, band structure and resulting carrier distribution provide a toolkit to match elements in an alloy for the deliberate engineering of hot carrier distribution."],"url":"http://arxiv.org/abs/2402.12337v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-19 17:49:23","title":"Landmark Stereo Dataset for Landmark Recognition and Moving Node Localization in a Non-GPS Battlefield Environment","abstract":"In this paper, we have proposed a new strategy of using the landmark anchor node instead of a radio-based anchor node to obtain the virtual coordinates (landmarkID, DISTANCE) of moving troops or defense forces that will help in tracking and maneuvering the troops along a safe path within a GPS-denied battlefield environment. The proposed strategy implements landmark recognition using the Yolov5 model and landmark distance estimation using an efficient Stereo Matching Algorithm. We consider that a moving node carrying a low-power mobile device facilitated with a calibrated stereo vision camera that captures stereo images of a scene containing landmarks within the battlefield region whose locations are stored in an offline server residing within the device itself. We created a custom landmark image dataset called MSTLandmarkv1 with 34 landmark classes and another landmark stereo dataset of those 34 landmark instances called MSTLandmarkStereov1. We trained the YOLOv5 model with MSTLandmarkv1 dataset and achieved 0.95 mAP @ 0.5 IoU and 0.767 mAP @ [0.5: 0.95] IoU. We calculated the distance from a node to the landmark utilizing the bounding box coordinates and the depth map generated by the improved SGM algorithm using MSTLandmarkStereov1. The tuple of landmark IDs obtained from the detection result and the distances calculated by the SGM algorithm are stored as the virtual coordinates of a node. In future work, we will use these virtual coordinates to obtain the location of a node using an efficient trilateration algorithm and optimize the node position using the appropriate optimization method.","sentences":["In this paper, we have proposed a new strategy of using the landmark anchor node instead of a radio-based anchor node to obtain the virtual coordinates (landmarkID, DISTANCE) of moving troops or defense forces that will help in tracking and maneuvering the troops along a safe path within a GPS-denied battlefield environment.","The proposed strategy implements landmark recognition using the Yolov5 model and landmark distance estimation using an efficient Stereo Matching Algorithm.","We consider that a moving node carrying a low-power mobile device facilitated with a calibrated stereo vision camera that captures stereo images of a scene containing landmarks within the battlefield region whose locations are stored in an offline server residing within the device itself.","We created a custom landmark image dataset called MSTLandmarkv1 with 34 landmark classes and another landmark stereo dataset of those 34 landmark instances called MSTLandmarkStereov1.","We trained the YOLOv5 model with MSTLandmarkv1 dataset and achieved 0.95 mAP @ 0.5 IoU and 0.767 mAP @","[0.5: 0.95] IoU. We calculated the distance from a node to the landmark utilizing the bounding box coordinates and the depth map generated by the improved SGM algorithm using MSTLandmarkStereov1.","The tuple of landmark IDs obtained from the detection result and the distances calculated by the SGM algorithm are stored as the virtual coordinates of a node.","In future work, we will use these virtual coordinates to obtain the location of a node using an efficient trilateration algorithm and optimize the node position using the appropriate optimization method."],"url":"http://arxiv.org/abs/2402.12320v1","category":"cs.CV"}
{"created":"2024-02-19 17:37:50","title":"Memory attacks in network nonlocality and self-testing","abstract":"We study what can or cannot be certified in communication scenarios where the assumption of independence and identical distribution (iid) between experimental rounds fails. In this respect, we prove that membership tests for non-convex sets of correlations cannot be formulated in the non-iid regime. Similarly, it is impossible to self-test non-extreme quantum operations, such as mixed states, or noisy quantum measurements, unless one allows more than a single use thereof within the same experimental round. One consequence of our results is that non-classicality in causal networks without inputs cannot be experimentally demonstrated. By analyzing optimal non-iid strategies in the triangle scenario, we raise the need to take into account the prior communication required to set up a causal network.","sentences":["We study what can or cannot be certified in communication scenarios where the assumption of independence and identical distribution (iid) between experimental rounds fails.","In this respect, we prove that membership tests for non-convex sets of correlations cannot be formulated in the non-iid regime.","Similarly, it is impossible to self-test non-extreme quantum operations, such as mixed states, or noisy quantum measurements, unless one allows more than a single use thereof within the same experimental round.","One consequence of our results is that non-classicality in causal networks without inputs cannot be experimentally demonstrated.","By analyzing optimal non-iid strategies in the triangle scenario, we raise the need to take into account the prior communication required to set up a causal network."],"url":"http://arxiv.org/abs/2402.12318v1","category":"quant-ph"}
{"created":"2024-02-19 16:58:03","title":"Statistical evaluation and optimization of entanglement purification protocols","abstract":"Quantitative characterization of two-qubit entanglement purification protocols is introduced. Our approach is based on the concurrence and the hit-and-run algorithm applied to the convex set of all two-qubit states. We demonstrate that pioneering protocols are unable to improve the estimated initial average concurrence of almost uniformly sampled density matrices, however, as it is known, they still generate pairs of qubits in a state that is close to a Bell state. We also develop a more efficient protocol and investigate it numerically together with a recent proposal based on an entangling rank-two projector. Furthermore, we present a class of variational purification protocols with continuous parameters and optimize their output concurrence. These optimized algorithms turn out to surpass former proposals and our new protocol by means of not wasting too many entangled states.","sentences":["Quantitative characterization of two-qubit entanglement purification protocols is introduced.","Our approach is based on the concurrence and the hit-and-run algorithm applied to the convex set of all two-qubit states.","We demonstrate that pioneering protocols are unable to improve the estimated initial average concurrence of almost uniformly sampled density matrices, however, as it is known, they still generate pairs of qubits in a state that is close to a Bell state.","We also develop a more efficient protocol and investigate it numerically together with a recent proposal based on an entangling rank-two projector.","Furthermore, we present a class of variational purification protocols with continuous parameters and optimize their output concurrence.","These optimized algorithms turn out to surpass former proposals and our new protocol by means of not wasting too many entangled states."],"url":"http://arxiv.org/abs/2402.12287v1","category":"quant-ph"}
{"created":"2024-02-19 16:51:24","title":"Solution Polishing via Path Relinking for Continuous Black-Box Optimization","abstract":"When faced with a limited budget of function evaluations, state-of-the-art black-box optimization (BBO) solvers struggle to obtain globally, or sometimes even locally, optimal solutions. In such cases, one may pursue solution polishing, i.e., a computational method to improve (or ``polish'') an incumbent solution, typically via some sort of evolutionary algorithm involving two or more solutions. While solution polishing in ``white-box'' optimization has existed for years, relatively little has been published regarding its application in costly-to-evaluate BBO. To fill this void, we explore two novel methods for performing solution polishing along one-dimensional curves rather than along straight lines. We introduce a convex quadratic program that can generate promising curves through multiple elite solutions, i.e., via path relinking, or around a single elite solution. In comparing four solution polishing techniques for continuous BBO, we show that solution polishing along a curve is competitive with solution polishing using a state-of-the-art BBO solver.","sentences":["When faced with a limited budget of function evaluations, state-of-the-art black-box optimization (BBO) solvers struggle to obtain globally, or sometimes even locally, optimal solutions.","In such cases, one may pursue solution polishing, i.e., a computational method to improve (or ``polish'') an incumbent solution, typically via some sort of evolutionary algorithm involving two or more solutions.","While solution polishing in ``white-box'' optimization has existed for years, relatively little has been published regarding its application in costly-to-evaluate BBO.","To fill this void, we explore two novel methods for performing solution polishing along one-dimensional curves rather than along straight lines.","We introduce a convex quadratic program that can generate promising curves through multiple elite solutions, i.e., via path relinking, or around a single elite solution.","In comparing four solution polishing techniques for continuous BBO, we show that solution polishing along a curve is competitive with solution polishing using a state-of-the-art BBO solver."],"url":"http://arxiv.org/abs/2402.12283v1","category":"math.OC"}
{"created":"2024-02-19 16:30:24","title":"The Quantitative Fractional Helly theorem","abstract":"Two celebrated extensions of Helly's theorem are the Fractional Helly theorem of Katchalski and Liu (1979) and the Quantitative Volume theorem of B\\'ar\\'any, Katchalski, and Pach (1982). Improving on several recent works, we prove an optimal combination of these two results. We show that given a family $\\mathcal{F}$ of $n$ convex sets in $\\mathbb{R}^d$ such that at least $\\alpha \\binom{n}{d+1}$ of the $(d+1)$-tuples of $\\mathcal{F}$ have an intersection of volume at least 1, then one can select $\\Omega_{d,\\alpha}(n)$ members of $\\mathcal{F}$ whose intersection has volume at least $\\Omega_d(1)$.   Furthermore, with the help of this theorem, we establish a quantitative version of the $(p,q)$ theorem of Alon and Kleitman. Let $p\\geq q\\geq d+1$ and let $\\mathcal{F}$ be a finite family of convex sets in $\\mathbb{R}^d$ such that among any $p$ elements of $\\mathcal{F}$, there are $q$ that have an intersection of volume at least $1$. Then, we prove that there exists a family $T$ of $O_{p,q}(1)$ ellipsoids of volume $\\Omega_d(1)$ such that every member of $\\mathcal{F}$ contains at least one element of $T$.","sentences":["Two celebrated extensions of Helly's theorem are the Fractional Helly theorem of Katchalski and Liu (1979) and the Quantitative Volume theorem of B\\'ar\\'any, Katchalski, and Pach (1982).","Improving on several recent works, we prove an optimal combination of these two results.","We show that given a family $\\mathcal{F}$ of $n$ convex sets in $\\mathbb{R}^d$ such that at least $\\alpha \\binom{n}{d+1}$ of the $(d+1)$-tuples of $\\mathcal{F}$ have an intersection of volume at least 1, then one can select $\\Omega_{d,\\alpha}(n)$ members of $\\mathcal{F}$ whose intersection has volume at least $\\Omega_d(1)$.   Furthermore, with the help of this theorem, we establish a quantitative version of the $(p,q)$ theorem of Alon and Kleitman.","Let $p\\geq q\\geq d+1$ and let $\\mathcal{F}$ be a finite family of convex sets in $\\mathbb{R}^d$ such that among any $p$ elements of $\\mathcal{F}$, there are $q$ that have an intersection of volume at least $1$.","Then, we prove that there exists a family $T$ of $O_{p,q}(1)$ ellipsoids of volume $\\Omega_d(1)$ such that every member of $\\mathcal{F}$ contains at least one element of $T$."],"url":"http://arxiv.org/abs/2402.12268v1","category":"math.CO"}
{"created":"2024-02-19 16:12:20","title":"Stability of the coronal magnetic field around large confined and eruptive solar flares","abstract":"In order to improve our understanding on the pre-requisites of eruptive solar flares, we study and compare different measures that characterize the eruptive potential of solar active regions - the critical height for torus instability as a local measure and the helicity ratio as a global measure - with the structural properties of the underlying magnetic field, namely the altitude of the center of the current-carrying magnetic structure. Using time series of 3D optimization-based nonlinear force-free magnetic field models for 10 different active regions (ARs) around the time of large solar flares, we determine the altitudes of the current-weighted centers of the non-potential model structures. Based on the potential magnetic field, we inspect the decay index, $n$, in multiple vertical planes oriented along of or perpendicular to the flare-relevant polarity inversion line, and estimate the critical height ($h_{\\mathrm{crit}}$) for torus instability (TI) using different thresholds of $n$. The critical heights are interpreted with respect to the altitudes of the current-weighted centers of the associated non-potential structures, as well as the eruptive character of the associated flares, and the eruptive potential of the host AR, as characterized by the helicity ratio. Our most important findings are that (i) $h_{\\mathrm{crit}}$ is more segregated in terms of flare type than the helicity ratio, and that (ii) coronal field configurations with a higher eruptive potential (in terms of the helicity ratio) also appear to be more prone to TI. Furthermore, we find no pronounced differences in the altitudes of the non-potential structures prior to confined and eruptive flares.","sentences":["In order to improve our understanding on the pre-requisites of eruptive solar flares, we study and compare different measures that characterize the eruptive potential of solar active regions - the critical height for torus instability as a local measure and the helicity ratio as a global measure - with the structural properties of the underlying magnetic field, namely the altitude of the center of the current-carrying magnetic structure.","Using time series of 3D optimization-based nonlinear force-free magnetic field models for 10 different active regions (ARs) around the time of large solar flares, we determine the altitudes of the current-weighted centers of the non-potential model structures.","Based on the potential magnetic field, we inspect the decay index, $n$, in multiple vertical planes oriented along of or perpendicular to the flare-relevant polarity inversion line, and estimate the critical height ($h_{\\mathrm{crit}}$) for torus instability (TI) using different thresholds of $n$. The critical heights are interpreted with respect to the altitudes of the current-weighted centers of the associated non-potential structures, as well as the eruptive character of the associated flares, and the eruptive potential of the host AR, as characterized by the helicity ratio.","Our most important findings are that (i) $h_{\\mathrm{crit}}$ is more segregated in terms of flare type than the helicity ratio, and that (ii) coronal field configurations with a higher eruptive potential (in terms of the helicity ratio) also appear to be more prone to TI.","Furthermore, we find no pronounced differences in the altitudes of the non-potential structures prior to confined and eruptive flares."],"url":"http://arxiv.org/abs/2402.12254v1","category":"astro-ph.SR"}
{"created":"2024-02-19 16:05:43","title":"Derivative-Free iterative One-Step Reconstruction for Multispectral CT","abstract":"Image reconstruction in Multispectral Computed Tomography (MSCT) requires solving a challenging nonlinear inverse problem, commonly tackled via iterative optimization algorithms. Existing methods necessitate computing the derivative of the forward map and potentially its regularized inverse. In this work, we present a simple yet highly effective algorithm for MSCT image reconstruction, utilizing iterative update mechanisms that leverage the full forward model in the forward step and a derivative-free adjoint problem. Our approach demonstrates both fast convergence and superior performance compared to existing algorithms, making it an interesting candidate for future work. We also discuss further generalizations of our method and its combination with additional regularization and other data discrepancy terms.","sentences":["Image reconstruction in Multispectral Computed Tomography (MSCT) requires solving a challenging nonlinear inverse problem, commonly tackled via iterative optimization algorithms.","Existing methods necessitate computing the derivative of the forward map and potentially its regularized inverse.","In this work, we present a simple yet highly effective algorithm for MSCT image reconstruction, utilizing iterative update mechanisms that leverage the full forward model in the forward step and a derivative-free adjoint problem.","Our approach demonstrates both fast convergence and superior performance compared to existing algorithms, making it an interesting candidate for future work.","We also discuss further generalizations of our method and its combination with additional regularization and other data discrepancy terms."],"url":"http://arxiv.org/abs/2402.12250v1","category":"math.NA"}
{"created":"2024-02-19 15:59:48","title":"Unconditional quantum MAGIC advantage in shallow circuit computation","abstract":"Quantum theory promises computation speed-ups than classical means. The full power is believed to reside in \"magic\" states, or equivalently non-Clifford operations -- the secret sauce to establish universal quantum computing. Despite the celebrated Gottesman-Knill Theorem stating that magic-free computation can be efficiently simulated by a classical computer, it is still questionable whether \"magic\" is really magical. Indeed, all the existing results establish its supremacy for efficient computation upon unproven complexity assumptions or queries to black-box oracles. In this work, we show that the magic advantage can be unconditionally established, at least in a shallow circuit with a constant depth. For this purpose, we first construct a specific nonlocal game inspired by the linear binary constraint system, which requires the magic resource to generate the desired nonlocal statistics or quantum \"pseudo telepathy.\" For a relation problem targeting generating such correlations between arbitrary nonlocal computation sites, we construct a shallow circuit with bounded fan-in gates that takes the strategy for quantum pseudo telepathy as a sub-routine to solve the problem with certainty. In contrast, magic-free counterparts inevitably require a logarithmic circuit depth to the input size, and the separation is proven optimal. As by-products, we prove that the nonlocal game we construct has non-unique perfect winning strategies, answering an open problem in quantum self-testing. We also provide an efficient algorithm to aid the search for potential magic-requiring nonlocal games similar to the current one. We anticipate our results to enlighten the ultimate establishment of the unconditional advantage of universal quantum computation.","sentences":["Quantum theory promises computation speed-ups than classical means.","The full power is believed to reside in \"magic\" states, or equivalently non-Clifford operations -- the secret sauce to establish universal quantum computing.","Despite the celebrated Gottesman-Knill Theorem stating that magic-free computation can be efficiently simulated by a classical computer, it is still questionable whether \"magic\" is really magical.","Indeed, all the existing results establish its supremacy for efficient computation upon unproven complexity assumptions or queries to black-box oracles.","In this work, we show that the magic advantage can be unconditionally established, at least in a shallow circuit with a constant depth.","For this purpose, we first construct a specific nonlocal game inspired by the linear binary constraint system, which requires the magic resource to generate the desired nonlocal statistics or quantum \"pseudo telepathy.\"","For a relation problem targeting generating such correlations between arbitrary nonlocal computation sites, we construct a shallow circuit with bounded fan-in gates that takes the strategy for quantum pseudo telepathy as a sub-routine to solve the problem with certainty.","In contrast, magic-free counterparts inevitably require a logarithmic circuit depth to the input size, and the separation is proven optimal.","As by-products, we prove that the nonlocal game we construct has non-unique perfect winning strategies, answering an open problem in quantum self-testing.","We also provide an efficient algorithm to aid the search for potential magic-requiring nonlocal games similar to the current one.","We anticipate our results to enlighten the ultimate establishment of the unconditional advantage of universal quantum computation."],"url":"http://arxiv.org/abs/2402.12246v1","category":"quant-ph"}
{"created":"2024-02-19 15:57:39","title":"Synthetic location trajectory generation using categorical diffusion models","abstract":"Diffusion probabilistic models (DPMs) have rapidly evolved to be one of the predominant generative models for the simulation of synthetic data, for instance, for computer vision, audio, natural language processing, or biomolecule generation. Here, we propose using DPMs for the generation of synthetic individual location trajectories (ILTs) which are sequences of variables representing physical locations visited by individuals. ILTs are of major importance in mobility research to understand the mobility behavior of populations and to ultimately inform political decision-making. We represent ILTs as multi-dimensional categorical random variables and propose to model their joint distribution using a continuous DPM by first applying the diffusion process in a continuous unconstrained space and then mapping the continuous variables into a discrete space. We demonstrate that our model can synthesize realistic ILPs by comparing conditionally and unconditionally generated sequences to real-world ILPs from a GNSS tracking data set which suggests the potential use of our model for synthetic data generation, for example, for benchmarking models used in mobility research.","sentences":["Diffusion probabilistic models (DPMs) have rapidly evolved to be one of the predominant generative models for the simulation of synthetic data, for instance, for computer vision, audio, natural language processing, or biomolecule generation.","Here, we propose using DPMs for the generation of synthetic individual location trajectories (ILTs) which are sequences of variables representing physical locations visited by individuals.","ILTs are of major importance in mobility research to understand the mobility behavior of populations and to ultimately inform political decision-making.","We represent ILTs as multi-dimensional categorical random variables and propose to model their joint distribution using a continuous DPM by first applying the diffusion process in a continuous unconstrained space and then mapping the continuous variables into a discrete space.","We demonstrate that our model can synthesize realistic ILPs by comparing conditionally and unconditionally generated sequences to real-world ILPs from a GNSS tracking data set which suggests the potential use of our model for synthetic data generation, for example, for benchmarking models used in mobility research."],"url":"http://arxiv.org/abs/2402.12242v1","category":"cs.LG"}
{"created":"2024-02-19 15:48:55","title":"Mixed Gaussian Flow for Diverse Trajectory Prediction","abstract":"Existing trajectory prediction studies intensively leverage generative models. Normalizing flow is one of the genres with the advantage of being invertible to derive the probability density of predicted trajectories. However, mapping from a standard Gaussian by a flow-based model hurts the capacity to capture complicated patterns of trajectories, ignoring the under-represented motion intentions in the training data. To solve the problem, we propose a flow-based model to transform a mixed Gaussian prior into the future trajectory manifold. The model shows a better capacity for generating diverse trajectory patterns. Also, by associating each sub-Gaussian with a certain subspace of trajectories, we can generate future trajectories with controllable motion intentions. In such a fashion, the flow-based model is not encouraged to simply seek the most likelihood of the intended manifold anymore but a family of controlled manifolds with explicit interpretability. Our proposed method is demonstrated to show state-of-the-art performance in the quantitative evaluation of sampling well-aligned trajectories in top-M generated candidates. We also demonstrate that it can generate diverse, controllable, and out-of-distribution trajectories. Code is available at https://github.com/mulplue/MGF.","sentences":["Existing trajectory prediction studies intensively leverage generative models.","Normalizing flow is one of the genres with the advantage of being invertible to derive the probability density of predicted trajectories.","However, mapping from a standard Gaussian by a flow-based model hurts the capacity to capture complicated patterns of trajectories, ignoring the under-represented motion intentions in the training data.","To solve the problem, we propose a flow-based model to transform a mixed Gaussian prior into the future trajectory manifold.","The model shows a better capacity for generating diverse trajectory patterns.","Also, by associating each sub-Gaussian with a certain subspace of trajectories, we can generate future trajectories with controllable motion intentions.","In such a fashion, the flow-based model is not encouraged to simply seek the most likelihood of the intended manifold anymore but a family of controlled manifolds with explicit interpretability.","Our proposed method is demonstrated to show state-of-the-art performance in the quantitative evaluation of sampling well-aligned trajectories in top-M generated candidates.","We also demonstrate that it can generate diverse, controllable, and out-of-distribution trajectories.","Code is available at https://github.com/mulplue/MGF."],"url":"http://arxiv.org/abs/2402.12238v1","category":"cs.CV"}
{"created":"2024-02-19 15:19:10","title":"Forming Long-range Order of Semiconducting Polymers through Liquid-phase Directional Molecular Assemblies","abstract":"Intermolecular interactions are crucial in determining the morphology of solution-processed semiconducting polymer thin films. However, these random interactions often lead to disordered or short-range ordered structures. Achieving long-range order in these films has been a challenge due to limited control over microscopic interactions in current techniques. Here, we present a molecular-level methodology that leverages spatial matching of intermolecular dynamics among solutes, solvents, and substrates to induce directional molecular assembly in weakly bonded polymers. Within the optimized dynamic scale of 2.5 \\r{A} between polymer side chains and self-assembled monolayers (SAMs) on nanogrooved substrates, our approach transforms random aggregates into unidirectional fibers with a remarkable increase in the anisotropic stacking ratio from 1 to 11. The Flory-Huggins-based molecular stacking model accurately predicts the transitioning order on various SAMs, validated by morphologic and spectroscopic observations. The enhanced structural ordering spans over 3 orders of magnitude in length, raising from the smallest 7.3 nm random crystallites to >14 um unidirectional fibers on sub-millimeter areas. Overall, this study provides insights into the control of complex intermolecular interactions and offers enhanced molecular-level controllability in solution-based processes.","sentences":["Intermolecular interactions are crucial in determining the morphology of solution-processed semiconducting polymer thin films.","However, these random interactions often lead to disordered or short-range ordered structures.","Achieving long-range order in these films has been a challenge due to limited control over microscopic interactions in current techniques.","Here, we present a molecular-level methodology that leverages spatial matching of intermolecular dynamics among solutes, solvents, and substrates to induce directional molecular assembly in weakly bonded polymers.","Within the optimized dynamic scale of 2.5 \\r{A} between polymer side chains and self-assembled monolayers (SAMs) on nanogrooved substrates, our approach transforms random aggregates into unidirectional fibers with a remarkable increase in the anisotropic stacking ratio from 1 to 11.","The Flory-Huggins-based molecular stacking model accurately predicts the transitioning order on various SAMs, validated by morphologic and spectroscopic observations.","The enhanced structural ordering spans over 3 orders of magnitude in length, raising from the smallest 7.3 nm random crystallites to >14 um unidirectional fibers on sub-millimeter areas.","Overall, this study provides insights into the control of complex intermolecular interactions and offers enhanced molecular-level controllability in solution-based processes."],"url":"http://arxiv.org/abs/2402.12215v1","category":"cond-mat.soft"}
{"created":"2024-02-19 15:07:24","title":"MPI Implementation Profiling for Better Application Performance","abstract":"While application profiling has been a mainstay in the HPC community for years, profiling of MPI and other communication middleware has not received the same degree of exploration. This paper adds to the discussion of MPI profiling, contributing two general-purpose profiling methods as well as practical applications of these methods to an existing implementation. The ability to detect performance defects in MPI codes using these methods increases the potential of further research and development in communication optimization.","sentences":["While application profiling has been a mainstay in the HPC community for years, profiling of MPI and other communication middleware has not received the same degree of exploration.","This paper adds to the discussion of MPI profiling, contributing two general-purpose profiling methods as well as practical applications of these methods to an existing implementation.","The ability to detect performance defects in MPI codes using these methods increases the potential of further research and development in communication optimization."],"url":"http://arxiv.org/abs/2402.12203v1","category":"cs.DC"}
{"created":"2024-02-19 14:54:23","title":"On Coupling Constraints in Linear Bilevel Optimization","abstract":"It is well-known that coupling constraints in linear bilevel optimization can lead to disconnected feasible sets, which is not possible without coupling constraints. However, there is no difference between linear bilevel problems with and without coupling constraints w.r.t. their complexity-theoretical hardness. In this note, we prove that, although there is a clear difference between these two classes of problems in terms of their feasible sets, the classes are equivalent on the level of optimal solutions. To this end, given a general linear bilevel problem with coupling constraints, we derive a respective problem without coupling constraints and prove that it has the same optimal solutions (when projected back to the original variable space).","sentences":["It is well-known that coupling constraints in linear bilevel optimization can lead to disconnected feasible sets, which is not possible without coupling constraints.","However, there is no difference between linear bilevel problems with and without coupling constraints w.r.t.","their complexity-theoretical hardness.","In this note, we prove that, although there is a clear difference between these two classes of problems in terms of their feasible sets, the classes are equivalent on the level of optimal solutions.","To this end, given a general linear bilevel problem with coupling constraints, we derive a respective problem without coupling constraints and prove that it has the same optimal solutions (when projected back to the original variable space)."],"url":"http://arxiv.org/abs/2402.12191v1","category":"math.OC"}
{"created":"2024-02-19 14:51:20","title":"Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep Learning via Adversarial Training","abstract":"Deep learning models continue to advance in accuracy, yet they remain vulnerable to adversarial attacks, which often lead to the misclassification of adversarial examples. Adversarial training is used to mitigate this problem by increasing robustness against these attacks. However, this approach typically reduces a model's standard accuracy on clean, non-adversarial samples. The necessity for deep learning models to balance both robustness and accuracy for security is obvious, but achieving this balance remains challenging, and the underlying reasons are yet to be clarified. This paper proposes a novel adversarial training method called Adversarial Feature Alignment (AFA), to address these problems. Our research unveils an intriguing insight: misalignment within the feature space often leads to misclassification, regardless of whether the samples are benign or adversarial. AFA mitigates this risk by employing a novel optimization algorithm based on contrastive learning to alleviate potential feature misalignment. Through our evaluations, we demonstrate the superior performance of AFA. The baseline AFA delivers higher robust accuracy than previous adversarial contrastive learning methods while minimizing the drop in clean accuracy to 1.86% and 8.91% on CIFAR10 and CIFAR100, respectively, in comparison to cross-entropy. We also show that joint optimization of AFA and TRADES, accompanied by data augmentation using a recent diffusion model, achieves state-of-the-art accuracy and robustness.","sentences":["Deep learning models continue to advance in accuracy, yet they remain vulnerable to adversarial attacks, which often lead to the misclassification of adversarial examples.","Adversarial training is used to mitigate this problem by increasing robustness against these attacks.","However, this approach typically reduces a model's standard accuracy on clean, non-adversarial samples.","The necessity for deep learning models to balance both robustness and accuracy for security is obvious, but achieving this balance remains challenging, and the underlying reasons are yet to be clarified.","This paper proposes a novel adversarial training method called Adversarial Feature Alignment (AFA), to address these problems.","Our research unveils an intriguing insight: misalignment within the feature space often leads to misclassification, regardless of whether the samples are benign or adversarial.","AFA mitigates this risk by employing a novel optimization algorithm based on contrastive learning to alleviate potential feature misalignment.","Through our evaluations, we demonstrate the superior performance of AFA.","The baseline AFA delivers higher robust accuracy than previous adversarial contrastive learning methods while minimizing the drop in clean accuracy to 1.86% and 8.91% on CIFAR10 and CIFAR100, respectively, in comparison to cross-entropy.","We also show that joint optimization of AFA and TRADES, accompanied by data augmentation using a recent diffusion model, achieves state-of-the-art accuracy and robustness."],"url":"http://arxiv.org/abs/2402.12187v1","category":"cs.CV"}
{"created":"2024-02-19 14:01:34","title":"Local certification of forbidden subgraphs","abstract":"Detecting specific structures in a network has been a very active theme of research in distributed computing for at least a decade. In this paper, we start the study of subgraph detection from the perspective of local certification. Remember that a local certification is a distributed mechanism enabling the nodes of a network to check the correctness of the current configuration, thanks to small pieces of information called certificates. Our main question is: For a given graph $H$, what is the minimum certificate size that allows checking that the network does not contain $H$ as a (possibly induced) subgraph?   We show a variety of lower and upper bounds, uncovering an interesting interplay between the optimal certificate size, the size of the forbidden subgraph, and the locality of the verification. Along the way we introduce several new technical tools, in particular what we call the \\emph{layered map}, which is not specific to forbidden subgraphs and that we expect to be useful for certifying many other properties.","sentences":["Detecting specific structures in a network has been a very active theme of research in distributed computing for at least a decade.","In this paper, we start the study of subgraph detection from the perspective of local certification.","Remember that a local certification is a distributed mechanism enabling the nodes of a network to check the correctness of the current configuration, thanks to small pieces of information called certificates.","Our main question is: For a given graph $H$, what is the minimum certificate size that allows checking that the network does not contain $H$ as a (possibly induced) subgraph?   ","We show a variety of lower and upper bounds, uncovering an interesting interplay between the optimal certificate size, the size of the forbidden subgraph, and the locality of the verification.","Along the way we introduce several new technical tools, in particular what we call the \\emph{layered map}, which is not specific to forbidden subgraphs and that we expect to be useful for certifying many other properties."],"url":"http://arxiv.org/abs/2402.12148v1","category":"cs.DC"}
{"created":"2024-02-19 13:53:13","title":"Connectivity Labeling in Faulty Colored Graphs","abstract":"Fault-tolerant connectivity labelings are schemes that, given an $n$-vertex graph $G=(V,E)$ and $f\\geq 1$, produce succinct yet informative labels for the elements of the graph. Given only the labels of two vertices $u,v$ and of the elements in a faulty-set $F$ with $|F|\\leq f$, one can determine if $u,v$ are connected in $G-F$, the surviving graph after removing $F$. For the edge or vertex faults models, i.e., $F\\subseteq E$ or $F\\subseteq V$, a sequence of recent work established schemes with $poly(f,\\log n)$-bit labels. This paper considers the color faults model, recently introduced in the context of spanners [Petruschka, Sapir and Tzalik, ITCS'24], which accounts for known correlations between failures. Here, the edges (or vertices) of the input $G$ are arbitrarily colored, and the faulty elements in $F$ are colors; a failing color causes all edges (vertices) of that color to crash.   Our main contribution is settling the label length complexity for connectivity under one color fault ($f=1$). The existing implicit solution, by applying the state-of-the-art scheme for edge faults of [Dory and Parter, PODC'21], might yield labels of $\\Omega(n)$ bits. We provide a deterministic scheme with labels of $\\tilde{O}(\\sqrt{n})$ bits in the worst case, and a matching lower bound. Moreover, our scheme is universally optimal: even schemes tailored to handle only colorings of one specific graph topology cannot produce asymptotically smaller labels. We extend our labeling approach to yield a routing scheme avoiding a single forbidden color. We also consider the centralized setting, and show an $\\tilde{O}(n)$-space oracle, answering connectivity queries under one color fault in $\\tilde{O}(1)$ time. Turning to $f\\geq 2$ color faults, we give a randomized labeling scheme with $\\tilde{O}(n^{1-1/2^f})$-bit labels, along with a lower bound of $\\Omega(n^{1-1/(f+1)})$ bits.","sentences":["Fault-tolerant connectivity labelings are schemes that, given an $n$-vertex graph $G=(V,E)$ and $f\\geq 1$, produce succinct yet informative labels for the elements of the graph.","Given only the labels of two vertices $u,v$ and of the elements in a faulty-set $F$ with $|F|\\leq f$, one can determine if $u,v$ are connected in $G-F$, the surviving graph after removing $F$. For the edge or vertex faults models, i.e., $F\\subseteq E$ or $F\\subseteq V$, a sequence of recent work established schemes with $poly(f,\\log n)$-bit labels.","This paper considers the color faults model, recently introduced in the context of spanners [Petruschka, Sapir and Tzalik, ITCS'24], which accounts for known correlations between failures.","Here, the edges (or vertices) of the input $G$ are arbitrarily colored, and the faulty elements in $F$ are colors; a failing color causes all edges (vertices) of that color to crash.   ","Our main contribution is settling the label length complexity for connectivity under one color fault ($f=1$).","The existing implicit solution, by applying the state-of-the-art scheme for edge faults of [Dory and Parter, PODC'21], might yield labels of $\\Omega(n)$ bits.","We provide a deterministic scheme with labels of $\\tilde{O}(\\sqrt{n})$ bits in the worst case, and a matching lower bound.","Moreover, our scheme is universally optimal: even schemes tailored to handle only colorings of one specific graph topology cannot produce asymptotically smaller labels.","We extend our labeling approach to yield a routing scheme avoiding a single forbidden color.","We also consider the centralized setting, and show an $\\tilde{O}(n)$-space oracle, answering connectivity queries under one color fault in $\\tilde{O}(1)$ time.","Turning to $f\\geq 2$ color faults, we give a randomized labeling scheme with $\\tilde{O}(n^{1-1/2^f})$-bit labels, along with a lower bound of $\\Omega(n^{1-1/(f+1)})$ bits."],"url":"http://arxiv.org/abs/2402.12144v1","category":"cs.DS"}
{"created":"2024-02-19 13:52:40","title":"Joint mode switching and resource allocation in wireless-powered RIS-aided multiuser communication systems","abstract":"This paper investigates a wireless-powered hybrid reflecting intelligent surface (hybrid RIS)-assisted multiple access system, where the RIS can harvest energy from energy station (ES) transmitted radio frequency signal (RF), and each reflecting element can flexibly switch between active mode, passive mode, and idle mode. The objective is to minimize the maximum energy consumption of the users by jointly optimizing the operating modes of each reflecting element, the amplification factor of active elements, the transmit power, and transmission time allocation, subject to quality-of-service (QoS) of each user and the available energy constraint of RIS. In the formulated optimization problem, the operating modes of each reflecting element are highly coupled with the amplification coefficient of the active reflecting elements, making it a challenging mixed-integer programming problem. To solve this problem, a hierarchical optimization method based on deep reinforcement learning is proposed, where the operating modes of each reflecting element and the amplification coefficient of active elements are obtained by solving the outer sub-problem using proximal policy optimization (PPO), and the transmit power and transmission time allocation are obtained by solving the inner sub-problem using convex optimization methods. Simulation results show that compared to the baseline scheme, the proposed scheme can reduce user energy consumption by $70 \\%$.","sentences":["This paper investigates a wireless-powered hybrid reflecting intelligent surface (hybrid RIS)-assisted multiple access system, where the RIS can harvest energy from energy station (ES) transmitted radio frequency signal (RF), and each reflecting element can flexibly switch between active mode, passive mode, and idle mode.","The objective is to minimize the maximum energy consumption of the users by jointly optimizing the operating modes of each reflecting element, the amplification factor of active elements, the transmit power, and transmission time allocation, subject to quality-of-service (QoS) of each user and the available energy constraint of RIS.","In the formulated optimization problem, the operating modes of each reflecting element are highly coupled with the amplification coefficient of the active reflecting elements, making it a challenging mixed-integer programming problem.","To solve this problem, a hierarchical optimization method based on deep reinforcement learning is proposed, where the operating modes of each reflecting element and the amplification coefficient of active elements are obtained by solving the outer sub-problem using proximal policy optimization (PPO), and the transmit power and transmission time allocation are obtained by solving the inner sub-problem using convex optimization methods.","Simulation results show that compared to the baseline scheme, the proposed scheme can reduce user energy consumption by $70 \\%$."],"url":"http://arxiv.org/abs/2402.12143v1","category":"eess.SP"}
{"created":"2024-02-19 13:46:49","title":"Many-Stage Optimal Stabilized Runge-Kutta Methods for Hyperbolic Partial Differential Equations","abstract":"A novel optimization procedure for the generation of stability polynomials of stabilized explicit Runge-Kutta method is devised. Intended for semidiscretizations of hyperbolic partial differential equations, the herein developed approach allows the optimization of stability polynomials with more than hundred stages. A potential application of these high degree stability polynomials are problems with locally varying characteristic speeds as found in non-uniformly refined meshes and different wave speeds.   To demonstrate the applicability of the stability polynomials we construct 2N storage many-stage Runge-Kutta methods that match their designed second order of accuracy when applied to a range of linear and nonlinear hyperbolic PDEs with smooth solutions. The methods are constructed to reduce the amplification of round off errors which becomes a significant concern for these many-stage methods.","sentences":["A novel optimization procedure for the generation of stability polynomials of stabilized explicit Runge-Kutta method is devised.","Intended for semidiscretizations of hyperbolic partial differential equations, the herein developed approach allows the optimization of stability polynomials with more than hundred stages.","A potential application of these high degree stability polynomials are problems with locally varying characteristic speeds as found in non-uniformly refined meshes and different wave speeds.   ","To demonstrate the applicability of the stability polynomials we construct 2N storage many-stage Runge-Kutta methods that match their designed second order of accuracy when applied to a range of linear and nonlinear hyperbolic PDEs with smooth solutions.","The methods are constructed to reduce the amplification of round off errors which becomes a significant concern for these many-stage methods."],"url":"http://arxiv.org/abs/2402.12140v2","category":"math.NA"}
{"created":"2024-02-19 13:32:30","title":"Molecule Generation and Optimization for Efficient Fragrance Creation","abstract":"This research introduces a Machine Learning-centric approach to replicate olfactory experiences, validated through experimental quantification of perfume perception. Key contributions encompass a hybrid model connecting perfume molecular structure to human olfactory perception. This model includes an AI-driven molecule generator (utilizing Graph and Generative Neural Networks), quantification and prediction of odor intensity, and refinery of optimal solvent and molecule combinations for desired fragrances. Additionally, a thermodynamic-based model establishes a link between olfactory perception and liquid-phase concentrations. The methodology employs Transfer Learning and selects the most suitable molecules based on vapor pressure and fragrance notes. Ultimately, a mathematical optimization problem is formulated to minimize discrepancies between new and target olfactory experiences. The methodology is validated by reproducing two distinct olfactory experiences using available experimental data.","sentences":["This research introduces a Machine Learning-centric approach to replicate olfactory experiences, validated through experimental quantification of perfume perception.","Key contributions encompass a hybrid model connecting perfume molecular structure to human olfactory perception.","This model includes an AI-driven molecule generator (utilizing Graph and Generative Neural Networks), quantification and prediction of odor intensity, and refinery of optimal solvent and molecule combinations for desired fragrances.","Additionally, a thermodynamic-based model establishes a link between olfactory perception and liquid-phase concentrations.","The methodology employs Transfer Learning and selects the most suitable molecules based on vapor pressure and fragrance notes.","Ultimately, a mathematical optimization problem is formulated to minimize discrepancies between new and target olfactory experiences.","The methodology is validated by reproducing two distinct olfactory experiences using available experimental data."],"url":"http://arxiv.org/abs/2402.12134v1","category":"physics.chem-ph"}
{"created":"2024-02-19 13:26:38","title":"Modified RRT* for Path Planning in Autonomous Driving","abstract":"Essential tasks in autonomous driving includes environment perception, detection and tracking, path planning and action control. This paper focus on path planning, which is one of the challenging task as it needs to find optimal path in highly complex and dynamic environments. Usually, a driving scenario has large number of obstacles in their route. In this paper, we propose a two-stage path planning algorithm named Angle-based Directed Rapidly exploring Random Trees (AD-RRT*) to address the problem of optimal path in complex environment. The proposed algorithm uses A* algorithm for global path planning and modifies RRT* to bound the samples using angle. The efficiency of the proposed algorithm is evaluated through experiments in different scenarios based on the location and number of obstacles. The proposed algorithm showed higher rate of convergence with reduced time and less number of nodes than the base RRT* algorithm.","sentences":["Essential tasks in autonomous driving includes environment perception, detection and tracking, path planning and action control.","This paper focus on path planning, which is one of the challenging task as it needs to find optimal path in highly complex and dynamic environments.","Usually, a driving scenario has large number of obstacles in their route.","In this paper, we propose a two-stage path planning algorithm named Angle-based Directed Rapidly exploring Random Trees (AD-RRT*) to address the problem of optimal path in complex environment.","The proposed algorithm uses A* algorithm for global path planning and modifies RRT* to bound the samples using angle.","The efficiency of the proposed algorithm is evaluated through experiments in different scenarios based on the location and number of obstacles.","The proposed algorithm showed higher rate of convergence with reduced time and less number of nodes than the base RRT* algorithm."],"url":"http://arxiv.org/abs/2402.12129v1","category":"cs.RO"}
{"created":"2024-02-19 13:23:17","title":"Rate-Splitting Multiple Access for Transmissive Reconfigurable Intelligent Surface Transceiver Empowered ISAC System","abstract":"In this paper, a novel transmissive reconfigurable intelligent surface (TRIS) transceiver empowered integrated sensing and communications (ISAC) system is proposed for future multi-demand terminals. To address interference management, we implement rate-splitting multiple access (RSMA), where the common stream is independently designed for the sensing service. We introduce the sensing quality of service (QoS) criteria based on this structure and construct an optimization problem with the sensing QoS criteria as the objective function to optimize the sensing stream precoding matrix and the communication stream precoding matrix. Due to the coupling of optimization variables, the formulated problem is a non-convex optimization problem that cannot be solved directly. To tackle the above-mentioned challenging problem, alternating optimization (AO) is utilized to decouple the optimization variables. Specifically, the problem is decoupled into three subproblems about the sensing stream precoding matrix, the communication stream precoding matrix, and the auxiliary variables, which is solved alternatively through AO until the convergence is reached. For solving the problem, successive convex approximation (SCA) is applied to deal with the sum-rate threshold constraints on communications, and difference-of-convex (DC) programming is utilized to solve rank-one non-convex constraints. Numerical simulation results verify the superiority of the proposed scheme in terms of improving the communication and sensing QoS.","sentences":["In this paper, a novel transmissive reconfigurable intelligent surface (TRIS) transceiver empowered integrated sensing and communications (ISAC) system is proposed for future multi-demand terminals.","To address interference management, we implement rate-splitting multiple access (RSMA), where the common stream is independently designed for the sensing service.","We introduce the sensing quality of service (QoS) criteria based on this structure and construct an optimization problem with the sensing QoS criteria as the objective function to optimize the sensing stream precoding matrix and the communication stream precoding matrix.","Due to the coupling of optimization variables, the formulated problem is a non-convex optimization problem that cannot be solved directly.","To tackle the above-mentioned challenging problem, alternating optimization (AO) is utilized to decouple the optimization variables.","Specifically, the problem is decoupled into three subproblems about the sensing stream precoding matrix, the communication stream precoding matrix, and the auxiliary variables, which is solved alternatively through AO until the convergence is reached.","For solving the problem, successive convex approximation (SCA) is applied to deal with the sum-rate threshold constraints on communications, and difference-of-convex (DC) programming is utilized to solve rank-one non-convex constraints.","Numerical simulation results verify the superiority of the proposed scheme in terms of improving the communication and sensing QoS."],"url":"http://arxiv.org/abs/2402.12127v1","category":"cs.IT"}
{"created":"2024-02-19 13:08:31","title":"A Spatiotemporal Illumination Model for 3D Image Fusion in Optical Coherence Tomography","abstract":"Optical coherence tomography (OCT) is a non-invasive, micrometer-scale imaging modality that has become a clinical standard in ophthalmology. By raster-scanning the retina, sequential cross-sectional image slices are acquired to generate volumetric data. In-vivo imaging suffers from discontinuities between slices that show up as motion and illumination artifacts. We present a new illumination model that exploits continuity in orthogonally raster-scanned volume data. Our novel spatiotemporal parametrization adheres to illumination continuity both temporally, along the imaged slices, as well as spatially, in the transverse directions. Yet, our formulation does not make inter-slice assumptions, which could have discontinuities. This is the first optimization of a 3D inverse model in an image reconstruction context in OCT. Evaluation in 68 volumes from eyes with pathology showed reduction of illumination artifacts in 88\\% of the data, and only 6\\% showed moderate residual illumination artifacts. The method enables the use of forward-warped motion corrected data, which is more accurate, and enables supersampling and advanced 3D image reconstruction in OCT.","sentences":["Optical coherence tomography (OCT) is a non-invasive, micrometer-scale imaging modality that has become a clinical standard in ophthalmology.","By raster-scanning the retina, sequential cross-sectional image slices are acquired to generate volumetric data.","In-vivo imaging suffers from discontinuities between slices that show up as motion and illumination artifacts.","We present a new illumination model that exploits continuity in orthogonally raster-scanned volume data.","Our novel spatiotemporal parametrization adheres to illumination continuity both temporally, along the imaged slices, as well as spatially, in the transverse directions.","Yet, our formulation does not make inter-slice assumptions, which could have discontinuities.","This is the first optimization of a 3D inverse model in an image reconstruction context in OCT.","Evaluation in 68 volumes from eyes with pathology showed reduction of illumination artifacts in 88\\% of the data, and only 6\\% showed moderate residual illumination artifacts.","The method enables the use of forward-warped motion corrected data, which is more accurate, and enables supersampling and advanced 3D image reconstruction in OCT."],"url":"http://arxiv.org/abs/2402.12114v1","category":"eess.IV"}
{"created":"2024-02-19 13:07:50","title":"Topological Phase Diagram of Optimally Shaken Honeycomb Lattices: A Dual Perspective from Stroboscopic and Non-Stroboscopic Floquet Hamiltonians","abstract":"We present a direct comparison between the stroboscopic and non-stroboscopic effective approaches for ultracold atoms in shaken honeycomb lattices, focusing specifically on the optimal driving introduced by A. Verdeny and F. Mintert [Phys. Rev. A 92, 063615 (2015)]. In the fast-driving regime, we compare the effective non-stroboscopic Hamiltonian derived through a perturbative expansion with a non-perturbative calculation of the stroboscopic Floquet Hamiltonian, obtained through a simple non-perturbative numerical approach. We show that while some of the tunneling parameters are inherently model-dependent, the topological properties of the system remains robust, as expected. Using the same numerical approach we compute the topological phase diagram, arguing that it is most effectively represented in terms of the physical parameters characterizing the driving and the bare Hamiltonian -- parameters directly accessible in experiments -- rather than the emergent tunneling parameters, that depend on the model representation.","sentences":["We present a direct comparison between the stroboscopic and non-stroboscopic effective approaches for ultracold atoms in shaken honeycomb lattices, focusing specifically on the optimal driving introduced by A. Verdeny and F. Mintert [Phys.","Rev. A 92, 063615 (2015)].","In the fast-driving regime, we compare the effective non-stroboscopic Hamiltonian derived through a perturbative expansion with a non-perturbative calculation of the stroboscopic Floquet Hamiltonian, obtained through a simple non-perturbative numerical approach.","We show that while some of the tunneling parameters are inherently model-dependent, the topological properties of the system remains robust, as expected.","Using the same numerical approach we compute the topological phase diagram, arguing that it is most effectively represented in terms of the physical parameters characterizing the driving and the bare Hamiltonian -- parameters directly accessible in experiments -- rather than the emergent tunneling parameters, that depend on the model representation."],"url":"http://arxiv.org/abs/2402.12113v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-19 12:58:53","title":"Persistent Homology-Driven Optimization of Effective Relative Density Range for Triply Periodic Minimal Surface","abstract":"Triply periodic minimal surfaces (TPMSs) play a vital role in the design of porous structures, with applications in bone tissue engineering, chemical engineering, and the creation of lightweight models. However, fabrication of TPMSs via additive manufacturing is feasible only within a specific range of relative densities, termed the effective relative density range (EDR), outside of which TPMSs exhibit unmanufacturable features. In this study, the persistent homology is applied to theoretically calculate and extend the EDRs of TPMSs. The TPMSs with extended EDRs are referred to as extended TPMSs. To achieve this, TPMSs are converted into implicit B-spline representation through fitting. By analyzing the symmetry of TPMSs, a partial fitting method is utilized to preserve the symmetry and enhance fitting precision. A topological objective function is modeled based on the understanding of topological features, resulting in extended TPMSs that possess extended EDRs while maintaining a high degree of similarity to the original TPMSs. Experimental validation confirms the effectiveness of the approach in extending the EDRs of TPMSs. Furthermore, the extended TPMSs demonstrate superior performance in porous model design and topology optimization compared to their original counterparts. The extended TPMSs with increased EDRs hold promise for replacing traditional TPMSs in applications that require porous structures with varying densities.","sentences":["Triply periodic minimal surfaces (TPMSs) play a vital role in the design of porous structures, with applications in bone tissue engineering, chemical engineering, and the creation of lightweight models.","However, fabrication of TPMSs via additive manufacturing is feasible only within a specific range of relative densities, termed the effective relative density range (EDR), outside of which TPMSs exhibit unmanufacturable features.","In this study, the persistent homology is applied to theoretically calculate and extend the EDRs of TPMSs.","The TPMSs with extended EDRs are referred to as extended TPMSs.","To achieve this, TPMSs are converted into implicit B-spline representation through fitting.","By analyzing the symmetry of TPMSs, a partial fitting method is utilized to preserve the symmetry and enhance fitting precision.","A topological objective function is modeled based on the understanding of topological features, resulting in extended TPMSs that possess extended EDRs while maintaining a high degree of similarity to the original TPMSs.","Experimental validation confirms the effectiveness of the approach in extending the EDRs of TPMSs.","Furthermore, the extended TPMSs demonstrate superior performance in porous model design and topology optimization compared to their original counterparts.","The extended TPMSs with increased EDRs hold promise for replacing traditional TPMSs in applications that require porous structures with varying densities."],"url":"http://arxiv.org/abs/2402.12109v1","category":"cs.GR"}
{"created":"2024-02-19 12:53:52","title":"A joint optimization approach of parameterized quantum circuits with a tensor network","abstract":"Despite the advantage quantum computers are expected to deliver when performing simulations compared to their classical counterparts, the current noisy intermediate-scale quantum (NISQ) devices remain limited in their capabilities. The training of parameterized quantum circuits (PQCs) remains a significant practical challenge, exacerbated by the requirement of shallow circuit depth necessary for their hardware implementation. Hybrid methods employing classical computers alongside quantum devices, such as the Variational Quantum Eigensolver (VQE), have proven useful for analyzing the capabilities of NISQ devices to solve relevant optimization problems. Still, in the simulation of complex structures involving the many-body problem in quantum mechanics, major issues remain about the representation of the system and obtaining results which clearly outperform classical computational devices. In this research contribution we propose the use of parameterized Tensor Networks (TNs) to attempt an improved performance of the VQE algorithm. A joint approach is presented where the Hamiltonian of a system is encapsulated into a Matrix Product Operator (MPO) within a parameterized unitary TN hereby splitting up the optimization task between the TN and the VQE. We show that the hybrid TN-VQE implementation improves the convergence of the algorithm in comparison to optimizing randomly-initialized quantum circuits via VQE.","sentences":["Despite the advantage quantum computers are expected to deliver when performing simulations compared to their classical counterparts, the current noisy intermediate-scale quantum (NISQ) devices remain limited in their capabilities.","The training of parameterized quantum circuits (PQCs) remains a significant practical challenge, exacerbated by the requirement of shallow circuit depth necessary for their hardware implementation.","Hybrid methods employing classical computers alongside quantum devices, such as the Variational Quantum Eigensolver (VQE), have proven useful for analyzing the capabilities of NISQ devices to solve relevant optimization problems.","Still, in the simulation of complex structures involving the many-body problem in quantum mechanics, major issues remain about the representation of the system and obtaining results which clearly outperform classical computational devices.","In this research contribution we propose the use of parameterized Tensor Networks (TNs) to attempt an improved performance of the VQE algorithm.","A joint approach is presented where the Hamiltonian of a system is encapsulated into a Matrix Product Operator (MPO) within a parameterized unitary TN hereby splitting up the optimization task between the TN and the VQE.","We show that the hybrid TN-VQE implementation improves the convergence of the algorithm in comparison to optimizing randomly-initialized quantum circuits via VQE."],"url":"http://arxiv.org/abs/2402.12105v1","category":"quant-ph"}
{"created":"2024-02-19 12:13:33","title":"Parameter Refinement of a Ballbot and Predictive Control for Reference Tracking with Linear Parameter-Varying Embedding","abstract":"In this study, we implement a control method for stabilizing a ballbot that simultaneously follows a reference. A ballbot is a robot balancing on a spherical wheel where the single point of contact with the ground makes it omnidirectional and highly maneuverable but with inherent instability. After introducing the scheduling parameters, we start the analysis by embedding the nonlinear dynamic model derived from first principles to a linear parameter-varying (LPV) formulation. Continuously, and as an extension of a past study, we refine the parameters of the nonlinear model that enhance significantly its accuracy. The crucial advantages of the LPV formulation are that it consists of a nonlinear predictor that can be used in model predictive control (MPC) by retaining the convexity of the quadratic optimization problem with linear constraints and further evades computational burdens that appear in other nonlinear MPC methods with only a slight loss in performance. The LPVMPC control method can be solved efficiently as a quadratic program (QP) that provides timing that supports real-time implementation. Finally, to illustrate the method, we test the control designs on a two-set point 1D non-smooth reference with sudden changes, to a 2D nonstationary smooth reference known as Lissajous curves, and to a single-set point 1D non-smooth reference where for this case theoretical guarantees such as stability and recursive feasibility are provided.","sentences":["In this study, we implement a control method for stabilizing a ballbot that simultaneously follows a reference.","A ballbot is a robot balancing on a spherical wheel where the single point of contact with the ground makes it omnidirectional and highly maneuverable but with inherent instability.","After introducing the scheduling parameters, we start the analysis by embedding the nonlinear dynamic model derived from first principles to a linear parameter-varying (LPV) formulation.","Continuously, and as an extension of a past study, we refine the parameters of the nonlinear model that enhance significantly its accuracy.","The crucial advantages of the LPV formulation are that it consists of a nonlinear predictor that can be used in model predictive control (MPC) by retaining the convexity of the quadratic optimization problem with linear constraints and further evades computational burdens that appear in other nonlinear MPC methods with only a slight loss in performance.","The LPVMPC control method can be solved efficiently as a quadratic program (QP) that provides timing that supports real-time implementation.","Finally, to illustrate the method, we test the control designs on a two-set point 1D non-smooth reference with sudden changes, to a 2D nonstationary smooth reference known as Lissajous curves, and to a single-set point 1D non-smooth reference where for this case theoretical guarantees such as stability and recursive feasibility are provided."],"url":"http://arxiv.org/abs/2402.12092v1","category":"math.OC"}
{"created":"2024-02-19 12:12:32","title":"Characterization of optimization problems that are solvable iteratively with linear convergence","abstract":"In this work, we state a general conjecture on the solvability of optimization problems via algorithms with linear convergence guarantees. We make a first step towards examining its correctness by fully characterizing the problems that are solvable via Riemannian gradient descent with linear convergence.","sentences":["In this work, we state a general conjecture on the solvability of optimization problems via algorithms with linear convergence guarantees.","We make a first step towards examining its correctness by fully characterizing the problems that are solvable via Riemannian gradient descent with linear convergence."],"url":"http://arxiv.org/abs/2402.12090v1","category":"math.OC"}
{"created":"2024-02-19 11:54:47","title":"Periodic Implicit Representation, Design and Optimization of Porous Structures Using Periodic B-splines","abstract":"Porous structures are intricate solid materials with numerous small pores, extensively used in fields like medicine, chemical engineering, and aerospace. However, the design of such structures using computer-aided tools is a time-consuming and tedious process.In this study, we propose a novel representation method and design approach for porous units that can be infinitely spliced to form a porous structure. We use periodic B-spline functions to represent periodic or symmetric porous units. Starting from a voxel representation of a porous sample, the discrete distance field is computed. To fit the discrete distance field with a periodic B-spline, we introduce the constrained least squares progressive-iterative approximation algorithm, which results in an implicit porous unit. This unit can be subject to optimization to enhance connectivity and utilized for topology optimization, thereby improving the model's stiffness while maintaining periodicity or symmetry. The experimental results demonstrate the potential of the designed complex porous units in enhancing the mechanical performance of the model. Consequently, this study has the potential to incorporate remarkable structures derived from artificial design or nature into the design of high-performing models, showing the promise for biomimetic applications.","sentences":["Porous structures are intricate solid materials with numerous small pores, extensively used in fields like medicine, chemical engineering, and aerospace.","However, the design of such structures using computer-aided tools is a time-consuming and tedious process.","In this study, we propose a novel representation method and design approach for porous units that can be infinitely spliced to form a porous structure.","We use periodic B-spline functions to represent periodic or symmetric porous units.","Starting from a voxel representation of a porous sample, the discrete distance field is computed.","To fit the discrete distance field with a periodic B-spline, we introduce the constrained least squares progressive-iterative approximation algorithm, which results in an implicit porous unit.","This unit can be subject to optimization to enhance connectivity and utilized for topology optimization, thereby improving the model's stiffness while maintaining periodicity or symmetry.","The experimental results demonstrate the potential of the designed complex porous units in enhancing the mechanical performance of the model.","Consequently, this study has the potential to incorporate remarkable structures derived from artificial design or nature into the design of high-performing models, showing the promise for biomimetic applications."],"url":"http://arxiv.org/abs/2402.12076v1","category":"cs.GR"}
{"created":"2024-02-19 11:53:42","title":"Order Estimation of Linear-Phase FIR Filters for DAC Equalization in Multiple Nyquist Bands","abstract":"This letter considers the design of linear-phase finite-length impulse response (FIR) filters for equalization of the frequency responses of digital-to-analog converters (DACs). The letter derives estimates for the filter orders required, as functions of the bandwidth and equalization accuracy, for four DAC pulses that are used in DACs in multiple Nyquist bands. The estimates are derived through a large set of minimax-optimal equalizers and the use of symbolic regression followed by minimax-optimal curve fitting for further enhancement. Design examples included demonstrate the accuracy of the proposed estimates. In addition, the letter discusses the appropriateness of the four types of linear-phase FIR filters, for the different equalizer cases, as well as the corresponding properties of the equalized systems.","sentences":["This letter considers the design of linear-phase finite-length impulse response (FIR) filters for equalization of the frequency responses of digital-to-analog converters (DACs).","The letter derives estimates for the filter orders required, as functions of the bandwidth and equalization accuracy, for four DAC pulses that are used in DACs in multiple Nyquist bands.","The estimates are derived through a large set of minimax-optimal equalizers and the use of symbolic regression followed by minimax-optimal curve fitting for further enhancement.","Design examples included demonstrate the accuracy of the proposed estimates.","In addition, the letter discusses the appropriateness of the four types of linear-phase FIR filters, for the different equalizer cases, as well as the corresponding properties of the equalized systems."],"url":"http://arxiv.org/abs/2402.12075v1","category":"eess.SP"}
{"created":"2024-02-19 11:42:26","title":"Inexact Restoration via random models for unconstrained noisy optimization","abstract":"We study the Inexact Restoration framework with random models for minimizing functions whose evaluation is subject to errors. We propose a constrained formulation that includes well-known stochastic problems and an algorithm applicable when the evaluation of both the function and its gradient is random and a specified accuracy of such evaluations is guaranteed with sufficiently high probability. The proposed algorithm combines the Inexact Restoration framework with a trust-region methodology based on random first-order models. We analyse the properties of the algorithm and provide the expected number of iterations performed to reach an approximate first-order optimality point. Numerical experiments show that the proposed algorithm compares well with a state-of-the-art competitor.","sentences":["We study the Inexact Restoration framework with random models for minimizing functions whose evaluation is subject to errors.","We propose a constrained formulation that includes well-known stochastic problems and an algorithm applicable when the evaluation of both the function and its gradient is random and a specified accuracy of such evaluations is guaranteed with sufficiently high probability.","The proposed algorithm combines the Inexact Restoration framework with a trust-region methodology based on random first-order models.","We analyse the properties of the algorithm and provide the expected number of iterations performed to reach an approximate first-order optimality point.","Numerical experiments show that the proposed algorithm compares well with a state-of-the-art competitor."],"url":"http://arxiv.org/abs/2402.12069v1","category":"math.OC"}
{"created":"2024-02-19 11:33:38","title":"Max-Min Fairness for Uplink Rate-Splitting Multiple Access with Finite Blocklength","abstract":"In this letter, we investigate the performance of Max Minimum Fairness (MMF) for uplink Rate-Splitting Multiple Access (RSMA) in short-packet communications. Specifically, considering a Single-Input Single-Output (SISO) Multiple Access Channel (MAC), we optimize the transmit power allocation between the splitting user messages to maximize the minimum rate among users with Finite Blocklength (FBL) constraints. To tackle this problem, we propose a Successive Convex Approximation (SCA)-based approach. Additionally, we introduce a low-complexity scheme to design the decoding order at the receiver. Numerical results show that RSMA outperforms conventional transmission schemes such as Non-orthogonal Multiple Access (NOMA) in terms of MMF.","sentences":["In this letter, we investigate the performance of Max Minimum Fairness (MMF) for uplink Rate-Splitting Multiple Access (RSMA) in short-packet communications.","Specifically, considering a Single-Input Single-Output (SISO) Multiple Access Channel (MAC), we optimize the transmit power allocation between the splitting user messages to maximize the minimum rate among users with Finite Blocklength (FBL) constraints.","To tackle this problem, we propose a Successive Convex Approximation (SCA)-based approach.","Additionally, we introduce a low-complexity scheme to design the decoding order at the receiver.","Numerical results show that RSMA outperforms conventional transmission schemes such as Non-orthogonal Multiple Access (NOMA) in terms of MMF."],"url":"http://arxiv.org/abs/2402.12066v1","category":"cs.IT"}
{"created":"2024-02-19 11:33:21","title":"WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More","abstract":"Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for parameter optimization. Experiments show that WKVQuant achieves almost comparable memory savings to weight-activation quantization, while also approaching the performance of weight-only quantization.","sentences":["Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process.","This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers.","We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs.","To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs.","Specifically, we incorporates past-only quantization to improve the computation of attention.","Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for parameter optimization.","Experiments show that WKVQuant achieves almost comparable memory savings to weight-activation quantization, while also approaching the performance of weight-only quantization."],"url":"http://arxiv.org/abs/2402.12065v2","category":"cs.LG"}
{"created":"2024-02-19 11:28:20","title":"All Language Models Large and Small","abstract":"Many leading language models (LMs) use high-intensity computational resources both during training and execution. This poses the challenge of lowering resource costs for deployment and faster execution of decision-making tasks among others. We introduce a novel plug-and-play LM framework named Language Optimising Network Distribution (LONDI) framework. LONDI learns to selectively employ large LMs only where complex decision-making and reasoning are required while using low-resource LMs everywhere else. LONDI consists of a system of two (off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning module that uses switching controls to quickly learn which system states to call the LLM. We then introduce a variant of LONDI that maintains budget constraints on LLM calls and hence its resource usage. Theoretically, we prove LONDI learns the subset of system states to activate the LLM required to solve the task. We then prove that LONDI converges to optimal solutions while also preserving budgetary constraints on LLM calls almost surely enabling it to solve various tasks while significantly lowering computational costs. We test LONDI's performance in a range of tasks in ScienceWorld and BabyAI-Text and demonstrate that LONDI can solve tasks only solvable by resource-intensive LLMs while reducing GPU usage by up to 30%.","sentences":["Many leading language models (LMs) use high-intensity computational resources both during training and execution.","This poses the challenge of lowering resource costs for deployment and faster execution of decision-making tasks among others.","We introduce a novel plug-and-play LM framework named Language Optimising Network Distribution (LONDI) framework.","LONDI learns to selectively employ large LMs only where complex decision-making and reasoning are required while using low-resource LMs everywhere else.","LONDI consists of a system of two (off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning module that uses switching controls to quickly learn which system states to call the LLM.","We then introduce a variant of LONDI that maintains budget constraints on LLM calls and hence its resource usage.","Theoretically, we prove LONDI learns the subset of system states to activate the LLM required to solve the task.","We then prove that LONDI converges to optimal solutions while also preserving budgetary constraints on LLM calls almost surely enabling it to solve various tasks while significantly lowering computational costs.","We test LONDI's performance in a range of tasks in ScienceWorld and BabyAI-Text and demonstrate that LONDI can solve tasks only solvable by resource-intensive LLMs while reducing GPU usage by up to 30%."],"url":"http://arxiv.org/abs/2402.12061v1","category":"cs.LG"}
{"created":"2024-02-19 11:06:36","title":"Reinforcement Learning for Optimal Execution when Liquidity is Time-Varying","abstract":"Optimal execution is an important problem faced by any trader. Most solutions are based on the assumption of constant market impact, while liquidity is known to be dynamic. Moreover, models with time-varying liquidity typically assume that it is observable, despite the fact that, in reality, it is latent and hard to measure in real time. In this paper we show that the use of Double Deep Q-learning, a form of Reinforcement Learning based on neural networks, is able to learn optimal trading policies when liquidity is time-varying. Specifically, we consider an Almgren-Chriss framework with temporary and permanent impact parameters following several deterministic and stochastic dynamics. Using extensive numerical experiments, we show that the trained algorithm learns the optimal policy when the analytical solution is available, and overcomes benchmarks and approximated solutions when the solution is not available.","sentences":["Optimal execution is an important problem faced by any trader.","Most solutions are based on the assumption of constant market impact, while liquidity is known to be dynamic.","Moreover, models with time-varying liquidity typically assume that it is observable, despite the fact that, in reality, it is latent and hard to measure in real time.","In this paper we show that the use of Double Deep Q-learning, a form of Reinforcement Learning based on neural networks, is able to learn optimal trading policies when liquidity is time-varying.","Specifically, we consider an Almgren-Chriss framework with temporary and permanent impact parameters following several deterministic and stochastic dynamics.","Using extensive numerical experiments, we show that the trained algorithm learns the optimal policy when the analytical solution is available, and overcomes benchmarks and approximated solutions when the solution is not available."],"url":"http://arxiv.org/abs/2402.12049v2","category":"q-fin.TR"}
{"created":"2024-02-19 10:39:14","title":"Flexible Robust Optimal Bidding of Renewable Virtual Power Plants in Sequential Markets","abstract":"In this paper, a novel approach to define the optimal bidding of renewable-only virtual power plants (RVPPs) in the day-ahead, secondary reserve, and intra-day markets is proposed. To this aim, a robust optimization algorithm is developed to account for the asymmetric nature of the uncertainties that characterize the market prices, as well as the energy production of the RVPP stochastic sources and flexible demand consumption. Simulation results show increased RVPP benefits compared to other existing solutions and demonstrate the potential of renewable sources to further increase their economic competitiveness. The simplicity of the implementation, the computational efficiency, and the flexible robustness are also verified.","sentences":["In this paper, a novel approach to define the optimal bidding of renewable-only virtual power plants (RVPPs) in the day-ahead, secondary reserve, and intra-day markets is proposed.","To this aim, a robust optimization algorithm is developed to account for the asymmetric nature of the uncertainties that characterize the market prices, as well as the energy production of the RVPP stochastic sources and flexible demand consumption.","Simulation results show increased RVPP benefits compared to other existing solutions and demonstrate the potential of renewable sources to further increase their economic competitiveness.","The simplicity of the implementation, the computational efficiency, and the flexible robustness are also verified."],"url":"http://arxiv.org/abs/2402.12032v1","category":"eess.SY"}
{"created":"2024-02-19 10:37:29","title":"Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs","abstract":"Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility. Knowledge distillation (KD) offers a solution by compressing knowledge from resource-intensive large models to smaller ones. Various strategies exist, some relying on the text generated by the teacher model and optionally utilizing his logits to enhance learning. However, these methods based on logits often require both teacher and student models to share the same tokenizer, limiting their applicability across different LLM families. In this paper, we introduce Universal Logit Distillation (ULD) loss, grounded in optimal transport, to address this limitation. Our experimental results demonstrate the effectiveness of ULD loss in enabling distillation across models with different architectures and tokenizers, paving the way to a more widespread use of distillation techniques.","sentences":["Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility.","Knowledge distillation (KD) offers a solution by compressing knowledge from resource-intensive large models to smaller ones.","Various strategies exist, some relying on the text generated by the teacher model and optionally utilizing his logits to enhance learning.","However, these methods based on logits often require both teacher and student models to share the same tokenizer, limiting their applicability across different LLM families.","In this paper, we introduce Universal Logit Distillation (ULD) loss, grounded in optimal transport, to address this limitation.","Our experimental results demonstrate the effectiveness of ULD loss in enabling distillation across models with different architectures and tokenizers, paving the way to a more widespread use of distillation techniques."],"url":"http://arxiv.org/abs/2402.12030v2","category":"cs.CL"}
{"created":"2024-02-19 10:30:10","title":"Projected Block Coordinate Descent for sparse spike estimation","abstract":"We consider the problem of recovering off-the-grid spikes from linear measurements. The state of the art Over-Parametrized Continuous Orthogonal Matching Pursuit (OP-COMP) with Projected Gradient Descent (PGD) successfully recovers those signals. In most cases, the main computational cost lies in a unique global descent on all parameters (positions and amplitudes). In this paper, we propose to improve this algorithm by accelerating this descent step. We introduce a new algorithm, based on Block Coordinate Descent, that takes advantages of the sparse structure of the problem. Based on qualitative theoretical results, this algorithm shows improvement in calculation times in realistic synthetic microscopy experiments.","sentences":["We consider the problem of recovering off-the-grid spikes from linear measurements.","The state of the art Over-Parametrized Continuous Orthogonal Matching Pursuit (OP-COMP) with Projected Gradient Descent (PGD) successfully recovers those signals.","In most cases, the main computational cost lies in a unique global descent on all parameters (positions and amplitudes).","In this paper, we propose to improve this algorithm by accelerating this descent step.","We introduce a new algorithm, based on Block Coordinate Descent, that takes advantages of the sparse structure of the problem.","Based on qualitative theoretical results, this algorithm shows improvement in calculation times in realistic synthetic microscopy experiments."],"url":"http://arxiv.org/abs/2402.12021v1","category":"math.NA"}
{"created":"2024-02-19 10:28:34","title":"Collision-Free Robot Scheduling","abstract":"Robots are becoming an increasingly common part of scientific work within laboratory environments. In this paper, we investigate the problem of designing \\emph{schedules} for completing a set of tasks at fixed locations with multiple robots in a laboratory. We represent the laboratory as a graph with tasks placed on fixed vertices and robots represented as agents, with the constraint that no two robots may occupy the same vertex at any given timestep. Each schedule is partitioned into a set of timesteps, corresponding to a walk through the graph (allowing for a robot to wait at a vertex to complete a task), with each timestep taking time equal to the time for a robot to move from one vertex to another and each task taking some given number of timesteps during the completion of which a robot must stay at the vertex containing the task. The goal is to determine a set of schedules, with one schedule for each robot, minimising the number of timesteps taken by the schedule taking the greatest number of timesteps within the set of schedules.   We show that this problem is NP-complete for many simple classes of graphs, the problem of determining the fastest schedule, defined by the number of time steps required for a robot to visit every vertex in the schedule and complete every task assigned in its assigned schedule. Explicitly, we provide this result for complete graphs, bipartite graphs, star graphs, and planar graphs. Finally, we provide positive results for line graphs, showing that we can find an optimal set of schedules for $k$ robots completing $m$ tasks of equal length of a path of length $n$ in $O(kmn)$ time, and a $k$-approximation when the length of the tasks is unbounded.","sentences":["Robots are becoming an increasingly common part of scientific work within laboratory environments.","In this paper, we investigate the problem of designing \\emph{schedules} for completing a set of tasks at fixed locations with multiple robots in a laboratory.","We represent the laboratory as a graph with tasks placed on fixed vertices and robots represented as agents, with the constraint that no two robots may occupy the same vertex at any given timestep.","Each schedule is partitioned into a set of timesteps, corresponding to a walk through the graph (allowing for a robot to wait at a vertex to complete a task), with each timestep taking time equal to the time for a robot to move from one vertex to another and each task taking some given number of timesteps during the completion of which a robot must stay at the vertex containing the task.","The goal is to determine a set of schedules, with one schedule for each robot, minimising the number of timesteps taken by the schedule taking the greatest number of timesteps within the set of schedules.   ","We show that this problem is NP-complete for many simple classes of graphs, the problem of determining the fastest schedule, defined by the number of time steps required for a robot to visit every vertex in the schedule and complete every task assigned in its assigned schedule.","Explicitly, we provide this result for complete graphs, bipartite graphs, star graphs, and planar graphs.","Finally, we provide positive results for line graphs, showing that we can find an optimal set of schedules for $k$ robots completing $m$ tasks of equal length of a path of length $n$ in $O(kmn)$ time, and a $k$-approximation when the length of the tasks is unbounded."],"url":"http://arxiv.org/abs/2402.12019v1","category":"cs.DS"}
{"created":"2024-02-19 10:20:21","title":"Private Interdependent Valuations: New Bounds for Single-Item Auctions and Matroids","abstract":"We study auction design within the widely acclaimed model of interdependent values, introduced by Milgrom and Weber [1982]. In this model, every bidder $i$ has a private signal $s_i$ for the item for sale, and a public valuation function $v_i(s_1,\\ldots,s_n)$ which maps every vector of private signals (of all bidders) into a real value. A recent line of work established the existence of approximately-optimal mechanisms within this framework, even in the more challenging scenario where each bidder's valuation function $v_i$ is also private. This body of work has primarily focused on single-item auctions with two natural classes of valuations: those exhibiting submodularity over signals (SOS) and $d$-critical valuations.   In this work we advance the state of the art on interdependent values with private valuation functions, with respect to both SOS and $d$-critical valuations. For SOS valuations, we devise a new mechanism that gives an improved approximation bound of $5$ for single-item auctions. This mechanism employs a novel variant of an \"eating mechanism\", leveraging LP-duality to achieve feasibility with reduced welfare loss. For $d$-critical valuations, we broaden the scope of existing results beyond single-item auctions, introducing a mechanism that gives a $(d+1)$-approximation for any environment with matroid feasibility constraints on the set of agents that can be simultaneously served. Notably, this approximation bound is tight, even with respect to single-item auctions.","sentences":["We study auction design within the widely acclaimed model of interdependent values, introduced by Milgrom and Weber [1982].","In this model, every bidder $i$ has a private signal $s_i$ for the item for sale, and a public valuation function $v_i(s_1,\\ldots,s_n)$ which maps every vector of private signals (of all bidders) into a real value.","A recent line of work established the existence of approximately-optimal mechanisms within this framework, even in the more challenging scenario where each bidder's valuation function $v_i$ is also private.","This body of work has primarily focused on single-item auctions with two natural classes of valuations: those exhibiting submodularity over signals (SOS) and $d$-critical valuations.   ","In this work we advance the state of the art on interdependent values with private valuation functions, with respect to both SOS and $d$-critical valuations.","For SOS valuations, we devise a new mechanism that gives an improved approximation bound of $5$ for single-item auctions.","This mechanism employs a novel variant of an \"eating mechanism\", leveraging LP-duality to achieve feasibility with reduced welfare loss.","For $d$-critical valuations, we broaden the scope of existing results beyond single-item auctions, introducing a mechanism that gives a $(d+1)$-approximation for any environment with matroid feasibility constraints on the set of agents that can be simultaneously served.","Notably, this approximation bound is tight, even with respect to single-item auctions."],"url":"http://arxiv.org/abs/2402.12017v1","category":"cs.GT"}
{"created":"2024-02-19 09:52:41","title":"Direct Consistency Optimization for Compositional Text-to-Image Personalization","abstract":"Text-to-image (T2I) diffusion models, when fine-tuned on a few personal images, are able to generate visuals with a high degree of consistency. However, they still lack in synthesizing images of different scenarios or styles that are possible in the original pretrained models. To address this, we propose to fine-tune the T2I model by maximizing consistency to reference images, while penalizing the deviation from the pretrained model. We devise a novel training objective for T2I diffusion models that minimally fine-tunes the pretrained model to achieve consistency. Our method, dubbed \\emph{Direct Consistency Optimization}, is as simple as regular diffusion loss, while significantly enhancing the compositionality of personalized T2I models. Also, our approach induces a new sampling method that controls the tradeoff between image fidelity and prompt fidelity. Lastly, we emphasize the necessity of using a comprehensive caption for reference images to further enhance the image-text alignment. We show the efficacy of the proposed method on the T2I personalization for subject, style, or both. In particular, our method results in a superior Pareto frontier to the baselines. Generated examples and codes are in our project page( https://dco-t2i.github.io/).","sentences":["Text-to-image (T2I) diffusion models, when fine-tuned on a few personal images, are able to generate visuals with a high degree of consistency.","However, they still lack in synthesizing images of different scenarios or styles that are possible in the original pretrained models.","To address this, we propose to fine-tune the T2I model by maximizing consistency to reference images, while penalizing the deviation from the pretrained model.","We devise a novel training objective for T2I diffusion models that minimally fine-tunes the pretrained model to achieve consistency.","Our method, dubbed \\emph{Direct Consistency Optimization}, is as simple as regular diffusion loss, while significantly enhancing the compositionality of personalized T2I models.","Also, our approach induces a new sampling method that controls the tradeoff between image fidelity and prompt fidelity.","Lastly, we emphasize the necessity of using a comprehensive caption for reference images to further enhance the image-text alignment.","We show the efficacy of the proposed method on the T2I personalization for subject, style, or both.","In particular, our method results in a superior Pareto frontier to the baselines.","Generated examples and codes are in our project page( https://dco-t2i.github.io/)."],"url":"http://arxiv.org/abs/2402.12004v1","category":"cs.CV"}
